[
    {
        "paper_id": 2301.0752,
        "authors": "Elisa Luciano and Matteo Cattaneo and Ron Kenett",
        "title": "Adversarial AI in Insurance: Pervasiveness and Resilience",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  The rapid and dynamic pace of Artificial Intelligence (AI) and Machine\nLearning (ML) is revolutionizing the insurance sector. AI offers significant,\nvery much welcome advantages to insurance companies, and is fundamental to\ntheir customer-centricity strategy. It also poses challenges, in the project\nand implementation phase. Among those, we study Adversarial Attacks, which\nconsist of the creation of modified input data to deceive an AI system and\nproduce false outputs. We provide examples of attacks on insurance AI\napplications, categorize them, and argue on defence methods and precautionary\nsystems, considering that they can involve few-shot and zero-shot\nmultilabelling. A related topic, with growing interest, is the validation and\nverification of systems incorporating AI and ML components. These topics are\ndiscussed in various sections of this paper.\n"
    },
    {
        "paper_id": 2301.07543,
        "authors": "John J. Horton",
        "title": "Large Language Models as Simulated Economic Agents: What Can We Learn\n  from Homo Silicus?",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Newly-developed large language models (LLM) -- because of how they are\ntrained and designed -- are implicit computational models of humans -- a homo\nsilicus. These models can be used the same way economists use homo economicus:\nthey can be given endowments, information, preferences, and so on and then\ntheir behavior can be explored in scenarios via simulation. I demonstrate this\napproach using OpenAI's GPT3 with experiments derived from Charness and Rabin\n(2002), Kahneman, Knetsch and Thaler (1986) and Samuelson and Zeckhauser\n(1988). The findings are qualitatively similar to the original results, but it\nis also trivially easy to try variations that offer fresh insights. Departing\nfrom the traditional laboratory paradigm, I also create a hiring scenario where\nan employer faces applicants that differ in experience and wage ask and then\nanalyze how a minimum wage affects realized wages and the extent of labor-labor\nsubstitution.\n"
    },
    {
        "paper_id": 2301.07671,
        "authors": "Richard S.J. Tol",
        "title": "Navigating the energy trilemma during geopolitical and environmental\n  crises",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  There are many indicators of energy security. Few measure what really matters\n-- affordable and reliable energy supply -- and the trade-offs between the two.\nReliability is physical, affordability is economic. Russia's latest invasion of\nUkraine highlights some of the problems with energy security, from long-term\ncontracts being broken to supposedly secure supplies being diverted to retired\npower plants being recommissioned to spillovers to other markets. The\ntransition to carbon-free energy poses new challenges for energy security, from\na shift in dependence from some resources (coal, oil, gas) to others (rare\nearths, wind, sunshine) to substantial redundancies in the energy capital stock\nto undercapitalized energy companies, while regulatory uncertainty deters\ninvestment. Renewables improve energy security in one dimension, but worsen it\nin others, particularly long spells of little wind. Security problems with rare\nearths and borrowed capital are less pronounced, as stock rather than flow.\n"
    },
    {
        "paper_id": 2301.07798,
        "authors": "Jos\\'e-Luis P\\'erez, Kazutoshi Yamazaki",
        "title": "L\\'evy bandits under Poissonian decision times",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We consider a version of the continuous-time multi-armed bandit problem where\ndecision opportunities arrive at Poisson arrival times, and study its Gittins\nindex policy. When driven by spectrally one-sided L\\'evy processes, the Gittins\nindex can be written explicitly in terms of the scale function, and is shown to\nconverge to that in the classical L\\'evy bandit of Kaspi and Mandelbaum (1995).\n"
    },
    {
        "paper_id": 2301.0783,
        "authors": "Shuzhen Yang and Wenqing Zhang",
        "title": "Fixed-point iterative algorithm for SVI model",
        "comments": "32 pages, 11 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  The stochastic volatility inspired (SVI) model is widely used to fit the\nimplied variance smile. Presently, most optimizer algorithms for the SVI model\nhave a strong dependence on the input starting point. In this study, we develop\nan efficient iterative algorithm for the SVI model based on a fixed-point and\nleast-square optimizer. Furthermore, we present the convergence results in\ncertain situations for this novel iterative algorithm. Compared with the\nquasi-explicit SVI method, we demonstrate the advantages of the fixed-point\niterative algorithm using simulation and market data.\n"
    },
    {
        "paper_id": 2301.07979,
        "authors": "Kathyrn R. Fair and Omar A. Guerrero",
        "title": "Endogenous Labour Flow Networks",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  In the last decade, the study of labour dynamics has led to the introduction\nof labour flow networks (LFNs) as a way to conceptualise job-to-job\ntransitions, and to the development of mathematical models to explore the\ndynamics of these networked flows. To date, LFN models have relied upon an\nassumption of static network structure. However, as recent events (increasing\nautomation in the workplace, the COVID-19 pandemic, a surge in the demand for\nprogramming skills, etc.) have shown, we are experiencing drastic shifts to the\njob landscape that are altering the ways individuals navigate the labour\nmarket. Here we develop a novel model that emerges LFNs from agent-level\nbehaviour, removing the necessity of assuming that future job-to-job flows will\nbe along the same paths where they have been historically observed. This model,\ninformed by microdata for the United Kingdom, generates empirical LFNs with a\nhigh level of accuracy. We use the model to explore how shocks impacting the\nunderlying distributions of jobs and wages alter the topology of the LFN. This\nframework represents a crucial step towards the development of models that can\nanswer questions about the future of work in an ever-changing world.\n"
    },
    {
        "paper_id": 2301.08083,
        "authors": "Emma van Inwegen, Zanele Munyikwa, John J. Horton",
        "title": "Algorithmic Writing Assistance on Jobseekers' Resumes Increases Hires",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  There is a strong association between the quality of the writing in a resume\nfor new labor market entrants and whether those entrants are ultimately hired.\nWe show that this relationship is, at least partially, causal: a field\nexperiment in an online labor market was conducted with nearly half a million\njobseekers in which a treated group received algorithmic writing assistance.\nTreated jobseekers experienced an 8% increase in the probability of getting\nhired. Contrary to concerns that the assistance is taking away a valuable\nsignal, we find no evidence that employers were less satisfied. We present a\nmodel in which better writing is not a signal of ability but helps employers\nascertain ability, which rationalizes our findings.\n"
    },
    {
        "paper_id": 2301.08088,
        "authors": "Rytis Kazakevi\\v{c}ius, Aleksejus Kononovicius",
        "title": "Anomalous diffusion and long-range memory in the scaled voter model",
        "comments": "24 pages, 4 figures",
        "journal-ref": "Phys. Rev. E 107, 024106 (2023)",
        "doi": "10.1103/PhysRevE.107.024106",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We analyze the scaled voter model, which is a generalization of the noisy\nvoter model with time-dependent herding behavior. We consider the case when the\nintensity of herding behavior grows as a power-law function of time. In this\ncase, the scaled voter model reduces to the usual noisy voter model, but it is\ndriven by the scaled Brownian motion. We derive analytical expressions for the\ntime evolution of the first and second moments of the scaled voter model. In\naddition, we have derived an analytical approximation of the first passage time\ndistribution. By numerical simulation, we confirm our analytical results as\nwell as show that the model exhibits long-range memory indicators despite being\na Markov model. The proposed model has steady-state distribution consistent\nwith the bounded fractional Brownian motion, thus we expect it to be a good\nsubstitute model for the bounded fractional Brownian motion.\n"
    },
    {
        "paper_id": 2301.08135,
        "authors": "Karl Naumann-Woleske",
        "title": "Agent-based Integrated Assessment Models: Alternative Foundations to the\n  Environment-Energy-Economics Nexus",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-sa/4.0/",
        "abstract": "  Climate change is a major global challenge today. To assess how policies may\nlead to mitigation, economists have developed Integrated Assessment Models,\nhowever, most of the equilibrium based models have faced heavy critiques.\nAgent-based models have recently come to the fore as an alternative\nmacroeconomic modeling framework. In this paper, four Agent-based Integrated\nAssessment Models linking environment, energy and economy are reviewed. These\nmodels have several advantages over existing models in terms of their\nheterogeneous agents, the allocation of damages amongst the individual agents,\nrepresentation of the financial system, and policy mixes. While Agent-based\nIntegrated Assessment Models have made strong advances, there are several\navenues into which research should be continued, including incorporation of\nnatural resources and spatial dynamics, closer analysis of distributional\neffects and feedbacks, and multi-sectoral firm network structures.\n"
    },
    {
        "paper_id": 2301.08136,
        "authors": "Nizar Riane, Claire David",
        "title": "Input-Output Analysis: New Results From Markov Chain Theory",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  In this work, we propose a new lecture of input-output model reconciliation\nMarkov chain and the dominance theory, in the field of interindustrial poles\ninteractions. A deeper lecture of Leontieff table in term of Markov chain is\ngiven, exploiting spectral properties and time to absorption to characterize\nproduction processes, then the dualities local-global/dominance- Sensitivity\nanalysis are established, allowing a better understanding of economic poles\narrangement. An application to the Moroccan economy is given.\n"
    },
    {
        "paper_id": 2301.08232,
        "authors": "Andrew Na and Justin Wan",
        "title": "Efficient Pricing and Hedging of High Dimensional American Options Using\n  Recurrent Networks",
        "comments": null,
        "journal-ref": null,
        "doi": "10.1080/14697688.2023.2167666",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We propose a deep Recurrent neural network (RNN) framework for computing\nprices and deltas of American options in high dimensions. Our proposed\nframework uses two deep RNNs, where one network learns the price and the other\nlearns the delta of the option for each timestep. Our proposed framework yields\nprices and deltas for the entire spacetime, not only at a given point (e.g. t =\n0). The computational cost of the proposed approach is linear in time, which\nimproves on the quadratic time seen for feedforward networks that price\nAmerican options. The computational memory cost of our method is constant in\nmemory, which is an improvement over the linear memory costs seen in\nfeedforward networks. Our numerical simulations demonstrate these\ncontributions, and show that the proposed deep RNN framework is computationally\nmore efficient than traditional feedforward neural network frameworks in time\nand memory.\n"
    },
    {
        "paper_id": 2301.08302,
        "authors": "Anne de Bortoli and Maxime Agez",
        "title": "Environmentally-Extended Input-Output analyses efficiently sketch\n  large-scale environmental transition plans -- illustration by Canada's road\n  industry",
        "comments": null,
        "journal-ref": "Journal of Cleaner Production, Elsevier, 2023, 136039",
        "doi": "10.1016/j.jclepro.2023.136039",
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  Industries struggle to build robust environmental transition plans as they\nlack the tools to quantify their ecological responsibility over their value\nchain. Companies mostly turn to sole greenhouse gas (GHG) emissions reporting\nor time-intensive Life Cycle Assessment (LCA), while Environmentally-Extended\nInput-Output (EEIO) analysis is more efficient on a wider scale. We illustrate\nEEIO analysis usefulness to sketch transition plans on the example of Canada s\nroad industry - estimation of national environmental contributions, most\nimportant environmental issues, main potential transition levers of the sector,\nand metrics prioritization for green purchase plans). To do so, openIO-Canada,\na new Canadian EEIO database, coupled with IMPACT World plus v1.30-1.48\ncharacterization method, provides a multicriteria environmental diagnosis of\nCanada s economy. The road industry generates a limited impact (0.5-1.8\npercent) but must reduce the environmental burden from material purchases -\nmainly concrete and asphalt products - through green purchase plans and\neco-design and invest in new machinery powered with cleaner energies such as\nlow-carbon electricity or bioenergies. EEIO analysis also captures impacts\noften neglected in process-based pavement LCAs - amortization of capital goods,\nstaff consumptions, and services - and shows some substantial impacts\nadvocating for enlarging system boundaries in standard LCA. Yet, pavement\nconstruction and maintenance only explain 5 percent of the life cycle carbon\nfootprint of Canada s road network, against 95 percent for the roads usage.\nThereby, a carbon-neutral pathway for the road industry must first focus on\nreducing vehicle consumption and wear through better design and maintenance of\nroads (...)\n"
    },
    {
        "paper_id": 2301.08359,
        "authors": "Yuanrong Wang, Yinsen Miao, Alexander CY Wong, Nikita P Granger,\n  Christian Michler",
        "title": "Domain-adapted Learning and Interpretability: DRL for Gas Trading",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  Deep Reinforcement Learning (Deep RL) has been explored for a number of\napplications in finance and stock trading. In this paper, we present a\npractical implementation of Deep RL for trading natural gas futures contracts.\nThe Sharpe Ratio obtained exceeds benchmarks given by trend following and mean\nreversion strategies as well as results reported in literature. Moreover, we\npropose a simple but effective ensemble learning scheme for trading, which\nsignificantly improves performance through enhanced model stability and\nrobustness as well as lower turnover and hence lower transaction cost. We\ndiscuss the resulting Deep RL strategy in terms of model explainability,\ntrading frequency and risk measures.\n"
    },
    {
        "paper_id": 2301.0836,
        "authors": "Yuanrong Wang, Vignesh Raja Swaminathan, Nikita P. Granger, Carlos Ros\n  Perez, Christian Michler",
        "title": "Domain-adapted Learning and Imitation: DRL for Power Arbitrage",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  In this paper, we discuss the Dutch power market, which is comprised of a\nday-ahead market and an intraday balancing market that operates like an\nauction. Due to fluctuations in power supply and demand, there is often an\nimbalance that leads to different prices in the two markets, providing an\nopportunity for arbitrage. To address this issue, we restructure the problem\nand propose a collaborative dual-agent reinforcement learning approach for this\nbi-level simulation and optimization of European power arbitrage trading. We\nalso introduce two new implementations designed to incorporate domain-specific\nknowledge by imitating the trading behaviours of power traders. By utilizing\nreward engineering to imitate domain expertise, we are able to reform the\nreward system for the RL agent, which improves convergence during training and\nenhances overall performance. Additionally, the tranching of orders increases\nbidding success rates and significantly boosts profit and loss (P&L). Our study\ndemonstrates that by leveraging domain expertise in a general learning problem,\nthe performance can be improved substantially, and the final integrated\napproach leads to a three-fold improvement in cumulative P&L compared to the\noriginal agent. Furthermore, our methodology outperforms the highest benchmark\npolicy by around 50% while maintaining efficient computational performance.\n"
    },
    {
        "paper_id": 2301.08558,
        "authors": "Th\\'eodore Conrad, Arthur Vinciguerra, Guillaume M\\'erou\\'e",
        "title": "About constant-product automated market makers",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Constant-product market making functions were first introduced by Hayden\nAdams in 2017 to create Uniswap, a decentralised exchange on Ethereum. This\nenables users to exchange assets at any given rate. Some variations such as\nBalancer and Curve were later introduced. In this paper, we analyse the maths\nthat rule this type of protocol. We show that splitting a trade in multiple\nsmaller trades does not impact the final exchange rate. We also show that the\nprotocol is safer and more profitable when no one recompounds their fees.\n"
    },
    {
        "paper_id": 2301.08688,
        "authors": "Peer Nagy, Jan-Peter Calliess and Stefan Zohren",
        "title": "Asynchronous Deep Double Duelling Q-Learning for Trading-Signal\n  Execution in Limit Order Book Markets",
        "comments": null,
        "journal-ref": "Front. Artif. Intell., 25 September 2023 Sec. Artificial\n  Intelligence in Finance Volume 6 - 2023",
        "doi": "10.3389/frai.2023.1151003",
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  We employ deep reinforcement learning (RL) to train an agent to successfully\ntranslate a high-frequency trading signal into a trading strategy that places\nindividual limit orders. Based on the ABIDES limit order book simulator, we\nbuild a reinforcement learning OpenAI gym environment and utilise it to\nsimulate a realistic trading environment for NASDAQ equities based on historic\norder book messages. To train a trading agent that learns to maximise its\ntrading return in this environment, we use Deep Duelling Double Q-learning with\nthe APEX (asynchronous prioritised experience replay) architecture. The agent\nobserves the current limit order book state, its recent history, and a\nshort-term directional forecast. To investigate the performance of RL for\nadaptive trading independently from a concrete forecasting algorithm, we study\nthe performance of our approach utilising synthetic alpha signals obtained by\nperturbing forward-looking returns with varying levels of noise. Here, we find\nthat the RL agent learns an effective trading strategy for inventory management\nand order placing that outperforms a heuristic benchmark trading strategy\nhaving access to the same signal.\n"
    },
    {
        "paper_id": 2301.08797,
        "authors": "Carl Bonander, Mats Ekman, Niklas Jakobsson",
        "title": "When do Default Nudges Work?",
        "comments": "21 pages",
        "journal-ref": "Oxford Open Economics, Volume 2, 2023, odad094",
        "doi": "10.1093/ooec/odad094",
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  Nudging is a burgeoning topic in science and in policy, but evidence on the\neffectiveness of nudges among differentially-incentivized groups is lacking.\nThis paper exploits regional variations in the roll-out of the Covid-19 vaccine\nin Sweden to examine the effect of a nudge on groups whose intrinsic incentives\nare different: 16-17-year-olds, for whom Covid-19 is not dangerous, and\n50-59-year-olds, who face a substantial risk of death or severe dis-ease. We\nfind a significantly stronger response in the younger group, consistent with\nthe theory that nudges are more effective for choices that are not meaningful\nto the individual.\n"
    },
    {
        "paper_id": 2301.08803,
        "authors": "Julio Guerrero, Maria del Carmen Galiano, Giuseppe Orlando",
        "title": "Modeling COVID-19 pandemic with financial markets models: The case of\n  Ja\\'en (Spain)",
        "comments": "15 pages, 12 figures, Latex document",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  The main objective of this work is to test whether some stochastic models\ntypically used in financial markets could be applied to the COVID-19 pandemic.\nTo this end we have implemented the ARIMAX and Cox-Ingersoll-Ross (CIR) models\noriginally designed for interest rate pricing but transformed by us into a\nforecasting tool. For the latter, which we denoted CIR*, both the\nEuler-Maruyama method and the Milstein method were used. Forecasts obtained\nwith the maximum likelihood method have been validated with 95\\% confidence\nintervals and with statistical measures of goodness of fit, such as the root\nmean square error (RMSE). We demonstrate that the accuracy of the obtained\nresults is consistent with the observations and sufficiently accurate to the\npoint that the proposed CIR* framework could be considered a valid alternative\nto the classical ARIMAX for modelling pandemics.\n"
    },
    {
        "paper_id": 2301.08847,
        "authors": "Jongsub Lee and Hayong Yun",
        "title": "Learning Production Process Heterogeneity Across Industries:\n  Implications of Deep Learning for Corporate M&A Decisions",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  Using deep learning techniques, we introduce a novel measure for production\nprocess heterogeneity across industries. For each pair of industries during\n1990-2021, we estimate the functional distance between two industries'\nproduction processes via deep neural network. Our estimates uncover the\nunderlying factors and weights reflected in the multi-stage production decision\ntree in each industry. We find that the greater the functional distance between\ntwo industries' production processes, the lower are the number of M&As, deal\ncompletion rates, announcement returns, and post-M&A survival likelihood. Our\nresults highlight the importance of structural heterogeneity in production\ntechnology to firms' business integration decisions.\n"
    },
    {
        "paper_id": 2301.09163,
        "authors": "Pierre Lavigne and Peter Tankov",
        "title": "Decarbonization of financial markets: a mean-field game approach",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We build a model of a financial market where a large number of firms\ndetermine their dynamic emission strategies under climate transition risk in\nthe presence of both green-minded and neutral investors. The firms aim to\nachieve a trade-off between financial and environmental performance, while\ninteracting through the stochastic discount factor, determined in equilibrium\nby the investors' allocations. We formalize the problem in the setting of\nmean-field games and prove the existence and uniqueness of a Nash equilibrium\nfor firms. We then present a convergent numerical algorithm for computing this\nequilibrium and illustrate the impact of climate transition risk and the\npresence of green-minded investors on the market decarbonization dynamics and\nshare prices. We show that uncertainty about future climate risks and policies\nleads to higher overall emissions and higher spreads between share prices of\ngreen and brown companies. This effect is partially reversed in the presence of\nenvironmentally concerned investors, whose impact on the cost of capital spurs\ncompanies to reduce emissions.\n"
    },
    {
        "paper_id": 2301.09173,
        "authors": "Mykola Pinchuk",
        "title": "Labor Income Risk and the Cross-Section of Expected Returns",
        "comments": "47 p, 5 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  This paper explores asset pricing implications of unemployment risk from\nsectoral shifts. I proxy for this risk using cross-industry dispersion (CID),\ndefined as a mean absolute deviation of returns of 49 industry portfolios. CID\npeaks during periods of accelerated sectoral reallocation and heightened\nuncertainty. I find that expected stock returns are related cross-sectionally\nto the sensitivities of returns to innovations in CID. Annualized returns of\nthe stocks with high sensitivity to CID are 5.9% lower than the returns of the\nstocks with low sensitivity. Abnormal returns with respect to the best factor\nmodel are 3.5%, suggesting that common factors can not explain this return\nspread. Stocks with high sensitivity to CID are likely to be the stocks, which\nbenefited from sectoral shifts. CID positively predicts unemployment through\nits long-term component, consistent with the hypothesis that CID is a proxy for\nunemployment risk from sectoral shifts.\n"
    },
    {
        "paper_id": 2301.09241,
        "authors": "Jianjun Chen, Yongming Li, Ariel Neufeld",
        "title": "Quantum Monte Carlo algorithm for solving Black-Scholes PDEs for\n  high-dimensional option pricing in finance and its complexity analysis",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper we provide a quantum Monte Carlo algorithm to solve\nhigh-dimensional Black-Scholes PDEs with correlation for high-dimensional\noption pricing. The payoff function of the option is of general form and is\nonly required to be continuous and piece-wise affine (CPWA), which covers most\nof the relevant payoff functions used in finance. We provide a rigorous error\nanalysis and complexity analysis of our algorithm. In particular, we prove that\nthe computational complexity of our algorithm is bounded polynomially in the\nspace dimension $d$ of the PDE and the reciprocal of the prescribed accuracy\n$\\varepsilon$. Moreover, we show that for payoff functions which are bounded,\nour algorithm indeed has a speed-up compared to classical Monte Carlo methods.\nFurthermore, we provide numerical simulations in one and two dimensions using\nour developed package within the Qiskit framework tailored to price CPWA\noptions with respect to the Black-Scholes model, as well as discuss the\npotential extension of the numerical simulations to arbitrary space dimension.\n"
    },
    {
        "paper_id": 2301.09252,
        "authors": "Carlos G\\'oes and Gladys Lopez-Acevedo and Raymond Robertson",
        "title": "Gender-Segmented Labor Markets and Foreign Demand Shocks",
        "comments": "35 pages, 4 figures, 13 tables",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
        "abstract": "  Gender segmentation in labor markets shapes the local effects of\ninternational trade. We develop a theory that combines exports with\ngender-segmented labor markets and show that, in this framework, foreign demand\nshocks may either increase or decrease the female-to-male employment ratio. If\na foreign demand shock happens in a female-intensive (male-intensive) sector,\nthe model predicts that the female-to-male employment ratio should increase\n(decrease). We then use plausibly exogenous variation in the exposure of\nTunisian local labor markets to foreign demand shocks and show that the\nempirical results are consistent with the theoretical prediction. In Tunisia, a\ndeveloping country with a high degree of gender segmentation in labor markets,\nforeign-demand shocks have been relatively larger in male-intensive sectors.\nThis induced a decrease in the female-to-male employment ratio, with households\nlikely substituting female for male labor supply.\n"
    },
    {
        "paper_id": 2301.09261,
        "authors": "Werry Febrianti, Kuntjoro Adji Sidarto, and Novriana Sumarti",
        "title": "The Combinational Mutation Strategy of Differential Evolution Algorithm\n  for Pricing Vanilla Options and Its Implementation on Data during Covid-19\n  Pandemic",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Investors always want to know about the profit and the risk that they will be\nget before buying some assets. Our main focus is getting the profit and the\nprobability of getting that profit using the differential evolution algorithm\nfor vanilla option pricing on data before and during COVID-19 pandemic.\nTherefore, we model the pricing of an option using a bi-objective optimization\nproblem using data before and during COVID-19 pandemic for one year expiration\ndate. We change this problem into an optimization problem using adaptive\nweighted sum method. We use metaheuristics algorithm like Differential\nEvolution (DE) algorithm to solve this bi-objective optimization problems. In\nthis paper, we also use modification of Differential Evolution for getting\nPareto optimal solutions on vanilla option pricing for all contract. The\nalgorithm is called Combinational Mutation Strategy of Differential Evolution\n(CmDE) algorithm. The results of our algorithm are satisfactory close to the\nreal option price in the market data. Besides that, we also compare our result\nwith the Black-Scholes results for validation. The results show that our\nresults can approximate the real market options more accurate than\nBlack-Scholes results. Hence, our bi-objective optimization using Combinational\nMutation Strategy of Differential Evolution algorithm can be used to\napproximate the market real vanilla option pricing before and during COVID-19\npandemic.\n"
    },
    {
        "paper_id": 2301.09279,
        "authors": "Jean Lee, Hoyoul Luis Youn, Josiah Poon, Soyeon Caren Han",
        "title": "StockEmotions: Discover Investor Emotions for Financial Sentiment\n  Analysis and Multivariate Time Series",
        "comments": "Preprint - Accepted by the AAAI-23 Bridge Program (AI for Financial\n  Services)",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-sa/4.0/",
        "abstract": "  There has been growing interest in applying NLP techniques in the financial\ndomain, however, resources are extremely limited. This paper introduces\nStockEmotions, a new dataset for detecting emotions in the stock market that\nconsists of 10,000 English comments collected from StockTwits, a financial\nsocial media platform. Inspired by behavioral finance, it proposes 12\nfine-grained emotion classes that span the roller coaster of investor emotion.\nUnlike existing financial sentiment datasets, StockEmotions presents granular\nfeatures such as investor sentiment classes, fine-grained emotions, emojis, and\ntime series data. To demonstrate the usability of the dataset, we perform a\ndataset analysis and conduct experimental downstream tasks. For financial\nsentiment/emotion classification tasks, DistilBERT outperforms other baselines,\nand for multivariate time series forecasting, a Temporal Attention LSTM model\ncombining price index, text, and emotion features achieves the best performance\nthan using a single feature.\n"
    },
    {
        "paper_id": 2301.09297,
        "authors": "Huifang Huang and Ting Gao and Pengbo Li and Jin Guo and Peng Zhang\n  and Nan Du",
        "title": "Model Based Reinforcement Learning with Non-Gaussian Environment\n  Dynamics and its Application to Portfolio Optimization",
        "comments": "arXiv admin note: text overlap with arXiv:2205.15056",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  With the fast development of quantitative portfolio optimization in financial\nengineering, lots of AI-based algorithmic trading strategies have demonstrated\npromising results, among which reinforcement learning begins to manifest\ncompetitive advantages. However, the environment from real financial markets is\ncomplex and hard to be fully simulated, considering the observation of abrupt\ntransitions, unpredictable hidden causal factors, heavy tail properties and so\non. Thus, in this paper, first, we adopt a heavy-tailed preserving normalizing\nflows to simulate high-dimensional joint probability of the complex trading\nenvironment and develop a model-based reinforcement learning framework to\nbetter understand the intrinsic mechanisms of quantitative online trading.\nSecond, we experiment with various stocks from three different financial\nmarkets (Dow, NASDAQ and S&P) and show that among these three financial\nmarkets, Dow gets the best performance based on various evaluation metrics\nunder our back-testing system. Especially, our proposed method is able to\nmitigate the impact of unpredictable financial market crises during the\nCOVID-19 pandemic period, resulting in a lower maximum drawdown. Third, we also\nexplore the explanation of our RL algorithm. (1), we utilize the pattern\ncausality method to study the interactive relation among different stocks in\nthe environment. (2), We analyze the dynamic loss and actor loss to ensure the\nconvergence of our strategies. (3), by visualizing high dimensional state\ntransition data comparisons from real and virtual buffers with t-SNE, we\nuncover some effective patterns of better portfolio optimization strategies.\n(4), we also utilize eigenvalue analysis to study the convergence properties of\nthe environmen's model.\n"
    },
    {
        "paper_id": 2301.09438,
        "authors": "Arturo Ramos, Till Massing, Atushi Ishikawa, Shouji Fujimoto, Takayuki\n  Mizuno",
        "title": "Composite distributions in the social sciences: A comparative empirical\n  study of firms' sales distribution for France, Germany, Italy, Japan, South\n  Korea, and Spain",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We study 17 different statistical distributions for sizes obtained {}from the\nclassical and recent literature to describe a relevant variable in the social\nsciences and Economics, namely the firms' sales distribution in six countries\nover an ample period. We find that the best results are obtained with mixtures\nof lognormal (LN), loglogistic (LL), and log Student's $t$ (LSt) distributions.\nThe single lognormal, in turn, is strongly not selected. We then find that the\nwhole firm size distribution is better described by a mixture, and there exist\nsubgroups of firms. Depending on the method of measurement, the best fitting\ndistribution cannot be defined by a single one, but as a mixture of at least\nthree distributions or even four or five. We assess a full sample analysis, an\nin-sample and out-of-sample analysis, and a doubly truncated sample analysis.\nWe also provide the formulation of the preferred models as solutions of the\nFokker--Planck or forward Kolmogorov equation.\n"
    },
    {
        "paper_id": 2301.0945,
        "authors": "Nils Engler, Filip Lindskog",
        "title": "Approximations of multi-period liability values by simple formulas",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  This paper is motivated by computational challenges arising in multi-period\nvaluation in insurance. Aggregate insurance liability cashflows typically\ncorrespond to stochastic payments several years into the future. However,\ninsurance regulation requires that capital requirements are computed for a\none-year horizon, by considering cashflows during the year and end-of-year\nliability values. This implies that liability values must be computed\nrecursively, backwards in time, starting from the year of the most distant\nliability payments. Solving such backward recursions with paper and pen is\nrarely possible, and numerical solutions give rise to major computational\nchallenges.\n  The aim of this paper is to provide explicit and easily computable\nexpressions for multi-period valuations that appear as limit objects for a\nsequence of multi-period models that converge in terms of conditional weak\nconvergence. Such convergence appears naturally if we consider large insurance\nportfolios such that the liability cashflows, appropriately centered and\nscaled, converge weakly as the size of the portfolio tends to infinity.\n"
    },
    {
        "paper_id": 2301.09705,
        "authors": "A. Papanicolaou, H. Fu, P. Krishnamurthy, B. Healy, F. Khorrami",
        "title": "An Optimal Control Strategy for Execution of Large Stock Orders Using\n  LSTMs",
        "comments": "This work was partially supported by NSF grant DMS-1907518 and in\n  part by the New York University Abu Dhabi (NYUAD) Center for Artificial\n  Intelligence and Robotics, funded by Tamkeen under the NYUAD Research\n  Institute Award CG010",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  In this paper, we simulate the execution of a large stock order with real\ndata and general power law in the Almgren and Chriss model. The example that we\nconsider is the liquidation of a large position executed over the course of a\nsingle trading day in a limit order book. Transaction costs are incurred\nbecause large orders walk the order book, that is, they consume order book\nliquidity beyond the best bid/ask. We model the order book with a power law\nthat is proportional to trading volume, and thus transaction costs are\ninversely proportional to a power of trading volume. We obtain a policy\napproximation by training a long short term memory (LSTM) neural network to\nminimize transaction costs accumulated when execution is carried out as a\nsequence of smaller suborders. Using historical S&P100 price and volume data,\nwe evaluate our LSTM strategy relative to strategies based on time-weighted\naverage price (TWAP) and volume-weighted average price (VWAP). For execution of\na single stock, the input to the LSTM is the cross section of data on all 100\nstocks, including prices, volumes, TWAPs and VWAPs. By using this data cross\nsection, the LSTM should be able to exploit inter-stock co-dependence in volume\nand price movements, thereby reducing transaction costs for the day. Our tests\non S&P100 data demonstrate that in fact this is so, as our LSTM strategy\nconsistently outperforms TWAP and VWAP-based strategies.\n"
    },
    {
        "paper_id": 2301.09722,
        "authors": "Beatrice Foroni, Luca Merlo, Lea Petrella",
        "title": "Expectile hidden Markov regression models for analyzing cryptocurrency\n  returns",
        "comments": null,
        "journal-ref": null,
        "doi": "10.1007/s11222-023-10377-2",
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  In this paper we develop a linear expectile hidden Markov model for the\nanalysis of cryptocurrency time series in a risk management framework. The\nmethodology proposed allows to focus on extreme returns and describe their\ntemporal evolution by introducing in the model time-dependent coefficients\nevolving according to a latent discrete homogeneous Markov chain. As it is\noften used in the expectile literature, estimation of the model parameters is\nbased on the asymmetric normal distribution. Maximum likelihood estimates are\nobtained via an Expectation-Maximization algorithm using efficient M-step\nupdate formulas for all parameters. We evaluate the introduced method with both\nartificial data under several experimental settings and real data investigating\nthe relationship between daily Bitcoin returns and major world market indices.\n"
    },
    {
        "paper_id": 2301.09856,
        "authors": "Anastasios Petropoulos, Vassilis Siakoulis, Konstantinos P. Panousis,\n  Loukas Papadoulas, Sotirios Chatzis",
        "title": "Macroeconomic forecasting and sovereign risk assessment using deep\n  learning techniques",
        "comments": "arXiv admin note: substantial text overlap with arXiv:2009.11075",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  In this study, we propose a novel approach of nowcasting and forecasting the\nmacroeconomic status of a country using deep learning techniques. We focus\nparticularly on the US economy but the methodology can be applied also to other\neconomies. Specifically US economy has suffered a severe recession from 2008 to\n2010 which practically breaks out conventional econometrics model attempts.\nDeep learning has the advantage that it models all macro variables\nsimultaneously taking into account all interdependencies among them and\ndetecting non-linear patterns which cannot be easily addressed under a\nunivariate modelling framework. Our empirical results indicate that the deep\nlearning methods have a superior out-of-sample performance when compared to\ntraditional econometric techniques such as Bayesian Model Averaging (BMA).\nTherefore our results provide a concise view of a more robust method for\nassessing sovereign risk which is a crucial component in investment and\nmonetary decisions.\n"
    },
    {
        "paper_id": 2301.09968,
        "authors": "Jean Cyrus de Gourcuff, David Makowski, Philippe Ciais, Marc\n  Barthelemy",
        "title": "Impact of the Ukrainian crisis on the global food security",
        "comments": "Main paper and supplementary material",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Using global wheat trade data and a network model for shock propagation, we\nstudy the impact of the Ukrainian crisis on food security. Depending on the\nlevel of reduction in Ukrainian wheat exports, the number of additional\nindividuals falling under the minimum dietary energy requirement varies from 1\nto 9 millions, and reaches about 4.8 millions for a $50\\%$ reduction in\nexports. In the most affected countries, supply reductions are mainly related\nto indirect trade restrictions.\n"
    },
    {
        "paper_id": 2301.09982,
        "authors": "Gerard J. van den Berg, Stephanie von Hinke, R. Adele H. Wang",
        "title": "Prenatal Sugar Consumption and Late-Life Human Capital and Health:\n  Analyses Based on Postwar Rationing and Polygenic Scores",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Maternal sugar consumption in utero may have a variety of effects on\noffspring. We exploit the abolishment of the rationing of sweet confectionery\nin the UK on April 24, 1949, and its subsequent reintroduction some months\nlater, in an era of otherwise uninterrupted rationing of confectionery\n(1942-1953), sugar (1940-1953) and many other foods, and we consider effects on\nlate-life cardiovascular disease, BMI, height, type-2 diabetes and the intake\nof sugar, fat and carbohydrates, as well as cognitive outcomes and birth\nweight. We use individual-level data from the UK Biobank for cohorts born\nbetween April 1947-May 1952. We also explore whether one's genetic\n\"predisposition\" to the outcome can moderate the effects of prenatal sugar\nexposure. We find that prenatal exposure to derationing increases education and\nreduces BMI and sugar consumption at higher ages, in line with the\n\"developmental origins\" explanatory framework, and that the sugar effects are\nstronger for those who are genetically \"predisposed\" to sugar consumption.\n"
    },
    {
        "paper_id": 2301.09996,
        "authors": "Richard J. Martin",
        "title": "Black-Scholes without stochastics or PDEs",
        "comments": "Added S1.7 on derivation of the delta",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  We show how to derive the Black-Scholes model and its generalisation to the\n`exchange-option' (to exchange one asset for another) via the continuum limit\nof the Binomial tree. No knowledge of stochastic calculus or partial\ndifferential equations is assumed, as we do not use them.\n"
    },
    {
        "paper_id": 2301.10044,
        "authors": "Kenichiro Shiraya, Tomohisa Yamakami",
        "title": "Constructing Copulas Using Corrected Hermite Polynomial Expansion for\n  Estimating Cross Foreign Exchange Volatility",
        "comments": "36 pages, 48 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Copulas are used to construct joint distributions in many areas. In some\nproblems, it is necessary to deal with correlation structures that are more\ncomplicated than the commonly known copulas. A finite order multivariate\nHermite polynomial expansion, as an approximation of a joint density function,\ncan handle complex correlation structures. However, it does not construct\ncopulas because the density function can take negative values. In this study,\nwe propose a method to construct a copula based on the finite sum of\nmultivariate Hermite polynomial expansions by applying corrections to the joint\ndensity function. Furthermore, we apply this copula to estimate the volatility\nsmile of cross currency pairs in the foreign exchange option market. This\nmethod can easily reproduce the volatility smile of cross currency pairs by\nappropriately adjusting the parameters and following the daily volatility\nfluctuations even if the higher-order parameters are fixed. In the numerical\nexperiments, we compare the estimation results of the volatility smile of\nEUR-JPY with those of USD-JPY and EUR-USD for the proposed and other copulas,\nand show the validity of the proposed copula.\n"
    },
    {
        "paper_id": 2301.10117,
        "authors": "Mykola Pinchuk",
        "title": "Bitcoin Does Not Hedge Inflation",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  This paper examines the response of major cryptocurrencies to macroeconomic\nnews announcements (MNA). While other cryptocurrencies exhibit no reaction to\nmajor MNA, Bitcoin responds negatively to inflation surprise. Price of Bitcoin\ndecreases by 24 bps in response to a 1 standard deviation inflationary\nsurprise. This reaction is inconsistent with widely-held beliefs of\npractitioners that Bitcoin can hedge inflation. I do not find support for the\nhypothesis that the negative response of Bitcoin to inflation is due to its\nnegative exposure to interest rates. Instead, I find support for the hypothesis\nthat Bitcoin is strongly affected by the shift in consumption-savings\ndecisions, driven by the rise in inflation. Consistent with this view, Bitcoin\nhas negative exposure to a proxy for the consumption-savings ratio.\n"
    },
    {
        "paper_id": 2301.10153,
        "authors": "Tzu-Ya Lai, Wen Jung Cheng and Jun-En Ding",
        "title": "Sequential Graph Attention Learning for Predicting Dynamic Stock Trends\n  (Student Abstract)",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  The stock market is characterized by a complex relationship between companies\nand the market. This study combines a sequential graph structure with attention\nmechanisms to learn global and local information within temporal time.\nSpecifically, our proposed \"GAT-AGNN\" module compares model performance across\nmultiple industries as well as within single industries. The results show that\nthe proposed framework outperforms the state-of-the-art methods in predicting\nstock trends across multiple industries on Taiwan Stock datasets.\n"
    },
    {
        "paper_id": 2301.10166,
        "authors": "Christopher Wimmer, Navid Rekabsaz",
        "title": "Leveraging Vision-Language Models for Granular Market Change Prediction",
        "comments": "Accepted at Multimodal AI for Financial Forecasting Workshop (Muffin)\n  at AAAI 2023",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Predicting future direction of stock markets using the historical data has\nbeen a fundamental component in financial forecasting. This historical data\ncontains the information of a stock in each specific time span, such as the\nopening, closing, lowest, and highest price. Leveraging this data, the future\ndirection of the market is commonly predicted using various time-series models\nsuch as Long-Short Term Memory networks. This work proposes modeling and\npredicting market movements with a fundamentally new approach, namely by\nutilizing image and byte-based number representation of the stock data\nprocessed with the recently introduced Vision-Language models. We conduct a\nlarge set of experiments on the hourly stock data of the German share index and\nevaluate various architectures on stock price prediction using historical stock\ndata. We conduct a comprehensive evaluation of the results with various metrics\nto accurately depict the actual performance of various approaches. Our\nevaluation results show that our novel approach based on representation of\nstock data as text (bytes) and image significantly outperforms strong deep\nlearning-based baselines.\n"
    },
    {
        "paper_id": 2301.10178,
        "authors": "Moawia Alghalith",
        "title": "Methods in Econophysics: Estimating the Probability Density and\n  Volatility",
        "comments": null,
        "journal-ref": null,
        "doi": "10.3389/fphy.2022.1050277",
        "license": "http://creativecommons.org/publicdomain/zero/1.0/",
        "abstract": "  We discuss and analyze some recent literature that introduced pioneering\nmethods in econophysics. In doing so, we review recent methods of estimating\nthe volatility, volatility of volatility, and probability densities. These\nmethods will have useful applications in econophysics and finance.\n"
    },
    {
        "paper_id": 2301.10179,
        "authors": "Zhanpeng Huang",
        "title": "Research on the Impact of Innovative City and Smart City Construction on\n  Digital Economy: Evidence from China",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Does the national innovation city and smart city pilot policy, as an\nimportant institutional design to promote the transformation of old and new\ndynamics, have an important impact on the digital economy? What are the\nintrinsic mechanisms? Based on the theoretical analysis of whether smart city\nand national innovation city policies promote urban digital economy, this paper\nconstructs a multi-temporal double difference model based on a quasi-natural\nexperiment with urban dual pilot policies and systematically investigates the\nimpact of dual pilot policies on the development of digital economy. It is\nfound that both smart cities and national innovation cities can promote the\ndevelopment of digital economy, while there is a synergistic effect between the\npolicies. The mechanism test shows that the smart city construction and\nnational innovation city construction mainly affect the digital economy through\ntalent agglomeration effect, technology agglomeration effect and financial\nagglomeration effect.\n"
    },
    {
        "paper_id": 2301.10423,
        "authors": "Bikramjit Das and Vicky Fasen-Hartmann",
        "title": "Aggregating heavy-tailed random vectors: from finite sums to L\\'evy\n  processes",
        "comments": "39 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  The tail behavior of aggregates of heavy-tailed random vectors is known to be\ndetermined by the so-called principle of \"one large jump'', be it for finite\nsums, random sums, or, L\\'evy processes. We establish that, in fact, a more\ngeneral principle is at play. Assuming that the random vectors are multivariate\nregularly varying on various subcones of the positive quadrant, first we show\nthat their aggregates are also multivariate regularly varying on these\nsubcones. This allows us to approximate certain tail probabilities which were\nrendered asymptotically negligible under classical regular variation, despite\nthe \"one large jump'' asymptotics. We also discover that depending on the\nstructure of the tail event of concern, the tail behavior of the aggregates may\nbe characterized by more than a single large jump. Eventually, we illustrate a\nsimilar phenomenon for multivariate regularly varying L\\'evy processes,\nestablishing as well a relationship between multivariate regular variation of a\nL\\'evy process and multivariate regular variation of its L\\'evy measure on\ndifferent subcones.\n"
    },
    {
        "paper_id": 2301.10541,
        "authors": "Jiasheng Zhu and Luyao Zhang",
        "title": "Educational Game on Cryptocurrency Investment: Using Microeconomic\n  Decision Making to Understand Macroeconomics Principles",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Gamification is an effective strategy for motivating and engaging users,\nwhich is grounded in business, marketing, and management by designing games in\nnongame contexts. Gamifying education, which consists of the design and study\nof educational games, is an emerging trend. However, the existing classroom\ngames for understanding macroeconomics have weak connections to the\nmicrofoundations of individual decision-making. We design an educational game\non cryptocurrency investment for understanding macroeconomic concepts in\nmicroeconomic decisions. We contribute to the literature by designing\ngame-based learning that engages students in understanding macroeconomics in\nincentivized individual investment decisions. Our game can be widely\nimplemented in online, in-person, and hybrid classrooms. We also reflect on\nstrategies for improving the user experience for future educational game\nimplementations.\n"
    },
    {
        "paper_id": 2301.10558,
        "authors": "Gerard J. van den Berg, Stephanie von Hinke, Nicolai Vitt",
        "title": "Early life exposure to measles and later-life outcomes: Evidence from\n  the introduction of a vaccine",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Until the mid 1960s, the UK experienced regular measles epidemics, with the\nvast majority of children being infected in early childhood. The introduction\nof a measles vaccine substantially reduced its incidence. The first part of\nthis paper examines the long-term human capital and health effects of this\nchange in the early childhood disease environment. The second part investigates\ninteractions between the vaccination campaign and individuals' endowments as\ncaptured using molecular genetic data, shedding light on complementarities\nbetween public health investments and individual endowments. We use two\nidentification approaches, based on the nationwide introduction of the vaccine\nin 1968 and local vaccination trials in 1966. Our results show that exposure to\nthe vaccination in early childhood positively affects adult height, but only\namong those with high genetic endowments for height. We find no effects on\nyears of education; neither a direct effect, nor evidence of complementarities.\n"
    },
    {
        "paper_id": 2301.10675,
        "authors": "Qiren Liu, Sen Luo and Robert Seamans",
        "title": "Pain or Anxiety? The Health Consequences of Rising Robot Adoption in\n  China",
        "comments": "22 pages, 8 tables",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  The rising adoption of industrial robots is radically changing the role of\nworkers in the production process. Robots can be used for some of the more\nphysically demanding and dangerous production work, thus reducing the\npossibility of worker injury. On the other hand, robots may replace workers,\npotentially increasing worker anxiety about their job safety. In this paper, we\ninvestigate how individual physical health and mental health outcomes vary with\nlocal exposure to robots for manufacturing workers in China. We find a link\nbetween robot exposure and better physical health of workers, particularly for\nyounger workers and those with less education. However, we also find that robot\nexposure is associated with more mental stress for Chinese workers,\nparticularly older and less educated workers.\n"
    },
    {
        "paper_id": 2301.10724,
        "authors": "Weiguang Han, Boyi Zhang, Qianqian Xie, Min Peng, Yanzhao Lai, Jimin\n  Huang",
        "title": "Select and Trade: Towards Unified Pair Trading with Hierarchical\n  Reinforcement Learning",
        "comments": "10 pages, 6 figures",
        "journal-ref": null,
        "doi": "10.1145/3580305.3599951",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Pair trading is one of the most effective statistical arbitrage strategies\nwhich seeks a neutral profit by hedging a pair of selected assets. Existing\nmethods generally decompose the task into two separate steps: pair selection\nand trading. However, the decoupling of two closely related subtasks can block\ninformation propagation and lead to limited overall performance. For pair\nselection, ignoring the trading performance results in the wrong assets being\nselected with irrelevant price movements, while the agent trained for trading\ncan overfit to the selected assets without any historical information of other\nassets. To address it, in this paper, we propose a paradigm for automatic pair\ntrading as a unified task rather than a two-step pipeline. We design a\nhierarchical reinforcement learning framework to jointly learn and optimize two\nsubtasks. A high-level policy would select two assets from all possible\ncombinations and a low-level policy would then perform a series of trading\nactions. Experimental results on real-world stock data demonstrate the\neffectiveness of our method on pair trading compared with both existing pair\nselection and trading methods.\n"
    },
    {
        "paper_id": 2301.10734,
        "authors": "Rakhymzhan Kazbek and Yogi Erlangga and Yerlan Amanbek and Dongming\n  Wei",
        "title": "Valuation of the Convertible Bonds under Penalty TF model using Finite\n  Element Method",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  In this paper, the TF system of two-coupled Black-Scholes equations for\npricing the convertible bonds is solved numerically by using the P1 and P2\nfinite elements with the inequality constraints approximated by the penalty\nmethod. The corresponding finite element ODE system is numerically solved by\nusing a modified Crank-Nicolson scheme, in which the non-linear system is\nsolved at each time step by the Newton-Raphson method for non-smooth functions.\nMoreover, the corresponding Greeks are also calculated by taking advantage of\nthe P1-P2 finite element approximation functions. Numerical solutions by the\nfinite element method compare favorably with the solutions by the finite\ndifference method in literature.\n"
    },
    {
        "paper_id": 2301.10869,
        "authors": "Andrew Papanicolaou, Hao Fu, Prashanth Krishnamurthy, Farshad Khorrami",
        "title": "A Deep Neural Network Algorithm for Linear-Quadratic Portfolio\n  Optimization with MGARCH and Small Transaction Costs",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We analyze a fixed-point algorithm for reinforcement learning (RL) of optimal\nportfolio mean-variance preferences in the setting of multivariate generalized\nautoregressive conditional-heteroskedasticity (MGARCH) with a small penalty on\ntrading. A numerical solution is obtained using a neural network (NN)\narchitecture within a recursive RL loop. A fixed-point theorem proves that NN\napproximation error has a big-oh bound that we can reduce by increasing the\nnumber of NN parameters. The functional form of the trading penalty has a\nparameter $\\epsilon>0$ that controls the magnitude of transaction costs. When\n$\\epsilon$ is small, we can implement an NN algorithm based on the expansion of\nthe solution in powers of $\\epsilon$. This expansion has a base term equal to a\nmyopic solution with an explicit form, and a first-order correction term that\nwe compute in the RL loop. Our expansion-based algorithm is stable, allows for\nfast computation, and outputs a solution that shows positive testing\nperformance.\n"
    },
    {
        "paper_id": 2301.10898,
        "authors": "Yuchao Dong and Jin Liang and Claude-Michel Brauner",
        "title": "Double free boundary problem for defaultable corporate bond with credit\n  rating migration risks and their asymptotic behaviors",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  In this work, a pricing model for a defaultable corporate bond with credit\nrating migration risk is established. The model turns out to be a free boundary\nproblem with two free boundaries. The latter are the level sets of the solution\nbut of different kinds. One is from the discontinuous second order term, the\nother from the obstacle. Existence, uniqueness, and regularity of the solution\nare obtained. We also prove that two free boundaries are $C^\\infty$. The\nasymptotic behavior of the solution is also considered: we show that it\nconverges to a traveling wave solution when time goes to infinity. Moreover,\nnumerical results are presented.\n"
    },
    {
        "paper_id": 2301.10944,
        "authors": "Yuxuan Lu, Qian Qi, Xi Chen",
        "title": "A Framework of Transaction Packaging in High-throughput Blockchains",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We develop a model of coordination and allocation of decentralized\nmulti-sided markets, in which our theoretical analysis is promisingly\noptimizing the decentralized transaction packaging process at high-throughput\nblockchains or Web 3.0 platforms. In contrast to the stylized centralized\nplatform, the decentralized platform is powered by blockchain technology, which\nallows for secure and transparent Peer-to-Peer transactions among users.\nTraditional single-chain-based blockchains suffer from the well-known\nblockchain trilemma. Beyond the single-chain-based scheme, decentralized\nhigh-throughput blockchains adopt parallel protocols to reconcile the\nblockchain trilemma, implementing any tasking and desired allocation. However,\nunneglectable network latency may induce partial observability, resulting in\nincoordination and misallocation issues for the decentralized transaction\npackaging process at the current high-throughput blockchain protocols.\n  To address this problem, we consider a strategic coordination mechanism for\nthe decentralized transaction packaging process by using a game-theoretic\napproach. Under a tractable two-period model, we find a Bayesian Nash\nequilibrium of the miner's strategic transaction packaging under partial\nobservability. Along with novel algorithms for computing equilibrium payoffs,\nwe show that the decentralized platform can achieve an efficient and stable\nmarket outcome. The model also highlights that the proposed mechanism can\nendogenously offer a base fee per gas without any restructuration of the\ninitial blockchain transaction fee mechanism. The theoretical results that\nunderlie the algorithms also imply bounds on the computational complexity of\nequilibrium payoffs.\n"
    },
    {
        "paper_id": 2301.10985,
        "authors": "Nassim Nicholas Taleb, Ron Richman, Marcos Carreira, James Sharpe",
        "title": "The Probability Conflation: A Reply",
        "comments": "Accepted, International Journal of Forecasting",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  We respond to Tetlock et al. (2022) showing 1) how expert judgment fails to\nreflect tail risk, 2) the lack of compatibility between forecasting tournaments\nand tail risk assessment methods (such as extreme value theory). More\nimportantly, we communicate a new result showing a greater gap between the\nproperties of tail expectation and those of the corresponding probability.\n"
    },
    {
        "paper_id": 2301.11078,
        "authors": "Moawia Alghalith",
        "title": "New developments in econophysics: Option pricing formulas",
        "comments": "This article is published in a journal",
        "journal-ref": "Front.Phys.10:1036571 (2022)",
        "doi": "10.3389/fphy.2022.1036571",
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  We synthesize and discuss some new developments in econophysics. In doing so,\nwe focus on option pricing. We relax the assumptions of constant volatility and\ninterest rate. In doing so, we rely on the square root of the Brownian motion.\nWe also provide simple, closed-form pricing formulas for the American and\nBermudan options.\n"
    },
    {
        "paper_id": 2301.11079,
        "authors": "Hakim Lyngstad{\\aa}s and Johannes Mauritzen",
        "title": "Adults in the room? The auditor and dividends in small firms: Evidence\n  from a natural experiment",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  We examine the effect of auditing on dividends in small private firms. We\nhypothesize that auditing can constrain dividends by way of promoting\naccounting conservatism. We use register data on private Norwegian firms and\nrandom variation induced by the introduction of a policy allowing small private\nfirms to forgo the use of an auditor to estimate the effect of auditing on\ndividend payout. Identification is obtained by a regression discontinuity\naround the arbitrary thresholds for the policy. Propensity score matching is\nused to create a balanced synthetic control. We consistently find that forgoing\nauditing led to a significant increase in dividends in small private firms.\n"
    },
    {
        "paper_id": 2301.11084,
        "authors": "Haosen Ge",
        "title": "Measuring Regulatory Barriers Using Annual Reports of Firms",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Existing studies show that regulation is a major barrier to global economic\nintegration. Nonetheless, identifying and measuring regulatory barriers remains\na challenging task for scholars. I propose a novel approach to quantify\nregulatory barriers at the country-year level. Utilizing information from\nannual reports of publicly listed companies in the U.S., I identify regulatory\nbarriers business practitioners encounter. The barrier information is first\nextracted from the text documents by a cutting-edge neural language model\ntrained on a hand-coded training set. Then, I feed the extracted barrier\ninformation into a dynamic item response theory model to estimate the numerical\nbarrier level of 40 countries between 2006 and 2015 while controlling for\nvarious channels of confounding. I argue that the results returned by this\napproach should be less likely to be contaminated by major confounders such as\ninternational politics. Thus, they are well-suited for future political science\nresearch.\n"
    },
    {
        "paper_id": 2301.11207,
        "authors": "Carlos Esteban Posada",
        "title": "Inflation targeting strategy and its credibility",
        "comments": "16 pages, 4 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/publicdomain/zero/1.0/",
        "abstract": "  The money supply is endogenous if the monetary policy strategy is the so\ncalled Inflation and Interest Rate Targeting, IRT. With that and perfect\ncredibility, the theory of the price level and inflation only needs the Fisher\nequation, but it interprets causality in a new sense: if the monetary authority\nraises the policy rate, it will raise the inflation target, and vice versa,\ngiven the natural interest rate. If credibility is not perfect or if\nexpectations are not completely rational, the theory needs something more. Here\nI present a model corresponding to this theory that includes both the steady\nstate case and the recovery dynamics after a supply shock, with and without\npolicy reactions to such a shock. But, under the finite horizon assumption for\nIRT, at some future point in time the money supply must become exogenous. This\ncreates the incentive for agents to examine, as of today, statistics on\nmonetary aggregates and form their forecasts of money supply growth and\ninflation rates. Additionally, inflation models of the small open economy allow\nus to deduce that the IRT in this case is much more powerful than otherwise,\nand for the same degree of credibility. But things are not necessarily easier\nfor the monetary authority: it must monitor not only internal indicators, but\nalso external inflation and its determinants, and it must, in certain\ncircumstances, make more intense adjustments to the interest rate.\n"
    },
    {
        "paper_id": 2301.11318,
        "authors": "Nhu Khoa Nguyen, Thierry Delahaut, Emanuela Boros, Antoine Doucet and\n  Ga\\\"el Lejeune",
        "title": "Contextualizing Emerging Trends in Financial News Articles",
        "comments": "Proceedings of the Fourth Workshop on Financial Technology and\n  Natural Language Processing (FinNLP), December 8, 2022",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Identifying and exploring emerging trends in the news is becoming more\nessential than ever with many changes occurring worldwide due to the global\nhealth crises. However, most of the recent research has focused mainly on\ndetecting trends in social media, thus, benefiting from social features (e.g.\nlikes and retweets on Twitter) which helped the task as they can be used to\nmeasure the engagement and diffusion rate of content. Yet, formal text data,\nunlike short social media posts, comes with a longer, less restricted writing\nformat, and thus, more challenging. In this paper, we focus our study on\nemerging trends detection in financial news articles about Microsoft, collected\nbefore and during the start of the COVID-19 pandemic (July 2019 to July 2020).\nWe make the dataset accessible and propose a strong baseline (Contextual\nLeap2Trend) for exploring the dynamics of similarities between pairs of\nkeywords based on topic modelling and term frequency. Finally, we evaluate\nagainst a gold standard (Google Trends) and present noteworthy real-world\nscenarios regarding the influence of the pandemic on Microsoft.\n"
    },
    {
        "paper_id": 2301.11394,
        "authors": "Mykola Pinchuk",
        "title": "Customer Momentum",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  This paper examines customer momentum, defined as a positive relationship\nbetween a firm's returns and past returns of its customers. I confirm previous\nevidence (Cohen and Frazzini 2008) that customer momentum is both statistically\nand economically significant. Long-short equally-weighted (value-weighted)\ndecile portfolio generates a monthly return of 122 (106) basis points and a\nt-statistic above 4 (2.8) with respect to Fama-French factor models. The paper\nreports that customer momentum neither explains nor is explained by price\nmomentum and earnings momentum. Customer momentum is partially driven by the\nlead-lag relationship between small and large stocks. I find that in the\npost-discovery sample, customer momentum has a smaller magnitude and loses\nstatistical significance. The results are consistent with the hypothesis that\nafter its discovery, customer momentum decreased due to exploitation by\ninvestors.\n"
    },
    {
        "paper_id": 2301.11475,
        "authors": "Eiji Yamamura, Yoshiro Tsutsui, Fumio Ohtake",
        "title": "The effect of primary school education on preventive behaviours during\n  COVID-19 in Japan",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Education plays a critical role on promoting preventive behaviours against\nthe spread of pandemics. In Japan, hand-washing education in primary schools\nwas positively correlated with preventive behaviours against COVID-19\ntransmission for adults in 2020 during the early stages of COVID-19 [1]. The\nfollowing year, the Tokyo Olympics were held in Japan, and a state of emergency\nwas declared several times. Public perceptions of and risks associated with the\npandemic changed drastically with the emergence of COVID-19 vaccines. We\nre-examine whether effect of hand-washing education on preventive behaviours\npersisted by covering a longer period of the COVID-19 pandemic than previous\nstudies. 26 surveys were conducted nearly once a month for 30 months from March\n2020 (the early stage of COVID-19) to September 2022 in Japan. By corresponding\nwith the same individuals across surveys, we comprehensively gathered data on\npreventive behaviours during this period. In addition, we asked about\nhand-washing education they had received in their primary school. We used the\ndata to investigate how and the degree to which school education is associated\nwith pandemic mitigating preventive behaviours. We found that hand-washing\neducation in primary school is positively associated with behaviours such as\nhand washing and mask wearing as a COVID-19 preventive measure, but not related\nto staying at home. We observed a statistically significant difference in hand\nwashing between adults who received childhood hand-washing education and those\nwho did not. This difference persisted throughout the study period. In\ncomparison, the difference in mask wearing between the two groups was smaller,\nbut still statistically significant. Furthermore, there was no difference in\nstaying at home between them.\n"
    },
    {
        "paper_id": 2301.11554,
        "authors": "Andrew Ireland, David Johnston and Rachel Knott",
        "title": "Heat and Worker Health",
        "comments": null,
        "journal-ref": "10.1016/j.jhealeco.2023.102800",
        "doi": "10.1016/j.jhealeco.2023.102800",
        "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
        "abstract": "  Extreme heat negatively impacts cognition, learning, and task performance.\nWith increasing global temperatures, workers may therefore be at increased risk\nof work-related injuries and illness. This study estimates the effects of\ntemperature on worker health using records spanning 1985-2020 from an\nAustralian mandatory insurance scheme. High temperatures are found to cause\nsignificantly more claims, particularly among manual workers in outdoor-based\nindustries. These adverse effects have not diminished across time, with the\nlargest effect observed for the 2015-2020 period, indicating increasing\nvulnerability to heat. Within occupations, the workers most adversely affected\nby heat are female, older-aged and higher-earning. Finally, results from\nfirm-level panel analyses show that the percentage increase in claims on hot\ndays is largest at \"safer\" firms.\n"
    },
    {
        "paper_id": 2301.11587,
        "authors": "Thibaut Th\\'eate, Antonio Sutera and Damien Ernst",
        "title": "Matching of Everyday Power Supply and Demand with Dynamic Pricing:\n  Problem Formalisation and Conceptual Analysis",
        "comments": "Research paper accepted for publication in the peer-reviewed journal\n  Energy Reports edited by Elsevier",
        "journal-ref": null,
        "doi": "10.1016/j.egyr.2023.01.040",
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  The energy transition is expected to significantly increase the share of\nrenewable energy sources whose production is intermittent in the electricity\nmix. Apart from key benefits, this development has the major drawback of\ngenerating a mismatch between power supply and demand. The innovative dynamic\npricing approach may significantly contribute to mitigating that critical\nproblem by taking advantage of the flexibility offered by the demand side. At\nits core, this idea consists in providing the consumer with a price signal\nwhich is evolving over time, in order to influence its consumption. This novel\napproach involves a challenging decision-making problem that can be summarised\nas follows: how to determine a price signal maximising the synchronisation\nbetween power supply and demand under the constraints of maintaining the\nproducer/retailer's profitability and benefiting the final consumer at the same\ntime? As a contribution, this research work presents a detailed formalisation\nof this particular decision-making problem. Moreover, the paper discusses the\ndiverse algorithmic components required to efficiently design a dynamic pricing\npolicy: different forecasting models together with an accurate statistical\nmodelling of the demand response to dynamic prices.\n"
    },
    {
        "paper_id": 2301.11776,
        "authors": "Daniel Goller and Maximilian Sp\\\"ath",
        "title": "'Good job!' The impact of positive and negative feedback on performance",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  We analyze the causal impact of positive and negative feedback on\nprofessional performance. We exploit a unique data source in which\nquasi-random, naturally occurring variations within subjective ratings serve as\npositive and negative feedback. The analysis shows that receiving positive\nfeedback has a favorable impact on subsequent performance, while negative\nfeedback does not have an effect. These main results are found in two different\nenvironments and for distinct cultural backgrounds, experiences, and gender of\nthe feedback recipients. The findings imply that managers should focus on\ngiving positive motivational feedback.\n"
    },
    {
        "paper_id": 2301.12072,
        "authors": "Chao Zheng and Jiangtao Pan",
        "title": "Unbiased estimators for the Heston model with stochastic interest rates",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We combine the unbiased estimators in Rhee and Glynn (Operations Research:\n63(5), 1026-1043, 2015) and the Heston model with stochastic interest rates.\nSpecifically, we first develop a semi-exact log-Euler scheme for the Heston\nmodel with stochastic interest rates. Then, under mild assumptions, we show\nthat the convergence rate in the $L^2$ norm is $O(h)$, where $h$ is the step\nsize. The result applies to a large class of models, such as the\nHeston-Hull-While model, the Heston-CIR model and the Heston-Black-Karasinski\nmodel. Numerical experiments support our theoretical convergence rate.\n"
    },
    {
        "paper_id": 2301.12075,
        "authors": "Adam Graham-Squire and David McCune",
        "title": "An Examination of Ranked Choice Voting in the United States, 2004-2022",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  From the perspective of social choice theory, ranked-choice voting (RCV) is\nknown to have many flaws. RCV can fail to elect a Condorcet winner and is\nsusceptible to monotonicity paradoxes and the spoiler effect, for example. We\nuse a database of 182 American ranked-choice elections for political office\nfrom the years 2004-2022 to investigate empirically how frequently RCV's\ndeficiencies manifest in practice. Our general finding is that RCV's weaknesses\nare rarely observed in real-world elections, with the exception that ballot\nexhaustion frequently causes majoritarian failures.\n"
    },
    {
        "paper_id": 2301.12091,
        "authors": "Hoda Heidari, Solon Barocas, Jon Kleinberg, and Karen Levy",
        "title": "Informational Diversity and Affinity Bias in Team Growth Dynamics",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Prior work has provided strong evidence that, within organizational settings,\nteams that bring a diversity of information and perspectives to a task are more\neffective than teams that do not. If this form of informational diversity\nconfers performance advantages, why do we often see largely homogeneous teams\nin practice? One canonical argument is that the benefits of informational\ndiversity are in tension with affinity bias. To better understand the impact of\nthis tension on the makeup of teams, we analyze a sequential model of team\nformation in which individuals care about their team's performance (captured in\nterms of accurately predicting some future outcome based on a set of features)\nbut experience a cost as a result of interacting with teammates who use\ndifferent approaches to the prediction task. Our analysis of this simple model\nreveals a set of subtle behaviors that team-growth dynamics can exhibit: (i)\nfrom certain initial team compositions, they can make progress toward better\nperformance but then get stuck partway to optimally diverse teams; while (ii)\nfrom other initial compositions, they can also move away from this optimal\nbalance as the majority group tries to crowd out the opinions of the minority.\nThe initial composition of the team can determine whether the dynamics will\nmove toward or away from performance optimality, painting a path-dependent\npicture of inefficiencies in team compositions. Our results formalize a\nfundamental limitation of utility-based motivations to drive informational\ndiversity in organizations and hint at interventions that may improve\ninformational diversity and performance simultaneously.\n"
    },
    {
        "paper_id": 2301.12255,
        "authors": "Christian Mitsch",
        "title": "The impact of surplus sharing on the outcomes of specific investments\n  under negotiated transfer pricing: An agent-based simulation with fuzzy\n  Q-learning agents",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  This paper focuses on specific investments under negotiated transfer pricing.\nReasons for transfer pricing studies are primarily to find conditions that\nmaximize the firm's overall profit, especially in cases with bilateral trading\nproblems with specific investments. However, the transfer pricing problem has\nbeen developed in the context where managers are fully individual rational\nutility maximizers. The underlying assumptions are rather heroic and, in\nparticular, how managers process information under uncertainty, do not\nperfectly match with human decision-making behavior. Therefore, this paper\nrelaxes key assumptions and studies whether cognitively bounded agents achieve\nthe same results as fully rational utility maximizers and, in particular,\nwhether the recommendations on managerial-compensation arrangements and\nbargaining infrastructures are designed to maximize headquarters' profit in\nsuch a setting. Based on an agent-based simulation with fuzzy Q-learning\nagents, it is shown that in case of symmetric marginal cost parameters, myopic\nfuzzy Q-learning agents invest only as much as in the classic hold-up problem,\nwhile non-myopic fuzzy Q-learning agents invest optimally. However, in\nscenarios with non-symmetric marginal cost parameters, a deviation from the\npreviously recommended surplus sharing rules can lead to higher investment\ndecisions and, thus, to an increase in the firm's overall profit.\n"
    },
    {
        "paper_id": 2301.12346,
        "authors": "Kazuki Amagai and Tomoya Suzuki",
        "title": "Long-Term Modeling of Financial Machine Learning for Active Portfolio\n  Management",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In the practical business of asset management by investment trusts and the\nlike, the general practice is to manage over the medium to long term owing to\nthe burden of operations and increase in transaction costs with the increase in\nturnover ratio. However, when machine learning is used to construct a\nmanagement model, the number of learning data decreases with the increase in\nthe long-term time scale; this causes a decline in the learning precision.\nAccordingly, in this study, data augmentation was applied by the combined use\nof not only the time scales of the target tasks but also the learning data of\nshorter term time scales, demonstrating that degradation of the generalization\nperformance can be inhibited even if the target tasks of machine learning have\nlong-term time scales. Moreover, as an illustration of how this data\naugmentation can be applied, we conducted portfolio management in which machine\nlearning of a multifactor model was done by an autoencoder and mispricing was\nused from the estimated theoretical values. The effectiveness could be\nconfirmed in not only the stock market but also the FX market, and a\ngeneral-purpose management model could be constructed in various financial\nmarkets.\n"
    },
    {
        "paper_id": 2301.1242,
        "authors": "Qinyu Wu, Fan Yang and Ping Zhang",
        "title": "Conditional generalized quantiles based on expected utility model and\n  equivalent characterization of properties",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  As a counterpart to the (static) risk measures of generalized quantiles and\nmotivated by Bellini et al. (2018), we propose a new kind of conditional risk\nmeasure called conditional generalized quantiles. We first show their\nwell-definedness and they can be equivalently characterised by a conditional\nfirst order condition. We also discuss their main properties, and, especially,\nWe give the characterization of coherency/convexity. For potential applications\nas a dynamic risk measure, we study their time consistency properties, and\nestablish their equivalent characterizations among conditional generalized\nquantiles.\n"
    },
    {
        "paper_id": 2301.12571,
        "authors": "Enoch Hyunwook Kang, P. R. Kumar",
        "title": "Bounded (O(1)) Regret Recommendation Learning via Synthetic Controls\n  Oracle",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  In online exploration systems where users with fixed preferences repeatedly\narrive, it has recently been shown that O(1), i.e., bounded regret, can be\nachieved when the system is modeled as a linear contextual bandit. This result\nmay be of interest for recommender systems, where the popularity of their items\nis often short-lived, as the exploration itself may be completed quickly before\npotential long-run non-stationarities come into play. However, in practice,\nexact knowledge of the linear model is difficult to justify. Furthermore,\npotential existence of unobservable covariates, uneven user arrival rates,\ninterpretation of the necessary rank condition, and users opting out of private\ndata tracking all need to be addressed for practical recommender system\napplications. In this work, we conduct a theoretical study to address all these\nissues while still achieving bounded regret. Aside from proof techniques, the\nkey differentiating assumption we make here is the presence of effective\nSynthetic Control Methods (SCM), which are shown to be a practical relaxation\nof the exact linear model knowledge assumption. We verify our theoretical\nbounded regret result using a minimal simulation experiment.\n"
    },
    {
        "paper_id": 2301.1271,
        "authors": "Benjamin Avanzi and Greg Taylor and Melantha Wang and Bernard Wong",
        "title": "Machine Learning with High-Cardinality Categorical Features in Actuarial\n  Applications",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  High-cardinality categorical features are pervasive in actuarial data (e.g.\noccupation in commercial property insurance). Standard categorical encoding\nmethods like one-hot encoding are inadequate in these settings.\n  In this work, we present a novel _Generalised Linear Mixed Model Neural\nNetwork_ (\"GLMMNet\") approach to the modelling of high-cardinality categorical\nfeatures. The GLMMNet integrates a generalised linear mixed model in a deep\nlearning framework, offering the predictive power of neural networks and the\ntransparency of random effects estimates, the latter of which cannot be\nobtained from the entity embedding models. Further, its flexibility to deal\nwith any distribution in the exponential dispersion (ED) family makes it widely\napplicable to many actuarial contexts and beyond.\n  We illustrate and compare the GLMMNet against existing approaches in a range\nof simulation experiments as well as in a real-life insurance case study.\nNotably, we find that the GLMMNet often outperforms or at least performs\ncomparably with an entity embedded neural network, while providing the\nadditional benefit of transparency, which is particularly valuable in practical\napplications.\n  Importantly, while our model was motivated by actuarial applications, it can\nhave wider applicability. The GLMMNet would suit any applications that involve\nhigh-cardinality categorical variables and where the response cannot be\nsufficiently modelled by a Gaussian distribution.\n"
    },
    {
        "paper_id": 2301.12719,
        "authors": "Solveig Flaig, Gero Junike",
        "title": "Validation of machine learning based scenario generators",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  Machine learning methods are getting more and more important in the\ndevelopment of internal models using scenario generation. As internal models\nunder Solvency 2 have to be validated, an important question is in which\naspects the validation of these data-driven models differs from a classical\ntheory-based model. On the specific example of market risk, we discuss the\nnecessity of two additional validation tasks: one to check the dependencies\nbetween the risk factors used and one to detect the unwanted memorizing effect.\nThe first one is necessary because in this new method, the dependencies are not\nderived from a financial-mathematical theory. The latter one arises when the\nmachine learning model only repeats empirical data instead of generating new\nscenarios. These measures are then applied for an machine learning based\neconomic scenario generator. It is shown that those measures lead to reasonable\nresults in this context and are able to be used for validation as well as for\nmodel optimization.\n"
    },
    {
        "paper_id": 2301.13009,
        "authors": "Deborah Miori, Mihai Cucuringu",
        "title": "DeFi: data-driven characterisation of Uniswap v3 ecosystem & an ideal\n  crypto law for liquidity pools",
        "comments": "26 pages, 21 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Uniswap is a Constant Product Market Maker built around liquidity pools,\nwhere pairs of tokens are exchanged subject to a fee that is proportional to\nthe size of transactions. At the time of writing, there exist more than 6,000\npools associated with Uniswap v3, implying that empirical investigations on the\nfull ecosystem can easily become computationally expensive. Thus, we propose a\nsystematic workflow to extract and analyse a meaningful but computationally\ntractable sub-universe of liquidity pools. Leveraging on the 34 pools found\nrelevant for the six-months time window January-June 2022, we then investigate\nthe related liquidity consumption behaviour of market participants. We propose\nto represent each liquidity taker by a suitably constructed transaction graph,\nwhich is a fully connected network where nodes are the liquidity taker's\nexecuted transactions, and edges contain weights encoding the time elapsed\nbetween any two transactions. We extend the NLP-inspired graph2vec algorithm to\nthe weighted undirected setting, and employ it to obtain an embedding of the\nset of graphs. This embedding allows us to extract seven clusters of liquidity\ntakers, with equivalent behavioural patters and interpretable trading\npreferences. We conclude our work by testing for relationships between the\ncharacteristic mechanisms of each pool, i.e. liquidity provision, consumption,\nand price variation. We introduce a related ideal crypto law, inspired from the\nideal gas law of thermodynamics, and demonstrate that pools adhering to this\nlaw are healthier trading venues in terms of sensitivity of liquidity and\nagents' activity. Regulators and practitioners could benefit from our model by\ndeveloping related pool health monitoring tools.\n"
    },
    {
        "paper_id": 2301.13174,
        "authors": "Cristiana L. Lara and John Wassick",
        "title": "Future of Supply Chain: Challenges, Trends, and Prospects",
        "comments": "7 pages, 1 figure. Proceedings of FOCAPO/CPC 2023 Conference",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper discusses the broad challenges shared by e-commerce and the\nprocess industries operating global supply chains. Specifically, we discuss how\nprocess industries and e-commerce differ in many aspects but have similar\nchallenges ahead of them in order to remain competitive, keep up with the\nalways increasing requirements of the customers and stakeholders, and gain\nefficiency. While both industries have been early adopters of decision support\ntools based on machine intelligence, both share unresolved challenges related\nto scalability, integration of decision-making over different time horizons\n(e.g. strategic, tactical and execution-level decisions) and across internal\nbusiness units, and orchestration of human and computer-based decision-makers.\nWe discuss future trends and research opportunities in the area of supply\nchain, and suggest that the methods of multi-agent systems supported by\nrigorous treatment of human decision-making in combination with machine\nintelligence is a great contender to address these critical challenges.\n"
    },
    {
        "paper_id": 2301.13204,
        "authors": "Hiroki Bessho, Takanari Sugimoto, Tomoya Suzuki",
        "title": "Forex Trading Strategy That Might Be Executed Due to the Popularity of\n  Gotobi Anomaly",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Our previous research has confirmed that the USD/JPY rate tends to rise\ntoward 9:55 every morning in the Gotobi days, which are divisible by five. This\nis called the Gotobi anomaly. In the present study, we verify the possible\ntrading strategy and its validity under the condition that investors recognize\nthe existence of the anomaly. Moreover, we illustrate the possibility that the\nwealth of Japanese companies might leak to FX traders due to the arbitrage\nopportunity if Japanese companies blindly keep making payments in the Gotobi\ndays as a business custom.\n"
    },
    {
        "paper_id": 2301.13235,
        "authors": "Christa Cuchiero, Guido Gazzani, Janka M\\\"oller, Sara Svaluto-Ferro",
        "title": "Joint calibration to SPX and VIX options with signature-based models",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  We consider a stochastic volatility model where the dynamics of the\nvolatility are described by a linear function of the (time extended) signature\nof a primary process which is supposed to be a polynomial diffusion. We obtain\nclosed form expressions for the VIX squared, exploiting the fact that the\ntruncated signature of a polynomial diffusion is again a polynomial diffusion.\nAdding to such a primary process the Brownian motion driving the stock price,\nallows then to express both the log-price and the VIX squared as linear\nfunctions of the signature of the corresponding augmented process. This feature\ncan then be efficiently used for pricing and calibration purposes. Indeed, as\nthe signature samples can be easily precomputed, the calibration task can be\nsplit into an offline sampling and a standard optimization. We also propose a\nFourier pricing approach for both VIX and SPX options exploiting that the\nsignature of the augmented primary process is an infinite dimensional affine\nprocess. For both the SPX and VIX options we obtain highly accurate calibration\nresults, showing that this model class allows to solve the joint calibration\nproblem without adding jumps or rough volatility.\n"
    },
    {
        "paper_id": 2301.13255,
        "authors": "Nathan Zavanelli",
        "title": "Wavelet Analysis for Time Series Financial Signals via Element Analysis",
        "comments": "6 pages with 2 columns. 10 pt times new romans font. 2 figures. Not\n  published in any conference or journal to date",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  The method of element analysis is proposed here as an alternative to\ntraditional wavelet-based approaches to analyzing perturbations in financial\nsignals by scale. In this method, the processes that generate oscillations in\nfinancial signals are modelled as scaled, shifted, and isolated events that\nproduce ripples of various frequencies across a sea of noise as opposed to a\nsimple sinusoidal or mixed frequency oscillation or an impulse. This allows one\nto directly estimate the wavelet parameters derived only from the generating\nfunctions, rejecting spurious perturbations driven by noise or extraneous\nfactors. Financial signals may then be reconstructed based on a finite set of\ngenerators localized in time and frequency. This method offers a marked\nadvantage compared to traditional econometric tools because it directly targets\nthe generators of oscillations. Furthermore, the choice of the Morse wavelet\nallows for wide latitude in capturing a broad set of diverse generators. In\nthis work, the basic mathematical principles underlying element analysis are\npresented, and the method is applied to the study of variance in financial\ndata, where the advantages of element analysis over traditional wavelet\ntechniques is demonstrated. Specifically, in the example analysis of inflation\nexpectations, element analysis shows a clear ability to distinguish between\noscillations formed by noise and those formed by generators logically matched\nto historical events.\n"
    },
    {
        "paper_id": 2301.13295,
        "authors": "Cameron Perot",
        "title": "Quantum Boltzmann Machines: Applications in Quantitative Finance",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  In this thesis we explore using the D-Wave Advantage 4.1 quantum annealer to\nsample from quantum Boltzmann distributions and train quantum Boltzmann\nmachines (QBMs). We focus on the real-world problem of using QBMs as generative\nmodels to produce synthetic foreign exchange market data and analyze how the\nresults stack up against classical models based on restricted Boltzmann\nmachines (RBMs). Additionally, we study a small 12-qubit problem which we use\nto compare samples obtained from the Advantage 4.1 with theory, and in the\nprocess gain vital insights into how well the Advantage 4.1 can sample quantum\nBoltzmann random variables and be used to train QBMs. Through this, we are able\nto show that the Advantage 4.1 can sample classical Boltzmann random variables\nto some extent, but is limited in its ability to sample from quantum Boltzmann\ndistributions. Our findings indicate that QBMs trained using the Advantage 4.1\nare much noisier than those trained using simulations and struggle to perform\nat the same level as classical RBMs. However, there is the potential for QBMs\nto outperform classical RBMs if future generation annealers can generate\nsamples closer to the desired theoretical distributions.\n"
    },
    {
        "paper_id": 2301.13505,
        "authors": "Yuki Sato, Kiyoshi Kanazawa",
        "title": "Can we infer microscopic financial information from the long memory in\n  market-order flow?: a quantitative test of the Lillo-Mike-Farmer model",
        "comments": "4 pages, 4 figures",
        "journal-ref": "Phys. Rev. Lett. 131, 197401 (2023)",
        "doi": "10.1103/PhysRevLett.131.197401",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In financial markets, the market order sign exhibits strong persistence,\nwidely known as the long-range correlation (LRC) of order flow; specifically,\nthe sign correlation function displays long memory with power-law exponent\n$\\gamma$, such that $C(\\tau) \\propto \\tau^{-\\gamma}$ for large time-lag $\\tau$.\nOne of the most promising microscopic hypotheses is the order-splitting\nbehaviour at the level of individual traders. Indeed, Lillo, Mike, and Farmer\n(LMF) introduced in 2005 a simple microscopic model of order-splitting\nbehaviour, which predicts that the macroscopic sign correlation is\nquantitatively associated with the microscopic distribution of metaorders.\nWhile this hypothesis has been a central issue of debate in econophysics, its\ndirect quantitative validation has been missing because it requires large\nmicroscopic datasets with high resolution to observe the order-splitting\nbehaviour of all individual traders. Here we present the first quantitative\nvalidation of this LFM prediction by analysing a large microscopic dataset in\nthe Tokyo Stock Exchange market for more than nine years. On classifying all\ntraders as either order-splitting traders or random traders as a statistical\nclustering, we directly measured the metaorder-length distributions\n$P(L)\\propto L^{-\\alpha-1}$ as the microscopic parameter of the LMF model and\nexamined the theoretical prediction on the macroscopic order correlation:\n$\\gamma \\approx \\alpha - 1$. We discover that the LMF prediction agrees with\nthe actual data even at the quantitative level. Our work provides the first\nsolid support of the microscopic model and solves directly a long-standing\nproblem in the field of econophysics and market microstructure.\n"
    },
    {
        "paper_id": 2301.13575,
        "authors": "Alessandra Cretarola, Benedetta Salterini",
        "title": "Utility-based indifference pricing of pure endowments in a\n  Markov-modulated market model",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper we study exponential utility indifference pricing of pure\nendowment policies in a stochastic-factor model for an insurance company, which\ncan also invest in a financial market. Specifically, we propose a modeling\nframework where the hazard rate is described by an observable general diffusion\nprocess and the risky asset price evolves as a jump diffusion affected by a\ncontinuous-time finite-state Markov chain representing regimes of the economy.\nUsing the classical stochastic control approach based on the\nHamilton-Jacobi-Bellman equation, we describe the optimal investment strategies\nwith and without the insurance derivative and characterize the indifference\nprice in terms of a classical solution to a linear PDE. We also provide its\nprobabilistic representation via an extension of the Feynman-Kac formula show\nthat it satisfies a final value problem. Furthermore, we also discuss the\nindifference price for a portfolio of insurance policies and for a term life\ninsurance. Finally, some numerical experiments are performed to address\nsensitivity analyses.\n"
    },
    {
        "paper_id": 2301.13594,
        "authors": "Trent Spears, Stefan Zohren and Stephen Roberts",
        "title": "View fusion vis-\\`a-vis a Bayesian interpretation of Black-Litterman for\n  portfolio allocation",
        "comments": "25 pages, 5 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  The Black-Litterman model extends the framework of the Markowitz Modern\nPortfolio Theory to incorporate investor views. We consider a case where\nmultiple view estimates, including uncertainties, are given for the same\nunderlying subset of assets at a point in time. This motivates our\nconsideration of data fusion techniques for combining information from multiple\nsources. In particular, we consider consistency-based methods that yield fused\nview and uncertainty pairs; such methods are not common to the quantitative\nfinance literature. We show a relevant, modern case of incorporating machine\nlearning model-derived view and uncertainty estimates, and the impact on\nportfolio allocation, with an example subsuming Arbitrage Pricing Theory. Hence\nwe show the value of the Black-Litterman model in combination with information\nfusion and artificial intelligence-grounded prediction methods.\n"
    },
    {
        "paper_id": 2301.13595,
        "authors": "V.M. Belyaev",
        "title": "Local Volatility in Interest Rate Models",
        "comments": "19 pages, 11 Figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Local Volatility (LV) is a very powerful tool for market modeling. This tool\ncan be used to generate arbitrage-free scenarios calibrated to all available\noptions. Here we demonstrate how to implement LV in order to reproduce most\nswaption prices within a single model.\n  There was a good agreement between market prices and Monte Carlo prices for\nall tenors and maturities from 2 to 20 years. Note that due to the use of a\nnormal distribution in the scenario generation process, the volatility of\nshort-term swaptions cannot be generated accurately.\n"
    },
    {
        "paper_id": 2302.00411,
        "authors": "Bartosz Uniejewski",
        "title": "Electricity price forecasting with Smoothing Quantile Regression\n  Averaging: Quantifying economic benefits of probabilistic forecasts",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In the world of the complex power market, accurate electricity price\nforecasting is essential for strategic bidding and affects both daily\noperations and long-term investments. This article introduce a new method\ndubbed Smoothing Quantile Regression (SQR) Averaging, that improves on\nwell-performing schemes for probabilistic forecasting. To showcase its utility,\na comprehensive study is conducted across four power markets, including recent\ndata encompassing the COVID-19 pandemic and the Russian invasion on Ukraine.\nThe performance of SQR Averaging is evaluated and compared to state-of-the-art\nbenchmark methods in terms of the reliability and sharpness measures.\nAdditionally, an evaluation scheme is introduced to quantify the economic\nbenefits derived from SQR Averaging predictions. This scheme can be applied in\nany day-ahead electricity market and is based on a trading strategy that\nleverages battery storage and sets limit orders using selected quantiles of the\npredictive distribution. The results reveal that, compared to the benchmark\nstrategy, utilizing SQR Averaging leads to average profit increases of up to\n14\\%. These findings provide strong evidence for the effectiveness of SQR\nAveraging in improving forecast accuracy and the practical value of utilizing\nprobabilistic forecasts in day-ahead electricity trading, even in the face of\nchallenging events such as the COVID-19 pandemic and geopolitical disruptions.\n"
    },
    {
        "paper_id": 2302.00417,
        "authors": "Asier Minondo",
        "title": "How exporters neutralized an increase in tariffs",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  I use the unanticipated and large additional tariffs the US imposed on\nEuropean Union products due to the Airbus-Boeing conflict to analyze how\nexporters reacted to a change in trade policy. Using firm-level data for Spain\nand applying a difference-in-differences methodology, I show that the export\nrevenue in the US of the firms affected by the tariff hike did not\nsignificantly decrease relative to the one of other Spanish exporters to the\nUS. I show that Spanish exporters were able to neutralize the increase in\ntariffs by substituting Spanish products with products originated in countries\nunaffected by tariffs and shifting to varieties not affected by tariffs. My\nresults show that tariff avoidance is another margin exporters can use to\ncounteract the effects of a tariff hike.\n"
    },
    {
        "paper_id": 2302.00434,
        "authors": "Christoph Reisinger and Maria Olympia Tsianni",
        "title": "Convergence of the Euler--Maruyama particle scheme for a regularised\n  McKean--Vlasov equation arising from the calibration of local-stochastic\n  volatility models",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper, we study the Euler--Maruyama scheme for a particle method to\napproximate the McKean--Vlasov dynamics of calibrated local-stochastic\nvolatility (LSV) models. Given the open question of well-posedness of the\noriginal problem, we work with regularised coefficients and prove that under\ncertain assumptions on the inputs, the regularised model is well-posed. Using\nthis result, we prove the strong convergence of the Euler--Maruyama scheme to\nthe particle system with rate 1/2 in the step-size and obtain an explicit\ndependence of the error on the regularisation parameters. Finally, we implement\nthe particle method for the calibration of a Heston-type LSV model to\nillustrate the convergence in practice and to investigate how the choice of\nregularisation parameters affects the accuracy of the calibration.\n"
    },
    {
        "paper_id": 2302.00452,
        "authors": "Rui Ding",
        "title": "f-Betas and Portfolio Optimization with f-Divergence induced Risk\n  Measures",
        "comments": "19 pages, 7 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  In this paper, we build on using the class of f-divergence induced coherent\nrisk measures for portfolio optimization and derive its necessary optimality\nconditions formulated in CAPM format. We derive a new f-Beta similar to the\nStandard Betas and also extended it to previous works in Drawdown Betas. The\nf-Beta evaluates portfolio performance under an optimally perturbed market\nprobability measure, and this family of Beta metrics gives various degrees of\nflexibility and interpretability. We conduct numerical experiments using\nselected stocks against a chosen S\\&P 500 market index as the optimal portfolio\nto demonstrate the new perspectives provided by Hellinger-Beta as compared with\nStandard Beta and Drawdown Betas. In our experiments, the squared Hellinger\ndistance is chosen to be the particular choice of the f-divergence function in\nthe f-divergence induced risk measures and f-Betas. We calculate Hellinger-Beta\nmetrics based on deviation measures and further extend this approach to\ncalculate Hellinger-Betas based on drawdown measures, resulting in another new\nmetric which is termed Hellinger-Drawdown Beta. We compare the resulting\nHellinger-Beta values under various choices of the risk aversion parameter to\nstudy their sensitivity to increasing stress levels.\n"
    },
    {
        "paper_id": 2302.00586,
        "authors": "Shuo Sun and Molei Qin and Xinrun Wang and Bo An",
        "title": "PRUDEX-Compass: Towards Systematic Evaluation of Reinforcement Learning\n  in Financial Markets",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  The financial markets, which involve more than $90 trillion market capitals,\nattract the attention of innumerable investors around the world. Recently,\nreinforcement learning in financial markets (FinRL) has emerged as a promising\ndirection to train agents for making profitable investment decisions. However,\nthe evaluation of most FinRL methods only focuses on profit-related measures\nand ignores many critical axes, which are far from satisfactory for financial\npractitioners to deploy these methods into real-world financial markets.\nTherefore, we introduce PRUDEX-Compass, which has 6 axes, i.e., Profitability,\nRisk-control, Universality, Diversity, rEliability, and eXplainability, with a\ntotal of 17 measures for a systematic evaluation. Specifically, i) we propose\nAlphaMix+ as a strong FinRL baseline, which leverages mixture-of-experts (MoE)\nand risk-sensitive approaches to make diversified risk-aware investment\ndecisions, ii) we evaluate 8 FinRL methods in 4 long-term real-world datasets\nof influential financial markets to demonstrate the usage of our\nPRUDEX-Compass, iii) PRUDEX-Compass together with 4 real-world datasets,\nstandard implementation of 8 FinRL methods and a portfolio management\nenvironment is released as public resources to facilitate the design and\ncomparison of new FinRL methods. We hope that PRUDEX-Compass can not only shed\nlight on future FinRL research to prevent untrustworthy results from stagnating\nFinRL into successful industry deployment but also provide a new challenging\nalgorithm evaluation scenario for the reinforcement learning (RL) community.\n"
    },
    {
        "paper_id": 2302.00728,
        "authors": "Vikranth Lokeshwar Dhandapani, Shashi Jain",
        "title": "Data-driven Approach for Static Hedging of Exchange Traded Options",
        "comments": "42 pages, 11 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper presents a data-driven interpretable machine learning algorithm\nfor semi-static hedging of Exchange Traded options, considering transaction\ncosts with efficient run-time. Further, we provide empirical evidence on the\nperformance of hedging longer-term National Stock Exchange (NSE) Index options\nusing a self-replicating portfolio of shorter-term options and cash position,\nachieved by the automated algorithm, under different modeling assumptions and\nmarket conditions, including Covid period. We also systematically assess the\nmodel's performance using the Superior Predictive Ability (SPA) test by\nbenchmarking against the static hedge proposed by Peter Carr and Liuren Wu and\nindustry-standard dynamic hedging. We finally perform a thorough Profit and\nLoss (PnL) attribution analysis on the target option and hedge portfolios\n(dynamic and static) to discern the factors explaining the superior performance\nof static hedging.\n"
    },
    {
        "paper_id": 2302.00761,
        "authors": "Mykola Pinchuk",
        "title": "Zero-Leverage Puzzle",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  In this paper, I examine why some firms have zero leverage. I fail to find\nevidence that firms are unlevered because of managerial entrenchment since\nthese firms do not have weaker corporate governance. I reject the hypothesis\nthat firms become zero-leverage after prolonged periods of high market\nvaluation, since before levering these firms do not suffer from declining\nvaluations and continue to issue large amounts of equity. I find strong\nevidence in favor of the financial constraints explanation of the zero-leverage\npuzzle. Zero-leverage firms appear to be financially constrained using three\ndifferent measures of financial constraints. I obtain mixed evidence on the\nfinancial flexibility hypothesis since all-equity firms increase investments\nand acquisitions after levering, but the probability of their levering\ndecreased during the financial crisis. My results suggest that financial\nconstraints are the first-order the driver of zero-leverage behavior and are\nmore important than less obvious explanations such as managerial entrenchment.\n"
    },
    {
        "paper_id": 2302.00846,
        "authors": "Jonathan A. Ch\\'avez-Casillas",
        "title": "A time-dependent Markovian model of a limit order book",
        "comments": "23 pages, 43 figures",
        "journal-ref": null,
        "doi": "10.1007/s10614-023-10356-9",
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  This paper considers a Markovian model of a limit order book where\ntime-dependent rates are allowed. With the objective of understanding the\nmechanisms through which a microscopic model of an orderbook can converge to\nmore general diffusion than a Brownian motion with constant coefficient, a\nsimple time-dependent model is proposed. The model considered here starts by\ndescribing the processes that govern the arrival of the different orders such\nas limit orders, market orders and cancellations. In this sense, this is a\nmicroscopic model rather than a ``mesoscopic'' model where the starting point\nis usually the point processes describing the times at which the price changes\noccur and aggregate in these all the information pertaining to the arrival of\nindividual orders. Furthermore, several empirical studies are performed to shed\nsome light into the validity of the modeling assumptions and to verify whether\ncertain stocks satisfy the conditions for their price process to converge to a\nmore complex diffusion.\n"
    },
    {
        "paper_id": 2302.0101,
        "authors": "Jan-Frederik Mai",
        "title": "Performance attribution with respect to interest rates, FX, carry, and\n  residual market risks",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We develop a method to decompose the PnL of a portfolio of assets into four\nparts: (a) PnL due to FX rate changes, (b) PnL due to interest rate changes,\n(c) carry gain due to time passing, (d) PnL due to residual market risk changes\n(credit risk, liquidity risk, volatility risk etc.). We demonstrate the\nusefulness of our approach by decomposing the performance of an FX- and\ninterest rate-hedged negative basis position in our fund XAIA Credit Basis II,\nand we apply the methodology to decompose the performance of our fund XAIA\nCredit Debt Capital in the first quarter of 2022 into PnL contributions of the\nsingle positions.\n"
    },
    {
        "paper_id": 2302.01169,
        "authors": "Rama Cont, Pierre Degond, Lifan Xuan",
        "title": "A mathematical framework for modelling order book dynamics",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We present a general framework for modelling the dynamics of limit order\nbooks, built on the combination of two modelling ingredients: the order flow,\nmodelled as a general spatial point process, and market clearing, modelled via\na deterministic mass transport operator acting on distributions of buy and sell\norders. At the mathematical level, this corresponds to a natural decomposition\nof the infinitesimal generator describing the evolution of the limit order book\ninto two operators: the generator of the order flow and the clearing operator.\nOur model provides a flexible framework for modelling and simulating order book\ndynamics and studying various scaling limits of discrete order book models. We\nshow that our framework includes previous models as special cases and yields\ninsights into the interplay between order flow and price dynamics.\n"
    },
    {
        "paper_id": 2302.01196,
        "authors": "Bernardo Freitas Paulo da Costa, Silvana M. Pesenti, Rodrigo S.\n  Targino",
        "title": "Risk Budgeting Portfolios from Simulations",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-sa/4.0/",
        "abstract": "  Risk budgeting is a portfolio strategy where each asset contributes a\nprespecified amount to the aggregate risk of the portfolio. In this work, we\npropose an efficient numerical framework that uses only simulations of returns\nfor estimating risk budgeting portfolios. Besides a general cutting planes\nalgorithm for determining the weights of risk budgeting portfolios for\narbitrary coherent distortion risk measures, we provide a specialised version\nfor the Expected Shortfall, and a tailored Stochastic Gradient Descent (SGD)\nalgorithm, also for the Expected Shortfall. We compare our algorithm to\nstandard convex optimisation solvers and illustrate different risk budgeting\nportfolios, constructed using an especially designed Julia package, on real\nfinancial data and compare it to classical portfolio strategies.\n"
    },
    {
        "paper_id": 2302.01216,
        "authors": "Aymeric Vie, J. Doyne Farmer",
        "title": "Towards Evology: a Market Ecology Agent-Based Model of US Equity Mutual\n  Funds II",
        "comments": "AI4ABM, ICLR23",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Agent-based models (ABMs) are fit to model heterogeneous, interacting systems\nlike financial markets. We present the latest advances in Evology: a\nheterogeneous, empirically calibrated market ecology agent-based model of the\nUS stock market. Prices emerge endogenously from the interactions of market\nparticipants with diverse investment behaviours and their reactions to\nfundamentals. This approach allows testing trading strategies while accounting\nfor the interactions of this strategy with other market participants and\nconditions. Those early results encourage a closer association between ABMs and\nML algorithms for testing and optimising investment strategies using machine\nlearning algorithms.\n"
    },
    {
        "paper_id": 2302.01362,
        "authors": "Christa Cuchiero, Sara Svaluto-Ferro and Josef Teichmann",
        "title": "Signature SDEs from an affine and polynomial perspective",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Signature stochastic differential equations (SDEs) constitute a large class\nof stochastic processes, here driven by Brownian motions, whose characteristics\nare entire or real-analytic functions of their own signature, i.e. of iterated\nintegrals of the process with itself, and allow therefore for a generic path\ndependence. We show that their prolongation with the corresponding signature is\nan affine and polynomial process taking values in subsets of group-like\nelements of the extended tensor algebra. By relying on the duality theory for\naffine and polynomial processes we obtain explicit formulas in terms of novel\nand proper notions of converging power series for the Fourier-Laplace transform\nand the expected value of entire functions of the signature process. The\ncoefficients of these power series are solutions of extended tensor algebra\nvalued Riccati and linear ordinary differential equations (ODEs), respectively,\nwhose vector fields can be expressed in terms of the entire characteristics of\nthe corresponding SDEs. In other words, we construct a class of stochastic\nprocesses, which is universal within It\\^o processes with path-dependent\ncharacteristics and which allows for a relatively explicit characterization of\nthe Fourier-Laplace transform and hence the full law on path space. We also\nanalyze the special case of one-dimensional signature SDEs, which correspond to\nclassical SDEs with real-analytic characteristics. Finally, the practical\nfeasibility of this affine and polynomial approach is illustrated by several\nnumerical examples.\n"
    },
    {
        "paper_id": 2302.01456,
        "authors": "Farhad Billimoria, Filiberto Fele, Iacopo Savelli, Thomas Morstyn,\n  Malcolm McCulloch",
        "title": "An Insurance Paradigm for Improving Power System Resilience via\n  Distributed Investment",
        "comments": "10 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
        "abstract": "  Extreme events, exacerbated by climate change, pose significant risks to the\nenergy system and its consumers. However there are natural limits to the degree\nof protection that can be delivered from a centralised market architecture.\nDistributed energy resources provide resilience to the energy system, but their\nvalue remains inadequately recognized by regulatory frameworks. We propose an\ninsurance framework to align residual outage risk exposure with locational\nincentives for distributed investment. We demonstrate that leveraging this\nframework in large-scale electricity systems could improve consumer welfare\noutcomes in the face of growing risks from extreme events via investment in\ndistributed energy.\n"
    },
    {
        "paper_id": 2302.01663,
        "authors": "Andrew W. Macpherson",
        "title": "Adversarial blockchain queues and trading on a CFMM",
        "comments": "20 pages. Keywords: queue, blockchain, mempool, cfmm, mev, dex,\n  priority discipline, model, sandwich, slippage",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We describe a plausible probabilistic model for a blockchain queueing\nenvironment in which rational, profit-maximising schedulers impose adversarial\ndisciplines on incoming messages containing a payload that encodes a state\ntransition in a machine. The model can be specialised to apply to chains with\nfixed or variable block times, traditional priority queue disciplines with\n`honest' schedulers, or adversarial public mempools. We find conditions under\nwhich the model behaves as a bulk-service queue with priority discipline and\nderive practical expressions for the relative block and message number of a\ntransaction.\n  We study this setup in the context of orders to a CFMM DEX where the\nexecution price a user receives may be quite sensitive to its positioning in\nthe chain -- in particular, to a string of transactions scheduled for prior\nexecution which is not knowable at the time of order creation. We derive\nstatistical models for the price impact of this order flow both in the presence\nand absence of MEV extraction activity.\n"
    },
    {
        "paper_id": 2302.01668,
        "authors": "Shunya Chomei",
        "title": "Empirical analysis in limit order book modeling for Nikkei 225 Stocks\n  with Cox-type intensities",
        "comments": "13 pages, 3 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper, we build on the analysis of Muni Toke and Yoshida (2020) and\nconduct several empirical studies using high-frequency financial data. Muni\nToke and Yoshida (2020) showed the consistency and asymptotic behavior of the\nCox-type model estimators for relative intensities of orders in the limit order\nbook, and then by using high-frequency trading data for 36 stocks traded on the\nParis Stock Exchange, they carry out model selection and trading sign\nprediction. In this study, we add new covariates and carry out model selection\nand trading sign prediction using high-frequency trading data for 222 stocks\ntraded on the Tokyo Stock Exchange. We not only show that the Cox-type model\nperforms well in the Japanese market as well as in the Euronext Paris market,\nbut also present the key factors for more accurate estimation. We also suggest\nhow often the covariates should be calibrated.\n"
    },
    {
        "paper_id": 2302.01816,
        "authors": "Jaros{\\l}aw Gruszka, Janusz Szwabi\\'nski",
        "title": "Portfolio Optimisation via the Heston Model Calibrated to Real Asset\n  Data",
        "comments": "24 pages, 4 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The debate between active and passive investment strategies has been ongoing\nfor many years and is far from being over. In this paper, we show that the\nchoice of an optimal portfolio management strategy depends on an investment\nclimate, which we measure via the parameters of the Heston model calibrated to\nthe real stock market data. Depending on the values of those parameters, the\npassive strategy may namely outperform the active ones or vice versa. The\nmethod is tested on three stock market indices: S\\&P500, DAX and WIG20.\n"
    },
    {
        "paper_id": 2302.01897,
        "authors": "Nathan Canen, Kristopher Ramsay",
        "title": "Quantifying Theory in Politics: Identification, Interpretation and the\n  Role of Structural Methods",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The best empirical research in political science clearly defines substantive\nparameters of interest, presents a set of assumptions that guarantee its\nidentification, and uses an appropriate estimator. We argue for the importance\nof explicitly integrating rigorous theory into this process and focus on the\nadvantages of doing so. By integrating theoretical structure into one's\nempirical strategy, researchers can quantify the effects of competing\nmechanisms, consider the ex-ante effects of new policies, extrapolate findings\nto new environments, estimate model-specific theoretical parameters, evaluate\nthe fit of a theoretical model, and test competing models that aim to explain\nthe same phenomena. As a guide to such a methodology, we provide an overview of\nstructural estimation, including formal definitions, implementation\nsuggestions, examples, and comparisons to other methods.\n"
    },
    {
        "paper_id": 2302.02221,
        "authors": "Jangho Lee, Lily Wu, Andrew E. Dessler",
        "title": "A quantification of how much crypto-miners are driving up the wholesale\n  cost of energy in Texas",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  The use of energy by cryptocurrency mining comes not just with an\nenvironmental cost but also an economic one through increases in electricity\nprices for other consumers. Here we investigate the increase in wholesale price\non Texas ERCOT grid due to energy consumption from cryptocurrency mining. For\nevery GW of cryptocurrency mining load on the grid, we find that the wholesale\nprice of electricity on the ERCOT grid increases by 2 per Cent. Given that\ntodays cryptocurrency mining load on the ERCOT grid is around 1 GW, it suggests\nthat wholesale prices have already risen this amount. There are 27 GW of mining\nload waiting to be hooked up to the ERCOT grid. If cryptocurrency mining\nincreases rapidly, the price of energy in Texas could skyrocket.\n"
    },
    {
        "paper_id": 2302.02269,
        "authors": "Jos\\'e-Manuel Pe\\~na, Fernando Su\\'arez, Omar Larr\\'e, Domingo\n  Ram\\'irez, Arturo Cifuentes",
        "title": "A Modified CTGAN-Plus-Features Based Method for Optimal Asset Allocation",
        "comments": "In figures 3 and 4, the labels \"Synthetic'' and \"Original'' were\n  swapped. Now these figures have the correct labels. Results unchanged",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We propose a new approach to portfolio optimization that utilizes a unique\ncombination of synthetic data generation and a CVaR-constraint. We formulate\nthe portfolio optimization problem as an asset allocation problem in which each\nasset class is accessed through a passive (index) fund. The asset-class weights\nare determined by solving an optimization problem which includes a\nCVaR-constraint. The optimization is carried out by means of a Modified CTGAN\nalgorithm which incorporates features (contextual information) and is used to\ngenerate synthetic return scenarios, which, in turn, are fed into the\noptimization engine. For contextual information we rely on several points along\nthe U.S. Treasury yield curve. The merits of this approach are demonstrated\nwith an example based on ten asset classes (covering stocks, bonds, and\ncommodities) over a fourteen-and-half year period (January 2008-June 2022). We\nalso show that the synthetic generation process is able to capture well the key\ncharacteristics of the original data, and the optimization scheme results in\nportfolios that exhibit satisfactory out-of-sample performance. We also show\nthat this approach outperforms the conventional equal-weights (1/N) asset\nallocation strategy and other optimization formulations based on historical\ndata only.\n"
    },
    {
        "paper_id": 2302.02485,
        "authors": "Robert Parham",
        "title": "Facts of US Firm Scale and Growth 1970-2019: An Illustrated Guide",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
        "abstract": "  This work analyzes data on all public US firms in the 50 year period\n1970-2019, and presents 18 stylized facts of their scale, income, growth,\nreturn, investment, and dynamism. Special attention is given to (i) identifying\ndistributional forms; and (ii) scale effects -- systematic difference between\nfirms based on their scale of operations. Notable findings are that the\nDifference-of-Log-Normals (DLN) distribution has a central role in describing\nfirm data, scale-dependent heteroskedasticity is rampant, and small firms are\nsystematically different from large firms.\n"
    },
    {
        "paper_id": 2302.02486,
        "authors": "Robert Parham",
        "title": "The Difference-of-Log-Normals Distribution: Properties, Estimation, and\n  Growth",
        "comments": null,
        "journal-ref": null,
        "doi": "10.48550/arXiv.2302.02486",
        "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
        "abstract": "  This paper describes the Difference-of-Log-Normals (DLN) distribution. A\ncompanion paper makes the case that the DLN is a fundamental distribution in\nnature, and shows how a simple application of the CLT gives rise to the DLN in\nmany disparate phenomena. Here, I characterize its PDF, CDF, moments, and\nparameter estimators; generalize it to N-dimensions using spherical\ndistribution theory; describe methods to deal with its signature\n``double-exponential'' nature; and use it to generalize growth measurement to\npossibly-negative variates distributing DLN. I also conduct Monte-Carlo\nexperiments to establish some properties of the estimators and measures\ndescribed.\n"
    },
    {
        "paper_id": 2302.02762,
        "authors": "Md Shah Naoaj and Mir Md Moyazzem Hosen",
        "title": "Does higher capital maintenance drive up banks cost of equity? Evidence\n  from Bangladesh",
        "comments": "19 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/publicdomain/zero/1.0/",
        "abstract": "  This paper assesses whether the higher capital maintenance drives up banks\ncost of equity. We investigate the hypothesis using fixed effect panel\nestimation with the data from a sample of 28 publicly listed commercial banks\nover the 2013 to 2019 periods. We find a significant negative relationship\nbetween banks capital and cost of equity. Empirically our baseline estimates\nentail that a 10 percent increase in capital would reduce the cost of equity by\n4.39 percent.\n"
    },
    {
        "paper_id": 2302.02767,
        "authors": "Filippo Bontadini, Mercedes Campi, and Marco Due\\~nas",
        "title": "Being at the core: firm product specialisation",
        "comments": "28 pages, 8 figures, and 7 tables",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  We propose a novel measure to investigate firms' product specialisation:\nproduct coreness, that captures the centrality of exported products within the\nfirm's export basket. We study product coreness using firm-product level data\nbetween 2018 and 2020 for Colombia, Ecuador, and Peru. Three main findings\nemerge from our analysis. First, the composition of firms' export baskets\nchanges relatively little from one year to the other, and products far from the\nfirm's core competencies, with low coreness, are more likely to be dropped.\nSecond, higher coreness is associated with larger export flows at the firm\nlevel. Third, such firm-level patterns also have implications at the aggregate\nlevel: products that are, on average, exported with higher coreness have higher\nexport flows at the country level, which holds across all levels of product\ncomplexity. Therefore, the paper shows that how closely a product fits within a\nfirm's capabilities is important for economic performance at both the firm and\ncountry level. We explore these issues within an econometric framework, finding\nrobust evidence both across our three countries and for each country\nseparately.\n"
    },
    {
        "paper_id": 2302.02769,
        "authors": "Federica De Domenico, Giacomo Livan, Guido Montagna and Oreste\n  Nicrosini",
        "title": "Modeling and Simulation of Financial Returns under Non-Gaussian\n  Distributions",
        "comments": "20 pages, 9 figures",
        "journal-ref": null,
        "doi": "10.1016/j.physa.2023.128886",
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  It is well known that the probability distribution of high-frequency\nfinancial returns is characterized by a leptokurtic, heavy-tailed shape. This\nbehavior undermines the typical assumption of Gaussian log-returns behind the\nstandard approach to risk management and option pricing. Yet, there is no\nconsensus on what class of probability distributions should be adopted to\ndescribe financial returns and different models used in the literature have\ndemonstrated, to varying extent, an ability to reproduce empirically observed\nstylized facts. In order to provide some clarity, in this paper we perform a\nthorough study of the most popular models of return distributions as obtained\nin the empirical analyses of high-frequency financial data. We compare the\nstatistical properties and simulate the dynamics of non-Gaussian financial\nfluctuations by means of Monte Carlo sampling from the different models in\nterms of realistic tail exponents. Our findings show a noticeable consistency\nbetween the considered return distributions in the modeling of the scaling\nproperties of large price changes. We also discuss the convergence rate to the\nasymptotic distributions of the non-Gaussian stochastic processes and we study,\nas a first example of possible applications, the impact of our results on\noption pricing in comparison with the standard Black and Scholes approach.\n"
    },
    {
        "paper_id": 2302.02808,
        "authors": "Niels Gillmann (1 and 2), Ostap Okhrin (2) ((1) ifo Institute Dresden,\n  (2) Technische Universit\\\"at Dresden)",
        "title": "Adaptive local VAR for dynamic economic policy uncertainty spillover",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  The availability of data on economic uncertainty sparked a lot of interest in\nmodels that can timely quantify episodes of international spillovers of\nuncertainty. This challenging task involves trading off estimation accuracy for\nmore timely quantification. This paper develops a local vector autoregressive\nmodel (VAR) that allows for adaptive estimation of the time-varying\nmultivariate dependency. Under local, we mean that for each point in time, we\nsimultaneously estimate the longest interval on which the model is constant\nwith the model parameters. The simulation study shows that the model can handle\none or multiple sudden breaks as well as a smooth break in the data. The\nempirical application is done using monthly Economic Policy Uncertainty data.\nThe local model highlights that the empirical data primarily consists of long\nhomogeneous episodes, interrupted by a small number of heterogeneous ones, that\ncorrespond to crises. Based on this observation, we create a crisis index,\nwhich reflects the homogeneity of the sample over time. Furthermore, the local\nmodel shows superiority against the rolling window estimation.\n"
    },
    {
        "paper_id": 2302.02833,
        "authors": "Pierre Pinson",
        "title": "What may future electricity markets look like?",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Should the organization, design and functioning of electricity markets be\ntaken for granted? Definitely not. While decades of evolution of electricity\nmarkets in countries that committed early to restructure their electric power\nsector made us believe that we may have found the right and future-proof model,\nthe substantially and rapidly evolving context of our power and energy systems\nis challenging this idea in many ways. Actually, that situation brings both\nchallenges and opportunities. Challenges include accommodation of renewable\nenergy generation, decentralization and support to investment, while\nopportunities are mainly that advances in technical and social sciences provide\nus with many more options in terms of future market design. We here take a\nholistic point of view, by trying to understand where we are coming from with\nelectricity markets and where we may be going. Future electricity markets\nshould be made fit for purpose by considering them as a way to organize and\noperate a socio-techno-economic system.\n"
    },
    {
        "paper_id": 2302.02875,
        "authors": "Mikhail V. Sokolov",
        "title": "NPV, IRR, PI, PP, and DPP: a unified view",
        "comments": "46 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper introduces a class of investment project's profitability metrics\nthat includes the net present value (NPV) criterion (which labels a project as\nweakly profitable if its NPV is nonnegative), internal rate of return (IRR),\nprofitability index (PI), payback period (PP), and discounted payback period\n(DPP) as special cases. We develop an axiomatic characterization of this class,\nas well as of the mentioned conventional metrics within the class. The proposed\napproach offers several key contributions. First, it provides a unified\ninterpretation of profitability metrics as indicators of a project's financial\nstability across various economic scenarios. Second, it reveals that, except\nfor the NPV criterion, a profitability metric is inherently undefined for some\nprojects. In particular, this implies that any extension of IRR to the space of\nall projects does not meet a set of reasonable conditions. A similar conclusion\nis valid for the other mentioned conventional metrics. For each of these\nmetrics, we offer a characterization of the pairs of comparable projects and\nidentify the largest set of projects to which the metric can be unequivocally\nextended. Third, our study identifies conditions under which the application of\none metric is superior to others, helping to guide decision-makers in selecting\nthe most appropriate metric for specific investment contexts.\n"
    },
    {
        "paper_id": 2302.03261,
        "authors": "Stanislav Levytskyi, Oleksandr Gneushev, Vasyl Makhlinets",
        "title": "The approach to modeling the value of statistical life using average per\n  capita income",
        "comments": "in Ukrainian language",
        "journal-ref": null,
        "doi": "10.21511/dm.17(4).2019.01",
        "license": "http://creativecommons.org/licenses/by-sa/4.0/",
        "abstract": "  The problem of determining the value of statistical life in Ukraine in order\nto find ways to improve it is an urgent one now. The current level of value is\nanalyzed, which is a direct consequence of the poor quality of life of a\ncitizen, hence his low level. The description of the basic theoretical and\nmethodological approaches to the estimation of the cost of human life is given.\nBased on the analysis, a number of hypotheses have been advanced about the use\nof statistical calculations to achieve the modeling objectives. Model\ncalculations are based on the example of Zaporozhye Oblast statistics for\n2018-2019. The article elaborates the approach to the estimation of the\neconomic equivalent of the cost of living on the basis of demographic\nindicators and average per capita income, and also analyzes the possibilities\nof their application in the realities of the national economy. Using\nStatistica, the regression equation parameters were determined for statistical\ndata of population distribution of Zaporizhzhia region by age groups for 2018.\nThe calculation parameters were also found using the Excel office application,\nusing the Solution Finder option to justify the quantitative range of metric\nvalues. It is proved that the proposed approach to modeling and calculations\nare simpler and more efficient than the calculation methods proposed earlier.\nThe study concluded that the value of statistical life in Ukraine is\nsignificantly undervalued.\n"
    },
    {
        "paper_id": 2302.03694,
        "authors": "Jean Marie Tshimula, D'Jeff K. Nkashama, Patrick Owusu, Marc Frappier,\n  Pierre-Martin Tardif, Froduald Kabanza, Armelle Brun, Jean-Marc Patenaude,\n  Shengrui Wang, Belkacem Chikhaoui",
        "title": "Characterizing Financial Market Coverage using Artificial Intelligence",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  This paper scrutinizes a database of over 4900 YouTube videos to characterize\nfinancial market coverage. Financial market coverage generates a large number\nof videos. Therefore, watching these videos to derive actionable insights could\nbe challenging and complex. In this paper, we leverage Whisper, a\nspeech-to-text model from OpenAI, to generate a text corpus of market coverage\nvideos from Bloomberg and Yahoo Finance. We employ natural language processing\nto extract insights regarding language use from the market coverage. Moreover,\nwe examine the prominent presence of trending topics and their evolution over\ntime, and the impacts that some individuals and organizations have on the\nfinancial market. Our characterization highlights the dynamics of the financial\nmarket coverage and provides valuable insights reflecting broad discussions\nregarding recent financial events and the world economy.\n"
    },
    {
        "paper_id": 2302.04034,
        "authors": "Jean-Gabriel Lauzier and Liyuan Lin and Ruodu Wang",
        "title": "Risk sharing, measuring variability, and distortion riskmetrics",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We address the problem of sharing risk among agents with preferences modelled\nby a general class of comonotonic additive and law-based functionals that need\nnot be either monotone or convex. Such functionals are called distortion\nriskmetrics, which include many statistical measures of risk and variability\nused in portfolio optimization and insurance. The set of Pareto-optimal\nallocations is characterized under various settings of general or comonotonic\nrisk sharing problems. We solve explicitly Pareto-optimal allocations among\nagents using the Gini deviation, the mean-median deviation, or the\ninter-quantile difference as the relevant variability measures. The latter is\nof particular interest, as optimal allocations are not comonotonic in the\npresence of inter-quantile difference agents; instead, the optimal allocation\nfeatures a mixture of pairwise counter-monotonic structures, showing some\npatterns of extremal negative dependence.\n"
    },
    {
        "paper_id": 2302.04055,
        "authors": "Maximilian Sp\\\"ath",
        "title": "The qualitative accuracy of the Becker-DeGroot-Marshak method",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  The Becker DeGroot Marshak method is widely used to elicit the valuation that\nan individual assigns to an object. Theoretically, the second-price structure\nof the method gives individuals the incentive to state their true valuation.\nYet, the elicitation methods empirical accuracy is subject to debate. With this\npaper, I provide a clear verification of the qualitative accuracy of the\nmethod. Participants of an incentivized laboratory experiment can sell a\nvirtual object. The value of the object is publicly known and experimentally\nvaried in a between-subjects design. Replicating previous findings on the low\nquantitative accuracy, I observe a very small share of individuals placing a\npayoff-optimal stated valuation. However, the analysis shows that the stated\nvaluation increases with the value of the object. This result shows the\nqualitative accuracy of the BDM method and suggests that the method can be\napplied in comparative studies.\n"
    },
    {
        "paper_id": 2302.04068,
        "authors": "Lioba Heimbach, Eric G. Schertenleib, Roger Wattenhofer",
        "title": "Short Squeeze in DeFi Lending Market: Decentralization in Jeopardy?",
        "comments": "In Proceedings of Workshop on Decentralized Finance (DeFi@FC)",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Anxiety levels in the Aave community spiked in November 2022 as Avi Eisenberg\nperformed an attack on Aave. Eisenberg attempted to short the CRV token by\nusing funds borrowed on the protocol to artificially deflate the value of CRV.\nWhile the attack was ultimately unsuccessful, it left the Aave community scared\nand even raised question marks regarding the feasibility of large lending\nplatforms under decentralized governance.\n  In this work, we analyze Avi Eisenberg's actions and show how he was able to\nartificially lower the price of CRV by selling large quantities of borrowed CRV\nfor stablecoins on both decentralized and centralized exchanges. Despite the\nfailure of his attack, it still led to irretrievable debt worth more than 1.5\nMio USD at the time and, thereby, quadrupled the protocol's irretrievable debt.\nFurthermore, we highlight that his attack was enabled by the vast proportion of\nCRV available to borrow as well as Aave's lending protocol design hindering\nrapid intervention. We stress Eisenberg's attack exposes a predicament of large\nDeFi lending protocols: limit the scope or compromise on 'decentralization'.\n"
    },
    {
        "paper_id": 2302.0411,
        "authors": "Alessio Tartaro, Adam Leon Smith, Patricia Shaw",
        "title": "Assessing the impact of regulations and standards on innovation in the\n  field of AI",
        "comments": "10 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  Regulations and standards in the field of artificial intelligence (AI) are\nnecessary to minimise risks and maximise benefits, yet some argue that they\nstifle innovation. This paper critically examines the idea that regulation\nstifles innovation in the field of AI. Current trends in AI regulation,\nparticularly the proposed European AI Act and the standards supporting its\nimplementation, are discussed. Arguments in support of the idea that regulation\nstifles innovation are analysed and criticised, and an alternative point of\nview is offered, showing how regulation and standards can foster innovation in\nthe field of AI.\n"
    },
    {
        "paper_id": 2302.04184,
        "authors": "Johann Lussange and Boris Gutkin",
        "title": "Order book regulatory impact on stock market quality: a multi-agent\n  reinforcement learning perspective",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Recent technological developments have changed the fundamental ways stock\nmarkets function, bringing regulatory instances to assess the benefits of these\ndevelopments. In parallel, the ongoing machine learning revolution and its\nmultiple applications to trading can now be used to design a next generation of\nfinancial models, and thereby explore the systemic complexity of financial\nstock markets in new ways. We here follow on a previous groundwork, where we\ndesigned and calibrated a novel agent-based model stock market simulator, where\neach agent autonomously learns to trade by reinforcement learning. In this\nPaper, we now study the predictions of this model from a regulator's\nperspective. In particular, we focus on how the market quality is impacted by\nsmaller order book tick sizes, increasingly larger metaorders, and higher\ntrading frequencies, respectively. Under our model assumptions, we find that\nthe market quality benefits from the latter, but not from the other two trends.\n"
    },
    {
        "paper_id": 2302.04201,
        "authors": "Hugo Sant'Anna and Samyam Shrestha",
        "title": "Labor Market Effects of the Venezuelan Refugee Crisis in Brazil",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  We use administrative panel data on the universe of Brazilian formal workers\nto investigate the labor market effects of the Venezuelan crisis in Brazil,\nfocusing on the border state of Roraima. The results using\ndifference-in-differences show that the monthly wages of Brazilians in Roraima\nincreased by around 2 percent, which was mostly driven by those working in\nsectors and occupations with no refugee involvement. The study finds negligible\njob displacement for Brazilians but finds evidence of native workers moving to\noccupations without immigrants. We also find that immigrants in the informal\nmarket offset the substitution effects in the formal market.\n"
    },
    {
        "paper_id": 2302.04307,
        "authors": "Paola Cecchi Dimeglio",
        "title": "Why the Mansfield Rule can't work: a supply demand analysis",
        "comments": "22 pages, 12 tables, 3 graphics",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Across the legal profession, statistics related to the numbers of women and\nother underrepresented groups in leadership roles continue to paint a bleak\npicture of diversity and inclusion. Some approaches to closing this gap have\nfocused on the cause; some have devised and applied solutions. Questions about\nthe efficacy of many of these solutions remain essentially unanswered. This\nempirical study represents one of the first of its kind. Studies of the legal\nprofession have not focused on the dynamics of supply and demand in the context\nof leadership positions (counsel and partner (equity and non-equity)). Neither\nhave they examined the interrelationships of these dynamics to race and gender\ndemographic factors (white female and minorities (male and female)). This\nresearch seeks to determine the supply-demand position of leadership in the\nlegal profession and establish market equilibrium for these counsel and partner\nroles.\n"
    },
    {
        "paper_id": 2302.04345,
        "authors": "Samuel Cohen, Marc Sabat\\'e Vidales, David \\v{S}i\\v{s}ka, {\\L}ukasz\n  Szpruch",
        "title": "Inefficiency of CFMs: hedging perspective and agent-based simulations",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We investigate whether the fee income from trades on the CFM is sufficient\nfor the liquidity providers to hedge away the exposure to market risk. We first\nanalyse this problem through the lens of continuous-time financial mathematics\nand derive an upper bound for not-arbitrage fee income that would make CFM\nefficient and liquidity provision fair. We then evaluate our findings by\nperforming multi-agent simulations by varying CFM fees, market volatility, and\nrate of arrival of liquidity takers. We observe that, on average, fee income\ngenerated from liquidity provision is insufficient to compensate for market\nrisk.\n"
    },
    {
        "paper_id": 2302.04734,
        "authors": "Henry Skeoch and David Pym",
        "title": "Pricing cyber-insurance for systems via maturity models",
        "comments": "32 pages, 11 figures, 12 tables",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
        "abstract": "  Pricing insurance for risks associated with information technology systems\npresents a complex modelling challenge, combining the disciplines of operations\nmanagement, security, and economics. This work proposes a socioeconomic\nmodelling framework for cyber-insurance decisions compromised of entity\nrelationship diagrams, security maturity models, and economic models,\naddressing a long-standing research challenge of capturing organizational\nstructure in the design and pricing of cyber-insurance policies. Insurance\npricing is usually informed by the long experience insurance companies have of\nthe magnitude and frequency of losses that arise in organizations based on\ntheir size, industry sector, and location. Consequently, their calculations of\npremia will start from a baseline determined by these considerations. A unique\nchallenge of cyber-insurance is that data history is limited and not\nnecessarily informative of future loss risk meaning that established actuarial\nmethodology for other lines of insurance may not be the optimal pricing\nstrategy. The modelling framework proposed in this paper provides a vehicle for\nagreement between practitioners in the cyber-insurance ecosystem on\ncyber-security risks and allows for the users to choose their desired level of\nabstraction in the description of a system.\n"
    },
    {
        "paper_id": 2302.04938,
        "authors": "Theo Diamandis, Max Resnick, Tarun Chitra, Guillermo Angeris",
        "title": "An Efficient Algorithm for Optimal Routing Through Constant Function\n  Market Makers",
        "comments": "To be presented at Financial Cryptography 2023",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Constant function market makers (CFMMs) such as Uniswap have facilitated\ntrillions of dollars of digital asset trades and have billions of dollars of\nliquidity. One natural question is how to optimally route trades across a\nnetwork of CFMMs in order to ensure the largest possible utility (as specified\nby a user). We present an efficient algorithm, based on a decomposition method,\nto solve the problem of optimally executing an order across a network of\ndecentralized exchanges. The decomposition method, as a side effect, makes it\nsimple to incorporate more complicated CFMMs, or even include 'aggregate CFMMs'\n(such as Uniswap v3), into the routing problem. Numerical results show\nsignificant performance improvements of this method, tested on realistic\nnetworks of CFMMs, when compared against an off-the-shelf commercial solver.\n"
    },
    {
        "paper_id": 2302.0517,
        "authors": "Shuaiqiang Liu, Graziana Colonna, Lech A. Grzelak and Cornelis W.\n  Oosterlee",
        "title": "GPU acceleration of the Seven-League Scheme for large time step\n  simulations of stochastic differential equations",
        "comments": null,
        "journal-ref": "Chapter in Mathematics: Key Enabling Technology for Scientific\n  Machine Learning by NDNS+, 2021 Cluster",
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Monte Carlo simulation is widely used to numerically solve stochastic\ndifferential equations. Although the method is flexible and easy to implement,\nit may be slow to converge. Moreover, an inaccurate solution will result when\nusing large time steps. The Seven League scheme, a deep learning-based\nnumerical method, has been proposed to address these issues. This paper\ngeneralizes the scheme regarding parallel computing, particularly on Graphics\nProcessing Units (GPUs), improving the computational speed.\n"
    },
    {
        "paper_id": 2302.05219,
        "authors": "Tobias Bitterli, Fabian Sch\\\"ar",
        "title": "Decentralized Exchanges: The Profitability Frontier of Constant Product\n  Market Makers",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper we analyze constant product market makers (CPMMs). We formalize\nthe liquidity providers' profitability conditions and introduce a concept we\ncall the profitability frontier in the xyk-space. We study the effect of mint\nand burn fees on the profitability frontier, consider various pool types, and\ncompile a large data set from all Uniswap V2 transactions. We use this data to\nfurther study our theoretical framework and the profitability conditions. We\nshow how the profitability of liquidity provision is severely affected by the\ncosts of mint and burn events relative to the portfolio size and the\ncharacteristics of the trading pair.\n"
    },
    {
        "paper_id": 2302.05243,
        "authors": "Will Hicks",
        "title": "Modelling Illiquid Stocks Using Quantum Stochastic Calculus",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Quantum Stochastic Calculus can be used as a means by which randomness can be\nintroduced to observables acting on a Hilbert space. In this article we show\nhow the mechanisms of Quantum Stochastic Calculus can be used to extend the\nclassical Black-Scholes framework by incorporating a breakdown in the liquidity\nof a traded asset. This is captured via the widening of the bid offer spread,\nand the impact on the nature of the resulting probability distribution is\nmodelled in this work.\n"
    },
    {
        "paper_id": 2302.05256,
        "authors": "Will Hicks",
        "title": "Modelling Illiquid Stocks Using Quantum Stochastic Calculus: Asymptotic\n  Methods",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This article investigates the Fokker-Planck equations that arise from the\napplication of quantum stochastic calculus to the modelling of illiquid\nfinancial markets, using asymptotic methods. We present a power series solution\nfor quantum stochastic processes with a non-zero conservation process. Whilst\nthe series in question are in general divergent, we show they can be used to\napproximate solutions for longer time frames, and provide estimates for the\nrelative error on the higher order terms.\n"
    },
    {
        "paper_id": 2302.05421,
        "authors": "Humayra Shoshi and Indranil SenGupta",
        "title": "Some asymptotics for short maturity Asian options",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Most of the existing methods for pricing Asian options are less efficient in\nthe limit of small maturities and small volatilities. In this paper, we use the\nlarge deviations theory for the analysis of short-maturity Asian options. We\npresent a local volatility model for the underlying market that incorporates a\njump term in addition to the drift and diffusion terms. We estimate the\nasymptotics for the out-of-the-money, in-the-money, and at-the-money\nshort-maturity Asian call and put options. Under appropriate assumptions, we\nshow that the asymptotics for out-of-the-money Asian call and put options are\ngoverned by rare events. For the at-the-money Asian options, the result is more\ninvolved and in that case, we find the upper and lower bounds of the\nasymptotics of the Asian option price.\n"
    },
    {
        "paper_id": 2302.05772,
        "authors": "Ni Yan, WenTing Tao",
        "title": "Set-Asides in USDA Food Procurement Auctions",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  We study the partial and full set-asides and their implication for changes in\nbidding behavior in first-price sealed-bid auctions in the context of United\nStates Department of Agriculture (USDA) food procurement auctions. Using five\nyears of bid data on different beef products, we implement weighted least\nsquares regression models to show that partial set-aside predicts decreases in\nboth offer prices and winning prices among large and small business bidders.\nFull set-aside predicts a small increase in offer prices and winning prices\namong small businesses. With these predictions, we infer that net profit of\nsmall businesses is unlikely to increase when set-asides are present.\n"
    },
    {
        "paper_id": 2302.05808,
        "authors": "R. Guy Thomas",
        "title": "Long-term option pricing with a lower reflecting barrier",
        "comments": "Accepted for publication in Annals of Actuarial Science",
        "journal-ref": null,
        "doi": "10.1017/S1748499522000227",
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  This paper considers the pricing of long-term options on assets such as\nhousing, where either government intervention or the economic nature of the\nasset is assumed to limit large falls in prices. The observed asset price is\nmodelled by a geometric Brownian motion (the 'notional price') reflected at a\nlower barrier. The resulting observed price has standard dynamics but with\nlocalised intervention at the barrier, which allows arbitrage with interim\nlosses; this is funded by the government's unlimited powers of intervention,\nand its exploitation is subject to credit constraints. Despite the lack of an\nequivalent martingale measure for the observed price, options on this price can\nbe expressed as compound options on the arbitrage-free notional price, to which\nstandard risk-neutral arguments can be applied. Because option deltas tend to\nzero when the observed price approaches the barrier, hedging with the observed\nprice gives the same results as hedging with the notional price, and so exactly\nreplicates option payoffs. Hedging schemes are not unique, with the cheapest\nscheme for any derivative being the one which best exploits the interventions\nat the barrier. The price of a put is clear: direct replication has a lower\ninitial cost than synthetic replication, and the replication portfolio always\nhas positive value. The price of a call is ambiguous: synthetic replication has\na lower initial cost than direct replication, but the replication portfolio may\ngive interim losses, and so the preferred replication strategy (and hence\nprice) of a call may depend on what margin payments need to be made on these\nlosses.\n"
    },
    {
        "paper_id": 2302.06348,
        "authors": "Ravi Kashyap",
        "title": "A Tale of Two Currencies: Cash and Crypto",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/publicdomain/zero/1.0/",
        "abstract": "  We discuss numerous justifications for why crypto-currencies would be highly\nconducive for the smooth functioning of today's society. We provide several\ncomparisons between cryptocurrencies issued by blockchain projects, crypto, and\nconventional government issued currencies, cash or fiat. We summarize seven\nfundamental innovations that would be required for participants to have greater\nconfidence in decentralized finance (DeFi) and to obtain wealth appreciation\ncoupled with better risk management. The conceptual ideas we discuss outline an\napproach to: 1) Strengthened Security Blueprint; 2) Rebalancing and Trade\nExecution Suited for Blockchain Nuances 3) Volatility and Variance Adjusted\nWeight Calculation 4) Accommodating Investor Preferences and Risk Parity\nConstruction; 5) Profit Sharing and Investor Protection; 6) Concentration Risk\nIndicator and Performance Metrics; 7) Multi-chain expansion and Select\nStrategic Initiatives including the notion of a Decentralized Autonomous\nOrganization (DAO). Incorporating these concepts into several projects would\nalso facilitate the growth of the overall blockchain eco-system so that this\ntechnology can, have wider mainstream adoption and, fulfill its potential in\ntransforming all aspects of human interactions.\n"
    },
    {
        "paper_id": 2302.06668,
        "authors": "Hamed Amini, Zhongyuan Cao, Andreea Minca and Agn\\`es Sulem",
        "title": "Ruin Probabilities for Risk Processes in Stochastic Networks",
        "comments": "31 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We study multidimensional Cram\\'er-Lundberg risk processes where agents,\nlocated on a large sparse network, receive losses form their neighbors. To\nreduce the dimensionality of the problem, we introduce classification of agents\naccording to an arbitrary countable set of types. The ruin of any agent\ntriggers losses for all of its neighbours. We consider the case when the loss\narrival process induced by the ensemble of ruined agents follows a Poisson\nprocess with general intensity function that scales with the network size. When\nthe size of the network goes to infinity, we provide explicit ruin\nprobabilities at the end of the loss propagation process for agents of any\ntype. These limiting probabilities depend, in addition to the agents' types and\nthe network structure, on the loss distribution and the loss arrival process.\nFor a more complex risk processes on open networks, when in addition to the\ninternal networked risk processes the agents receive losses from external\nusers, we provide bounds on ruin probabilities.\n"
    },
    {
        "paper_id": 2302.06682,
        "authors": "Arun Kumar Polala and Bernhard Hientzsch",
        "title": "Parametric Differential Machine Learning for Pricing and Calibration",
        "comments": "45 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Differential machine learning (DML) is a recently proposed technique that\nuses samplewise state derivatives to regularize least square fits to learn\nconditional expectations of functionals of stochastic processes as functions of\nstate variables. Exploiting the derivative information leads to fewer samples\nthan a vanilla ML approach for the same level of precision. This paper extends\nthe methodology to parametric problems where the processes and functionals also\ndepend on model and contract parameters, respectively. In addition, we propose\nadaptive parameter sampling to improve relative accuracy when the functionals\nhave different magnitudes for different parameter sets. For calibration, we\nconstruct pricing surrogates for calibration instruments and optimize over them\nglobally. We discuss strategies for robust calibration. We demonstrate the\nusefulness of our methodology on one-factor Cheyette models with benchmark rate\nvolatility specification with an extra stochastic volatility factor on\n(two-curve) caplet prices at different strikes and maturities, first for\nparametric pricing, and then by calibrating to a given caplet volatility\nsurface. To allow convenient and efficient simulation of processes and\nfunctionals and in particular the corresponding computation of samplewise\nderivatives, we propose to specify the processes and functionals in a low-code\nway close to mathematical notation which is then used to generate efficient\ncomputation of the functionals and derivatives in TensorFlow.\n"
    },
    {
        "paper_id": 2302.06778,
        "authors": "Minglian Lin and Indranil SenGupta",
        "title": "Analysis of optimal portfolio on finite and small-time horizons for a\n  stochastic volatility model with multiple correlated assets",
        "comments": "arXiv admin note: text overlap with arXiv:2104.06293",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper, we consider the portfolio optimization problem in a financial\nmarket where the underlying stochastic volatility model is driven by\nn-dimensional Brownian motions. At first, we derive a Hamilton-Jacobi-Bellman\nequation including the correlations among the standard Brownian motions. We use\nan approximation method for the optimization of portfolios. With such\napproximation, the value function is analyzed using the first-order terms of\nexpansion of the utility function in the powers of time to the horizon. The\nerror of this approximation is controlled using the second-order terms of\nexpansion of the utility function. It is also shown that the one-dimensional\nversion of this analysis corresponds to a known result in the literature. We\nalso generate a close-to-optimal portfolio near the time to horizon using the\nfirst-order approximation of the utility function. It is shown that the error\nis controlled by the square of the time to the horizon. Finally, we provide an\napproximation scheme to the value function for all times and generate a\nclose-to-optimal portfolio.\n"
    },
    {
        "paper_id": 2302.07117,
        "authors": "Quyen Van, Vy Tran",
        "title": "Control of Emerging-Market Target, Abnormal Stock Return: Evidence in\n  Vietnam",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Joining with the upward trend of Global Foreign direct investment and FDI in\nemerging economies and emerging Asian economies, FDI to Vietnam, especially\nM&As have increased significantly in both numbers and value of deals from 1995\nto 2015...\n"
    },
    {
        "paper_id": 2302.0732,
        "authors": "Mohamed Hamdouche and Pierre Henry-Labordere and Huyen Pham",
        "title": "Policy gradient learning methods for stochastic control with exit time\n  and applications to share repurchase pricing",
        "comments": "19 pages, 6 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/publicdomain/zero/1.0/",
        "abstract": "  We develop policy gradients methods for stochastic control with exit time in\na model-free setting. We propose two types of algorithms for learning either\ndirectly the optimal policy or by learning alternately the value function\n(critic) and the optimal control (actor). The use of randomized policies is\ncrucial for overcoming notably the issue related to the exit time in the\ngradient computation. We demonstrate the effectiveness of our approach by\nimplementing our numerical schemes in the application to the problem of share\nrepurchase pricing. Our results show that the proposed policy gradient methods\noutperform PDE or other neural networks techniques in a model-based setting.\nFurthermore, our algorithms are flexible enough to incorporate realistic market\nconditions like e.g. price impact or transaction costs.\n"
    },
    {
        "paper_id": 2302.0747,
        "authors": "Shuoqing Deng, Xiang Yu, Jiacheng Zhang",
        "title": "On time-consistent equilibrium stopping under aggregation of diverse\n  discount rates",
        "comments": "Typos are corrected. A new section on the connection between optimal\n  mild equilibrium and weak equilibrium is added",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper studies the central planner's decision making on behalf of a group\nof members with diverse discount rates. In the context of optimal stopping, we\nwork with a smooth aggregation preference to incorporate all heterogeneous\ndiscount rates with an attitude function that reflects the aggregation rule in\nthe same spirit of ambiguity aversion in the smooth ambiguity preference\nproposed in Klibanoff et al.(2005). The optimal stopping problem renders to be\ntime inconsistent, for which we develop an iterative approach using consistent\nplanning and characterize all time-consistent equilibria as fixed points of an\noperator in the setting of one-dimensional diffusion processes. We provide some\nsufficient conditions on both the underlying models and the attitude function\nsuch that the smallest equilibrium attains the optimal equilibrium in which the\nattitude function becomes equivalent to the linear aggregation rule as of\ndiversity neutral. In addition, we show that the optimal equilibrium is a weak\nequilibrium in the existing literature. When the sufficient condition of the\nattitude function is violated, we can illustrate by various examples that the\ncharacterization of the optimal equilibrium may differ significantly from some\nexisting results for an individual agent.\n"
    },
    {
        "paper_id": 2302.07525,
        "authors": "Thomas Standfuss, Georg Hirte, Michael Schultz, and Hartmut Fricke",
        "title": "Efficiency in European Air Traffic Management -- A Fundamental Analysis\n  of Data, Models, and Methods",
        "comments": "14 Pages, to be published in JATM",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  We systematically study cornerstones that must be solved to define an air\ntraffic control benchmarking system based on a Data Envelopment Analysis.\nPrimarily, we examine the appropriate decision-making units, what to consider\nand what to avoid when choosing inputs and outputs in the case that several\ncountries are included, and how we can identify and deal with outliers, like\nthe Maastricht Service Provider. We argue that Air Navigation Service Providers\nwould be a good choice of decision units within the European context. Based on\nthat, we discuss candidates for DEA inputs and outputs and emphasize that\nmonetary values should be excluded. We, further suggest to use super-efficiency\nDEA for eliminating outliers. In this context, we compare different DEA\napproaches and find that standard DEA is performing well.\n"
    },
    {
        "paper_id": 2302.07619,
        "authors": "Burina Fujiwara",
        "title": "A study on Non-Performing Assets Cases and Cryptocurrency in Japan",
        "comments": "8 pages, 5 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The economic bubble bursting resulted in a large number of non-performing\nloans in Japanese financial institutions, which weakened their functions and\nprevented them from extending credit for normal economic activities. However,\ncryptocurrency operations are thriving in Japan. In this way, this paper\nfocuses on non-performing assets and cryptocurrencies. The goal is to use\nliterature analysis methods to summarise the development process, types of\nissuance, mechanisms, evaluation models, application scenarios, and trends in\nhow cryptocurrencies are supervised.\n"
    },
    {
        "paper_id": 2302.07631,
        "authors": "Qi Chen and Chao Guo",
        "title": "Path Integral Method for Pricing Proportional Step Double-Barrier Option\n  with Time Dependent Parameters",
        "comments": "18 pages, 3 figures, 1 table. arXiv admin note: substantial text\n  overlap with arXiv:2209.12542",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Path integral method in quantum mechanics provides a new thinking for barrier\noption pricing. For proportional double-barrier step (PDBS) options, the option\nprice changing process is analogous to a particle moving in a finite symmetric\nsquare potential well. We have derived the pricing kernel of PDBS options with\ntime dependent interest rate and volatility. Numerical results of option price\nas a function of underlying asset price are shown as well. Path integral method\ncan be easily generalized to the pricing of PDBS options with curved\nboundaries.\n"
    },
    {
        "paper_id": 2302.07695,
        "authors": "Deniz Preil, Michael Krapp",
        "title": "Genetic multi-armed bandits: a reinforcement learning approach for\n  discrete optimization via simulation",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper proposes a new algorithm, referred to as GMAB, that combines\nconcepts from the reinforcement learning domain of multi-armed bandits and\nrandom search strategies from the domain of genetic algorithms to solve\ndiscrete stochastic optimization problems via simulation. In particular, the\nfocus is on noisy large-scale problems, which often involve a multitude of\ndimensions as well as multiple local optima. Our aim is to combine the property\nof multi-armed bandits to cope with volatile simulation observations with the\nability of genetic algorithms to handle high-dimensional solution spaces\naccompanied by an enormous number of feasible solutions. For this purpose, a\nmulti-armed bandit framework serves as a foundation, where each observed\nsimulation is incorporated into the memory of GMAB. Based on this memory,\ngenetic operators guide the search, as they provide powerful tools for\nexploration as well as exploitation. The empirical results demonstrate that\nGMAB achieves superior performance compared to benchmark algorithms from the\nliterature in a large variety of test problems. In all experiments, GMAB\nrequired considerably fewer simulations to achieve similar or (far) better\nsolutions than those generated by existing methods. At the same time, GMAB's\noverhead with regard to the required runtime is extremely small due to the\nsuggested tree-based implementation of its memory. Furthermore, we prove its\nconvergence to the set of global optima as the simulation effort goes to\ninfinity.\n"
    },
    {
        "paper_id": 2302.07721,
        "authors": "Andreas Celary (1), Paul Eisenberg (1), Zehra Eksi (1) ((1) Institute\n  for Statistics and Mathematics, WU-University of Economics and Business,\n  Vienna, Austria)",
        "title": "Regime-switching affine term structures",
        "comments": "31 pages, 5 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We consider an HJM model setting for Markov-chain modulated forward rates.\nThe underlying Markov chain is assumed to induce regime switches on the forward\ncurve dynamics. Our primary focus is on the interest rate and energy futures\nmarkets. After deriving HJM-drift conditions for the two markets, we prove\nunder the assumption of affine structure for the term structure that the\nforward curves are solutions to specific systems of ODEs that can be solved\nexplicitly in many cases. This allows for a tractable model setting, and we\npresent an algorithm for obtaining consistent forward curve models within our\nframework. We conclude by presenting some simple numerical examples.\n"
    },
    {
        "paper_id": 2302.07758,
        "authors": "Aur\\'elien Alfonsi",
        "title": "Nonnegativity preserving convolution kernels. Application to Stochastic\n  Volterra Equations in closed convex domains and their approximation",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This work defines and studies convolution kernels that preserve\nnonnegativity. When the past dynamics of a process is integrated with a\nconvolution kernel like in Stochastic Volterra Equations or in the jump\nintensity of Hawkes processes, this property allows to get the nonnegativity of\nthe integral. We give characterizations of these kernels and show in particular\nthat completely monotone kernels preserve nonnegativity. We then apply these\nresults to analyze the stochastic invariance of a closed convex set by\nStochastic Volterra Equations. We also get a comparison result in dimension\none. Last, when the kernel is a positive linear combination of decaying\nexponential functions, we present a second order approximation scheme for the\nweak error that stays in the closed convex domain under suitable assumptions.\nWe apply these results to the rough Heston model and give numerical\nillustrations.\n"
    },
    {
        "paper_id": 2302.07796,
        "authors": "H. T. Shehzad, M. A. Anwar, M. Razzaq",
        "title": "A Comparative Predicting Stock Prices using Heston and Geometric\n  Brownian Motion Models",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  This paper presents a novel approach to predicting stock prices using\ntechnical analysis. By utilizing Ito's lemma and Euler-Maruyama methods, the\nresearchers develop Heston and Geometric Brownian Motion models that take into\naccount volatility, interest rate, and historical stock prices to generate\npredictions. The results of the study demonstrate that these models are\neffective in accurately predicting stock prices and outperform commonly used\nstatistical indicators. The authors conclude that this technical analysis-based\nmethod offers a promising solution for stock market prediction.\n"
    },
    {
        "paper_id": 2302.07822,
        "authors": "Nicola Cantarutti, Alex Harker, Carter Woetzel",
        "title": "Silkswap: An asymmetric automated market maker model for stablecoins",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  Silkswap is an automated market maker model designed for efficient stablecoin\ntrading with minimal price impact. The original purpose of Silkswap is to\nfacilitate the trading of fiat-pegged stablecoins with the stablecoin Silk, but\nit can be applied to any pair of stablecoins. The Silkswap invariant is a\nhybrid function that generates an asymmetric price impact curve. We present the\nderivation of the Silkswap model and its mathematical properties. We also\ncompare different numerical methods used to solve the invariant equation.\nFinally, we compare our model with the well-known Curve Finance model.\n"
    },
    {
        "paper_id": 2302.07911,
        "authors": "Giulio Caldarelli",
        "title": "From Reality Keys to Oraclize. A Deep Dive into the History of Bitcoin\n  Oracles",
        "comments": "Literature background and methodology are deliberately omitted at\n  this stage (preprint). To improve readability for a broader audience, the\n  content is presented more like a story",
        "journal-ref": null,
        "doi": "10.1109/ACCESS.2023.3279106",
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Before the advent of alternative blockchains such as Ethereum, the future of\ndecentralization was all in the hands of Bitcoin. Together with Nakamoto\nitself, early developers were trying to leverage Bitcoin potential to\ndecentralize traditionally centralized applications. However, being Bitcoin a\ndecentralized machine, available non-trustless oracles were considered\nunsuitable. Therefore, strategies had to be elaborated to solve the so-called\noracle problem in the newborn scenario. By interviewing early developers and\ncrawling early forums and repositories, this paper aims to retrace and\nreconstruct the chain of events and contributions that gave birth to oracles on\nBitcoin. The evolution of early trust models and approaches to solving the\noracle problem is also outlined. Analyzing technical and social barriers to\nbuilding oracles on Bitcoin, the transition to Ethereum will also be discussed.\n"
    },
    {
        "paper_id": 2302.07935,
        "authors": "Victor Olkhov",
        "title": "Market-Based Probability of Stock Returns",
        "comments": "17 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Markets possess all available information on stock returns. The randomness of\nmarket trade determines the statistics of stock returns. This paper describes\nthe dependence of the first four market-based statistical moments of stock\nreturns on statistical moments and correlations of current and past trade\nvalues. The mean return of trades during the averaging period coincides with\nMarkowitz's definition of portfolio value weighted return. We derive the\nmarket-based volatility of return and return-value correlations. We present\napproximations of the characteristic functions and probability measures of\nstock return by a finite number of market-based statistical moments. To\nforecast market-based average return or volatility of return, one should\npredict the statistical moments and correlations of current and past market\ntrade values at the same time horizon.\n"
    },
    {
        "paper_id": 2302.07968,
        "authors": "Paul X. McCarthy, Xian Gong, Fabian Stephany, Fabian Braesemann,\n  Marian-Andrei Rizoiu, Margaret L. Kern",
        "title": "The Science of Startups: The Impact of Founder Personalities on Company\n  Success",
        "comments": null,
        "journal-ref": "Sci Rep 13, 17200 (2023)",
        "doi": "10.1038/s41598-023-41980-y",
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Startup companies solve many of today's most complex and challenging\nscientific, technical and social problems, such as the decarbonisation of the\neconomy, air pollution, and the development of novel life-saving vaccines.\nStartups are a vital source of social, scientific and economic innovation, yet\nthe most innovative are also the least likely to survive. The probability of\nsuccess of startups has been shown to relate to several firm-level factors such\nas industry, location and the economy of the day. Still, attention has\nincreasingly considered internal factors relating to the firm's founding team,\nincluding their previous experiences and failures, their centrality in a global\nnetwork of other founders and investors as well as the team's size. The effects\nof founders' personalities on the success of new ventures are mainly unknown.\nHere we show that founder personality traits are a significant feature of a\nfirm's ultimate success. We draw upon detailed data about the success of a\nlarge-scale global sample of startups. We found that the Big 5 personality\ntraits of startup founders across 30 dimensions significantly differed from\nthat of the population at large. Key personality facets that distinguish\nsuccessful entrepreneurs include a preference for variety, novelty and starting\nnew things (openness to adventure), like being the centre of attention (lower\nlevels of modesty) and being exuberant (higher activity levels). However, we do\nnot find one \"Founder-type\" personality; instead, six different personality\ntypes appear, with startups founded by a \"Hipster, Hacker and Hustler\" being\ntwice as likely to succeed. Our results also demonstrate the benefits of\nlarger, personality-diverse teams in startups, which has the potential to be\nextended through further research into other team settings within business,\ngovernment and research.\n"
    },
    {
        "paper_id": 2302.07996,
        "authors": "Ali Fathi and Bernhard Hientzsch",
        "title": "A Comparison of Reinforcement Learning and Deep Trajectory Based\n  Stochastic Control Agents for Stepwise Mean-Variance Hedging",
        "comments": "37 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We consider two data-driven approaches to hedging, Reinforcement Learning and\nDeep Trajectory-based Stochastic Optimal Control, under a stepwise\nmean-variance objective. We compare their performance for a European call\noption in the presence of transaction costs under discrete trading schedules.\nWe do this for a setting where stock prices follow Black-Scholes-Merton\ndynamics and the \"book-keeping\" price for the option is given by the\nBlack-Scholes-Merton model with the same parameters. This simulated data\nsetting provides a \"sanitized\" lab environment with simple enough features\nwhere we can conduct a detailed study of strengths, features, issues, and\nlimitations of these two approaches. However, the formulation is model free and\ncould allow any other setting with available book-keeping prices. We consider\nthis study as a first step to develop, test, and validate autonomous hedging\nagents, and we provide blueprints for such efforts that address various\nconcerns and requirements.\n"
    },
    {
        "paper_id": 2302.08002,
        "authors": "Chen Liu, Chao Wang, Minh-Ngoc Tran, Robert Kohn",
        "title": "Deep Learning Enhanced Realized GARCH",
        "comments": "47 pages, 12 tables",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  We propose a new approach to volatility modeling by combining deep learning\n(LSTM) and realized volatility measures. This LSTM-enhanced realized GARCH\nframework incorporates and distills modeling advances from financial\neconometrics, high frequency trading data and deep learning. Bayesian inference\nvia the Sequential Monte Carlo method is employed for statistical inference and\nforecasting. The new framework can jointly model the returns and realized\nvolatility measures, has an excellent in-sample fit and superior predictive\nperformance compared to several benchmark models, while being able to adapt\nwell to the stylized facts in volatility. The performance of the new framework\nis tested using a wide range of metrics, from marginal likelihood, volatility\nforecasting, to tail risk forecasting and option pricing. We report on a\ncomprehensive empirical study using 31 widely traded stock indices over a time\nperiod that includes COVID-19 pandemic.\n"
    },
    {
        "paper_id": 2302.08041,
        "authors": "Dongdong Hu, Hasanjan Sayit, Frederi Viens",
        "title": "Pricing basket options with the first three moments of the basket:\n  log-normal models and beyond",
        "comments": "34 pages, 6 tables",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Options on baskets (linear combinations) of assets are notoriously\nchallenging to price using even the simplest log-normal continuous-time\nstochastic models for the individual assets. The paper [5] gives a closed form\napproximation formula for pricing basket options with potentially negative\nportfolio weights under log-normal models by moment matching. This\napproximation formula is conceptually simple, methodologically sound, and turns\nout to be highly accurate. However it involves solving a system of nonlinear\nequations which usually produces multiple solutions and which is sensitive to\nthe selection of initial values in the numerical procedures, making the method\ncomputationally challenging. In the current paper, we take the moment-matching\nmethodology in [5] a step further by obtaining a closed form solution for this\nnon-linear system of equations, by identifying a unary cubic equation based\nsolely on the basket's skewness, which parametrizes all model parameters, and\nwe use it to express the approximation formula as an explicit function of the\nmean, variance, and skewness of the basket. Numerical comparisons with the\nbaskets considered in [5] show a very high level of agreement, and thus of\naccuracy relative to the true basket option price.\n"
    },
    {
        "paper_id": 2302.08065,
        "authors": "Andrew W. Reddie, Ruby E. Booth, Bethany L. Goldblum, Kiran Lakkaraju,\n  Jason Reinhardt",
        "title": "Wargames as Data: Addressing the Wargamer's Trilemma",
        "comments": "3 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Policymakers often want the very best data with which to make\ndecisions--particularly when concerned with questions of national and\ninternational security. But what happens when this data is not available? In\nthose instances, analysts have come to rely on synthetic data-generating\nprocesses--turning to modeling and simulation tools and survey experiments\namong other methods. In the cyber domain, where empirical data at the strategic\nlevel are limited, this is no different--cyber wargames are quickly becoming a\nprincipal method for both exploring and analyzing the security challenges posed\nby state and non-state actors in cyberspace. In this chapter, we examine the\ndesign decisions associated with this method.\n"
    },
    {
        "paper_id": 2302.08167,
        "authors": "Jaehyuk Choi, Lan Ju, Jian Li, Zhiyong Tu",
        "title": "Information extraction and artwork pricing",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Traditional art pricing models often lack fine measurements of painting\ncontent. This paper proposes a new content measurement: the Shannon information\nquantity measured by the singular value decomposition (SVD) entropy of the\npainting image. Using a large sample of artworks' auction records and images,\nwe show that the SVD entropy positively affects the sales price at 1%\nsignificance level. Compared to the other commonly adopted content variables,\nthe SVD entropy has advantages in variable significance, sample robustness as\nwell as model fit. Considering the convenient availability of digital painting\nimages and the straightforward calculation algorithm of this measurement, we\nexpect its wide application in future research.\n"
    },
    {
        "paper_id": 2302.08208,
        "authors": "M. Raddant and T. Di Matteo",
        "title": "A Look at Financial Dependencies by Means of Econophysics and Financial\n  Economics",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This is a review about financial dependencies which merges efforts in\neconophysics and financial economics during the last few years. We focus on the\nmost relevant contributions to the analysis of asset markets' dependencies,\nespecially correlational studies, which in our opinion are beneficial for\nresearchers in both fields. In econophysics, these dependencies can be modeled\nto describe financial markets as evolving complex networks. In particular we\nshow that a useful way to describe dependencies is by means of information\nfiltering networks that are able to retrieve relevant and meaningful\ninformation in complex financial data sets. In financial economics these\ndependencies can describe asset comovement and spill-overs. In particular,\nseveral models are presented that show how network and factor model approaches\nare related to modeling of multivariate volatility and asset returns\nrespectively. Finally, we sketch out how these studies can inspire future\nresearch and how they contribute to support researchers in both fields to find\na better and a stronger common language.\n"
    },
    {
        "paper_id": 2302.08253,
        "authors": "Marina Santacroce, Paola Siri and Barbara Trivellato",
        "title": "Forward Backward SDEs Systems for Utility Maximization in Jump Diffusion\n  Models",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We consider the classical problem of maximizing the expected utility of\nterminal net wealth with a final random liability in a simple jump-diffusion\nmodel. In the spirit of Horst et al. (2014) and Santacroce-Trivellato (2014),\nunder suitable conditions the optimal strategy is expressed in implicit form in\nterms of a forward backward system of equations. Some explicit results are\npresented for the pure jump model and for exponential utilities.\n"
    },
    {
        "paper_id": 2302.08302,
        "authors": "Lijun Bo, Yijie Huang, Xiang Yu",
        "title": "Stochastic control problems with state-reflections arising from relaxed\n  benchmark tracking",
        "comments": "Keywords: Relaxed benchmark tracking, optimal consumption, Neumann\n  boundary conditions, probabilistic representation, reflected diffusion\n  process",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper studies stochastic control problems motivated by optimal\nconsumption with wealth benchmark tracking. The benchmark process is modeled by\na combination of a geometric Brownian motion and a running maximum process,\nindicating its increasing trend in the long run. We consider a relaxed tracking\nformulation such that the wealth compensated by the injected capital always\ndominates the benchmark process. The stochastic control problem is to maximize\nthe expected utility of consumption deducted by the cost of the capital\ninjection under the dynamic floor constraint. By introducing two auxiliary\nstate processes with reflections, an equivalent auxiliary control problem is\nformulated and studied, which leads to the HJB equation with two Neumann\nboundary conditions. We establish the existence of a unique classical solution\nto the dual PDE using some novel probabilistic representations involving the\nlocal time of some dual processes together with a tailor-made\ndecomposition-homogenization technique. The proof of the verification theorem\non the optimal feedback control can be carried out by some stochastic flow\nanalysis and technical estimations of the optimal control.\n"
    },
    {
        "paper_id": 2302.08323,
        "authors": "Alper Deniz Karakas",
        "title": "Reevaluating the Taylor Rule with Machine Learning",
        "comments": "20 pages, 7 figures, 1 table",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
        "abstract": "  This paper aims to reevaluate the Taylor Rule, through a linear and a\nnonlinear method, such that its estimated federal funds rates match those\nactually previously implemented by the Federal Reserve Bank. In the linear\nmethod, this paper uses an OLS regression model to find more accurate\ncoefficients within the same Taylor Rule equation in which the dependent\nvariable is the federal funds rate, and the independent variables are the\ninflation rate, the inflation gap, and the output gap. The intercept in the OLS\nregression model would capture the constant equilibrium target real interest\nrate set at 2. The linear OLS method suggests that the Taylor Rule\noverestimates the output gap and standalone inflation rate's coefficients for\nthe Taylor Rule. The coefficients this paper suggests are shown in equation\n(2). In the nonlinear method, this paper uses a machine learning system in\nwhich the two inputs are the inflation rate and the output gap and the output\nis the federal funds rate. This system utilizes gradient descent error\nminimization to create a model that minimizes the error between the estimated\nfederal funds rate and the actual previously implemented federal funds rate.\nSince the machine learning system allows the model to capture the more\nrealistic nonlinear relationship between the variables, it significantly\nincreases the estimation accuracy as a result. The actual and estimated federal\nfunds rates are almost identical besides three recessions caused by bubble\nbursts, which the paper addresses in the concluding remarks. Overall, the first\nmethod provides theoretical insight while the second suggests a model with\nimproved applicability.\n"
    },
    {
        "paper_id": 2302.08456,
        "authors": "Kelton Minor and Esteban Moro and Nick Obradovich",
        "title": "Adverse weather amplifies social media activity",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  Humanity spends an increasing proportion of its time interacting online.\nScholars are intensively investigating the societal drivers and resultant\nimpacts of this collective shift in our allocation of time and attention. Yet,\nthe external factors that regularly shape online behavior remain markedly\nunderstudied. Do environmental factors alter rates of online activity? Here we\nshow that adverse meteorological conditions markedly increase social media use\nin the United States. To do so, we employ climate econometric methods alongside\nover three and a half billion social media posts from tens of millions of\nindividuals from both Facebook and Twitter between 2009 and 2016. We find that\nmore extreme temperatures and added precipitation each independently amplify\nsocial media activity. Weather that is adverse on both the temperature and\nprecipitation dimensions produces markedly larger increases in social media\nactivity. On average across both platforms, compared to the temperate weather\nbaseline, days colder than -5{\\deg}C with 1.5-2cm of precipitation elevate\nsocial media activity by 35%. This effect is nearly three times the typical\nincrease in social media activity observed on New Year's Eve in New York City.\nWe observe meteorological effects on social media participation at both the\naggregate and individual level, even accounting for individual-specific,\ntemporal, and location-specific potential confounds.\n"
    },
    {
        "paper_id": 2302.08541,
        "authors": "Mikhail Freer and Khushboo Surana",
        "title": "Stable Marriage, Children, and Intrahousehold Allocations",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We present a revealed preference framework to study sharing of resources in\nhouseholds with children. We explicitly model the impact of the presence of\nchildren in the context of stable marriage markets under both potential types\nof custody arrangement - joint custody and sole custody. Our models deliver\ntestable revealed preference conditions and allow for the identification of\nintrahousehold allocation of resources. Empirical applications to household\ndata from the Netherlands (joint custody) and Russia (sole custody) show the\nmethods' potential to identify intrahousehold allocation.\n"
    },
    {
        "paper_id": 2302.08731,
        "authors": "Guohui Guan, Zongxia Liang, Yi Xia",
        "title": "Optimal management of DB pension fund under both underfunded and\n  overfunded cases",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper investigates the optimal management of an aggregated defined\nbenefit pension plan in a stochastic environment. The interest rate follows the\nOrnstein-Uhlenbeck model, the benefits follow the geometric Brownian motion\nwhile the contribution rate is determined by the spread method of fund\namortization. The pension manager invests in the financial market with three\nassets: cash, bond and stock. Regardless of the initial status of the plan, we\nsuppose that the pension fund may become underfunded or overfunded in the\nplanning horizon. The optimization goal of the manager is to maximize the\nexpected utility in the overfunded region minus the weighted solvency risk in\nthe underfunded region. By introducing an auxiliary process and related\nequivalent optimization problems and using the martingale method, the optimal\nwealth process, optimal portfolio and efficient frontier are obtained under\nfour cases (high tolerance towards solvency risk, low tolerance towards\nsolvency risk, a specific lower bound, and high lower bound). Moreover, we also\nobtain the probabilities that the optimal terminal wealth falls in the\noverfunded and underfunded regions. At last, we present numerical analyses to\nillustrate the manager's economic behaviors.\n"
    },
    {
        "paper_id": 2302.08758,
        "authors": "Jaehyuk Choi, Jeonggyu Huh, Nan Su",
        "title": "Tighter 'Uniform Bounds for Black-Scholes Implied Volatility' and the\n  applications to root-finding",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This note improves the lower and upper bounds of the Black-Scholes implied\nvolatility (IV) in Tehranchi (SIAM J. Financial Math., 7 (2016), p. 893). The\nproposed tighter bounds are systematically based on the bounds of the option\ndelta. While Tehranchi used the bounds to prove IV asymptotics, we apply the\nresult to the accurate numerical root-finding of IV. We alternatively formulate\nthe Newton-Raphson method on the log price and demonstrate that the iteration\nalways converges rapidly for all price ranges if the new lower bound found in\nthis study is used as an initial guess.\n"
    },
    {
        "paper_id": 2302.08809,
        "authors": "Filippo de Feo, Salvatore Federico, and Andrzej \\'Swi\\k{e}ch",
        "title": "Optimal control of stochastic delay differential equations and\n  applications to path-dependent financial and economic models",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this manuscript we consider a class optimal control problem for stochastic\ndifferential delay equations. First, we rewrite the problem in a suitable\ninfinite-dimensional Hilbert space. Then, using the dynamic programming\napproach, we characterize the value function of the problem as the unique\nviscosity solution of the associated infinite-dimensional\nHamilton-Jacobi-Bellman equation. Finally, we prove a $C^{1,\\alpha}$-partial\nregularity of the value function. We apply these results to path dependent\nfinancial and economic problems (Merton-like portfolio problem and optimal\nadvertising).\n"
    },
    {
        "paper_id": 2302.08819,
        "authors": "Alexander Lipton and Adil Reghai",
        "title": "SPX, VIX and scale-invariant LSV\\footnote{Local Stochastic Volatility}",
        "comments": "17 pages, 11 figures, 6 tables",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Local Stochastic Volatility (LSV) models have been used for pricing and\nhedging derivatives positions for over twenty years. An enormous body of\nliterature covers analytical and numerical techniques for calibrating the model\nto market data. However, the literature misses a potent approach commonly used\nin physics and works with absolute (dimensional) variables rather than with\nrelative (non-dimensional) ones. While model parameters defined in absolute\nterms are counter-intuitive for trading desks and tend to be heavily\ntime-dependent, relative parameters are intuitive and stable, making it easy to\nsteer the model adequately and consistently with its Profit and Loss (PnL)\nexplanation power. We propose a specification that first explores historical\ndata and uses physically well-defined relative quantities to design the model.\nWe then develop an efficient hybrid method to price derivatives under this\nspecification. We also show how our method can be used for robust scenario\ngeneration purposes - an important risk management task vital for buy-side\nfirms.\\footnote{The authors would like to thank Prof. Marcos Lopez de Prado and\nDr. Vincent Davy Zoonekynd for valuable comments.}\n"
    },
    {
        "paper_id": 2302.08829,
        "authors": "Matteo Smerlak",
        "title": "Great year, bad Sharpe? A note on the joint distribution of performance\n  and risk-adjusted return",
        "comments": "4 pages, 4 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Returns distributions are heavy-tailed across asset classes. In this note, I\nexamine the implications of this well-known stylized fact for the joint\nstatistics of performance (absolute return) and Sharpe ratio (risk-adjusted\nreturn). Using both synthetic and real data, I show that, all other things\nbeing equal, the investments with the best in-sample performance are never\nassociated with the best in-sample Sharpe ratios (and vice versa). This\ncounter-intuitive effect is unrelated to the risk-return tradeoff familiar from\nportfolio theory: it is, rather, a consequence of asymptotic correlations\nbetween the sample mean and sample standard deviation of heavy-tailed\nvariables. In addition to its large sample noise, this non-monotonic\nassociation of the Sharpe ratio with performance puts into question its status\nas the gold standard metric of investment quality.\n"
    },
    {
        "paper_id": 2302.08838,
        "authors": "Roberto Fontana and Patrizia Semeraro",
        "title": "Measuring distribution risk in discrete models",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Model risk measures consequences of choosing a model in a class of possible\nalternatives. We find analytical and simulated bounds for payoff functions on\nclasses of plausible alternatives of a given discrete model. We measure the\nimpact of choosing a risk-neutral measure on convex derivative pricing in\nincomplete markets. We find analytical bounds for prices of European and\nAmerican options in the class of all risk-neutral measures, and we also find\nsimulated bounds for given classes of perturbations of the minimal martingale\nequivalent measure.\n"
    },
    {
        "paper_id": 2302.08897,
        "authors": "Mostafa R. Sarkandiz",
        "title": "Forecasting the Turkish Lira Exchange Rates through Univariate\n  Techniques: Can the Simple Models Outperform the Sophisticated Ones?",
        "comments": "The paper has been accepted for publication in the journal \"Finance:\n  Theory and Practice\" for Volume 28, Issue 02, the Year 2024",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Throughout the past year, Turkey's central bank policy to decrease the\nnominal interest rate has caused episodes of severe fluctuations in Turkish\nlira exchange rates. According to these conditions, the daily return of the\nUSD/TRY have attracted the risk-taker investors' attention. Therefore, the\nuncertainty about the rates has pushed algorithmic traders toward finding the\nbest forecasting model. While there is a growing tendency to employ\nsophisticated models to forecast financial time series, in most cases, simple\nmodels can provide more precise forecasts. To examine that claim, present study\nhas utilized several models to predict daily exchange rates for a short\nhorizon. Interestingly, the simple exponential smoothing model outperformed all\nother alternatives. Besides, in contrast to the initial inferences, the time\nseries neither had structural break nor exhibited signs of the ARCH and\nleverage effects. Despite that behavior, there was undeniable evidence of a\nlong-memory trend. That means the series tends to keep a movement, at least for\na short period. Finally, the study concluded the simple models provide better\nforecasts for exchange rates than the complicated approaches.\n"
    },
    {
        "paper_id": 2302.08911,
        "authors": "Raihan Tanvir, Md Tanvir Rouf Shawon, Md. Golam Rabiul Alam",
        "title": "DSE Stock Price Prediction using Hidden Markov Model",
        "comments": "6 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Stock market forecasting is a classic problem that has been thoroughly\ninvestigated using machine learning and artificial neural network based tools\nand techniques. Interesting aspects of this problem include its time reliance\nas well as its volatility and other complex relationships. To combine them,\nhidden markov models (HMMs) have been utilized to anticipate the price of\nstocks. We demonstrated the Maximum A Posteriori (MAP) HMM method for\npredicting stock prices for the next day based on previous data. An HMM is\ntrained by analyzing the fractional change in the stock price as well as the\nintraday high and low values. It is then utilized to produce a MAP estimate\nacross all possible stock prices for the next day. The approach demonstrated in\nour work is quite generalized and can be used to predict the stock price for\nany company, given that the HMM is trained on the dataset of that company's\nstocks dataset. We evaluated the accuracy of our models using some extensively\nused accuracy metrics for regression problems and came up with a satisfactory\noutcome.\n"
    },
    {
        "paper_id": 2302.0892,
        "authors": "Martin G\\\"achter, Elias Hasler and Florian Huber",
        "title": "A tale of two tails: 130 years of growth-at-risk",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
        "abstract": "  We extend the existing growth-at-risk (GaR) literature by examining a long\ntime period of 130 years in a time-varying parameter regression model. We\nidentify several important insights for policymakers. First, both the level as\nwell as the determinants of GaR vary significantly over time. Second, the\nstability of upside risks to GDP growth reported in earlier research is\nspecific to the period known as the Great Moderation, with the distribution of\nrisks being more balanced before the 1970s. Third, the distribution of GDP\ngrowth has significantly narrowed since the end of the Bretton Woods system.\nFourth, financial stress is always linked to higher downside risks, but it does\nnot affect upside risks. Finally, other risk indicators, such as credit growth\nand house prices, not only drive downside risks, but also contribute to\nincreased upside risks during boom periods. In this context, the paper also\nadds to the financial cycle literature by completing the picture of drivers\n(and risks) for both booms and recessions over time.\n"
    },
    {
        "paper_id": 2302.08987,
        "authors": "Johannes Stangl, Andr\\'as Borsos, Christian Diem, Tobias Reisch,\n  Stefan Thurner",
        "title": "Firm-level supply chains to minimize unemployment and economic losses in\n  rapid decarbonization scenarios",
        "comments": "Nature Sustainability (2024)",
        "journal-ref": null,
        "doi": "10.1038/s41893-024-01321-x",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Urgently needed carbon emissions reductions might lead to strict\ncommand-and-control decarbonization strategies with potentially negative\neconomic consequences. Analysing the entire firm-level production network of a\nEuropean economy, we have explored how the worst outcomes of such approaches\ncan be avoided. We compared the systemic relevance of every firm in Hungary\nwith its annual CO2 emissions to identify optimal emission-reducing strategies\nwith a minimum of additional unemployment and economic losses. Setting specific\nreduction targets, we studied various decarbonization scenarios and quantified\ntheir economic consequences. We determined that for an emissions reduction of\n20%, the most effective strategy leads to losses of about 2% of jobs and 2% of\neconomic output. In contrast, a naive scenario targeting the largest emitters\nfirst results in 28% job losses and 33% output reduction for the same target.\nThis demonstrates that it is possible to use firm-level production networks to\ndesign highly effective decarbonization strategies that practically preserve\nemployment and economic output.\n"
    },
    {
        "paper_id": 2302.09009,
        "authors": "Peplluis R. Esteva and Alberto Ballesteros Rodr\\'iguez",
        "title": "Invoice discounting using kelly criterion by automated market\n  makers-like implementations",
        "comments": "43 pages, UCL-CBT Report",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
        "abstract": "  There is a persistent lack of funding, especially for SMEs, that cyclically\nworsens. The factoring and invoice discounting market appears to address delays\nin paying commercial invoices: sellers bring still-to-be-paid invoices to\nfinancial organizations, intermediaries, typically banks that provide an\nadvance payment. This article contains research on novel decentralized\napproaches to said lending services without intermediaries by using liquidity\npools and its associated heuristics, creating an Automated Market Maker. In our\napproach, the contributed collateral and the invoice trades with risk is\nmeasured with a formula: The Kelly criterion is used to calculate the optimal\npremium to be contributed to a liquidity pool in the funding of the said\ninvoices. The behavior of the algorithm is studied in several scenarios of\nstreams of invoices with representative amounts, collaterals, payment delays,\nand nonpayments rates or mora. We completed the study with hack scenarios with\nbogus, nonpayable invoices. As a result, we have created a resilient solution\nthat performs the best with partially collateralized invoices. The outcome is\ndecentralized market developed with the Kelly criterion that is reasonably\nresilient to a wide variety of the invoicing cases that provides sound profit\nto liquidity providers, and several premium distribution policies were checked\nthat contributed with extra resilience to the performance of the algorithm.\n"
    },
    {
        "paper_id": 2302.09176,
        "authors": "Anastasis Kratsios and Cody Hyndman",
        "title": "Generative Ornstein-Uhlenbeck Markets via Geometric Deep Learning",
        "comments": "9 Pages, 1 Figure",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/publicdomain/zero/1.0/",
        "abstract": "  We consider the problem of simultaneously approximating the conditional\ndistribution of market prices and their log returns with a single machine\nlearning model. We show that an instance of the GDN model of Kratsios and Papon\n(2022) solves this problem without having prior assumptions on the market's\n\"clipped\" log returns, other than that they follow a generalized\nOrnstein-Uhlenbeck process with a priori unknown dynamics. We provide universal\napproximation guarantees for these conditional distributions and contingent\nclaims with a Lipschitz payoff function.\n"
    },
    {
        "paper_id": 2302.09218,
        "authors": "Lin He, Zongxia Liang, Zhaojie Ren, Yilun Song",
        "title": "Optimal Mix Among PAYGO, EET and Individual Savings",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In order to deal with the aging problem, pension system is actively\ntransformed into the funded scheme. However, the funded scheme does not\ncompletely replace PAYGO (Pay as You Go) scheme and there exist heterogeneous\nmixes among PAYGO, EET (Exempt, Exempt, Taxed) and individual savings in\ndifferent countries. In this paper, we establish the optimal mix by solving a\nNash equilibrium between the pension participants and the government. Given the\nobligatory PAYGO and EET contribution rates, the participants choose the\noptimal asset allocation of the individual savings and the consumption policies\nto achieve the objective. The results extend the ``Samuelson-Aaron\" criterion\nto age-dependent preference orderings. And we identify three critical ages to\ndistinguish the multiple outcomes of preference orderings based on\nheterogeneous characteristic parameters. The government is fully aware of the\noptimal feedback of the participants. It chooses the optimal PAYGO and EET\ncontribution rates to maximize the overall utility of the participants weighted\nby each cohort's population. As such, the negative population growth rate leads\nto the decline of the PAYGO attractiveness as well as the increase of the older\ncohorts' weight in the government decision-making. The optimal mix is the\ncomprehensive result of the two effects.\n"
    },
    {
        "paper_id": 2302.09297,
        "authors": "Aymeric Ricome (JRC), Kamel Louhichi, Sergio Gomez y Paloma (JRC)",
        "title": "Subsidizing agricultural inputs in Senegal: Comparative analysis of\n  three modes of intervention using a farm household model",
        "comments": "in French language",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This report presents the results of an ex-ante impact assessment of several\nscenarios related to the farmer targeting of the input subsidy programme\ncurrently implemented in Senegal. This study has been achieved with the\nagricultural household model FSSIM-Dev, calibrated on a sample of 2 278 farm\nhouseholds from the ESPS-2 survey. The impacts on crop mix, fertilizer\napplication, farm income and on the government cost are presented and\ndiscussed.\n"
    },
    {
        "paper_id": 2302.09382,
        "authors": "Yutong Lu, Gesine Reinert and Mihai Cucuringu",
        "title": "Co-trading networks for modeling dynamic interdependency structures and\n  estimating high-dimensional covariances in US equity markets",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The time proximity of trades across stocks reveals interesting topological\nstructures of the equity market in the United States. In this article, we\ninvestigate how such concurrent cross-stock trading behaviors, which we denote\nas co-trading, shape the market structures and affect stock price co-movements.\nBy leveraging a co-trading-based pairwise similarity measure, we propose a\nnovel method to construct dynamic networks of stocks. Our empirical studies\nemploy high-frequency limit order book data from 2017-01-03 to 2019-12-09. By\napplying spectral clustering on co-trading networks, we uncover economically\nmeaningful clusters of stocks. Beyond the static Global Industry Classification\nStandard (GICS) sectors, our data-driven clusters capture the time evolution of\nthe dependency among stocks. Furthermore, we demonstrate statistically\nsignificant positive relations between low-latency co-trading and return\ncovariance. With the aid of co-trading networks, we develop a robust estimator\nfor high-dimensional covariance matrix, which yields superior economic value on\nportfolio allocation. The mean-variance portfolios based on our covariance\nestimates achieve both lower volatility and higher Sharpe ratios than standard\nbenchmarks.\n"
    },
    {
        "paper_id": 2302.09537,
        "authors": "Md Shah Naoaj",
        "title": "The Globalization-Inequality Nexus: A Comparative Study of Developed and\n  Developing Countries",
        "comments": "6 Pages",
        "journal-ref": "IOSR Journal of Economics and Finance (IOSR-JEF), 2023",
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  This study examines the relationship between globalization and income\ninequality, utilizing panel data spanning from 1992 to 2020. Globalization is\nmeasured by the World Bank global-link indicators such as FDI, Remittance,\nTrade Openness, and Migration while income inequality is measured by Gini\nCoefficient and the median income of 50% of the population. The fixed effect\npanel data analysis provides empirical evidence indicating that globalization\ntends to reduce income inequality, though its impact varies between developed\nand developing countries. The analysis reveals a strong negative correlation\nbetween net foreign direct investment (FDI) inflows and inequality in\ndeveloping countries, while no such relationship was found for developed\ncountries.The relationship holds even if we consider an alternative measure of\ninequality. However, when dividing countries by developed and developing\ngroups, no statistically significant relationship was observed. Policymakers\ncan use these findings to support efforts to increase FDI, trade, tourism, and\nmigration to promote growth and reduce income inequality.\n"
    },
    {
        "paper_id": 2302.09551,
        "authors": "Jiahua Xu, Daniel Perez, Yebo Feng, Benjamin Livshits",
        "title": "Auto.gov: Learning-based On-chain Governance for Decentralized Finance\n  (DeFi)",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In recent years, decentralized finance (DeFi) has experienced remarkable\ngrowth, with various protocols such as lending protocols and automated market\nmakers (AMMs) emerging. Traditionally, these protocols employ off-chain\ngovernance, where token holders vote to modify parameters. However, manual\nparameter adjustment, often conducted by the protocol's core team, is\nvulnerable to collusion, compromising the integrity and security of the system.\nFurthermore, purely deterministic, algorithm-based approaches may expose the\nprotocol to novel exploits and attacks.\n  In this paper, we present \"Auto.gov\", a learning-based on-chain governance\nframework for DeFi that enhances security and reduces susceptibility to\nattacks. Our model leverages a deep Q- network (DQN) reinforcement learning\napproach to propose semi-automated, intuitive governance proposals with\nquantitative justifications. This methodology enables the system to efficiently\nadapt to and mitigate the negative impact of malicious behaviors, such as price\noracle attacks, more effectively than benchmark models. Our evaluation\ndemonstrates that Auto.gov offers a more reactive, objective, efficient, and\nresilient solution compared to existing manual processes, thereby significantly\nbolstering the security and, ultimately, enhancing the profitability of DeFi\nprotocols.\n"
    },
    {
        "paper_id": 2302.09906,
        "authors": "Luca Mungo, Jos\\'e Moran",
        "title": "Revealing production networks from firm growth dynamics",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  We study the correlation structure of firm growth rates. We show that most\nfirms are correlated because of their exposure to a common factor but that\nfirms linked through the supply chain exhibit a stronger correlation on average\nthan firms that are not. Removing this common factor significantly reduces the\naverage correlation between two firms with no relationship in the supply chain\nwhile maintaining a significant correlation between two firms that are linked.\nWe then investigate if this observation can be used to reconstruct the topology\nof a supply chain network using Gaussian Markov Models.\n"
    },
    {
        "paper_id": 2302.09986,
        "authors": "Thomas Standfuss, Georg Hirte, Frank Fichert and Hartmut Fricke",
        "title": "Determinants of Performance in European ATM -- How to Analyze a Diverse\n  Industry",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Air traffic control is considered to be a bottleneck in European air traffic\nmanagement. As a result, the performance of the air navigation service\nproviders is critically examined and also used for benchmarking. Using\nquantitative methods, we investigate which endogenous and exogenous factors\naffect the performance of air traffic control units on different levels. The\nmethodological discussion is complemented by an empirical analysis. Results may\nbe used to derive recommendations for operators, airspace users, and\npolicymakers. We find that efficiency depends significantly on traffic patterns\nand the decisions of airspace users, but changes in the airspace structure\ncould also make a significant contribution to performance improvements.\n"
    },
    {
        "paper_id": 2302.10026,
        "authors": "Francois-Xavier Ladant and Julien Hedou and Paolo Sestito and Falco J.\n  Bargagli-Stoffi",
        "title": "What is essential is visible to the eye: Saliency in primary school\n  ranking and its effect on academic achievements",
        "comments": "A previous version of this paper was circulated under the title\n  \"Rather first in a village or second in Rome? The effect of students' class\n  rank in primary school on subsequent academic achievements\"",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  We propose a new strategy to identify the impact of class rank, exploiting a\n\"visible\" primary school rank from teachers' exam grades, and an \"invisible\"\nrank from unreported standardized test scores. Leveraging a unique panel\ndataset on Italian students, we show that the visible rank has a substantial\nimpact on students' perceptions, which affects subsequent academic performance.\nHowever, the effect of being surrounded by higher-SES or higher-achieving peers\nremains positive even accounting for the decrease in rank. Higher-ranked\nstudents self-select into high schools with higher average student\nachievements. Finally, exploiting an extensive survey, we identify\npsychological mechanisms channeling the rank effect.\n"
    },
    {
        "paper_id": 2302.1014,
        "authors": "Gianmarco Bet, Francesco Dainelli and Eugenio Fabrizi",
        "title": "The financial health of a company and the risk of its default: Back to\n  the future",
        "comments": "42 pages, 7 figures, 5 tables",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  We theorize the financial health of a company and the risk of its default. A\ncompany is financially healthy as long as its equilibrium in the financial\nsystem is maintained, which depends on the cost attributable to the probability\nthat equilibrium may decay. The estimate of that probability is based on the\ncredibility and uncertainty of the company's financial forecasts. Accordingly,\nwe develop an equilibrium model establishing ranges of interest rates as a\nfunction of predictable corporate performance and of its credit supply\nconditions. As a result, our model estimates idiosyncratic default risk and\nprovides intrinsically forward-looking PD.\n"
    },
    {
        "paper_id": 2302.10175,
        "authors": "Wee Ling Tan, Stephen Roberts, Stefan Zohren",
        "title": "Spatio-Temporal Momentum: Jointly Learning Time-Series and\n  Cross-Sectional Strategies",
        "comments": null,
        "journal-ref": "The Journal of Financial Data Science, Summer 2023",
        "doi": "10.3905/jfds.2023.1.130",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We introduce Spatio-Temporal Momentum strategies, a class of models that\nunify both time-series and cross-sectional momentum strategies by trading\nassets based on their cross-sectional momentum features over time. While both\ntime-series and cross-sectional momentum strategies are designed to\nsystematically capture momentum risk premia, these strategies are regarded as\ndistinct implementations and do not consider the concurrent relationship and\npredictability between temporal and cross-sectional momentum features of\ndifferent assets. We model spatio-temporal momentum with neural networks of\nvarying complexities and demonstrate that a simple neural network with only a\nsingle fully connected layer learns to simultaneously generate trading signals\nfor all assets in a portfolio by incorporating both their time-series and\ncross-sectional momentum features. Backtesting on portfolios of 46\nactively-traded US equities and 12 equity index futures contracts, we\ndemonstrate that the model is able to retain its performance over benchmarks in\nthe presence of high transaction costs of up to 5-10 basis points. In\nparticular, we find that the model when coupled with least absolute shrinkage\nand turnover regularization results in the best performance over various\ntransaction cost scenarios.\n"
    },
    {
        "paper_id": 2302.10252,
        "authors": "Antzelos Kyriazis, Iason Ofeidis, Georgios Palaiokrassas, Leandros\n  Tassiulas",
        "title": "Monetary Policy, Digital Assets, and DeFi Activity",
        "comments": "33 pages, 11 figures, 9 tables",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  This paper studies the effects of unexpected changes in US monetary policy on\ndigital asset returns. We use event study regressions and find that monetary\npolicy surprises negatively affect BTC and ETH, the two largest digital assets,\nbut do not significantly affect the rest of the market. Second, we use\nhigh-frequency price data to examine the effect of the FOMC statements release\nand Minutes release on the prices of the assets with the higher collateral\nusage on the Ethereum Blockchain Decentralized Finance (DeFi) ecosystem. The\nFOMC statement release strongly affects the volatility of digital asset\nreturns, while the effect of the Minutes release is weaker. The volatility\neffect strengthened after December 2021, when the Federal Reserve changed its\npolicy to fight inflation. We also show that some borrowing interest rates in\nthe Ethereum DeFi ecosystem are affected positively by unexpected changes in\nmonetary policy. In contrast, the debt outstanding and the total value locked\nare negatively affected. Finally, we utilize a local Ethereum Blockchain node\nto record the activity history of primary DeFi functions, such as depositing,\nborrowing, and liquidating, and study how these are influenced by the FOMC\nannouncements over time.\n"
    },
    {
        "paper_id": 2302.10485,
        "authors": "Peter Bank and Yan Dolinsky",
        "title": "Optimal investment with a noisy signal of future stock prices",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  We consider an investor who is dynamically informed about the future\nevolution of one of the independent Brownian motions driving a stock's price\nfluctuations. With linear temporary price impact the resulting optimal\ninvestment problem with exponential utility turns out to be not only well\nposed, but it even allows for a closed-form solution. We describe this solution\nand the resulting problem value for this stochastic control problem with\npartial observation by solving its convex-analytic dual problem.\n"
    },
    {
        "paper_id": 2302.1049,
        "authors": "Sam Dannels",
        "title": "Creating Disasters: Recession Forecasting with GAN-Generated Synthetic\n  Time Series Data",
        "comments": "18 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  A common problem when forecasting rare events, such as recessions, is limited\ndata availability. Recent advancements in deep learning and generative\nadversarial networks (GANs) make it possible to produce high-fidelity synthetic\ndata in large quantities. This paper uses a model called DoppelGANger, a GAN\ntailored to producing synthetic time series data, to generate synthetic\nTreasury yield time series and associated recession indicators. It is then\nshown that short-range forecasting performance for Treasury yields is improved\nfor models trained on synthetic data relative to models trained only on real\ndata. Finally, synthetic recession conditions are produced and used to train\nclassification models to predict the probability of a future recession. It is\nshown that training models on synthetic recessions can improve a model's\nability to predict future recessions over a model trained only on real data.\n"
    },
    {
        "paper_id": 2302.10562,
        "authors": "Nikita Belyak, Steven A. Gabriel, Nikolay Khabarov, Fabricio Oliveira",
        "title": "Renewable Energy Expansion under Taxes and Subsidies: A Transmission\n  Operator's Perspective",
        "comments": "33 pages, 20 Figures, 12 Tables",
        "journal-ref": null,
        "doi": "10.1016/j.jclepro.2024.141955",
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  This paper investigates the role of a transmission system operator within a\ncarbon footprint reduction strategy incorporating carbon taxes and renewable\nenergy generation subsidies in the decentralised energy market. This is\nachieved via an optimisation bi-level model in which a welfare-maximizing\ntransmission system operator makes investments in transmission lines at the\nupper level while considering power market dynamics at the lower level. To\naccount for the deregulated energy market structure, this paper assumes that\nthe generation companies at the lower level make capacity investments as\nprice-takers in perfect competition. Considering alternative transmission\ninfrastructure expansion budgets, carbon emission taxes and monetary incentives\nfor renewable energy generation capacity expansion, the impact of alternative\ncompositions of these factors is analysed against three output factors: the\nshare of renewable energy in the generation mix, total generation amount, and\nsocial welfare. The proposed modelling assessment is applied to an illustrative\nthree-node instance and a case study considering a simplified representation of\nthe energy system of the Nordic and Baltic countries. The results highlight\nthat, under certain circumstances, renewable energy generation subsidies may\nlead to an increase of renewable energy in the generation mix followed by a\nsimultaneous fall in the total generation amount. Nevertheless, when applied\ntogether, these three measures demonstrated a positive impact on all output\nfactors within Nordics' and Baltics' energy systems. The experiments\nadditionally suggest that considering the high value of the carbon tax does not\nhave an impact on the output factors while the composition of high values of\nrenewable energy generation subsidies and budget for transmission\ninfrastructure expansion has the strongest effect.\n"
    },
    {
        "paper_id": 2302.10573,
        "authors": "Andries Steenkamp",
        "title": "Convex scalarizations of the mean-variance-skewness-kurtosis problem in\n  portfolio selection",
        "comments": "36 pages, 12 figures, 1 table",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  We consider the multi-objective mean-variance-skewness-kurtosis (MVSK)\nproblem in portfolio selection, with and without shorting and leverage.\nAdditionally, we define a sparse variant of MVSK where feasible portfolios have\nsupports contained in a chosen class of sets. To find the MVSK problem's Pareto\nfront, we linearly scalarize the four objectives of MVSK into a scalar-valued\ndegree four polynomial $F_{\\lambda}$ depending on some hyper-parameter $\\lambda\n\\in \\Delta^4$. As one of our main results, we identify a set of\nhyper-parameters for which $F_{\\lambda}$ is convex over the probability simplex\n(or over the cube). By exploiting the convexity and neatness of the\nscalarization, we can compute part of the Pareto front. We compute an optimizer\nof the scalarization $F_{\\lambda}$ for each $\\lambda$ in a grid sampling of\n$\\Delta^4$. To see each optimizer's quality, we plot scaled portfolio objective\nvalues against hyper-parameters. Doing so, we reveal a sub-set of optimizers\nthat provide a superior trade-off among the four objectives in MVSK.\n"
    },
    {
        "paper_id": 2302.11017,
        "authors": "Thomas M\\\"obius, Mira Watermeyer, Oliver Grothe, Felix M\\\"usgens",
        "title": "Enhancing Energy System Models Using Better Load Forecasts",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-sa/4.0/",
        "abstract": "  Energy system models require a large amount of technical and economic data,\nthe quality of which significantly influences the reliability of the results.\nSome of the variables on the important data source ENTSO-E transparency\nplatform, such as transmission system operators' day-ahead load forecasts, are\nknown to be biased. These biases and high errors affect the quality of energy\nsystem models. We propose a simple time series model that does not require any\ninput variables other than the load forecast history to significantly improve\nthe transmission system operators' load forecast data on the ENTSO-E\ntransparency platform in real-time, i.e., we successively improve each incoming\ndata point. We further present an energy system model developed specifically\nfor the short-term day-ahead market. We show that the improved load data as\ninputs reduce pricing errors of the model, with strong reductions particularly\nin times when prices are high and the market is tight.\n"
    },
    {
        "paper_id": 2302.11212,
        "authors": "Chunbing Cai, Jordan Roulleau-Pasdeloup",
        "title": "Simple Analytics of the Government Investment Multiplier",
        "comments": "40 pages without Online Appendix, 6 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  What are the effects of investing in public infrastructure? We answer this\nquestion with a New Keynesian model. We recast the model as a Markov chain and\ndevelop a general solution method that nests existing ones inside/outside the\nzero lower bound as special cases. Our framework delivers a simple expression\nfor the contribution of public infrastructure. We show that it provides a\nunified framework to study the effects of public investment in three scenarios:\n$(i)$ normal times $(ii)$ short-lived liquidity trap $(iii)$ long-lived\nliquidity trap. We find that calibrations commonly used lead to multipliers\nthat diverge with the duration of the trap.\n"
    },
    {
        "paper_id": 2302.1125,
        "authors": "Henri Froese, Martin Hoefer, Lisa Wilhelmi",
        "title": "The Complexity of Debt Swapping",
        "comments": "36 pages, 10 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  A debt swap is an elementary edge swap in a directed, weighted graph, where\ntwo edges with the same weight swap their targets. Debt swaps are a natural and\nappealing operation in financial networks, in which nodes are banks and edges\nrepresent debt contracts. They can improve the clearing payments and the\nstability of these networks. However, their algorithmic properties are not\nwell-understood.\n  We analyze the computational complexity of debt swapping in networks with\nranking-based clearing. Our main interest lies in semi-positive swaps, in which\nno creditor strictly suffers and at least one strictly profits. These swaps\nlead to a Pareto-improvement in the entire network. We consider network\noptimization via sequences of $v$-improving debt swaps from which a given bank\n$v$ strictly profits. We show that every sequence of semi-positive\n$v$-improving swaps has polynomial length. In contrast, for arbitrary\n$v$-improving swaps, the problem of reaching a network configuration that\nallows no further swaps is PLS-complete. We identify cases in which short\nsequences of semi-positive swaps exist even without the $v$-improving property.\n  In addition, we study reachability problems, i.e., deciding if a sequence of\nswaps exists between given initial and final networks. We identify a\npolynomial-time algorithm for arbitrary swaps, show NP-hardness for\nsemi-positive swaps and even PSPACE-completeness for $v$-improving swaps or\nswaps that only maintain a lower bound on the assets of a given bank $v$. A\nvariety of our results can be extended to arbitrary monotone clearing.\n"
    },
    {
        "paper_id": 2302.11371,
        "authors": "David Vidal-Tom\\'as, Antonio Briola, Tomaso Aste",
        "title": "FTX's downfall and Binance's consolidation: The fragility of centralised\n  digital finance",
        "comments": "23 pages, 10 figures, 2 tables",
        "journal-ref": null,
        "doi": "10.1016/j.physa.2023.129044",
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  This paper investigates the causes of the FTX digital currency exchange's\nfailure in November 2022. We identify the collapse of the Terra-Luna ecosystem\nas the pivotal event that triggered a significant decrease in the exchange's\nliquidity. Analysing on-chain data, we report that FTX heavily relied on\nleveraging and misusing its native token, FTT, and we show how this behaviour\nexacerbated the company's fragile financial situation. To gain further insights\ninto the downfall, we study evolutionary dependency structures of 199\ncryptocurrencies on an hourly basis, and we investigate public trades at the\ntime of the events. Results suggest that the collapse was actively accelerated\nby Binance tweets causing a systemic reaction in the cryptocurrency market.\nFinally, identifying the actors who mostly benefited from the FTX's collapse\nand highlighting a generalised trend toward centralisation in the crypto space,\nwe emphasise the importance of genuinely decentralised finance for a\ntransparent, future digital economy.\n"
    },
    {
        "paper_id": 2302.11376,
        "authors": "Bj\\\"orn Alecke and Timo Mitze",
        "title": "Institutional reforms and the employment effects of spatially targeted\n  investment grants: The case of Germany's GRW",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  Spatially targeted investment grant schemes are a common tool to support\nfirms in lagging regions. We exploit exogenous variations in Germany's main\nregional policy instrument (GRW) arriving from institutional reforms to analyse\nlocal employment effects of investment grants. Findings for reduced-form and IV\nregressions point to a significant policy channel running from higher funding\nrates to increased firm-level investments and newly created jobs. When we\ncontrast effects for regions with high but declining funding rates to those\nwith low but rising rates, we find that GRW reforms led to diminishing\nemployment increases. Especially small firms responded to changing funding\nconditions.\n"
    },
    {
        "paper_id": 2302.11423,
        "authors": "Li Lin and Didier Sornette",
        "title": "The inverse Cox-Ingersoll-Ross process for parsimonious financial price\n  modeling",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  We propose a formulation to construct new classes of financial price\nprocesses based on the insight that the key variable driving prices $P$ is the\nearning-over-price ratio $\\gamma \\simeq 1/P$, which we refer to as the earning\nyield and is analogous to the yield-to-maturity of an equivalent perpetual\nbond. This modeling strategy is illustrated with the choice for real-time\n$\\gamma$ in the form of the Cox-Ingersoll-Ross (CIR) process, which allows us\nto derive analytically many stylised facts of financial prices and returns,\nsuch as the power law distribution of returns, transient super-exponential\nbubble behavior, and the fat-tailed distribution of prices before bubbles\nburst. Our model sheds new light on rationalizing the excess volatility and the\nequity premium puzzles. The model is calibrated to five well-known historical\nbubbles in the US and China stock markets via a quasi-maximum likelihood method\nwith the L-BFGS-B optimization algorithm. Using $\\phi$-divergence statistics\nadapted to models prescribed in terms of stochastic differential equations, we\nshow the superiority of the CIR process for $\\gamma_t$ against three\nalternative models.\n"
    },
    {
        "paper_id": 2302.11436,
        "authors": "Mckay Jensen, Nicholas Emery-Xu, Robert Trager",
        "title": "Industrial Policy for Advanced AI: Compute Pricing and the Safety Tax",
        "comments": "32 pages, 7 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Using a model in which agents compete to develop a potentially dangerous new\ntechnology (AI), we study how changes in the pricing of factors of production\n(computational resources) affect agents' strategies, particularly their\nspending on safety meant to reduce the danger from the new technology. In the\nmodel, agents split spending between safety and performance, with safety\ndetermining the probability of a ``disaster\" outcome, and performance\ndetermining the agents' competitiveness relative to their peers. For given\nparameterizations, we determine the theoretically optimal spending strategies\nby numerically computing Nash equilibria. Using this approach we find that (1)\nin symmetric scenarios, compute price increases are safety-promoting if and\nonly if the production of performance scales faster than the production of\nsafety; (2) the probability of a disaster can be made arbitrarily low by\nproviding a sufficiently large subsidy to a single agent; (3) when agents\ndiffer in productivity, providing a subsidy to the more productive agent is\noften better for aggregate safety than providing the same subsidy to other\nagent(s) (with some qualifications, which we discuss); (4) when one agent is\nmuch more safety-conscious, in the sense of believing that safety is more\ndifficult to achieve, relative to his competitors, subsidizing that agent is\ntypically better for aggregate safety than subsidizing its competitors;\nhowever, subsidizing an agent that is only somewhat more safety-conscious often\ndecreases safety. Thus, although subsidizing a much more safety-conscious, or\nproductive, agent often improves safety as intuition suggests, subsidizing a\nsomewhat more safety-conscious or productive agent can often be harmful.\n"
    },
    {
        "paper_id": 2302.11451,
        "authors": "Christian Diem and Andr\\'as Borsos and Tobias Reisch and J\\'anos\n  Kert\\'esz and Stefan Thurner",
        "title": "Estimating the loss of economic predictability from aggregating\n  firm-level production networks",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  To estimate the reaction of economies to political interventions or external\ndisturbances, input-output (IO) tables -- constructed by aggregating data into\nindustrial sectors -- are extensively used. However, economic growth,\nrobustness, and resilience crucially depend on the detailed structure of\nnon-aggregated firm-level production networks (FPNs). Due to non-availability\nof data little is known about how much aggregated sector-based and detailed\nfirm-level-based model-predictions differ. Using a nearly complete nationwide\nFPN, containing 243,399 Hungarian firms with 1,104,141 supplier-buyer-relations\nwe self-consistently compare production losses on the aggregated industry-level\nproduction network (IPN) and the granular FPN. For this we model the\npropagation of shocks of the same size on both, the IPN and FPN, where the\nlatter captures relevant heterogeneities within industries. In a COVID-19\ninspired scenario we model the shock based on detailed firm-level data during\nthe early pandemic. We find that using IPNs instead of FPNs leads to errors up\nto 37% in the estimation of economic losses, demonstrating a natural limitation\nof industry-level IO-models in predicting economic outcomes. We ascribe the\nlarge discrepancy to the significant heterogeneity of firms within industries:\nwe find that firms within one sector only sell 23.5% to and buy 19.3% from the\nsame industries on average, emphasizing the strong limitations of industrial\nsectors for representing the firms they include. Similar error-levels are\nexpected when estimating economic growth, CO2 emissions, and the impact of\npolicy interventions with industry-level IO models. Granular data is key for\nreasonable predictions of dynamical economic systems.\n"
    },
    {
        "paper_id": 2302.11643,
        "authors": "Soheil Ghili, Russ Yoon",
        "title": "An Empirical Analysis of Optimal Nonlinear Pricing in\n  Business-to-Business Markets",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In continuous-choice settings, consumers decide not only on whether to\npurchase a product, but also on how much to purchase. Thus, firms optimize a\nfull price schedule rather than a single price point. This paper provides a\nmethodology to empirically estimate the optimal schedule under\nmulti-dimensional consumer heterogeneity with a focus on B2B applications. We\napply our method to novel data from an educational-services firm that contains\npurchase-size information not only for deals that materialized, but also for\npotential deals that eventually failed. We show that this data, combined with\nidentifying assumptions, helps infer how price sensitivity varies with\n\"customer size\". Using our estimated model, we show that the optimal\nsecond-degree price discrimination (i.e., optimal nonlinear tariff) improves\nthe firm's profit upon linear pricing by at least 8.2%. That said, this\nsecond-degree price discrimination scheme only recovers 7.1% of the gap between\nthe profitability of linear pricing and that of infeasible first degree price\ndiscrimination. We also conduct several further simulation analyses (i)\nempirically quantifying the magnitude by which incentive-compatibility\nconstraints impact the optimal pricing and profits, (ii) comparing the role of\ndemand- v.s. cost-side factors in shaping the optimal price schedule, and (iii)\nstudying the implications of fixed fees for the optimal contract and\nprofitability.\n"
    },
    {
        "paper_id": 2302.11652,
        "authors": "Jason Milionis, Ciamac C. Moallemi, Tim Roughgarden",
        "title": "Complexity-Approximation Trade-offs in Exchange Mechanisms: AMMs vs.\n  LOBs",
        "comments": "16 pages, 1 figure, accepted for publication in FC'23",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper presents a general framework for the design and analysis of\nexchange mechanisms between two assets that unifies and enables comparisons\nbetween the two dominant paradigms for exchange, constant function market\nmarkers (CFMMs) and limit order books (LOBs). In our framework, each liquidity\nprovider (LP) submits to the exchange a downward-sloping demand curve,\nspecifying the quantity of the risky asset it wishes to hold at each price; the\nexchange buys and sells the risky asset so as to satisfy the aggregate\nsubmitted demand. In general, such a mechanism is budget-balanced and enables\nprice discovery. Different exchange mechanisms correspond to different\nrestrictions on the set of acceptable demand curves. The primary goal of this\npaper is to formalize an approximation-complexity trade-off that pervades the\ndesign of exchange mechanisms. For example, CFMMs give up expressiveness in\nfavor of simplicity: the aggregate demand curve of the LPs can be described\nusing constant space, but most demand curves cannot be well approximated by any\nfunction in the corresponding single-dimensional family. LOBs, intuitively,\nmake the opposite trade-off: any downward-slowing demand curve can be well\napproximated by a collection of limit orders, but the space needed to describe\nthe state of a LOB can be large. This paper introduces a general measure of\n{\\em exchange complexity}, defined by the minimal set of basis functions that\ngenerate, through their conical hull, all of the demand functions allowed by an\nexchange. With this complexity measure in place, we investigate the design of\n{\\em optimally expressive} exchange mechanisms, meaning the lowest complexity\nmechanisms that allow for arbitrary downward-sloping demand curves to be well\napproximated. As a case study, we interpret the complexity-approximation\ntrade-offs in the widely-used Uniswap v3 AMM through the lens of our framework.\n"
    },
    {
        "paper_id": 2302.11675,
        "authors": "Bijesh Mishra",
        "title": "Economics and human dimension of active managment of forest grassland\n  ecotone in south-central USA under changing climate",
        "comments": "PhD thesis",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
        "abstract": "  The south central ecoregion was a mosiac ecoregion of forest and grassland\ncontinnum which is transiting towards closed canopy forests and losing\necosystem benefits. We studied role of active management, its economic benefit,\nand landonwers atttitdue and behavior towards restoring ecosystem services in\nthis region. We further studed how the economic benefit varies in this region\nwith the change in rainfall.\n"
    },
    {
        "paper_id": 2302.11701,
        "authors": "Jean-Gabriel Lauzier and Liyuan Lin and Ruodu Wang",
        "title": "Pairwise counter-monotonicity",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We systematically study pairwise counter-monotonicity, an extremal notion of\nnegative dependence. A stochastic representation and an invariance property are\nestablished for this dependence structure. We show that pairwise\ncounter-monotonicity implies negative association, and it is equivalent to\njoint mix dependence if both are possible for the same marginal distributions.\nWe find an intimate connection between pairwise counter-monotonicity and risk\nsharing problems for quantile agents. This result highlights the importance of\nthis extremal negative dependence structure in optimal allocations for agents\nwho are not risk averse in the classic sense.\n"
    },
    {
        "paper_id": 2302.11729,
        "authors": "David Ardia, Keven Bluteau, Gabriel Lortie-Cloutier, Thien-Duy Tran",
        "title": "Factor Exposure Heterogeneity in Green and Brown Stocks",
        "comments": "Forthcoming in Finance Research Letters. arXiv admin note:\n  substantial text overlap with arXiv:2201.05709",
        "journal-ref": null,
        "doi": "10.1016/j.frl.2023.103900",
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  Using the peer-exposure ratio, we explore the factor exposure heterogeneity\nin green and brown stocks. By looking at peer groups of S&P 500 index firms\nover 2014-2020 based on their greenhouse gas emission levels, we find that, on\naverage, green stocks exhibit less factor exposure heterogeneity than brown\nstocks for most of the traditional equity factors but the value factor. Hence,\ninvestment managers shifting their investments from brown stocks to green\nstocks have less room to differentiate themselves regarding their factor\nexposures. Finally, we find that factor exposure heterogeneity has increased\nfor green stocks compared to earlier periods.\n"
    },
    {
        "paper_id": 2302.11822,
        "authors": "Kyungsub Lee",
        "title": "Multi-kernel property in high-frequency price dynamics under Hawkes\n  model",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  This study investigates and uses multi-kernel Hawkes models to describe a\nhigh-frequency mid-price process. Each kernel represents a different responsive\nspeed of market participants. Using the conditional Hessian, we examine whether\nthe numerical optimizer effectively finds the global maximum of the\nlog-likelihood function under complicated modeling. Empirical studies that use\nstock prices in the US equity market show the existence of multi-kernels\nclassified as ultra-high-frequency (UHF), very-high-frequency (VHF), and\nhigh-frequency (HF). We estimate the conditional expectations of arrival times\nand the degree of contribution to the high-frequency activities for each\nkernel.\n"
    },
    {
        "paper_id": 2302.11835,
        "authors": "Aldo Glielmo, Marco Favorito, Debmallya Chanda and Domenico Delli\n  Gatti",
        "title": "Reinforcement Learning for Combining Search Methods in the Calibration\n  of Economic ABMs",
        "comments": "9 pages and 5 figures, presented at the AAAI bridge program 'AI for\n  Financial Institutions' (https://aaai23.bankit.art/), at the ICLR bridge\n  program 'AI4ABM' (https://ai4abm.org/workshop_iclr2023/) and at ICAIF '23\n  (https://ai-finance.org/). Proceedings of the Fourth ACM International\n  Conference on AI in Finance, (ICAIF 23), Association for Computing Machinery,\n  New York, NY, USA",
        "journal-ref": null,
        "doi": "10.1145/3604237.3626889",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Calibrating agent-based models (ABMs) in economics and finance typically\ninvolves a derivative-free search in a very large parameter space. In this\nwork, we benchmark a number of search methods in the calibration of a\nwell-known macroeconomic ABM on real data, and further assess the performance\nof \"mixed strategies\" made by combining different methods. We find that methods\nbased on random-forest surrogates are particularly efficient, and that\ncombining search methods generally increases performance since the biases of\nany single method are mitigated. Moving from these observations, we propose a\nreinforcement learning (RL) scheme to automatically select and combine search\nmethods on-the-fly during a calibration run. The RL agent keeps exploiting a\nspecific method only as long as this keeps performing well, but explores new\nstrategies when the specific method reaches a performance plateau. The\nresulting RL search scheme outperforms any other method or method combination\ntested, and does not rely on any prior information or trial and error\nprocedure.\n"
    },
    {
        "paper_id": 2302.11942,
        "authors": "Niccol\\`o Bardoscia, Alessandro Nodari",
        "title": "Liquidity Providers Greeks and Impermanent Gain",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-sa/4.0/",
        "abstract": "  In traditional finance, the Black & Scholes model has guided almost 50 years\nof derivatives pricing, defining a standard to model any volatility-based\nproduct. With the rise of Decentralized Finance (DeFi) and constant product\nAutomated Market Makers (AMMs), Liquidity Providers (LPs) are playing an\nincreasingly important role in markets functioning, but, as the recent bear\nmarket highlighted, they are exposed to important risks such as Impermanent\nLoss (IL). In this paper, we tailor the formulas introduced by Black & Scholes\nto DeFi, proposing a method to calculate the greeks of an LP. We also introduce\nImpermanent Gain, a product that LPs can use to hedge their position and\ntraders can use to bet on a rise in volatility and benefit from large market\nmoves.\n"
    },
    {
        "paper_id": 2302.12118,
        "authors": "Yuan Gao, Biao Jiang, Jietong Zhou",
        "title": "Financial Distress Prediction For Small And Medium Enterprises Using\n  Machine Learning Techniques",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Financial Distress Prediction plays a crucial role in the economy by\naccurately forecasting the number and probability of failing structures,\nproviding insight into the growth and stability of a country's economy.\nHowever, predicting financial distress for Small and Medium Enterprises is\nchallenging due to their inherent ambiguity, leading to increased funding costs\nand decreased chances of receiving funds. While several strategies have been\ndeveloped for effective FCP, their implementation, accuracy, and data security\nfall short of practical applications. Additionally, many of these strategies\nperform well for a portion of the dataset but are not adaptable to various\ndatasets. As a result, there is a need to develop a productive prediction model\nfor better order execution and adaptability to different datasets. In this\nreview, we propose a feature selection algorithm for FCP based on element\ncredits and data source collection. Current financial distress prediction\nmodels rely mainly on financial statements and disregard the timeliness of\norganization tests. Therefore, we propose a corporate FCP model that better\naligns with industry practice and incorporates the gathering of thin-head\ncomponent analysis of financial data, corporate governance qualities, and\nmarket exchange data with a Relevant Vector Machine. Experimental results\ndemonstrate that this strategy can improve the forecast efficiency of financial\ndistress with fewer characteristic factors.\n"
    },
    {
        "paper_id": 2302.12167,
        "authors": "Ren\\'e A\\\"id, Annika Kemper, Nizar Touzi",
        "title": "A Principal-Agent Framework for Optimal Incentives in Renewable\n  Investments",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We investigate the optimal regulation of energy production reflecting the\nlong-term goals of the Paris Climate Agreement. We analyze the optimal\nregulatory incentives to foster the development of non-emissive electricity\ngeneration when the demand for power is served either by a monopoly or by two\ncompeting agents. The regulator wishes to encourage green investments to limit\ncarbon emissions, while simultaneously reducing intermittency of the total\nenergy production. We find that the regulation of a competitive market is more\nefficient than the one of the monopoly as measured with the certainty\nequivalent of the Principal's value function. This higher efficiency is\nachieved thanks to a higher degree of freedom of the incentive mechanisms which\ninvolves cross-subsidies between firms. A numerical study quantifies the impact\nof the designed second-best contract in both market structures compared to the\nbusiness-as-usual scenario.\n  In addition, we expand the monopolistic and competitive setup to a more\ngeneral class of tractable Principal-Multi-Agent incentives problems when both\nthe drift and the volatility of a multi-dimensional diffusion process can be\ncontrolled by the Agents. We follow the resolution methodology of Cvitani\\'c et\nal. (2018) in an extended linear quadratic setting with exponential utilities\nand a multi-dimensional state process of Ornstein-Uhlenbeck type. We provide\nclosed-form expression of the second-best contracts. In particular, we show\nthat they are in rebate form involving time-dependent prices of each\nstate-variable.\n"
    },
    {
        "paper_id": 2302.12225,
        "authors": "Fatemeh Nazari, Mohamadhossein Noruzoliaee, Abolfazl Mohammadian",
        "title": "Behavioral acceptance of automated vehicles: The roles of perceived\n  safety concern and current travel behavior",
        "comments": "The initial version with the primary results is presented at\n  Transportation Research Board 98th Annual Meeting Transportation Research\n  Board",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  With the prospect of next-generation automated mobility ecosystem, the\nrealization of the contended traffic efficiency and safety benefits are\ncontingent upon the demand landscape for automated vehicles (AVs). Focusing on\nthe public acceptance behavior of AVs, this empirical study addresses two gaps\nin the plethora of travel behavior research on identifying the potential\ndeterminants thereof. First, a clear behavioral understanding is lacking as to\nthe perceived concern about AV safety and the consequent effect on AV\nacceptance behavior. Second, how people appraise the benefits of enhanced\nautomated mobility to meet their current (pre-AV era) travel behavior and\nneeds, along with the resulting impacts on AV acceptance and perceived safety\nconcern, remain equivocal. To fill these gaps, a recursive trivariate\neconometric model with ordinal-continuous outcomes is employed, which jointly\nestimates AV acceptance (ordinal), perceived AV safety concern (ordinal), and\ncurrent annual vehicle-miles traveled (VMT) approximating the current travel\nbehavior (continuous). Importantly, the co-estimation of the three endogenous\noutcomes allows to capture the true interdependencies among them, net of any\ncorrelated unobserved factors that can have common impacts on these outcomes.\nBesides the classical socio-economic characteristics, the outcome variables are\nfurther explained by the latent preferences for vehicle attributes (including\nvehicle cost, reliability, performance, and refueling) and for existing shared\nmobility systems. The model estimation results on a stated preference survey in\nthe State of California provide insights into proactive policies that can\npopularize AVs through gearing towards the most affected population groups,\nparticularly vehicle cost-conscious, safety-concerned, and lower-VMT (such as\ntravel-restrictive) individuals.\n"
    },
    {
        "paper_id": 2302.12319,
        "authors": "Arthur A. B. Pessa, Matjaz Perc, Haroldo V. Ribeiro",
        "title": "Age and market capitalization drive large price variations of\n  cryptocurrencies",
        "comments": "16 pages, 4 figures, supplementary information; accepted for\n  publication in Scientific Reports",
        "journal-ref": "Sci. Rep. 13, 3351 (2023)",
        "doi": "10.1038/s41598-023-30431-3",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Cryptocurrencies are considered the latest innovation in finance with\nconsiderable impact across social, technological, and economic dimensions. This\nnew class of financial assets has also motivated a myriad of scientific\ninvestigations focused on understanding their statistical properties, such as\nthe distribution of price returns. However, research so far has only considered\nBitcoin or at most a few cryptocurrencies, whilst ignoring that price returns\nmight depend on cryptocurrency age or be influenced by market capitalization.\nHere, we therefore present a comprehensive investigation of large price\nvariations for more than seven thousand digital currencies and explore whether\nprice returns change with the coming-of-age and growth of the cryptocurrency\nmarket. We find that tail distributions of price returns follow power-law\nfunctions over the entire history of the considered cryptocurrency portfolio,\nwith typical exponents implying the absence of characteristic scales for price\nvariations in about half of them. Moreover, these tail distributions are\nasymmetric as positive returns more often display smaller exponents, indicating\nthat large positive price variations are more likely than negative ones. Our\nresults further reveal that changes in the tail exponents are very often\nsimultaneously related to cryptocurrency age and market capitalization or only\nto age, with only a minority of cryptoassets being affected just by market\ncapitalization or neither of the two quantities. Lastly, we find that the\ntrends in power-law exponents usually point to mixed directions, and that large\nprice variations are likely to become less frequent only in about 28\\% of the\ncryptocurrencies as they age and grow in market capitalization.\n"
    },
    {
        "paper_id": 2302.12439,
        "authors": "Ivan Guo, Nicolas Langren\\'e and Jiahao Wu",
        "title": "Simultaneous upper and lower bounds of American option prices with\n  hedging via neural networks",
        "comments": "26 pages including references and the appendix, 8 figures, 7 tables",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper, we introduce two methods to solve the American-style option\npricing problem and its dual form at the same time using neural networks.\nWithout applying nested Monte Carlo, the first method uses a series of neural\nnetworks to simultaneously compute both the lower and upper bounds of the\noption price, and the second one accomplishes the same goal with one global\nnetwork. The avoidance of extra simulations and the use of neural networks\nsignificantly reduce the computational complexity and allow us to price\nBermudan options with frequent exercise opportunities in high dimensions, as\nillustrated by the provided numerical experiments. As a by-product, these\nmethods also derive a hedging strategy for the option, which can also be used\nas a control variate for variance reduction.\n"
    },
    {
        "paper_id": 2302.125,
        "authors": "Yen-Jui Chang, Wei-Ting Wang, Hao-Yuan Chen, Shih-Wei Liao, Ching-Ray\n  Chang",
        "title": "Preparing random state for quantum financing with quantum walks",
        "comments": "11 pages, 7 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  In recent years, there has been an emerging trend of combining two\ninnovations in computer science and physics to achieve better computation\ncapability. Exploring the potential of quantum computation to achieve highly\nefficient performance in various tasks is a vital development in engineering\nand a valuable question in sciences, as it has a significant potential to\nprovide exponential speedups for technologically complex problems that are\nspecifically advantageous to quantum computers. However, one key issue in\nunleashing this potential is constructing an efficient approach to load\nclassical data into quantum states that can be executed by quantum computers or\nquantum simulators on classical hardware. Therefore, the split-step quantum\nwalks (SSQW) algorithm was proposed to address this limitation. We facilitate\nSSQW to design parameterized quantum circuits (PQC) that can generate\nprobability distributions and optimize the parameters to achieve the desired\ndistribution using a variational solver. A practical example of implementing\nSSQW using Qiskit has been released as open-source software. Showing its\npotential as a promising method for generating desired probability amplitude\ndistributions highlights the potential application of SSQW in option pricing\nthrough quantum simulation.\n"
    },
    {
        "paper_id": 2302.12612,
        "authors": "Camilla Damian and R\\\"udiger Frey",
        "title": "Detecting Rough Volatility: A Filtering Approach",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper, we focus on the estimation of historical volatility of asset\nprices from high-frequency data. Stochastic volatility models pose a major\nstatistical challenge: since in reality historical volatility is not\nobservable, its current level and, possibly, the parameters governing its\ndynamics have to be estimated from the observable time series of asset prices.\nTo complicate matters further, recent research has analyzed the rough behavior\nof volatility time series to challenge the common assumption that the\nvolatility process is a Brownian semimartingale. In order to tackle the arising\ninferential task efficiently in this setting, we use the fact that a fractional\nBrownian motion can be represented as a superposition of Markovian\nsemimartingales (Ornstein-Uhlenbeck processes) and we solve the filtering (and\nparameter estimation) problem by resorting to more standard techniques, such as\nparticle methods.\n"
    },
    {
        "paper_id": 2302.12999,
        "authors": "Shweta Bahl, Ajay Sharma",
        "title": "Informality, Education-Occupation Mismatch, and Wages: Evidence from\n  India",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  This article examines the intertwining relationship between informality and\neducation-occupation mismatch and the consequent impact on wages. In\nparticular, we discuss two issues: first, the relative importance of\ninformality and education-occupation mismatch in determining wages, and second,\nthe relevance of EOM for formal and informal workers. The analysis reveals that\nalthough both informality and EOM are significant determinants of wages, the\nformer is more crucial for a developing country like India. Further, we find\nthat EOM is one of the crucial determinants of wages for formal workers, but it\nis not critical for informal workers. The study highlights the need for\nconsidering the bifurcation of formal-informal workers to understand the\ncomplete dynamics of EOM, especially for developing countries where informality\nis predominant.\n"
    },
    {
        "paper_id": 2302.1307,
        "authors": "M\\\"ucahit Ayg\\\"un, Fabio Bellini, Roger J. A. Laeven",
        "title": "Elicitability of Return Risk Measures",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Informally, a risk measure is said to be elicitable if there exists a\nsuitable scoring function such that minimizing its expected value recovers the\nrisk measure. In this paper, we analyze the elicitability properties of the\nclass of return risk measures (i.e., normalized, monotone and positively\nhomogeneous risk measures). First, we provide dual representation results for\nconvex and geometrically convex return risk measures. Next, we establish new\naxiomatic characterizations of Orlicz premia (i.e., Luxemburg norms). More\nspecifically, we prove, under different sets of conditions, that Orlicz premia\nnaturally arise as the only elicitable return risk measures. Finally, we\nprovide a general family of strictly consistent scoring functions for Orlicz\npremia, a myriad of specific examples and a mixture representation suitable for\nconstructing Murphy diagrams.\n"
    },
    {
        "paper_id": 2302.13076,
        "authors": "Martin Ho, Henry CW Price, Tim S Evans, Eoin O'Sullivan",
        "title": "Order in Innovation",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Is calendar time the true clock of innovation? By combining complexity\nscience with innovation economics and using vaccine datasets containing over\nthree million citations and eight regulatory authorisations, we discover that\ncalendar time and network order describe innovation progress at varying\naccuracy. First, we present a method to establish a mathematical link between\ntechnological evolution and complex networks. The result is a path of events\nthat narrates innovation bottlenecks. Next, we quantify the position and\nproximity of documents to these innovation paths and find that research, by and\nlarge, proceed from basic research, applied research, development, to\ncommercialisation. By extension, we are able to causally quantify the\nparticipation of innovation funders. When it comes to vaccine innovation,\ndiffusion-oriented entities are preoccupied with basic, later-stage research;\nbiopharmaceuticals tend to participate in applied development activities and\nclinical trials at the later-stage; while mission-oriented entities tend to\ninitiate early-stage research. Future innovation programs and funding\nallocations would benefit from better understanding innovation orders.\n"
    },
    {
        "paper_id": 2302.13121,
        "authors": "Olga Kalantzi, Dimitrios Tsiotas, Serafeim Polyzos",
        "title": "The Contribution of Tourism in National Economies: Evidence of Greece",
        "comments": null,
        "journal-ref": "European Journal of Business and Social Sciences, Vol. 5, No. 10,\n  January 2017, pp.41-64",
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Greece constitutes a coastal country with a lot of geomorphologic, climatic,\ncultural and historic peculiarities favoring the development of many aspects of\ntourism. Within this framework, this article examines what are the effects of\ntourism in Greece and how determinative these effects are, by applying a\nmacroscopic analysis on empirical data for the estimation of the contribution\nof tourism in the Greek Economy. The available data regard records of the\nBalance of Payments in Greece and of the major components of the Balance of the\nInvisible Revenues, where a measurable aspect of tourism, the Travel or Tourism\nExchange, is included. At the time period of the available data (2000-2012) two\nevents of the recent Greek history are distinguished as the most significant\n(the Olympic Games in the year 2004 and the economic crisis initiated in the\nyear 2009) and their impact on the diachronic evolution in the tourism is\ndiscussed. Under an overall assessment, the analysis illustrated that tourism\nis a sector of the Greek economy, which is described by a significant\nresilience, but it seems that it has not yet been submitted to an effective\ndevelopmental plan exploiting the endogenous tourism dynamics of the country,\nsuggesting currently a promising investment of low risk for the economic growth\nof country and the exit of the economic crisis\n"
    },
    {
        "paper_id": 2302.13245,
        "authors": "Naresh Kumar Devulapally and Tulasi Narendra Das Tripurana",
        "title": "Physical Momentum in the Indian Stock Market",
        "comments": "15 pages, 4 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Our study focuses on determining the presence of abnormal returns for\nphysical momentum portfolios in the context of the Indian market. The physical\nmomentum portfolios, comprising stocks from the NSE 500, are constructed for\nthe daily, weekly, monthly, and yearly timescales. In the aforementioned\ntimescales, we empirically evaluate the historical returns and varied risk\nprofiles of these portfolios for the years 2014-2021. It has been observed that\nthe best-performing physical momentum portfolios from each of the four\ntimescales achieved higher returns and better risk measures when compared to\nthe benchmark NIFTY 50 portfolio. We further find that the high-frequency daily\ntime scale exhibits the strongest reversal in the physical momentum effect,\nwherein the portfolio yielded a 16-fold profit over the initial investment.\n"
    },
    {
        "paper_id": 2302.13426,
        "authors": "Christian Keller, Michael C. Tseng",
        "title": "Arrow-Debreu Meets Kyle: Price Discovery for Derivatives",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  We analyze price discovery in a model where an agent has arbitrary private\ninformation regarding state probabilities and trades state-contingent claims.\nOur model unifies the elements of Arrow and Debreu (1954) and Kyle (1985). In\nthe equivalent options formulation, the informed agent has arbitrary\ninformation regarding an underlying asset's payoff distribution and trades\noption portfolios. We characterize the informed demand, the price impact, and\nthe information efficiency of prices. Our informed demand formula explains\nthose option trading strategies used in practice -- e.g., for volatility\ntrading.\n"
    },
    {
        "paper_id": 2302.13427,
        "authors": "Jingfang Zhang and Emir Malikov",
        "title": "Detecting Learning by Exporting and from Exporters",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Existing literature at the nexus of firm productivity and export behavior\nmostly focuses on \"learning by exporting,\" whereby firms can improve their\nperformance by engaging in exports. Whereas, the secondary channel of learning\nvia cross-firm spillovers from exporting peers, or \"learning from exporters,\"\nhas largely been neglected. Omitting this important mechanism, which can\nbenefit both exporters and non-exporters, may provide an incomplete assessment\nof the total productivity benefits of exporting. In this paper, we develop a\nunified empirical framework for productivity measurement that explicitly\naccommodates both channels. To do this, we formalize the evolution of firm\nproductivity as an export-controlled process, allowing future productivity to\nbe affected by both the firm's own export behavior as well as export behavior\nof spatially proximate, same-industry peers. This facilitates a simultaneous,\n\"internally consistent\" identification of firm productivity and the\ncorresponding effects of exporting. We apply our methodology to a panel of\nmanufacturing plants in Chile in 1995-2007 and find significant evidence in\nsupport of both direct and spillover effects of exporting that substantially\nboost the productivity of domestic firms.\n"
    },
    {
        "paper_id": 2302.13429,
        "authors": "Emir Malikov, Shunan Zhao, Jingfang Zhang",
        "title": "A System Approach to Structural Identification of Production Functions\n  with Multi-Dimensional Productivity",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  There is growing empirical evidence that firm heterogeneity is\ntechnologically non-neutral. This paper extends Gandhi et al.'s (2020) proxy\nvariable framework for structurally identifying production functions to a more\ngeneral case when latent firm productivity is multi-dimensional, with both\nfactor-neutral and (biased) factor-augmenting components. Unlike alternative\nmethodologies, our model can be identified under weaker data requirements,\nnotably, without relying on the typically unavailable cross-sectional variation\nin input prices for instrumentation. When markets are perfectly competitive, we\nachieve point identification by leveraging the information contained in static\noptimality conditions, effectively adopting a system-of-equations approach. We\nalso show how one can partially identify the non-neutral production technology\nin the traditional proxy variable framework when firms have market power.\n"
    },
    {
        "paper_id": 2302.1343,
        "authors": "Emir Malikov, Jingfang Zhang, Shunan Zhao, Subal C. Kumbhakar",
        "title": "Accounting for Cross-Location Technological Heterogeneity in the\n  Measurement of Operations Efficiency and Productivity",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Motivated by the long-standing interest in understanding the role of location\nfor firm performance, this paper provides a semiparametric methodology to\naccommodate locational heterogeneity in production analysis. Our approach is\nnovel in that we explicitly model spatial variation in parameters in the\nproduction-function estimation. We accomplish this by allowing both the\ninput-elasticity and productivity parameters to be unknown functions of the\nfirm's geographic location and estimate them via local kernel methods. This\nallows the production technology to vary across space, thereby accommodating\nneighborhood influences on firm production. In doing so, we are also able to\nexamine the role of cross-location differences in explaining the variation in\noperational productivity among firms. Our model is superior to the alternative\nspatial production-function formulations because it (i) explicitly estimates\nthe cross-locational variation in production functions, (ii) is readily\nreconcilable with the conventional production axioms and, more importantly,\n(iii) can be identified from the data by building on the popular proxy-variable\nmethods, which we extend to incorporate locational heterogeneity. Using our\nmethodology, we study China's chemicals manufacturing industry and find that\ndifferences in technology (as opposed to in idiosyncratic firm heterogeneity)\nare the main source of the cross-location differential in total productivity in\nthis industry.\n"
    },
    {
        "paper_id": 2302.13646,
        "authors": "Jan Rosenzweig",
        "title": "A Tale of Tail Covariances (and Diversified Tails)",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  This paper deals with tail diversification in financial time series through\nthe concept of statistical independence by way of differential entropy and\nmutual information. By using moments as contrast functions to isolate the tails\nof the return distributions, we recover the tail covariance matrix, a specific\ntwo-dimensional slice of the mixed moment tensor, as a key driver of tail\ndiversification.\n  We further explore the links between the moment contrast approach and the\noriginal entropy formulation, and show an example of in- and out-of-sample\ndiversification on a broad stock universe.\n"
    },
    {
        "paper_id": 2302.13695,
        "authors": "Jian-An Li and Li Wang and Wen-Jie Xie and Wei-Xing Zhou",
        "title": "Impact of shocks to economies on the efficiency and robustness of the\n  international pesticide trade networks",
        "comments": "12 pages, 5 figures",
        "journal-ref": "Eur. Phys. J. B 96, 25 (2023)",
        "doi": "10.1140/epjb/s10051-023-00493-3",
        "license": "http://creativecommons.org/publicdomain/zero/1.0/",
        "abstract": "  Pesticides are important agricultural inputs to increase agricultural\nproductivity and improve food security. The availability of pesticides is\npartially achieved through international trade. However, economies involved in\nthe international trade of pesticides are impacted by internal and external\nshocks from time to time, which influence the redistribution efficiency of\npesticides all over the world. In this work, we adopt simulations to quantify\nthe efficiency and robustness of the international pesticide trade networks\nunder shocks to economies. Shocks are simulated based on nine node metrics, and\nthree strategies are utilized based on descending, random, and ascending node\nremoval. It is found that the efficiency and robustness of the international\ntrade networks of pesticides increased for all the node metrics except the\nclustering coefficient. Moreover, the international pesticide trade networks\nare more fragile when import-oriented economies are affected by shocks.\n"
    },
    {
        "paper_id": 2302.13718,
        "authors": "Emil Chrisander and Andreas Bjerre-Nielsen",
        "title": "Why Do Students Lie and Should We Worry? An Analysis of Non-truthful\n  Reporting",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
        "abstract": "  A core aspect in market design is to encourage participants to truthfully\nreport their preferences to ensure efficiency and fairness. Our research paper\nanalyzes the factors that contribute to and the consequences of students\nreporting non-truthfully in admissions applications. We survey college\napplicants in Denmark about their perceptions of the admission process and\npersonality to examine recent theories of misreporting preferences. Our\nanalysis reveals that omissions in reports are largely driven by students'\npessimistic beliefs about their chances of admission. Moreover, such erroneous\nbeliefs largely account for whether an omission led to a missed opportunity for\nadmission. However, the low frequency of these errors suggests that most\nnon-truthful reports are \"white lies\" with minimal negative impact. We find a\nnovel role of personality and individual circumstances that co-determine the\nextent of omissions. We also find that estimates of students' demand are biased\nif it is assumed that students report truthfully, and demonstrate that this\nbias can be reduced by making a less restrictive assumption. Our results have\nimplications for the modeling of preferences, information acquisition, and\nsubjective admission beliefs in strategy-proof mechanisms\n"
    },
    {
        "paper_id": 2302.13781,
        "authors": "Kim Chol-jun",
        "title": "Distribution in the Geometrically Growing System and Its Evolution",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Recently, we developed a theory of a geometrically growing system. Here we\nshow that the theory can explain some phenomena of power-law distribution\nincluding classical demographic and economic and novel pandemic instances,\nwithout introduction of delicate economic models but only on the statistical\nway. A convexity in the low-size part of the distribution is one peculiarity of\nthe theory, which is absent in the power-law distribution. We found that the\ndistribution of the geometrically growing system could have a trend to flatten\nin the evolution of the system so that the relative ratio of size within the\nsystem increases. The system can act as a reverse machine to covert a diffusion\nin parametric space to a concentration in the size distribution.\n"
    },
    {
        "paper_id": 2302.13823,
        "authors": "James Bell",
        "title": "The global economic impact of AI technologies in the fight against\n  financial crime",
        "comments": "82 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  Is the rapid adoption of Artificial Intelligence a sign that creative\ndestruction (a capitalist innovation process first theorised in 1942) is\noccurring? Although its theory suggests that it is only visible over time in\naggregate, this paper devises three hypotheses to test its presence on a macro\nlevel and research methods to produce the required data. This paper tests the\ntheory using news archives, questionnaires, and interviews with industry\nprofessionals. It considers the risks of adopting Artificial Intelligence, its\ncurrent performance in the market and its general applicability to the role.\nThe results suggest that creative destruction is occurring in the AML industry\ndespite the activities of the regulators acting as natural blockers to\ninnovation. This is a pressurised situation where current-generation Artificial\nIntelligence may offer more harm than benefit. For managers, this papers\nresults suggest that safely pursuing AI in AML requires having realistic\nexpectations of Artificial Intelligence's benefits combined with using a\nframework for AI Ethics.\n"
    },
    {
        "paper_id": 2302.1385,
        "authors": "Fazl Barez, Paul Bilokon, Arthur Gervais, Nikita Lisitsyn",
        "title": "Exploring the Advantages of Transformers for High-Frequency Trading",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  This paper explores the novel deep learning Transformers architectures for\nhigh-frequency Bitcoin-USDT log-return forecasting and compares them to the\ntraditional Long Short-Term Memory models. A hybrid Transformer model, called\n\\textbf{HFformer}, is then introduced for time series forecasting which\nincorporates a Transformer encoder, linear decoder, spiking activations, and\nquantile loss function, and does not use position encoding. Furthermore,\npossible high-frequency trading strategies for use with the HFformer model are\ndiscussed, including trade sizing, trading signal aggregation, and minimal\ntrading threshold. Ultimately, the performance of the HFformer and Long\nShort-Term Memory models are assessed and results indicate that the HFformer\nachieves a higher cumulative PnL than the LSTM when trading with multiple\nsignals during backtesting.\n"
    },
    {
        "paper_id": 2302.13979,
        "authors": "Jonathan Yu-Meng Li",
        "title": "Wasserstein-Kelly Portfolios: A Robust Data-Driven Solution to Optimize\n  Portfolio Growth",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We introduce a robust variant of the Kelly portfolio optimization model,\ncalled the Wasserstein-Kelly portfolio optimization. Our model, taking a\nWasserstein distributionally robust optimization (DRO) formulation, addresses\nthe fundamental issue of estimation error in Kelly portfolio optimization by\ndefining a ``ball\" of distributions close to the empirical return distribution\nusing the Wasserstein metric and seeking a robust log-optimal portfolio against\nthe worst-case distribution from the Wasserstein ball. Enhancing the Kelly\nportfolio using Wasserstein DRO is a natural step to take, given many\nsuccessful applications of the latter in areas such as machine learning for\ngenerating robust data-driven solutions. However, naive application of\nWasserstein DRO to the growth-optimal portfolio problem can lead to several\nissues, which we resolve through careful modelling. Our proposed model is both\npractically motivated and efficiently solvable as a convex program. Using\nempirical financial data, our numerical study demonstrates that the\nWasserstein-Kelly portfolio can outperform the Kelly portfolio in out-of-sample\ntesting across multiple performance metrics and exhibits greater stability.\n"
    },
    {
        "paper_id": 2302.13994,
        "authors": "Bernhard K Meister",
        "title": "Gambling the World Away: Myopic Investors",
        "comments": "5 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Myopic investors are locally rational decision-makers but globally\nirrational. Their suboptimal portfolios lag the market. As a consequence, other\nmarket participants are provided with profit opportunities. Not subterfuge but\nconstrained optimisation leads to disparities. Four overlapping examples are\ngiven. The first case centres on the difference between local and global\noptimisers and their respective Kelly fractions, the second on isolated versus\ncombined optimisation, the third on the distinction between qualitative and\nquantitative investor, the fourth on the non-commutative nature of information\nand the resulting asymmetries.\n"
    },
    {
        "paper_id": 2302.14114,
        "authors": "Marouane Daoui",
        "title": "Econometric assessment of the monetary policy shocks in Morocco:\n  Evidence from a Bayesian Factor-Augmented VAR",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  The analysis of the effects of monetary policy shocks using the common\neconometric models (such as VAR or SVAR) poses several empirical anomalies.\nHowever, it is known that in these econometric models the use of a large amount\nof information is accompanied by dimensionality problems. In this context, the\napproach in terms of FAVAR (Factor Augmented VAR) models tries to solve this\nproblem. Moreover, the information contained in the factors is important for\nthe correct identification of monetary policy shocks and it helps to correct\nthe empirical anomalies usually encountered in empirical work. Following\nBernanke, Boivin and Eliasz (2005) procedure, we will use the FAVAR model to\nanalyze the impact of monetary policy shocks on the Moroccan economy. The model\nused allows us to obtain impulse response functions for all indicators in the\nmacroeconomic dataset used (117 quarterly frequency series from 1985: Q1 to\n2018: Q4) to have a more realistic and complete representation of the impact of\nmonetary policy shocks in Morocco.\n"
    },
    {
        "paper_id": 2302.14133,
        "authors": "David Boto-Garc\\'ia and Federico Perali",
        "title": "The association between Marital Locus of Control and break-up intentions",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Understanding couple instability is a topic of social and economic relevance.\nThis paper investigates how the risk of dissolution relates to efforts to solve\ndisagreements. We study whether the prevalence of relationship instability in\nthe past among couples is associated with marital locus of control. This is a\nnoncognitive trait that captures individuals perception of control over\nproblems within the couple. We implement a list experiment using the count-item\ntechnique to a sample of current real-life couples to elicit truthful answers\nabout couple break-up intentions in the past at the individual level. We find\nthat around 44 per cent of our sample has considered to end their relationship\nwith their partner in the past. The intention to break-up is more prevalent\namong those who score low in marital locus of control, males, low-income\nearners, individuals with university studies and couples without children.\n"
    },
    {
        "paper_id": 2302.14164,
        "authors": "Jingyi Gu, Fadi P. Deek, Guiling Wang",
        "title": "Stock Broad-Index Trend Patterns Learning via Domain Knowledge Informed\n  Generative Network",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Predicting the Stock movement attracts much attention from both industry and\nacademia. Despite such significant efforts, the results remain unsatisfactory\ndue to the inherently complicated nature of the stock market driven by factors\nincluding supply and demand, the state of the economy, the political climate,\nand even irrational human behavior. Recently, Generative Adversarial Networks\n(GAN) have been extended for time series data; however, robust methods are\nprimarily for synthetic series generation, which fall short for appropriate\nstock prediction. This is because existing GANs for stock applications suffer\nfrom mode collapse and only consider one-step prediction, thus underutilizing\nthe potential of GAN. Furthermore, merging news and market volatility are\nneglected in current GANs. To address these issues, we exploit expert domain\nknowledge in finance and, for the first time, attempt to formulate stock\nmovement prediction into a Wasserstein GAN framework for multi-step prediction.\nWe propose IndexGAN, which includes deliberate designs for the inherent\ncharacteristics of the stock market, leverages news context learning to\nthoroughly investigate textual information and develop an attentive seq2seq\nlearning network that captures the temporal dependency among stock prices,\nnews, and market sentiment. We also utilize the critic to approximate the\nWasserstein distance between actual and predicted sequences and develop a\nrolling strategy for deployment that mitigates noise from the financial market.\nExtensive experiments are conducted on real-world broad-based indices,\ndemonstrating the superior performance of our architecture over other\nstate-of-the-art baselines, also validating all its contributing components.\n"
    },
    {
        "paper_id": 2302.14358,
        "authors": "Alex Chin and Zhiwei Qin",
        "title": "A Unified Representation Framework for Rideshare Marketplace Equilibrium\n  and Efficiency",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  Ridesharing platforms are a type of two-sided marketplace where\n``supply-demand balance'' is critical for market efficiency and yet is complex\nto define and analyze. We present a unified analytical framework based on the\ngraph-based equilibrium metric (GEM) for quantifying the supply-demand\nspatiotemporal state and efficiency of a ridesharing marketplace. GEM was\ndeveloped as a generalized Wasserstein distance between the supply and demand\ndistributions in a ridesharing market and has been used as an evaluation metric\nfor algorithms expected to improve supply-demand alignment. Building upon GEM,\nwe develop SD-GEM, a dual-perspective (supply- and demand-side) representation\nof rideshare market equilibrium. We show that there are often disparities\nbetween the two views and examine how this dual-view leads to the notion of\nmarket efficiency, in which we propose novel statistical tests for capturing\nimprovement and explaining the underlying driving factors.\n"
    },
    {
        "paper_id": 2302.1444,
        "authors": "Ulrika Ahrsj\\\"o, Ren\\'e Karadakic and Joachim Kahr Rasmussen",
        "title": "Intergenerational Mobility Trends and the Changing Role of Female Labor",
        "comments": "67 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Using harmonized administrative data from Scandinavia, we find that\nintergenerational rank associations in income have increased uniformly across\nSweden, Denmark, and Norway for cohorts born between 1951 and 1979. Splitting\nthese trends by gender, we find that father-son mobility has been stable, while\nfamily correlations for mothers and daughters trend upward. Similar patterns\nappear in US survey data, albeit with slightly different timing. Finally, based\non evidence from records on occupations and educational attainments, we argue\nthat the observed decline in intergenerational mobility is consistent with\nfemale skills becoming increasingly valued in the labor market.\n"
    },
    {
        "paper_id": 2302.14602,
        "authors": "Emir Malikov and Shunan Zhao",
        "title": "On the Estimation of Cross-Firm Productivity Spillovers with an\n  Application to FDI",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  We develop a novel methodology for the proxy variable identification of firm\nproductivity in the presence of productivity-modifying learning and spillovers\nwhich facilitates a unified \"internally consistent\" analysis of the spillover\neffects between firms. Contrary to the popular two-step empirical approach,\nours does not postulate contradictory assumptions about firm productivity\nacross the estimation steps. Instead, we explicitly accommodate cross-sectional\ndependence in productivity induced by spillovers which facilitates\nidentification of both the productivity and spillover effects therein\nsimultaneously. We apply our model to study cross-firm spillovers in China's\nelectric machinery manufacturing, with a particular focus on productivity\neffects of inbound FDI.\n"
    },
    {
        "paper_id": 2302.14603,
        "authors": "Jingfang Zhang and Emir Malikov",
        "title": "Off-Balance Sheet Activities and Scope Economies in U.S. Banking",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Propelled by the recent financial product innovations involving derivatives,\nsecuritization and mortgages, commercial banks are becoming more complex,\nbranching out into many \"nontraditional\" banking operations beyond issuance of\nloans. This broadening of operational scope in a pursuit of revenue\ndiversification may be beneficial if banks exhibit scope economies. The\nexisting (two-decade-old) empirical evidence lends no support for such\nproduct-scope-driven cost economies in banking, but it is greatly outdated and,\nsurprisingly, there has been little (if any) research on this subject despite\nthe drastic transformations that the U.S. banking industry has undergone over\nthe past two decades in the wake of technological advancements and regulatory\nchanges. Commercial banks have significantly shifted towards nontraditional\noperations, making the portfolio of products offered by present-day banks very\ndifferent from that two decades ago. In this paper, we provide new and more\nrobust evidence about scope economies in U.S. commercial banking. We improve\nupon the prior literature not only by analyzing the most recent data and\naccounting for bank's nontraditional off-balance sheet operations, but also in\nmultiple methodological ways. To test for scope economies, we estimate a\nflexible time-varying-coefficient panel-data quantile regression model which\naccommodates three-way heterogeneity across banks. Our results provide strong\nevidence in support of significantly positive scope economies across banks of\nvirtually all sizes. Contrary to earlier studies, we find no empirical\ncorroboration for scope diseconomies.\n"
    },
    {
        "paper_id": 2302.14784,
        "authors": "Juan Marcelo Virdis, Fernando Delbianco, Mar\\'ia Eugenia Elorza",
        "title": "Evaluaci\\'on del efecto del PAMI en la cobertura en salud de los adultos\n  mayores en Argentina",
        "comments": "in Spanish language",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  We conducted regression discontinuity design models in order to evaluate\nchanges in access to healthcare services and financial protection, using as a\nnatural experiment the age required to retire in Argentina, the moment in which\npeople are able to enroll in the free social health insurance called PAMI. The\ndependent variables were indicators of the population with health insurance,\nout-of-pocket health expenditure, and use of health services. The results show\nthat PAMI causes a high increase in the population with health insurance and\nmarginal reductions in health expenditure. No effects on healthcare use were\nfound.\n"
    },
    {
        "paper_id": 2303.0008,
        "authors": "Zijian Shi and John Cartlidge",
        "title": "Neural Stochastic Agent-Based Limit Order Book Simulation: A Hybrid\n  Methodology",
        "comments": "13 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Modern financial exchanges use an electronic limit order book (LOB) to store\nbid and ask orders for a specific financial asset. As the most fine-grained\ninformation depicting the demand and supply of an asset, LOB data is essential\nin understanding market dynamics. Therefore, realistic LOB simulations offer a\nvaluable methodology for explaining empirical properties of markets. Mainstream\nsimulation models include agent-based models (ABMs) and stochastic models\n(SMs). However, ABMs tend not to be grounded on real historical data, while SMs\ntend not to enable dynamic agent-interaction. To overcome these limitations, we\npropose a novel hybrid LOB simulation paradigm characterised by: (1)\nrepresenting the aggregation of market events' logic by a neural stochastic\nbackground trader that is pre-trained on historical LOB data through a neural\npoint process model; and (2) embedding the background trader in a multi-agent\nsimulation with other trading agents. We instantiate this hybrid NS-ABM model\nusing the ABIDES platform. We first run the background trader in isolation and\nshow that the simulated LOB can recreate a comprehensive list of stylised facts\nthat demonstrate realistic market behaviour. We then introduce a population of\n`trend' and `value' trading agents, which interact with the background trader.\nWe show that the stylised facts remain and we demonstrate order flow impact and\nfinancial herding behaviours that are in accordance with empirical observations\nof real markets.\n"
    },
    {
        "paper_id": 2303.00208,
        "authors": "Jason Milionis, Ciamac C. Moallemi, Tim Roughgarden",
        "title": "A Myersonian Framework for Optimal Liquidity Provision in Automated\n  Market Makers",
        "comments": "20 pages, to appear in the 15th Innovations in Theoretical Computer\n  Science conference (ITCS 2024)",
        "journal-ref": null,
        "doi": "10.4230/LIPIcs.ITCS.2024.80",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In decentralized finance (\"DeFi\"), automated market makers (AMMs) enable\ntraders to programmatically exchange one asset for another. Such trades are\nenabled by the assets deposited by liquidity providers (LPs). The goal of this\npaper is to characterize and interpret the optimal (i.e., profit-maximizing)\nstrategy of a monopolist liquidity provider, as a function of that LP's beliefs\nabout asset prices and trader behavior. We introduce a general framework for\nreasoning about AMMs based on a Bayesian-like belief inference framework, where\nLPs maintain an asset price estimate. In this model, the market maker (i.e.,\nLP) chooses a demand curve that specifies the quantity of a risky asset to be\nheld at each dollar price. Traders arrive sequentially and submit a price bid\nthat can be interpreted as their estimate of the risky asset price; the AMM\nresponds to this submitted bid with an allocation of the risky asset to the\ntrader, a payment that the trader must pay, and a revised internal estimate for\nthe true asset price. We define an incentive-compatible (IC) AMM as one in\nwhich a trader's optimal strategy is to submit its true estimate of the asset\nprice, and characterize the IC AMMs as those with downward-sloping demand\ncurves and payments defined by a formula familiar from Myerson's optimal\nauction theory. We generalize Myerson's virtual values, and characterize the\nprofit-maximizing IC AMM. The optimal demand curve generally has a jump that\ncan be interpreted as a \"bid-ask spread,\" which we show is caused by a\ncombination of adverse selection risk (dominant when the degree of information\nasymmetry is large) and monopoly pricing (dominant when asymmetry is small).\nThis work opens up new research directions into the study of automated exchange\nmechanisms from the lens of optimal auction theory and iterative belief\ninference, using tools of theoretical computer science in a novel way.\n"
    },
    {
        "paper_id": 2303.00314,
        "authors": "David Franzmann, Heidi Heinrichs, Felix Lippkau, Thushara Addanki,\n  Christoph Winkler, Patrick Buchenberg, Thomas Hamacher, Markus Blesl, Jochen\n  Lin{\\ss}en and Detlef Stolten",
        "title": "Green Hydrogen Cost-Potentials for Global Trade",
        "comments": "in press",
        "journal-ref": "International Journal of Hydrogen Energy, 2023",
        "doi": "10.1016/j.ijhydene.2023.05.012",
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Green hydrogen is expected to be traded globally in future greenhouse gas\nneutral energy systems. However, there is still a lack of temporally- and\nspatially-explicit cost-potentials for green hydrogen considering the full\nprocess chain, which are necessary for creating effective global strategies.\nTherefore, this study provides such detailed cost-potential-curves for 28\nselected countries worldwide until 2050, using an optimizing energy systems\napproach based on open-field photovoltaics (PV) and onshore wind. The results\nreveal huge hydrogen potentials (>1,500 PWhLHV/a) and 79 PWhLHV/a at costs\nbelow 2.30 EUR/kg in 2050, dominated by solar-rich countries in Africa and the\nMiddle East. Decentralized PV-based hydrogen production, even in wind-rich\ncountries, is always preferred. Supplying sustainable water for hydrogen\nproduction is needed while having minor impact on hydrogen cost. Additional\ncosts for imports from democratic regions are only total 7% higher. Hence, such\nregions could boost the geostrategic security of supply for greenhouse gas\nneutral energy systems.\n"
    },
    {
        "paper_id": 2303.00356,
        "authors": "Boian Lazov",
        "title": "A Deep Reinforcement Learning Trader without Offline Training",
        "comments": "17 pages, 5 figures, full Mathematica code included",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper we pursue the question of a fully online trading algorithm\n(i.e. one that does not need offline training on previously gathered data). For\nthis task we use Double Deep $Q$-learning in the episodic setting with Fast\nLearning Networks approximating the expected reward $Q$. Additionally, we\ndefine the possible terminal states of an episode in such a way as to introduce\na mechanism to conserve some of the money in the trading pool when market\nconditions are seen as unfavourable. Some of these money are taken as profit\nand some are reused at a later time according to certain criteria. After\ndescribing the algorithm, we test it using the 1-minute-tick data for Cardano's\nprice on Binance. We see that the agent performs better than trading with\nrandomly chosen actions on each timestep. And it does so when tested on the\nwhole dataset as well as on different subsets, capturing different market\ntrends.\n"
    },
    {
        "paper_id": 2303.00495,
        "authors": "Marcin W\\k{a}torek and Jaros{\\l}aw Kwapie\\'n and Stanis{\\l}aw\n  Dro\\.zd\\.z",
        "title": "Cryptocurrencies Are Becoming Part of the World Global Financial Market",
        "comments": null,
        "journal-ref": "Entropy 2023, 25(2), 377",
        "doi": "10.3390/e25020377",
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  In this study the cross-correlations between the cryptocurrency market\nrepresented by the two most liquid and highest-capitalized cryptocurrencies:\nbitcoin and ethereum, on the one side, and the instruments representing the\ntraditional financial markets: stock indices, Forex, commodities, on the other\nside, are measured in the period: January 2020--October 2022. Our purpose is to\naddress the question whether the cryptocurrency market still preserves its\nautonomy with respect to the traditional financial markets or it has already\naligned with them in expense of its independence. We are motivated by the fact\nthat some previous related studies gave mixed results. By calculating the\n$q$-dependent detrended cross-correlation coefficient based on the high\nfrequency 10 s data in the rolling window, the dependence on various time\nscales, different fluctuation magnitudes, and different market periods are\nexamined. There is a strong indication that the dynamics of the bitcoin and\nethereum price changes since the March 2020 Covid-19 panic is no longer\nindependent. Instead, it is related to the dynamics of the traditional\nfinancial markets, which is especially evident now in 2022, when the bitcoin\nand ethereum coupling to the US tech stocks is observed during the market bear\nphase. It is also worth emphasizing that the cryptocurrencies have begun to\nreact to the economic data such as the Consumer Price Index readings in a\nsimilar way as traditional instruments. Such a spontaneous coupling of the so\nfar independent degrees of freedom can be interpreted as a kind of phase\ntransition that resembles the collective phenomena typical for the complex\nsystems. Our results indicate that the cryptocurrencies cannot be considered as\na safe haven for the financial investments.\n"
    },
    {
        "paper_id": 2303.00858,
        "authors": "Erhan Bayraktar, Donghan Kim, Abhishek Tilva",
        "title": "Quantifying dimensional change in stochastic portfolio theory",
        "comments": "41 pages, 4 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
        "abstract": "  In this paper, we develop the theory of functional generation of portfolios\nin an equity market with changing dimension. By introducing dimensional jumps\nin the market, as well as jumps in stock capitalization between the dimensional\njumps, we construct different types of self-financing stock portfolios\n(additive, multiplicative, and rank-based) in a very general setting. Our study\nexplains how a dimensional change caused by a listing or delisting event of a\nstock, and unexpected shocks in the market, affect portfolio return. We also\nprovide empirical analyses of some classical portfolios, quantifying the impact\nof dimensional change in portfolio performance relative to the market.\n"
    },
    {
        "paper_id": 2303.00859,
        "authors": "Vedant Choudhary, Sebastian Jaimungal, Maxime Bergeron",
        "title": "FuNVol: A Multi-Asset Implied Volatility Market Simulator using\n  Functional Principal Components and Neural SDEs",
        "comments": "38 pages, 19 figures, 5 tables",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-sa/4.0/",
        "abstract": "  We introduce a new approach for generating sequences of implied volatility\n(IV) surfaces across multiple assets that is faithful to historical prices. We\ndo so using a combination of functional data analysis and neural stochastic\ndifferential equations (SDEs) combined with a probability integral transform\npenalty to reduce model misspecification. We demonstrate that learning the\njoint dynamics of IV surfaces and prices produces market scenarios that are\nconsistent with historical features and lie within the sub-manifold of surfaces\nthat are essentially free of static arbitrage. Finally, we demonstrate that\ndelta hedging using the simulated surfaces generates profit and loss (P&L)\ndistributions that are consistent with realised P&Ls.\n"
    },
    {
        "paper_id": 2303.01111,
        "authors": "Matej Steinbacher",
        "title": "Predicting Stock Price Movement as an Image Classification Problem",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  The paper studies intraday price movement of stocks that is considered as an\nimage classification problem. Using a CNN-based model we make a compelling case\nfor the high-level relationship between the first hour of trading and the\nclose. The algorithm managed to adequately separate between the two opposing\nclasses and investing according to the algorithm's predictions outperformed all\nalternative constructs but the theoretical maximum. To support the thesis, we\nran several additional tests. The findings in the paper highlight the\nsuitability of computer vision techniques for studying financial markets and in\nparticular prediction of stock price movements.\n"
    },
    {
        "paper_id": 2303.01157,
        "authors": "Ed Felten, Manav Raj, Robert Seamans",
        "title": "How will Language Modelers like ChatGPT Affect Occupations and\n  Industries?",
        "comments": "33 pages, 3 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Recent dramatic increases in AI language modeling capabilities has led to\nmany questions about the effect of these technologies on the economy. In this\npaper we present a methodology to systematically assess the extent to which\noccupations, industries and geographies are exposed to advances in AI language\nmodeling capabilities. We find that the top occupations exposed to language\nmodeling include telemarketers and a variety of post-secondary teachers such as\nEnglish language and literature, foreign language and literature, and history\nteachers. We find the top industries exposed to advances in language modeling\nare legal services and securities, commodities, and investments. We also find a\npositive correlation between wages and exposure to AI language modeling.\n"
    },
    {
        "paper_id": 2303.01167,
        "authors": "Cosimo Munari, Justin Pl\\\"uckebaum, Stefan Weber",
        "title": "Robust portfolio selection under Recovery Average Value at Risk",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We study mean-risk optimal portfolio problems where risk is measured by\nRecovery Average Value at Risk, a prominent example in the class of recovery\nrisk measures. We establish existence results in the situation where the joint\ndistribution of portfolio assets is known as well as in the situation where it\nis uncertain and only assumed to belong to a set of mixtures of benchmark\ndistributions (mixture uncertainty) or to a cloud around a benchmark\ndistribution (box uncertainty). The comparison with the classical Average Value\nat Risk shows that portfolio selection under its recovery version enables\nfinancial institutions to exert better control on the recovery on liabilities\nwhile still allowing for tractable computations.\n"
    },
    {
        "paper_id": 2303.01485,
        "authors": "Eduardo C. Garrido-Merch\\'an, Gabriel Gonz\\'alez Piris, Maria Coronado\n  Vaca",
        "title": "Bayesian Optimization of ESG Financial Investments",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Financial experts and analysts seek to predict the variability of financial\nmarkets. In particular, the correct prediction of this variability ensures\ninvestors successful investments. However, there has been a big trend in\nfinance in the last years, which are the ESG criteria. Concretely, ESG\n(Economic, Social and Governance) criteria have become more significant in\nfinance due to the growing importance of investments being socially\nresponsible, and because of the financial impact companies suffer when not\ncomplying with them. Consequently, creating a stock portfolio should not only\ntake into account its performance but compliance with ESG criteria. Hence, this\npaper combines mathematical modelling, with ESG and finance. In more detail, we\nuse Bayesian optimization (BO), a sequential state-of-the-art design strategy\nto optimize black-boxes with unknown analytical and costly-to compute\nexpressions, to maximize the performance of a stock portfolio under the\npresence of ESG criteria soft constraints incorporated to the objective\nfunction. In an illustrative experiment, we use the Sharpe ratio, that takes\ninto consideration the portfolio returns and its variance, in other words, it\nbalances the trade-off between maximizing returns and minimizing risks. In the\npresent work, ESG criteria have been divided into fourteen independent\ncategories used in a linear combination to estimate a firm total ESG score.\nMost importantly, our presented approach would scale to alternative black-box\nmethods of estimating the performance and ESG compliance of the stock\nportfolio. In particular, this research has opened the door to many new\nresearch lines, as it has proved that a portfolio can be optimized using a BO\nthat takes into consideration financial performance and the accomplishment of\nESG criteria.\n"
    },
    {
        "paper_id": 2303.01651,
        "authors": "Yuru Sun, Worapree Maneesoonthorn, Ruben Loaiza-Maya and Gael M.\n  Martin",
        "title": "Optimal probabilistic forecasts for risk management",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  This paper explores the implications of producing forecast distributions that\nare optimized according to scoring rules that are relevant to financial risk\nmanagement. We assess the predictive performance of optimal forecasts from\npotentially misspecified models for i) value-at-risk and expected shortfall\npredictions; and ii) prediction of the VIX volatility index for use in hedging\nstrategies involving VIX futures. Our empirical results show that calibrating\nthe predictive distribution using a score that rewards the accurate prediction\nof extreme returns improves the VaR and ES predictions. Tail-focused predictive\ndistributions are also shown to yield better outcomes in hedging strategies\nusing VIX futures.\n"
    },
    {
        "paper_id": 2303.01824,
        "authors": "Jules Depersin, B\\'ereng\\`ere Patault",
        "title": "Revisiting the effect of search frictions on market concentration",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Search frictions can impede the formation of optimal matches between consumer\nand supplier, or employee and employer, and lead to inefficiencies. This paper\nrevisits the effect of search frictions on the firm size distribution when\nchallenging two common but strong assumptions: that all agents share the same\nranking of firms, and that agents meet all firms, whether small or large, at\nthe same rate. We build a random search model in which we relax those two\nassumptions and show that the intensity of search frictions has a non monotonic\neffect on market concentration. An increase in friction intensity increases\nmarket concentration up to a certain threshold of frictions, that depends on\nthe slope of the meeting rate with respect to firm size. We leverage unique\nFrench customs data to estimate this slope. First, we find that in a range of\nplausible scenarios, search frictions intensity increases market concentration.\nSecond, we show that slopes have increased over time, which unambiguously\nincreases market concentration in our model. Overall, we shed light on the\nimportance of the structure of frictions, rather than their intensity, to\nunderstand market concentration.\n"
    },
    {
        "paper_id": 2303.01855,
        "authors": "Joseph de Vilmarest, Nicklas Werge",
        "title": "An adaptive volatility method for probabilistic forecasting and its\n  application to the M6 financial forecasting competition",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper, we address the problem of probabilistic forecasting using an\nadaptive volatility method rooted in classical time-varying volatility models\nand leveraging online stochastic optimization algorithms. These principles were\nsuccessfully applied in the M6 forecasting competition under the team named\nAdaGaussMC. Our approach takes a unique path by embracing the Efficient Market\nHypothesis (EMH) instead of trying to beat the market directly. We focus on\nevaluating the efficient market, emphasizing the importance of online\nforecasting in adapting to the dynamic nature of financial markets. The three\nkey points of our approach are: (a) apply the univariate time-varying\nvolatility model AdaVol, (b) obtain probabilistic forecasts of future returns,\nand (c) optimize the competition metrics using stochastic gradient-based\nalgorithms. We contend that the simplicity of our approach contributes to its\nrobustness and consistency. Remarkably, our performance in the M6 competition\nresulted in an overall 7th ranking, with a noteworthy 5th position in the\nforecasting task. This achievement, considering the perceived simplicity of our\napproach, underscores the efficacy of our adaptive volatility method in the\nrealm of probabilistic forecasting.\n"
    },
    {
        "paper_id": 2303.01909,
        "authors": "Martin Vesely",
        "title": "Finding the Optimal Currency Composition of Foreign Exchange Reserves\n  with a Quantum Computer",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Portfolio optimization is an inseparable part of strategic asset allocation\nat the Czech National Bank. Quantum computing is a new technology offering\nalgorithms for that problem. The capabilities and limitations of quantum\ncomputers with regard to portfolio optimization should therefore be\ninvestigated. In this paper, we focus on applications of quantum algorithms to\ndynamic portfolio optimization based on the Markowitz model. In particular, we\ncompare algorithms for universal gate-based quantum computers (the QAOA, the\nVQE and Grover adaptive search), single-purpose quantum annealers, the\nclassical exact branch and bound solver and classical heuristic algorithms\n(simulated annealing and genetic optimization). To run the quantum algorithms\nwe use the IBM Quantum\\textsuperscript{TM} gate-based quantum computer. We also\nemploy the quantum annealer offered by D-Wave. We demonstrate portfolio\noptimization on finding the optimal currency composition of the CNB's FX\nreserves. A secondary goal of the paper is to provide staff of central banks\nand other financial market regulators with literature on quantum optimization\nalgorithms, because financial firms are active in finding possible applications\nof quantum computing.\n"
    },
    {
        "paper_id": 2303.01923,
        "authors": "Yaojun Zhang, Lanpeng Ji, Georgios Aivaliotis, and Charles Taylor",
        "title": "Bayesian CART models for insurance claims frequency",
        "comments": "46 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Accuracy and interpretability of a (non-life) insurance pricing model are\nessential qualities to ensure fair and transparent premiums for policy-holders,\nthat reflect their risk. In recent years, the classification and regression\ntrees (CARTs) and their ensembles have gained popularity in the actuarial\nliterature, since they offer good prediction performance and are relatively\neasily interpretable. In this paper, we introduce Bayesian CART models for\ninsurance pricing, with a particular focus on claims frequency modelling.\nAdditionally to the common Poisson and negative binomial (NB) distributions\nused for claims frequency, we implement Bayesian CART for the zero-inflated\nPoisson (ZIP) distribution to address the difficulty arising from the\nimbalanced insurance claims data. To this end, we introduce a general MCMC\nalgorithm using data augmentation methods for posterior tree exploration. We\nalso introduce the deviance information criterion (DIC) for the tree model\nselection. The proposed models are able to identify trees which can better\nclassify the policy-holders into risk groups. Some simulations and real\ninsurance data will be discussed to illustrate the applicability of these\nmodels.\n"
    },
    {
        "paper_id": 2303.02038,
        "authors": "Ruihua Ruan, Emmanuel Bacry and Jean-Fran\\c{c}ois Muzy",
        "title": "The self-exciting nature of the bid-ask spread dynamics",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The bid-ask spread, which is defined by the difference between the best\nselling price and the best buying price in a Limit Order Book at a given time,\nis a crucial factor in the analysis of financial securities. In this study, we\npropose a \"State-dependent Spread Hawkes model\" (SDSH) that accounts for\nvarious spread jump sizes and incorporates the impact of the current spread\nstate on its intensity functions. We apply this model to the high-frequency\ndata from the Cac40 Euronext market and capture several statistical properties,\nsuch as the spread distributions, inter-event time distributions, and spread\nautocorrelation functions. We illustrate the ability of the SDSH model to\nforecast spread values at short-term horizons.\n"
    },
    {
        "paper_id": 2303.02061,
        "authors": "Henry Skeoch and Christos Ioannidis",
        "title": "The barriers to sustainable risk transfer in the cyber-insurance market",
        "comments": "32 pages, 9 figures, 17 tables",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
        "abstract": "  Efficient risk transfer is an important condition for ensuring the\nsustainability of a market according to the established economics literature.\nIn an inefficient market, significant financial imbalances may develop and\npotentially jeopardise the solvency of some market participants. The constantly\nevolving nature of cyber-threats and lack of public data sharing mean that the\neconomic conditions required for quoted cyber-insurance premiums to be\nconsidered efficient are highly unlikely to be met. This paper develops Monte\nCarlo simulations of an artificial cyber-insurance market and compares the\nefficient and inefficient outcomes based on the informational setup between the\nmarket participants. The existence of diverse loss distributions is justified\nby the dynamic nature of cyber-threats and the absence of any reliable and\ncentralised incident reporting. It is shown that the limited involvement of\nreinsurers when loss expectations are not shared leads to increased premiums\nand lower overall capacity. This suggests that the sustainability of the\ncyber-insurance market requires both better data sharing and external sources\nof risk tolerant capital.\n"
    },
    {
        "paper_id": 2303.02223,
        "authors": "Hakan Pabuccu, Adrian Barbu",
        "title": "Feature Selection with Annealing for Forecasting Financial Time Series",
        "comments": "37 pages, 1 figures and 12 tables",
        "journal-ref": null,
        "doi": "10.48550/arXiv.2303.02223",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Stock market and cryptocurrency forecasting is very important to investors as\nthey aspire to achieve even the slightest improvement to their buy or hold\nstrategies so that they may increase profitability. However, obtaining accurate\nand reliable predictions is challenging, noting that accuracy does not equate\nto reliability, especially when financial time-series forecasting is applied\nowing to its complex and chaotic tendencies. To mitigate this complexity, this\nstudy provides a comprehensive method for forecasting financial time series\nbased on tactical input output feature mapping techniques using machine\nlearning (ML) models. During the prediction process, selecting the relevant\nindicators is vital to obtaining the desired results. In the financial field,\nlimited attention has been paid to this problem with ML solutions. We\ninvestigate the use of feature selection with annealing (FSA) for the first\ntime in this field, and we apply the least absolute shrinkage and selection\noperator (Lasso) method to select the features from more than 1,000 candidates\nobtained from 26 technical classifiers with different periods and lags. Boruta\n(BOR) feature selection, a wrapper method, is used as a baseline for\ncomparison. Logistic regression (LR), extreme gradient boosting (XGBoost), and\nlong short-term memory (LSTM) are then applied to the selected features for\nforecasting purposes using 10 different financial datasets containing\ncryptocurrencies and stocks. The dependent variables consisted of daily\nlogarithmic returns and trends. The mean-squared error for regression, area\nunder the receiver operating characteristic curve, and classification accuracy\nwere used to evaluate model performance, and the statistical significance of\nthe forecasting results was tested using paired t-tests. Experiments indicate\nthat the FSA algorithm increased the performance of ML models, regardless of\nproblem type.\n"
    },
    {
        "paper_id": 2303.0223,
        "authors": "Peter Egger, Susie Xi Rao, Sebastiano Papini",
        "title": "Building Floorspace in China: A Dataset and Learning Pipeline",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
        "abstract": "  This paper provides a first milestone in measuring the floorspace of\nbuildings (that is, building footprint and height) for 40 major Chinese cities.\nThe intent is to maximize city coverage and, eventually provide longitudinal\ndata. Doing so requires building on imagery that is of a medium-fine-grained\ngranularity, as larger cross sections of cities and longer time series for them\nare only available in such format. We use a multi-task object segmenter\napproach to learn the building footprint and height in the same framework in\nparallel: (1) we determine the surface area is covered by any buildings (the\nsquare footage of occupied land); (2) we determine floorspace from multi-image\nrepresentations of buildings from various angles to determine the height of\nbuildings. We use Sentinel-1 and -2 satellite images as our main data source.\nThe benefits of these data are their large cross-sectional and longitudinal\nscope plus their unrestricted accessibility. We provide a detailed description\nof our data, algorithms, and evaluations. In addition, we analyze the quality\nof reference data and their role for measuring the building floorspace with\nminimal error. We conduct extensive quantitative and qualitative analyses with\nShenzhen as a case study using our multi-task learner. Finally, we conduct\ncorrelation studies between our results (on both pixel and aggregated urban\narea levels) and nightlight data to gauge the merits of our approach in\nstudying urban development. Our data and codebase are publicly accessible under\nhttps://gitlab.ethz.ch/raox/urban-satellite-public-v2.\n"
    },
    {
        "paper_id": 2303.02298,
        "authors": "Zhou Fang",
        "title": "Continuous-Time Path-Dependent Exploratory Mean-Variance Portfolio\n  Construction",
        "comments": "Any constructive comment is welcomed",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  In this paper, we present an extended exploratory continuous-time\nmean-variance framework for portfolio management. Our strategy involves a new\nclustering method based on simulated annealing, which allows for more practical\nasset selection. Additionally, we consider past wealth evolution when\nconstructing the mean-variance portfolio. We found that our strategy\neffectively learns from the past and performs well in practice.\n"
    },
    {
        "paper_id": 2303.02303,
        "authors": "Zhou Fang",
        "title": "Electricity Virtual Bidding Strategy Via Entropy-Regularized Stochastic\n  Control Method",
        "comments": "Any constructive critiques are welcome",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  We propose a virtual bidding strategy by modeling the price differences\nbetween the day-ahead market and the real-time market as Brownian motion with\ndrift, where the drift rate and volatility are functions of meteorological\nvariables. We then transform the virtual bidding problem into a mean-variance\nportfolio management problem, where we approach the mean-variance portfolio\nmanagement problem by using the exploratory mean-variance portfolio management\nframework\n"
    },
    {
        "paper_id": 2303.02317,
        "authors": "Zafar Ahmad, Reilly Browne, Rezaul Chowdhury, Rathish Das, Yushen\n  Huang, and Yimin Zhu",
        "title": "Fast American Option Pricing using Nonlinear Stencils",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  We study the binomial, trinomial, and Black-Scholes-Merton models of option\npricing. We present fast parallel discrete-time finite-difference algorithms\nfor American call option pricing under the binomial and trinomial models and\nAmerican put option pricing under the Black-Scholes-Merton model. For $T$-step\nfinite differences, each algorithm runs in $O(\\left(T\\log^2{T}\\right)/p + T)$\ntime under a greedy scheduler on $p$ processing cores, which is a significant\nimprovement over the $\\Theta({T^2}/{p}) + \\Omega(T\\log{T})$ time taken by the\ncorresponding state-of-the-art parallel algorithm. Even when run on a single\ncore, the $O(T\\log^2{T})$ time taken by our algorithms is asymptotically much\nsmaller than the $\\Theta(T^2)$ running time of the fastest known serial\nalgorithms. Implementations of our algorithms significantly outperform the\nfastest implementations of existing algorithms in practice, e.g., when run for\n$T \\approx 1000$ steps on a 48-core machine, our algorithm for the binomial\nmodel runs at least $15\\times$ faster than the fastest existing parallel\nprogram for the same model with the speed-up factor gradually reaching beyond\n$500\\times$ for $T \\approx 0.5 \\times 10^6$. It saves more than 80\\% energy\nwhen $T \\approx 4000$, and more than 99\\% energy for $T > 60,000$.\n  Our option pricing algorithms can be viewed as solving a class of nonlinear\n1D stencil (i.e., finite-difference) computation problems efficiently using the\nFast Fourier Transform (FFT). To our knowledge, ours are the first algorithms\nto handle such stencils in $o(T^2)$ time. These contributions are of\nindependent interest as stencil computations have a wide range of applications\nbeyond quantitative finance.\n"
    },
    {
        "paper_id": 2303.02613,
        "authors": "Chung-Han Hsieh",
        "title": "On Data-Driven Drawdown Control with Restart Mechanism in Trading",
        "comments": null,
        "journal-ref": "IFAC-PapersOnline (Proceedings of the IFAC World Congress 2023)",
        "doi": "10.1016/j.ifacol.2023.10.219",
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  This paper extends the existing drawdown modulation control policy to include\na novel restart mechanism for trading. It is known that the drawdown modulation\npolicy guarantees the maximum percentage drawdown no larger than a prespecified\ndrawdown limit for all time with probability one. However, when the\nprespecified limit is approaching in practice, such a modulation policy becomes\na stop-loss order, which may miss the profitable follow-up opportunities if\nany. Motivated by this, we add a data-driven restart mechanism into the\ndrawdown modulation trading system to auto-tune the performance. We find that\nwith the restart mechanism, our policy may achieve a superior trading\nperformance to that without the restart, even with a nonzero transaction costs\nsetting. To support our findings, some empirical studies using equity ETF and\ncryptocurrency with historical price data are provided.\n"
    },
    {
        "paper_id": 2303.02773,
        "authors": "David Audretsch, Paul P. Momtaz, Hanna Motuzenko, Silvio Vismara",
        "title": "The Economic Costs of the Russia-Ukraine War: A Synthetic Control Study\n  of (Lost) Entrepreneurship",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  This synthetic control study quantifies the economic costs of the\nRusso-Ukrainian war in terms of foregone entrepreneurial activity in both\ncountries since the invasion of Crimea in 2014. Relative to its synthetic\ncounterfactual, Ukraine's number of self-employed dropped by 675,000,\ncorresponding to a relative loss of 20%. The number of Ukrainian SMEs\ntemporarily dropped by 71,000 (14%) and recovered within five years of the\nconflict. In contrast, Russia had lost more than 1.4 million SMEs (42%) five\nyears into the conflict. The disappearance of Russian SMEs is driven by both\nfewer new businesses created and more existing business closures.\n"
    },
    {
        "paper_id": 2303.03073,
        "authors": "Sobin Joseph and Shashi Jain",
        "title": "A neural network based model for multi-dimensional nonlinear Hawkes\n  processes",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
        "abstract": "  This paper introduces the Neural Network for Nonlinear Hawkes processes\n(NNNH), a non-parametric method based on neural networks to fit nonlinear\nHawkes processes. Our method is suitable for analyzing large datasets in which\nevents exhibit both mutually-exciting and inhibitive patterns. The NNNH\napproach models the individual kernels and the base intensity of the nonlinear\nHawkes process using feed forward neural networks and jointly calibrates the\nparameters of the networks by maximizing the log-likelihood function. We\nutilize Stochastic Gradient Descent to search for the optimal parameters and\npropose an unbiased estimator for the gradient, as well as an efficient\ncomputation method. We demonstrate the flexibility and accuracy of our method\nthrough numerical experiments on both simulated and real-world data, and\ncompare it with state-of-the-art methods. Our results highlight the\neffectiveness of the NNNH method in accurately capturing the complexities of\nnonlinear Hawkes processes.\n"
    },
    {
        "paper_id": 2303.0308,
        "authors": "Arno Botha, Esmerelda Oberholzer, Janette Larney, Riaan de Jongh",
        "title": "Defining and comparing SICR-events for classifying impaired loans under\n  IFRS 9",
        "comments": "30 pages (including appendix), 9180 words, 10 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  The IFRS 9 accounting standard requires the prediction of credit\ndeterioration in financial instruments, i.e., significant increases in credit\nrisk (SICR). However, the definition of such a SICR-event is inherently\nambiguous, given its reliance on comparing two subsequent estimates of default\nrisk against some arbitrary threshold. We examine the shortcomings of this\napproach and propose an alternative framework for generating SICR-definitions,\nbased on three parameters: delinquency, stickiness, and the outcome period.\nHaving varied these parameters, we obtain 27 unique SICR-definitions and fit\nlogistic regression models accordingly using rich South African mortgage data;\nitself containing various macroeconomic and obligor-specific input variables.\nThis new SICR-modelling approach is demonstrated by analysing the resulting\nportfolio-level SICR-rates (of each SICR-definition) on their stability over\ntime and their responsiveness to economic downturns. At the account-level, we\ncompare both the accuracy and flexibility of the SICR-predictions across all\nSICR-definitions, and discover several interesting trends during this process.\nThese trends and trade-offs can help in selecting the three parameters, as\ndemonstrated in our recommendations for defining SICR-events. In summary, our\nwork can guide the formulation, testing, and modelling of any SICR-definition,\nthereby promoting the timeous recognition of credit losses; the main imperative\nof IFRS 9.\n"
    },
    {
        "paper_id": 2303.03162,
        "authors": "Marouane Daoui",
        "title": "Monetary Policy and Economic Growth in Developing Countries: A\n  Literature Review",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  This article conducts a literature review on the topic of monetary policy in\ndeveloping countries and focuses on the effectiveness of monetary policy in\npromoting economic growth and the relationship between monetary policy and\neconomic growth. The literature review finds that the activities of central\nbanks in developing countries are often overlooked by economic models, but\nrecent studies have shown that there are many factors that can affect the\neffectiveness of monetary policy in these countries. These factors include the\nprofitability of central banks and monetary unions, the independence of central\nbanks in their operations, and lags, rigidities, and disequilibrium analysis.\nThe literature review also finds that studies on the topic have produced mixed\nresults, with some studies finding that monetary policy has a limited or\nnon-existent impact on economic growth and others finding that it plays a\ncrucial role. The article aims to provide a comprehensive understanding of the\ncurrent state of research in this field and to identify areas for future study.\n"
    },
    {
        "paper_id": 2303.03174,
        "authors": "Paolo Bova and Alessandro Di Stefano and The Anh Han",
        "title": "Both eyes open: Vigilant Incentives help Regulatory Markets improve AI\n  Safety",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  In the context of rapid discoveries by leaders in AI, governments must\nconsider how to design regulation that matches the increasing pace of new AI\ncapabilities. Regulatory Markets for AI is a proposal designed with\nadaptability in mind. It involves governments setting outcome-based targets for\nAI companies to achieve, which they can show by purchasing services from a\nmarket of private regulators. We use an evolutionary game theory model to\nexplore the role governments can play in building a Regulatory Market for AI\nsystems that deters reckless behaviour. We warn that it is alarmingly easy to\nstumble on incentives which would prevent Regulatory Markets from achieving\nthis goal. These 'Bounty Incentives' only reward private regulators for\ncatching unsafe behaviour. We argue that AI companies will likely learn to\ntailor their behaviour to how much effort regulators invest, discouraging\nregulators from innovating. Instead, we recommend that governments always\nreward regulators, except when they find that those regulators failed to detect\nunsafe behaviour that they should have. These 'Vigilant Incentives' could\nencourage private regulators to find innovative ways to evaluate cutting-edge\nAI systems.\n"
    },
    {
        "paper_id": 2303.03214,
        "authors": "Frederico Dutilh Novaes, Gabriel de Abreu Madeira, Aurimar Cerqueira",
        "title": "The Economics of the DeLend Project: Agent-based Simulations",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-sa/4.0/",
        "abstract": "  This paper presents our methodology to simulate the behavior of the DeLend\nPlatform. Such simulations are important to verify if the system is able to\nconnect the different sets of agents linked to the platform in a functional\nmanner. They also provide inputs to guide the choices of operational\nparameters, such as the platform spread, and strategies by DeLend, since they\nestimate how the key variables of interest respond to different policies. We\ndiscuss the methodology and provide examples meant to clarify the approach and\nto how we intend to use the tool in practice -- they should not be interpreted\nas representative of real life scenarios.\n"
    },
    {
        "paper_id": 2303.03303,
        "authors": "Deepanshu Vasal",
        "title": "Social herding in mean field games",
        "comments": "4 pages, 6 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  In this paper, we consider a mean field model of social behavior where there\nare an infinite number of players, each of whom observes a type privately that\nrepresents her preference, and publicly observes a mean field state of types\nand actions of the players in the society. The types (and equivalently\npreferences) of the players are dynamically evolving. Each player is fully\nrational and forward-looking and makes a decision in each round t to buy a\nproduct. She receives a higher utility if the product she bought is aligned\nwith her current preference and if there is a higher fraction of people who\nbought that product (thus a game of strategic complementarity). We show that\nfor certain parameters when the weight of strategic complementarity is high,\nplayers eventually herd towards one of the actions with probability 1 which is\nwhen each player buys a product irrespective of her preference.\n"
    },
    {
        "paper_id": 2303.03371,
        "authors": "Ho-Chun Herbert Chang, Brooke Harrington, Feng Fu, and Daniel Rockmore",
        "title": "Complex Systems of Secrecy: The Offshore Networks of Oligarchs",
        "comments": null,
        "journal-ref": null,
        "doi": "10.1093/pnasnexus/pgad051/",
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Following the invasion of Ukraine, the US, UK, and EU governments--among\nothers--sanctioned oligarchs close to Putin. This approach has come under\nscrutiny, as evidence has emerged of the oligarchs' successful evasion of these\npunishments. To address this problem, we analyze the role of an overlooked but\nhighly influential group: the secretive professional intermediaries who create\nand administer the oligarchs' offshore financial empires. Drawing on the\nOffshore Leaks Database provided by the International Consortium of\nInvestigative Journalists (ICIJ), we examine the ties linking offshore expert\nadvisors (lawyers, accountants, and other wealth management professionals) to\nultra-high-net-worth individuals from four countries: Russia, China, the United\nStates, and Hong Kong. We find that resulting nation-level \"oligarch networks\"\nshare a scale-free structure characterized by heterogeneity of heavy-tailed\ndegree distributions of wealth managers; however, network topologies diverge\nacross clients from democratic versus autocratic regimes. While generally\nrobust, scale-free networks are fragile when targeted by attacks on\nhighly-connected nodes. Our \"knock-out\" experiments pinpoint this vulnerability\nto the small group of wealth managers themselves, suggesting that sanctioning\nthese professional intermediaries may be more effective and efficient in\ndisrupting dark finance flows than sanctions on their wealthy clients. This\nvulnerability is especially pronounced amongst Russian oligarchs, who\nconcentrate their offshore business in a handful of boutique wealth management\nfirms. The distinctive patterns we identify suggest a new approach to\nsanctions, focused on expert intermediaries to disrupt the finances and\nalliances of their wealthy clients. More generally, our research contributes to\nthe larger body of work on complexity science and the structures of secrecy.\n"
    },
    {
        "paper_id": 2303.03549,
        "authors": "Fabian Baumann, Daniel Halpern, Ariel D. Procaccia, Iyad Rahwan, Itai\n  Shapira, Manuel Wuthrich",
        "title": "Optimal Engagement-Diversity Tradeoffs in Social Media",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  Social media platforms are known to optimize user engagement with the help of\nalgorithms. It is widely understood that this practice gives rise to echo\nchambers\\emdash users are mainly exposed to opinions that are similar to their\nown. In this paper, we ask whether echo chambers are an inevitable result of\nhigh engagement; we address this question in a novel model. Our main\ntheoretical results establish bounds on the maximum engagement achievable under\na diversity constraint, for suitable measures of engagement and diversity; we\ncan therefore quantify the worst-case tradeoff between these two objectives.\nOur empirical results, based on real data from Twitter, chart the Pareto\nfrontier of the engagement-diversity tradeoff.\n"
    },
    {
        "paper_id": 2303.03573,
        "authors": "Karen Grigorian and Robert A. Jarrow",
        "title": "Enlargement of Filtrations: An Exposition of Core Ideas with Financial\n  Examples",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper we provide an exhaustive survey of the current state of the\nmathematics of filtration enlargement and an interpretation of the key results\nof the literature from the viewpoint of mathematical finance. The emphasis is\non providing a well-structured compendium of known mathematical results that\ncan be used by researchers in mathematical finance. We mainly state the results\nand discuss their role and significance, with references provided for the\nomitted proofs. The discussion of mathematical results is accompanied by\nnumerous examples from mathematical finance.\n"
    },
    {
        "paper_id": 2303.03661,
        "authors": "Manuela M. Dantas, Kenneth J. Merkley, Felipe B. G. Silva",
        "title": "Government Guarantees and Banks' Income Smoothing",
        "comments": "46 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  We propose four channels through which government guarantees affect banks'\nincentives to smooth income. Empirically, we exploit two complementary settings\nthat represent plausible exogenous changes in government guarantees: the\nincrease in implicit guarantees following the creation of the Eurozone and the\nremoval of explicit guarantees granted to the Landesbanken. We show that\nincreases (decreases) in government guarantees are associated with significant\ndecreases (increases) in banks' income smoothing. Taken together, our results\nlargely corroborate the predominance of a tail-risk channel, wherein government\nguarantees reduce banks' tail risk, thereby reducing managers' incentives to\nengage in income smoothing.\n"
    },
    {
        "paper_id": 2303.04101,
        "authors": "Md Deluair Hossen",
        "title": "Exchange Rate Pass-Through and Data Frequency: Firm-Level Evidence from\n  Bangladesh",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  The vast literature on exchange rate fluctuations estimates the exchange rate\npass-through (ERPT). Most ERPT studies consider annually aggregated data for\ndeveloped or large developing countries for estimating ERPT. These estimates\nvary widely depending on the type of country, data coverage, and frequency.\nHowever, the ERPT estimation using firm-level high-frequency export data of a\nsmall developing country is rare. In this paper, I estimate the pricing to\nmarket and the exchange rate pass-through at a monthly, quarterly, and annual\nlevel of data frequency to deal with aggregation bias. Furthermore, I\ninvestigate how delivery time-based factors such as frequent shipments and\nfaster transport affect a firm`s pricing-to-market behavior. Using\ntransaction-level export data of Bangladesh from 2005 to 2013 and the Poisson\nPseudo Maximum Likelihood (PPML) estimation method, I find very small pricing\nto the markets to the exchange rates in the exporter's price. As pass-through\nshows how the exporters respond to macro shocks, for Bangladesh, this low\nexport price response to the exchange rate changes indicates that currency\ndevaluation might not have a significant effect on the exporter. The minimal\nprice response and high pass-through contrast with the literature on incomplete\npass-through at the annual level. By considering the characteristics of the\nfirms, products, and destinations, I investigate the heterogeneity of the\npass-through. The findings remain consistent with several robustness checks.\n"
    },
    {
        "paper_id": 2303.04204,
        "authors": "Qingyi Wang, Shenhao Wang, Yunhan Zheng, Hongzhou Lin, Xiaohu Zhang,\n  Jinhua Zhao, Joan Walker",
        "title": "Deep hybrid model with satellite imagery: how to combine demand modeling\n  and computer vision for behavior analysis?",
        "comments": null,
        "journal-ref": "Transportation Research Part B: Methodological, Volume 179, 2024,\n  102869",
        "doi": "10.1016/j.trb.2023.102869",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Classical demand modeling analyzes travel behavior using only low-dimensional\nnumeric data (i.e. sociodemographics and travel attributes) but not\nhigh-dimensional urban imagery. However, travel behavior depends on the factors\nrepresented by both numeric data and urban imagery, thus necessitating a\nsynergetic framework to combine them. This study creates a theoretical\nframework of deep hybrid models with a crossing structure consisting of a\nmixing operator and a behavioral predictor, thus integrating the numeric and\nimagery data into a latent space. Empirically, this framework is applied to\nanalyze travel mode choice using the MyDailyTravel Survey from Chicago as the\nnumeric inputs and the satellite images as the imagery inputs. We found that\ndeep hybrid models outperform both the traditional demand models and the recent\ndeep learning in predicting the aggregate and disaggregate travel behavior with\nour supervision-as-mixing design. The latent space in deep hybrid models can be\ninterpreted, because it reveals meaningful spatial and social patterns. The\ndeep hybrid models can also generate new urban images that do not exist in\nreality and interpret them with economic theory, such as computing substitution\npatterns and social welfare changes. Overall, the deep hybrid models\ndemonstrate the complementarity between the low-dimensional numeric and\nhigh-dimensional imagery data and between the traditional demand modeling and\nrecent deep learning. It generalizes the latent classes and variables in\nclassical hybrid demand models to a latent space, and leverages the\ncomputational power of deep learning for imagery while retaining the economic\ninterpretability on the microeconomics foundation.\n"
    },
    {
        "paper_id": 2303.04223,
        "authors": "Md Deluair Hossen",
        "title": "Financing Costs, Per-Shipment Costs and Shipping Frequency: Firm-Level\n  Evidence from Bangladesh",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  In international trade, firms face lengthy ordering-producing-delivery times\nand make shipping frequency decisions based on the per-shipment costs and\nfinancing costs. In this paper, I develop a model of importer-exporter\nprocurement where the importer procures international inputs from exporting\nfirms in developing countries. The exporters are credit constrained for working\ncapital, incur the per-shipment fixed costs, and get paid after goods are\ndelivered to the importer. The model shows that the shipping frequency\nincreases for high financing costs in origin and destination. Furthermore,\nlonger delivery times increase shipping frequency as well as procurement costs.\nThe model also shows that the higher per-shipment fixed costs reduce the\nshipping frequency, in line with previous literature. Reduced transaction costs\nlower the exporter's demand for financial services through shipping frequency\nadjustment, mitigating the financial frictions of the firm. Then, I empirically\ninvestigate whether the conclusions regarding the effect of per-shipment fixed\ncosts on shipping frequency from the theoretical model and in the existing\nliterature extend to developing countries. My estimation method addresses\nseveral biases. First, I deal with aggregation bias with the firm, product, and\ncountry-level analysis. Second, I consider the Poisson Pseudo Maximum\nLikelihood (PPML) estimation method to deal with heteroscedasticity bias from\nthe OLS estimation of log-linear models. Third, I fix the distance\nnon-linearity of Bangladeshi exports. Finally, I consider the effect of\nfinancing cost on shipping frequency to address omitted variable bias. Using\ntransaction-level export data from Bangladesh, I find that 10% higher\nper-shipment costs reduce the shipping frequency by 3.45%. The findings are\nrobust to different specifications and subsamples.\n"
    },
    {
        "paper_id": 2303.04236,
        "authors": "Hugo E. Ramirez and Rafael Serrano",
        "title": "Optimal investment with insurable background risk and nonlinear\n  portfolio allocation frictions",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  We study investment and insurance demand decisions for an agent in a\ntheoretical continuous-time expected utility maximization model that combines\nrisky assets with an (exogenous) insurable background risk. This risk takes the\nform of a jump-diffusion process with negative jumps in the return rate of the\n(self-financed) wealth. The main distinctive feature of our model is that the\nagent's decision on portfolio choice and insurance demand causes nonlinear\nfriction in the dynamics of the wealth process. We use the dynamic programming\napproach to find optimality conditions under which the agent assumes the\ninsurable risk entirely, or partially, or purchases total insurance against it.\nIn particular, we consider differential and piece-wise linear portfolio\nallocation frictions, with differential borrowing and lending rates as our most\nemblematic example. Finally, we present a mutual-fund separation result and\nillustrate our results with several numerical examples when the adverse jump\nrisk has Beta distribution.\n"
    },
    {
        "paper_id": 2303.04539,
        "authors": "Riccardo Leoncini, Mariele Macaluso, Annalivia Polselli",
        "title": "Gender Segregation: Analysis across Sectoral-Dominance in the UK Labour\n  Market",
        "comments": "22 pages, 6 tables and 5 figures in text",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/publicdomain/zero/1.0/",
        "abstract": "  This paper aims to evaluate how changing patterns of sectoral gender\nsegregation play a role in accounting for women's employment contracts and\nwages in the UK between 2005 and 2020. We then study wage differentials in\ngender-specific dominated sectors. We found that the propensity of women to be\ndistributed differently across sectors is a major factor contributing to\nexplaining the differences in wages and contract opportunities. Hence, the\ndisproportion of women in female-dominated sectors implies contractual features\nand lower wages typical of that sector, on average, for all workers. This\ndifference is primarily explained by \"persistent discriminatory constraints\",\nwhile human capital-related characteristics play a minor role. However, wage\ndifferentials would shrink if workers had the same potential and residual wages\nas men in male-dominated sectors. Moreover, this does not happen at the top of\nthe wage distribution, where wage differentials among women working in\nfemale-dominated sectors are always more pronounced than those of men.\n"
    },
    {
        "paper_id": 2303.04581,
        "authors": "Fuquan Tang",
        "title": "Application of supervised learning models in the Chinese futures market",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-sa/4.0/",
        "abstract": "  Based on the characteristics of the Chinese futures market, this paper builds\na supervised learning model to predict the trend of futures prices and then\ndesigns a trading strategy based on the prediction results. The Precision,\nRecall and F1-score of the classification problem show that our model can meet\nthe accuracy requirements for the classification of futures price movements in\nterms of test data. The backtest results show that our trading system has an\nupward trending return curve with low capital retracement.\n"
    },
    {
        "paper_id": 2303.04642,
        "authors": "Hakan Pabuccu, Serdar Ongan and Ayse Ongan",
        "title": "Forecasting the movements of Bitcoin prices: an application of machine\n  learning algorithms",
        "comments": "14 pages, 2 figures and 15 tables",
        "journal-ref": null,
        "doi": "10.3934/QFE.2020031",
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Cryptocurrencies, such as Bitcoin, are one of the most controversial and\ncomplex technological innovations in today's financial system. This study aims\nto forecast the movements of Bitcoin prices at a high degree of accuracy. To\nthis aim, four different Machine Learning (ML) algorithms are applied, namely,\nthe Support Vector Machines (SVM), the Artificial Neural Network (ANN), the\nNaive Bayes (NB) and the Random Forest (RF) besides the logistic regression\n(LR) as a benchmark model. In order to test these algorithms, besides existing\ncontinuous dataset, discrete dataset was also created and used. For the\nevaluations of algorithm performances, the F statistic, accuracy statistic, the\nMean Absolute Error (MAE), the Root Mean Square Error (RMSE) and the Root\nAbsolute Error (RAE) metrics were used. The t test was used to compare the\nperformances of the SVM, ANN, NB and RF with the performance of the LR.\nEmpirical findings reveal that, while the RF has the highest forecasting\nperformance in the continuous dataset, the NB has the lowest. On the other\nhand, while the ANN has the highest and the NB the lowest performance in the\ndiscrete dataset. Furthermore, the discrete dataset improves the overall\nforecasting performance in all algorithms (models) estimated.\n"
    },
    {
        "paper_id": 2303.04688,
        "authors": "Yanci Zhang, Mengjia Xia, Mingyang Li, Haitao Mao, Yutong Lu, Yupeng\n  Lan, Jinlin Ye, Rui Dai",
        "title": "Form 10-K Itemization",
        "comments": "For demo website, see http://review10-k.ddns.net",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  Form 10-K report is a financial report disclosing the annual financial state\nof a public company. It is an important evidence to conduct financial analysis,\ni.e., asset pricing, corporate finance. Practitioners and researchers are\nconstantly designing algorithms to better conduct analysis on information in\nthe Form 10-K report. The vast majority of previous works focus on quantitative\ndata. With recent advancement on natural language processing (NLP), textual\ndata in financial filing attracts more attention. However, to incorporate\ntextual data for analyzing, Form 10-K Itemization is a necessary pre-process\nstep. It aims to segment the whole document into several Item sections, where\neach Item section focuses on a specific financial aspect of the company. With\nthe segmented Item sections, NLP techniques can directly apply on those Item\nsections related to downstream tasks. In this paper, we develop a Form 10-K\nItemization system which can automatically segment all the Item sections in\n10-K documents. The system is both effective and efficient. It reaches a\nretrieval rate of 93%.\n"
    },
    {
        "paper_id": 2303.04812,
        "authors": "Xize Wang, John L. Renne",
        "title": "Socioeconomics of Urban Travel in the U.S.: Evidence from the 2017 NHTS",
        "comments": null,
        "journal-ref": "Transportation Research Part D: Transport and Environment. 103622",
        "doi": "10.1016/j.trd.2023.103622",
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Using the 2017 National Household Travel Survey (NHTS), this study analyzes\nAmerica's urban travel trends compared with earlier nationwide travel surveys,\nand examines the variations in travel behaviors among a range of socioeconomic\ngroups. The most noticeable trend for the 2017 NHTS is that although private\nautomobiles continue to be the dominant travel mode in American cities, the\nshare of car trips has slightly and steadily decreased since its peak in 2001.\nIn contrast, the share of transit, non-motorized, and taxicab (including\nride-hailing) trips has steadily increased. Besides this overall trend, there\nare important variations in travel behaviors across income, home ownership,\nethnicity, gender, age, and life-cycle stages. Although the trends in transit\ndevelopment, shared mobility, e-commerce, and lifestyle changes offer optimism\nabout American cities becoming more multimodal, policymakers should consider\nthese differences in socioeconomic factors and try to provide more equitable\naccess to sustainable mobility across different socioeconomic groups.\n"
    },
    {
        "paper_id": 2303.04833,
        "authors": "Ruitu Xu, Yifei Min, Tianhao Wang, Zhaoran Wang, Michael I. Jordan,\n  Zhuoran Yang",
        "title": "Finding Regularized Competitive Equilibria of Heterogeneous Agent\n  Macroeconomic Models with Reinforcement Learning",
        "comments": "44 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  We study a heterogeneous agent macroeconomic model with an infinite number of\nhouseholds and firms competing in a labor market. Each household earns income\nand engages in consumption at each time step while aiming to maximize a concave\nutility subject to the underlying market conditions. The households aim to find\nthe optimal saving strategy that maximizes their discounted cumulative utility\ngiven the market condition, while the firms determine the market conditions\nthrough maximizing corporate profit based on the household population behavior.\nThe model captures a wide range of applications in macroeconomic studies, and\nwe propose a data-driven reinforcement learning framework that finds the\nregularized competitive equilibrium of the model. The proposed algorithm enjoys\ntheoretical guarantees in converging to the equilibrium of the market at a\nsub-linear rate.\n"
    },
    {
        "paper_id": 2303.04905,
        "authors": "Anna Naszodi",
        "title": "Direct comparison or indirect comparison via a series of counterfactual\n  decompositions?",
        "comments": "arXiv admin note: text overlap with arXiv:2103.06991",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We illustrate the point with an empirical analysis of assortative mating in\nthe US, namely, that the outcome of comparing two distant groups can be\nsensitive to whether comparing the groups directly, or indirectly via a series\nof counterfactual decompositions involving the groups' comparisons to some\nintermediate groups. We argue that the latter approach is typically more fit\nfor its purpose.\n"
    },
    {
        "paper_id": 2303.0538,
        "authors": "Minou Goetze, Christina Herdt, Ricarda Conrad, Stephan Stricker",
        "title": "Preferences and Attitudes towards Debt Collection: A Cross-Generational\n  Investigation",
        "comments": "27 pages, 9 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  Preliminary research indicated that an increasing number of young adults end\nup in debt collection. Yet, debt collection agencies (DCAs) are still lacking\nknowledge on how to approach these consumers. A large-scale mixed-methods\nsurvey of consumers in Germany (N = 996) was conducted to investigate\npreference shifts from traditional to digital payment, and communication\nchannels; and attitude shifts towards financial institutions. Our results show\nthat, indeed, younger consumers are more likely to prefer digital payment\nmethods (e.g., Paypal, Apple Pay), while older consumers are more likely to\nprefer traditional payment methods such as manual transfer. In the case of\ncommunication channels, we found that older consumers were more likely to\nprefer letters than younger consumers. Additional factors that had an influence\non payment and communication preferences include gender, income and living in\nan urban area. Finally, we observed attitude shifts of younger consumers by\nexhibiting more openness when talking about their debt than older consumers. In\nsummary, our findings show that consumers' preferences are influenced by\nindividual differences, specifically age, and we discuss how DCAs can leverage\nthese insights to optimize their processes.\n"
    },
    {
        "paper_id": 2303.05421,
        "authors": "Fallou Niakh",
        "title": "A fixed point approach for computing actuarially fair Pareto optimal\n  risk-sharing rules",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Risk-sharing is one way to pool risks without the need for a third party. To\nensure the attractiveness of such a system, the rule should be accepted and\nunderstood by all participants. A desirable risk-sharing rule should fulfill\nactuarial fairness and Pareto optimality while being easy to compute. This\npaper establishes a one-to-one correspondence between an actuarially fair\nPareto optimal (AFPO) risk-sharing rule and a fixed point of a specific\nfunction. A fast numerical method for computing these risk-sharing rules is\nalso derived. As a result, we are able to compute AFPO risk-sharing rules for a\nlarge number of heterogeneous participants in this framework.\n"
    },
    {
        "paper_id": 2303.05427,
        "authors": "Oguz Koc, Omur Ugur, A. Sevtap Kestel",
        "title": "The Impact of Feature Selection and Transformation on Machine Learning\n  Methods in Determining the Credit Scoring",
        "comments": null,
        "journal-ref": "Oguz Koc. Comparison of Machine Learning Algorithms on Consumer\n  Credit Classification. M.Sc. Thesis, Middle East Technical University, 2019",
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Banks utilize credit scoring as an important indicator of financial strength\nand eligibility for credit. Scoring models aim to assign statistical odds or\nprobabilities for predicting if there is a risk of nonpayment in relation to\nmany other factors which may be involved in. This paper aims to illustrate the\nbeneficial use of the eight machine learning (ML) methods (Support Vector\nMachine, Gaussian Naive Bayes, Decision Trees, Random Forest, XGBoost,\nK-Nearest Neighbors, Multi-layer Perceptron Neural Networks) and Logistic\nRegression in finding the default risk as well as the features contributing to\nit. An extensive comparison is made in three aspects: (i) which ML models with\nand without its own wrapper feature selection performs the best; (ii) how\nfeature selection combined with appropriate data scaling method influences the\nperformance; (iii) which of the most successful combination (algorithm, feature\nselection, and scaling) delivers the best validation indicators such as\naccuracy rate, Type I and II errors and AUC. An open-access credit scoring\ndefault risk data sets on German and Australian cases are taken into account,\nfor which we determine the best method, scaling, and features contributing to\ndefault risk best and compare our findings with the literature ones in related.\nWe illustrate the positive contribution of the selection method and scaling on\nthe performance indicators compared to the existing literature.\n"
    },
    {
        "paper_id": 2303.05432,
        "authors": "Anwesha Sengupta, Shashankaditya Upadhyay, Indranil Mukherjee and\n  Prasanta K. Panigrahi",
        "title": "Describing the effect of influential spreaders on the different sectors\n  of Indian market: a complex networks perspective",
        "comments": "17 pages, 21 figures",
        "journal-ref": "J Comput Soc Sc (2023) 1-41",
        "doi": "10.1007/s42001-023-00229-4",
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  Market competition has a role which is directly or indirectly associated with\ninfluential effects of individual sectors on other sectors of the economy. The\npresent work studies the relative position of a product in the market through\nthe identification of influential spreaders and its corresponding effect on the\nother sectors of the market using complex network analysis during the pre-,\nin-, and post-crisis induced lockdown periods using daily data of NSE from\nDecember, 2019 to June, 2021. The existing approaches using different\ncentrality measures failed to distinguish between the positive and negative\ninfluences of the different sectors in the market which act as spreaders. To\nobviate this problem, this paper presents an effective measure called LIEST\n(Local Influential Effects for Specific Target) that can examine the positive\nand negative influences separately with respect to any crisis period. LIEST\nconsiders the combined impact of all possible nodes which are at most three\nsteps away from the specific targets for the networks. The essence of\nnon-linearity in the network dynamics without considering single node effect\nbecomes visible particularly in the proposed network.\n"
    },
    {
        "paper_id": 2303.05515,
        "authors": "Anna Naszodi",
        "title": "The iterative proportional fitting algorithm and the NM-method:\n  solutions for two different sets of problems",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper, we identify two different sets of problems. The first covers\nthe problems that the iterative proportional fitting (IPF) algorithm was\ndeveloped to solve. These concern completing a population table by using a\nsample. The other set concerns constructing a counterfactual population table\nwith the purpose of comparing two populations. The IPF is commonly applied by\nsocial scientists to solve problems not only in the first set, but also in the\nsecond one. We show that while it is legitimate to use the IPF for the first\nset of problems, it is not the right tool to address the problems of the second\nkind. We promote an alternative of the IPF, the NM-method, for solving problems\nin the second set. We provide both theoretical and empirical comparisons of\nthese methods.\n"
    },
    {
        "paper_id": 2303.05654,
        "authors": "Thilini V. Mahanama, Abootaleb Shirvani, Svetlozar Rachev",
        "title": "The Financial Market of Indices of Socioeconomic Wellbeing",
        "comments": "33 pages, 20 figures",
        "journal-ref": null,
        "doi": "10.13140/RG.2.2.20393.06247",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The financial industry should be involved in mitigating the risk of downturns\nin the financial wellbeing indices around the world by implementing\nwell-developed financial tools such as insurance instruments on the underlying\nwellbeing indices. We define a new quantitative measure of the wellbeing of a\ncountry's population for those countries using the world development indicators\nprovided by the World Bank. We monetize the indices of socioeconomic wellbeing,\nwhich serve as \"risky assets,\" and consequently develop a global financial\nmarket for them, which serves as a \"market of indices of socioeconomic\nwellbeing.\" Then, we compare the wellbeing of different countries using\nfinancial econometric analysis and dynamic asset pricing theory. We provide the\noptimal portfolio weight composition along with the efficient frontiers of the\nwellbeing socioeconomic indices with different risk-return measures. We derive\ninsurance instruments, such as put options, which allow the financial industry\nto monitor, manage, and trade these indices, creating the funds for insurance\nagainst adverse movements of those indices. Our findings should help financial\ninstitutions to incorporate socioeconomic issues as an additional dimension to\ntheir \"two-dimensional\" risk-return adjusted optimal financial portfolios.\n"
    },
    {
        "paper_id": 2303.05666,
        "authors": "Xiaobin Tang and Nuo Lei",
        "title": "Research on CPI Prediction Based on Natural Language Processing",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  In the past, the seed keywords for CPI prediction were often selected based\non empirical summaries of research and literature studies, which were prone to\nselect omitted and invalid variables. In this paper, we design a keyword\nexpansion technique for CPI prediction based on the cutting-edge NLP model,\nPANGU. We improve the CPI prediction ability using the corresponding web search\nindex. Compared with the unsupervised pre-training and supervised downstream\nfine-tuning natural language processing models such as BERT and NEZHA, the\nPANGU model can be expanded to obtain more reliable CPI-generated keywords by\nits excellent zero-sample learning capability without the limitation of the\ndownstream fine-tuning data set. Finally, this paper empirically tests the\nkeyword prediction ability obtained by this keyword expansion method with\nhistorical CPI data.\n"
    },
    {
        "paper_id": 2303.05783,
        "authors": "Guanxing Fu and Paul P. Hager and Ulrich Horst",
        "title": "Mean-Field Liquidation Games with Market Drop-out",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  We consider a novel class of portfolio liquidation games with market drop-out\n(\"absorption\"). More precisely, we consider mean-field and finite player\nliquidation games where a player drops out of the market when her position hits\nzero. In particular round-trips are not admissible. This can be viewed as a no\nstatistical arbitrage condition. In a model with only sellers we prove that the\nabsorption condition is equivalent to a short selling constraint. We prove that\nequilibria (both in the mean-field and the finite player game) are given as\nsolutions to a non-linear higher-order integral equation with endogenous\nterminal condition. We prove the existence of a unique solution to the integral\nequation from which we obtain the existence of a unique equilibrium in the MFG\nand the existence of a unique equilibrium in the $N$-player game. We establish\nthe convergence of the equilibria in the finite player games to the obtained\nmean-field equilibrium and illustrate the impact of the drop-out constraint on\nequilibrium trading rates.\n"
    },
    {
        "paper_id": 2303.05895,
        "authors": "Anna Naszodi",
        "title": "What do surveys say about the historical trend of inequality and the\n  applicability of two table-transformation methods?",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We apply a pseudo panel analysis of survey data from the years 2010 and 2017\nabout Americans' self-reported marital preferences and perform some formal\ntests on the sign and magnitude of the change in educational homophily from the\ngeneration of the early Boomers to the late Boomers, as well as from the early\nGenerationX to the late GenerationX. In the analysis, we control for changes in\npreferences over the course of the survey respondents' lives. We use the test\nresults to decide whether the popular iterative proportional fitting (IPF)\nalgorithm, or its alternative, the NM-method is more suitable for analyzing\nrevealed marital preferences. These two methods construct different tables\nrepresenting counterfactual joint educational distributions of couples.\nThereby, they disagree on the trend of revealed preferences identified from the\nprevalence of homogamy by counterfactual decompositions. By finding\nself-reported homophily to display a U-shaped pattern, our tests reject the\nhypothesis that the IPF is suitable for constructing counterfactuals in\ngeneral, while we cannot reject the applicability of the NM. The significance\nof our survey-based method-selection is due to the fact that the choice between\nthe IPF and the NM makes a difference not only to the identified historical\ntrend of revealed homophily, but also to what future paths of social inequality\nare believed to be possible.\n"
    },
    {
        "paper_id": 2303.05985,
        "authors": "David McCune",
        "title": "Ranked Choice Bedlam in a 2022 Oakland School Director Election",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The November 2022 ranked choice election for District 4 School Director in\nOakland, CA, was very interesting from the perspective of social choice theory.\nThe election did not contain a Condorcet winner and exhibited downward and\nupward monotonicity paradoxes, for example. Furthermore, an error in the\nsettings of the ranked choice tabulation software led to the wrong candidate\nbeing declared the winner. This article explores the strange features of this\nelection and places it in the broader context of ranked choice elections in the\nUnited States.\n"
    },
    {
        "paper_id": 2303.06051,
        "authors": "Andrea Barbon and Angelo Ranaldo",
        "title": "NFT Bubbles",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  By investigating nonfungible tokens (NFTs), we provide the first systematic\nstudy of retail investor behavior through asset bubbles. Given that NFTs are\nrecorded in public blockchains, we are able to track investor behavior over\ntime, leading to the identification of numerous price run-ups and crashes. Our\nstudy reveals that agent-level variables, such as investor sophistication,\nheterogeneity, and wash trading, in addition to aggregate variables, such as\nvolatility, price acceleration, and turnover, significantly predict bubble\nformation and price crashes. We find that sophisticated investors consistently\noutperform others and exhibit characteristics consistent with superior\ninformation and skills, supporting the narrative surrounding asset pricing\nbubbles.\n"
    },
    {
        "paper_id": 2303.06089,
        "authors": "Alberto Manzano, Gonzalo Ferro, \\'Alvaro Leitao, Carlos V\\'azquez,\n  Andr\\'es G\\'omez",
        "title": "Real Option Pricing using Quantum Computers",
        "comments": "32 pages, 15 figures, 3 tables",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  In this work we present an alternative methodology to the standard Quantum\nAccelerated Monte Carlo (QAMC) applied to derivatives pricing. Our pipeline\nbenefits from the combination of a new encoding protocol, referred to as the\ndirect encoding, and a amplitude estimation algorithm, the modified Real\nQuantum Amplitude Estimation (mRQAE) algorithm. On the one hand, the direct\nencoding prepares a quantum state which contains the information about the sign\nof the expected payoff. On the other hand, the mRQAE is able to read all the\ninformation contained in the quantum state. Although the procedure we describe\nis different from the standard one, the main building blocks are almost the\nsame. Thus, all the extensive research that has been performed is still\napplicable. Moreover, we experimentally compare the performance of the proposed\nmethodology against the standard QAMC employing a quantum emulator and show\nthat we retain the speedups.\n"
    },
    {
        "paper_id": 2303.06097,
        "authors": "David Audretsch, Christian Fisch, Chiara Franzoni, Paul P. Momtaz,\n  Silvio Vismara",
        "title": "Academic Freedom and Innovation: A Research Note",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  The first-ever article published in Research Policy was Casimir's (1971)\nadvocacy of academic freedom in light of the industry's increasing influence on\nresearch in universities. Half a century later, the literature attests to the\ndearth of work on the role of academic freedom for innovation. To fill this\ngap, we employ instrumental variable techniques to identify the impact of\nacademic freedom on the quantity (patent applications) and quality (patent\ncitations) of innovation output. The empirical evidence suggests that improving\nacademic freedom by one standard deviation increases patent applications and\nforward citations by 41% and 29%, respectively. The results hold in a\nrepresentative sample of 157 countries over the 1900-2015 period. This research\nnote is also an alarming plea to policymakers: Global academic freedom has\ndeclined over the past decade for the first time in the last century. Our\nestimates suggest that the decline of academic freedom has resulted in a global\nloss quantifiable with at least 4.0% fewer patents filed and 5.9% fewer patent\ncitations.\n"
    },
    {
        "paper_id": 2303.06106,
        "authors": "Richard S.J. Tol",
        "title": "The Nobel Family",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Nobel laureates cluster together. 696 of the 727 winners of the Nobel Prize\nin physics, chemistry, medicine, and economics belong to one single academic\nfamily tree. 668 trace their ancestry to Emmanuel Stupanus, 228 to Lord\nRayleigh (physics, 1904). Craig Mello (medicine, 2006) counts 51 Nobelists\namong his ancestors. Chemistry laureates have the most Nobel ancestors and\ndescendants, economics laureates the fewest. Chemistry is the central\ndiscipline. Its Nobelists have trained and are trained by Nobelists in other\nfields. Nobelists in physics (medicine) have trained (by) others. Economics\nstands apart. Openness to other disciplines is the same in recent and earlier\ntimes. The familial concentration of Nobelists is lower now than it used to be.\n"
    },
    {
        "paper_id": 2303.06148,
        "authors": "Andrius Grigutis",
        "title": "Probabilistic Overview of Probabilities of Default for Low Default\n  Portfolios by K. Pluto and D. Tasche",
        "comments": null,
        "journal-ref": null,
        "doi": "10.17721/1812-5409.2023/2.7",
        "license": "http://creativecommons.org/publicdomain/zero/1.0/",
        "abstract": "  This article gives a probabilistic overview of the widely used method of\ndefault probability estimation proposed by K. Pluto and D. Tasche. There are\nlisted detailed assumptions and derivation of the inequality where the\nprobability of default is involved under the influence of systematic factor.\nThe author anticipates adding more clarity, especially for early career\nanalysts or scholars, regarding the assumption of borrowers' independence,\nconditional independence and interaction between the probability distributions\nsuch as binomial, beta, normal and others. There is also shown the relation\nbetween the probability of default and the joint distribution of\n$\\sqrt{\\varrho}X-\\sqrt{1-\\varrho}Y$, where $X$, including but not limiting, is\nthe standard normal, $Y$ admits, including but not limiting, the beta-normal\ndistribution and $X,\\,Y$ are independent.\n"
    },
    {
        "paper_id": 2303.06217,
        "authors": "Manav Raj, Justin Berg, Rob Seamans",
        "title": "Art-ificial Intelligence: The Effect of AI Disclosure on Evaluations of\n  Creative Content",
        "comments": "We are currently expanding the scope of the project and working on a\n  revision. The current findings do not comprehensively and accurately reflect\n  what the new data suggests",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The emergence of generative AI technologies, such as OpenAI's ChatGPT\nchatbot, has expanded the scope of tasks that AI tools can accomplish and\nenabled AI-generated creative content. In this study, we explore how disclosure\nregarding the use of AI in the creation of creative content affects human\nevaluation of such content. In a series of pre-registered experimental studies,\nwe show that AI disclosure has no meaningful effect on evaluation either for\ncreative or descriptive short stories, but that AI disclosure has a negative\neffect on evaluations for emotionally evocative poems written in the first\nperson. We interpret this result to suggest that reactions to AI-generated\ncontent may be negative when the content is viewed as distinctly \"human.\" We\ndiscuss the implications of this work and outline planned pathways of research\nto better understand whether and when AI disclosure may affect the evaluation\nof creative content.\n"
    },
    {
        "paper_id": 2303.06603,
        "authors": "Silvia Bartolucci, Fabio Caccioli, Francesco Caravelli, Pierpaolo Vivo",
        "title": "Correlation between upstreamness and downstreamness in random global\n  value chains",
        "comments": "14 pages, 10 figures. Added experiments about random reshuffling of\n  I-O tables",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper is concerned with upstreamness and downstreamness of industries\nand countries in global value chains. Upstreamness and downstreamness measure\nrespectively the average distance of an industrial sector from final\nconsumption and from primary inputs, and they are computed from based on the\nmost used global Input-Output tables databases, e.g., the World Input-Output\nDatabase (WIOD). Recently, Antr\\`as and Chor reported a puzzling and\ncounter-intuitive finding in data from the period 1995-2011, namely that (at\ncountry level) upstreamness appears to be positively correlated with\ndownstreamness, with a correlation slope close to $+1$. This effect is stable\nover time and across countries, and it has been confirmed and validated by\nlater analyses. We first analyze a simple model of random Input/Output tables,\nand we show that, under minimal and realistic structural assumptions, there is\na natural positive correlation emerging between upstreamness and downstreamness\nof the same industrial sector/country, with correlation slope equal to $+1$.\nThis effect is robust against changes in the randomness of the entries of the\nI/O table and different aggregation protocols. Secondly, we perform experiments\nby randomly reshuffling the entries of the empirical I/O table where these\npuzzling correlations are detected, in such a way that the global structural\nconstraints are preserved. Again, we find that the upstreamness and\ndownstreamness of the same industrial sector/country are positively correlated\nwith slope close to $+1$, even though the random reshuffling has destroyed any\nunderlying economic information about inter-sectorial connections and trends.\nOur results strongly suggest that the empirically observed puzzling correlation\nmay rather be a necessary consequence of the few structural constraints that\nInput/Output tables must meet.\n"
    },
    {
        "paper_id": 2303.06701,
        "authors": "Job Boerma and Aleh Tsyvinski and Ruodu Wang and Zhenyuan Zhang",
        "title": "Composite Sorting",
        "comments": "81 pages, 26 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We propose a new sorting framework: composite sorting. Composite sorting\ncomprises of (1) distinct worker types assigned to the same occupation, and (2)\na given worker type simultaneously being part of both positive and negative\nsorting. Composite sorting arises when fixed investments mitigate variable\ncosts of mismatch. We completely characterize optimal sorting and additionally\nshow it is more positive when mismatch costs are less concave. We then\ncharacterize equilibrium wages. Wages have a regional hierarchical structure -\nrelative wages depend solely on sorting within skill groups. Quantitatively,\ncomposite sorting can generate a sizable portion of within-occupations wage\ndispersion in the US.\n"
    },
    {
        "paper_id": 2303.07044,
        "authors": "Oleksandr Rossolov, Yusak O. Susilo",
        "title": "Are consumers ready to pay extra for crowd-shipping e-groceries and why?\n  A hybrid choice analysis for developing economies",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper presents the behavioral study's results on willingness-to-pay the\nextra money by the customers for e-groceries deliveries based on\ncrowd-shipping. The proposed methodology was tested for Ukraine, i.e., a\ndeveloping country where the crowd-shipping services are under development\nconditions. To account for the behavior complexity of the consumers who have\nnot faced the crowd-shipping services in the past, the choice model was\nenhanced with a latent variable. The findings indicate the revealed readiness\nof the e-shoppers to pay extra money for crowd-shipping delivery if it provides\nmore flexible and consumer-oriented service. The expected environmental impact\nof the crowd-shipping delivery was not considered as important by the\ne-shoppers, which is explained by low concerns about the environment and\ncar-oriented mobility in the considered case study.\n"
    },
    {
        "paper_id": 2303.07158,
        "authors": "Sungchul Hong and Jong-June Jeon",
        "title": "Uniform Pessimistic Risk and its Optimal Portfolio",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
        "abstract": "  The optimal allocation of assets has been widely discussed with the\ntheoretical analysis of risk measures, and pessimism is one of the most\nattractive approaches beyond the conventional optimal portfolio model. The\n$\\alpha$-risk plays a crucial role in deriving a broad class of pessimistic\noptimal portfolios. However, estimating an optimal portfolio assessed by a\npessimistic risk is still challenging due to the absence of a computationally\ntractable model. In this study, we propose an integral of $\\alpha$-risk called\nthe \\textit{uniform pessimistic risk} and the computational algorithm to obtain\nan optimal portfolio based on the risk. Further, we investigate the theoretical\nproperties of the proposed risk in view of three different approaches: multiple\nquantile regression, the proper scoring rule, and distributionally robust\noptimization. Real data analysis of three stock datasets (S\\&P500, CSI500,\nKOSPI200) demonstrates the usefulness of the proposed risk and portfolio model.\n"
    },
    {
        "paper_id": 2303.07222,
        "authors": "Eduardo Abi Jaber, Nathan De Carvalho",
        "title": "Reconciling rough volatility with jumps",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  We reconcile rough volatility models and jump models using a class of\nreversionary Heston models with fast mean reversions and large vol-of-vols.\nStarting from hyper-rough Heston models with a Hurst index $H \\in (-1/2,1/2)$,\nwe derive a Markovian approximating class of one dimensional reversionary\nHeston-type models. Such proxies encode a trade-off between an exploding\nvol-of-vol and a fast mean-reversion speed controlled by a reversionary\ntime-scale $\\epsilon>0$ and an unconstrained parameter $H \\in \\mathbb R$.\nSending $\\epsilon$ to 0 yields convergence of the reversionary Heston model\ntowards different explicit asymptotic regimes based on the value of the\nparameter H. In particular, for $H \\leq -1/2$, the reversionary Heston model\nconverges to a class of L\\'evy jump processes of Normal Inverse Gaussian type.\nNumerical illustrations show that the reversionary Heston model is capable of\ngenerating at-the-money skews similar to the ones generated by rough,\nhyper-rough and jump models.\n"
    },
    {
        "paper_id": 2303.07244,
        "authors": "Muhammad Aufaristama",
        "title": "The Stock Price Relationship between Holding Companies and Subsidiaries:\n  A Case study of Indonesia Multiholding Companies",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  This study aimed to examine the correlation between the stock prices of two\nmajor Indonesian holding companies, MNC Group and Elang Mahkota Teknologi\n(Emtek) Group, and their respective subsidiaries as case studies. The data for\nthe analysis were collected from 2013 to 2022, and Spearman correlation was\nused to determine the strength and direction of the relationship between the\nstock prices of the holding companies and their subsidiaries. The results of\nthe analysis revealed that there were varying degrees of correlation between\nthe stock prices of the holding companies and their subsidiaries. The strongest\npositive correlation was observed between BHIT and BMTR, while the weakest\ncorrelations were found between BHIT and IPTV, and BHIT and MSIN. The\ncorrelations were also found to have changed over time, possibly due to market\nconditions, company-specific events, or changes in industry sectors.In the case\nof Emtek Group, the analysis suggested that EMTK's stock price movements had a\nsignificant impact on the stock prices of its subsidiaries, with varying\nstrengths of relationships. The negative correlation between EMTK and SCMA over\nthe entire period suggested an inverse relationship, while positive\ncorrelations with BUKA, AMOR, BBHI, and RSGK indicated a tendency to move in\nthe same direction as EMTK's stock price. The correlations were found to have\nincreased over time, possibly due to market conditions and EMTK's ownership\nstake in these companies. Overall, the findings of this study suggest that\nthere is a complex interplay between the stock prices of parent companies and\ntheir subsidiaries, and that there are a variety of factors that can influence\nthese relationships over time. These findings may be useful for investors in\nmaking informed decisions about their investment portfolios, as changes in the\ncorrelations could impact their portfolio's performance.\n"
    },
    {
        "paper_id": 2303.07393,
        "authors": "Matthew Dicks, Andrew Paskaramoorthy, Tim Gebbie",
        "title": "Many learning agents interacting with an agent-based market model",
        "comments": "16 pages, 8 figures, 5 tables, enhanced discussion and figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
        "abstract": "  We consider the dynamics and the interactions of multiple reinforcement\nlearning optimal execution trading agents interacting with a reactive\nAgent-Based Model (ABM) of a financial market in event time. The model\nrepresents a market ecology with 3-trophic levels represented by: optimal\nexecution learning agents, minimally intelligent liquidity takers, and fast\nelectronic liquidity providers. The optimal execution agent classes include\nbuying and selling agents that can either use a combination of limit orders and\nmarket orders, or only trade using market orders. The reward function\nexplicitly balances trade execution slippage against the penalty of not\nexecuting the order timeously. This work demonstrates how multiple competing\nlearning agents impact a minimally intelligent market simulation as functions\nof the number of agents, the size of agents' initial orders, and the state\nspaces used for learning. We use phase space plots to examine the dynamics of\nthe ABM, when various specifications of learning agents are included. Further,\nwe examine whether the inclusion of optimal execution agents that can learn is\nable to produce dynamics with the same complexity as empirical data. We find\nthat the inclusion of optimal execution agents changes the stylised facts\nproduced by ABM to conform more with empirical data, and are a necessary\ninclusion for ABMs investigating market micro-structure. However, including\nexecution agents to chartist-fundamentalist-noise ABMs is insufficient to\nrecover the complexity observed in empirical data.\n"
    },
    {
        "paper_id": 2303.07462,
        "authors": "Minkyu Shin, Jin Kim, Bas van Opheusden, and Thomas L. Griffiths",
        "title": "Superhuman Artificial Intelligence Can Improve Human Decision Making by\n  Increasing Novelty",
        "comments": "This paper is published in PNAS:\n  https://www.pnas.org/doi/10.1073/pnas.2214840120 Minor edits to v1 include\n  the addition of watermark and link to the published paper in the footer",
        "journal-ref": "Proceedings of the National Academy of Sciences, 120 (12),\n  e2214840120 (2023)",
        "doi": "10.1073/pnas.2214840120",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  How will superhuman artificial intelligence (AI) affect human decision\nmaking? And what will be the mechanisms behind this effect? We address these\nquestions in a domain where AI already exceeds human performance, analyzing\nmore than 5.8 million move decisions made by professional Go players over the\npast 71 years (1950-2021). To address the first question, we use a superhuman\nAI program to estimate the quality of human decisions across time, generating\n58 billion counterfactual game patterns and comparing the win rates of actual\nhuman decisions with those of counterfactual AI decisions. We find that humans\nbegan to make significantly better decisions following the advent of superhuman\nAI. We then examine human players' strategies across time and find that novel\ndecisions (i.e., previously unobserved moves) occurred more frequently and\nbecame associated with higher decision quality after the advent of superhuman\nAI. Our findings suggest that the development of superhuman AI programs may\nhave prompted human players to break away from traditional strategies and\ninduced them to explore novel moves, which in turn may have improved their\ndecision-making.\n"
    },
    {
        "paper_id": 2303.07705,
        "authors": "Krzysztof Burnecki, Zbigniew Palmowski, Marek Teuerle and Aleksandra\n  Wilkowska",
        "title": "Ruin probability for the quota share model with~phase-type distributed\n  claims",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  In this paper, we generalise the results presented in the literature for the\nruin probability for the insurer--reinsurer model under a pro-rata reinsurance\ncontract. We consider claim amounts that are described by a phase-type\ndistribution that includes exponential, mixture of exponential, Erlang, and\nmixture of Erlang distributions. We derive the ruin probability formulas with\nthe use of change-of-measure technique and present important special cases. We\nillustrate the usefulness of the introduced model by fitting it to the\nreal-world loss data. With the use of statistical tests and graphical tools, we\nshow that the mixture of Erlangs is well-fitted to the data and is superior to\nother considered distributions. This justifies the fact that the presented\nresults can be useful in the context of risk assessment of co-operating\ninsurance companies.\n"
    },
    {
        "paper_id": 2303.07773,
        "authors": "Marcus C Christiansen",
        "title": "Axiomatic characterization of pointwise Shapley decompositions",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  A common problem in various applications is the additive decomposition of the\noutput of a function with respect to its input variables. Functions with binary\narguments can be axiomatically decomposed by the famous Shapley value. For the\ndecomposition of functions with real arguments, a popular method is the\npointwise application of the Shapley value on the domain. However, this\npointwise application largely ignores the overall structure of functions. In\nthis paper, axioms are developed which fully preserve functional structures and\nlead to unique decompositions for all Borel measurable functions.\n"
    },
    {
        "paper_id": 2303.07786,
        "authors": "Petra Tschuchnig, Manfred Mayr, Maximilian Tschuchnig, Peter Haber",
        "title": "A Commons-Compatible Implementation of the Sharing Economy:\n  Blockchain-Based Open Source Mediation",
        "comments": null,
        "journal-ref": "In Proceedings of the 2nd International Conference on Finance,\n  Economics, Management and IT Business - FEMIB, 2020, ISBN 978-989-758-422-0,\n  pages 72-79",
        "doi": "10.5220/0009345600720079",
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  The network economical sharing economy, with direct exchange as a core\ncharacteristic, is implemented both, on a commons and platform economical\nbasis. This is due to a gain in importance of trust, collaborative consumption\nand democratic management as well as technological progress, in the form of\nnear zero marginal costs, open source contributions and digital transformation.\nConcurrent to these commons-based drivers, the grey area between commerce and\nprivate exchange is used to exploit work, safety and tax regulations by central\nplatform economists. Instead of central intermediators, the blockchain\ntechnology makes decentralized consensus finding, using Proof-of-Work (PoW)\nwithin a self-sustaining Peer-to-Peer network, possible. Therefore, a\nblockchain-based open source mediation seems to offer a commons-compatible\nimplementation of the sharing economy. This thesis is investigated through a\nqualitative case study of Sardex and Interlace with their blockchain\napplication, based on expert interviews and a structured content analysis. To\ndetect the most commons-compatible implementation, the different implementation\noptions through conventional platform intermediators, an open source blockchain\nwith PoW as well as Interlaces' permissioned blockchain approach, are compared.\nThe following confrontation is based on deductive criteria, which illustrates\nthe inherent characteristics of a commons-based sharing economy.\n"
    },
    {
        "paper_id": 2303.07925,
        "authors": "Thomas Wong, Mauricio Barahona",
        "title": "Deep incremental learning models for financial temporal tabular datasets\n  with distribution shifts",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  We present a robust deep incremental learning framework for regression tasks\non financial temporal tabular datasets which is built upon the incremental use\nof commonly available tabular and time series prediction models to adapt to\ndistributional shifts typical of financial datasets. The framework uses a\nsimple basic building block (decision trees) to build self-similar models of\nany required complexity to deliver robust performance under adverse situations\nsuch as regime changes, fat-tailed distributions, and low signal-to-noise\nratios. As a detailed study, we demonstrate our scheme using XGBoost models\ntrained on the Numerai dataset and show that a two layer deep ensemble of\nXGBoost models over different model snapshots delivers high quality predictions\nunder different market regimes. We also show that the performance of XGBoost\nmodels with different number of boosting rounds in three scenarios (small,\nstandard and large) is monotonically increasing with respect to model size and\nconverges towards the generalisation upper bound. We also evaluate the\nrobustness of the model under variability of different hyperparameters, such as\nmodel complexity and data sampling settings. Our model has low hardware\nrequirements as no specialised neural architectures are used and each base\nmodel can be independently trained in parallel.\n"
    },
    {
        "paper_id": 2303.07941,
        "authors": "Anastasiya Tanana",
        "title": "Relative performance criteria of multiplicative form in complete markets",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We consider existence and uniqueness of Nash equilibria in an $N$-player game\nof utility maximization under relative performance criteria of multiplicative\nform in complete semimartingale markets. For a large class of players' utility\nfunctions, a general characterization of Nash equilibria for a given initial\nwealth vector is provided in terms of invertibility of a map from\n$\\mathbb{R}^N$ to $\\mathbb{R}^N$. As a consequence of the general theorem, we\nderive existence and uniqueness of Nash equilibria for an arbitrary initial\nwealth vector, as well as their convergence, if either (i) players' utility\nfunctions are close to CRRA, or (ii) players' competition weights are small and\nrelative risk aversions are bounded away from infinity.\n"
    },
    {
        "paper_id": 2303.07996,
        "authors": "Mao Fabrice Djete, Gaoyue Guo, Nizar Touzi",
        "title": "Mean field game of mutual holding with defaultable agents, and systemic\n  risk",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We introduce the possibility of default in the mean field game of mutual\nholding of Djete and Touzi [11]. This is modeled by introducing absorption at\nthe origin of the equity process. We provide an explicit solution of this mean\nfield game. Moreover, we provide a particle system approximation, and we derive\nan autonomous equation for the time evolution of the default probability, or\nequivalently the law of the hitting time of the origin by the equity process.\nThe systemic risk is thus described by the evolution of the default\nprobability.\n"
    },
    {
        "paper_id": 2303.08217,
        "authors": "Max Nendel and Jan Streicher",
        "title": "An axiomatic approach to default risk and model uncertainty in rating\n  systems",
        "comments": "References have been updated, typos have been corrected, final\n  version to appear in Journal of Mathematical Economics",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper, we deal with an axiomatic approach to default risk. We\nintroduce the notion of a default risk measure, which generalizes the classical\nprobability of default (PD), and allows to incorporate model risk in various\nforms. We discuss different properties and representations of default risk\nmeasures via monetary risk measures, families of related tail risk measures,\nand Choquet capacities. In a second step, we turn our focus on default risk\nmeasures, which are given as worst-case PDs and distorted PDs. The latter are\nfrequently used in order to take into account model risk for the computation of\ncapital requirements through risk-weighted assets (RWAs), as demanded by the\nCapital Requirement Regulation (CRR). In this context, we discuss the impact of\ndifferent default risk measures and margins of conservatism on the amount of\nrisk-weighted assets.\n"
    },
    {
        "paper_id": 2303.08286,
        "authors": "Anuradha Singh, Jyoti Yadav, Sarahana Shrestha, Aparna S. Varde",
        "title": "Linking Alternative Fuel Vehicles Adoption with Socioeconomic Status and\n  Air Quality Index",
        "comments": null,
        "journal-ref": "AAAI 2023 the 37th AAAI Conference on Artificial Intelligence\n  (AISG workshop)",
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  This is a study on the potential widespread usage of alternative fuel\nvehicles, linking them with the socio-economic status of the respective\nconsumers as well as the impact on the resulting air quality index. Research in\nthis area aims to leverage machine learning techniques in order to promote\nappropriate policies for the proliferation of alternative fuel vehicles such as\nelectric vehicles with due justice to different population groups. Pearson\ncorrelation coefficient is deployed in the modeling the relationships between\nsocio-economic data, air quality index and data on alternative fuel vehicles.\nLinear regression is used to conduct predictive modeling on air quality index\nas per the adoption of alternative fuel vehicles, based on socio-economic\nfactors. This work exemplifies artificial intelligence for social good.\n"
    },
    {
        "paper_id": 2303.08445,
        "authors": "Anna Naszodi and Liliana Cuccu",
        "title": "Are high school degrees and university diplomas equally heritable in the\n  US? A new measure of relative intergenerational mobility",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper proposes a new measure of relative intergenerational mobility\nalong the educational trait as a proxy of inequality of opportunity. The new\nmeasure is more suitable for controlling for the variations in the trait\ndistributions of individuals and their parents than the commonly used\nintergenerational persistence coefficient. This point is illustrated by our\nempirical analysis of US census data from the period between 1960 and 2015: we\nshow that controlling for the variations in the trait distributions adequately\nis vital in assessing the part of intergenerational mobility which is not\ncaused by the educational expansion. Failing to do so can potentially reverse\nthe relative priority of various policies aiming at reducing the \"heritability\"\nof high school degrees and tertiary education diplomas.\n"
    },
    {
        "paper_id": 2303.08462,
        "authors": "Kenneth Tsz Hin Ng, Wing Fung Chong",
        "title": "Optimal Investment in Defined Contribution Pension Schemes with Forward\n  Utility Preferences",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  Optimal investment strategies of an individual worker during the accumulation\nphase in the defined contribution pension scheme have been well studied in the\nliterature. Most of them adopted the classical backward model and approach, but\nany pre-specifications of retirement time, preferences, and market environment\nmodels do not often hold in such a prolonged horizon of the pension scheme.\nPre-commitment to ensure the time-consistency of an optimal investment strategy\nderived from the backward model and approach leads the supposedly optimal\nstrategy to be sub-optimal in the actual realizations. This paper revisits the\noptimal investment problem for the worker during the accumulation phase in the\ndefined contribution pension scheme, via the forward preferences, in which an\nenvironment-adapting strategy is able to hold optimality and time-consistency\ntogether. Stochastic partial differential equation representation for the\nworker's forward preferences is illustrated. This paper constructs two of the\nforward utility preferences and solves the corresponding optimal investment\nstrategies, in the cases of initial power and exponential utility functions.\n"
    },
    {
        "paper_id": 2303.08477,
        "authors": "Micha{\\l} Barski, Rafa{\\l} {\\L}ochowski",
        "title": "Classification and calibration of affine models driven by independent\n  L\\'evy processes",
        "comments": "arXiv admin note: substantial text overlap with arXiv:2204.07245",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  The paper is devoted to the study of the short rate equation of the form $$\ndR(t)=F(R(t))dt+\\sum_{i=1}^{d}G_i(R(t-))dZ_i(t), \\quad R(0)=x\\geq 0, \\quad t>0,\n$$ with deterministic functions $F,G_1,...,G_d$ and independent L\\'evy\nprocesses of infinite variation $Z_1,...,Z_d$ with regularly varying Laplace\nexponents. The equation is supposed to have a nonnegative solution which\ngenerates an affine term structure model. A precise form of the generator of\n$R$ is characterized and a related classification of equations which generate\naffine models introduced in the spirit of Dai and Singleton\n\\cite{DaiSingleton}. Each class is shown to have its own canonical\nrepresentation which is an equation with the same drift and the jump diffusion\npart based on a L\\'evy process taking values in $\\mathbb{R}^{g}, 1\\leq g\\leq\nd$, with independent coordinates being stable processes with stability indices\nin the range $(1,2]$. Numerical calibration results of canonical\nrepresentations to the market term structure of interest rates are presented\nand compared with the classical CIR model. The paper generalizes the classical\nresults on the CIR model from \\cite{CIR}, as well as on its extended version\nfrom \\cite{BarskiZabczykCIR} and \\cite{BarskiZabczyk} where $Z$ was a\none-dimensional L\\'evy process.\n"
    },
    {
        "paper_id": 2303.08521,
        "authors": "Nicole B\\\"auerle and Antje Mahayni",
        "title": "Optimal investment in ambiguous financial markets with learning",
        "comments": null,
        "journal-ref": "European Journal of Operational Research, vol. 315, no 1, 2024,\n  393-410",
        "doi": "10.1016/j.ejor2024.01.022",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We consider the classical multi-asset Merton investment problem under drift\nuncertainty, i.e. the asset price dynamics are given by geometric Brownian\nmotions with constant but unknown drift coefficients. The investor assumes a\nprior drift distribution and is able to learn by observing the asset prize\nrealizations during the investment horizon. While the solution of an expected\nutility maximizing investor with constant relative risk aversion (CRRA) is well\nknown, we consider the optimization problem under risk and ambiguity\npreferences by means of the KMM (Klibanoff et al. (2005)) approach. Here, the\ninvestor maximizes a double certainty equivalent. The inner certainty\nequivalent is for given drift coefficient, the outer is based on a drift\ndistribution. Assuming also a CRRA type ambiguity function, it turns out that\nthe optimal strategy can be stated in terms of the solution without ambiguity\npreferences but an adjusted drift distribution. To the best of our knowledge an\nexplicit solution method in this setting is new. We rely on some duality\ntheorems to prove our statements.\n  Based on our theoretical results, we are able to shed light on the impact of\nthe prior drift distribution as well as the consequences of ambiguity\npreferences via the transfer to an adjusted drift distribution, i.e. we are\nable to explain the interaction of risk and ambiguity preferences. We compare\nour results with the ones in a pre-commitment setup where the investor is\nrestricted to deterministic strategies. It turns out that (under risk and\nambiguity aversion) an infinite investment horizon implies in both cases a\nmaximin decision rule, i.e. the investor follows the worst (best) Merton\nfraction (over all realizations of it) if she is more (less) risk averse than a\nlog-investor. We illustrate our findings with an extensive numerical study.\n"
    },
    {
        "paper_id": 2303.08565,
        "authors": "Katarzyna Maciejowska and Tomasz Serafin and Bartosz Uniejewski",
        "title": "Probabilistic forecasting with Factor Quantile Regression: Application\n  to electricity trading",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper presents a novel approach for constructing probabilistic\nforecasts, which combines both the Quantile Regression Averaging (QRA) method\nand the Principal Component Analysis (PCA) averaging scheme. The performance of\nthe approach is evaluated on datasets from two European energy markets - the\nGerman EPEX SPOT and the Polish Power Exchange (TGE). The results indicate that\nnewly proposed solutions yield results, which are more accurate than the\nliterature benchmarks. Additionally, empirical evidence indicates that the\nproposed method outperforms its competitors in terms of the empirical coverage\nand the Christoffersen test. In addition, the economic value of the\nprobabilistic forecast is evaluated on the basis of financial metrics. We test\nthe performance of forecasting models taking into account a day-ahead market\ntrading strategy that utilizes probabilistic price predictions and an energy\nstorage system. The results indicate that profits of up to 10 EUR per 1 MWh\ntransaction can be obtained when predictions are generated using the novel\napproach.\n"
    },
    {
        "paper_id": 2303.08615,
        "authors": "Viktor Witkovsk\\'y",
        "title": "Characteristic Function of the Tsallis $q$-Gaussian and Its Applications\n  in Measurement and Metrology",
        "comments": "19 pages",
        "journal-ref": "Metrology 2023, 3(2), 222-236",
        "doi": "10.3390/metrology3020012",
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  The Tsallis $q$-Gaussian distribution is a powerful generalization of the\nstandard Gaussian distribution and is commonly used in various fields,\nincluding non-extensive statistical mechanics, financial markets and image\nprocessing. It belongs to the $q$-distribution family, which is characterized\nby a non-additive entropy. Due to their versatility and practicality,\n$q$-Gaussians are a natural choice for modeling input quantities in measurement\nmodels. This paper presents the characteristic function of a linear combination\nof independent $q$-Gaussian random variables and proposes a numerical method\nfor its inversion. The proposed technique makes it possible to determine the\nexact probability distribution of the output quantity in linear measurement\nmodels, with the input quantities modeled as independent $q$-Gaussian random\nvariables. It provides an alternative computational procedure to the Monte\nCarlo method for uncertainty analysis through the propagation of distributions.\n"
    },
    {
        "paper_id": 2303.08748,
        "authors": "Lioba Heimbach, Eric Schertenleib, Roger Wattenhofer",
        "title": "DeFi Lending During The Merge",
        "comments": "This paper was accepted at the 5th Conference on Advances in\n  Financial Technologies (AFT 2023)",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Lending protocols in decentralized finance enable the permissionless exchange\nof capital from lenders to borrowers without relying on a trusted third party\nfor clearing or market-making. Interest rates are set by the supply and demand\nof capital according to a pre-defined function. In the lead-up to The Merge:\nEthereum blockchain's transition from proof-of-work (PoW) to proof-of-stake\n(PoS), a fraction of the Ethereum ecosystem announced plans of continuing with\na PoW-chain. Owners of ETH - whether their ETH was borrowed or not - would hold\nthe native tokens on each chain. This development alarmed lending protocols.\nThey feared spiking ETH borrowing rates would lead to mass liquidations which\ncould undermine their viability. Thus, the decentralized autonomous\norganization running the protocols saw no alternative to intervention -\nrestricting users' ability to borrow.\n  We investigate the effects of the merge and the aforementioned intervention\non the two biggest lending protocols on Ethereum: AAVE and Compound. Our\nanalysis finds that borrowing rates were extremely volatile, jumping by two\norders of magnitude, and borrowing at times reached 100% of the available\nfunds. Despite this, no spike in mass liquidations or irretrievable loans\nmaterialized. Further, we are the first to quantify and analyze\nhard-fork-arbitrage, profiting from holding debt in the native blockchain token\nduring a hard fork. We find that arbitrageurs transferred tokens to centralized\nexchanges which at the time were worth more than 13 Mio US$, money that was\neffectively extracted from the platforms' lenders.\n"
    },
    {
        "paper_id": 2303.0876,
        "authors": "Young Shin Kim, Hyangju Kim, Jaehyung Choi",
        "title": "Deep Calibration With Artificial Neural Network: A Performance\n  Comparison on Option Pricing Models",
        "comments": "26 pages, 3 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper explores Artificial Neural Network (ANN) as a model-free solution\nfor a calibration algorithm of option pricing models. We construct ANNs to\ncalibrate parameters for two well-known GARCH-type option pricing models:\nDuan's GARCH and the classical tempered stable GARCH that significantly improve\nupon the limitation of the Black-Scholes model but have suffered from\ncomputation complexity. To mitigate this technical difficulty, we train ANNs\nwith a dataset generated by Monte Carlo Simulation (MCS) method and apply them\nto calibrate optimal parameters. The performance results indicate that the ANN\napproach consistently outperforms MCS and takes advantage of faster computation\ntimes once trained. The Greeks of options are also discussed.\n"
    },
    {
        "paper_id": 2303.08765,
        "authors": "Juan Andres Espinosa-Torres, Jaime Ramirez-Cuellar",
        "title": "The Effects of the Pandemic on Market Power and Profitability",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  We explore firm-level markup and profit rates during the COVID-19 pandemic\nfor a panel of 3,611 publicly traded firms in Compustat and find increases for\nthe average firm. We offer conditions to give markups and profit rate forecasts\na causal interpretation of what would have happened had the pandemic not\nhappened. Our estimations suggest that had the pandemic not happened, markups\nwould have been 4% and 7% higher than observed in 2020 and 2021, respectively,\nand profit rates would have been 2.1 and 6.4 percentage points lower. We\nperform a battery of tests to assess the robustness of our approach. We further\nshow significant heterogeneity in the impact of the pandemic on firms by key\nfirm characteristics and industry. We find that firms with lower than\nforecasted markups tend to have lower stock-exchange tenure and fewer\nemployees.\n"
    },
    {
        "paper_id": 2303.08867,
        "authors": "Louis Saddier and Matteo Marsili",
        "title": "A Bayesian theory of market impact",
        "comments": "30 pages, 7 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  The available liquidity at any time in financial markets falls largely short\nof the typical size of the orders that institutional investors would trade. In\norder to reduce the impact on prices due to the execution of large orders,\ntraders in financial markets split large orders into a series of smaller ones,\nwhich are executed sequentially. The resulting sequence of trades is called a\nmeta-order. Empirical studies have revealed a non-trivial set of statistical\nlaws on how meta-orders affect prices, which include i) the square-root\nbehaviour of the expected price variation with the total volume traded, ii) its\ncrossover to a linear regime for small volumes, and iii) a reversion of average\nprices towards its initial value, after the sequence of trades is over. Here we\nrecover this phenomenology within a minimal theoretical framework where the\nmarket sets prices by incorporating all information on the direction and speed\nof trade of the meta-order in a Bayesian manner. The simplicity of this\nderivation lends further support to the robustness and universality of market\nimpact laws. In particular, it suggests that the square-root impact law\noriginates from the over-estimation of order flows originating from\nmeta-orders.\n"
    },
    {
        "paper_id": 2303.08968,
        "authors": "Pieter M. van Staden and Peter A. Forsyth and Yuying Li",
        "title": "A parsimonious neural network approach to solve portfolio optimization\n  problems without using dynamic programming",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  We present a parsimonious neural network approach, which does not rely on\ndynamic programming techniques, to solve dynamic portfolio optimization\nproblems subject to multiple investment constraints. The number of parameters\nof the (potentially deep) neural network remains independent of the number of\nportfolio rebalancing events, and in contrast to, for example, reinforcement\nlearning, the approach avoids the computation of high-dimensional conditional\nexpectations. As a result, the approach remains practical even when considering\nlarge numbers of underlying assets, long investment time horizons or very\nfrequent rebalancing events. We prove convergence of the numerical solution to\nthe theoretical optimal solution of a large class of problems under fairly\ngeneral conditions, and present ground truth analyses for a number of popular\nformulations, including mean-variance and mean-conditional value-at-risk\nproblems. We also show that it is feasible to solve Sortino ratio-inspired\nobjectives (penalizing only the variance of wealth outcomes below the mean) in\ndynamic trading settings with the proposed approach. Using numerical\nexperiments, we demonstrate that if the investment objective functional is\nseparable in the sense of dynamic programming, the correct time-consistent\noptimal investment strategy is recovered, otherwise we obtain the correct\npre-commitment (time-inconsistent) investment strategy. The proposed approach\nremains agnostic as to the underlying data generating assumptions, and results\nare illustrated using (i) parametric models for underlying asset returns, (ii)\nstationary block bootstrap resampling of empirical returns, and (iii)\ngenerative adversarial network (GAN)-generated synthetic asset returns.\n"
    },
    {
        "paper_id": 2303.09011,
        "authors": "Philip Metzger",
        "title": "Economics of In-Space Industry and Competitiveness of Lunar-Derived\n  Rocket Propellant",
        "comments": "To be published in Acta Astronautica. 50 pages, 16 figures",
        "journal-ref": null,
        "doi": "10.1016/j.actaastro.2023.03.014",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Economic parameters are identified for an in-space industry where the capital\nis made on one planet, it is transported to and teleoperated on a second\nplanet, and the product is transported off the second planet for consumption.\nThis framework is used to model the long-run cost of lunar propellant\nproduction to help answer whether it is commercially competitive against\npropellant launched from Earth. The prior techno-economic analyses (TEAs) of\nlunar propellant production had disagreed over this. The \"gear ratio on cost\"\nfor capital transport, G, and the production mass ratio of the capital, phi,\nare identified as the most important factors determining competitiveness. The\nprior TEAs are examined for how they handled these two metrics. This identifies\ncrucial mistakes in some of the TEAs: choosing transportation architectures\nwith high G, and neglecting to make choices for the capital that could achieve\nadequate phi. The tent sublimation technology has a value of phi that is an\norder of magnitude better than the threshold for competitiveness even in low\nEarth orbit (LEO). The strip mining technology is closer to the threshold, but\ntechnological improvements plus several years of operating experience will\nimprove its competitiveness, according to the model. Objections from members of\nthe aerospace community are discussed, especially the question whether the\ntechnology can attain adequate reliability in the lunar environment. The\nresults suggest that lunar propellant production will be commercially viable\nand that it should lower the cost of doing everything else in space.\n"
    },
    {
        "paper_id": 2303.09147,
        "authors": "Klaus M. Miller and Bernd Skiera",
        "title": "Economic Consequences of Online Tracking Restrictions: Evidence from\n  Cookies",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  In recent years, European regulators have debated restricting the time an\nonline tracker can track a user to protect consumer privacy better. Despite the\nsignificance of these debates, there has been a noticeable absence of any\ncomprehensive cost-benefit analysis. This article fills this gap on the cost\nside by suggesting an approach to estimate the economic consequences of\nlifetime restrictions on cookies for publishers. The empirical study on cookies\nof 54,127 users who received 128 million ad impressions over 2.5 years yields\nan average cookie lifetime of 279 days, with an average value of EUR 2.52 per\ncookie. Only 13% of all cookies increase their daily value over time, but their\naverage value is about four times larger than the average value of all cookies.\nRestricting cookies lifetime to one year (two years) decreases their lifetime\nvalue by 25% (19%), which represents a decrease in the value of all cookies of\n9% (5%). In light of the EUR 10.60 billion cookie-based display ad revenue in\nEurope, such restrictions would endanger EUR 904 million (EUR 576 million)\nannually, equivalent to EUR 2.08 (EUR 1.33) per EU internet user. The article\ndiscusses these results' marketing strategy challenges and opportunities for\nadvertisers and publishers.\n"
    },
    {
        "paper_id": 2303.09176,
        "authors": "Volodymyr Savchuk",
        "title": "Real Options Technique as a Tool of Strategic Risk Management",
        "comments": "22 pages, 11 figures, 12 tables",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
        "abstract": "  The real options approach is now considered an effective alternative to the\ncorporate DCF model for a feasibility study. The current paper offers a\npractical methodology employing binomial trees and real options techniques for\nevaluating investment projects. A general computation procedure is suggested\nfor the decision tree with two active stages of real options, which correspond\nto additional investments. The suggested technique can be used for most real\noptions, which are practically essential regarding enterprise strategy. The\nspecial case named Binomial-Random-Cash-Flow Real Options Model with random\noutcomes is developed as the next step of real options modelling. Project Value\nat Risk is introduced and used as a criterion of investment project feasibility\nunder the assumption regarding random outcomes. In particular, the Gaussian\nprobability distribution is used for modelling option outcomes uncertainty. The\nchoice of the Gaussian distribution is caused by the desire to obtain estimates\nin the final analytical form. Choosing another distribution for random outcomes\nleads to using Monte Carlo simulation, for which a general framework is\ndeveloped by demonstrating some instances. The author could avoid the\ncomputational complexity that makes these solutions feasible for business\npractice.\n"
    },
    {
        "paper_id": 2303.09323,
        "authors": "Shima Nabiee, Nader Bagherzadeh",
        "title": "Stock Trend Prediction: A Semantic Segmentation Approach",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  Market financial forecasting is a trending area in deep learning. Deep\nlearning models are capable of tackling the classic challenges in stock market\ndata, such as its extremely complicated dynamics as well as long-term temporal\ncorrelation. To capture the temporal relationship among these time series,\nrecurrent neural networks are employed. However, it is difficult for recurrent\nmodels to learn to keep track of long-term information. Convolutional Neural\nNetworks have been utilized to better capture the dynamics and extract features\nfor both short- and long-term forecasting. However, semantic segmentation and\nits well-designed fully convolutional networks have never been studied for\ntime-series dense classification. We present a novel approach to predict\nlong-term daily stock price change trends with fully 2D-convolutional\nencoder-decoders. We generate input frames with daily prices for a time-frame\nof T days. The aim is to predict future trends by pixel-wise classification of\nthe current price frame. We propose a hierarchical CNN structure to encode\nmultiple price frames to multiscale latent representation in parallel using\nAtrous Spatial Pyramid Pooling blocks and take that temporal coarse feature\nstacks into account in the decoding stages. Our hierarchical structure of CNNs\nmakes it capable of capturing both long and short-term temporal relationships\neffectively. The effect of increasing the input time horizon via incrementing\nparallel encoders has been studied with interesting and substantial changes in\nthe output segmentation masks. We achieve overall accuracy and AUC of %78.18\nand 0.88 for joint trend prediction over the next 20 days, surpassing other\nsemantic segmentation approaches. We compared our proposed model with several\ndeep models specifically designed for technical analysis and found that for\ndifferent output horizons, our proposed models outperformed other models.\n"
    },
    {
        "paper_id": 2303.0933,
        "authors": "Claudiu Vinte, Marcel Ausloos",
        "title": "Portfolio Volatility Estimation Relative to Stock Market Cross-Sectional\n  Intrinsic Entropy",
        "comments": "24 pages, 12 figures, 4 tables",
        "journal-ref": "Journal of Risk Financial Management 2023, 16(2), 114",
        "doi": "10.3390/jrfm16020114",
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  Selecting stock portfolios and assessing their relative volatility risk\ncompared to the market as a whole, market indices, or other portfolios is of\ngreat importance to professional fund managers and individual investors alike.\nOur research uses the cross-sectional intrinsic entropy (CSIE) model to\nestimate the cross-sectional volatility of the stock groups that can be\nconsidered together as portfolio constituents. In our study, we benchmark\nportfolio volatility risks against the volatility of the entire market provided\nby the CSIE and the volatility of market indices computed using longitudinal\ndata. This article introduces CSIE-based betas to characterise the relative\nvolatility risk of the portfolio against market indices and the market as a\nwhole. We empirically prove that, through CSIE-based betas, multiple sets of\nsymbols that outperform the market indices in terms of rate of return while\nmaintaining the same level of risk or even lower than the one exhibited by the\nmarket index can be discovered, for any given time interval. These sets of\nsymbols can be used as constituent stock portfolios and, in connection with the\nperspective provided by the CSIE volatility estimates, to hierarchically assess\ntheir relative volatility risk within the broader context of the overall\nvolatility of the stock market.\n"
    },
    {
        "paper_id": 2303.09393,
        "authors": "Na Hyeon Park, Hanna Kim, Chanhee Lee, Changhoon Yoon, Seunghyeon Lee,\n  Youngjin jin, Seungwon Shin",
        "title": "A Deep Dive into NFT Whales: A Longitudinal Study of the NFT Trading\n  Ecosystem",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  NFT (Non-fungible Token) has drastically increased in its size, accounting\nfor over \\$16.9B of total market capitalization. Despite the rapid growth of\nNFTs, this market has not been examined thoroughly from a financial\nperspective. In this paper, we conduct methodical analyses to identify NFT\nmarket movers who play a significant role in potentially manipulating and\noscillating NFT values. We collect over 3.8M NFT transaction data from the\nEthereum Blockchain from January 2021 to February 2022 to extract trading\ninformation in line with the NFT lifecycle: (i) mint, (ii) transfer/sale, and\n(iii) burn. Based on the size of held NFT values, we classify NFT traders into\nthree groups (whales, dolphins, and minnows). In total, we analyze 430K traders\nfrom 91 different NFT collection sources. We find that the top 0.1\\% of NFT\ntraders (i.e., whales) drive the NFT market with consistent, high returns. We\nthen identify and characterize the NFT whales' unique investment strategies\n(e.g., mint/sale patterns, wash trading) to empirically understand the whales\nin the NFT market for the first time.\n"
    },
    {
        "paper_id": 2303.09397,
        "authors": "Haritha GB and Sahana N.B",
        "title": "Cryptocurrency Price Prediction using Twitter Sentiment Analysis",
        "comments": "10 pages, NIAI 2023",
        "journal-ref": null,
        "doi": "10.5121/csit.2023.130302",
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  The cryptocurrency ecosystem has been the centre of discussion on many social\nmedia platforms, following its noted volatility and varied opinions. Twitter is\nrapidly being utilised as a news source and a medium for bitcoin discussion.\nOur algorithm seeks to use historical prices and sentiment of tweets to\nforecast the price of Bitcoin. In this study, we develop an end-to-end model\nthat can forecast the sentiment of a set of tweets (using a Bidirectional\nEncoder Representations from Transformers - based Neural Network Model) and\nforecast the price of Bitcoin (using Gated Recurrent Unit) using the predicted\nsentiment and other metrics like historical cryptocurrency price data, tweet\nvolume, a user's following, and whether or not a user is verified. The\nsentiment prediction gave a Mean Absolute Percentage Error of 9.45%, an average\nof real-time data, and test data. The mean absolute percent error for the price\nprediction was 3.6%.\n"
    },
    {
        "paper_id": 2303.09399,
        "authors": "Christian Flamant",
        "title": "Main Concepts and Principles of Political Economy -- Production and\n  Values, Distribution and Prices, Reproduction and Profits",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This book starts from the basic questions that had been raised by the\nfounders of Economic theory, Smith, Ricardo, and Marx: what makes the value of\ncommodities, what are production, exchange, money and incomes like profits,\nwages and rents. The answers that these economists had provided were mostly\nwrong, above all by defining the equivalence of commodities at the level of\nexchange, but also because of a confusion made between values and prices, and\nwrong views of what production really is and the role of fixed capital. Using\nthe mathematical theory of measurement and the physical theory of dimensional\nanalysis, this book provides a coherent theory of value based on an equivalence\nrelation not at the level of exchange, but of production. Indeed exchange is\nconsidered here as an equivalence relation between money and a monetary price,\nand not between commodities, modern monetary theory having demonstrated that\nmoney is not a commodity. The book rejects the conception of production as a\nsurplus, which owes much to Sraffa's theory of production prices, and is shown\nto be severely flawed. It founds the equivalence of commodities at the level of\na production process considered as a transformation process. It rehabilitates\nthe labor theory of value, based on the connection between money and labor due\nthe monetary payment of wages, which allows the homogenization of various kinds\nof concrete labor into abstract labor. It shows that value is then a dimension\nof commodities and that this dimension is time, i.e. the time of physics. On\nthis background, the book shows that the calculation of values for all\ncommodities is always possible, even in the case of joint production, and that\nthere cannot be any commodity residue left by this calculation. As a further\nstep, this book provides a coherent theory of the realization of the product,\nwhich occurs in the circulation process. Using an idea - the widow's cruse -\nintroduced by Keynes in his Treatise on Money, it brings to light the mechanism\nbehind the transformation of money values into money prices and of\nsurplus-value into profits and other transfer incomes, ensuring the formation\nof monetary profits. The book sheds some light on the rate of profit, its\ndeterminants and its evolution, showing in particular the paramount importance\nof capitalist consumption as one of its main determinants. In passing it\nexplains the reasons why in the real world there is a multiplicity of profit\nrates. Finally, it allows to solve in a precise and illustrated way the\nproblems raised by the Marxist law of the tendency of the rate of profit to\nfall. Most of the results obtained translate into principles, the first ones\nbeing truly basic, the following ones less basic, but all of them being\nfundamental. All in all, this book might provide the first building blocks to\ndevelop a full-fledged and scientific economic theory to many fellow\neconomists, critical of neo-classical theory, but who have not yet dicovered\nthe bases of a complete and coherent alternative.\n"
    },
    {
        "paper_id": 2303.09405,
        "authors": "Fabio Ashtar Telarico",
        "title": "Simplifying and Improving: Revisiting Bulgaria's Revenue Forecasting\n  Models",
        "comments": null,
        "journal-ref": "Economic Thought, 2023, 20 (6)",
        "doi": "10.56497/etj2367601",
        "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
        "abstract": "  In the thirty years since the end of real socialism, Bulgaria's went from\nhaving a rather radically 'different' tax system to adopting flat-rate taxation\nwith marginal tax rates that fell from figures as high as 40% to 10% for both\nthe corporate-income tax and the personal-income tax. Crucially, the\neconometric forecasting models in use at the Bulgarian Ministry of Finance\nhinted at an increase in tax revenue compatible with the so-called 'Laffer\ncurve'. Similarly, many economists held the view that revenues would have\nincreased. However, reality fell short of those expectations based on\nforecasting models and rooted in mainstream economic theory. Thus, this paper\nasks whether there are betterperforming forecasting models for personal-and\ncorporate-income tax-revenues in Bulgaria that are readily implementable and\noverperform the ones currently in use. After articulating a constructive\ncritique of the current forecasting models, the paper offers readily\nimplementable, transparent alternatives and proves their superiority.\n"
    },
    {
        "paper_id": 2303.09406,
        "authors": "Chang Liu and Sandra Paterlini",
        "title": "Stock Price Prediction Using Temporal Graph Model with Value Chain Data",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  Stock price prediction is a crucial element in financial trading as it allows\ntraders to make informed decisions about buying, selling, and holding stocks.\nAccurate predictions of future stock prices can help traders optimize their\ntrading strategies and maximize their profits. In this paper, we introduce a\nneural network-based stock return prediction method, the Long Short-Term Memory\nGraph Convolutional Neural Network (LSTM-GCN) model, which combines the Graph\nConvolutional Network (GCN) and Long Short-Term Memory (LSTM) Cells.\nSpecifically, the GCN is used to capture complex topological structures and\nspatial dependence from value chain data, while the LSTM captures temporal\ndependence and dynamic changes in stock returns data. We evaluated the LSTM-GCN\nmodel on two datasets consisting of constituents of Eurostoxx 600 and S&P 500.\nOur experiments demonstrate that the LSTM-GCN model can capture additional\ninformation from value chain data that are not fully reflected in price data,\nand the predictions outperform baseline models on both datasets.\n"
    },
    {
        "paper_id": 2303.09407,
        "authors": "Keer Yang, Guanqun Zhang, Chuan Bi, Qiang Guan, Hailu Xu, Shuai Xu",
        "title": "Improving CNN-base Stock Trading By Considering Data Heterogeneity and\n  Burst",
        "comments": null,
        "journal-ref": null,
        "doi": "10.5121/ijci.2023.120201",
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  In recent years, there have been quite a few attempts to apply intelligent\ntechniques to financial trading, i.e., constructing automatic and intelligent\ntrading framework based on historical stock price. Due to the unpredictable,\nuncertainty and volatile nature of financial market, researchers have also\nresorted to deep learning to construct the intelligent trading framework. In\nthis paper, we propose to use CNN as the core functionality of such framework,\nbecause it is able to learn the spatial dependency (i.e., between rows and\ncolumns) of the input data. However, different with existing deep\nlearning-based trading frameworks, we develop novel normalization process to\nprepare the stock data. In particular, we first empirically observe that the\nstock data is intrinsically heterogeneous and bursty, and then validate the\nheterogeneity and burst nature of stock data from a statistical perspective.\nNext, we design the data normalization method in a way such that the data\nheterogeneity is preserved and bursty events are suppressed. We verify out\ndeveloped CNN-based trading framework plus our new normalization method on 29\nstocks. Experiment results show that our approach can outperform other\ncomparing approaches.\n"
    },
    {
        "paper_id": 2303.09652,
        "authors": "Sanjay Bhattacherjee and Palash Sarkar",
        "title": "On Using Proportional Representation Methods as Alternatives to Pro-Rata\n  Based Order Matching Algorithms in Stock Exchanges",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
        "abstract": "  The first observation of the paper is that methods for determining\nproportional representation in electoral systems may be suitable as\nalternatives to the pro-rata order matching algorithm used in stock exchanges.\nThe main part of our work is to comprehensively consider various well known\nproportional representation methods and analyse in details their suitability\nfor replacing the pro-rata algorithm. Our analysis consists of a theoretical\nstudy as well as simulation studies based on data sampled from a distribution\nwhich has been suggested in the literature as models of limit orders. Based on\nour analysis, we put forward the suggestion that the well known Hamilton's\nmethod is a superior alternative to the pro-rata algorithm for order matching\napplications.\n"
    },
    {
        "paper_id": 2303.09682,
        "authors": "Titos Matsakos and Stuart Nield",
        "title": "Quantum Monte Carlo simulations for financial risk analytics: scenario\n  generation for equity, rate, and credit risk factors",
        "comments": "Accepted for publication in Quantum. 35 pages",
        "journal-ref": "Quantum 8, 1306 (2024)",
        "doi": "10.22331/q-2024-04-04-1306",
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Monte Carlo (MC) simulations are widely used in financial risk management,\nfrom estimating value-at-risk (VaR) to pricing over-the-counter derivatives.\nHowever, they come at a significant computational cost due to the number of\nscenarios required for convergence. If a probability distribution is available,\nQuantum Amplitude Estimation (QAE) algorithms can provide a quadratic speed-up\nin measuring its properties as compared to their classical counterparts. Recent\nstudies have explored the calculation of common risk measures and the\noptimisation of QAE algorithms by initialising the input quantum states with\npre-computed probability distributions. If such distributions are not available\nin closed form, however, they need to be generated numerically, and the\nassociated computational cost may limit the quantum advantage. In this paper,\nwe bypass this challenge by incorporating scenario generation -- i.e.\nsimulation of the risk factor evolution over time to generate probability\ndistributions -- into the quantum computation; we refer to this process as\nQuantum MC (QMC) simulations. Specifically, we assemble quantum circuits that\nimplement stochastic models for equity (geometric Brownian motion), interest\nrate (mean-reversion models), and credit (structural, reduced-form, and rating\nmigration credit models) risk factors. We then integrate these models with QAE\nto provide end-to-end examples for both market and credit risk use cases.\n"
    },
    {
        "paper_id": 2303.0972,
        "authors": "Hirotaka Goto",
        "title": "Econotaxis in modeling urbanization by labor force migration",
        "comments": "9 pages, 6 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Individual participants in human society collectively exhibit aggregation\nbehavior. In this study, we present a simple microscopic model of labor force\nmigration by employing the active Brownian particles framework. Through\nagent-based simulations, we find that our model produces clusters of agents\nfrom a random initial distribution. Furthermore, two empirical regularities\ncalled Zipf's and Okun's laws were observed in our model. To reveal the\nmechanism underlying the reproduced agglomeration phenomena, we derived an\nextended Keller-Segel system, a classic model that describes the aggregation\nbehavior of biological organisms called \"taxis,\" from our microscopic model.\nThe obtained macroscopic system indicates that the agglomeration of the\nworkforce in real world can be accounted for through a new type of taxis\ncentral to human behavior, which highlights the relevance of urbanization to\nblow-up phenomena in the derived PDE system. We term it \"econotaxis.\"\n"
    },
    {
        "paper_id": 2303.09835,
        "authors": "Marcos Escobar-Anel, Michel Kschonnek and Rudi Zagst",
        "title": "Portfolio Optimization with Allocation Constraints and Stochastic Factor\n  Market Dynamics",
        "comments": "38 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  We study the expected utility portfolio optimization problem in an incomplete\nfinancial market where the risky asset dynamics depend on stochastic factors\nand the portfolio allocation is constrained to lie within a given convex set.\nWe employ fundamental duality results from real constrained optimization to\nformally derive a dual representation of the associated HJB PDE. Using this\nrepresentation, we provide a condition on the market dynamics and the\nallocation constraints, which ensures that the solution to the HJB PDE is\nexponentially affine and separable. This condition is used to derive an\nexplicit expression for the optimal allocation-constrained portfolio up to a\ndeterministic minimizer and the solution to a system of Riccati ODEs in a\nmarket with CIR volatility and in a market with multi-factor OU short rate.\n"
    },
    {
        "paper_id": 2303.10019,
        "authors": "Jonathan Berrisch, Florian Ziel",
        "title": "Multivariate Probabilistic CRPS Learning with an Application to\n  Day-Ahead Electricity Prices",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper presents a new method for combining (or aggregating or ensembling)\nmultivariate probabilistic forecasts, considering dependencies between\nquantiles and marginals through a smoothing procedure that allows for online\nlearning. We discuss two smoothing methods: dimensionality reduction using\nBasis matrices and penalized smoothing. The new online learning algorithm\ngeneralizes the standard CRPS learning framework into multivariate dimensions.\nIt is based on Bernstein Online Aggregation (BOA) and yields optimal asymptotic\nlearning properties. The procedure uses horizontal aggregation, i.e.,\naggregation across quantiles. We provide an in-depth discussion on possible\nextensions of the algorithm and several nested cases related to the existing\nliterature on online forecast combination. We apply the proposed methodology to\nforecasting day-ahead electricity prices, which are 24-dimensional\ndistributional forecasts. The proposed method yields significant improvements\nover uniform combination in terms of continuous ranked probability score\n(CRPS). We discuss the temporal evolution of the weights and hyperparameters\nand present the results of reduced versions of the preferred model. A fast C++\nimplementation of the proposed algorithm is provided in the open-source\nR-Package profoc on CRAN.\n"
    },
    {
        "paper_id": 2303.10043,
        "authors": "Hugo E. Ramirez and Juli\\'an Fernando Sanchez",
        "title": "Optimal liquidation with temporary and permanent price impact, an\n  application to cryptocurrencies",
        "comments": "20 pages, 20 figures, 3 tables",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  This paper studies the optimal liquidation of stocks in the presence of\ntemporary and permanent price impacts, and we focus in the case of\ncryptocurrencies. We start by presenting analytical solutions to the problem\nwith linear temporary impact, and linear and quadratic permanent impact. Then,\nusing data from the order book of the BNB cryptocurrency, we estimate the\nfunctional form of the temporary and permanent price impact in three different\nscenarios: underestimation, overestimation and average estimation, finding\ndifferent functional forms for each scenario. Using finite differences and\noptimal policy iteration, we solve the problem numerically and observe\ninteresting changes in the optimal liquidation policy when applying calibrated\nlinear and power forms for the temporary and permanent price impacts. Then,\nwith these optimal policies, we identify optimal liquidation trajectories and\nsimulate the liquidation of initial inventories to compare the performance\namong the optimal strategies under different parametrizations and against a\nnaive strategy. Finally, we characterize the optimal policies based on the\nfunctional form of the inventory and find that policies generating the highest\nrevenue are those starting with a low trading rate and increasing it as time\npasses.\n"
    },
    {
        "paper_id": 2303.1013,
        "authors": "Tyna Eloundou, Sam Manning, Pamela Mishkin, Daniel Rock",
        "title": "GPTs are GPTs: An Early Look at the Labor Market Impact Potential of\n  Large Language Models",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-sa/4.0/",
        "abstract": "  We investigate the potential implications of large language models (LLMs),\nsuch as Generative Pre-trained Transformers (GPTs), on the U.S. labor market,\nfocusing on the increased capabilities arising from LLM-powered software\ncompared to LLMs on their own. Using a new rubric, we assess occupations based\non their alignment with LLM capabilities, integrating both human expertise and\nGPT-4 classifications. Our findings reveal that around 80% of the U.S.\nworkforce could have at least 10% of their work tasks affected by the\nintroduction of LLMs, while approximately 19% of workers may see at least 50%\nof their tasks impacted. We do not make predictions about the development or\nadoption timeline of such LLMs. The projected effects span all wage levels,\nwith higher-income jobs potentially facing greater exposure to LLM capabilities\nand LLM-powered software. Significantly, these impacts are not restricted to\nindustries with higher recent productivity growth. Our analysis suggests that,\nwith access to an LLM, about 15% of all worker tasks in the US could be\ncompleted significantly faster at the same level of quality. When incorporating\nsoftware and tooling built on top of LLMs, this share increases to between 47\nand 56% of all tasks. This finding implies that LLM-powered software will have\na substantial effect on scaling the economic impacts of the underlying models.\nWe conclude that LLMs such as GPTs exhibit traits of general-purpose\ntechnologies, indicating that they could have considerable economic, social,\nand policy implications.\n"
    },
    {
        "paper_id": 2303.10367,
        "authors": "Carl Hase",
        "title": "Minimum Wage Pass-through to Wholesale and Retail Prices: Evidence from\n  Cannabis Scanner Data",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  A growing empirical literature finds that firms pass the cost of minimum wage\nhikes onto consumers via higher retail prices. Yet, little is known about\nminimum wage effects on wholesale prices and whether retailers face a wholesale\ncost shock in addition to the labor cost shock. I exploit the vertically\ndisintegrated market structure of Washington state's legal recreational\ncannabis industry to investigate minimum wage pass-through to wholesale and\nretail prices. In a difference-in-differences with continuous treatment\nframework, I utilize scanner data on $6 billion of transactions across the\nsupply chain and leverage geographic variation in firms' minimum wage exposure\nacross six minimum wage hikes between 2018 and 2021. When ignoring wholesale\ncost effects, I find retail pass-through elasticities consistent with existing\nliterature -- yet retail pass-through elasticities more than double once\nwholesale cost effects are accounted for. Retail markups do not adjust to the\nwholesale cost shock, indicating a full pass-through of the wholesale cost\nshock to retail prices. The results highlight the importance of analyzing the\nentire supply chain when evaluating the product market effects of minimum wage\nhikes.\n"
    },
    {
        "paper_id": 2303.10417,
        "authors": "Anton V. Proskurnikov, B. Ross Barmish",
        "title": "On the Benefit of Nonlinear Control for Robust Logarithmic Growth: Coin\n  Flipping Games as a Demonstration Case",
        "comments": null,
        "journal-ref": null,
        "doi": "10.1109/LCSYS.2023.3286312",
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  The takeoff point for this paper is the voluminous body of literature\naddressing recursive betting games with expected logarithmic growth of wealth\nbeing the performance criterion. Whereas almost all existing papers involve use\nof linear feedback, the use of nonlinear control is conspicuously absent. This\nis epitomized by the large subset of this literature dealing with Kelly\nBetting. With this as the high-level motivation, we study the potential for use\nof nonlinear control in this framework. To this end, we consider a\n``demonstration case'' which is one of the simplest scenarios encountered in\nthis line of research: repeated flips of a biased coin with probability of\nheads~$p$, and even-money payoff on each flip. First, we formulate a new robust\nnonlinear control problem which we believe is both simple to understand and\napropos for dealing with concerns about distributional robustness; i.e.,\ninstead of assuming that~$p$ is perfectly known as in the case of the classical\nKelly formulation, we begin with a bounding set ~${\\cal P} \\subseteq [0,1]$ for\nthis probability. Then, we provide a theorem, our main result, which gives a\nclosed-form description of the optimal robust nonlinear controller and a\ncorollary which establishes that it robustly outperforms linear controllers\nsuch as those found in the literature. A second, less significant, contribution\nof this paper bears upon the computability of our solution. For an $n$-flip\ngame, whereas an admissible controller has~$2^n-1$ parameters, at the optimum\nonly~$O(n^2)$ of them turn out to be distinct. Finally, it is noted that the\ninitial assumptions on payoffs and the use of the uniform distribution on~$p$\nare made solely for simplicity of the exposition and compliance with length\nrequirements for a Letter. Accordingly, the paper also includes a new section\nwith a discussion indicating how these assumptions can be relaxed.\n"
    },
    {
        "paper_id": 2303.10481,
        "authors": "Bilal Hungund, Shilpa Rastogi",
        "title": "Predictive Optimized Model on Money Markets Instruments With Capital\n  Market and Bank Rates Ratio",
        "comments": null,
        "journal-ref": null,
        "doi": "10.4018/IJDA.319024",
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  The money market and the capital market of the Indian financial markets have\na symbiotic relationship in the development of the Indian economy. The nature\nand the characteristics of the markets differ to a large extent as the money\nmarket ensures liquidity in the system through the monetary policy by the\nregulators; capital markets propel and act as the engine driver for the economy\nin the long term. Therefore, the final throughput of the economy is the\naggregation of the output of both the markets. Does that imply that the\ndevelopment of both markets is parallel in nature or is any one superior to the\nother or are they competitors? To understand the influence of one over the\nother the research was undertaken through a correlation matrix and time series\nmodel. A predictive model was further constructed for predicting the volume of\nmoney market instrument on the basis of fourteen days historical.\n"
    },
    {
        "paper_id": 2303.1055,
        "authors": "Greeshma Balabhadra, El Mehdi Ainasse, Pawel Polak",
        "title": "High-Frequency Volatility Estimation with Fast Multiple Change Points\n  Detection",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We propose a method for constructing sparse high-frequency volatility\nestimators that are robust against change points in the spot volatility\nprocess. The estimators we propose are $\\ell_1$-regularized versions of\nexisting volatility estimators. We focus on power variation estimators as they\nrepresent a fundamental class of volatility estimators. We establish\nconsistency of these estimators for the true unobserved volatility and the\nchange points locations, showing that minimax rates can be achieved for\nparticular volatility estimators. The new estimators utilize the\ncomputationally efficient least angle regression algorithm for estimation\npurposes, followed by a reduced dynamic programming step to refine the final\nnumber of change points. In terms of numerical performance, these estimators\nare not only computationally fast but also accurately identify breakpoints near\nthe end of the sample, both features highly desirable in today's electronic\ntrading environment. In terms of out-of-sample volatility prediction, our new\nestimators provide more realistic and smoother volatility forecasts,\noutperforming a broad range of classical and recent volatility estimators\nacross various frequencies and forecasting horizons.\n"
    },
    {
        "paper_id": 2303.10684,
        "authors": "Noam Angrist, Matthew C. H. Jukes, Sian Clarke, R. Matthew Chico,\n  Charles Opondo, Donald Bundy, Lauren M. Cohee",
        "title": "School-based malaria chemoprevention as a cost-effective approach to\n  improve cognitive and educational outcomes: a meta-analysis",
        "comments": "12 pages main text including 4 figures, Supplemental Information 5\n  pages with 6 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
        "abstract": "  There is limited evidence of health interventions impact on cognitive\nfunction and educational outcomes. We build on two prior systematic reviews to\nconduct a meta-analysis, exploring the effects of one of the most consequential\nhealth interventions, malaria chemoprevention, on education outcomes. We pool\ndata from nine study treatment groups (N=4,075) and outcomes across four\ncountries. We find evidence of a positive effect (Cohen's d = 0.12, 95% CI\n[0.08, 0.16]) on student cognitive function, achieved at low cost. These\nresults show that malaria chemoprevention can be highly cost effective in\nimproving some cognitive skills, such as sustained attention. Moreover, we\nconduct simulations using a new common metric (learning-adjusted years of\ndevelopment) to compare cost-effectiveness across diverse interventions. While\nwe might expect that traditional education interventions provide an immediate\nlearning gain, health interventions such as malaria prevention can have\nsurprisingly cost-effective education benefits, enabling children to achieve\ntheir full human capital potential.\n"
    },
    {
        "paper_id": 2303.10806,
        "authors": "Xin-Yu Wang and Chung-Han Hsieh",
        "title": "On Robustness of Double Linear Policy with Time-Varying Weights",
        "comments": "Submitted for possible publication",
        "journal-ref": "Proceedings of the IEEE Conference of Decision and Control (CDC),\n  2023",
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  In this paper, we extend the existing double linear policy by incorporating\ntime-varying weights instead of constant weights and study a certain robustness\nproperty, called robust positive expectation (RPE), in a discrete-time setting.\nWe prove that the RPE property holds by employing a novel elementary symmetric\npolynomials characterization approach and derive an explicit expression for\nboth the expected cumulative gain-loss function and its variance. To validate\nour theory, we perform extensive Monte Carlo simulations using various\nweighting functions. Furthermore, we demonstrate how this policy can be\neffectively incorporated with standard technical analysis techniques, using the\nmoving average as a trading signal.\n"
    },
    {
        "paper_id": 2303.10818,
        "authors": "Darryl Biggar",
        "title": "A Re-Examination of the Foundations of Cost of Capital for Regulatory\n  Purposes",
        "comments": "33 pages, 3 tables, 2 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  In regulatory proceedings, few issues are more hotly debated than the cost of\ncapital. This article formalises the theoretical foundation of cost of capital\nestimation for regulatory purposes. Several common regulatory practices lack a\nsolid foundation in the theory. For example, the common practice of estimating\na single cost of capital for the regulated firm suffers from a circularity\nproblem, especially in the context of a multi-year regulatory period. In\naddition, the relevant cost of debt cannot be estimated using the\nyield-to-maturity on a corporate bond. We suggest possible directions for\nreform of cost of capital practices in regulatory proceedings.\n"
    },
    {
        "paper_id": 2303.10835,
        "authors": "Xinyu Li",
        "title": "Bifurcation analysis of the Keynesian cross model",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  This study rigorously investigates the Keynesian cross model of a national\neconomy with a focus on the dynamic relationship between government spending\nand economic equilibrium. The model consists of two ordinary differential\nequations regarding the rate of change of national income and the rate of\nconsumer spending. Three dynamic relationships between national income and\ngovernment spending are studied. This study aims to classify the stabilities of\nequilibrium states for the economy by discussing different cases of government\nspending. Furthermore, the implication of government spending on the national\neconomy is investigated based on phase portraits and bifurcation analysis of\nthe dynamical system in each scenario.\n"
    },
    {
        "paper_id": 2303.10857,
        "authors": "Wenlong Wang and Thomas Pfeiffer",
        "title": "Proxy Forecasting to Avoid Stochastic Decision Rules in Decision Markets",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Information that is of relevance for decision-making is often distributed,\nand held by self-interested agents. Decision markets are well-suited mechanisms\nto elicit such information and aggregate it into conditional forecasts that can\nbe used for decision-making. However, for incentive-compatible elicitation,\ndecision markets rely on stochastic decision rules which entails that sometimes\nactions have to be taken that have been predicted to be sub-optimal. In this\nwork, we propose three closely related mechanisms that elicit and aggregate\ninformation similar to a decision market, but are incentive compatible despite\nusing a deterministic decision rule. Following ideas from peer prediction\nmechanisms, proxies rather than observed future outcomes are used to score\npredictions. The first mechanism requires the principal to have her own signal,\nwhich is then used as a proxy to elicit information from a group of\nself-interested agents. The principal then deterministically maps the\naggregated forecasts and the proxy to the best possible decision. The second\nand third mechanisms expand the first to cover a scenario where the principal\ndoes not have access to her own signal. The principal offers a partial profit\nto align the interest of one agent and retrieve its signal as a proxy; or\nalternatively uses a proper peer prediction mechanism to elicit signals from\ntwo agents. Aggregation and decision-making then follow the first mechanism. We\nevaluate our first mechanism using a multi-agent bandit learning system. The\nresult suggests that the mechanism can train agents to achieve a performance\nsimilar to a Bayesian inference model with access to all information held by\nthe agents.\n"
    },
    {
        "paper_id": 2303.10906,
        "authors": "Rene Laub, Klaus M. Miller, Bernd Skiera",
        "title": "The Economic Value of User Tracking for Publishers",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Regulators and browsers increasingly restrict user tracking to protect users'\nprivacy online. In two large-scale empirical studies, we study the economic\nimplications for publishers relying on selling advertising space to finance\ntheir content. In our first study, we draw on 42 million ad impressions from\n111 publishers covering EU desktop browsing traffic in 2016. In our second\nstudy, we use 218 million ad impressions from 10,526 publishers (i.e., apps)\ncovering EU and US mobile in-app browsing traffic in 2023. The two studies\ndiffer in the share of trackable users (Study 1: 85%; Study 2: Apple: 17%,\nAndroid: 91%). Still, we find similar average ad impression price decreases\n(Study 1: 18% and Study 2: 23%) when user tracking is unavailable. More than\n90% of the publishers realize lower prices when selling ad impressions for\nuntrackable users. Publishers offering content on sports, cars, lifestyle &\nshopping, and news & information suffer the most. Premium publishers with\nhigh-quality edited content and strong reputations, thematic-focused (niche)\npublishers, and smaller publishers suffer less from the unavailability of user\ntracking. In contrast, non-premium publishers with non-edited or user-generated\ncontent, thematic-broad (general news) publishers, and larger publishers suffer\nmore. The availability of a user ID generates the highest value for publishers,\nwhereas collecting a user's browsing history, perceived as intrusive by most\nusers, generates only a small value for publishers. These results affirm that\nensuring user privacy online has substantial costs for online publishers, but\nthose costs differ across publishers and the type of collected data. This\narticle offers suggestions to reduce these costs.\n"
    },
    {
        "paper_id": 2303.1091,
        "authors": "David Haritone Shikumo, Oluoch Oluoch, Joshua Matanda Wepukhulu",
        "title": "Financial Structure, Firm Size and Financial Growth of Non-Financial\n  Firms Listed at the Nairobi Securities Exchange",
        "comments": "14 pages. arXiv admin note: text overlap with arXiv:2108.10244,\n  arXiv:2011.03339, arXiv:2010.12596",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/publicdomain/zero/1.0/",
        "abstract": "  A significant number of the non-financial firms listed at the Nairobi\nSecurities Exchange have been experiencing declining financial performance and\nfinancial growth, which deter investors from investing in such firms. Hence,\nthe study aimed at establishing the effect of financial structure on the\nfinancial growth of non-financial firms listed at the Nairobi Securities\nExchange.\n"
    },
    {
        "paper_id": 2303.11013,
        "authors": "Francesco Farina, Mike Arpaia, Harpal Khing, Jonas Vetterle",
        "title": "Venture Capital Portfolio Construction and the Main Factors Impacting\n  the Optimal Strategy",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The optimal portfolio size for a venture capital (VC) fund is a topic often\ndebated, but there is no consensus on the best strategy. This is because it is\na function of many factors. It is not easy to find a general formula that can\nbe applied to all situations, and it largely depends on the goal of the fund.\nIn this report, we will go through the different factors step by step, studying\nhow they affect fund returns and the optimal portfolio size, starting with some\nbasic assumptions and then increasing the complexity of the model.\n"
    },
    {
        "paper_id": 2303.1103,
        "authors": "Yun-Shi Dai, Peng-Fei Dai, Wei-Xing Zhou",
        "title": "Tail dependence structure and extreme risk spillover effects between the\n  international agricultural futures and spot markets",
        "comments": "37 pages, 7 figures",
        "journal-ref": "Journal of International Financial Markets, Institutions and Money\n  88, 101820 (2023)",
        "doi": "10.1016/j.intfin.2023.101820",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper combines the Copula-CoVaR approach with the ARMA-GARCH-skewed\nStudent-t model to investigate the tail dependence structure and extreme risk\nspillover effects between the international agricultural futures and spot\nmarkets, taking four main agricultural commodities, namely soybean, maize,\nwheat, and rice as examples. The empirical results indicate that the tail\ndependence structures for the four futures-spot pairs are quite different, and\neach of them exhibits a certain degree of asymmetry. In addition, the futures\nmarket for each agricultural commodity has significant and robust extreme\ndownside and upside risk spillover effects on the spot market, and the downside\nrisk spillover effects for both soybeans and maize are significantly stronger\nthan their corresponding upside risk spillover effects, while there is no\nsignificant strength difference between the two risk spillover effects for\nwheat, and rice. This study provides a theoretical basis for strengthening\nglobal food cooperation and maintaining global food security, and has practical\nsignificance for investors to use agricultural commodities for risk management\nand portfolio optimization.\n"
    },
    {
        "paper_id": 2303.11064,
        "authors": "Raffaele Mattera and Philipp Otto",
        "title": "Network log-ARCH models for forecasting stock market volatility",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  This paper presents a novel dynamic network autoregressive conditional\nheteroscedasticity (ARCH) model based on spatiotemporal ARCH models to forecast\nvolatility in the US stock market. To improve the forecasting accuracy, the\nmodel integrates temporally lagged volatility information and information from\nadjacent nodes, which may instantaneously spill across the entire network. The\nmodel is also suitable for high-dimensional cases where multivariate ARCH\nmodels are typically no longer applicable. We adopt the theoretical foundations\nfrom spatiotemporal statistics and transfer the dynamic ARCH model for\nprocesses to networks. This new approach is compared with independent\nunivariate log-ARCH models. We could quantify the improvements due to the\ninstantaneous network ARCH effects, which are studied for the first time in\nthis paper. The edges are determined based on various distance and correlation\nmeasures between the time series. The performances of the alternative networks'\ndefinitions are compared in terms of out-of-sample accuracy. Furthermore, we\nconsider ensemble forecasts based on different network definitions.\n"
    },
    {
        "paper_id": 2303.11118,
        "authors": "Masaaki Fukasawa, Basile Maire, and Marcus Wunsch",
        "title": "Model-free Hedging of Impermanent Loss in Geometric Mean Market Makers",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We consider Geometric Mean Market Makers -- a special type of Decentralized\nExchange -- with two types of users: liquidity takers and arbitrageurs.\nLiquidity takers trade at prices that can create arbitrage opportunities, while\narbitrageurs align the exchange's price with the external market price. We show\nthat in Geometric Mean Market Makers charging proportional transaction fees,\nImpermanent Loss can be super-hedged by a model-free rebalancing strategy.\nMoreover, we demonstrate that in such a DEX, the exchange rate is of finite\nvariation, so that loss-versus-rebalancing (the shortfall of providing\nliquidity versus the corresponding constant-weights portfolio) vanishes.\n"
    },
    {
        "paper_id": 2303.11414,
        "authors": "Dipti Rani Hazra, Md. Shah Naoaj, Mohammed Mahinur Alam, Abdul Kader",
        "title": "Cost of Implementation of Basel III reforms in Bangladesh -- A Panel\n  data analysis",
        "comments": "13 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Inspired by the recent debate on the macroeconomic implications of the new\nbank regulatory standards known as Basel III, we tried to find out in this\nstudy that the impact of Basel III liquidity and capital requirements in\nBangladesh proposed by Basel Committee on Banking Supervision (BCBS, 2010a). A\nsmall set of macro variables, using a sample of 22 private commercial banks\noperating in Bangladesh for the period of 2010-2014, are used to estimate\nlong-run relationships among the variables. The macroeconomic variables are\nincluded The profitability of banks, GDP, banks' lending to private sector, Net\nStable Funding Ratio, Tier 1 capital Ratio, Interest rate spread, real interest\nrate. The cost is quantified using Driscoll and Kraay panel data models with\nfixed effect. Impact of higher capital and liquidity requirement on Interest\nrate spread and lending to private sector of banks were considered as the cost\nto the economy as a whole whereas impact of higher capital and liquidity\nrequirement on profitability of banks(ROE) was considered as the cost of banks.\nHere it is found that, the interest rate level is positively affected by the\ntighter liquidity and capital requirements which driven toward lessen of the\nprivate sector lending of banks. The return on equity of banks varies\nnegatively with the liquidity and capital. The economic costs are considerably\nbelow the estimated positive benefit that the reform should have by reducing\nthe probability of banking crises and the associated banking losses (BCBS,\n2010b).\n"
    },
    {
        "paper_id": 2303.11716,
        "authors": "Dapeng Li, Feiyang Pan, Jia He, Zhiwei Xu, Dandan Tu, Guoliang Fan",
        "title": "Style Miner: Find Significant and Stable Explanatory Factors in Time\n  Series with Constrained Reinforcement Learning",
        "comments": "9 pages, 6 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In high-dimensional time-series analysis, it is essential to have a set of\nkey factors (namely, the style factors) that explain the change of the observed\nvariable. For example, volatility modeling in finance relies on a set of risk\nfactors, and climate change studies in climatology rely on a set of causal\nfactors. The ideal low-dimensional style factors should balance significance\n(with high explanatory power) and stability (consistent, no significant\nfluctuations). However, previous supervised and unsupervised feature extraction\nmethods can hardly address the tradeoff. In this paper, we propose Style Miner,\na reinforcement learning method to generate style factors. We first formulate\nthe problem as a Constrained Markov Decision Process with explanatory power as\nthe return and stability as the constraint. Then, we design fine-grained\nimmediate rewards and costs and use a Lagrangian heuristic to balance them\nadaptively. Experiments on real-world financial data sets show that Style Miner\noutperforms existing learning-based methods by a large margin and achieves a\nrelatively 10% gain in R-squared explanatory power compared to the\nindustry-renowned factors proposed by human experts.\n"
    },
    {
        "paper_id": 2303.11956,
        "authors": "David Roodman",
        "title": "Large-Scale Education Reform in General Equilibrium: Regression\n  Discontinuity Evidence from India: Comment",
        "comments": "18 pages + references + appendices; 2 figures + 3 tables in main text",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
        "abstract": "  This paper reanalyzes Khanna (2023), which studies labor market effects of\nschooling in India through regression discontinuity designs. Absent from the\ndata are four districts close to the discontinuity; restoring them cuts the\nreduced-form impacts on schooling and log wages by 57% and 63%. Using\nregression-specific optimal band-widths and a robust variance estimator\nclustered at the geographic unit of treatment makes impacts statistically\nindistinguishable from 0. That finding is robust to varying the identifying\nthreshold and the bandwidth. The estimates of general equilibrium effects and\nelasticities of substitution are not unbiased and have effectively infinite\nfirst and second moments.\n"
    },
    {
        "paper_id": 2303.11959,
        "authors": "Hengxi Zhang, Zhendong Shi, Yuanquan Hu, Wenbo Ding, Ercan E.\n  Kuruoglu, Xiao-Ping Zhang",
        "title": "Optimizing Trading Strategies in Quantitative Markets using Multi-Agent\n  Reinforcement Learning",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Quantitative markets are characterized by swift dynamics and abundant\nuncertainties, making the pursuit of profit-driven stock trading actions\ninherently challenging. Within this context, reinforcement learning (RL), which\noperates on a reward-centric mechanism for optimal control, has surfaced as a\npotentially effective solution to the intricate financial decision-making\nconundrums presented. This paper delves into the fusion of two established\nfinancial trading strategies, namely the constant proportion portfolio\ninsurance (CPPI) and the time-invariant portfolio protection (TIPP), with the\nmulti-agent deep deterministic policy gradient (MADDPG) framework. As a result,\nwe introduce two novel multi-agent RL (MARL) methods, CPPI-MADDPG and\nTIPP-MADDPG, tailored for probing strategic trading within quantitative\nmarkets. To validate these innovations, we implemented them on a diverse\nselection of 100 real-market shares. Our empirical findings reveal that the\nCPPI-MADDPG and TIPP-MADDPG strategies consistently outpace their traditional\ncounterparts, affirming their efficacy in the realm of quantitative trading.\n"
    },
    {
        "paper_id": 2303.12209,
        "authors": "Young Shin Kim",
        "title": "Portfolio Optimization with Relative Tail Risk",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper proposes analytic forms of portfolio CoVaR and CoCVaR on the\nnormal tempered stable market model. Since CoCVaR captures the relative risk of\nthe portfolio with respect to a benchmark return, we apply it to the relative\nportfolio optimization. Moreover, we derive analytic forms for the marginal\ncontribution to CoVaR and the marginal contribution to CoCVaR. We discuss the\nMonte-Carlo simulation method to calculate CoCVaR and the marginal\ncontributions of CoVaR and CoCVaR. As the empirical illustration, we show\nrelative portfolio optimization with thirty stocks under the distress condition\nof the Dow Jones Industrial Average. Finally, we perform the risk budgeting\nmethod to reduce the CoVaR and CoCVaR of the portfolio based on the marginal\ncontributions to CoVaR and CoCVaR.\n"
    },
    {
        "paper_id": 2303.1235,
        "authors": "Qian Qi",
        "title": "Artificial Intelligence and Dual Contract",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper explores the capacity of artificial intelligence (AI) algorithms\nto autonomously design incentive-compatible contracts in dual-principal-agent\nsettings, a relatively unexplored aspect of algorithmic mechanism design. We\ndevelop a dynamic model where two principals, each equipped with independent\nQ-learning algorithms, interact with a single agent. Our findings reveal that\nthe strategic behavior of AI principals (cooperation vs. competition) hinges\ncrucially on the alignment of their profits. Notably, greater profit alignment\nfosters collusive strategies, yielding higher principal profits at the expense\nof agent incentives. This emergent behavior persists across varying degrees of\nprincipal heterogeneity, multiple principals, and environments with\nuncertainty. Our study underscores the potential of AI for contract automation\nwhile raising critical concerns regarding strategic manipulation and the\nemergence of unintended collusion in AI-driven systems, particularly in the\ncontext of the broader AI alignment problem.\n"
    },
    {
        "paper_id": 2303.12483,
        "authors": "Giulia Livieri, Davide Radi, Elia Smaniotto",
        "title": "Pricing Transition Risk with a Jump-Diffusion Credit Risk Model:\n  Evidences from the CDS market",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Transition risk can be defined as the business-risk related to the enactment\nof green policies, aimed at driving the society towards a sustainable and\nlow-carbon economy. In particular, the value of certain firms' assets can be\nlower because they need to transition to a less carbon-intensive business\nmodel. In this paper we derive formulas for the pricing of defaultable coupon\nbonds and Credit Default Swaps to empirically demonstrate that a jump-diffusion\ncredit risk model in which the downward jumps in the firm value are due to\ntighter green laws can capture, at least partially, the transition risk. The\nempirical investigation consists in the model calibration on the CDS\nterm-structure, performing a quantile regression to assess the relationship\nbetween implied prices and a proxy of the transition risk. Additionally, we\nshow that a model without jumps lacks this property, confirming the jump-like\nnature of the transition risk.\n"
    },
    {
        "paper_id": 2303.12527,
        "authors": "Annika Kemper and Maren Diane Schmeck",
        "title": "The Market Price of Jump Risk for Delivery Periods: Pricing of\n  Electricity Swaps with Geometric Averaging",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper, we extend the market price of risk for delivery periods (MPDP)\nof electricity swap contracts by introducing a dimension for jump risk. As\nintroduced by Kemper et al. (2022), the MPDP arises through the use of\ngeometric averaging while pricing electricity swaps in a geometric framework.\nWe adjust the work by Kemper et al. (2022) in two directions: First, we examine\na Merton type model taking jumps into account. Second, we transfer the model to\nthe physical measure by implementing mean-reverting behavior. We compare swap\nprices resulting from the classical arithmetic (approximated) average to the\ngeometric weighted average. Under the physical measure, we discover a\ndecomposition of the swap's market price of risk into the classical one and the\nMPDP.\n"
    },
    {
        "paper_id": 2303.12567,
        "authors": "Andrew Lyasoff",
        "title": "Self-Aware Transport of Economic Agents",
        "comments": "53 pages, 16 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The paper is concerned with the simultaneous solution of a very large number\nof optimization problems the structure of which is not given in the outset and\nis dynamically agreed upon while a large number of optimizers work in an\norchestra. The proposed new approach to such problems was prompted by the\nsurprising discovery that the common strategy, adopted in a large body of\nresearch, for producing time-invariant equilibrium in the classical\nAiyagari-Bewley-Huggett model fails to achieve its objective in a widely cited\nbenchmark study, with the implication that a central problem in macroeconomics\nhas been without an adequate solution, and despite recent advances based on\nnovel mathematical techniques borrowed from the theory of mean field games. It\nis shown that the intrinsic structure of a generic heterogeneous agent\nincomplete market model imposes connections across time that existing\nmathematical frameworks cannot capture. The new technique is shown to provide\nnumerically verifiable equilibria in some widely researched, yet still\nunsolved, concrete instances of heterogeneous agent models. The scope of \"the\napproximate aggregation conjecture\" of Krusell and Smith (still an open problem\nin macroeconomics) is clarified and a new computational strategy, which does\nnot rely on simulation or the need to postulate infinite time horizon, for\nmodels with common shocks is developed. New insights about the fluctuations in\nthe population distribution in such models are drawn and some novel closed-form\nexpressions are obtained.\n"
    },
    {
        "paper_id": 2303.12751,
        "authors": "Weichuan Deng, Pawel Polak, Abolfazl Safikhani, Ronakdilip Shah",
        "title": "A Unified Framework for Fast Large-Scale Portfolio Optimization",
        "comments": "35 pages, 11 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We introduce a unified framework for rapid, large-scale portfolio\noptimization that incorporates both shrinkage and regularization techniques.\nThis framework addresses multiple objectives, including minimum variance,\nmean-variance, and the maximum Sharpe ratio, and also adapts to various\nportfolio weight constraints. For each optimization scenario, we detail the\ntranslation into the corresponding quadratic programming (QP) problem and then\nintegrate these solutions into a new open-source Python library. Using 50 years\nof return data from US mid to large-sized companies, and 33 distinct\nfirm-specific characteristics, we utilize our framework to assess the\nout-of-sample monthly rebalanced portfolio performance of widely-adopted\ncovariance matrix estimators and factor models, examining both daily and\nmonthly returns. These estimators include the sample covariance matrix, linear\nand nonlinear shrinkage estimators, and factor portfolios based on Asset\nPricing (AP) Trees, Principal Component Analysis (PCA), Risk Premium PCA\n(RP-PCA), and Instrumented PCA (IPCA). Our findings emphasize that AP-Trees and\nPCA-based factor models consistently outperform all other approaches in\nout-of-sample portfolio performance. Finally, we develop new l1 and l2\nregularizations of factor portfolio norms which not only elevate the portfolio\nperformance of AP-Trees and PCA-based factor models but they have a potential\nto reduce an excessive turnover and transaction costs often associated with\nthese models.\n"
    },
    {
        "paper_id": 2303.13282,
        "authors": "\\'Alvaro Rubio-Garc\\'ia and Samuel Fern\\'andez-Lorenzo and Juan Jos\\'e\n  Garc\\'ia-Ripoll and Diego Porras",
        "title": "Accurate solution of the Index Tracking problem with a hybrid simulated\n  annealing algorithm",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  An actively managed portfolio almost never beats the market in the long term.\nThus, many investors often resort to passively managed portfolios whose aim is\nto follow a certain financial index. The task of building such passive\nportfolios aiming also to minimize the transaction costs is called Index\nTracking (IT), where the goal is to track the index by holding only a small\nsubset of assets in the index. As such, it is an NP-hard problem and becomes\nunfeasible to solve exactly for indices with more than 100 assets. In this\nwork, we present a novel hybrid simulated annealing method that can efficiently\nsolve the IT problem for large indices and is flexible enough to adapt to\nfinancially relevant constraints. By tracking the S&P-500 index between the\nyears 2011 and 2018 we show that our algorithm is capable of finding optimal\nsolutions in the in-sample period of past returns and can be tuned to provide\noptimal returns in the out-of-sample period of future returns. Finally, we\nfocus on the task of holding an IT portfolio during one year and rebalancing\nthe portfolio every month. Here, our hybrid simulated annealing algorithm is\ncapable of producing financially optimal portfolios already for small subsets\nof assets and using reasonable computational resources, making it an\nappropriate tool for financial managers.\n"
    },
    {
        "paper_id": 2303.13319,
        "authors": "Pascal Michaillat",
        "title": "Modeling the Displacement of Native Workers by Immigrants",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Immigrants are always accused of stealing people's jobs. Yet, by assumption,\nstandard immigration models -- the neoclassical model and\nDiamond-Mortensen-Pissarides matching model -- rule out displacement of native\nworkers by immigrants. In these models, when immigrants enter the labor force,\nthey are absorbed by firms without taking jobs away from native jobseekers.\nThis paper develops a more general model of immigration, which allows for\ndisplacement of native workers by immigrants. Such generalization seems crucial\nto understand and study all the possible effects of immigration on labor\nmarkets. The model blends a matching framework with job rationing. In it, the\nentry of immigrants increases the unemployment rate of native workers.\nMoreover, the reduction in employment rate is sharper when the labor market is\ndepressed because jobs are scarcer then. On the plus side, immigration makes it\neasier for firms to recruit, which improves firm profits. The overall effect of\nimmigration on native welfare depends on the state of the labor market.\nImmigration always reduces welfare when the labor market is inefficiently\nslack, but some immigration improves welfare when the labor market is\ninefficiently tight.\n"
    },
    {
        "paper_id": 2303.13346,
        "authors": "Giovanni Amici, Paolo Brandimarte, Francesco Messeri, Patrizia\n  Semeraro",
        "title": "Multivariate L\\'evy Models: Calibration and Pricing",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The goal of this paper is to investigate how the marginal and dependence\nstructures of a variety of multivariate L\\'evy models affect calibration and\npricing. To this aim, we study the approaches of Luciano and Semeraro (2010)\nand Ballotta and Bonfiglioli (2016) to construct multivariate processes. We\nexplore several calibration methods that can be used to fine-tune the models,\nand that deal with the observed trade-off between marginal and correlation fit.\nWe carry out a thorough empirical analysis to evaluate the ability of the\nmodels to fit market data, price exotic derivatives, and embed a rich\ndependence structure. By merging theoretical aspects with the results of the\nempirical test, we provide tools to make suitable decisions about the models\nand calibration techniques to employ in a real context.\n"
    },
    {
        "paper_id": 2303.13669,
        "authors": "Kate Schneider (1), Jessica Fanzo (1), Lawrence Haddad (2), Mario\n  Herrero (3), Jose Rosero Moncayo (4), Anna Herforth (5), Roseline Reman (6),\n  Alejandro Guarin (7), Danielle Resnick (8), Namukolo Covic (9), Christophe\n  B\\'en\\'e (6 and 10), Andrea Cattaneo (4), Nancy Aburto (4), Ramya Ambikapathi\n  (3), Destan Aytekin (1), Simon Barquera (11), Jane Battersby-Lennard (12), Ty\n  Beal (2), Paulina Bizzoto Molina (13), Carlo Cafiero (4), Christine Campeau\n  (14), Patrick Caron (15), Piero Conforti (4), Kerstin Damerau (3), Michael\n  DiGirolamo (1), Fabrice DeClerck (16), Deviana Dewi (1), Ismahane Elouafi\n  (4), Carola Fabi (4), Pat Foley (17), Ty Frazier (18), Jessica Gephart (19),\n  Christopher Golden (5), Carlos Gonzalez Fischer (3), Sheryl Hendriks (20),\n  Maddalena Honorati (21), Jikun Huang (22), Gina Kennedy (2), Amos Laar (23),\n  Rattan Lal (24), Preetmoninder Lidder (4), Brent Loken (25), Quinn Marshall\n  (26), Yuta Masuda (27), Rebecca McLaren (1), Lais Miachon (1), Hern\\'an\n  Mu\\~noz (4), Stella Nordhagen (2), Naina Qayyum (28), Michaela Saisana (29),\n  Diana Suhardiman (30), Rashid Sumaila (31), Maximo Torrero Cullen (4),\n  Francesco Tubiello (4), Jose-Luis Vivero-Pol (17), Patrick Webb (28), Keith\n  Wiebe (26) ((1) Johns Hopkins University (2) Global Alliance for Improved\n  Nutrition (GAIN) (3) Cornell University (4) Food and Agriculture Organization\n  of the United Nations (FAO) (5) Harvard University (6) Alliance of\n  CIAT-Bioversity (7) International Institute for Environment & Development\n  (IIED) (8) Brookings Institution (9) International Livestock Research\n  Institute (ILRI) (10) Wageningen Economic Research Group (11) Instituto\n  Nacional de Salud P\\'ublica (INSP), M\\'exico (12) University of Cape Town\n  (13) European Centre for Development Policy Management (14) CARE (15)\n  University of Montpellier, Cirad, ART-DEV (16) EAT Forum (17) United Nations\n  World Food Programme (WFP) (18) Oakridge National Laboratory (19) American\n  University (20) University of Greenwich (21) World Bank (22) Peking\n  University (23) University of Ghana (24) Ohio State University (25) World\n  Wildlife Fund (WWF) (26) International Food Policy Research Institute (27)\n  Vulcan (28) Tufts University (29) Joint Research Centre (JRC) of the European\n  Commission (30) KIT Royal Tropical Institute (Netherlands) (31) University of\n  British Columbia)",
        "title": "The State of Food Systems Worldwide: Counting Down to 2030",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Transforming food systems is essential to bring about a healthier, equitable,\nsustainable, and resilient future, including achieving global development and\nsustainability goals. To date, no comprehensive framework exists to track food\nsystems transformation and their contributions to global goals. In 2021, the\nFood Systems Countdown to 2030 Initiative (FSCI) articulated an architecture to\nmonitor food systems across five themes: 1 diets, nutrition, and health; 2\nenvironment, natural resources, and production; 3 livelihoods, poverty, and\nequity; 4 governance; and 5 resilience and sustainability. Each theme comprises\nthree-to-five indicator domains. This paper builds on that architecture,\npresenting the inclusive, consultative process used to select indicators and an\napplication of the indicator framework using the latest available data,\nconstructing the first global food systems baseline to track transformation.\nWhile data are available to cover most themes and domains, critical indicator\ngaps exist such as off-farm livelihoods, food loss and waste, and governance.\nBaseline results demonstrate every region or country can claim positive\noutcomes in some parts of food systems, but none are optimal across all\ndomains, and some indicators are independent of national income. These results\nunderscore the need for dedicated monitoring and transformation agendas\nspecific to food systems. Tracking these indicators to 2030 and beyond will\nallow for data-driven food systems governance at all scales and increase\naccountability for urgently needed progress toward achieving global goals.\n"
    },
    {
        "paper_id": 2303.13956,
        "authors": "Yukihiro Tsuzuki",
        "title": "Pitman's Theorem, Black-Scholes Equation, and Derivative Pricing for\n  Fundraisers",
        "comments": "25 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We propose a financial market model that comprises a savings account and a\nstock, where the stock price process is modeled as a one-dimensional diffusion,\nwherein two types of agents exist: an ordinary investor and a fundraiser who\nbuys or sells stocks as funding activities. Although the investor information\nis the natural filtration of the diffusion, the fundraiser possesses extra\ninformation regarding the funding, as well as additional cash flows as a result\nof the funding. This concept is modeled using Pitman's theorem for the\nthree-dimensional Bessel process. Two contributions are presented: First, the\nprices of European options for the fundraiser are derived. Second, a numerical\nscheme is proposed for call option prices in a market with a bubble, where\nmultiple solutions exist for the Black-Scholes equation and the derivative\nprices are characterized as the smallest nonnegative supersolution. More\nprecisely, the call option price in such a market is approximated from below by\nthe prices for the fundraiser. This scheme overcomes the difficulty that stems\nfrom the discrepancy that the payoff shows linear growth, whereas the price\nfunction shows strictly sublinear growth.\n"
    },
    {
        "paper_id": 2303.13966,
        "authors": "Martin Keller-Ressel, Felix Sachse",
        "title": "State space decomposition and classification of term structure shapes in\n  the two-factor Vasicek model",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Using the concept of envelopes we show how to divide the state space $\\RR^2$\nof the two-factor Vasicek model into regions of identical term-structure shape.\nWe develop a formula for determining the shapes utilizing winding numbers and\ngive a nearly complete classification of the parameter space regarding the\noccurring shapes.\n"
    },
    {
        "paper_id": 2303.14182,
        "authors": "Theresa Graefe",
        "title": "The effect of the Austrian-German bidding zone split on unplanned\n  cross-border flows",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  In 2013, TSOs from the Central European Region complained to the Agency for\nthe Cooperation of Energy Regulators because of increasing unplanned flows that\nwere presumed to be caused by a joint German-Austrian bidding zone in the\nEuropean electricity market. This paper empirically analyses the effects of the\nsplit of this bidding zone in 2018 on planned and unplanned cross-border flows\nbetween Germany, Austria, Poland, the Czech Republic, Slovakia, and Hungary.\nFor all bidding zones, apart from the German-Austrian one, planned flows\nincreased. Further, I find that around the policy intervention between 2017 and\n2019, unplanned flows between Germany and Austria as well as for the Czech\nRepublic and Slovakia decreased. However, for Poland increasing unplanned flows\nare found.\n"
    },
    {
        "paper_id": 2303.14232,
        "authors": "Stephenson Strobel",
        "title": "Effects of extending residencies on the supply and quality of family\n  medicine practitioners; difference-in-differences evidence from the\n  implementation of mandatory family medicine residencies in Canada",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  I examine the impacts of extending residency training programs on the supply\nand quality of physicians practicing primary care. I leverage mandated extended\nresidency lengths for primary care practitioners that were rolled out over 20\nyears in Canada on a province-by-province basis. I compare these primary care\nspecialties to other specialties that did not change residency length (first\ndifference) before and after the policy implementation (second difference) to\nassess how physician supply evolved in response. To examine quality outcomes, I\nuse a set of scraped data and repeat this difference-in-differences\nidentification strategy for complaints resulting in censure against physicians\nin Ontario.\n  I find declines in the number of primary care providers by 5% for up to nine\nyears after the policy change. These changes are particularly pronounced in new\ngrads and younger physicians suggesting that the policy change dissuaded these\nphysicians from entering primary care residencies. I find no impacts on quality\nof physician as measured by public censure of physicians. This suggests that\nextending primary care training caused declines in physician supply without any\nconcomitant improvement in the quality of these physicians. This has\nimplications for current plans to extend residency training programs.\n"
    },
    {
        "paper_id": 2303.14263,
        "authors": "Ruiqi Rich Zhu, Cheng He, Yu Jeffrey Hu",
        "title": "The Effect of Product Recommendations on Online Investor Behaviors",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Despite the popularity of product recommendations on online investment\nplatforms, few studies have explored their impact on investor behaviors. Using\ndata from a global e-commerce platform, we apply regression discontinuity\ndesign to causally examine the effects of product recommendations on online\ninvestors' mutual fund investments. Our findings indicate that recommended\nfunds experience a significant rise in purchases, especially among low\nsocioeconomic status investors who are most influenced by these\nrecommendations. However, investors tend to suffer significantly worse\ninvestment returns after purchasing recommended funds, and this negative impact\nis also most significant for investors with low socioeconomic status. To\nexplain this disparity, we find investors tend to gather less information and\nexpend reduced effort in fund research when buying recommended funds.\nFurthermore, investors' redemption timing of recommended funds is less optimal\nthan non-recommended funds. We also find that recommended funds experience a\nlarger return reversal than non-recommended funds. In conclusion, product\nrecommendations make investors behave more irrationally and these negative\nconsequences are most significant for investors with low socioeconomic status,\nwhich can amplify wealth inequality among investors in financial markets.\n"
    },
    {
        "paper_id": 2303.14447,
        "authors": "Carlos Ferreira",
        "title": "Foreign participation in federal biddings: A quantitative approach using\n  the procurement panel",
        "comments": "12 pages, 7 figures, 2 tables",
        "journal-ref": "Revista Do Servi\\c{c}o P\\'ublico, 72(4), 779-802,2021",
        "doi": "10.21874/rsp.v72.i4.4628",
        "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
        "abstract": "  The bidding is the Public Administration's administrative process and other\ndesignated persons by law to select the best proposal, through objective and\nimpersonal criteria, for contracting services and purchasing goods. In times of\nglobalization, it is common for companies seeking to expand their business by\nparticipating in biddings. Brazilian legislation allows the participation of\nforeign suppliers in bids held in the country. Through a quantitative approach,\nthis article discusses the weight of foreign suppliers' involvement in federal\nbidding between 2011 and 2018. To this end, a literature review was conducted\non public procurement and international biddings. Besides, an extensive data\nsearch was achieved through the Federal Government Procurement Panel. The\nresults showed that between 2011 and 2018, more than R\\$ 422.6 billion was\nconfirmed in public procurement processes, and of this total, about R\\$ 28.9\nbillion was confirmed to foreign suppliers. The Ministry of Health accounted\nfor approximately 88.67% of these confirmations. The Invitation, Competition\nand International Competition modalities accounted for 0.83% of the amounts\nconfirmed to foreign suppliers. Impossible Bidding, Waived Bidding, and Reverse\nAuction modalities accounted for 99.17% of the confirmed quantities to foreign\nsuppliers. Based on the discussion of the results and the limitations found,\nsome directions for further studies and measures to increase public resources\nexpenditures' effectiveness and efficiency are suggested.\n"
    },
    {
        "paper_id": 2303.14458,
        "authors": "\\\"Onder Nomaler and Bart Verspagen",
        "title": "Related or Unrelated Diversification: What is Smart Specialization?",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  In this paper, we investigate the nature of the density metric, which is\nemployed in the literature on smart specialization and the product space. We\nfind that although density is supposed to capture relatedness between a\ncountry's current specialization pattern and potential products that it may\ndiversify into, density is also correlated strongly to the level of\ndiversification of the country, and (less strongly) to the ubiquity of the\nproduct. Together, diversity and ubiquity capture 93% of the variance of\ndensity. We split density into a part that corresponds to related variety, and\na part that does not (i.e., unrelated variety). In regressions for predicting\ngain or loss of specialization, both these parts are significant. The relative\ninfluence of related variety increases with the level of diversification of the\ncountry: only countries that are already diversified show a strong influence of\nrelated variety. In our empirical analysis, we put equal emphasis on gains and\nlosses of specialization. Our data show that the specializations that were lost\nby a country often represented higher product complexity than the\nspecializations that were gained over the same period. This suggests that\n'smart' specialization should be aimed at preserving (some) existing\nspecializations in addition to gaining new ones. Our regressions indicate that\nthe relative roles of related and unrelated variety for explaining loss of\nspecialization are similar to the case of specialization gains. Finally, we\nalso show that unrelated variety is also important in indicators that are\nderived from density, such as the Economic Complexity Outlook Index.\n"
    },
    {
        "paper_id": 2303.14486,
        "authors": "Sebastian T. Braun and Jan Stuhler",
        "title": "Exposure to War and Its Labor Market Consequences over the Life Cycle",
        "comments": "JEL Code: J24, J26, N34, Keywords: World War II; labor market\n  careers; war injuries; prisoners of war, displacement; life-cycle models",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
        "abstract": "  With 70 million dead, World War II remains the most devastating conflict in\nhistory. Of the survivors, millions were displaced, returned maimed from the\nbattlefield, or spent years in captivity. We examine the impact of such wartime\nexperiences on labor market careers and show that they often become apparent\nonly at certain life stages. While war injuries reduced employment in old age,\nformer prisoners of war postponed their retirement. Many displaced workers,\nparticularly women, never returned to employment. These responses are in line\nwith standard life-cycle theory and thus likely extend to other conflicts.\n"
    },
    {
        "paper_id": 2303.14515,
        "authors": "Christian Mitsch",
        "title": "Specific investments under negotiated transfer pricing: effects of\n  different surplus sharing parameters on managerial performance: An\n  agent-based simulation with fuzzy Q-learning agents",
        "comments": "arXiv admin note: substantial text overlap with arXiv:2301.12255",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  This paper focuses on a decentralized profit-center firm that uses negotiated\ntransfer pricing as an instrument to coordinate the production process.\nMoreover, the firm's headquarters gives its divisions full authority over\noperating decisions and it is assumed that each division can additionally make\nan upfront investment decision that enhances the value of internal trade. On\nearly works, the paper expands the number of divisions by one downstream\ndivision and relaxes basic assumptions, such as the assumption of common\nknowledge of rationality. Based on an agent-based simulation, it is examined\nwhether cognitively bounded individuals modeled by fuzzy Q-learning achieve the\nsame results as fully rational utility maximizers. In addition, the paper\ninvestigates different constellations of bargaining power to see whether a\ndeviation from the recommended optimal bargaining power leads to a higher\nmanagerial performance. The simulation results show that fuzzy Q-learning\nagents perform at least as well or better than fully individual rational\nutility maximizers. The study also indicates that, in scenarios with different\nmarginal costs of divisions, a deviation from the recommended optimal\ndistribution ratio of the bargaining power of divisions can lead to higher\ninvestment levels and, thus, to an increase in the headquarters' profit.\n"
    },
    {
        "paper_id": 2303.14533,
        "authors": "Carter Davis",
        "title": "The Elasticity of Quantitative Investment",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  What is the demand elasticity of statistical arbitrageurs that invest\naccording to the advice of modern cross-sectional asset pricing models?\nThirteen models from the literature exhibit strikingly inelastic demand, in\ncontrast to classical models that rely on statistical arbitrageurs to create\nelastic market demand for assets. This inelasticity arises from the difficulty\nof trading against price changes. A quantitative equilibrium model shows that\naggregate demand remains inelastic even with these statistical arbitrageurs in\nthe market.\n"
    },
    {
        "paper_id": 2303.14732,
        "authors": "Minsu Park, Suman Kalyan Maity, Stefan Wuchty, Dashun Wang",
        "title": "Interdisciplinary Papers Supported by Disciplinary Grants Garner Deep\n  and Broad Scientific Impact",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  Interdisciplinary research has emerged as a hotbed for innovation and a key\napproach to addressing complex societal challenges. The increasing dominance of\ngrant-supported research in shaping scientific advances, coupled with growing\ninterest in funding interdisciplinary work, raises fundamental questions about\nthe effectiveness of interdisciplinary grants in fostering high-impact\ninterdisciplinary research outcomes. Here, we quantify the interdisciplinarity\nof both research grants and publications, capturing 350,000 grants from 164\nfunding agencies across 26 countries and 1.3 million papers that acknowledged\ntheir support from 1985 to 2009. Our analysis uncovers two seemingly\ncontradictory patterns: Interdisciplinary grants tend to produce\ninterdisciplinary papers, which are generally associated with high impact.\nHowever, compared to disciplinary grants, interdisciplinary grants on average\nyield fewer papers and interdisciplinary papers they support tend to have\nsubstantially reduced impact. We demonstrate that the key to explaining this\nparadox lies in the power of disciplinary grants in propelling high-impact\ninterdisciplinary research. Specifically, our results show that highly\ninterdisciplinary papers supported by deeply disciplinary grants garner\ndisproportionately more citations, both within their core disciplines and from\nbroader fields. Moreover, disciplinary grants, particularly when combined with\nother similar grants, are more effective in producing high-impact\ninterdisciplinary research. Amidst the rapid rise of support for\ninterdisciplinary work across the sciences, these results highlight the\nhitherto unknown role of disciplinary grants in driving crucial\ninterdisciplinary advances, suggesting that interdisciplinary research requires\ndeep disciplinary expertise and investments.\n"
    },
    {
        "paper_id": 2303.14802,
        "authors": "Marlon Azinovic, Jan \\v{Z}emli\\v{c}ka",
        "title": "Economics-Inspired Neural Networks with Stabilizing Homotopies",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Contemporary deep learning based solution methods used to compute approximate\nequilibria of high-dimensional dynamic stochastic economic models are often\nfaced with two pain points. The first problem is that the loss function\ntypically encodes a diverse set of equilibrium conditions, such as market\nclearing and households' or firms' optimality conditions. Hence the training\nalgorithm trades off errors between those -- potentially very different --\nequilibrium conditions. This renders the interpretation of the remaining errors\nchallenging. The second problem is that portfolio choice in models with\nmultiple assets is only pinned down for low errors in the corresponding\nequilibrium conditions. In the beginning of training, this can lead to\nfluctuating policies for different assets, which hampers the training process.\nTo alleviate these issues, we propose two complementary innovations. First, we\nintroduce Market Clearing Layers, a neural network architecture that\nautomatically enforces all the market clearing conditions and borrowing\nconstraints in the economy. Encoding economic constraints into the neural\nnetwork architecture reduces the number of terms in the loss function and\nenhances the interpretability of the remaining equilibrium errors. Furthermore,\nwe present a homotopy algorithm for solving portfolio choice problems with\nmultiple assets, which ameliorates numerical instabilities arising in the\ncontext of deep learning. To illustrate our method we solve an overlapping\ngenerations model with two permanent risk aversion types, three distinct\nassets, and aggregate shocks.\n"
    },
    {
        "paper_id": 2303.14947,
        "authors": "Lukas J\\\"urgensmeier, Bernd Skiera",
        "title": "Measuring Self-Preferencing on Digital Platforms",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Digital platforms use recommendations to facilitate exchanges between\nplatform actors, such as trade between buyers and sellers. Aiming to protect\nconsumers and guarantee fair competition on platforms, legislators increasingly\nrequire that recommendations on market-dominating platforms be free from\nself-preferencing. That is, platforms that also act as sellers (e.g., Amazon)\nor information providers (e.g., Google) must not prefer their own offers over\ncomparable third-party offers. Yet, successful enforcement of self-preferencing\nbans -- to the potential benefit of consumers and third-party actors --\nrequires defining and measuring self-preferencing across a platform. In the\ncontext of recommendations through search results, this research contributes by\ni) conceptualizing a \"recommendation\" as an offer's level of search engine\nvisibility across an entire platform (instead of its position in specific\nsearch queries, as in previous research); ii) discussing two tests for\nself-preferencing, and iii) implementing them in two empirical studies across\nthree international Amazon marketplaces. Contrary to consumer expectations and\nemerging literature, our analysis finds almost no evidence for\nself-preferencing. A survey reveals that even if Amazon were proven to engage\nin self-preferencing, most consumers would not change their shopping behavior\non the platform -- highlighting Amazon's significant market power and\nsuggesting the need for robust protections for sellers and consumers.\n"
    },
    {
        "paper_id": 2303.15162,
        "authors": "Kaihua Qin, Jens Ernstberger, Liyi Zhou, Philipp Jovanovic, Arthur\n  Gervais",
        "title": "Mitigating Decentralized Finance Liquidations with Reversible Call\n  Options",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Liquidations in Decentralized Finance (DeFi) are both a blessing and a curse\n-- whereas liquidations prevent lenders from capital loss, they simultaneously\nlead to liquidation spirals and system-wide failures. Since most lending and\nborrowing protocols assume liquidations are indispensable, there is an\nincreased interest in alternative constructions that prevent immediate\nsystemic-failure under uncertain circumstances.\n  In this work, we introduce reversible call options, a novel financial\nprimitive that enables the seller of a call option to terminate it before\nmaturity. We apply reversible call options to lending in DeFi and devise\nMiqado, a protocol for lending platforms to replace the liquidation mechanisms.\nTo the best of our knowledge, Miqado is the first protocol that actively\nmitigates liquidations to reduce the risk of liquidation spirals. Instead of\nselling collateral, Miqado incentivizes external entities, so-called\nsupporters, to top-up a borrowing position and grant the borrower additional\ntime to rescue the debt. Our simulation shows that Miqado reduces the amount of\nliquidated collateral by 89.82% in a worst-case scenario.\n"
    },
    {
        "paper_id": 2303.15163,
        "authors": "Victor P. Seidel, Christoph Riedl",
        "title": "How creative versus technical constraints affect individual learning in\n  an online innovation community",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Online innovation communities allow for a search for novel solutions within a\ndesign space bounded by constraints. Past research has focused on the effect of\ncreative constraints on individual projects, but less is known about how\nconstraints affect learning from repeated design submissions and the effect of\nthe technical constraints that are integral to online platforms. How do\ncreative versus technical constraints affect individual learning in exploring a\ndesign space in online communities? We analyzed ten years of data from an\nonline innovation community that crowdsourced 136,989 design submissions from\n33,813 individuals. We leveraged data from two types of design\ncontests-creatively constrained and unconstrained-running in parallel on the\nplatform, and we evaluated a natural experiment where a platform change reduced\ntechnical constraints. We find that creative constraints lead to high rates of\nlearning only if technical constraints are sufficiently relaxed. Our findings\nhave implications for the management of creative design work and the downstream\neffects of the technical constraints of the information systems that support\nonline innovation communities.\n"
    },
    {
        "paper_id": 2303.15164,
        "authors": "Christoph J. B\\\"orner, Ingo Hoffmann, John H. Stiebel",
        "title": "On the Connection between Temperature and Volatility in Ideal Agent\n  Systems",
        "comments": "Theoretical Contribution",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Models for spin systems known from statistical physics are applied by analogy\nin econometrics in the form of agent-based models. Researchers suggest that the\nstate variable temperature $T$ corresponds to volatility $\\sigma$ in capital\nmarket theory problems. To the best of our knowledge, this has not yet been\ntheoretically derived, for example, for an ideal agent system. In the present\npaper, we derive the exact algebraic relation between $T$ and $\\sigma$ for an\nideal agent system and discuss implications and limitations.\n"
    },
    {
        "paper_id": 2303.15216,
        "authors": "David Wu, Sebastian Jaimungal",
        "title": "Robust Risk-Aware Option Hedging",
        "comments": "18 pages, 14 figures, 1 table",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-sa/4.0/",
        "abstract": "  The objectives of option hedging/trading extend beyond mere protection\nagainst downside risks, with a desire to seek gains also driving agent's\nstrategies. In this study, we showcase the potential of robust risk-aware\nreinforcement learning (RL) in mitigating the risks associated with\npath-dependent financial derivatives. We accomplish this by leveraging a policy\ngradient approach that optimises robust risk-aware performance criteria. We\nspecifically apply this methodology to the hedging of barrier options, and\nhighlight how the optimal hedging strategy undergoes distortions as the agent\nmoves from being risk-averse to risk-seeking. As well as how the agent\nrobustifies their strategy. We further investigate the performance of the hedge\nwhen the data generating process (DGP) varies from the training DGP, and\ndemonstrate that the robust strategies outperform the non-robust ones.\n"
    },
    {
        "paper_id": 2303.1583,
        "authors": "Weiping Wu and Yu Lin and Jianjun Gao and Ke Zhou",
        "title": "Mean-variance hybrid portfolio optimization with quantile-based risk\n  measure",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper addresses the importance of incorporating various risk measures in\nportfolio management and proposes a dynamic hybrid portfolio optimization model\nthat combines the spectral risk measure and the Value-at-Risk in the\nmean-variance formulation. By utilizing the quantile optimization technique and\nmartingale representation, we offer a solution framework for these issues and\nalso develop a closed-form portfolio policy when all market parameters are\ndeterministic. Our hybrid model outperforms the classical continuous-time\nmean-variance portfolio policy by allocating a higher position of the risky\nasset in favorable market states and a less risky asset in unfavorable market\nstates. This desirable property leads to promising numerical experiment\nresults, including improved Sortino ratio and reduced downside risk compared to\nthe benchmark models.\n"
    },
    {
        "paper_id": 2303.16012,
        "authors": "Gero Junike",
        "title": "On the number of terms in the COS method for European option pricing",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  The Fourier-cosine expansion (COS) method is used to price European options\nnumerically in a very efficient way. To apply the COS method, one has to\nspecify two parameters: a truncation range for the density of the log-returns\nand a number of terms N to approximate the truncated density by a cosine\nseries. How to choose the truncation range is already known. Here, we are able\nto find an explicit and useful bound for N as well for pricing and for the\nsensitivities, i.e., the Greeks Delta and Gamma, provided the density of the\nlog-returns is smooth. We further show that the COS method has an exponential\norder of convergence when the density is smooth and decays exponentially.\nHowever, when the density is smooth and has heavy tails, as in the Finite\nMoment Log Stable model, the COS method does not have exponential order of\nconvergence. Numerical experiments confirm the theoretical results.\n"
    },
    {
        "paper_id": 2303.16117,
        "authors": "Thomas Wong, Mauricio Barahona",
        "title": "Feature Engineering Methods on Multivariate Time-Series Data for\n  Financial Data Science Competitions",
        "comments": "arXiv admin note: substantial text overlap with arXiv:2303.07925",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  This paper is a work in progress. We are looking for collaborators to provide\nus financial datasets in Equity/Futures market to conduct more bench-marking\nstudies. The authors have papers employing similar methods applied on the\nNumerai dataset, which is freely available but obfuscated.\n  We apply different feature engineering methods for time-series to US market\nprice data. The predictive power of models are tested against Numerai-Signals\ntargets.\n"
    },
    {
        "paper_id": 2303.16148,
        "authors": "Rasoul Amirzadeh, Asef Nazari, Dhananjay Thiruvady and Mong Shan Ee",
        "title": "Modelling Determinants of Cryptocurrency Prices: A Bayesian Network\n  Approach",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  The growth of market capitalisation and the number of altcoins\n(cryptocurrencies other than Bitcoin) provide investment opportunities and\ncomplicate the prediction of their price movements. A significant challenge in\nthis volatile and relatively immature market is the problem of predicting\ncryptocurrency prices which needs to identify the factors influencing these\nprices. The focus of this study is to investigate the factors influencing\naltcoin prices, and these factors have been investigated from a causal analysis\nperspective using Bayesian networks. In particular, studying the nature of\ninteractions between five leading altcoins, traditional financial assets\nincluding gold, oil, and S\\&P 500, and social media is the research question.\nTo provide an answer to the question, we create causal networks which are built\nfrom the historic price data of five traditional financial assets, social media\ndata, and price data of altcoins. The ensuing networks are used for causal\nreasoning and diagnosis, and the results indicate that social media (in\nparticular Twitter data in this study) is the most significant influencing\nfactor of the prices of altcoins. Furthermore, it is not possible to generalise\nthe coins' reactions against the changes in the factors. Consequently, the\ncoins need to be studied separately for a particular price movement\ninvestigation.\n"
    },
    {
        "paper_id": 2303.16149,
        "authors": "Davood Pirayesh Neghab, Mucahit Cevik, M. I. M. Wahab",
        "title": "Explaining Exchange Rate Forecasts with Macroeconomic Fundamentals Using\n  Interpretive Machine Learning",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The complexity and ambiguity of financial and economic systems, along with\nfrequent changes in the economic environment, have made it difficult to make\nprecise predictions that are supported by theory-consistent explanations.\nInterpreting the prediction models used for forecasting important macroeconomic\nindicators is highly valuable for understanding relations among different\nfactors, increasing trust towards the prediction models, and making predictions\nmore actionable. In this study, we develop a fundamental-based model for the\nCanadian-U.S. dollar exchange rate within an interpretative framework. We\npropose a comprehensive approach using machine learning to predict the exchange\nrate and employ interpretability methods to accurately analyze the\nrelationships among macroeconomic variables. Moreover, we implement an ablation\nstudy based on the output of the interpretations to improve the predictive\naccuracy of the models. Our empirical results show that crude oil, as Canada's\nmain commodity export, is the leading factor that determines the exchange rate\ndynamics with time-varying effects. The changes in the sign and magnitude of\nthe contributions of crude oil to the exchange rate are consistent with\nsignificant events in the commodity and energy markets and the evolution of the\ncrude oil trend in Canada. Gold and the TSX stock index are found to be the\nsecond and third most important variables that influence the exchange rate.\nAccordingly, this analysis provides trustworthy and practical insights for\npolicymakers and economists and accurate knowledge about the predictive model's\ndecisions, which are supported by theoretical considerations.\n"
    },
    {
        "paper_id": 2303.16151,
        "authors": "Rafael Alves, Diego S. de Brito, Marcelo C. Medeiros, Ruy M. Ribeiro",
        "title": "Forecasting Large Realized Covariance Matrices: The Benefits of Factor\n  Models and Shrinkage",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  We propose a model to forecast large realized covariance matrices of returns,\napplying it to the constituents of the S\\&P 500 daily. To address the curse of\ndimensionality, we decompose the return covariance matrix using standard\nfirm-level factors (e.g., size, value, and profitability) and use sectoral\nrestrictions in the residual covariance matrix. This restricted model is then\nestimated using vector heterogeneous autoregressive (VHAR) models with the\nleast absolute shrinkage and selection operator (LASSO). Our methodology\nimproves forecasting precision relative to standard benchmarks and leads to\nbetter estimates of minimum variance portfolios.\n"
    },
    {
        "paper_id": 2303.16153,
        "authors": "William H. Press",
        "title": "Optimal Cross-Correlation Estimates from Asynchronous Tick-by-Tick\n  Trading Data",
        "comments": "21 pages, 6 figures, 3 tables",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
        "abstract": "  Given two time series, A and B, sampled asynchronously at different times\n{t_A_i} and {t_B_j}, termed \"ticks\", how can one best estimate the correlation\ncoefficient \\rho between changes in A and B? We derive a natural,\nminimum-variance estimator that does not use any interpolation or binning, then\nderive from it a fast (linear time) estimator that is demonstrably nearly as\ngood. This \"fast tickwise estimator\" is compared in simulation to the usual\nmethod of interpolating changes to a regular grid. Even when the grid spacing\nis optimized for the particular parameters (not often possible in practice),\nthe fast tickwise estimator has generally smaller estimation errors, often by a\nlarge factor. These results are directly applicable to tick-by-tick price data\nof financial assets.\n"
    },
    {
        "paper_id": 2303.16155,
        "authors": "Ewa A. Drzazga-Szcz\\c{e}\\'sniak, Piotr Szczepanik, Adam Z. Kaczmarek,\n  Dominik Szcz\\c{e}\\'sniak",
        "title": "Entropy of financial time series due to the shock of war",
        "comments": "8 pages, 5 figures",
        "journal-ref": null,
        "doi": "10.3390/e25050823",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The concept of entropy is not uniquely relevant to the statistical mechanics\nbut among others it can play pivotal role in the analysis of a time series,\nparticularly the stock market data. In this area sudden events are especially\ninteresting as they describe abrupt data changes which may have long-lasting\neffects. Here, we investigate the impact of such events on the entropy of\nfinancial time series. As a case study we assume data of polish stock market in\nthe context of its main cumulative index. This index is discussed for the\nfinite time periods before and after outbreak of the 2022 Russian invasion of\nUkraine, acting as the sudden event. The analysis allows us to validate the\nentropy-based methodology in assessing market changes as driven by the extreme\nexternal factors. We show that qualitative features of market changes can be\ncaptured quantitatively in terms of the entropy. In addition to that, the\nmagnitude of the impact is analysed over various time periods in terms of the\nintroduced entropic index. To this end, the present work also attempts to\nanswer whether or not the recent war can be considered as a reason or at least\ncatalyst to the current economic crisis.\n"
    },
    {
        "paper_id": 2303.16158,
        "authors": "Murray Z. Frank, Jing Gao, Keer Yang",
        "title": "Behavioral Machine Learning? Computer Predictions of Corporate Earnings\n  also Overreact",
        "comments": "stock analysts, machine learning, behavioral, overreaction",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  There is considerable evidence that machine learning algorithms have better\npredictive abilities than humans in various financial settings. But, the\nliterature has not tested whether these algorithmic predictions are more\nrational than human predictions. We study the predictions of corporate earnings\nfrom several algorithms, notably linear regressions and a popular algorithm\ncalled Gradient Boosted Regression Trees (GBRT). On average, GBRT outperformed\nboth linear regressions and human stock analysts, but it still overreacted to\nnews and did not satisfy rational expectation as normally defined. By reducing\nthe learning rate, the magnitude of overreaction can be minimized, but it comes\nwith the cost of poorer out-of-sample prediction accuracy. Human stock analysts\nwho have been trained in machine learning methods overreact less than\ntraditionally trained analysts. Additionally, stock analyst predictions reflect\ninformation not otherwise available to machine algorithms.\n"
    },
    {
        "paper_id": 2303.16266,
        "authors": "{\\L}ukasz Lepak and Pawe{\\l} Wawrzy\\'nski",
        "title": "On-line reinforcement learning for optimization of real-life energy\n  trading strategy",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  An increasing share of energy is produced from renewable sources by many\nsmall producers. The efficiency of those sources is volatile and, to some\nextent, random, exacerbating the problem of energy market balancing. In many\ncountries, this balancing is done on the day-ahead (DA) energy markets. This\npaper considers automated trading on the DA energy market by a medium-sized\nprosumer. We model this activity as a Markov Decision Process and formalize a\nframework in which an applicable in real-life strategy can be optimized with\noff-line data. We design a trading strategy that is fed with the available\nenvironmental information that can impact future prices, including weather\nforecasts. We use state-of-the-art reinforcement learning (RL) algorithms to\noptimize this strategy. For comparison, we also synthesize simple parametric\ntrading strategies and optimize them with an evolutionary algorithm. Results\nshow that our RL-based strategy generates the highest market profits.\n"
    },
    {
        "paper_id": 2303.16314,
        "authors": "Axel A. Araneda",
        "title": "A multifractional option pricing formula",
        "comments": "9 Pages",
        "journal-ref": "Fluctuation and Noise Letters, 2024",
        "doi": "10.1142/S0219477524500603",
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Fractional Brownian motion has become a standard tool to address long-range\ndependence in financial time series. However, a constant memory parameter is\ntoo restrictive to address different market conditions. Here we model the price\nfluctuations using a multifractional Brownian motion assuming that the Hurst\nexponent is a time-deterministic function. Through the multifractional Ito\ncalculus, both the related transition density function and the analytical\nEuropean Call option pricing formula are obtained. The empirical performance of\nthe multifractional Black-Scholes model is tested by calibration of option\nmarket quotes for the SPX index and offers best fit than its counterparts based\non standard and fractional Brownian motions.\n"
    },
    {
        "paper_id": 2303.16331,
        "authors": "Zhimeng Yang, Ariah Klages-Mundt, Lewis Gudgeon",
        "title": "Oracle Counterpoint: Relationships between On-chain and Off-chain Market\n  Data",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We investigate the theoretical and empirical relationships between activity\nin on-chain markets and pricing in off-chain cryptocurrency markets (e.g.,\nETH/USD prices). The motivation is to develop methods for proxying off-chain\nmarket data using data and computation that is in principle verifiable on-chain\nand could provide an alternative approach to blockchain price oracles. We\nexplore relationships in PoW mining, PoS validation, block space markets,\nnetwork decentralization, usage and monetary velocity, and on-chain Automated\nMarket Makers (AMMs). We select key features from these markets, which we\nanalyze through graphical models, mutual information, and ensemble machine\nlearning models to explore the degree to which off-chain pricing information\ncan be recovered entirely on-chain. We find that a large amount of pricing\ninformation is contained in on-chain data, but that it is generally hard to\nrecover precise prices except on short time scales of retraining the model. We\ndiscuss how even noisy information recovered from on-chain data could help to\ndetect anomalies in oracle-reported prices on-chain.\n"
    },
    {
        "paper_id": 2303.16371,
        "authors": "Gurdip Bakshi and John Crosby and Xiaohui Gao",
        "title": "Dark Matter in (Volatility and) Equity Option Risk Premiums",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Emphasizing the statistics of jumps crossing the strike and local time, we\ndevelop a decomposition of equity option risk premiums. Operationalizing this\ntheoretical treatment, we equip the pricing kernel process with unspanned\nrisks, embed (unspanned) jump risks, and allow equity return volatility to\ncontain unspanned risks. Unspanned risks are consistent with negative risk\npremiums for jumps crossing the strike and local time and imply negative risk\npremiums for out-of-the-money call options and straddles. The empirical\nevidence from weekly and farther-dated index options is supportive of our\ntheory of economically relevant unspanned risks and reveals ``dark matter\" in\noption risk premiums.\n"
    },
    {
        "paper_id": 2303.16532,
        "authors": "Min Hu, Zhizhong Tan, Bin Liu, Guosheng Yin",
        "title": "Futures Quantitative Investment with Heterogeneous Continual Graph\n  Neural Network",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  This study aims to address the challenges of futures price prediction in\nhigh-frequency trading (HFT) by proposing a continuous learning factor\npredictor based on graph neural networks. The model integrates multi-factor\npricing theories with real-time market dynamics, effectively bypassing the\nlimitations of existing methods that lack financial theory guidance and ignore\nvarious trend signals and their interactions. We propose three heterogeneous\ntasks, including price moving average regression, price gap regression and\nchange-point detection to trace the short-, intermediate-, and long-term trend\nfactors present in the data. In addition, this study also considers the\ncross-sectional correlation characteristics of future contracts, where prices\nof different futures often show strong dynamic correlations. Each variable\n(future contract) depends not only on its historical values (temporal) but also\non the observation of other variables (cross-sectional). To capture these\ndynamic relationships more accurately, we resort to the spatio-temporal graph\nneural network (STGNN) to enhance the predictive power of the model. The model\nemploys a continuous learning strategy to simultaneously consider these tasks\n(factors). Additionally, due to the heterogeneity of the tasks, we propose to\ncalculate parameter importance with mutual information between original\nobservations and the extracted features to mitigate the catastrophic forgetting\n(CF) problem. Empirical tests on 49 commodity futures in China's futures market\ndemonstrate that the proposed model outperforms other state-of-the-art models\nin terms of prediction accuracy. Not only does this research promote the\nintegration of financial theory and deep learning, but it also provides a\nscientific basis for actual trading decisions.\n"
    },
    {
        "paper_id": 2303.16585,
        "authors": "El Amine Cherrat, Snehal Raj, Iordanis Kerenidis, Abhishek Shekhar,\n  Ben Wood, Jon Dee, Shouvanik Chakrabarti, Richard Chen, Dylan Herman, Shaohan\n  Hu, Pierre Minssen, Ruslan Shaydulin, Yue Sun, Romina Yalovetzky, Marco\n  Pistoia",
        "title": "Quantum Deep Hedging",
        "comments": null,
        "journal-ref": "Quantum 7, 1191 (2023)",
        "doi": "10.22331/q-2023-11-29-1191",
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Quantum machine learning has the potential for a transformative impact across\nindustry sectors and in particular in finance. In our work we look at the\nproblem of hedging where deep reinforcement learning offers a powerful\nframework for real markets. We develop quantum reinforcement learning methods\nbased on policy-search and distributional actor-critic algorithms that use\nquantum neural network architectures with orthogonal and compound layers for\nthe policy and value functions. We prove that the quantum neural networks we\nuse are trainable, and we perform extensive simulations that show that quantum\nmodels can reduce the number of trainable parameters while achieving comparable\nperformance and that the distributional approach obtains better performance\nthan other standard approaches, both classical and quantum. We successfully\nimplement the proposed models on a trapped-ion quantum processor, utilizing\ncircuits with up to $16$ qubits, and observe performance that agrees well with\nnoiseless simulation. Our quantum techniques are general and can be applied to\nother reinforcement learning problems beyond hedging.\n"
    },
    {
        "paper_id": 2303.16595,
        "authors": "Rui Yao, Shlomo Bekhor",
        "title": "A general equilibrium model for multi-passenger ridesharing systems with\n  stable matching",
        "comments": null,
        "journal-ref": "Transportation Research Part B: Methodological, 175, 102775 (2023)",
        "doi": "10.1016/j.trb.2023.05.012",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper proposes a general equilibrium model for multi-passenger\nridesharing systems, in which interactions between ridesharing drivers,\npassengers, platforms, and transportation networks are endogenously captured.\nStable matching is modeled as an equilibrium problem in which no ridesharing\ndriver or passenger can reduce ridesharing disutility by unilaterally switching\nto another matching sequence. This paper is one of the first studies that\nexplicitly integrates the ridesharing platform multi-passenger matching problem\ninto the model. By integrating matching sequence with hyper-network,\nridesharing-passenger transfers are avoided in a multi-passenger ridesharing\nsystem. Moreover, the matching stability between the ridesharing drivers and\npassengers is extended to address the multi-OD multi-passenger case in terms of\nmatching sequence. The paper provides a proof for the existence of the proposed\ngeneral equilibrium. A sequence-bush algorithm is developed for solving the\nmulti-passenger ridesharing equilibrium problem. This algorithm is capable to\nhandle complex ridesharing constraints implicitly. Results illustrate that the\nproposed sequence-bush algorithm outperforms general-purpose solver, and\nprovides insights into the equilibrium of the joint stable matching and route\nchoice problem. Numerical experiments indicate that ridesharing trips are\ntypically longer than average trip lengths. Sensitivity analysis suggests that\na properly designed ridesharing unit price is necessary to achieve network\nbenefits, and travelers with relatively lower values of time are more likely to\nparticipate in ridesharing.\n"
    },
    {
        "paper_id": 2303.16629,
        "authors": "Carlos Gaete-Morales, Julius J\\\"ohrens, Florian Heining, Wolf-Peter\n  Schill",
        "title": "Power sector effects of alternative options for de-fossilizing\n  heavy-duty vehicles -- go electric, and charge smartly",
        "comments": null,
        "journal-ref": null,
        "doi": "10.1016/j.crsus.2024.100123",
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Various options are discussed to de-fossilize heavy-duty vehicles (HDV),\nincluding battery-electric vehicles (BEV), electric road systems (ERS), and\nindirect electrification via hydrogen fuel cells or e-fuels. We investigate\ntheir power sector implications in future scenarios of Germany with high\nrenewable energy shares, using an open-source capacity expansion model and\nroute-based truck traffic data. Power sector costs are lowest for flexibly\ncharged BEV that also carry out vehicle-to-grid operations, and highest for\ne-fuels. If BEV and ERS-BEV are not optimally charged, power sector costs\nincrease, but are still substantially lower than in scenarios with hydrogen or\ne-fuels. This is because indirect electrification is less energy efficient,\nwhich outweighs potential flexibility benefits. BEV and ERS-BEV favor solar\nphotovoltaic energy, while hydrogen and e-fuels favor wind power and increase\nfossil electricity generation. Results remain qualitatively robust in\nsensitivity analyses.\n"
    },
    {
        "paper_id": 2303.16773,
        "authors": "Nizar Riane",
        "title": "The inverse Black-Scholes problem in Radon measures space revisited:\n  towards a new measure of market uncertainty",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  In this paper, we revisit the inverse Black-Scholes model, the existence of\nthe solution is proved in more rigorous way, and the empirical study is done\nusing different approach based on finite element method. The article leads to a\nmeasure of incertitude in the option market.\n"
    },
    {
        "paper_id": 2303.16855,
        "authors": "Alexander Ugarov",
        "title": "Peer Prediction for Peer Review: Designing a Marketplace for Ideas",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
        "abstract": "  The paper describes a potential platform to facilitate academic peer review\nwith emphasis on early-stage research. This platform aims to make peer review\nmore accurate and timely by rewarding reviewers on the basis of peer prediction\nalgorithms. The algorithm uses a variation of Peer Truth Serum for\nCrowdsourcing (Radanovic et al., 2016) with human raters competing against a\nmachine learning benchmark. We explain how our approach addresses two large\nproductive inefficiencies in science: mismatch between research questions and\npublication bias. Better peer review for early research creates additional\nincentives for sharing it, which simplifies matching ideas to teams and makes\nnegative results and p-hacking more visible.\n"
    },
    {
        "paper_id": 2303.17014,
        "authors": "Yuan Hu, W. Brent Lindquist, Svetlozar T. Rachev, Frank J. Fabozzi",
        "title": "Option pricing using a skew random walk pricing tree",
        "comments": "49 pages, 7 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Motivated by the Corns-Satchell, continuous time, option pricing model, we\ndevelop a binary tree pricing model with underlying asset price dynamics\nfollowing It\\^o-Mckean skew Brownian motion. While the Corns-Satchell market\nmodel is incomplete, our discrete time market model is defined in the natural\nworld; extended to the risk neutral world under the no-arbitrage condition\nwhere derivatives are priced under uniquely determined risk-neutral\nprobabilities; and is complete. The skewness introduced in the natural world is\npreserved in the risk neutral world. Furthermore, we show that the model\npreserves skewness under the continuous-time limit. We provide numerical\napplications of our model to the valuation of European put and call options on\nexchange-traded funds tracking the S&P Global 1200 index.\n"
    },
    {
        "paper_id": 2303.17029,
        "authors": "Hsuan-Hua Huang, Hsing-Wen Han, Kuang-Ta Lo, Tzu-Ting Yang",
        "title": "Liquidity Constraints, Cash Windfalls, and Entrepreneurship: Evidence\n  from Administrative Data on Lottery Winners",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Using administrative data on Taiwanese lottery winners, this paper examines\nthe effects of cash windfalls on entrepreneurship. We compare the start-up\ndecisions of households winning more than 1.5 million NTD (50,000 USD) in the\nlottery in a particular year with those of households winning less than 15,000\nNTD (500 USD). Our results suggest that a substantial windfall increases the\nlikelihood of starting a business by 1.5 percentage points (125% from the\nbaseline mean). Startup wealth elasticity is 0.25 to 0.36. Moreover, households\nwho tend to be liquidity-constrained drive the windfall-induced entrepreneurial\nresponse. Finally, we examine how households with a business react to a cash\nwindfall and find that serial entrepreneurs are more likely to start a new\nbusiness but do not change their decision to continue the current business.\n"
    },
    {
        "paper_id": 2303.1713,
        "authors": "Xavier Lawrence D. Mendoza",
        "title": "Entrepreneurial Capability And Engagement Of Persons With Disabilities\n  Toward A Framework For Inclusive Entrepreneurship",
        "comments": null,
        "journal-ref": "Asian Intellect Research and Education Journal, 20(1), 2021,\n  179-186",
        "doi": "10.5281/zenodo.7782073",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The study was designed to determine the entrepreneurial capability and\nengagement of persons with disabilities toward a framework for inclusive\nentrepreneurship. The researcher used descriptive and correlational approaches\nthrough purposive random sampling. The sample came from the City of General\nTrias and the Municipality of Rosario, registered under their respective\nPersons with Disabilities Affairs Offices (PDAO). The findings indicated that\nthe respondents are from the working class, are primarily female, are mostly\nsingle, have college degrees, live in a medium-sized home, and earn the bare\nminimum. Furthermore, PWDs' perceived capability level in entrepreneurship was\nsomehow capable, and the majority of engagement level responses were somehow\nengaged. Considerably, age and civil status have significant relationships with\nmost of the variables under study. Finally, the perceived challenges of PWDs'\nrespondents noted the following: lack of financial capacity, access to credit\nand other financial institutions, absence of business information, absence of\naccess to data, lack of competent business skills, lack of family support, and\nlack of personal motivation. As a result, the author proposed a framework that\nemphasizes interaction and cooperation between national and local government\nunits in the formulation of policies promoting inclusive entrepreneurship for\npeople with disabilities.\n"
    },
    {
        "paper_id": 2303.1718,
        "authors": "Kanis Saengchote, Voraprapa Nakavachara, Yishuang Xu",
        "title": "Capitalising the Network Externalities of New Land Supply in the\n  Metaverse",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  When land becomes more connected, its value can change because of network\nexternalities. This idea is intuitive and appealing to developers and\npolicymakers, but documenting their importance is empirically challenging\nbecause it is difficult to isolate the determinants of land value in practice.\nWe address this challenge with real estate in The Sandbox, a virtual economy\nbuilt on blockchain, which provides a series of natural experiments that can be\nused to estimate the causal impact of land-based of network externalities. Our\nresults show that when new land becomes available, the network value of\nexisting land increases, but there is a trade-off as new land also competes\nwith existing supply. Our work illustrates the benefits of using virtual worlds\nto conduct policy experiments.\n"
    },
    {
        "paper_id": 2303.17266,
        "authors": "Carole Bernard, Jinghui Chen, Ludger Ruschendorf, Steven Vanduffel",
        "title": "Coskewness under dependence uncertainty",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  We study the impact of dependence uncertainty on the expectation of the\nproduct of $d$ random variables, $\\mathbb{E}(X_1X_2\\cdots X_d)$ when $X_i \\sim\nF_i$ for all~$i$. Under some conditions on the $F_i$, explicit sharp bounds are\nobtained and a numerical method is provided to approximate them for arbitrary\nchoices of the $F_i$. The results are applied to assess the impact of\ndependence uncertainty on coskewness. In this regard, we introduce a novel\nnotion of \"standardized rank coskewness,\" which is invariant under strictly\nincreasing transformations and takes values in $[-1,\\ 1]$.\n"
    },
    {
        "paper_id": 2303.17564,
        "authors": "Shijie Wu, Ozan Irsoy, Steven Lu, Vadim Dabravolski, Mark Dredze,\n  Sebastian Gehrmann, Prabhanjan Kambadur, David Rosenberg, Gideon Mann",
        "title": "BloombergGPT: A Large Language Model for Finance",
        "comments": "Updated to include Training Chronicles (Appendix C)",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The use of NLP in the realm of financial technology is broad and complex,\nwith applications ranging from sentiment analysis and named entity recognition\nto question answering. Large Language Models (LLMs) have been shown to be\neffective on a variety of tasks; however, no LLM specialized for the financial\ndomain has been reported in literature. In this work, we present BloombergGPT,\na 50 billion parameter language model that is trained on a wide range of\nfinancial data. We construct a 363 billion token dataset based on Bloomberg's\nextensive data sources, perhaps the largest domain-specific dataset yet,\naugmented with 345 billion tokens from general purpose datasets. We validate\nBloombergGPT on standard LLM benchmarks, open financial benchmarks, and a suite\nof internal benchmarks that most accurately reflect our intended usage. Our\nmixed dataset training leads to a model that outperforms existing models on\nfinancial tasks by significant margins without sacrificing performance on\ngeneral LLM benchmarks. Additionally, we explain our modeling choices, training\nprocess, and evaluation methodology. We release Training Chronicles (Appendix\nC) detailing our experience in training BloombergGPT.\n"
    },
    {
        "paper_id": 2303.17667,
        "authors": "Nicholas Milikich and Joshua Johnson",
        "title": "Taureau: A Stock Market Movement Inference Framework Based on Twitter\n  Sentiment Analysis",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  With the advent of fast-paced information dissemination and retrieval, it has\nbecome inherently important to resort to automated means of predicting stock\nmarket prices. In this paper, we propose Taureau, a framework that leverages\nTwitter sentiment analysis for predicting stock market movement. The aim of our\nresearch is to determine whether Twitter, which is assumed to be representative\nof the general public, can give insight into the public perception of a\nparticular company and has any correlation to that company's stock price\nmovement. We intend to utilize this correlation to predict stock price\nmovement. We first utilize Tweepy and getOldTweets to obtain historical tweets\nindicating public opinions for a set of top companies during periods of major\nevents. We filter and label the tweets using standard programming libraries. We\nthen vectorize and generate word embedding from the obtained tweets. Afterward,\nwe leverage TextBlob, a state-of-the-art sentiment analytics engine, to assess\nand quantify the users' moods based on the tweets. Next, we correlate the\ntemporal dimensions of the obtained sentiment scores with monthly stock price\nmovement data. Finally, we design and evaluate a predictive model to forecast\nstock price movement from lagged sentiment scores. We evaluate our framework\nusing actual stock price movement data to assess its ability to predict\nmovement direction.\n"
    },
    {
        "paper_id": 2303.18161,
        "authors": "Nicole B\\\"auerle, Tamara G\\\"oll",
        "title": "Nash equilibria for relative investors with (non)linear price impact",
        "comments": "20 pages, 3 figures",
        "journal-ref": "B\\\"auerle, N., G\\\"oll, T. Nash equilibria for relative investors\n  with (non)linear price impact. Math Finan Econ (2024)",
        "doi": "10.1007/s11579-024-00356-0",
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  We consider the strategic interaction of $n$ investors who are able to\ninfluence a stock price process and at the same time measure their utilities\nrelative to the other investors. Our main aim is to find Nash equilibrium\ninvestment strategies in this setting in a financial market driven by a\nBrownian motion and investigate the influence the price impact has on the\nequilibrium. We consider both CRRA and CARA utility functions. Our findings\nshow that the problem is well-posed as long as the price impact is at most\nlinear. Moreover, numerical results reveal that the investors behave very\naggressively when the price impact is beyond a critical parameter.\n"
    },
    {
        "paper_id": 2304.00081,
        "authors": "Andrea Bacilieri, Pablo Austudillo-Estevez",
        "title": "Reconstructing firm-level input-output networks from partial information",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  There is a large consensus on the fundamental role of firm-level supply chain\nnetworks in macroeconomics. However, data on supply chains at the fine-grained,\nfirm level are scarce and frequently incomplete. For listed firms, some\ncommercial datasets exist but only contain information about the existence of a\ntrade relationship between two companies, not the value of the monetary\ntransaction. We use a recently developed maximum entropy method to reconstruct\nthe values of the transactions based on information about their existence and\naggregate information disclosed by firms in financial statements. We test the\nmethod on the administrative dataset of Ecuador and reconstruct a commercial\ndataset (FactSet). We test the method's performance on the weights, the\ntechnical and allocation coefficients (microscale quantities), two measures of\nfirms' systemic importance and GDP volatility. The method reconstructs the\ndistribution of microscale quantities reasonably well but shows diverging\nresults for the measures of firms' systemic importance. Due to the network\nstructure of supply chains and the sampling process of firms and links,\nquantities relying on the number of customers firms have (out-degrees) are\nharder to reconstruct. We also reconstruct the input-output table of globally\nlisted firms and merge it with a global input-output table at the sector level\n(the WIOD). Differences in accounting standards between national accounts and\nfirms' financial statements significantly reduce the quality of the\nreconstruction.\n"
    },
    {
        "paper_id": 2304.00086,
        "authors": "Ajit Desai",
        "title": "Machine Learning for Economics Research: When What and How?",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  This article provides a curated review of selected papers published in\nprominent economics journals that use machine learning (ML) tools for research\nand policy analysis. The review focuses on three key questions: (1) when ML is\nused in economics, (2) what ML models are commonly preferred, and (3) how they\nare used for economic applications. The review highlights that ML is\nparticularly used to process nontraditional and unstructured data, capture\nstrong nonlinearity, and improve prediction accuracy. Deep learning models are\nsuitable for nontraditional data, whereas ensemble learning models are\npreferred for traditional datasets. While traditional econometric models may\nsuffice for analyzing low-complexity data, the increasing complexity of\neconomic data due to rapid digitalization and the growing literature suggests\nthat ML is becoming an essential addition to the econometrician's toolbox.\n"
    },
    {
        "paper_id": 2304.00323,
        "authors": "Yanci Zhang, Yutong Lu, Haitao Mao, Jiawei Huang, Cien Zhang, Xinyi\n  Li, Rui Dai",
        "title": "Company Competition Graph",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Financial market participants frequently rely on numerous business\nrelationships to make investment decisions. Investors can learn about potential\nrisks and opportunities associated with other connected entities through these\ncorporate connections. Nonetheless, human annotation of a large corpus to\nextract such relationships is highly time-consuming, not to mention that it\nrequires a considerable amount of industry expertise and professional training.\nMeanwhile, we have yet to observe means to generate reliable knowledge graphs\nof corporate relationships due to the lack of impartial and granular data\nsources. This study proposes a system to process financial reports and\nconstruct the public competitor graph to fill the void. Our method can retrieve\nmore than 83\\% competition relationship of the S\\&P 500 index companies. Based\non the output from our system, we construct a knowledge graph with more than\n700 nodes and 1200 edges. A demo interactive graph interface is available.\n"
    },
    {
        "paper_id": 2304.00364,
        "authors": "Weiguang Han, Jimin Huang, Qianqian Xie, Boyi Zhang, Yanzhao Lai, Min\n  Peng",
        "title": "Mastering Pair Trading with Risk-Aware Recurrent Reinforcement Learning",
        "comments": "8 pages, 5 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Although pair trading is the simplest hedging strategy for an investor to\neliminate market risk, it is still a great challenge for reinforcement learning\n(RL) methods to perform pair trading as human expertise. It requires RL methods\nto make thousands of correct actions that nevertheless have no obvious\nrelations to the overall trading profit, and to reason over infinite states of\nthe time-varying market most of which have never appeared in history. However,\nexisting RL methods ignore the temporal connections between asset price\nmovements and the risk of the performed trading. These lead to frequent\ntradings with high transaction costs and potential losses, which barely reach\nthe human expertise level of trading. Therefore, we introduce CREDIT, a\nrisk-aware agent capable of learning to exploit long-term trading opportunities\nin pair trading similar to a human expert. CREDIT is the first to apply\nbidirectional GRU along with the temporal attention mechanism to fully consider\nthe temporal correlations embedded in the states, which allows CREDIT to\ncapture long-term patterns of the price movements of two assets to earn higher\nprofit. We also design the risk-aware reward inspired by the economic theory,\nthat models both the profit and risk of the tradings during the trading period.\nIt helps our agent to master pair trading with a robust trading preference that\navoids risky trading with possible high returns and losses. Experiments show\nthat it outperforms existing reinforcement learning methods in pair trading and\nachieves a significant profit over five years of U.S. stock data.\n"
    },
    {
        "paper_id": 2304.00456,
        "authors": "Haonan Zhang",
        "title": "Life cycle costing analysis of deep energy retrofits of a mid-rise\n  building to understand the impact of energy conservation measures",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Building energy retrofits have been identified as key to realizing climate\nmitigation goals in Canada. This study aims to provide a roadmap for existing\nmid-rise building retrofits in order to understand the required capital\ninvestment, energy savings, energy cost savings, and carbon footprint for\nmid-rise residential buildings in Canada. This study employed EnergyPlus to\nexamine the energy performance of 11 energy retrofit measures for a typical\nmulti-unit residential building (MURB) in Metro Vancouver, British Columbia,\nCanada. The author employed the energy simulation software (EnergyPlus) to\nevaluate the pre-and post-retrofit operational energy performance of the\nselected MURB. Two base building models powered by natural gas (NG-building)\nand electricity (E-building) were created by SketchUP. The energy simulation\nresults were combined with cost and emission impact data to evaluate the\neconomic and environmental performance of the selected energy retrofit\nmeasures. The results indicated that the NG-building can produce significant\nGHG emission reductions (from 27.64 tCO2e to 3.77 tCO2e) by implementing these\nenergy retrofit measures. In terms of energy savings, solar PV, ASHP, water\nheater HP, and HRV enhancement have great energy saving potential compared to\nother energy retrofit measures. In addition, temperature setback, lighting, and\nairtightness enhancement present the best economic performance from a life\ncycle perspective. However, windows, ASHP, and solar PV, are not economical\nchoices because of higher life cycle costs. While ASHP can increase life cycle\ncosts for the NG-building, with the financial incentives provided by the\ngovernments, ASHP could be the best choice to reduce GHG emissions when\nstakeholders make decisions on implementing energy retrofits.\n"
    },
    {
        "paper_id": 2304.00482,
        "authors": "Zuleika Ferre, Patricia Triunfo and Jos\\'e-Ignacio Ant\\'on",
        "title": "Immigrant assimilation in health care utilisation in Spain",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Abundant evidence has tracked the labour market and health assimilation of\nimmigrants, including static analyses of differences in how foreign-born and\nnative-born residents consume health care services. However, we know much less\nabout how migrants' patterns of health care usage evolve with time of\nresidence, especially in countries providing universal or quasi-universal\ncoverage. We investigate this process in Spain by combining all the available\nwaves of the local health survey, which allows us to separately identify\nperiod, cohort, and assimilation effects. We find that the evidence of health\nassimilation is limited and solely applies to migrant females' visits to\ngeneral practitioners. Nevertheless, the differential effects of ageing on\nhealth care use between foreign-born and native-born populations contributes to\nthe convergence of utilisation patterns in most health services after 20 years\nin Spain. Substantial heterogeneity over time and by region of origin both\nsuggest that studies modelling future welfare state finances would benefit from\na more thorough assessment of migration.\n"
    },
    {
        "paper_id": 2304.00489,
        "authors": "Samidh Pal",
        "title": "Reduction of Excess Capacity with Response of Capital Intensity",
        "comments": "1 table, 2 figure, 1 map",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Purpose: The objective of this research was to show the response of the\npotential reduction of excess capacity in terms of capital intensity to the\ngrowth rate of labor productivity in the manufacturing industrial sector.\nDesign/Methodology/Approach: The research was carried out in 2019 in 55 groups\nof Indian manufacturing industry within six major Indian industrial states.\nMainly, the research used the modified VES (Variable Elasticity Substitution)\nestimation model. The research focused on the value of the additional\nsubstitution parameter of capital intensity (mu > 0). Findings: Almost all\nselected industry groups with in six states need capital-intensive production.\nThe results found additional parameter of capital intensity (mu) is greater\nthan zero for all industry groups. It means that a higher product per man can\nbe obtained by increasing the capital per worker. Practical Implications:\nResearch shows that an increasingly need for capital investment in need for\nhigher labor productivity is likely to induce the manufacturing unit to use\nmore capacity in existence. It reveals that investors in these selected six\nstates can increase their capital investment. Originality/Value: The analysis\nof the result allowed to determine the fact that capital intensity is an\nessential variable for reduction of excess capacity which cannot be ignored in\nexplaining productivity.\n"
    },
    {
        "paper_id": 2304.0051,
        "authors": "Monika Baloda",
        "title": "The Tech Decoupling",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Financial market volatility is a crucial factor for investment planning,\noption pricing, and financial market regulation, and technology is widely\nrecognized as a key driver of economic growth. In this project, we investigate\nthe co-movement of technology and non-technology sectors over the last two\ndecades. We identify a decoupling phenomenon in the levels and volatility of\nthe two sectors after 2015 and argue that this cannot be attributed to the\nCOVID-19 shock. Furthermore, we demonstrate that the technology sector serves\nas a leading indicator of growth for the rest of the economy. Using ARIMA\nmodeling and stationarity tests, we process time series data to test our\nhypotheses, finding that the technology sector follows an ARIMA(3,1,3) model,\nwhile the non-technology sector follows an ARIMA(2,1,4) model. Our analysis\nencompasses data wrangling, pre-processing, missing value treatment, and\nexploratory data analysis, and we discuss the merits and shortcomings of our\nwork to aid in the interpretation of our results. Overall, our findings shed\nnew light on the relationship between technology and economic growth. Our\nresults can be used for understanding labor market fluctuations in the\ntechnology sector and investment planning assessments.\n"
    },
    {
        "paper_id": 2304.00539,
        "authors": "Zuleika Ferre, Patricia Triunfo and Jos\\'e-Ignacio Ant\\'on",
        "title": "The short- and long-term determinants of fertility in Uruguay",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  This paper examines the determinants of fertility among women at different\nstages of their reproductive lives in Uruguay. To this end, we employ time\nseries analysis methods based on data from 1968 to 2021 and panel data\ntechniques based on department-level statistical information from 1984 to 2019.\nThe results of our first econometric exercise indicate a cointegration\nrelationship between fertility and economic performance, education and infant\nmortality, with differences observed by reproductive stage. We find a negative\nrelationship between income and fertility for women aged 20-29 that persists\nfor women aged 30 and over. This result suggests that having children is\nperceived as an opportunity cost for women in this age group. We also observe a\nnegative relationship between education and adolescent fertility, which has\nimplications for the design of public policies. A panel data analysis with\neconometric techniques allowing us to control for unobserved heterogeneity\nconfirms that income is a relevant factor for all groups of women and\nreinforces the crucial role of education in reducing teenage fertility. We also\nidentify a negative correlation between fertility and employment rates for\nwomen aged 30 and above. We outline some possible explanations for these\nfindings in the context of work-life balance issues and argue for the\nimportance of implementing social policies to address them.\n"
    },
    {
        "paper_id": 2304.00544,
        "authors": "Carlos Carrillo-Tudela and Ludo Visschers",
        "title": "Unemployment and Endogenous Reallocation over the Business Cycle",
        "comments": "This working paper combines main text and online appendix (page 46)\n  of \"Unemployment and Endogenous Reallocation over the Business Cycle\"\n  (accepted in Econometrica in Feb. 2023) with the supplementary appendices\n  that provide further background and investigation. Supplementary appendix A\n  (code error correction) starts on p.73, Supp. appendix B (data) on p.117,\n  Supp. appendix C (theory) on p.218",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  This paper studies the extent to which the cyclicality of occupational\nmobility shapes that of aggregate unemployment and its duration distribution.\nWe document the relation between workers' occupational mobility and\nunemployment duration over the long run and business cycle. To interpret this\nevidence, we develop a multi-sector business cycle model with heterogenous\nagents. The model is quantitatively consistent with several important features\nof the US labor market: procyclical gross and countercyclical net occupational\nmobility, the large volatility of unemployment and the cyclical properties of\nthe unemployment duration distribution, among many others. Our analysis shows\nthat occupational mobility due to workers; changing career prospects, and not\noccupation-wide differences, interacts with aggregate conditions to drive the\nfluctuations of the unemployment duration distribution and the aggregate\nunemployment rate.\n"
    },
    {
        "paper_id": 2304.00566,
        "authors": "Agata Angelika Rzoska and Aleksandra Drozd-Rzoska",
        "title": "The Story about One Island and Four Cities. The Socio-Economic Soft\n  Matter Model - Based Report",
        "comments": "17 pages, 9 Figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  The report discusses the emergence of the Socio-Economic Soft Matter (SE-SM)\nas the result of interactions between physics and economy. First, demographic\nchanges since the Industrial Revolution onset are tested using Soft Matter\nscience tools. Notable in the support of innovative derivative-based and\ndistortions-sensitive analytic tools. It revealed the Weibull type powered\nexponential increase, with a notably lesser rising rate since the crossover\ndetected near the year 1970. Subsequently, demographic (SE-SM) patterns are\ntested for Rapa Nui (Easter) Island model case and for four large 'hallmark\ncities' where the rise and decay phases have occurred. They are Detroit and\nCleveland in the USA and Lodz (former textile industry center) and Bytom\n(former coal mining center) in Poland. The analysis explicitly revealed scaling\npatterns for demographic changes, influenced by the historical and\nsocio-economic backgrounds and the long-lasting determinism in population\nchanges. Universalistic features of demographic changes are discussed within\nthe Socio-Economic Soft Matter concept.\n"
    },
    {
        "paper_id": 2304.00651,
        "authors": "Syngjoo Choi, Kyu Sup Hahn, Byung-Yeon Kim, Eungik Lee, Jungmin Lee,\n  Sokbae Lee",
        "title": "Implicit Bias against a Capitalistic Society Predicts Market Earnings",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper investigates whether ideological indoctrination by living in a\ncommunist regime relates to low economic performance in a market economy. We\nrecruit North Korean refugees and measure their implicit bias against South\nKorea by using the Implicit Association Test. Conducting double auction and\nbilateral bargaining market experiments, we find that North Korean refugees\nwith a larger bias against the capitalistic society have lower expectations\nabout their earning potential, exhibit trading behavior with lower target\nprofits, and earn less profits. These associations are robust to conditioning\non correlates of preferences, human capital, and assimilation experiences.\n"
    },
    {
        "paper_id": 2304.01207,
        "authors": "St\\'ephane Cr\\'epey (LPSM (UMR\\_8001)), Noufel Frikha (CES), Azar\n  Louzi (LPSM (UMR\\_8001))",
        "title": "A Multilevel Stochastic Approximation Algorithm for Value-at-Risk and\n  Expected Shortfall Estimation",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We propose a multilevel stochastic approximation (MLSA) scheme for the\ncomputation of the value-at-risk (VaR) and expected shortfall (ES) of a\nfinancial loss, which can only be computed via simulations conditional on the\nrealization of future risk factors. Thus, the problem of estimating its VaR and\nES is nested in nature and can be viewed as an instance of stochastic\napproximation problems with biased innovations. In this framework, for a\nprescribed accuracy $\\epsilon$, the optimal complexity of a nested stochastic\napproximation algorithm is shown to be of order $\\epsilon$--3. To estimate the\nVaR, our MLSA algorithm attains an optimal complexity of order\n$\\epsilon$--2--$\\delta$ , where $\\delta$ \\< 1 is some parameter depending on\nthe integrability degree of the loss, while to estimate the ES, it achieves an\noptimal complexity of order $\\epsilon$--2 |ln $\\epsilon$|2. Numerical studies\nof the joint evolution of the error rate and the execution time demonstrate how\nour MLSA algorithm regains a significant amount of the performance lost due to\nthe nested nature of the problem.\n"
    },
    {
        "paper_id": 2304.01272,
        "authors": "Scott Robertson",
        "title": "Equilibrium with Heterogeneous Information Flows",
        "comments": "47 pages, 2 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We study a continuous time economy where throughout time, insiders receive\nprivate signals regarding the risky assets' terminal payoff. We prove existence\nof a partial communication equilibrium where, at each private signal time, the\npublic receives a signal of the same form as the associated insider, but of\nlower quality. This causes a jump in both the public information flow and\nequilibrium asset price. The resultant markets, while complete between each\njump time, are incomplete over each jump. After establishing equilibrium for a\nfinite number of private signal times, we consider the limit as the private\nsignals become more and more frequent. Under appropriate scaling we prove\nconvergence of the public filtration to the natural filtration generated by\nboth the fundamental factor process $X$ and a continuous time process $J$\ntaking the form $J_t = X_1 + Y_t$ where $X_1$ is the terminal payoff and $Y$ an\nindependent Gaussian process. This coincides with the filtration considered in\n'Additional Utility of Insiders with Imperfect Dynamical Information'\n(Corcuera, et al. Finance & Stochastics 2004). However, while therein the\nfiltration was exogenously assumed to be that of an insider who observes a\nprivate signal flow, here it arises endogenously as the public filtration when\nthere are a large number of insiders receiving signals throughout time.\n"
    },
    {
        "paper_id": 2304.0149,
        "authors": "Finn Lattimore, Daniel M. Steinberg, Anna Zhu",
        "title": "The Economic Effect of Gaining a New Qualification Later in Life",
        "comments": "80 pages, 16 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-sa/4.0/",
        "abstract": "  Pursuing educational qualifications later in life is an increasingly common\nphenomenon within OECD countries since technological change and automation\ncontinues to drive the evolution of skills needed in many professions. We focus\non the causal impacts to economic returns of degrees completed later in life,\nwhere motivations and capabilities to acquire additional education may be\ndistinct from education in early years. We find that completing an additional\ndegree leads to more than \\$3000 (AUD, 2019) extra income per year compared to\nthose who do not complete additional study. For outcomes, treatment and\ncontrols we use the extremely rich and nationally representative longitudinal\ndata from the Household Income and Labour Dynamics Australia survey (HILDA). To\ntake full advantage of the complexity and richness of this data we use a\nMachine Learning (ML) based methodology for causal effect estimation. We are\nalso able to use ML to discover sources of heterogeneity in the effects of\ngaining additional qualifications. For example, those younger than 45 years of\nage when obtaining additional qualifications tend to reap more benefits (as\nmuch as \\$50 per week more) than others.\n"
    },
    {
        "paper_id": 2304.01709,
        "authors": "Johannes Mauritzen",
        "title": "With great power (prices) comes great tail pipe emissions? \\\\ A natural\n  experiment of electricity prices and electric car adoption",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  A fundemantal and unanswered question for the widely shared goal of\nelectrifying passenger vehicles is how the price of electricity, which can vary\ngreatly across countries and regions, affects buying behavior. I make use of a\nnatural experiment in Norway in the period 2021-2022 when large price\ndifferences between north and south emerged to estimate the effect of\nelectricity prices on the decision to purchase a pure battery-electric vehicle.\nSimple difference estimates along the border of the price zones as well as a\ndifference-in-difference regression model suggest a significant but\neconomically modest effect of a 2-4\\% reduction in the probability of\npurchasing an electric vehicle in the high price zone. A counterfactual\nsimulation suggests that there would have been about 3000 to 6000 fewer\nelectric vehicles sold in the high-price south compared to a scenario where the\nsouth had equally low prices as in the north.\n"
    },
    {
        "paper_id": 2304.01907,
        "authors": "Roberto Mota Navarro, Francois Leyvraz, Hern\\'an Larralde",
        "title": "Dynamical properties of volume at the spread in the Bitcoin/USD market",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  The study of order volumes in financial markets has shown that these display\nseveral non-trivial statistical properties. Most studies have been focused on\nthe bulk properties of volume of incoming orders or of realized transactions\nrather than the dynamical aspects. The present work is a study of the dynamical\nproperties of volume. Unlike previous works, we studied the volume available at\nthe spread rather than the volume of incoming orders or of realized\ntransactions. We found evidence that suggests mean reverting volume changes and\nstrong asymmetries in the equilibrium of sell and buy orders as well as the\npresence of clustering.\n"
    },
    {
        "paper_id": 2304.01918,
        "authors": "Mansur Bestas",
        "title": "Decentralized Finance (DeFi)",
        "comments": "in Turkish language",
        "journal-ref": "International Journal of Social Humanities Sciences Research\n  (JSHSR). volume 91-13 (2023)",
        "doi": "10.26450/jshsr.3445",
        "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
        "abstract": "  Decentralized finance, powered by blockchain technology, is growing day by\nday. This field, which emerged a few years ago, today manages $70 billion in\nassets. In this study, the concept of decentralized finance is discussed and\nexplained the differences from traditional finance. Then, compliance with the\nlegal regulations and the requirements to ensure compliance are mentioned. An\nevaluation has been made about the financial services offered by the\ndecentralized finance field and the stock market and stablecoins that it uses\nas a tool while providing these services. Its economic effects, security and,\nprivacy dimensions are examined. In the study, the differences between\ncentralized and decentralized finance, which generally covers legal, economic,\nsecurity, privacy, and market manipulation, are systematically analyzed. A\nstructured methodology is presented to distinguish between centralized and\ndecentralized financial services. Keywords: decentralized finance, FinTech,\nfinancial regulation, blockchain, distributed ledger technology.\n"
    },
    {
        "paper_id": 2304.02094,
        "authors": "Faraz Sasani, Ramin Mousa, Ali Karkehabadi, Samin Dehbashi, Ali\n  Mohammadi",
        "title": "TM-vector: A Novel Forecasting Approach for Market stock movement with a\n  Rich Representation of Twitter and Market data",
        "comments": "24 page",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Stock market forecasting has been a challenging part for many analysts and\nresearchers. Trend analysis, statistical techniques, and movement indicators\nhave traditionally been used to predict stock price movements, but text\nextraction has emerged as a promising method in recent years. The use of neural\nnetworks, especially recurrent neural networks, is abundant in the literature.\nIn most studies, the impact of different users was considered equal or ignored,\nwhereas users can have other effects. In the current study, we will introduce\nTM-vector and then use this vector to train an IndRNN and ultimately model the\nmarket users' behaviour. In the proposed model, TM-vector is simultaneously\ntrained with both the extracted Twitter features and market information.\nVarious factors have been used for the effectiveness of the proposed\nforecasting approach, including the characteristics of each individual user,\ntheir impact on each other, and their impact on the market, to predict market\ndirection more accurately. Dow Jones 30 index has been used in current work.\nThe accuracy obtained for predicting daily stock changes of Apple is based on\nvarious models, closed to over 95\\% and for the other stocks is significant.\nOur results indicate the effectiveness of TM-vector in predicting stock market\ndirection.\n"
    },
    {
        "paper_id": 2304.0218,
        "authors": "Sebastian Jaimungal, Yuri F. Saporito, Max O. Souza and Yuri Thamsten",
        "title": "Optimal Trading in Automatic Market Makers with Deep Learning",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
        "abstract": "  This article explores the optimisation of trading strategies in Constant\nFunction Market Makers (CFMMs) and centralised exchanges. We develop a model\nthat accounts for the interaction between these two markets, estimating the\nconditional dependence between variables using the concept of conditional\nelicitability. Furthermore, we pose an optimal execution problem where the\nagent hides their orders by controlling the rate at which they trade. We do so\nwithout approximating the market dynamics. The resulting dynamic programming\nequation is not analytically tractable, therefore, we employ the deep Galerkin\nmethod to solve it. Finally, we conduct numerical experiments and illustrate\nthat the optimal strategy is not prone to price slippage and outperforms\nna\\\"ive strategies.\n"
    },
    {
        "paper_id": 2304.02272,
        "authors": "Wei Tian, Seojeong Lee, Valentyn Panchenko",
        "title": "Synthetic Controls with Multiple Outcomes",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  We generalize the synthetic control (SC) method to a multiple-outcome\nframework, where the conventional pre-treatment time dimension is supplemented\nwith the extra dimension of related outcomes in computing the SC weights. This\ngeneralization improves the reliability of treatment effect estimation, and can\nbe particularly useful for evaluating the effect of a treatment on multiple\noutcomes or when only a small number of pre-treatment periods are available. To\nillustrate our method, we provide a new perspective on the classic SC\napplication to the 1990 German reunification.\n"
    },
    {
        "paper_id": 2304.02356,
        "authors": "Davide Lauria, W. Brent Lindquist, Svetlozar T. Rachev and Yuan Hu",
        "title": "Unifying Market Microstructure and Dynamic Asset Pricing",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  We introduce a discrete binary tree for pricing contingent claims with the\nunderlying security prices exhibiting history dependence characteristic of that\ninduced by market microstructure phenomena. Example dependencies considered\ninclude moving average or autoregressive behavior. Our model is\nmarket-complete, arbitrage-free, and preserves all of the parameters governing\nthe historical (natural world) price dynamics when passing to an equivalent\nmartingale (risk-neutral) measure. Specifically, this includes the\ninstantaneous mean and variance of the asset return and the instantaneous\nprobabilities for the direction of asset price movement. We believe this is the\nfirst paper to demonstrate the ability to include market microstructure effects\nin dynamic asset/option pricing in a market-complete, no-arbitrage, format.\n"
    },
    {
        "paper_id": 2304.02362,
        "authors": "Ruixue Jing and Luis Enrique Correa Rocha",
        "title": "A network-based strategy of price correlations for optimal\n  cryptocurrency portfolios",
        "comments": "Comments welcomed",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  A cryptocurrency is a digital asset maintained by a decentralised system\nusing cryptography. Investors in this emerging digital market are exploring the\nprofitability potential of portfolios in place of single coins. Portfolios are\nparticularly useful given that price forecasting in such a volatile market is\nchallenging. The crypto market is a self-organised complex system where the\ncomplex inter-dependencies between the cryptocurrencies may be exploited to\nunderstand the market dynamics and build efficient portfolios. In this letter,\nwe use network methods to identify highly decorrelated cryptocurrencies to\ncreate diversified portfolios using the Markowitz Portfolio Theory agnostic to\nfuture market behaviour. The performance of our network-based portfolios is\noptimal with 46 coins and superior to benchmarks up to an investment horizon of\n14 days, reaching up to 1,066% average expected return within 1 day, with\nreasonable associated risks. We also show that popular cryptocurrencies are\ntypically not included in the optimal portfolios. Past price correlations\nreduce risk and may improve the performance of crypto portfolios in comparison\nto methodologies based exclusively on price auto-correlations. Short-term\ncrypto investments may be competitive to traditional high-risk investments such\nas the stock market or commodity market but call for caution given the high\nvariability of prices.\n"
    },
    {
        "paper_id": 2304.0243,
        "authors": "Samidh Pal",
        "title": "A Comparative Study of Inter-Regional Intra-Industry Disparity",
        "comments": "1 map, 5 tables, 7 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper investigates the inter-regional intra-industry disparity within\nselected Indian manufacturing industries and industrial states. The study uses\nthree measures - the Output-Capital Ratio, the Capital-Labor Ratio, and the\nOutput-Labor Ratio - to critically evaluate the level of disparity in average\nefficiency of labor and capital, as well as capital intensity. Additionally,\nthe paper compares the rate of disparity of per capita income between six major\nindustrial states. The study finds that underutilization of capacity is driven\nby an unequal distribution of high-skilled labor supply and upgraded\ntechnologies. To address these disparities, the paper suggests that\npolicymakers campaign for labor training and technology promotion schemes\nthroughout all regions of India. By doing so, the study argues, the country can\nreduce regional inequality and improve economic outcomes for all.\n"
    },
    {
        "paper_id": 2304.02472,
        "authors": "Artem Lensky, Mingyu Hao",
        "title": "Learning to Predict Short-Term Volatility with Order Flow Image\n  Representation",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Introduction: The paper addresses the challenging problem of predicting the\nshort-term realized volatility of the Bitcoin price using order flow\ninformation. The inherent stochastic nature and anti-persistence of price pose\ndifficulties in accurate prediction.\n  Methods: To address this, we propose a method that transforms order flow data\nover a fixed time interval (snapshots) into images. The order flow includes\ntrade sizes, trade directions, and limit order book, and is mapped into image\ncolour channels. These images are then used to train both a simple 3-layer\nConvolutional Neural Network (CNN) and more advanced ResNet-18 and ConvMixer,\nwith additionally supplementing them with hand-crafted features. The models are\nevaluated against classical GARCH, Multilayer Perceptron trained on raw data,\nand a naive guess method that considers current volatility as a prediction.\n  Results: The experiments are conducted using price data from January 2021 and\nevaluate model performance in terms of root mean square error (RMSPE). The\nresults show that our order flow representation with a CNN as a predictive\nmodel achieves the best performance, with an RMSPE of 0.85+/-1.1 for the model\nwith aggregated features and 1.0+/-1.4 for the model without feature\nsupplementation. ConvMixer with feature supplementation follows closely. In\ncomparison, the RMSPE for the naive guess method was 1.4+/-3.0.\n"
    },
    {
        "paper_id": 2304.02479,
        "authors": "Cyril B\\'en\\'ezet (LaMME, ENSIIE), St\\'ephane Cr\\'epey (LPSM\n  (UMR\\_8001), UPCit\\'e), Dounia Essaket (LPSM (UMR\\_8001), UPCit\\'e)",
        "title": "Hedging Valuation Adjustment for Callable Claims",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Darwinian model risk is the risk of mis-price-and-hedge biased toward\nshort-to-medium systematic profits of a trader, which are only the compensator\nof long term losses becoming apparent under extreme scenarios where the bad\nmodel of the trader no longer calibrates to the market. The alpha leakages that\ncharacterize Darwinian model risk are undetectable by the usual market risk\ntools such as value-at-risk, expected shortfall, or stressed\nvalue-at-risk.Darwinian model risk can only be seen by simulating the hedging\nbehavior of a bad model within a good model. In this paper we extend to\ncallable assets the notion of hedging valuation adjustment introduced in\nprevious work for quantifying and handling such risk. The mathematics of\nDarwinian model risk for callable assets are illustrated by exact numerics on a\nstylized callable range accrual example. Accounting for the wrong hedges and\nexercise decisions, the magnitude of the hedging valuation adjustment can be\nseveral times larger than the mere difference, customarily used in banks as a\nreserve against model risk, between the trader's price of a callable asset and\nits fair valuation.\n"
    },
    {
        "paper_id": 2304.02573,
        "authors": "Ana Lucia Luis, Natalia Teixeira, Rui Braz",
        "title": "Portuguese Households Savings in Times of Pandemic: A Way to Better\n  Resist the Escalating Inflation?",
        "comments": "8 pages, 5 figures, 2 tables",
        "journal-ref": "International Research Journal of Economics and Management\n  Studies, Vol. 2, No. 1, pp. 283-290, 2023",
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  March 2020 confinement has shot Portuguese savings to historic levels,\nreaching 13.4% of gross disposable income in early 2021 (INE, 2023). To find\nsimilar savings figures we need to go back to 1999. With consumption reduced to\na bare minimum, the Portuguese were forced to save. Households reduced spending\nmore because of a lack of alternatives to consumption than for any other\nreason. The relationship between consumption, savings, and income has occupied\nan important role in economic thought [(Keynes, 1936; 1937); (Friedman, 1957)].\nTraditionally, high levels of savings have been associated with benefits to the\neconomy, since financing capacity is enhanced (Singh, 2010). However, the\neffects here can be twofold. On the one hand, it seems that Portugal faced the\nso-called Savings Paradox (Keynes, 1936). If consumers decide to save a\nconsiderable part of their income, there will be less demand for the goods\nproduced. Lower demand will lead to lower supply, production, income, and,\nparadoxically, fewer savings. On the other hand, after having accumulated\nsavings at the peak of the pandemic, the Portuguese are now using them to carry\nout postponed consumption and, hopefully, to better resist the escalating\ninflation. This study aims to examine Portuguese households' savings evolution\nduring the most critical period of the pandemic, between March 2020 and April\n2022. The methodology analyses the correlation between savings, consumption,\nand GDP as well as GDP's decomposition into its various components and\nconcluded that these suddenly forced savings do not fit traditional economic\ntheories of savings.\n"
    },
    {
        "paper_id": 2304.02723,
        "authors": "Daoping Yu, Vytaras Brazauskas, Ricardas Zitikis",
        "title": "Measuring Discrete Risks on Infinite Domains: Theoretical Foundations,\n  Conditional Five Number Summaries, and Data Analyses",
        "comments": "22 pages, 1 figure, 7 tables",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  To accommodate numerous practical scenarios, in this paper we extend\nstatistical inference for smoothed quantile estimators from finite domains to\ninfinite domains. We accomplish the task with the help of a newly designed\ntruncation methodology for discrete loss distributions with infinite domains. A\nsimulation study illustrates the methodology in the case of several\ndistributions, such as Poisson, negative binomial, and their zero inflated\nversions, which are commonly used in insurance industry to model claim\nfrequencies. Additionally, we propose a very flexible bootstrap-based approach\nfor the use in practice. Using automobile accident data and their\nmodifications, we compute what we have termed the conditional five number\nsummary (C5NS) for the tail risk and construct confidence intervals for each of\nthe five quantiles making up C5NS, and then calculate the tail probabilities.\nThe results show that the smoothed quantile approach classifies the tail\nriskiness of portfolios not only more accurately but also produces lower\ncoefficients of variation in the estimation of tail probabilities than those\nobtained using the linear interpolation approach.\n"
    },
    {
        "paper_id": 2304.02737,
        "authors": "Jacob Carlson and Tom Bryan and Melissa Dell",
        "title": "Efficient OCR for Building a Diverse Digital History",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Thousands of users consult digital archives daily, but the information they\ncan access is unrepresentative of the diversity of documentary history. The\nsequence-to-sequence architecture typically used for optical character\nrecognition (OCR) - which jointly learns a vision and language model - is\npoorly extensible to low-resource document collections, as learning a\nlanguage-vision model requires extensive labeled sequences and compute. This\nstudy models OCR as a character level image retrieval problem, using a\ncontrastively trained vision encoder. Because the model only learns characters'\nvisual features, it is more sample efficient and extensible than existing\narchitectures, enabling accurate OCR in settings where existing solutions fail.\nCrucially, the model opens new avenues for community engagement in making\ndigital history more representative of documentary history.\n"
    },
    {
        "paper_id": 2304.03038,
        "authors": "Greig Cowan, Salvatore Mercuri, Raad Khraishi",
        "title": "Modelling customer lifetime-value in the retail banking industry",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Understanding customer lifetime value is key to nurturing long-term customer\nrelationships, however, estimating it is far from straightforward. In the\nretail banking industry, commonly used approaches rely on simple heuristics and\ndo not take advantage of the high predictive ability of modern machine learning\ntechniques. We present a general framework for modelling customer lifetime\nvalue which may be applied to industries with long-lasting contractual and\nproduct-centric customer relationships, of which retail banking is an example.\nThis framework is novel in facilitating CLV predictions over arbitrary time\nhorizons and product-based propensity models. We also detail an implementation\nof this model which is currently in production at a large UK lender. In\ntesting, we estimate an 43% improvement in out-of-time CLV prediction error\nrelative to a popular baseline approach. Propensity models derived from our CLV\nmodel have been used to support customer contact marketing campaigns. In\ntesting, we saw that the top 10% of customers ranked by their propensity to\ntake up investment products were 3.2 times more likely to take up an investment\nproduct in the next year than a customer chosen at random.\n"
    },
    {
        "paper_id": 2304.03042,
        "authors": "Ofelia Bonesini, Antoine Jacquier and Alexandre Pannier",
        "title": "Rough volatility, path-dependent PDEs and weak rates of convergence",
        "comments": "52 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In the setting of stochastic Volterra equations, and in particular rough\nvolatility models, we show that conditional expectations are the unique\nclassical solutions to path-dependent PDEs. The latter arise from the\nfunctional It\\^o formula developed by [Viens, F., & Zhang, J. (2019). A\nmartingale approach for fractional Brownian motions and related path dependent\nPDEs. Ann. Appl. Probab.]. We then leverage these tools to study weak rates of\nconvergence for discretised stochastic integrals of smooth functions of a\nRiemann-Liouville fractional Brownian motion with Hurst parameter $H \\in\n(0,1/2)$. These integrals approximate log-stock prices in rough volatility\nmodels. We obtain weak error rates of order 1 if the test function is quadratic\nand of order $H+1/2$ for smooth test functions.\n"
    },
    {
        "paper_id": 2304.03403,
        "authors": "Haonan Zhang",
        "title": "Leveraging policy instruments and financial incentives to reduce\n  embodied carbon in energy retrofits",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  The existing buildings and building construction sectors together are\nresponsible for over one-third of the total global energy consumption and\nnearly 40% of total greenhouse gas (GHG) emissions. GHG emissions from the\nbuilding sector are made up of embodied emissions and operational emissions.\nRecognizing the importance of reducing energy use and emissions associated with\nthe building sector, governments have introduced policies, standards, and\ndesign guidelines to improve building energy performance and reduce GHG\nemissions associated with operating buildings. However, policy initiatives that\nreduce embodied emissions of the existing building sector are lacking. This\nresearch aims to develop policy strategies to reduce embodied carbon emissions\nin retrofits. In order to achieve this goal, this research conducted a\nliterature review and identification of policies and financial incentives in\nBritish Columbia (BC) for reducing overall GHG emissions from the existing\nbuilding sector. Then, this research analyzed worldwide policies and incentives\nthat reduce embodied carbon emissions in the existing building sector. After\nreviewing the two categories of retrofit policies, the author identified links\nand opportunities between existing BC strategies, tools, and incentives, and\nglobal embodied emission strategies. Finally, this research compiled key\nfindings from all resources and provided policy recommendations for reducing\nembodied carbon emissions in retrofits in BC.\n"
    },
    {
        "paper_id": 2304.03436,
        "authors": "Bikramaditya Datta, Rajiv Sethi",
        "title": "The Dynamics of Leverage and the Belief Distribution of Wealth",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The scale and terms of aggregate borrowing in an economy depend on the manner\nin which wealth is distributed across potential creditors with heterogeneous\nbeliefs about the future. This distribution evolves over time as uncertainty is\nresolved, in favour of optimists if loans are repaid in full, and in favour of\npessimists if there is widespread default. We model this process in an economy\nwith two assets - risky bonds and risk-free cash. Within periods, given the\ninherited distribution of wealth across belief types, the scale and terms of\nborrowing are endogenously determined. Following good states, aggregate\nborrowing and the face value of debt both rise, and the interest rate falls. In\nthe absence of noise, wealth converges to beliefs that differ systematically\nfrom the objective probability governing state realisations, with greater\nrisk-aversion associated with greater optimism. In the presence of noise, the\neconomy exhibits periods of high performance, punctuated by periods of crisis\nand stagnation.\n"
    },
    {
        "paper_id": 2304.03437,
        "authors": "Haoyu Wang, Junpeng Di, Yuegu Xie",
        "title": "Echo disappears: momentum term structure and cyclic information in\n  turnover",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  We extract cyclic information in turnover and find it can explain the\nmomentum echo. The reversal in recent month momentum is the key factor that\ncancels out the recent month momentum and excluding it makes the echo regress\nto a damped shape. Both rational and behavioral theories can explain the\nreversal. This study is the first explanation of the momentum echo in U.S.\nstock markets.\n"
    },
    {
        "paper_id": 2304.03464,
        "authors": "Abhishek Arora and Xinmei Yang and Shao-Yu Jheng and Melissa Dell",
        "title": "Linking Representations with Multimodal Contrastive Learning",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Many applications require linking individuals, firms, or locations across\ndatasets. Most widely used methods, especially in social science, do not employ\ndeep learning, with record linkage commonly approached using string matching\ntechniques. Moreover, existing methods do not exploit the inherently multimodal\nnature of documents. In historical record linkage applications, documents are\ntypically noisily transcribed by optical character recognition (OCR). Linkage\nwith just OCR'ed texts may fail due to noise, whereas linkage with just image\ncrops may also fail because vision models lack language understanding (e.g., of\nabbreviations or other different ways of writing firm names). To leverage\nmultimodal learning, this study develops CLIPPINGS (Contrastively LInking\nPooled Pre-trained Embeddings). CLIPPINGS aligns symmetric vision and language\nbi-encoders, through contrastive language-image pre-training on document images\nand their corresponding OCR'ed texts. It then contrastively learns a metric\nspace where the pooled image-text embedding for a given instance is close to\nembeddings in the same class (e.g., the same firm or location) and distant from\nembeddings of a different class. Data are linked by treating linkage as a\nnearest neighbor retrieval problem with the multimodal embeddings. CLIPPINGS\noutperforms widely used string matching methods by a wide margin in linking\nmid-20th century Japanese firms across financial documents. A purely\nself-supervised model - trained only by aligning the embeddings for the image\ncrop of a firm name and its corresponding OCR'ed text - also outperforms\npopular string matching methods. Fascinatingly, a multimodally pre-trained\nvision-only encoder outperforms a unimodally pre-trained vision-only encoder,\nillustrating the power of multimodal pre-training even if only one modality is\navailable for linking at inference time.\n"
    },
    {
        "paper_id": 2304.03525,
        "authors": "Mohib Jafri, Andy Wu",
        "title": "Distributed VC Firms: The Next Iteration of Venture Capital",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Using a combination of incentive modeling and empirical meta-analyses, this\npaper provides a pointed critique at the incentive systems that drive venture\ncapital firms to optimize their practices towards activities that increase\nGeneral Partner utility yet are disjoint from improving the underlying asset of\nstartup equity. We propose a \"distributed venture firm\" powered by software\nautomations and governed by a set of functional teams called \"Pods\" that carry\nout specific tasks with immediate and long-term payouts given on a deal-by-deal\nbasis. Avenues are provided for further research to validate this model and\ndiscover likely paths to implementation.\n"
    },
    {
        "paper_id": 2304.03676,
        "authors": "Rama K. Malladi (California State University, Dominguez Hills),\n  Phillip Thompson (The Idaho Black History Museum)",
        "title": "Idaho Blacks: Quiet Economic Triumph of Enduring Champions",
        "comments": "Keywords: Idaho; Black History; African-American Achievements;\n  Economic History; Economic Disparity. JEL Classification: B15, B55, N31, N32,\n  D7",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  As the United States is witnessing elevated racial differences pertaining to\neconomic disparities, we have found a unique example contrary to the\ntraditional narrative. Idaho is the only US state where Blacks earn more than\nWhites and all other races. In this paper, we examine how Idaho Blacks might\nhave achieved economic success and, more importantly, what factors might have\nled to this achievement in reducing racial and economic disparities.\nPreliminary research suggests that fewer barriers to land ownership, smaller\npopulations, well-knit communities, men's involvement in the family, and a\nrelatively less hostile environment have played a significant role. Further\nresearch by historians can help the nation uncover the underlying factors to\nsee if some factors are transportable to other parts of the country.\n"
    },
    {
        "paper_id": 2304.03877,
        "authors": "Nikolas Michael, Mihai Cucuringu, Sam Howison",
        "title": "OFTER: An Online Pipeline for Time Series Forecasting",
        "comments": "26 pages, 12 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  We introduce OFTER, a time series forecasting pipeline tailored for mid-sized\nmultivariate time series. OFTER utilizes the non-parametric models of k-nearest\nneighbors and Generalized Regression Neural Networks, integrated with a\ndimensionality reduction component. To circumvent the curse of dimensionality,\nwe employ a weighted norm based on a modified version of the maximal\ncorrelation coefficient. The pipeline we introduce is specifically designed for\nonline tasks, has an interpretable output, and is able to outperform several\nstate-of-the art baselines. The computational efficacy of the algorithm, its\nonline nature, and its ability to operate in low signal-to-noise regimes,\nrender OFTER an ideal approach for financial multivariate time series problems,\nsuch as daily equity forecasting. Our work demonstrates that while deep\nlearning models hold significant promise for time series forecasting,\ntraditional methods carefully integrating mainstream tools remain very\ncompetitive alternatives with the added benefits of scalability and\ninterpretability.\n"
    },
    {
        "paper_id": 2304.04236,
        "authors": "Anindya Bhattacharya, Anirban Kar, Alita Nandi",
        "title": "Asymmetric networks, clientelism and their impacts: households' access\n  to workfare employment in rural India",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper we explore two intertwined issues. First, using primary data we\nexamine the impact of asymmetric networks, built on rich relational information\non several spheres of living, on access to workfare employment in rural India.\nWe find that unidirectional relations, as opposed to reciprocal relations, and\nthe concentration of such unidirectional relations increase access to workfare\njobs. Further in-depth exploration provides evidence that patron-client\nrelations are responsible for this differential access to such employment for\nrural households. Complementary to our empirical exercises, we construct and\nanalyse a game-theoretical model supporting our findings.\n"
    },
    {
        "paper_id": 2304.04242,
        "authors": "Alberto Baccini and Cristina Re",
        "title": "Who are the gatekeepers of economics? Geographic diversity, gender\n  composition, and interlocking editorship of journal boards",
        "comments": "23 pages, 17 tables, 6 figures",
        "journal-ref": "Review of Political Economy, 2024",
        "doi": "10.1080/09538259.2024.2303654",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This study investigates the role of editorial board members as gatekeepers in\nscience, creating and utilizing a database of 1,516 active economics journals\nin 2019, which includes more than 44,000 scholars from over 6,000 institutions\nand 142 countries. The composition of these editorial boards is explored in\nterms of geographic affiliation, institutional affiliation, and gender. Results\nhighlight that the academic publishing environment is primarily governed by men\naffiliated with elite universities in the United States. The study further\nexplores social similarities among journals using a network analysis\nperspective based on interlocking editorship. Comparison of networks generated\nby all scholars, editorial leaders, and non-editorial leaders reveals\nsignificant structural similarities and associations among clusters of\njournals. These results indicate that links between pairs of journals tend to\nbe redundant, and this can be interpreted in terms of social and intellectual\nhomophily within each board, and between boards of journals belonging to the\nsame cluster. Finally, the analysis of the most central journals and scholars\nin the networks suggests that journals probably adopt 'strategic decisions' in\nthe selection of the editorial board members. The documented high concentration\nof editorial power poses a serious risk to innovative research in economics.\n"
    },
    {
        "paper_id": 2304.04372,
        "authors": "Jir\\^o Akahori, Nien-Lin Liu, Maria Elvira Mancino, Tommaso Mariotti,\n  Yukie Yasuda",
        "title": "Symmetric positive semi-definite Fourier estimator of instantaneous\n  variance-covariance matrix",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper we propose an estimator of spot covariance matrix which ensure\nsymmetric positive semi-definite estimations. The proposed estimator relies on\na suitable modification of the Fourier covariance estimator in Malliavin and\nMancino (2009) and it is consistent for suitable choices of the weighting\nkernel. The accuracy and the ability of the estimator to produce positive\nsemi-definite covariance matrices is evaluated with an extensive numerical\nstudy, in comparison with the competitors present in the literature. The\nresults of the simulation study are confirmed under many scenarios, that\nconsider the dimensionality of the problem, the asynchronicity of data and the\npresence of several specification of market microstructure noise.\n"
    },
    {
        "paper_id": 2304.04396,
        "authors": "Weiwei Li and Dejian Tian",
        "title": "Robust optimized certainty equivalents and quantiles for loss positions\n  with distribution uncertainty",
        "comments": "5 figures, 24 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  The paper investigates the robust optimized certainty equivalents and\nanalyzes the relevant properties of them as risk measures for loss positions\nwith distribution uncertainty. On this basis, the robust generalized quantiles\nare proposed and discussed. The robust expectiles with two specific\npenalization functions $\\varphi_{1}$ and $\\varphi_{2}$ are further considered\nrespectively. The robust expectiles with $\\varphi_{1}$ are proved to be\ncoherent risk measures, and the dual representation theorems are established.\nIn addition, the effect of penalization functions on the robust expectiles and\nits comparison with expectiles are examined and simulated numerically.\n"
    },
    {
        "paper_id": 2304.04453,
        "authors": "Claudio Fontana, Simone Pavarana, Wolfgang J. Runggaldier",
        "title": "A stochastic control perspective on term structure models with roll-over\n  risk",
        "comments": "25 pages (revised version)",
        "journal-ref": "Finance and Stochastics (2023), 27: 903-932",
        "doi": "10.1007/s00780-023-00515-z",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper, we consider a generic interest rate market in the presence of\nroll-over risk, which generates spreads in spot/forward term rates. We do not\nrequire classical absence of arbitrage and rely instead on a minimal market\nviability assumption, which enables us to work in the context of the benchmark\napproach. In a Markovian setting, we extend the control theoretic approach of\nGombani & Runggaldier (2013) and derive representations of spot/forward spreads\nas value functions of suitable stochastic optimal control problems, formulated\nunder the real-world probability and with power-type objective functionals. We\ndetermine endogenously the funding-liquidity spread by relating it to the\nrisk-sensitive optimization problem of a representative investor.\n"
    },
    {
        "paper_id": 2304.04626,
        "authors": "Alex A.T. Rathke",
        "title": "On the state-space model of unawareness",
        "comments": "working paper, please reference this version in further studies",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  We show that the knowledge of an agent carrying non-trivial unawareness\nviolates the standard property of 'necessitation', therefore necessitation\ncannot be used to refute the standard state-space model. A revised version of\nnecessitation preserves non-trivial unawareness and solves the classical\nDekel-Lipman-Rustichini result. We propose a generalised knowledge operator\nconsistent with the standard state-space model of unawareness, including the\nmodel of infinite state-space.\n"
    },
    {
        "paper_id": 2304.04676,
        "authors": "Ke Zhang",
        "title": "Adjust factor with volatility model using MAXFLAT low-pass filter and\n  construct portfolio in China A share market",
        "comments": "Volatility model, Multi-factors model, Portfolio construction, China\n  A share market",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  In the field of quantitative finance, volatility models, such as ARCH, GARCH,\nFIGARCH, SV, EWMA, play the key role in risk and portfolio management.\nMeanwhile, factor investing is more and more famous since mid of 20 century.\nCAPM, Fama French three factor model, Fama French five-factor model, MSCI Barra\nfactor model are mentioned and developed during this period. In this paper, we\nwill show why we need adjust group of factors by our MAXFLAT low-pass\nvolatility model. All of our experiments are under China's CSI 300 and CSI 500\nuniverse which represent China's large cap stocks and mid-small cap stocks. Our\nresult shows adjust factors by MAXFLAT volatility model have better performance\nin both large cap and small cap universe than original factors or other risk\nadjust factors in China A share. Also the portfolio constructed by MAXFLAT risk\nadjust factors have continuous excess return and lower beta compare with\nbenchmark index.\n"
    },
    {
        "paper_id": 2304.04912,
        "authors": "Zhen Zeng, Rachneet Kaur, Suchetha Siddagangappa, Saba Rahimi, Tucker\n  Balch, Manuela Veloso",
        "title": "Financial Time Series Forecasting using CNN and Transformer",
        "comments": "Published at AAAI 2023 - AI for Financial Services Bridge",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Time series forecasting is important across various domains for\ndecision-making. In particular, financial time series such as stock prices can\nbe hard to predict as it is difficult to model short-term and long-term\ntemporal dependencies between data points. Convolutional Neural Networks (CNN)\nare good at capturing local patterns for modeling short-term dependencies.\nHowever, CNNs cannot learn long-term dependencies due to the limited receptive\nfield. Transformers on the other hand are capable of learning global context\nand long-term dependencies. In this paper, we propose to harness the power of\nCNNs and Transformers to model both short-term and long-term dependencies\nwithin a time series, and forecast if the price would go up, down or remain the\nsame (flat) in the future. In our experiments, we demonstrated the success of\nthe proposed method in comparison to commonly adopted statistical and deep\nlearning methods on forecasting intraday stock price change of S&P 500\nconstituents.\n"
    },
    {
        "paper_id": 2304.04914,
        "authors": "Gillian K. Hadfield, Jack Clark",
        "title": "Regulatory Markets: The Future of AI Governance",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  Appropriately regulating artificial intelligence is an increasingly urgent\npolicy challenge. Legislatures and regulators lack the specialized knowledge\nrequired to best translate public demands into legal requirements. Overreliance\non industry self-regulation fails to hold producers and users of AI systems\naccountable to democratic demands. Regulatory markets, in which governments\nrequire the targets of regulation to purchase regulatory services from a\nprivate regulator, are proposed. This approach to AI regulation could overcome\nthe limitations of both command-and-control regulation and self-regulation.\nRegulatory market could enable governments to establish policy priorities for\nthe regulation of AI, whilst relying on market forces and industry R&D efforts\nto pioneer the methods of regulation that best achieve policymakers' stated\nobjectives.\n"
    },
    {
        "paper_id": 2304.05004,
        "authors": "Bikramjit Das and Vicky Fasen-Hartmann",
        "title": "On heavy-tailed risks under Gaussian copula: the effects of marginal\n  transformation",
        "comments": "23 pages, 3 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper, we compute multivariate tail risk probabilities where the\nmarginal risks are heavy-tailed and the dependence structure is a Gaussian\ncopula. The marginal heavy-tailed risks are modeled using regular variation\nwhich leads to a few interesting consequences. First, as the threshold\nincreases, we note that the rate of decay of probabilities of tail sets vary\ndepending on the type of tail sets considered and the Gaussian correlation\nmatrix. Second, we discover that although any multivariate model with a\nGaussian copula admits the so called asymptotic tail independence property, the\njoint tail behavior under heavier tailed marginal variables is structurally\ndistinct from that under Gaussian marginal variables. The results obtained are\nillustrated using examples and simulations.\n"
    },
    {
        "paper_id": 2304.05033,
        "authors": "Nigel Adams, Adriano Augusto, Michael Davern, Marcello La Rosa",
        "title": "Five guidelines to improve context-aware process selection: an\n  Australian banking perspective",
        "comments": "16 pages, 5 figures, 3 tables",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  As the first phase in the Business Process Management (BPM) lifecycle,\nprocess identification addresses the problem of identifying which processes to\nprioritize for improvement. Process selection plays a critical role in this\nphase, but it is a step with known pitfalls. Decision makers rely frequently on\nsubjective criteria, and their knowledge of the alternative processes put\nforward for selection is often inconsistent. This leads to poor quality\ndecision-making and wastes resources. In recent years, a rejection of a\none-size-fits-all approach to BPM in favor of a more context-aware approach has\ngained significant academic attention. In this study, the role of context in\nthe process selection step is considered. The context is qualitative,\nsubjective, sensitive to decision-making bias and politically charged. We\napplied a design-science approach and engaged industry decision makers through\na combination of research methods to assess how different configurations of\nprocess inputs influence and ultimately improve the quality of the process\nselection step. The study highlights the impact of framing effects on context\nand provides five guidelines to improve effectiveness.\n"
    },
    {
        "paper_id": 2304.05093,
        "authors": "Mohamed Hamdouche (LPSM), Pierre Henry-Labordere, Huy\\^en Pham (LPSM)",
        "title": "Generative modeling for time series via Schr{\\\"o}dinger bridge",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We propose a novel generative model for time series based on Schr{\\\"o}dinger\nbridge (SB) approach. This consists in the entropic interpolation via optimal\ntransport between a reference probability measure on path space and a target\nmeasure consistent with the joint data distribution of the time series. The\nsolution is characterized by a stochastic differential equation on finite\nhorizon with a path-dependent drift function, hence respecting the temporal\ndynamics of the time series distribution. We can estimate the drift function\nfrom data samples either by kernel regression methods or with LSTM neural\nnetworks, and the simulation of the SB diffusion yields new synthetic data\nsamples of the time series. The performance of our generative model is\nevaluated through a series of numerical experiments. First, we test with a toy\nautoregressive model, a GARCH Model, and the example of fractional Brownian\nmotion, and measure the accuracy of our algorithm with marginal and temporal\ndependencies metrics. Next, we use our SB generated synthetic samples for the\napplication to deep hedging on real-data sets. Finally, we illustrate the SB\napproach for generating sequence of images.\n"
    },
    {
        "paper_id": 2304.05115,
        "authors": "Jianfei Zhang and Mathieu Rosenbaum",
        "title": "Towards systematic intraday news screening: a liquidity-focused approach",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  News can convey bearish or bullish views on financial assets. Institutional\ninvestors need to evaluate automatically the implied news sentiment based on\ntextual data. Given the huge amount of news articles published each day, most\nof which are neutral, we present a systematic news screening method to identify\nthe ``true'' impactful ones, aiming for more effective development of news\nsentiment learning methods. Based on several liquidity-driven variables,\nincluding volatility, turnover, bid-ask spread, and book size, we associate\neach 5-min time bin to one of two specific liquidity modes. One represents the\n``calm'' state at which the market stays for most of the time and the other,\nfeatured with relatively higher levels of volatility and trading volume,\ndescribes the regime driven by some exogenous events. Then we focus on the\nmoments where the liquidity mode switches from the former to the latter and\nconsider the news articles published nearby impactful. We apply naive Bayes on\nthese filtered samples for news sentiment classification as an illustrative\nexample. We show that the screened dataset leads to more effective feature\ncapturing and thus superior performance on short-term asset return prediction\ncompared to the original dataset.\n"
    },
    {
        "paper_id": 2304.05251,
        "authors": "Sabrina Aufiero, Giordano De Marzo, Angelica Sbardella, Andrea\n  Zaccaria",
        "title": "Mapping job complexity and skills into wages",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  We use algorithmic and network-based tools to build and analyze the bipartite\nnetwork connecting jobs with the skills they require. We quantify and represent\nthe relatedness between jobs and skills by using statistically validated\nnetworks. Using the fitness and complexity algorithm, we compute a skill-based\ncomplexity of jobs. This quantity is positively correlated with the average\nsalary, abstraction, and non-routinarity level of jobs. Furthermore, coherent\njobs - defined as the ones requiring closely related skills - have, on average,\nlower wages. We find that salaries may not always reflect the intrinsic value\nof a job, but rather other wage-setting dynamics that may not be directly\nrelated to its skill composition. Our results provide valuable information for\npolicymakers, employers, and individuals to better understand the dynamics of\nthe labor market and make informed decisions about their careers.\n"
    },
    {
        "paper_id": 2304.0529,
        "authors": "Ambra Amico, Luca Verginer, Giona Casiraghi, Giacomo Vaccario, Frank\n  Schweitzer",
        "title": "Adapting to Disruptions: Flexibility as a Pillar of Supply Chain\n  Resilience",
        "comments": null,
        "journal-ref": null,
        "doi": "10.1126/sciadv.adj1194",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Supply chain disruptions cause shortages of raw material and products. To\nincrease resilience, i.e., the ability to cope with shocks, substituting goods\nin established supply chains can become an effective alternative to creating\nnew distribution links. We demonstrate its impact on supply deficits through a\ndetailed analysis of the US opioid distribution system. Reconstructing 40\nbillion empirical distribution paths, our data-driven model allows a unique\ninspection of policies that increase the substitution flexibility. Our approach\nenables policymakers to quantify the trade-off between increasing flexibility,\ni.e., reduced supply deficits, and increasing complexity of the supply chain,\nwhich could make it more expensive to operate.\n"
    },
    {
        "paper_id": 2304.05297,
        "authors": "Chendi Ni, Yuying Li, Peter A. Forsyth",
        "title": "Neural Network Approach to Portfolio Optimization with Leverage\n  Constraints:a Case Study on High Inflation Investment",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Motivated by the current global high inflation scenario, we aim to discover a\ndynamic multi-period allocation strategy to optimally outperform a passive\nbenchmark while adhering to a bounded leverage limit. To this end, we formulate\nan optimal control problem to outperform a benchmark portfolio throughout the\ninvestment horizon. Assuming the asset prices follow the jump-diffusion model\nduring high inflation periods, we first establish a closed-form solution for\nthe optimal strategy that outperforms a passive strategy under the cumulative\nquadratic tracking difference (CD) objective, assuming continuous trading and\nno bankruptcy. To obtain strategies under the bounded leverage constraint among\nother realistic constraints, we then propose a novel leverage-feasible neural\nnetwork (LFNN) to represent control, which converts the original constrained\noptimization problem into an unconstrained optimization problem that is\ncomputationally feasible with standard optimization methods. We establish\nmathematically that the LFNN approximation can yield a solution that is\narbitrarily close to the solution of the original optimal control problem with\nbounded leverage. We further apply the LFNN approach to a four-asset investment\nscenario with bootstrap resampled asset returns from the filtered high\ninflation regime data. The LFNN strategy is shown to consistently outperform\nthe passive benchmark strategy by about 200 bps (median annualized return),\nwith a greater than 90% probability of outperforming the benchmark at the end\nof the investment horizon.\n"
    },
    {
        "paper_id": 2304.05351,
        "authors": "Qianqian Xie, Weiguang Han, Yanzhao Lai, Min Peng, Jimin Huang",
        "title": "The Wall Street Neophyte: A Zero-Shot Analysis of ChatGPT Over\n  MultiModal Stock Movement Prediction Challenges",
        "comments": "13 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Recently, large language models (LLMs) like ChatGPT have demonstrated\nremarkable performance across a variety of natural language processing tasks.\nHowever, their effectiveness in the financial domain, specifically in\npredicting stock market movements, remains to be explored. In this paper, we\nconduct an extensive zero-shot analysis of ChatGPT's capabilities in multimodal\nstock movement prediction, on three tweets and historical stock price datasets.\nOur findings indicate that ChatGPT is a \"Wall Street Neophyte\" with limited\nsuccess in predicting stock movements, as it underperforms not only\nstate-of-the-art methods but also traditional methods like linear regression\nusing price features. Despite the potential of Chain-of-Thought prompting\nstrategies and the inclusion of tweets, ChatGPT's performance remains subpar.\nFurthermore, we observe limitations in its explainability and stability,\nsuggesting the need for more specialized training or fine-tuning. This research\nprovides insights into ChatGPT's capabilities and serves as a foundation for\nfuture work aimed at improving financial market analysis and prediction by\nleveraging social media sentiment and historical stock data.\n"
    },
    {
        "paper_id": 2304.05517,
        "authors": "M. Bel\\'en Arouxet, Aurelio F. Bariviera, Ver\\'onica Pastor, Victoria\n  Vampa",
        "title": "Time-frequency co-movements between commodities and economic policy\n  uncertainty across different crises",
        "comments": "17 pages, 8 figures, 2 tables",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
        "abstract": "  Commodity futures constitute an attractive asset class for portfolio\nmanagers. Propelled by their low correlation with other assets, commodities\nbegin gaining popularity among investors, as they allow to capture\ndiversification benefits. After more than two decades of active investing\nexperience, this paper examines the time and frequency of spillovers between\nEconomic Policy Uncertainty (Davis, 2016) and a broad set of commodities. The\nperiod under examination goes from December 1997 until April 2022, covering\npolitical, economic, and even health crises. We apply a wavelet coherence\nanalysis between time series, in order to shed light on the time-frequency\ncomovements and lead-lag relationships. This research finds a distinct impact\non the commodities, depending on the nature of the crisis. In particular,\nduring the global financial crisis and the Covid-19 crisis, comovements are\nstronger in most commodities.\n"
    },
    {
        "paper_id": 2304.05605,
        "authors": "Audrey Guo",
        "title": "Payroll Tax Incidence: Evidence from Unemployment Insurance",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Economic models assume that payroll tax burdens fall fully on workers, but\nwhere does tax incidence fall when taxes are firm-specific and time-varying?\nUnemployment insurance in the United States has the key feature of varying both\nacross employers and over time, creating the potential for labor demand\nresponses if tax costs cannot be fully passed on to worker wages. Using state\npolicy changes and matched employer-employee job spells from the LEHD, I study\nhow employment and earnings respond to payroll tax increases for highly exposed\nemployers. I find significant drops in employment growth driven by lower\nhiring, and minimal evidence of pass-through to earnings. The negative\nemployment effects are strongest for young and low-earning workers.\n"
    },
    {
        "paper_id": 2304.0576,
        "authors": "Hao-Ran Liu, Ming-Xia Li, and Wei-Xing Zhou (ECUST)",
        "title": "Visibility graph analysis of the grains and oilseeds indices",
        "comments": "16 pages, 9 figures",
        "journal-ref": "Physica A 650, 130004 (2024)",
        "doi": "10.1016/j.physa.2024.130004",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The Grains and Oilseeds Index (GOI) and its sub-indices of wheat, maize,\nsoyabeans, rice, and barley are daily price indexes reflect the price changes\nof the global spot markets of staple agro-food crops. In this paper, we carry\nout a visibility graph (VG) analysis of the GOI and its five sub-indices. Our\nfindings reveal that the degree distributions of the VGs, except for rice,\nexhibit exponentially truncated power-law tails, while the rice VG conforms to\na power-law tail. The average clustering coefficients of the six VGs are quite\nlarge ($>0.5$) and exhibit a nice power-law relation with respect to the\naverage degrees of the VGs. For each VG, the clustering coefficients of nodes\nare inversely proportional to their degrees for large degrees and are\ncorrelated to their degrees as a power law for small degrees. All the six VGs\nexhibit small-world characteristics. The degree-degree correlation coefficients\nshows that the VGs for maize and soyabeans indices exhibit weak assortative\nmixing patterns, while the other four VGs are weakly disassortative. The\naverage nearest neighbor degree functions have similar patterns, and each\nfunction shows a more complex mixing pattern which decreases for small degrees,\nincreases for mediate degrees, and decreases again for large degrees.\n"
    },
    {
        "paper_id": 2304.05794,
        "authors": "Luka Klin\\v{c}i\\'c, Vinko Zlati\\'c, Guido Caldarelli and Hrvoje\n  \\v{S}tefan\\v{c}i\\'c",
        "title": "Systemic risk measured by systems resiliency to initial shocks",
        "comments": "11 pages, 12 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The study of systemic risk is often presented through the analysis of several\nmeasures referring to quantities used by practitioners and policy makers.\nAlmost invariably, those measures evaluate the size of the impact that\nexogenous events can exhibit on a financial system without analysing the nature\nof initial shock. Here we present a symmetric approach and propose a set of\nmeasures that are based on the amount of exogenous shock that can be absorbed\nby the system before it starts to deteriorate. For this purpose, we use a\nlinearized version of DebtRank that allows to clearly show the onset of\nfinancial distress towards a correct systemic risk estimation. We show how we\ncan explicitly compute localized and uniform exogenous shocks and explained\ntheir behavior though spectral graph theory. We also extend analysis to\nheterogeneous shocks that have to be computed by means of Monte Carlo\nsimulations. We believe that our approach is more general and natural and\nallows to express in a standard way the failure risk in financial systems.\n"
    },
    {
        "paper_id": 2304.059,
        "authors": "Soumyadip Sarkar",
        "title": "Managing Portfolio for Maximizing Alpha and Minimizing Beta",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Portfolio management is an essential component of investment strategy that\naims to maximize returns while minimizing risk. This paper explores several\nportfolio management strategies, including asset allocation, diversification,\nactive management, and risk management, and their importance in optimizing\nportfolio performance. These strategies are examined individually and in\ncombination to demonstrate how they can help investors maximize alpha and\nminimize beta. Asset allocation is the process of dividing a portfolio among\ndifferent asset classes to achieve the desired level of risk and return.\nDiversification involves spreading investments across different securities and\nsectors to minimize the impact of individual security or sector-specific risks.\nActive management involves security selection and risk management techniques to\ngenerate excess returns while minimizing losses. Risk management strategies,\nsuch as stop-loss orders and options strategies, aim to minimize losses in\nadverse market conditions. The importance of combining these strategies for\noptimizing portfolio performance is emphasized in this paper. The proper\nimplementation of these strategies can help investors achieve their investment\ngoals over the long-term, while minimizing exposure to risks. A call to action\nfor investors to utilize portfolio management strategies to maximize alpha and\nminimize beta is also provided.\n"
    },
    {
        "paper_id": 2304.05935,
        "authors": "Md Shah Naoaj",
        "title": "Exploring the Determinants of Capital Adequacy in Commercial Banks: A\n  Study of Bangladesh's Banking Sector",
        "comments": "5 pages",
        "journal-ref": null,
        "doi": "10.24018/ejbmr.2023.8.2.1887",
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  This study investigates the factors that influence the capital adequacy of\ncommercial banks in Bangladesh using panel data from 28 banks over the period\nof 2013-2019. Three analytical methods, including the Fixed Effect model,\nRandom Effect model, and Pooled Ordinary Least Square (POLS) method, are\nemployed to analyze two versions of the capital adequacy ratio, namely the\nCapital Adequacy Ratio (CAR) and Tier 1 Capital Ratio. The study reveals that\ncapital adequacy is significantly affected by several independent variables,\nwith leverage and liquidity risk having a negative and positive relationship,\nrespectively. Additionally, the study finds a positive correlation between real\nGDP and net profit and capital adequacy, while inflation has a negative\ncorrelation. For the Tier 1 Ratio, the study shows no significant relationship\nbetweenleverage and liquidity risk, but a positive correlation with the number\nof employees, net profit, and real GDP, while a negative correlation with size\nand GDP deflator. Pooled OLS analysis reveals a negative correlation with\nleverage, size, and inflation for both CAR and Tier 1 Capital Ratio, and a\npositive correlation with liquidity risk, net profit, and real GDP. Based on\nthe Hausman test, the Random Effect model is deemed moresuitable for this\ndataset. These findings have important implications for policymakers,\ninvestors, and bank managers in Bangladesh by providing insights into the\nfactors that impact the capital ratios of commercial banks.\n"
    },
    {
        "paper_id": 2304.06037,
        "authors": "Soumyadip Sarkar",
        "title": "Quantitative Trading using Deep Q Learning",
        "comments": null,
        "journal-ref": null,
        "doi": "10.22214/ijraset.2023.50170",
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Reinforcement learning (RL) is a branch of machine learning that has been\nused in a variety of applications such as robotics, game playing, and\nautonomous systems. In recent years, there has been growing interest in\napplying RL to quantitative trading, where the goal is to make profitable\ntrades in financial markets. This paper explores the use of RL in quantitative\ntrading and presents a case study of a RL-based trading algorithm. The results\nshow that RL can be a powerful tool for quantitative trading, and that it has\nthe potential to outperform traditional trading algorithms. The use of\nreinforcement learning in quantitative trading represents a promising area of\nresearch that can potentially lead to the development of more sophisticated and\neffective trading systems. Future work could explore the use of alternative\nreinforcement learning algorithms, incorporate additional data sources, and\ntest the system on different asset classes. Overall, our research demonstrates\nthe potential of using reinforcement learning in quantitative trading and\nhighlights the importance of continued research and development in this area.\nBy developing more sophisticated and effective trading systems, we can\npotentially improve the efficiency of financial markets and generate greater\nreturns for investors.\n"
    },
    {
        "paper_id": 2304.0606,
        "authors": "A.H. Nzokem",
        "title": "European Option Pricing Under Generalized Tempered Stable Process:\n  Empirical Analysis",
        "comments": "12 page",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  The paper investigates the performance of the European option price when the\nlog asset price follows a rich class of Generalized Tempered Stable (GTS)\ndistribution. The GTS distribution is an alternative to Normal distribution and\n$\\alpha$-stable distribution for modeling asset return and many physical and\neconomic systems. The data used in the option pricing computation comes from\nfitting the GTS distribution to the underlying S\\&P 500 Index return\ndistribution. The Esscher transform method shows that the GTS distribution\npreserves its structure. The extended Black-Scholes formula and the Generalized\nBlack-Scholes Formula are applied in the study. The 12-point rule Composite\nNewton-Cotes Quadrature and the Fractional Fast Fourier (FRFT) algorithms were\nimplemented and they yield the same European option price at two decimal\nplaces. Compared to the option price under the GTS distribution, the\nBlack-Scholes (BS) model is underpriced for the near-the-Money (NTM) and the\nin-the-money (ITM) options. However, the BS model and GTS European options\nyield the same option price for the deep out-of-the-money (OTM) and the\ndeep-in-the-money (ITM) options.\n"
    },
    {
        "paper_id": 2304.06202,
        "authors": "Karen Grigorian, Robert Jarrow",
        "title": "Filtration Reduction and Completeness in Jump-Diffusion Models",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper studies the pricing and hedging of derivatives in frictionless and\ncompetitive, but incomplete jump-diffusion markets. A unique equivalent\nmartingale measure (EMM) is obtained using filtration reduction to a fictitious\ncomplete market. This unique EMM in the fictitious market is uplifted to the\noriginal economy using the notion of consistency. For pedagogical purposes, we\nbegin with simple setups and progressively extend to models of increasing\ngenerality.\n"
    },
    {
        "paper_id": 2304.06205,
        "authors": "Juan C. Perdomo and Tolani Britton and Moritz Hardt and Rediet Abebe",
        "title": "Difficult Lessons on Social Prediction from Wisconsin Public Schools",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  Early warning systems (EWS) are predictive tools at the center of recent\nefforts to improve graduation rates in public schools across the United States.\nThese systems assist in targeting interventions to individual students by\npredicting which students are at risk of dropping out. Despite significant\ninvestments in their widespread adoption, there remain large gaps in our\nunderstanding of the efficacy of EWS, and the role of statistical risk scores\nin education.\n  In this work, we draw on nearly a decade's worth of data from a system used\nthroughout Wisconsin to provide the first large-scale evaluation of the\nlong-term impact of EWS on graduation outcomes. We present empirical evidence\nthat the prediction system accurately sorts students by their dropout risk. We\nalso find that it may have caused a single-digit percentage increase in\ngraduation rates, though our empirical analyses cannot reliably rule out that\nthere has been no positive treatment effect.\n  Going beyond a retrospective evaluation of DEWS, we draw attention to a\ncentral question at the heart of the use of EWS: Are individual risk scores\nnecessary for effectively targeting interventions? We propose a simple\nmechanism that only uses information about students' environments -- such as\ntheir schools, and districts -- and argue that this mechanism can target\ninterventions just as efficiently as the individual risk score-based mechanism.\nOur argument holds even if individual predictions are highly accurate and\neffective interventions exist. In addition to motivating this simple targeting\nmechanism, our work provides a novel empirical backbone for the robust\nqualitative understanding among education researchers that dropout is\nstructurally determined. Combined, our insights call into question the marginal\nvalue of individual predictions in settings where outcomes are driven by high\nlevels of inequality.\n"
    },
    {
        "paper_id": 2304.06466,
        "authors": "Victor Olkhov",
        "title": "Market-Based \"Actual\" Returns of Investors",
        "comments": "16 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We describe how the market-based average and volatility of the \"actual\"\nreturn, which the investors gain within their market sales, depend on the\nstatistical moments, volatilities, and correlations of the current and past\nmarket trade values. We describe three successive approximations. First, we\nderive the dependence of the market-based average and volatility of a single\nsale return on market trade statistical moments determined by multiple\npurchases in the past. Then, we describe the dependence of average and\nvolatility of return that a single investor gains during the \"trading day.\"\nFinally, we derive the market-based average and volatility of return of\ndifferent investors during the \"trading day\" as a function of volatilities and\ncorrelations of market trade values. That highlights the distribution of the\n\"actual\" return of market trade and can serve as a benchmark for \"purchasing\"\ninvestors.\n"
    },
    {
        "paper_id": 2304.06484,
        "authors": "Howard Zhong, Mark Hamilton",
        "title": "Exploring Gender and Race Biases in the NFT Market",
        "comments": null,
        "journal-ref": null,
        "doi": "10.1016/j.frl.2023.103651",
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Non-Fungible Tokens (NFTs) are non-interchangeable assets, usually digital\nart, which are stored on the blockchain. Preliminary studies find that female\nand darker-skinned NFTs are valued less than their male and lighter-skinned\ncounterparts. However, these studies analyze only the CryptoPunks collection.\nWe test the statistical significance of race and gender biases in the prices of\nCryptoPunks and present the first study of gender bias in the broader NFT\nmarket. We find evidence of racial bias but not gender bias. Our work also\nintroduces a dataset of gender-labeled NFT collections to advance the broader\nstudy of social equity in this emerging market.\n"
    },
    {
        "paper_id": 2304.06859,
        "authors": "Peter B. Lerner",
        "title": "A Natural Copula",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-sa/4.0/",
        "abstract": "  Copulas are widely used in financial economics as well as in other areas of\napplied mathematics. Yet, there is much arbitrariness in their choice. The\nauthor proposes \"a natural copula\" concept, which minimizes Wasserstein\ndistance between distributions in some space, in which both these distributions\nare embedded. Transport properties and hydrodynamic interpretation are\ndiscussed with two examples of distributions of financial significance. A\nnatural copula can be parsimoniously estimated by the methods of linear\nprogramming.\n"
    },
    {
        "paper_id": 2304.06877,
        "authors": "Samuel W. Akingbade, Marian Gidea, Matteo Manzi, Vahid Nateghi",
        "title": "Why Topological Data Analysis Detects Financial Bubbles?",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  We present a heuristic argument for the propensity of Topological Data\nAnalysis (TDA) to detect early warning signals of critical transitions in\nfinancial time series. Our argument is based on the Log-Periodic Power Law\nSingularity (LPPLS) model, which characterizes financial bubbles as\nsuper-exponential growth (or decay) of an asset price superimposed with\noscillations increasing in frequency and decreasing in amplitude when\napproaching a critical transition (tipping point). We show that whenever the\nLPPLS model is fitting with the data, TDA generates early warning signals. As\nan application, we illustrate this approach on a sample of positive and\nnegative bubbles in the Bitcoin historical price.\n"
    },
    {
        "paper_id": 2304.06938,
        "authors": "Yunhong Li, Zuo Quan Xu, Xun Yu Zhou",
        "title": "Robust utility maximization with intractable claims",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We study a continuous-time expected utility maximization problem in which the\ninvestor at maturity receives the value of a contingent claim in addition to\nthe investment payoff from the financial market. The investor knows nothing\nabout the claim other than its probability distribution, hence an ``intractable\nclaim''. In view of the lack of necessary information about the claim, we\nconsider a robust formulation to maximize her utility in the worst scenario. We\napply the quantile formulation to solve the problem, expressing the quantile\nfunction of the optimal terminal investment income as the solution of certain\nvariational inequalities of ordinary differential equations and obtaining the\nresulting optimal trading strategy. In the case of an exponential utility, the\nproblem reduces to a (non-robust) rank--dependent utility maximization with\nprobability distortion whose solution is available in the literature. The\nresults can also be used to determine the utility indifference price of the\nintractable claim.\n"
    },
    {
        "paper_id": 2304.0695,
        "authors": "Julien Hambuckers, Marie Kratz, Antoine Usseglio-Carleve",
        "title": "Efficient Estimation in Extreme Value Regression Models of Hedge Fund\n  Tail Risks",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  We introduce a method to estimate simultaneously the tail and the threshold\nparameters of an extreme value regression model. This standard model finds its\nuse in finance to assess the effect of market variables on extreme loss\ndistributions of investment vehicles such as hedge funds. However, a major\nlimitation is the need to select ex ante a threshold below which data are\ndiscarded, leading to estimation inefficiencies. To solve these issues, we\nextend the tail regression model to non-tail observations with an auxiliary\nsplicing density, enabling the threshold to be selected automatically. We then\napply an artificial censoring mechanism of the likelihood contributions in the\nbulk of the data to decrease specification issues at the estimation stage. We\nillustrate the superiority of our approach for inference over classical\npeaks-over-threshold methods in a simulation study. Empirically, we investigate\nthe determinants of hedge fund tail risks over time, using pooled returns of\n1,484 hedge funds. We find a significant link between tail risks and factors\nsuch as equity momentum, financial stability index, and credit spreads.\nMoreover, sorting funds along exposure to our tail risk measure discriminates\nbetween high and low alpha funds, supporting the existence of a fear premium.\n"
    },
    {
        "paper_id": 2304.07019,
        "authors": "Jann Michael Weinand, Ganga Vandenberg, Stanley Risch, Johannes\n  Behrens, Noah Pflugradt, Jochen Lin{\\ss}en, Detlef Stolten",
        "title": "Low-carbon Lithium Extraction Makes Deep Geothermal Plants\n  Cost-competitive in Energy Systems",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Lithium is a critical material for the energy transition, but conventional\nprocurement methods have significant environmental impacts. In this study, we\nutilize regional energy system optimizations to investigate the techno-economic\npotential of the low-carbon alternative of direct lithium extraction in deep\ngeothermal plants. We show that geothermal plants will become cost-competitive\nin conjunction with lithium extraction, even under unfavorable conditions and\npartially displace photovoltaics, wind power, and storage from energy systems.\nOur analysis indicates that if 10% of municipalities in the Upper Rhine Graben\narea in Germany constructed deep geothermal plants, they could provide enough\nlithium to produce about 1.2 million electric vehicle battery packs per year,\nequivalent to 70% of today`s annual electric vehicle registrations in the\nEuropean Union. This approach could offer significant environmental benefits\nand has high potential for mass application also in other countries, such as\nthe United States, United Kingdom, France, and Italy, highlighting the\nimportance of further research and development of this technology.\n"
    },
    {
        "paper_id": 2304.07045,
        "authors": "Benoit Oriol and Alexandre Miot",
        "title": "Ledoit-Wolf linear shrinkage with unknown mean",
        "comments": "50 pages, 6 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This work addresses large dimensional covariance matrix estimation with\nunknown mean. The empirical covariance estimator fails when dimension and\nnumber of samples are proportional and tend to infinity, settings known as\nKolmogorov asymptotics. When the mean is known, Ledoit and Wolf (2004) proposed\na linear shrinkage estimator and proved its convergence under those\nasymptotics. To the best of our knowledge, no formal proof has been proposed\nwhen the mean is unknown. To address this issue, we propose a new estimator and\nprove its quadratic convergence under the Ledoit and Wolf assumptions. Finally,\nwe show empirically that it outperforms other standard estimators.\n"
    },
    {
        "paper_id": 2304.07108,
        "authors": "Masaaki Fujii, Masashi Sekine",
        "title": "Mean-field equilibrium price formation with exponential utility",
        "comments": "28 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
        "abstract": "  In this paper, using the mean-field game theory, we study a problem of\nequilibrium price formation among many investors with exponential utility in\nthe presence of liabilities unspanned by the security prices. The investors are\nheterogeneous in their initial wealth, risk-averseness parameter, as well as\nstochastic liability at the terminal time. We characterize the equilibrium\nrisk-premium process of the risky stocks in terms of the solution to a novel\nmean-field backward stochastic differential equation (BSDE), whose driver has\nquadratic growth both in the stochastic integrands and in their conditional\nexpectations. We prove the existence of a solution to the mean-field BSDE under\nseveral conditions and show that the resultant risk-premium process actually\nclears the market in the large population limit.\n"
    },
    {
        "paper_id": 2304.07377,
        "authors": "Nabil Kahale",
        "title": "Simulating Gaussian vectors via randomized dimension reduction and PCA",
        "comments": "9 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We study the problem of estimating E(g(X)), where g is a real-valued function\nof d variables and X is a d-dimensional Gaussian vector with a given covariance\nmatrix. We present a new unbiased estimator for E(g(X)) that combines the\nrandomized dimension reduction technique with principal components analysis.\nUnder suitable conditions, we prove that our algorithm outperforms the standard\nMonte Carlo method by a factor of order d.\n"
    },
    {
        "paper_id": 2304.07568,
        "authors": "O.A. Malafeyev, N.D.Redinskikh, V. F. Bogachev",
        "title": "Game theoretical models of geopolitical processes. Part I",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  In this paper, the interaction of geopolitical actors in the production and\nsale of military equipment is studied. In section 2 the production of military\nequipment is considered as the two person zero-sum game. In such game, the\nstrategies of the players are defined by the information state of the actors.\nThe optimal strategy of geopolitical actors is found. In section 3, the\nconflict process is considered, the optimal strategy is determined for each\ngeopolitical actor.\n"
    },
    {
        "paper_id": 2304.07619,
        "authors": "Alejandro Lopez-Lira and Yuehua Tang",
        "title": "Can ChatGPT Forecast Stock Price Movements? Return Predictability and\n  Large Language Models",
        "comments": "Previously posted in SSRN\n  https://papers.ssrn.com/sol3/papers.cfm?abstract_id=4412788",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  We examine the potential of ChatGPT and other large language models in\npredicting stock market returns using news headlines. We use ChatGPT to assess\nwhether each headline is good, bad, or neutral for firms' stock prices. We\ndocument a significantly positive correlation between ChatGPT scores and\nsubsequent daily stock returns. We find that ChatGPT outperforms traditional\nsentiment analysis methods. More basic models such as GPT-1, GPT-2, and BERT\ncannot accurately forecast returns, indicating return predictability is an\nemerging capacity of complex language models. Long-short strategies based on\nChatGPT-4 deliver the highest Sharpe ratio. Furthermore, we find predictability\nin both small and large stocks, suggesting market underreaction to company\nnews. Predictability is stronger among smaller stocks and stocks with bad news,\nconsistent with limits-to-arbitrage also playing an important role. Finally, we\npropose a new method to evaluate and understand the models' reasoning\ncapabilities. Overall, our results suggest that incorporating advanced language\nmodels into the investment decision-making process can yield more accurate\npredictions and enhance the performance of quantitative trading strategies.\n"
    },
    {
        "paper_id": 2304.07672,
        "authors": "Yingting Miao and Qiang Zhang",
        "title": "Optimal Investment and Consumption Strategies with General and Linear\n  Transaction Costs under CRRA Utility",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Transaction costs play a critical role in asset allocation and consumption\nstrategies in portfolio management. We apply the methods of dynamic programming\nand singular perturbation expansion to derive the closed-form leading solutions\nto this problem for small transaction costs with arbitrary transaction cost\nstructure by maximizing the expected CRRA (constant relative risk aversion)\nutility function for this problem. We also discuss in detail the case which\nconsists of both fixed and proportional transaction costs.\n"
    },
    {
        "paper_id": 2304.07835,
        "authors": "Asuna Gilfoyle",
        "title": "The Impact of Automation on Income Inequality: A Cross-Country Analysis",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-sa/4.0/",
        "abstract": "  This study examines the relationship between automation and income inequality\nacross different countries, taking into account the varying levels of\ntechnological adoption and labor market institutions. The research employs a\npanel data analysis using data from the World Bank, the International Labour\nOrganization, and other reputable sources. The findings suggest that while\nautomation leads to an increase in productivity, its effect on income\ninequality depends on the country's labor market institutions and social\npolicies.\n"
    },
    {
        "paper_id": 2304.07851,
        "authors": "Adit Vinod Nair, Adarsh Damani, Devansh Khandelwal, Harshita Sachdev,\n  Sreayans Jain",
        "title": "Study on the tea market in India",
        "comments": "14 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/publicdomain/zero/1.0/",
        "abstract": "  India's tea business has a long history and plays a significant role in the\neconomics of the nation. India is the world's second-largest producer of tea,\nwith Assam and Darjeeling being the most well-known tea-growing regions. Since\nthe British introduced tea cultivation to India in the 1820s, the nation has\nproduced tea. Millions of people are employed in the tea sector today, and it\ncontributes significantly to the Indian economy in terms of revenue. The\nproduction of tea has changed significantly in India over the years, moving\nmore and more towards organic and sustainable practices. The industry has also\nhad to deal with difficulties like competition from other nations that produce\ntea, varying tea prices, and labor-related problems. Despite these obstacles,\nthe Indian tea business is still growing and produces a wide variety of teas,\nsuch as black tea, green tea, and chai tea. Additionally, the sector encourages\ntravel through \"tea tourism,\" which allows tourists to see how tea is made and\ndiscover its origins in India. Overall, India's tea business continues to play\na significant role in its history, culture, and economy.\n"
    },
    {
        "paper_id": 2304.07975,
        "authors": "Oleg V. Pavlov and Jason M. Sardell",
        "title": "Economic Origins of the Sicilian Mafia: A Simulation Feedback Model",
        "comments": "In R. Y. Cavana, B. C. Dangerfield, O. V. Pavlov, M. J. Radzicki, &\n  I. D. Wheat (Eds.), Feedback Economics: Economic Modeling with System\n  Dynamics. Pp. 137-161. New York: Springer",
        "journal-ref": null,
        "doi": "10.1007/978-3-030-67190-7_6",
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  This chapter develops a feedback economic model that explains the rise of the\nSicilian mafia in the 19th century. Grounded in economic theory, the model\nincorporates causal relationships between the mafia activities, predation, law\nenforcement, and the profitability of local businesses. Using computational\nexperiments with the model, we explore how different factors and feedback\neffects impact the mafia activity levels. The model explains important\nhistorical observations such as the emergence of the mafia in wealthier regions\nand its absence in the poorer districts despite the greater levels of banditry.\n"
    },
    {
        "paper_id": 2304.08008,
        "authors": "Ali Lazrak and Jianfeng Zhang",
        "title": "Democratic Policy Decisions with Decentralized Promises Contingent on\n  Vote Outcome",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  We study pre-vote interactions in a committee that enacts a welfare-improving\nreform through voting. Committee members use decentralized promises contingent\non the reform enactment to influence the vote outcome. Equilibrium promises\nprevent beneficial coalitional deviations and minimize total promises. We show\nthat multiple equilibria exist, involving promises from high- to low-intensity\nmembers to enact the reform. Promises dissuade reform opponents from enticing\nthe least enthusiastic reform supporters to vote against the reform. We explore\nwhether some recipients of the promises can be supporters of the reform and\ndiscuss the impact of polarization on the total promises.\n"
    },
    {
        "paper_id": 2304.08049,
        "authors": "Francisco Estrada, Richard S.J. Tol, Wouter Botzen",
        "title": "Economic consequences of the spatial and temporal variability of climate\n  change",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Damage functions in integrated assessment models (IAMs) map changes in\nclimate to economic impacts and form the basis for most of estimates of the\nsocial cost of carbon. Implicit in these functions lies an unwarranted\nassumption that restricts the spatial variation (Svar) and temporal variability\n(Tvar) of changes in climate to be null. This could bias damage estimates and\nthe climate policy advice from IAMs. While the effects of Tvar have been\nstudied in the literature, those of Svar and their interactions with Tvar have\nnot. Here we present estimates of the economic costs of climate change that\naccount for both Tvar and Svar, as well as for the seasonality of damages\nacross sectors. Contrary to the results of recent studies which show little\neffect that of Tvar on expected losses, we reveal that ignoring Svar produces\nlarge downward biases, as warming is highly heterogeneous over space. Using a\nconservative calibration for the damage function, we show that previous\nestimates are biased downwards by about 23-36%, which represents additional\nlosses of about US$1,400-US$2,300 billion by 2050 and US$17-US$28 trillion by\nthe end of the century, under a high emissions scenario. The present value of\nlosses during the period 2020-2100 would be larger than reported in previous\nstudies by $47-$66 trillion or about 1/2 to 3/4 of annual global GDP in 2020.\nOur results imply that using global mean temperature change in IAMs as a\nsummary measure of warming is not adequate for estimating the costs of climate\nchange. Instead, IAMs should include a more complete description of climate\nconditions.\n"
    },
    {
        "paper_id": 2304.08217,
        "authors": "Ha Nguyen",
        "title": "Credit Risk and Financial Performance of Commercial Banks: Evidence from\n  Vietnam",
        "comments": "49 pages, 2 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Credit risk is a crucial topic in the field of financial stability,\nespecially at this time given the profound impact of the ongoing pandemic on\nthe world economy. This study provides insight into the impact of credit risk\non the financial performance of 26 commercial banks in Vietnam for the period\nfrom 2006 to 2016. The financial performance of commercial banks is measured by\nreturn on assets (ROA), return on equity (ROE), and Net interest margin (NIM);\ncredit risk is measured by the Non-performing loan ratio (NPLR); control\nvariables are measured by bank-specific characteristics, including bank size\n(SIZE), loan loss provision ratio (LLPR), and capital adequacy ratio (CAR), and\nmacroeconomic factors such as annual gross domestic product (GDP) growth and\nannual inflation rate (INF). The assumption tests show that models have\nautocorrelation, non-constant variance, and endogeneity. Hence, a dynamic\nDifference Generalized Method of Moments (dynamic Difference GMM) approach is\nemployed to thoroughly address these problems. The empirical results show that\nthe financial performance of commercial banks measured by ROE and NIM persists\nfrom one year to the next. Furthermore, SIZE and NPLR variables have a\nsignificant negative effect on ROA and ROE but not on NIM. There is no evidence\nfound in support of the LLPR and CAR variables on models. The effect of GDP\ngrowth is statistically significant and positive on ROA, ROE, and NIM, whereas\nthe INF is only found to have a significant positive impact on ROA and NIM.\n"
    },
    {
        "paper_id": 2304.0844,
        "authors": "Foued Sa\\^adaoui",
        "title": "Structured Multifractal Scaling of the Principal Cryptocurrencies:\n  Examination using a Self-Explainable Machine Learning",
        "comments": "Preprint",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Multifractal analysis is a forecasting technique used to study the scaling\nregularity properties of financial returns, to analyze the long-term memory and\npredictability of financial markets. In this paper, we propose a novel\nstructural detrended multifractal fluctuation analysis (S-MF-DFA) to\ninvestigate the efficiency of the main cryptocurrencies. The new methodology\ngeneralizes the conventional approach by allowing it to proceed on the\ndifferent fluctuation regimes previously determined using a change-points\ndetection test. In this framework, the characterization of the various\nexogenous factors influencing the scaling behavior is performed on the basis of\na single-factor model, thus creating a kind of self-explainable machine\nlearning for price forecasting. The proposal is tested on the daily data of the\nthree among the main cryptocurrencies in order to examine whether the digital\nmarket has experienced upheavals in recent years and whether this has in some\nways led to a structured multifractal behavior. The sampled period ranges from\nApril 2017 to December 2022. We especially detect common periods of local\nscaling for the three prices with a decreasing multifractality after 2018.\nComplementary tests on shuffled and surrogate data prove that the distribution,\nlinear correlation, and nonlinear structure also explain at some level the\nstructural multifractality. Finally, prediction experiments based on neural\nnetworks fed with multi-fractionally differentiated data show the interest of\nthis new self-explained algorithm, thus giving decision-makers and investors\nthe ability to use it for more accurate and interpretable forecasts.\n"
    },
    {
        "paper_id": 2304.0859,
        "authors": "Vittorio Astarita",
        "title": "Risks and opportunities in arbitrage and market-making in\n  blockchain-based currency markets. Part 1 : Risks",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  This study provides a practical introduction to high-frequency trading in\nblockchain-based currency markets. These types of markets have some specific\ncharacteristics that differentiate them from the stock markets, such as a large\nnumber of trading exchanges (centralized and decentralized), relative\nsimplicity in moving funds from one exchange to another, and the large number\nof new currencies that have very little liquidity. This study analyzes the\npossible risks that specifically characterize this type of trading operation,\nthe potential opportunities, and the algorithms that are mostly used, providing\ninformation that can be useful for practitioners who intend to operate in these\nmarkets by providing (and risking) liquidity.\n"
    },
    {
        "paper_id": 2304.08793,
        "authors": "Mark-Oliver Wolf, Tom Ewen, Ivica Turkalj",
        "title": "Quantum Architecture Search for Quantum Monte Carlo Integration via\n  Conditional Parameterized Circuits with Application to Finance",
        "comments": "10 pages, 12 figures, 2 algorithms",
        "journal-ref": "2023 IEEE International Conference on Quantum Computing and\n  Engineering (QCE23)",
        "doi": "10.1109/QCE57702.2023.00070",
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Classical Monte Carlo algorithms can theoretically be sped up on a quantum\ncomputer by employing amplitude estimation (AE). To realize this, an efficient\nimplementation of state-dependent functions is crucial. We develop a\nstraightforward approach based on pretraining parameterized quantum circuits,\nand show how they can be transformed into their conditional variant, making\nthem usable as a subroutine in an AE algorithm. To identify a suitable circuit,\nwe propose a genetic optimization approach that combines variable ansatzes and\ndata encoding. We apply our algorithm to the problem of pricing financial\nderivatives. At the expense of a costly pretraining process, this results in a\nquantum circuit implementing the derivatives' payoff function more efficiently\nthan previously existing quantum algorithms. In particular, we compare the\nperformance for European vanilla and basket options.\n"
    },
    {
        "paper_id": 2304.08819,
        "authors": "Zhuo Jin, Zuo Quan Xu, Bin Zou",
        "title": "Optimal moral-hazard-free reinsurance under extended distortion premium\n  principles",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We study an optimal reinsurance problem under a diffusion risk model for an\ninsurer who aims to minimize the probability of lifetime ruin. To rule out\nmoral hazard issues, we only consider moral-hazard-free reinsurance contracts\nby imposing the incentive compatibility constraint on indemnity functions. The\nreinsurance premium is calculated under an extended distortion premium\nprinciple, in which the distortion function is not necessarily concave. We\nfirst show that an optimal reinsurance contract always exists and then derive\ntwo sufficient and necessary conditions to characterize it. Due to the presence\nof the incentive compatibility constraint and the nonconcavity of the\ndistortion, the optimal contract is obtained as a solution to a double obstacle\nproblem. At last, we apply the general result to study three examples and\nobtain the optimal contract in (semi)closed form.\n"
    },
    {
        "paper_id": 2304.08883,
        "authors": "Daniel Oeltz and Jan Hamaekers and Kay F. Pilz",
        "title": "Parameterized Neural Networks for Finance",
        "comments": "24 pages, 17 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We discuss and analyze a neural network architecture, that enables learning a\nmodel class for a set of different data samples rather than just learning a\nsingle model for a specific data sample. In this sense, it may help to reduce\nthe overfitting problem, since, after learning the model class over a larger\ndata sample consisting of such different data sets, just a few parameters need\nto be adjusted for modeling a new, specific problem. After analyzing the method\ntheoretically and by regression examples for different one-dimensional\nproblems, we finally apply the approach to one of the standard problems asset\nmanagers and banks are facing: the calibration of spread curves. The presented\nresults clearly show the potential that lies within this method. Furthermore,\nthis application is of particular interest to financial practitioners, since\nnearly all asset managers and banks which are having solutions in place may\nneed to adapt or even change their current methodologies when ESG ratings\nadditionally affect the bond spreads.\n"
    },
    {
        "paper_id": 2304.08902,
        "authors": "Nick James and Max Menzies",
        "title": "Collective dynamics, diversification and optimal portfolio construction\n  for cryptocurrencies",
        "comments": "Accepted manuscript. Minor edits since v1. Equal contribution",
        "journal-ref": "Entropy 25(6), 931 (2023)",
        "doi": "10.3390/e25060931",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Since its conception, the cryptocurrency market has been frequently described\nas an immature market, characterized by significant swings in volatility and\noccasionally described as lacking rhyme or reason. There has been great\nspeculation as to what role it plays in a diversified portfolio. For instance,\nis cryptocurrency exposure an inflationary hedge or a speculative investment\nthat follows broad market sentiment with amplified beta? We have recently\nexplored similar questions with a clear focus on the equity market. There, our\nresearch revealed several noteworthy dynamics such as: an increase in the\nmarket's collective strength and uniformity during crises, greater\ndiversification benefits across equity sectors (rather than within them), and\nthe existence of a \"best value\" portfolio of equities. In essence, we can now\ncontrast any potential signatures of maturity we identify in the cryptocurrency\nmarket and contrast these with the substantially larger, older and better\nestablished equity market. This paper aims to investigate whether the\ncryptocurrency market has recently exhibited similar mathematical properties as\nthe equity market. Instead of relying on traditional portfolio theory, which is\ngrounded in the financial dynamics of equity securities, we adjust our\nexperimental focus to capture the presumed behavioral purchasing patterns of\nretail cryptocurrency investors. Our focus is on collective dynamics and\nportfolio diversification in the cryptocurrency market, and examining whether\npreviously established results in the equity market hold in the cryptocurrency\nmarket, and to what extent. Results reveal nuanced signatures of maturity\nrelated to the equity market, including the fact that correlations collectively\nspike around exchange collapses, and identify an ideal portfolio size and\nspread across different groups of cryptocurrencies.\n"
    },
    {
        "paper_id": 2304.0891,
        "authors": "S\\'ebastien Lleo and Wolfgang J. Runggaldier",
        "title": "On the Separation of Estimation and Control in Risk-Sensitive Investment\n  Problems under Incomplete Observation",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  A typical approach to tackle stochastic control problems with partial\nobservation is to separate the control and estimation tasks. However, it is\nwell known that this separation generally fails to deliver an actual optimal\nsolution for risk-sensitive control problems. This paper investigates the\nseparability of a general class of risk-sensitive investment management\nproblems when a finite-dimensional filter exists. We show that the\ncorresponding separated problem, where instead of the unobserved quantities,\none considers their conditional filter distribution given the observations, is\nstrictly equivalent to the original control problem. We widen the applicability\nof the so-called Modified Zakai Equation (MZE) for the study of the separated\nproblem and prove that the MZE simplifies to a PDE in our approach.\nFurthermore, we derive criteria for separability. We do not solve the separated\ncontrol problem but note that the existence of a finite-dimensional filter\nleads to a finite state space for the separated problem. Hence, the difficulty\nis equivalent to solving a complete observation risk-sensitive problem. Our\nresults have implications for existing risk-sensitive investment management\nmodels with partial observations in that they establish their separability.\nTheir implications for future research on new applications is mainly to provide\nconditions to ensure separability.\n"
    },
    {
        "paper_id": 2304.08957,
        "authors": "Christopher J. Smith, Alaa Al Khourdajie, Pu Yang, Doris Folini",
        "title": "Climate uncertainty impacts on optimal mitigation pathways and social\n  cost of carbon",
        "comments": "15 pages, 3 figures",
        "journal-ref": null,
        "doi": "10.1088/1748-9326/acedc6",
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Emissions pathways used in climate policy analysis are often derived from\nintegrated assessment models. However, such emissions pathways do not typically\ninclude climate feedbacks on socioeconomic systems and by extension do not\nconsider climate uncertainty in their construction. We use a well-known\ncost-benefit integrated assessment model, the Dynamic Integrated\nClimate-Economy (DICE) model, with its climate component replaced by the\nFinite-amplitude Impulse Response (FaIR) model (v2.1). The climate uncertainty\nin FaIR is sampled with an ensemble that is consistent with historically\nobserved climate and Intergovernmental Panel on Climate Change (IPCC) assessed\nranges of key climate variables such as equilibrium climate sensitivity. Three\nscenarios are produced: a pathway similar to the \"optimal welfare\" scenario of\nDICE that has similar warming outcomes to current policies, and pathways that\nlimit warming to \"well-below\" 2C and 1.5C with low overshoot, in line with\nParis Agreement long-term temperature goals. Climate uncertainty alone is\nresponsible for a factor of five variation (5-95% range) in the social cost of\ncarbon in the 1.5C scenario. CO2 emissions trajectories resulting from the\noptimal level of emissions abatement in all pathways are also sensitive to\nclimate uncertainty, with 2050 emissions ranging from -12 to +14 GtCO2/yr in\nthe 1.5C scenario. Equilibrium climate sensitivity and the strength of\npresent-day aerosol effective radiative forcing are strong determinants of\nsocial cost of carbon and mid-century CO2 emissions. This shows that narrowing\nclimate uncertainty leads to more refined estimates for the social cost of\ncarbon and provides more certainty about the optimal rate of emissions\nabatement. Including climate and climate uncertainty in integrated assessment\nmodel derived emissions scenarios would address a key missing feedback in\nscenario construction.\n"
    },
    {
        "paper_id": 2304.09339,
        "authors": "David H. Kreitmeir and Paul A. Raschky",
        "title": "The Unintended Consequences of Censoring Digital Technology -- Evidence\n  from Italy's ChatGPT Ban",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  We analyse the effects of the ban of ChatGPT, a generative pre-trained\ntransformer chatbot, on individual productivity. We first compile data on the\nhourly coding output of over 8,000 professional GitHub users in Italy and other\nEuropean countries to analyse the impact of the ban on individual productivity.\nCombining the high-frequency data with the sudden announcement of the ban in a\ndifference-in-differences framework, we find that the output of Italian\ndevelopers decreased by around 50% in the first two business days after the ban\nand recovered after that. Applying a synthetic control approach to daily Google\nsearch and Tor usage data shows that the ban led to a significant increase in\nthe use of censorship bypassing tools. Our findings show that users swiftly\nimplement strategies to bypass Internet restrictions but this adaptation\nactivity creates short-term disruptions and hampers productivity.\n"
    },
    {
        "paper_id": 2304.09551,
        "authors": "Benjamin Jourdain and Gudmund Pammer",
        "title": "An extension of martingale transport and stability in robust finance",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  While many questions in robust finance can be posed in the martingale optimal\ntransport framework or its weak extension, others like the subreplication price\nof VIX futures, the robust pricing of American options or the construction of\nshadow couplings necessitate additional information to be incorporated into the\noptimization problem beyond that of the underlying asset. In the present paper,\nwe take into account this extra information by introducing an additional\nparameter to the weak martingale optimal transport problem. We prove the\nstability of the resulting problem with respect to the risk neutral marginal\ndistributions of the underlying asset, thus extending the results in\n\\cite{BeJoMaPa21b}. A key step is the generalization of the main result in\n\\cite{BJMP22} to include the extra parameter into the setting. This result\nestablishes that any martingale coupling can be approximated by a sequence of\nmartingale couplings with specified marginals, provided that the marginals of\nthis sequence converge to those of the original coupling. Finally, we deduce\nstability of the three previously mentioned motivating examples.\n"
    },
    {
        "paper_id": 2304.0975,
        "authors": "Raj G. Patel, Tomas Dominguez, Mohammad Dib, Samuel Palmer, Andrea\n  Cadarso, Fernando De Lope Contreras, Abdelkader Ratnani, Francisco Gomez\n  Casanova, Senaida Hern\\'andez-Santana, \\'Alvaro D\\'iaz-Fern\\'andez, Eva\n  Andr\\'es, Jorge Luis-Hita, Escol\\'astico S\\'anchez-Mart\\'inez, Samuel Mugel,\n  Roman Orus",
        "title": "Application of Tensor Neural Networks to Pricing Bermudan Swaptions",
        "comments": "16 pages, 9 figures, 2 tables, minor changes",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The Cheyette model is a quasi-Gaussian volatility interest rate model widely\nused to price interest rate derivatives such as European and Bermudan Swaptions\nfor which Monte Carlo simulation has become the industry standard. In low\ndimensions, these approaches provide accurate and robust prices for European\nSwaptions but, even in this computationally simple setting, they are known to\nunderestimate the value of Bermudan Swaptions when using the state variables as\nregressors. This is mainly due to the use of a finite number of predetermined\nbasis functions in the regression. Moreover, in high-dimensional settings,\nthese approaches succumb to the Curse of Dimensionality. To address these\nissues, Deep-learning techniques have been used to solve the backward\nStochastic Differential Equation associated with the value process for European\nand Bermudan Swaptions; however, these methods are constrained by training time\nand memory. To overcome these limitations, we propose leveraging Tensor Neural\nNetworks as they can provide significant parameter savings while attaining the\nsame accuracy as classical Dense Neural Networks. In this paper we rigorously\nbenchmark the performance of Tensor Neural Networks and Dense Neural Networks\nfor pricing European and Bermudan Swaptions, and we show that Tensor Neural\nNetworks can be trained faster than Dense Neural Networks and provide more\naccurate and robust prices than their Dense counterparts.\n"
    },
    {
        "paper_id": 2304.09761,
        "authors": "Mayank Ratan Bhardwaj (1), Jaydeep Pawar (1), Abhijnya Bhat (2),\n  Deepanshu (1), Inavamsi Enaganti (1), Kartik Sagar (1), Y. Narahari (1) ((1)\n  Indian Institute of Science, (2) PES University)",
        "title": "An innovative Deep Learning Based Approach for Accurate Agricultural\n  Crop Price Prediction",
        "comments": "9 pages, 3 figures, 3 tables",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  Accurate prediction of agricultural crop prices is a crucial input for\ndecision-making by various stakeholders in agriculture: farmers, consumers,\nretailers, wholesalers, and the Government. These decisions have significant\nimplications including, most importantly, the economic well-being of the\nfarmers. In this paper, our objective is to accurately predict crop prices\nusing historical price information, climate conditions, soil type, location,\nand other key determinants of crop prices. This is a technically challenging\nproblem, which has been attempted before. In this paper, we propose an\ninnovative deep learning based approach to achieve increased accuracy in price\nprediction. The proposed approach uses graph neural networks (GNNs) in\nconjunction with a standard convolutional neural network (CNN) model to exploit\ngeospatial dependencies in prices. Our approach works well with noisy legacy\ndata and produces a performance that is at least 20% better than the results\navailable in the literature. We are able to predict prices up to 30 days ahead.\nWe choose two vegetables, potato (stable price behavior) and tomato (volatile\nprice behavior) and work with noisy public data available from Indian\nagricultural markets.\n"
    },
    {
        "paper_id": 2304.0984,
        "authors": "Adamantios Ntakaris, Moncef Gabbouj, Juho Kanniainen",
        "title": "Optimum Output Long Short-Term Memory Cell for High-Frequency Trading\n  Forecasting",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  High-frequency trading requires fast data processing without information lags\nfor precise stock price forecasting. This high-paced stock price forecasting is\nusually based on vectors that need to be treated as sequential and\ntime-independent signals due to the time irregularities that are inherent in\nhigh-frequency trading. A well-documented and tested method that considers\nthese time-irregularities is a type of recurrent neural network, named long\nshort-term memory neural network. This type of neural network is formed based\non cells that perform sequential and stale calculations via gates and states\nwithout knowing whether their order, within the cell, is optimal. In this\npaper, we propose a revised and real-time adjusted long short-term memory cell\nthat selects the best gate or state as its final output. Our cell is running\nunder a shallow topology, has a minimal look-back period, and is trained\nonline. This revised cell achieves lower forecasting error compared to other\nrecurrent neural networks for online high-frequency trading forecasting tasks\nsuch as the limit order book mid-price prediction as it has been tested on two\nhigh-liquid US and two less-liquid Nordic stocks.\n"
    },
    {
        "paper_id": 2304.09936,
        "authors": "Aayush Shah, Mann Doshi, Meet Parekh, Nirmit Deliwala, Prof. Pramila\n  M. Chawan",
        "title": "Identifying Trades Using Technical Analysis and ML/DL Models",
        "comments": "14 pages, 9 figures, 5 tables",
        "journal-ref": "Volume 11, Issue 4, April 2023",
        "doi": "10.15680/IJIRCCE.2023.1104038",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The importance of predicting stock market prices cannot be overstated. It is\na pivotal task for investors and financial institutions as it enables them to\nmake informed investment decisions, manage risks, and ensure the stability of\nthe financial system. Accurate stock market predictions can help investors\nmaximize their returns and minimize their losses, while financial institutions\ncan use this information to develop effective risk management policies.\nHowever, stock market prediction is a challenging task due to the complex\nnature of the stock market and the multitude of factors that can affect stock\nprices. As a result, advanced technologies such as deep learning are being\nincreasingly utilized to analyze vast amounts of data and provide valuable\ninsights into the behavior of the stock market. While deep learning has shown\npromise in accurately predicting stock prices, there is still much research to\nbe done in this area.\n"
    },
    {
        "paper_id": 2304.09937,
        "authors": "Li Rong Wang, Hsuan Fu, Xiuyi Fan",
        "title": "Stock Price Predictability and the Business Cycle via Machine Learning",
        "comments": "14 pages, 5 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We study the impacts of business cycles on machine learning (ML) predictions.\nUsing the S&P 500 index, we find that ML models perform worse during most\nrecessions, and the inclusion of recession history or the risk-free rate does\nnot necessarily improve their performance. Investigating recessions where\nmodels perform well, we find that they exhibit lower market volatility than\nother recessions. This implies that the improved performance is not due to the\nmerit of ML methods but rather factors such as effective monetary policies that\nstabilized the market. We recommend that ML practitioners evaluate their models\nduring both recessions and expansions.\n"
    },
    {
        "paper_id": 2304.09939,
        "authors": "Jevgeni Tarassov and Nicolas Houli\\'e",
        "title": "Bitcoin: A life in crises",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  In this study, we investigate the BTC price time-series (17 August 2010-27\nJune 2021) and show that the 2017 pricing episode is not unique. We describe at\nleast ten new events, which occurred since 2010-2011 and span more than five\norders of price magnitudes ($US 1-$US 60k). We find that those events have a\nsimilar duration of approx. 50-100 days. Although we are not able to predict\ntimes of a price peak, we however succeed to approximate the BTC price\nevolution using a function that is similar to a Fibonacci sequence. Finally, we\ncomplete a comparison with other types of financial instruments (equities,\ncurrencies, gold) which suggests that BTC may be classified as an illiquid\nasset.\n"
    },
    {
        "paper_id": 2304.09947,
        "authors": "Jiaju Miao and Pawel Polak",
        "title": "Online Ensemble of Models for Optimal Predictive Performance with\n  Applications to Sector Rotation Strategy",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Asset-specific factors are commonly used to forecast financial returns and\nquantify asset-specific risk premia. Using various machine learning models, we\ndemonstrate that the information contained in these factors leads to even\nlarger economic gains in terms of forecasts of sector returns and the\nmeasurement of sector-specific risk premia. To capitalize on the strong\npredictive results of individual models for the performance of different\nsectors, we develop a novel online ensemble algorithm that learns to optimize\npredictive performance. The algorithm continuously adapts over time to\ndetermine the optimal combination of individual models by solely analyzing\ntheir most recent prediction performance. This makes it particularly suited for\ntime series problems, rolling window backtesting procedures, and systems of\npotentially black-box models. We derive the optimal gain function, express the\ncorresponding regret bounds in terms of the out-of-sample R-squared measure,\nand derive optimal learning rate for the algorithm. Empirically, the new\nensemble outperforms both individual machine learning models and their simple\naverages in providing better measurements of sector risk premia. Moreover, it\nallows for performance attribution of different factors across various sectors,\nwithout conditioning on a specific model. Finally, by utilizing monthly\npredictions from our ensemble, we develop a sector rotation strategy that\nsignificantly outperforms the market. The strategy remains robust against\nvarious financial factors, periods of financial distress, and conservative\ntransaction costs. Notably, the strategy's efficacy persists over time,\nexhibiting consistent improvement throughout an extended backtesting period and\nyielding substantial profits during the economic turbulence of the COVID-19\npandemic.\n"
    },
    {
        "paper_id": 2304.09971,
        "authors": "Dave Costenaro",
        "title": "The Pie: How Has Human Evolution Distributed Non-Financial Wealth?",
        "comments": "14 pages. All data and code is open source and available on GitHub,\n  linked at end of paper",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Income and wealth allocation are foundational components of how economies\noperate. These are complex distributions, and it is hard to get a real sense\nfor their dynamics using simplifications like average or median. One metric\nthat characterizes such distributions better is the Gini Index, which on one\nextreme is 0, a completely equitable distribution, and on the other extreme is\n1, the most inequitable, where a single individual has all the resources. Most\nexperts agree that viable economies cannot exist at either extreme, but\nidentifying a preferred range has historically been a matter of conflicting\npolitical philosophies and emotional appeals. This research explores instead\nwhether there might be a theoretical and empirical basis for a preferred Gini\nIndex. Specifically, I explore a simple question: Before financial systems\nexisted, how were natural assets allocated? Intrinsic human attributes such as\nheight, strength, & beauty were the original measures of value in which human\nsocial groups traded. Each of these attributes is distributed in a diverse and\ncharacteristic way, which I propose has gradually established the acceptable\nbounds of inequality through evolutionary psychology. I collect data for a wide\narray of such traits and calculate a Gini Index for them in a novel way\nassuming their magnitudes to be analogous to levels of financial wealth. The\nvalues fall into a surprisingly intuitive pattern ranging from Gini=0.02 to\n0.51. Income distributions in many countries are within the top half of this\nrange after taxes and transfers are applied (0.2 to 0.4). Wealth distributions\non the other hand are mostly outside of this range, with the United States at a\nvery inequitable 0.82. Additional research is needed on the interconnections\nbetween this range of \"natural\" Gini Indexes and human contentment; and whether\nany explicit policy goals should be considered to target them.\n"
    },
    {
        "paper_id": 2304.10027,
        "authors": "Justin Hastings, David Ubilava",
        "title": "Agricultural Roots of Social Conflict in Southeast Asia",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  We examine whether harvest-time transitory shifts in employment and income\nlead to changes in political violence and social unrest in rice-producing\ncroplands of Southeast Asia. Using monthly data from 2010 to 2023 on over\n86,000 incidents covering 376 one-degree cells across eight Southeast Asian\ncountries, we estimate a general increase in political violence and a decrease\nin social unrest in croplands with rice production during the harvest season\nrelative to the rest of the crop year. In a finding that is least sensitive to\nalternative model specifications and data subsetting, we estimate a nine\npercent increase in violence against civilians in locations with considerable\nrice production compared to other parts of the region during the harvest\nseason, relative to the rest of the year. We show that the harvest-time changes\nin conflict are most evident in rural cells with rainfed agriculture. Using\nlocation-specific annual variation in growing season rainfall, we then show\nthat the harvest-time increase in violence against civilians occurs in\npresumably good harvest years, whereas increase in battles between actors of\npolitical violence follows growing seasons with scarce rainfall. The\nharvest-time decrease in social unrest, protests in particular, occurs after\npresumably bad harvest years. These findings contribute to research on the\nagroclimatic and economic roots of conflict and offer insights to policymakers\nby suggesting the spatiotemporal concentration of conflict as well as diverging\neffects by forms of conflict at harvest time in the rice-producing regions of\nSoutheast Asia.\n"
    },
    {
        "paper_id": 2304.10203,
        "authors": "Tom Savage, Antonio del Rio Chanona, Gbemi Oluleye",
        "title": "Robust Market Potential Assessment: Designing optimal policies for\n  low-carbon technology adoption in an increasingly uncertain world",
        "comments": "39 pages, 13 figures, submitted to Applied Energy",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Increasing the adoption of alternative technologies is vital to ensure a\nsuccessful transition to net-zero emissions in the manufacturing sector. Yet\nthere is no model to analyse technology adoption and the impact of policy\ninterventions in generating sufficient demand to reduce cost. Such a model is\nvital for assessing policy-instruments for the implementation of future energy\nscenarios. The design of successful policies for technology uptake becomes\nincreasingly difficult when associated market forces/factors are uncertain,\nsuch as energy prices or technology efficiencies. In this paper we formulate a\nnovel robust market potential assessment problem under uncertainty, resulting\nin policies that are immune to uncertain factors. We demonstrate two case\nstudies: the potential use of carbon capture and storage for iron and steel\nproduction across the EU, and the transition to hydrogen from natural gas in\nsteam boilers across the chemicals industry in the UK. Each robust optimisation\nproblem is solved using an iterative cutting planes algorithm which enables\nexisting models to be solved under uncertainty. By taking advantage of\nparallelisation we are able to solve the nonlinear robust market assessment\nproblem for technology adoption in times within the same order of magnitude as\nthe nominal problem. Policy makers often wish to trade-off certainty with\neffectiveness of a solution. Therefore, we apply an approximation to chance\nconstraints, varying the amount of uncertainty to locate less certain but more\neffective solutions. Our results demonstrate the possibility of locating robust\npolicies for the implementation of low-carbon technologies, as well as\nproviding direct insights for policy-makers into the decrease in policy\neffectiveness resulting from increasing robustness. The approach we present is\nextensible to a large number of policy design and alternative technology\nadoption problems.\n"
    },
    {
        "paper_id": 2304.10212,
        "authors": "Michael Kopp",
        "title": "The impact of the AI revolution on asset management",
        "comments": "6 pages, 1 figure",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
        "abstract": "  Recent progress in deep learning, a special form of machine learning, has led\nto remarkable capabilities machines can now be endowed with: they can read and\nunderstand free flowing text, reason and bargain with human counterparts,\ntranslate texts between languages, learn how to take decisions to maximize\ncertain outcomes, etc. Today, machines have revolutionized the detection of\ncancer, the prediction of protein structures, the design of drugs, the control\nof nuclear fusion reactors etc. Although these capabilities are still in their\ninfancy, it seems clear that their continued refinement and application will\nresult in a technological impact on nearly all social and economic areas of\nhuman activity, the likes of which we have not seen before. In this article, I\nwill share my view as to how AI will likely impact asset management in general\nand I will provide a mental framework that will equip readers with a simple\ncriterion to assess whether and to what degree a given fund really exploits\ndeep learning and whether a large disruption risk from deep learning exist.\n"
    },
    {
        "paper_id": 2304.10344,
        "authors": "Matteo Basei, Giorgio Ferrari, Neofytos Rodosthenous",
        "title": "Uncertainty over Uncertainty in Environmental Policy Adoption: Bayesian\n  Learning of Unpredictable Socioeconomic Costs",
        "comments": "31 pages; produced new economic results on the value of waiting and\n  improved presentation. Accepted for publication on \"Journal of Economic\n  Dynamics and Control\"",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The socioeconomic impact of pollution naturally comes with uncertainty due\nto, e.g., current new technological developments in emissions' abatement or\ndemographic changes. On top of that, the trend of the future costs of the\nenvironmental damage is unknown: Will global warming dominate or technological\nadvancements prevail? The truth is that we do not know which scenario will be\nrealised and the scientific debate is still open. This paper captures those two\nlayers of uncertainty by developing a real-options-like model in which a\ndecision maker aims at adopting a once-and-for-all costly reduction in the\ncurrent emissions rate, when the stochastic dynamics of the socioeconomic costs\nof pollution are subject to Brownian shocks and the drift is an unobservable\nrandom variable. By keeping track of the actual evolution of the costs, the\ndecision maker is able to learn the unknown drift and to form a posterior\ndynamic belief of its true value. The resulting decision maker's timing problem\nboils down to a truly two-dimensional optimal stopping problem which we address\nvia probabilistic free-boundary methods and a state-space transformation. We\ncompletely characterise the solution by showing that the optimal timing for\nimplementing the emissions reduction policy is the first time that the learning\nprocess has become ``decisive'' enough; that is, when it exceeds a\ntime-dependent percentage. This is given in terms of an endogenously determined\nthreshold function, which solves uniquely a nonlinear integral equation. We\nnumerically illustrate our results, discuss the implications of the optimal\npolicy and also perform comparative statics to understand the role of the\nrelevant model's parameters in the optimal policy.\n"
    },
    {
        "paper_id": 2304.10349,
        "authors": "Yannick Hoga",
        "title": "The Estimation Risk in Extreme Systemic Risk Forecasts",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Systemic risk measures have been shown to be predictive of financial crises\nand declines in real activity. Thus, forecasting them is of major importance in\nfinance and economics. In this paper, we propose a new forecasting method for\nsystemic risk as measured by the marginal expected shortfall (MES). It is based\non first de-volatilizing the observations and, then, calculating systemic risk\nfor the residuals using an estimator based on extreme value theory. We show the\nvalidity of the method by establishing the asymptotic normality of the MES\nforecasts. The good finite-sample coverage of the implied MES forecast\nintervals is confirmed in simulations. An empirical application to major US\nbanks illustrates the significant time variation in the precision of MES\nforecasts, and explores the implications of this fact from a regulatory\nperspective.\n"
    },
    {
        "paper_id": 2304.10382,
        "authors": "Salvatore Certo, Anh Pham, Nicolas Robles, Andrew Vlasic",
        "title": "Conditional Generative Models for Learning Stochastic Processes",
        "comments": null,
        "journal-ref": "Quantum Machine Intelligence 2023",
        "doi": "10.1007/s42484-023-00129-w",
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  A framework to learn a multi-modal distribution is proposed, denoted as the\nConditional Quantum Generative Adversarial Network (C-qGAN). The neural network\nstructure is strictly within a quantum circuit and, as a consequence, is shown\nto represent a more efficient state preparation procedure than current methods.\nThis methodology has the potential to speed-up algorithms, such as Monte Carlo\nanalysis. In particular, after demonstrating the effectiveness of the network\nin the learning task, the technique is applied to price Asian option\nderivatives, providing the foundation for further research on other\npath-dependent options.\n"
    },
    {
        "paper_id": 2304.1049,
        "authors": "Seyed Mojtaba Hosseini Bamakan, Nasim Nezhadsistani, Omid Bodaghi and\n  Qiang Qu",
        "title": "Patents and intellectual property assets as non-fungible tokens: key\n  technologies and challenges",
        "comments": null,
        "journal-ref": null,
        "doi": "10.1038/s41598-022-05920-6",
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  With the explosive development of decentralized finance, we witness a\nphenomenal growth in tokenization of all kinds of assets, including equity,\nfunds, debt, and real estate. By taking advantage of blockchain technology,\ndigital assets are broadly grouped into fungible and non-fungible tokens (NFT).\nHere non-fungible tokens refer to those with unique and non-substitutable\nproperties. NFT has widely attracted attention, and its protocols, standards,\nand applications are developing exponentially. It has been successfully applied\nto digital fantasy artwork, games, collectibles, etc. However, there is a lack\nof research in utilizing NFT in issues such as Intellectual Property. Applying\nfor a patent and trademark is not only a time-consuming and lengthy process but\nalso costly. NFT has considerable potential in the intellectual property\ndomain. It can promote transparency and liquidity and open the market to\ninnovators who aim to commercialize their inventions efficiently. The main\nobjective of this paper is to examine the requirements of presenting\nintellectual property assets, specifically patents, as NFTs. Hence, we offer a\nlayered conceptual NFT-based patent framework. Furthermore, a series of open\nchallenges about NFT-based patents and the possible future directions are\nhighlighted. The proposed framework provides fundamental elements and guidance\nfor businesses in taking advantage of NFTs in real-world problems such as grant\npatents, funding, biotechnology, and so forth.\n"
    },
    {
        "paper_id": 2304.10636,
        "authors": "Joppe de Ree, Matthijs Oosterveen and Dinand Webbink",
        "title": "The quality of school track assignment decisions by teachers",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We study the quality of secondary school track assignment decisions in the\nNetherlands, using a regression discontinuity design. In 6th grade, primary\nschool teachers assign each student to a secondary school track. If a student\nscores above a track-specific cutoff on the standardized end-of-primary\neducation test, the teacher can upwardly revise this assignment. By comparing\nstudents just left and right of these cutoffs, we find that between 50-90% of\nthe students are \"trapped in track\": these students are on the high track after\nfour years, only if they started on the high track in first year. The remaining\n(minority of) students are \"always low\": they are always on the low track after\nfour years, independently of where they started. These proportions hold for\nstudents near the cutoffs that shift from the low to the high track in first\nyear by scoring above the cutoff. Hence, for a majority of these students the\ninitial (unrevised) track assignment decision is too low. The results replicate\nacross most of the secondary school tracks, from the vocational to the academic\ntracks, and stand out against an education system with a lot of upward and\ndownward track mobility.\n"
    },
    {
        "paper_id": 2304.1074,
        "authors": "Mahsa Tavakoli, Rohitash Chandra, Fengrui Tian, Cristi\\'an Bravo",
        "title": "Multi-Modal Deep Learning for Credit Rating Prediction Using Text and\n  Numerical Data Streams",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  Knowing which factors are significant in credit rating assignment leads to\nbetter decision-making. However, the focus of the literature thus far has been\nmostly on structured data, and fewer studies have addressed unstructured or\nmulti-modal datasets. In this paper, we present an analysis of the most\neffective architectures for the fusion of deep learning models for the\nprediction of company credit rating classes, by using structured and\nunstructured datasets of different types. In these models, we tested different\ncombinations of fusion strategies with different deep learning models,\nincluding CNN, LSTM, GRU, and BERT. We studied data fusion strategies in terms\nof level (including early and intermediate fusion) and techniques (including\nconcatenation and cross-attention). Our results show that a CNN-based\nmulti-modal model with two fusion strategies outperformed other multi-modal\ntechniques. In addition, by comparing simple architectures with more complex\nones, we found that more sophisticated deep learning models do not necessarily\nproduce the highest performance; however, if attention-based models are\nproducing the best results, cross-attention is necessary as a fusion strategy.\nFinally, our comparison of rating agencies on short-, medium-, and long-term\nperformance shows that Moody's credit ratings outperform those of other\nagencies like Standard & Poor's and Fitch Ratings.\n"
    },
    {
        "paper_id": 2304.10776,
        "authors": "Massimo Finocchiaro Castroa, Calogero Guccio and Ilde Rizzo",
        "title": "How 'one-size-fits-all' public works contract does it better? An\n  assessment of infrastructure provision in Italy",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Public infrastructure procurement is crucial as a prerequisite for public and\nprivate investments and for economic and social capital growth. However, low\nperformance in execution severely hinders infrastructure provision and benefits\ndelivery. One of the most sensitive phases in public infrastructure procurement\nis the design because of the strategic relationship that it potentially creates\nbetween procurers and contractors in the execution stage, affecting the costs\nand the duration of the contract. In this paper, using recent developments in\nnon-parametric frontiers and propensity score matching, we evaluate the\nperformance in the execution of public works in Italy. The analysis provides\nrobust evidence of significant improvement of performance where procurers opt\nfor a design and build contracts, which lead to lower transaction costs,\nallowing contractors to better accommodate the project in the execution. Our\nfindings bear considerable policy implications.\n"
    },
    {
        "paper_id": 2304.10802,
        "authors": "Lijun Bo, Yijie Huang, Xiang Yu",
        "title": "An extended Merton problem with relaxed benchmark tracking",
        "comments": "Keywords: Benchmark tracking, capital injection, expected largest\n  shortfall, consumption and portfolio choice, Neumann boundary condition",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper studies a Merton's optimal consumption problem in an extended\nformulation incorporating the tracking of a benchmark process described by a\ngeometric Brownian motion. We consider a relaxed tracking formulation such that\nthe wealth process compensated by a fictitious capital injection outperforms\nthe benchmark at all times. The fund manager aims to maximize the expected\nutility of consumption deducted by the cost of the capital injection, where the\nlatter term can also be regarded as the expected largest shortfall of the\nwealth with reference to the benchmark. By introducing an auxiliary state\nprocess with reflection, we formulate and tackle an equivalent stochastic\ncontrol problem by means of the dual transform and probabilistic\nrepresentation, where the dual PDE can be solved explicitly. On the strength of\nthe closed-form results, we can derive and verify the optimal feedback control\nfor the primal control problem, allowing us to discuss some new and interesting\nfinancial implications induced by the additional risk-taking from the capital\ninjection and the goal of tracking.\n"
    },
    {
        "paper_id": 2304.1101,
        "authors": "Alan Guo",
        "title": "Invariance properties of maximal extractable value",
        "comments": "39 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  We develop a formalism for reasoning about trading on decentralized exchanges\non blockchains and a formulation of a particular form of maximal extractable\nvalue (MEV) that represents the total arbitrage opportunity extractable from\non-chain liquidity. We use this formalism to prove that for blockchains with\ndeterministic block times whose liquidity pools satisfy some natural properties\nthat are satisfied by pools in practice, this form of MEV is invariant under\nchanges to the ordering mechanism of the blockchain and distribution of block\ntimes. We do this by characterizing the MEV as the profit of a particularly\nsimple arbitrage strategy when left uncontested. These results can inform\ndesign of blockchain protocols by ruling out designs aiming to increase trading\nopportunity by changing the ordering mechanism or shortening block times.\n"
    },
    {
        "paper_id": 2304.11043,
        "authors": "Jiezhu Cheng, Kaizhu Huang, Zibin Zheng",
        "title": "Can Perturbations Help Reduce Investment Risks? Risk-Aware Stock\n  Recommendation via Split Variational Adversarial Training",
        "comments": "Accepted by TOIS",
        "journal-ref": null,
        "doi": "10.1145/3643131",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In the stock market, a successful investment requires a good balance between\nprofits and risks. Based on the learning to rank paradigm, stock recommendation\nhas been widely studied in quantitative finance to recommend stocks with higher\nreturn ratios for investors. Despite the efforts to make profits, many existing\nrecommendation approaches still have some limitations in risk control, which\nmay lead to intolerable paper losses in practical stock investing. To\neffectively reduce risks, we draw inspiration from adversarial learning and\npropose a novel Split Variational Adversarial Training (SVAT) method for\nrisk-aware stock recommendation. Essentially, SVAT encourages the stock model\nto be sensitive to adversarial perturbations of risky stock examples and\nenhances the model's risk awareness by learning from perturbations. To generate\nrepresentative adversarial examples as risk indicators, we devise a variational\nperturbation generator to model diverse risk factors. Particularly, the\nvariational architecture enables our method to provide a rough risk\nquantification for investors, showing an additional advantage of\ninterpretability. Experiments on several real-world stock market datasets\ndemonstrate the superiority of our SVAT method. By lowering the volatility of\nthe stock recommendation model, SVAT effectively reduces investment risks and\noutperforms state-of-the-art baselines by more than 30% in terms of\nrisk-adjusted profits. All the experimental data and source code are available\nat\nhttps://drive.google.com/drive/folders/14AdM7WENEvIp5x5bV3zV_i4Aev21C9g6?usp=sharing.\n"
    },
    {
        "paper_id": 2304.11415,
        "authors": "In\\'acio B\\'o and Li Chen and Rustamdjan Hakimov",
        "title": "Strategic Responses to Personalized Pricing and Demand for Privacy: An\n  Experiment",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We consider situations where consumers are aware that a statistical model\ndetermines the price of a product based on their observed behavior. Using a\nnovel experiment varying the context similarity between participant data and a\nproduct, we find that participants manipulate their responses to a survey about\npersonal characteristics, and manipulation is more successful when the contexts\nare similar. Moreover, participants demand less privacy, and make less optimal\nprivacy choices when the contexts are less similar. Our findings highlight the\nimportance of data privacy policies in the age of big data, where behavior in\nseemingly unrelated contexts might affect prices.\n"
    },
    {
        "paper_id": 2304.11518,
        "authors": "Junjie Zhao",
        "title": "Breaking the general election effect. The impact of the 2020 US\n  presidential election on Chinese economy and counter strategies",
        "comments": "PKU-NUS Annual International Conference on Quantitative Finance and\n  Economics",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  The study of US-China relations has always been a crucial topic in our\neconomic development [4][5][7], and the US presidential election plays an\nintegral role in shaping these relations. The presidential election is held\nevery four years, and it is crucial to assess the impact of the 2020 election\non China to prepare for the potential effects of the 2024 US presidential\nelection on the Chinese economy [8][16][20]. To achieve this, we have gathered\nstatistical data from nearly 70 years and analyzed data related to the US\neconomy. We have classified the collected data and utilized the analytic\nhierarchy process [1][2][3] to evaluate the President's policy\nimplementation.This approach allowed us to obtain a comprehensive ranking of\nthe indicators [6][9][11][33]. We then quantified the index data and employed\nthe entropy weight method to calculate the weight of each index data. Finally,\nwe used the weighted total score calculation to evaluate the economic status of\nthe United States in a hierarchical manner after the election of Presidents\nTrump and Biden [15][18]. We optimized the index system by incorporating\nadditional dimension indexes such as \"foreign policy\". We then crawled China's\nspecific development data from 1990-2020 and substituted it into the model for\nanalysis and evaluation. This enabled us to obtain detailed quantitative index\ndata of the degree of influence [10][12][14]. To address China's shortcomings\nin science and technology innovation, we recommend strengthening economic\ncooperation with developed countries, diversifying market development, and\nactively expanding the domestic market through feasible solutions\n[13][16][23][36].\n"
    },
    {
        "paper_id": 2304.11531,
        "authors": "Hirokuni Iiboshi, Daikuke Ozaki, Yui Yoshii",
        "title": "Child Care, Time Allocation, and the Life Cycle",
        "comments": "29 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
        "abstract": "  This study explores the impact of gender differences in preferences and\nproductivity in home production on the time allocation in married couples,\nparticularly in relation to childcare responsibilities. Using aggregated data\nfrom Japan, we estimate a life-cycle model that tracks the development of a\nchild from infancy to adulthood by extending the work of Blundell et al.\n(2018). Our findings reveal a decrease in maternal earnings following\nchildbirth, aligning with the empirical evidence of the child penalty. However,\nthe model's projections and the actual data diverge significantly during the\nphase of maternal earnings recovery, showing a discrepancy of approximately\n50%, the size of which implies an involuntary reduction in the wife's market\nwork earnings.\n"
    },
    {
        "paper_id": 2304.11577,
        "authors": "Ali Lazrak, Hanxiao Wang, Jiongmin Yong",
        "title": "Present-Biased Lobbyists in Linear Quadratic Stochastic Differential\n  Games",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  We investigate a linear quadratic stochastic zero-sum game where two players\nlobby a political representative to invest in a wind turbine farm. Players are\ntime-inconsistent because they discount performance with a non-constant rate.\nOur objective is to identify a consistent planning equilibrium in which the\nplayers are aware of their inconsistency and cannot commit to a lobbying\npolicy. We analyze the equilibrium behavior in both single player and\ntwo-player cases, and compare the behavior of the game under constant and\nnon-constant discount rates. The equilibrium behavior is provided in\nclosed-loop form, either analytically or via numerical approximation. Our\nnumerical analysis of the equilibrium reveals that strategic behavior leads to\nmore intense lobbying without resulting in overshooting.\n"
    },
    {
        "paper_id": 2304.11586,
        "authors": "Ha Nguyen",
        "title": "Particle MCMC in forecasting frailty correlated default models with\n  expert opinion",
        "comments": "31 pages, 2 figures",
        "journal-ref": "Journal of Risk and Financial Management. 2023; 16(7):334",
        "doi": "10.3390/jrfm16070334",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Predicting corporate default risk has long been a crucial topic in the\nfinance field, as bankruptcies impose enormous costs on market participants as\nwell as the economy as a whole. This paper aims to forecast frailty correlated\ndefault models with subjective judgements on a sample of U.S. public\nnon-financial firms spanning January 1980-June 2019. We consider a reduced-form\nmodel and adopt a Bayesian approach coupled with the Particle Markov Chain\nMonte Carlo (Particle MCMC) algorithm to scrutinize this problem. The findings\nshow that the volatility and the mean reversion of the hidden factor, which\ndetermine the dependence of the unobserved default intensities on the latent\nvariable, have a highly economically and statistically significant positive\nimpact on the default intensities of the firms. The results also indicate that\nthe 1-year prediction for frailty correlated default models with different\nprior distributions is relatively good, whereas the prediction accuracy ratios\nfor frailty-correlated default models with non-informative and subjective prior\ndistributions over various prediction horizons are not significantly different.\n"
    },
    {
        "paper_id": 2304.11771,
        "authors": "Erik Brynjolfsson, Danielle Li and Lindsey Raymond",
        "title": "Generative AI at Work",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  We study the staggered introduction of a generative AI-based conversational\nassistant using data from 5,000 customer support agents. Access to the tool\nincreases productivity, as measured by issues resolved per hour, by 14 percent\non average, with the greatest impact on novice and low-skilled workers, and\nminimal impact on experienced and highly skilled workers. We provide suggestive\nevidence that the AI model disseminates the potentially tacit knowledge of more\nable workers and helps newer workers move down the experience curve. In\naddition, we show that AI assistance improves customer sentiment, reduces\nrequests for managerial intervention, and improves employee retention.\n"
    },
    {
        "paper_id": 2304.11856,
        "authors": "Jiwook Kim and Minhyeok Lee",
        "title": "Portfolio Optimization using Predictive Auxiliary Classifier Generative\n  Adversarial Networks with Measuring Uncertainty",
        "comments": "11 pages, 3 figures, and one table",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
        "abstract": "  In financial engineering, portfolio optimization has been of consistent\ninterest. Portfolio optimization is a process of modulating asset distributions\nto maximize expected returns and minimize risks. To obtain the expected\nreturns, deep learning models have been explored in recent years. However, due\nto the deterministic nature of the models, it is difficult to consider the risk\nof portfolios because conventional deep learning models do not know how\nreliable their predictions can be. To address this limitation, this paper\nproposes a probabilistic model, namely predictive auxiliary classifier\ngenerative adversarial networks (PredACGAN). The proposed PredACGAN utilizes\nthe characteristic of the ACGAN framework in which the output of the generator\nforms a distribution. While ACGAN has not been employed for predictive models\nand is generally utilized for image sample generation, this paper proposes a\nmethod to use the ACGAN structure for a probabilistic and predictive model.\nAdditionally, an algorithm to use the risk measurement obtained by PredACGAN is\nproposed. In the algorithm, the assets that are predicted to be at high risk\nare eliminated from the investment universe at the rebalancing moment.\nTherefore, PredACGAN considers both return and risk to optimize portfolios. The\nproposed algorithm and PredACGAN have been evaluated with daily close price\ndata of S&P 500 from 1990 to 2020. Experimental scenarios are assumed to\nrebalance the portfolios monthly according to predictions and risk measures\nwith PredACGAN. As a result, a portfolio using PredACGAN exhibits 9.123% yearly\nreturns and a Sharpe ratio of 1.054, while a portfolio without considering risk\nmeasures shows 1.024% yearly returns and a Sharpe ratio of 0.236 in the same\nscenario. Also, the maximum drawdown of the proposed portfolio is lower than\nthe portfolio without PredACGAN.\n"
    },
    {
        "paper_id": 2304.11883,
        "authors": "Kyungsub Lee",
        "title": "Recurrent neural network based parameter estimation of Hawkes model on\n  high-frequency financial data",
        "comments": "To appear in Finance Research Letters",
        "journal-ref": null,
        "doi": "10.1016/j.frl.2023.103922",
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  This study examines the use of a recurrent neural network for estimating the\nparameters of a Hawkes model based on high-frequency financial data, and\nsubsequently, for computing volatility. Neural networks have shown promising\nresults in various fields, and interest in finance is also growing. Our\napproach demonstrates significantly faster computational performance compared\nto traditional maximum likelihood estimation methods while yielding comparable\naccuracy in both simulation and empirical studies. Furthermore, we demonstrate\nthe application of this method for real-time volatility measurement, enabling\nthe continuous estimation of financial volatility as new price data keeps\ncoming from the market.\n"
    },
    {
        "paper_id": 2304.11888,
        "authors": "Hannes Wallimann and Silvio Sticher",
        "title": "On suspicious tracks: machine-learning based approaches to detect\n  cartels in railway-infrastructure procurement",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In railway infrastructure, construction and maintenance is typically procured\nusing competitive procedures such as auctions. However, these procedures only\nfulfill their purpose - using (taxpayers') money efficiently - if bidders do\nnot collude. Employing a unique dataset of the Swiss Federal Railways, we\npresent two methods in order to detect potential collusion: First, we apply\nmachine learning to screen tender databases for suspicious patterns. Second, we\nestablish a novel category-managers' tool, which allows for sequential and\ndecentralized screening. To the best of our knowledge, we pioneer illustrating\nthe adaption and application of machine-learning based price screens to a\nrailway-infrastructure market.\n"
    },
    {
        "paper_id": 2304.12176,
        "authors": "Shoyeb Khan, Satyendra Kumar Sharma, Arnab Kumar Laha",
        "title": "From Misalignment to Synergy: Analysis of Patents from Indian\n  Universities & Research Institutions",
        "comments": "22 pages, 14 figures, Preprint for journal submission in future",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Indian Universities and Research Institutions have been the cornerstone of\nhuman resource development in the country, nurturing bright minds and shaping\nthe leaders of tomorrow. Their unwavering commitment to excellence in education\nand research has not only empowered individuals but has also made significant\ncontributions to the overall growth and progress of the nation. Despite the\nsignificant strides made by Indian universities and research institutions, the\ncountry still lags behind many developed nations in terms of the number of\npatents filed as well as in the commercialization of the granted patents. With\n34 percent1 of students choosing STEM fields in India, and over 750\nUniversities and nearly 40,000 colleges, the concentration of patent\napplications in only a few top 10 institutions raises concerns.\n  Innovation and technological advancement have become key drivers of economic\ngrowth and development in modern times. Therefore, our study aims to unravel\nthe patent landscape of Indian Universities and Research Institutions,\nexamining it through the lens of supply and demand for innovations and ideas.\nDelving into the dynamics of patent filing and innovation trends, this study\nseeks to shed light on the current state of intellectual property generation in\nthe country's academic and research ecosystem.\n"
    },
    {
        "paper_id": 2304.12245,
        "authors": "Matteo Bruno, Dario Mazzilli, Aurelio Patelli, Tiziano Squartini,\n  Fabio Saracco",
        "title": "Inferring comparative advantage via entropy maximization",
        "comments": null,
        "journal-ref": "2023 J. Phys. Complex. 4 045011",
        "doi": "10.1088/2632-072X/ad1411",
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  We revise the procedure proposed by Balassa to infer comparative advantage,\nwhich is a standard tool, in Economics, to analyze specialization (of\ncountries, regions, etc.). Balassa's approach compares the export of a product\nfor each country with what would be expected from a benchmark based on the\ntotal volumes of countries and products flows. Based on results in the\nliterature, we show that the implementation of Balassa's idea generates a bias:\nthe prescription of the maximum likelihood used to calculate the parameters of\nthe benchmark model conflicts with the model's definition. Moreover, Balassa's\napproach does not implement any statistical validation. Hence, we propose an\nalternative procedure to overcome such a limitation, based upon the framework\nof entropy maximisation and implementing a proper test of hypothesis: the `key\nproducts' of a country are, now, the ones whose production is significantly\nlarger than expected, under a null-model constraining the same amount of\ninformation employed by Balassa's approach. What we found is that countries\ndiversification is always observed, regardless of the strictness of the\nvalidation procedure. Besides, the ranking of countries' fitness is only\npartially affected by the details of the validation scheme employed for the\nanalysis while large differences are found to affect the rankings of products\nComplexities. The routine for implementing the entropy-based filtering\nprocedures employed here is freely available through the official Python\nPackage Index PyPI.\n"
    },
    {
        "paper_id": 2304.12433,
        "authors": "Mariam Kamal and Josu Arteche",
        "title": "Long memory, fractional integration and cointegration analysis of real\n  convergence in Spain",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper investigates economic convergence in terms of real income per\ncapita among the autonomous regions of Spain. In order to converge, the series\nshould cointegrate. This necessary condition is checked using two testing\nstrategies recently proposed for fractional cointegration, finding no evidence\nof cointegration, which rules out the possibility of convergence between all or\nsome of the Spanish regions. As an additional contribution, an extension of the\ncritical values of one of the tests of fractional cointegration is provided for\na different number of variables and sample sizes from those originally provided\nby the author, fitting those considered in this paper.\n"
    },
    {
        "paper_id": 2304.1245,
        "authors": "Carsten H. Chong, Viktor Todorov",
        "title": "Asymptotic Expansions for High-Frequency Option Data",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We derive a nonparametric higher-order asymptotic expansion for small-time\nchanges of conditional characteristic functions of It\\^o semimartingale\nincrements. The asymptotics setup is of joint type: both the length of the time\ninterval of the increment of the underlying process and the time gap between\nevaluating the conditional characteristic function are shrinking. The spot\nsemimartingale characteristics of the underlying process as well as their spot\nsemimartingale characteristics appear as leading terms in the derived\nasymptotic expansions. The analysis applies to a general class of It\\^o\nsemimartingales that includes in particular L\\'evy-driven SDEs and time-changed\nL\\'evy processes. The asymptotic expansion results are of direct use for\nconstructing nonparametric estimates pertaining to the stochastic volatility\ndynamics of an asset from high-frequency data of options written on the\nunderlying asset.\n"
    },
    {
        "paper_id": 2304.12501,
        "authors": "Nozomu Kobayashi, Yoshiyuki Suimon, Koichi Miyamoto, Kosuke Mitarai",
        "title": "The cross-sectional stock return predictions via quantum neural network\n  and tensor network",
        "comments": "19 pages, 7 figures",
        "journal-ref": "Quantum Mach. Intell. 5, 46 (2023)",
        "doi": "10.1007/s42484-023-00136-x",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper, we investigate the application of quantum and quantum-inspired\nmachine learning algorithms to stock return predictions. Specifically, we\nevaluate the performance of quantum neural network, an algorithm suited for\nnoisy intermediate-scale quantum computers, and tensor network, a\nquantum-inspired machine learning algorithm, against classical models such as\nlinear regression and neural networks. To evaluate their abilities, we\nconstruct portfolios based on their predictions and measure investment\nperformances. The empirical study on the Japanese stock market shows the tensor\nnetwork model achieves superior performance compared to classical benchmark\nmodels, including linear and neural network models. Though the quantum neural\nnetwork model attains a lowered risk-adjusted excess return than the classical\nneural network models over the whole period, both the quantum neural network\nand tensor network models have superior performances in the latest market\nenvironment, which suggests the capability of the model's capturing\nnon-linearity between input features.\n"
    },
    {
        "paper_id": 2304.13128,
        "authors": "Andrew Na and Meixin Zhang and Justin Wan",
        "title": "Computing Volatility Surfaces using Generative Adversarial Networks with\n  Minimal Arbitrage Violations",
        "comments": "This is an accompanying work for the presentation of the same title\n  at AAAI 2024 bridge program AI in Financial Services",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper, we propose a generative adversarial network (GAN) approach for\nefficiently computing volatility surfaces. The idea is to make use of the\nspecial GAN neural architecture so that on one hand, we can learn volatility\nsurfaces from training data and on the other hand, enforce no-arbitrage\nconditions. In particular, the generator network is assisted in training by a\ndiscriminator that evaluates whether the generated volatility matches the\ntarget distribution. Meanwhile, our framework trains the GAN network to satisfy\nthe no-arbitrage constraints by introducing penalties as regularization terms.\nThe proposed GAN model allows the use of shallow networks which results in much\nless computational costs. In our experiments, we demonstrate the performance of\nthe proposed method by comparing with the state-of-the-art methods for\ncomputing implied and local volatility surfaces. We show that our GAN model can\noutperform artificial neural network (ANN) approaches in terms of accuracy and\ncomputational time.\n"
    },
    {
        "paper_id": 2304.13402,
        "authors": "David Garc\\'ia-Lorite and Raul Merino",
        "title": "Convexity adjustments \\`a la Malliavin",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper, we develop a novel method based on Malliavin calculus to find\nan approximation for the convexity adjustment for various classical interest\nrate products. Malliavin calculus provides a simple way to get a template for\nthe convexity adjustment. We find the approximation for Futures, OIS Futures,\nFRAs, and CMSs under a general family of the one-factor Cheyette model. We have\nalso seen the excellent quality of the numerical accuracy of the formulas\nobtained.\n"
    },
    {
        "paper_id": 2304.1361,
        "authors": "Fabien Le Floc'h and Winfried Koller",
        "title": "Maximum Implied Variance Slope -- Practical Aspects",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  In the Black-Scholes model, the absence of arbitrages imposes necessary\nconstraints on the slope of the implied variance in terms of log-moneyness,\nasymptotically for large log-moneyness. The constraints are used for example in\nthe SVI implied volatility parameterization to ensure the resulting smile has\nno arbitrages. This note shows that those no-arbitrage contraints are very\nmild, and that arbitrage is almost always guaranteed in a large range of slopes\nwhere the contraints are enforced.\n"
    },
    {
        "paper_id": 2304.13818,
        "authors": "Mohsen Mortazavi",
        "title": "Selecting Sustainable Optimal Stock by Using Multi-Criteria Fuzzy\n  Decision-Making Approaches Based on the Development of the Gordon Model: A\n  case study of the Toronto Stock Exchange",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Choosing the right stock portfolio with the highest efficiencies has always\nconcerned accurate and legal investors. Investors have always been concerned\nabout the accuracy and legitimacy of choosing the right stock portfolio with\nhigh efficiency. Therefore, this paper aims to determine the criteria for\nselecting an optimal stock portfolio with a high-efficiency ratio in the\nToronto Stock Exchange using the integrated evaluation and decision-making\ntrial laboratory (DEMATEL) model and Multi-Criteria Fuzzy decision-making\napproaches regarding the development of the Gordon model. In the current study,\nresults obtained using combined multi-criteria fuzzy decision-making\napproaches, the practical factors, the relative weight of dividends, discount\nrate, and dividend growth rate have been comprehensively illustrated using\ncombined multi-criteria fuzzy decision-making approaches. A group of 10 experts\nwith at least a ten-year of experience in the stock exchange field was formed\nto review the different and new aspects of the subject (portfolio selection) to\ndecide the interaction between the group members and the exchange of attitudes\nand ideas regarding the criteria. The sequence of influence and effectiveness\nof the main criteria with DEMATEL has shown that the profitability criterion\ninteracts most with other criteria. The criteria of managing methods and\noperations (MPO), market, risk, and growth criteria are ranked next in terms of\ninteraction with other criteria. This study concludes that regarding the\nmodel's appropriate and reliable validity in choosing the optimal stock\nportfolio, it is recommended that portfolio managers in companies, investment\nfunds, and capital owners use the model to select stocks in the Toronto Stock\nExchange optimally.\n"
    },
    {
        "paper_id": 2304.13936,
        "authors": "Rodrigo Zeidan and Silvio Luiz de Almeida and In\\'acio B\\'o and Neil\n  Lewis Jr",
        "title": "Racial and income-based affirmative action in higher education\n  admissions: lessons from the Brazilian experience",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This survey article provides insights regarding the future of affirmative\naction by analyzing the implementation methods and the empirical evidence on\nthe use of placement quotas in the Brazilian higher education system. All\nfederal universities have required income and racial-based quotas in Brazil\nsince 2012. Affirmative action in federal universities is uniformly applied\nacross the country, which makes evaluating its effects particularly valuable.\nAffirmative action improves the outcomes of targeted students. Specifically,\nrace-based quotas raise the share of black students in federal universities, an\neffect not observed with income-based quotas alone. Affirmative action has\ndownstream positive consequences for labor market outcomes. The results suggest\nthat income and race-based quotas beneficiaries experience substantial\nlong-term welfare benefits. There is no evidence of mismatching or negative\nconsequences for targeted students' peers.\n"
    },
    {
        "paper_id": 2304.13985,
        "authors": "Ziyi Xu and Xue Cheng",
        "title": "The Effects of High-frequency Anticipatory Trading: Small Informed\n  Trader vs. Round-Tripper",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In an extended Kyle's model, the interactions between a large informed trader\nand a high-frequency trader (HFT) who can anticipate the former's incoming\norder are studied. We find that, in equilibrium, HFT may play the role of\nSmall-IT or Round-Tripper: both of them trade in the same direction as IT in\nadvance, but when IT's order arrives, Small-IT continues to take liquidity\naway, while Round-Tripper supplies liquidity back. So Small-IT always harms IT,\nwhile Round-Tripper may benefit her. What's more, with an anticipatory HFT,\nnormal-speed small uninformed traders suffer less and price discovery is\naccelerated.\n"
    },
    {
        "paper_id": 2304.14001,
        "authors": "Steffen Jaap Bakker and E. Ruben van Beesten and Ingvild Synn{\\o}ve\n  Brynildsen and Anette Sandvig and Marit Siqveland and Asgeir Tomasgard",
        "title": "STraM: a framework for strategic national freight transport modeling",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  To achieve carbon emission targets worldwide, decarbonization of the freight\ntransport sector will be an important factor. To this end, national governments\nmust make plans that facilitate this transition. National freight transport\nmodels are a useful tool to assess what the effects of various policies and\ninvestments may be. The state of the art consists of very detailed, static\nmodels. While useful for short-term policy assessment, these models are less\nsuitable for the long-term planning necessary to facilitate the transition to\nlow-carbon transportation in the upcoming decades.\n  In this paper, we fill this gap by developing a framework for strategic\nnational freight transport modeling, which we call STraM, and which can be\ncharacterized as a multi-period stochastic network design model, based on a\nmultimodal freight transport formulation. In STraM, we explicitly include\nseveral aspects that are lacking in state-of-the art national freight transport\nmodels: the dynamic nature of long-term planning, as well as new, low-carbon\nfuel technologies and long-term uncertainties in the development of these\ntechnologies. We illustrate our model using a case study of Norway and discuss\nthe resulting insights. In particular, we demonstrate the relevance of modeling\nmultiple time periods, the importance of including long-term uncertainty in\ntechnology development, and the efficacy of carbon pricing.\n"
    },
    {
        "paper_id": 2304.14098,
        "authors": "Christian Bongiorno, Marco Berritta",
        "title": "Optimal Covariance Cleaning for Heavy-Tailed Distributions: Insights\n  from Information Theory",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In optimal covariance cleaning theory, minimizing the Frobenius norm between\nthe true population covariance matrix and a rotational invariant estimator is a\nkey step. This estimator can be obtained asymptotically for large covariance\nmatrices, without knowledge of the true covariance matrix. In this study, we\ndemonstrate that this minimization problem is equivalent to minimizing the loss\nof information between the true population covariance and the rotational\ninvariant estimator for normal multivariate variables. However, for Student's t\ndistributions, the minimal Frobenius norm does not necessarily minimize the\ninformation loss in finite-sized matrices. Nevertheless, such deviations vanish\nin the asymptotic regime of large matrices, which might extend the\napplicability of random matrix theory results to Student's t distributions.\nThese distributions are characterized by heavy tails and are frequently\nencountered in real-world applications such as finance, turbulence, or nuclear\nphysics. Therefore, our work establishes a connection between statistical\nrandom matrix theory and estimation theory in physics, which is predominantly\nbased on information theory.\n"
    },
    {
        "paper_id": 2304.14264,
        "authors": "Anna Stelzer",
        "title": "Monetary policy and the joint distribution of income and wealth: The\n  heterogeneous case of the euro area",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  This papers aims to establish the empirical relationship between income, net\nwealth and their joint distribution in a selected group of euro area countries.\nI estimate measures of dependence between income and net wealth using a\nsemiparametric copula approach and calculate a bivariate Gini coefficient. By\ncombining structural inference from vector autoregressions on the macroeconomic\nlevel with a simulation using microeconomic data, I investigate how\nconventional and unconventional monetary policy measures affect the joint\ndistribution. Results indicate that effects of monetary policy are highly\nheterogeneous across different countries, both in terms of the dependence of\nincome and net wealth on each other, and in terms of inequality in both income\nand net wealth.\n"
    },
    {
        "paper_id": 2304.1487,
        "authors": "Sungwoo Kang, Jong-Kook Kim",
        "title": "Using a Deep Learning Model to Simulate Human Stock Trader's Methods of\n  Chart Analysis",
        "comments": "35 pages, 14 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Despite the efficient market hypothesis, many studies suggest the existence\nof inefficiencies in the stock market leading to the development of techniques\nto gain above-market returns. Systematic trading has undergone significant\nadvances in recent decades with deep learning schemes emerging as a powerful\ntool for analyzing and predicting market behavior. In this paper, a method is\nproposed that is inspired by how professional technical analysts trade. This\nscheme looks at stock prices of the previous 600 days and predicts whether the\nstock price will rise or fall 10% or 20% within the next D days. The proposed\nmethod uses the Resnet's (a deep learning model) skip connections and logits to\nincrease the probability of the prediction. The model was trained and tested\nusing historical data from both the Korea and US stock markets. The backtest is\ndone using the data from 2020 to 2022. Using the proposed method for the Korea\nmarket it gave return of 75.36% having Sharpe ratio of 1.57, which far exceeds\nthe market return by 36% and 0.61, respectively. On the US market it gives\ntotal return of 27.17% with Sharpe ratio of 0.61, which outperforms other\nbenchmarks such as NASDAQ, S&P500, DOW JONES index by 17.69% and 0.27,\nrespectively.\n"
    },
    {
        "paper_id": 2304.14941,
        "authors": "Sugandha Srivastav and Tanmay Singh",
        "title": "Greening our Laws: Revising Land Acquisition Law for Coal Mining in\n  India",
        "comments": null,
        "journal-ref": "Economic and Political Weekly. Vol. 57, Issue No. 46 (2022)",
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Laws that govern land acquisition can lock in old paradigms. We study one\nsuch case, the Coal Bearing Areas Act of 1957 (CBAA) which provides minimal\nsocial and environmental safegaurds, and deviates in important ways from the\nRight to Fair Compensation and Transparency in Land Acquisition, Rehabilitation\nand Resettlement Act 2013 (LARR). The lack of due diligence protocol in the\nCBAA confers an undue comparative advantage to coal development, which is\ninconsistent with India's stance to phase down coal use, reduce air pollution,\nand advance modern sources of energy. We argue that the premise under which the\nCBAA was historically justified is no longer valid due to a significant change\nin the local context. Namely, the environmental and social costs of coal energy\nare far more salient and the market has cleaner energy alternatives that are\ncost competitive. We recommend updating land acquisition laws to bring coal\nunder the general purview of LARR or, at minimum, amending the CBAA to ensure\nadequate environmental and social safeguards are in place, both in letter and\npractice.\n"
    },
    {
        "paper_id": 2304.1496,
        "authors": "Sugandha Srivastav and Ryan Rafaty",
        "title": "Political Strategies to Overcome Climate Policy Obstructionism",
        "comments": null,
        "journal-ref": "Perspectives on Politics, First View: pp.1-11 (2022)",
        "doi": "10.1017/S1537592722002080",
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Great socio-economic transitions see the demise of certain industries and the\nrise of others. The losers of the transition tend to deploy a variety of\ntactics to obstruct change. We develop a political-economy model of interest\ngroup competition and garner evidence of tactics deployed in the global climate\nmovement. From this we deduce a set of strategies for how the climate movement\ncompetes against entrenched hydrocarbon interests. Five strategies for\novercoming obstructionism emerge: (1) Appeasement, which involves compensating\nthe losers; (2) Co-optation, which seeks to instigate change by working with\nincumbents; (3) Institutionalism, which involves changes to public institutions\nto support decarbonization; (4) Antagonism, which creates reputational or\nlitigation costs to inaction; and (5) Countervailance, which makes low-carbon\nalternatives more competitive. We argue that each strategy addresses the\nproblem of obstructionism through a different lens, reflecting a diversity of\nactors and theories of change within the climate movement. The choice of which\nstrategy to pursue depends on the institutional context.\n"
    },
    {
        "paper_id": 2305.00044,
        "authors": "Patrick Bajari, Zhihao Cen, Victor Chernozhukov, Manoj Manukonda,\n  Suhas Vijaykumar, Jin Wang, Ramon Huerta, Junbo Li, Ling Leng, George\n  Monokroussos, and Shan Wan",
        "title": "Hedonic Prices and Quality Adjusted Price Indices Powered by AI",
        "comments": "Revised CEMMAP Working Paper (CWP08/23)",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Accurate, real-time measurements of price index changes using electronic\nrecords are essential for tracking inflation and productivity in today's\neconomic environment. We develop empirical hedonic models that can process\nlarge amounts of unstructured product data (text, images, prices, quantities)\nand output accurate hedonic price estimates and derived indices. To accomplish\nthis, we generate abstract product attributes, or ``features,'' from text\ndescriptions and images using deep neural networks, and then use these\nattributes to estimate the hedonic price function. Specifically, we convert\ntextual information about the product to numeric features using large language\nmodels based on transformers, trained or fine-tuned using product descriptions,\nand convert the product image to numeric features using a residual network\nmodel. To produce the estimated hedonic price function, we again use a\nmulti-task neural network trained to predict a product's price in all time\nperiods simultaneously. To demonstrate the performance of this approach, we\napply the models to Amazon's data for first-party apparel sales and estimate\nhedonic prices. The resulting models have high predictive accuracy, with $R^2$\nranging from $80\\%$ to $90\\%$. Finally, we construct the AI-based hedonic\nFisher price index, chained at the year-over-year frequency. We contrast the\nindex with the CPI and other electronic indices.\n"
    },
    {
        "paper_id": 2305.002,
        "authors": "Gregoire Loeper, Jan Obloj, Benjamin Joseph",
        "title": "Calibration of Local Volatility Models with Stochastic Interest Rates\n  using Optimal Transport",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We develop a non-parametric, optimal transport driven, calibration\nmethodology for local volatility models with stochastic interest rate. The\nmethod finds a fully calibrated model which is the closest to a given reference\nmodel. We establish a general duality result which allows to solve the problem\nvia optimising over solutions to a non-linear HJB equation. We then apply the\nmethod to a sequential calibration setup: we assume that an interest rate model\nis given and is calibrated to the observed term structure in the market. We\nthen seek to calibrate a stock price local volatility model with volatility\ncoefficient depending on time, the underlying and the short rate process, and\ndriven by a Brownian motion which can be correlated with the randomness driving\nthe rates process. The local volatility model is calibrated to a finite number\nof European options prices via a convex optimisation problem derived from the\nPDE formulation of semimartingale optimal transport. Our methodology is\nanalogous to Guo, Loeper, and Wang, 2022 and Guo, Loeper, Obloj, et al., 2022a\nbut features a novel element of solving for discounted densities, or\nsub-probability measures. We present numerical experiments and test the\neffectiveness of the proposed methodology.\n"
    },
    {
        "paper_id": 2305.00231,
        "authors": "Anna Naszodi",
        "title": "Historical trend in educational homophily: U-shaped or not U-shaped? Or,\n  how to set a criterion to choose a criterion?",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Measuring changes in overall inequality between different educational groups\nis often performed by quantifying variations in educational marital homophily\nacross consecutive generations. However, this task becomes challenging when the\neducation level of marriageable individuals is generation-specific. To address\nthis challenge, various indicators have been proposed in the assortative mating\nliterature.\n  In this paper, we review a set of criteria that indicators must satisfy to be\nconsidered as suitable measures of homophily and inequality. Our analytical\ncriteria include robustness to the number of educational categories\ndistinguished and the negative association between intergenerational mobility\nand homophily. Additionally, we also impose an empirical criterion on the\nidentified qualitative historical trend in homophily between 1960 and 2010 in\nthe US at the national and sub-national levels.\n  Our analysis reveals that while a specific cardinal indicator meets all three\ncriteria, many commonly applied indices do not. We propose the application of\nthis well-performing indicator to quantify the trend in overall inequality in\nany country, including European countries, with available population data on\ncouples' education level.\n"
    },
    {
        "paper_id": 2305.00245,
        "authors": "Rian Dolphin, Barry Smyth, Ruihai Dong",
        "title": "Industry Classification Using a Novel Financial Time-Series Case\n  Representation",
        "comments": "15 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  The financial domain has proven to be a fertile source of challenging machine\nlearning problems across a variety of tasks including prediction, clustering,\nand classification. Researchers can access an abundance of time-series data and\neven modest performance improvements can be translated into significant\nadditional value. In this work, we consider the use of case-based reasoning for\nan important task in this domain, by using historical stock returns time-series\ndata for industry sector classification. We discuss why time-series data can\npresent some significant representational challenges for conventional\ncase-based reasoning approaches, and in response, we propose a novel\nrepresentation based on stock returns embeddings, which can be readily\ncalculated from raw stock returns data. We argue that this representation is\nwell suited to case-based reasoning and evaluate our approach using a\nlarge-scale public dataset for the industry sector classification task,\ndemonstrating substantial performance improvements over several baselines using\nmore conventional representations.\n"
    },
    {
        "paper_id": 2305.00509,
        "authors": "Liyuan Lin and Fangda Liu and Jingzhen Liu abd Luyang Yu",
        "title": "The optimal reinsurance strategy with price-competition between two\n  reinsurers",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We study optimal reinsurance in the framework of stochastic game theory, in\nwhich there is an insurer and two reinsurers. A Stackelberg model is\nestablished to analyze the non-cooperative relationship between the insurer and\nreinsurers, where the insurer is considered as the follower and the reinsurers\nare considered as the leaders. The insurer is a price taker who determines\nreinsurance demand in the reinsurance market, while the reinsurers can price\nthe reinsurance treaties. Our contribution is to use a Nash game to describe\nthe price-competition between two reinsurers. We assume that one of the\nreinsurers adopts the variance premium principle and the other adopts the\nexpected value premium principle. The insurer and the reinsurers aim to\nmaximize their respective mean-variance cost functions which lead to a\ntime-inconsistency control problem. To overcome the time-inconsistency issue in\nthe game, we formulate the optimization problem of each player as an embedded\ngame and solve it via a corresponding extended Hamilton-Jacobi-Bellman\nequation. We find that the insurer will sign propositional and excess loss\nreinsurance strategies with reinsurer 1 and reinsurer 2, respectively. When the\nclaim size follows exponential distribution, there exists a unique equilibrium\nreinsurance premium strategy. Our numerical analysis verifies the impact of\nclaim size, risk aversion and interest rates of the insurer and reinsurers on\nequilibrium reinsurance strategy and premium strategy, which can help to\nunderstand competition in the reinsurance market\n"
    },
    {
        "paper_id": 2305.00541,
        "authors": "Ren\\'e Aid, Matteo Basei, Giorgio Ferrari",
        "title": "A Stationary Mean-Field Equilibrium Model of Irreversible Investment in\n  a Two-Regime Economy",
        "comments": "32 pages; 4 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We consider a mean-field model of firms competing \\`a la Cournot on a\ncommodity market, where the commodity price is given in terms of a power\ninverse demand function of the industry-aggregate production. Investment is\nirreversible and production capacity depreciates at a constant rate. Production\nis subject to Gaussian productivity shocks, while large non-anticipated\nmacroeconomic events driven by a two-state continuous-time Markov chain can\nchange the volatility of the shocks, as well as the price function. Firms wish\nto maximize expected discounted revenues of production, net of investment and\noperational costs. Investment decisions are based on the long-run stationary\nprice of the commodity. We prove existence, uniqueness and characterization of\nthe stationary mean-field equilibrium of the model. The equilibrium investment\nstrategy is of barrier-type and it is triggered by a couple of endogenously\ndetermined investment thresholds, one per state of the economy. We provide a\nquasi-closed form expression of the stationary density of the state and we show\nthat our model can produce Pareto distribution of firms' size. This is a\nfeature that is consistent both with observations at the aggregate level of\nindustries and at the level of a particular industry. We establish a relation\nbetween economic instability and market concentration and we show how\nmacroeconomic instability can harm firms' profitability more than productivity\nfluctuations.\n"
    },
    {
        "paper_id": 2305.00545,
        "authors": "Achim Ahrens, Alessandra Stampi-Bombelli, Selina Kurer, Dominik\n  Hangartner",
        "title": "Optimal multi-action treatment allocation: A two-phase field experiment\n  to boost immigrant naturalization",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Research underscores the role of naturalization in enhancing immigrants'\nsocio-economic integration, yet application rates remain low. We estimate a\npolicy rule for a letter-based information campaign encouraging newly eligible\nimmigrants in Zurich, Switzerland, to naturalize. The policy rule assigns one\nout of three treatment letters to each individual, based on their observed\ncharacteristics. We field the policy rule to one-half of 1,717 immigrants,\nwhile sending random treatment letters to the other half. Despite only moderate\ntreatment effect heterogeneity, the policy tree yields a larger, albeit\ninsignificant, increase in application rates compared to assigning the same\nletter to everyone.\n"
    },
    {
        "paper_id": 2305.00565,
        "authors": "Benjamin Jourdain, Kexin Shao",
        "title": "Non-decreasing martingale couplings",
        "comments": "41 pages, 1 figure",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  For many examples of couples $(\\mu,\\nu)$ of probability measures on the real\nline in the convex order, we observe numerically that the Hobson and Neuberger\nmartingale coupling, which maximizes for $\\rho=1$ the integral of $|y-x|^\\rho$\nwith respect to any martingale coupling between $\\mu$ and $\\nu$, is still a\nmaximizer for $\\rho\\in(0,2)$ and a minimizer for $\\rho>2$. We investigate the\ntheoretical validity of this numerical observation and give rather restrictive\nsufficient conditions for the property to hold. We also exhibit couples\n$(\\mu,\\nu)$ such that it does not hold. The support of the Hobson and Neuberger\ncoupling is known to satisfy some monotonicity property which we call\nnon-decreasing. We check that the non-decreasing property is preserved for\nmaximizers when $\\rho\\in(0,1]$. In general, there exist distinct non-decreasing\nmartingale couplings, and we find some decomposition of $\\nu$ which is in\none-to-one correspondence with martingale couplings non-decreasing in a\ngeneralized sense.\n"
    },
    {
        "paper_id": 2305.00585,
        "authors": "C\\'elestin Coquid\\'e and Jos\\'e Lages and Dima L. Shepelyansky",
        "title": "Prospects of BRICS currency dominance in international trade",
        "comments": "18 pages, 3 figures. Appendix: 4 figures and 3 tables",
        "journal-ref": "Coquid`{e}, C., Lages, J. & Shepelyansky, D.L. Prospects of BRICS\n  currency dominance in international trade. Appl Netw Sci 8, 65 (2023)",
        "doi": "10.1007/s41109-023-00590-3",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  During his state visit to China in April 2023, Brazilian President Lula\nproposed the creation of a trade currency supported by the BRICS countries.\nUsing the United Nations Comtrade database, providing the frame of the world\ntrade network associated to 194 UN countries during the decade 2010 - 2020, we\nstudy a mathematical model of influence battle of three currencies, namely, the\nUS dollar, the euro, and such a hypothetical BRICS currency. In this model, a\ncountry trade preference for one of the three currencies is determined by a\nmultiplicative factor based on trade flows between countries and their relative\nweights in the global international trade. The three currency seed groups are\nformed by 9 eurozone countries for the euro, 5 Anglo-Saxon countries for the US\ndollar and the 5 BRICS countries for the new proposed currency. The countries\nbelonging to these 3 currency seed groups trade only with their own associated\ncurrency whereas the other countries choose their preferred trade currency as a\nfunction of the trade relations with their commercial partners. The trade\ncurrency preferences of countries are determined on the basis of a Monte Carlo\nmodeling of Ising type interactions in magnetic spin systems commonly used to\nmodel opinion formation in social networks. We adapt here these models to the\nworld trade network analysis. The results obtained from our mathematical\nmodeling of the structure of the global trade network show that as early as\n2012 about 58 percent of countries would have preferred to trade with the BRICS\ncurrency, 23 percent with the euro and 19 percent with the US dollar. Our\nresults announce favorable prospects for a dominance of the BRICS currency in\ninternational trade, if only trade relations are taken into account, whereas\npolitical and other aspects are neglected.\n"
    },
    {
        "paper_id": 2305.00693,
        "authors": "Dorje C. Brody and Tomooki Yuasa",
        "title": "Three candidate election strategy",
        "comments": "17 pages, 6 figures, version to appear in the Royal Society Open\n  Science",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  The probability of a given candidate winning a future election is worked out\nin closed form as a function of (i) the current support rates for each\ncandidate, (ii) the relative positioning of the candidates within the political\nspectrum, (iii) the time left to the election, and (iv) the rate at which noisy\ninformation is revealed to the electorate from now to the election day, when\nthere are three or more candidates. It is shown, in particular, that the\noptimal strategy for controlling information can be intricate and nontrivial,\nin contrast to a two-candidate race. A surprising finding is that for a\ncandidate taking the centre ground in an electoral competition among a\npolarised electorate, certain strategies are fatal in that the resulting\nwinning probability for that candidate vanishes identically.\n"
    },
    {
        "paper_id": 2305.00799,
        "authors": "Dangxing Chen, Weicheng Ye",
        "title": "How to address monotonicity for model risk management?",
        "comments": null,
        "journal-ref": "In Proceedings of the 40th International Conference on Machine\n  Learning, 2023, (Proceedings of Machine Learning Research, Vol. 202). PMLR,\n  5282-5295",
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  In this paper, we study the problem of establishing the accountability and\nfairness of transparent machine learning models through monotonicity. Although\nthere have been numerous studies on individual monotonicity, pairwise\nmonotonicity is often overlooked in the existing literature. This paper studies\ntransparent neural networks in the presence of three types of monotonicity:\nindividual monotonicity, weak pairwise monotonicity, and strong pairwise\nmonotonicity. As a means of achieving monotonicity while maintaining\ntransparency, we propose the monotonic groves of neural additive models. As a\nresult of empirical examples, we demonstrate that monotonicity is often\nviolated in practice and that monotonic groves of neural additive models are\ntransparent, accountable, and fair.\n"
    },
    {
        "paper_id": 2305.00887,
        "authors": "Petri P. Karenlampi",
        "title": "Disturbance Effects on Financial Timberland Returns in Austria",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Probability theory is applied for the effect of severe disturbances on the\nreturn rate on capital within multiannual stands growing crops. Two management\nregimes are discussed, rotations of even-aged plants on the one hand, and\nuneven-aged semi-stationary state on the other. The effect of any disturbance\nappears two-fold, contributing to both earnings and capitalization. Results are\nillustrated using data from a recently published study, regarding spruce (Picea\nabies) forests in Austria. The economic results differ from those of the paper\nwhere the data is presented, here indicating continuous-cover forestry is\nfinancially inferior to rotation forestry. Any severe disturbance may induce a\nregime shift from continuous-cover to even-aged forestry. If such a regime\nshift is not accepted, the disturbance losses reduce profits but do not affect\ncapitalization, making continuous-cover forestry financially more sensitive to\ndisturbances. Revenue from carbon rent favors the management regime with higher\ncarbon stock. The methods introduced in this paper can be applied to any\ndataset, regardless of location and tree species.\n"
    },
    {
        "paper_id": 2305.01035,
        "authors": "Antoine Jacquier and Zan Zuric",
        "title": "Random neural networks for rough volatility",
        "comments": "33 pages, 3 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We construct a deep learning-based numerical algorithm to solve\npath-dependent partial differential equations arising in the context of rough\nvolatility. Our approach is based on interpreting the PDE as a solution to an\nSPDE, building upon recent insights by Bayer, Qiu and Yao, and on constructing\na neural network of reservoir type as originally developed by Gonon,\nGrigoryeva, Ortega. The reservoir approach allows us to formulate the\noptimisation problem as a simple least-square regression for which we prove\ntheoretical convergence properties.\n"
    },
    {
        "paper_id": 2305.01209,
        "authors": "Edoardo Gallo, Joseph Lee, Yohanes Eko Riyanto, and Erwin Wong",
        "title": "Cooperation and Cognition in Social Networks",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Social networks can sustain cooperation by amplifying the consequences of a\nsingle defection through a cascade of relationship losses. Building on Jackson\net al. (2012), we introduce a novel robustness notion to characterize low\ncognitive complexity (LCC) networks - a subset of equilibrium networks that\nimposes a minimal cognitive burden to calculate and comprehend the consequences\nof defection. We test our theory in a laboratory experiment and find that\ncooperation is higher in equilibrium than in non-equilibrium networks. Within\nequilibrium networks, LCC networks exhibit higher levels of cooperation than\nnon-LCC networks. Learning is essential for the emergence of equilibrium play.\n"
    },
    {
        "paper_id": 2305.01478,
        "authors": "Oleg Nivievskyi, Pavlo Martyshev and Sergiy Kvasha",
        "title": "Agricultural Policy in Ukraine",
        "comments": "English version of the Chapter 2 in the Textbook of Kvasha, S.,\n  A.Dibrova, O. Nivievskyi, and P. Martyshev (2022). Agricultural policy.\n  NUBiP, Kyiv",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Building upon the theory and methodology of agricultural policy developed in\nthe previous chapter, in Chapter 2 we analyse and assess agricultural policy\nmaking in Ukraine since the breakup of Soviet Union till today. Going from top\ndown to the bottom, we begin by describing the evolution of state policy in the\nagri-food sector. In the beginning, we describe the major milestones of\nagricultural policy making since independence, paving the way to the political\neconomy of the modern agricultural policy in Ukraine. Then we describe the role\nof agri-food sector in the national economy as well as globally in ensuring\nfood security in the world. After, we dig deeper and focus on a detailed\nperformance of agricultural sector by looking at farm structures, their land\nuse, overall and sector-wise untapped productivity potential. Modern\nagricultural policy and institutional set-up is contained and analyzed in\ndetails in the section 2.4. A review of the agricultural up- and downstream\nsectors wraps up this chapter\n"
    },
    {
        "paper_id": 2305.01485,
        "authors": "Matteo Gardini and Edoardo Santilli",
        "title": "A Heath-Jarrow-Morton framework for energy markets: a pragmatic approach",
        "comments": "41 pages, 24 figures, 2 tables",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  In this article we discuss the application of the Heath-Jarrow-Morton\nframework Heath et al. [26] to energy markets. The goal of the article is to\ngive a detailed overview of the topic, focusing on practical aspects rather\nthan on theory, which has been widely studied in literature. This work aims to\nbe a guide for practitioners and for all those who deal with the practical\nissues of this approach to energy market. In particular, we focus on the\nmarkets' structure, model calibration by dimension reduction with Principal\nComponent Analysis (PCA), Monte Carlo simulations and derivatives pricing. As\napplication, we focus on European power and gas markets: we calibrate the model\non historical futures quotations, we perform futures and spot simulations and\nwe analyze the results.\n"
    },
    {
        "paper_id": 2305.0149,
        "authors": "yaacov Kopeliovich",
        "title": "Optimal control problems for stochastic processes with absorbing regime",
        "comments": "7 pages",
        "journal-ref": "Journal of Stochastic Analysis vol.4 2023",
        "doi": "10.31390/josa.4.1.06",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper we formulate and solve an optimal problem for Stochastic\nprocess with a regime absorbing state. The solution for this problem is\nobtained through a system of partial differential equations. The method is\napplied to obtain an explicit solution for the Merton portfolio problem when an\nasset has a default probability in case of a log utility.\n"
    },
    {
        "paper_id": 2305.01543,
        "authors": "Derek Liu, Francesco Piccoli, Katie Chen, Adrina Tang, Victor Fang",
        "title": "NFT Wash Trading Detection",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Wash trading is a form of market manipulation where the same entity sells an\nasset to themselves to drive up market prices, launder money under the cover of\na legitimate transaction, or claim a tax loss without losing ownership of an\nasset. Although the practice is illegal with traditional assets, lack of\nsupervision in the non-fungible token market enables criminals to wash trade\nand scam unsuspecting buyers while operating under regulators radar. AnChain.AI\ndesigned an algorithm that flags transactions within an NFT collection history\nas wash trades when a wallet repurchases a token within 30 days of previously\nselling it. The algorithm also identifies intermediate transactions within a\nwash trade cycle. Testing on 7 popular NFT collections reveals that on average,\n0.14% of transactions, 0.11% of wallets, and 0.16% of tokens in each collection\nare involved in wash trading. These wash trades generate an overall total price\nmanipulation, sales, and repurchase profit of \\$900K, \\$1.1M, and negative\n\\$1.6M respectively. The results draw attention to the prevalent market\nmanipulation taking place and inform unsuspecting buyers which tokens and\nsellers may be involved in criminal activity.\n"
    },
    {
        "paper_id": 2305.01558,
        "authors": "Chen Fengxian, Lv Xiaoyao",
        "title": "Carbon Emission Reduction Effect of RMB Appreciation: Empirical Evidence\n  from 283 Prefecture-Level Cities of China",
        "comments": "in Chinese language",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  Based on the panel data of 283 prefecture-level cities in China from 2006 to\n2019, this paper measures the extent and mechanism of the impact of RMB real\neffective exchange rate fluctuations on carbon emission intensity. The results\nshow that: (1) For every 1% appreciation of the real effective exchange rate of\nRMB, the carbon emission intensity decreases by an average of 0.463 tons/10000\nyuan; (2) The \"carbon emission reduction effect\" of RMB real effective exchange\nrate appreciation is more obvious in the eastern regions, coastal areas,\nregions with high urbanization levels, and areas with open information; (3) The\nappreciation of RMB real effective exchange rate can reduce carbon dioxide\nemission intensity by improving regional R&D and innovation ability,\nrestraining foreign trade and foreign investment, promoting industrial\nstructure optimization and upgrading, and improving income inequality.\n"
    },
    {
        "paper_id": 2305.01559,
        "authors": "Oleg Nivievskyi, Roman Neyter, Olha Halytsia, Pavlo Martyshev and\n  Oleksandr Donchenko",
        "title": "Non-refunding of VAT to soybean exporters or economic impact of Soybean\n  amendments",
        "comments": "available in Ukrainian only",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Exempting soybean and rapeseed exporters from VAT has a negative effect on\nthe economy of $\\$$44.5-60.5 million per year. The implemented policy aimed to\nincrease the processing of soybeans and rapeseed by Ukrainian plants. As a\nresult, the processors received $\\$$26 million and the state budget gained\n$\\$$2-18 million. However, soybean farmers, mostly small and medium-sized,\nreceived $ 88.5 million in losses, far outweighing the benefits of processors\nand the state budget.\n"
    },
    {
        "paper_id": 2305.01642,
        "authors": "Ke Zhang",
        "title": "Construct sparse portfolio with mutual fund's favourite stocks in China\n  A share market",
        "comments": "Portfolio construction, Sparsity, Index tracking, Mutual fund, China\n  A share market",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Unlike developed market, some emerging markets are dominated by retail and\nunprofessional trading. China A share market is a good and fitting example in\nlast 20 years. Meanwhile, lots of research show professional investor in China\nA share market continuously generate excess return compare with total market\nindex. Specifically, this excess return mostly come from stock selectivity\nability instead of market timing. However for some reason such as fund capacity\nlimit, fund manager change or market regional switch, it is very hard to find a\nfund could continuously beat market. Therefore, in order to get excess return\nfrom mutual fund industry, we use quantitative way to build the sparse\nportfolio that take advantage of favorite stocks by mutual fund in China A\nmarket. Firstly we do the analysis about favourite stocks by mutual fund and\ncompare the different method to construct our portfolio. Then we build a sparse\nstock portfolio with constraint on both individual stock and industry exposure\nusing portfolio optimizer to closely track the partial equity funds index\n930950.CSI with median 0.985 correlation. This problem is much more difficult\nthan tracking full information index or traditional ETF as higher turnover of\nmutual fund, just first 10 holding of mutual fund available and fund report\nupdated quarterly with 15 days delay. Finally we build another low risk and\nbalanced sparse portfolio that consistently outperform benchmark 930950.CSI.\n"
    },
    {
        "paper_id": 2305.01815,
        "authors": "Juan Ignacio Iba\\~nez and Alexander Freier",
        "title": "Don't Trust, Verify: Towards a Framework for the Greening of Bitcoin",
        "comments": "7 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  For more than a decade, Bitcoin has gained as much adoption as it has\nreceived criticism. Fundamentally, Bitcoin is under fire for the high carbon\nfootprint that results from the energy-intensive proof-of-work (PoW) consensus\nalgorithm. There is a trend however for Bitcoin mining to adopt a trajectory\ntoward achieving carbon-negative status, notably due to the adoption of\nmethane-based mining and mining-based flexible load response (FLR) to\ncomplement variable renewable energy (VRE) generation. Miners and electricity\nsellers may increase their profitability not only by taking advantage of excess\nenergy, but also by selling green tokens to buyers interested in greening their\nportfolios. Nevertheless, a proper ''green Bitcoin'' accounting system requires\na standard framework for the accreditation of sustainable bitcoin holdings. The\nproper way to build such a framework remains contested. In this paper, we\nsurvey the different sustainable Bitcoin accounting systems. Analyzing the\nvarious alternatives, we suggest a path forward.\n"
    },
    {
        "paper_id": 2305.02027,
        "authors": "Yasin K\\\"ur\\c{s}at \\\"Onder",
        "title": "Black-box Optimizers vs Taste Shocks",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We evaluate and extend the solution methods for models with binary and\nmultiple continuous choice variables in dynamic programming, particularly in\ncases where a discrete state space solution method is not viable. Therefore, we\napproximate the solution using taste shocks or black-box optimizers that\napplied mathematicians use to benchmark their algorithms. We apply these\nmethods to a default framework in which agents have to solve a portfolio\nproblem with long-term debt. We show that the choice of solution method\nmatters, as taste shocks fail to attain convergence in multidimensional\nproblems. We compare the relative advantages of using four optimization\nalgorithms: the Nelder-Mead downhill simplex algorithm, Powell's direction-set\nalgorithm with LINMIN, the conjugate gradient method BOBYQA, and the\nquasi-Newton Davidon-Fletcher-Powell (DFPMIN) algorithm. All of these methods,\nexcept for the last one, are preferred when derivatives cannot be easily\ncomputed. Ultimately, we find that Powell's routine evaluated with B-splines,\nwhile slow, is the most viable option. BOBYQA came in second place, while the\nother two methods performed poorly.\n"
    },
    {
        "paper_id": 2305.02138,
        "authors": "Zhiheng Yi",
        "title": "The Relationship between Consumption and Economic Growth of Chinese\n  Urban and Rural Residents since Reform and Opening-up -- An Empirical\n  Analysis Based on Econometrics Models",
        "comments": "In Chinese language, 10 pages, 13 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Since Reform and Opening-up 40 years ago, China has made remarkable\nachievements in economic fields. And consumption activities, including\nhousehold consumption, have played an important role in it. Consumer activity\nis the end of economic activity, because the ultimate aim of other economic\nactivities is to meet consumer demand; consumer activity is the starting point\nof economic activity, because consumption can drive economic and social\ndevelopment. This paper selects the economic data of more than 40 years since\nReform and Opening-up, and establishes the Vector Autoregressive (VAR) model\nand Vector Error Correction (VEC) model, analyzing the influence of consumption\nlevel and total consumption of urban and rural residents on economic growth.\nThe conclusion is that the increase of urban consumption and rural consumption\ncan lead to the increase of GDP, and in the long run, urban consumption can\npromote economic growth more than rural consumption. According to this\nconclusion, we analyze the reasons and puts forward some policy suggestions.\n"
    },
    {
        "paper_id": 2305.02159,
        "authors": "Uche Oluku and Shaoming Cheng",
        "title": "A Mediation Analysis of the Relationship Between Land Use Regulation\n  Stringency and Employment Dynamics",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The paper examines the effects of stringent land use regulations, measured\nusing the Wharton Residential Land Use Regulatory Index (WRLURI), on employment\ngrowth during the period 2010-2020 in the Retail, Professional, and Information\nsectors across 878 local jurisdictions in the United States. All the local\njurisdictions exist in both (2006 and 2018) waves of the WRLURI surveys and\nhence constitute a unique panel data. We apply a mediation analytical framework\nto decompose the direct and indirect effects of land use regulation stringency\non sectoral employment growth and specialization. Our analysis suggests a fully\nmediated pattern in the relationship between excessive land use regulations and\nemployment growth, with housing cost burden as the mediator. Specifically, a\none standard deviation increase in the WRLURI index is associated with an\napproximate increase of 0.8 percentage point in the proportion of cost burdened\nrenters. Relatedly, higher prevalence of cost-burdened renters has moderate\nadverse effects on employment growth in two sectors. A one percentage point\nincrease in the proportion of cost burdened renters is associated with 0.04 and\n0.017 percentage point decreases in the Professional and Information sectors,\nrespectively.\n"
    },
    {
        "paper_id": 2305.02229,
        "authors": "Md. Fazlul Huq Khan and Md. Masum Billah",
        "title": "Macroeconomic factors and Stock exchange return: A Statistical Analysis",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/publicdomain/zero/1.0/",
        "abstract": "  The purpose of this research is to examine the relationship between the Dhaka\nStock exchange index return and macroeconomic variables such as exchange rate,\ninflation, money supply etc. The long-term relationship between macroeconomic\nvariables and stock market returns has been analyzed by using the Johnson\nCointegration test, Augmented Dicky Fuller (ADF) and Phillip Perron (PP) tests.\nThe results revealed the existence of cointegrating relationship between stock\nprices and the macroeconomic variables in the Dhaka stock exchange. The\nconsumer price index, money supply, and exchange rates proved to be strongly\nassociated with stock returns, while market capitalization was found to be\nnegatively associated with stock returns. The findings suggest that in the long\nrun, the Dhaka stock exchange is reactive to macroeconomic indicators.\n"
    },
    {
        "paper_id": 2305.02476,
        "authors": "Xian Gong, Claire McFarland, Paul McCarthy, Colin Griffith,\n  Marian-Andrei Rizoiu",
        "title": "Informing Innovation Management: Linking Leading R&D Firms and Emerging\n  Technologies",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
        "abstract": "  Understanding the relationship between emerging technology and research and\ndevelopment has long been of interest to companies, policy makers and\nresearchers. In this paper new sources of data and tools are combined with a\nnovel technique to construct a model linking a defined set of emerging\ntechnologies with the global leading R&D spending companies. The result is a\nnew map of this landscape. This map reveals the proximity of technologies and\ncompanies in the knowledge embedded in their corresponding Wikipedia profiles,\nenabling analysis of the closest associations between the companies and\nemerging technologies. A significant positive correlation for a related set of\npatent data validates the approach. Finally, a set of Circular Economy Emerging\nTechnologies are matched to their closest leading R&D spending company,\nprompting future research ideas in broader or narrower application of the model\nto specific technology themes, company competitor landscapes and national\ninterest concerns.\n"
    },
    {
        "paper_id": 2305.02481,
        "authors": "Dejian Tian and Xunlian Wang",
        "title": "Dynamic star-shaped risk measures and $g$-expectations",
        "comments": "21 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  Motivated by the results of static monetary or star-shaped risk measures, the\npaper investigates the representation theorems in the dynamic framework. We\nshow that dynamic monetary risk measures can be represented as the lower\nenvelope of a family of dynamic convex risk measures, and normalized dynamic\nstar-shaped risk measures can be represented as the lower envelope of a family\nof normalized dynamic convex risk measures. The link between dynamic monetary\nrisk measures and dynamic star-shaped risk measures are established. Besides,\nthe sensitivity and time consistency problems are also studied. A specific\nnormalized time consistent dynamic star-shaped risk measures induced by $ g\n$-expectations are illustrated and discussed in detail.\n"
    },
    {
        "paper_id": 2305.02513,
        "authors": "Cecilia Machado and Germ\\'an Reyes and Evan Riehl",
        "title": "The Direct and Spillover Effects of Large-scale Affirmative Action at an\n  Elite Brazilian University",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  We examine the effects of an affirmative action policy at an elite Brazilian\nuniversity that reserved 45 percent of admission slots for Black and low-income\nstudents. We find that marginally-admitted students who enrolled through the\naffirmative action tracks experienced a 14 percent increase in early-career\nearnings. But the adoption of affirmative action also caused a large decrease\nin earnings for the university's most highly-ranked students. We present\nevidence that the negative spillover effects on highly-ranked students'\nearnings were driven by both a reduction in human capital accumulation and a\ndecline in the value of networking.\n"
    },
    {
        "paper_id": 2305.02523,
        "authors": "Ke Wan and Alain Kornhauser",
        "title": "Market Making and Pricing of Financial Derivatives based on Road Travel\n  Times",
        "comments": "26 pages, 5 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
        "abstract": "  Travel time derivatives are financial instruments that derive their value\nfrom road travel times, serving as an underlying asset that cannot be directly\ntraded. Within the transportation domain, these derivatives are proposed as a\nmore comprehensive approach to value pricing. They enable road pricing based\nnot only on the level of travel time but also its volatility. In the financial\nmarket, travel time derivatives are introduced as innovative hedging\ninstruments to mitigate market risk, particularly in light of recent stress\nexperienced by the crypto market and traditional banking sector.\n  The paper focuses on three main aspects: (1) the motivation behind the\nintroduction of these derivatives, driven by the demand for hedging; (2)\nexploring the potential market for these instruments; and (3) delving into the\nproduct design and pricing schemes associated with them. The pricing schemes\nare devised by utilizing real-time travel time data captured by sensors. These\ndata are modeled using Ornstein-Uhlenbeck processes and, more broadly,\ncontinuous time autoregressive moving average (CARMA) models. The calibration\nof these models is achieved through a hidden factor model, which describes the\ndynamics of travel time processes. The risk-neutral pricing principle is then\nemployed to determine the prices of the derivatives, employing well-designed\nprocedures to identify the market value of risk.\n"
    },
    {
        "paper_id": 2305.02546,
        "authors": "Dagmara Celik Katreniak, Alexey Khazanov, Omer Moav, Zvika Neeman,\n  Hosny Zoabi",
        "title": "Why Not Borrow, Invest, and Escape Poverty?",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Take up of microcredit by the poor for investment in businesses or human\ncapital turned out to be very low. We show that this could be explained by risk\naversion, without relying on fixed costs or other forms of non-convexity in the\ntechnology, if the investment is aimed at increasing the probability of\nsuccess. Under this framework, rational risk-averse agents choose corner\nsolutions, unlike in the case of a risky investment with an exogenous\nprobability of success. Our online experiment confirms our theoretical\npredictions about how agents' choices differ when facing the two types of\ninvestments.\n"
    },
    {
        "paper_id": 2305.02552,
        "authors": "Luyao Zhang and Fan Zhang",
        "title": "Understand Waiting Time in Transaction Fee Mechanism: An\n  Interdisciplinary Perspective",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Blockchain enables peer-to-peer transactions in cyberspace without a trusted\nthird party. The rapid growth of Ethereum and smart contract blockchains\ngenerally calls for well-designed Transaction Fee Mechanisms (TFMs) to allocate\nlimited storage and computation resources. However, existing research on TFMs\nmust consider the waiting time for transactions, which is essential for\ncomputer security and economic efficiency. Integrating data from the Ethereum\nblockchain and memory pool (mempool), we explore how two types of events affect\ntransaction latency. First, we apply regression discontinuity design (RDD) to\nstudy the causal inference of the Merge, the most recent significant upgrade of\nEthereum. Our results show that the Merge significantly reduces the long\nwaiting time, network loads, and market congestion. In addition, we verify our\nresults' robustness by inspecting other compounding factors, such as censorship\nand unobserved delays of transactions via private changes. Second, examining\nthree major protocol changes during the merge, we identify block interval\nshortening as the most plausible cause for our empirical results. Furthermore,\nin a mathematical model, we show block interval as a unique mechanism design\nchoice for EIP1559 TFM to achieve better security and efficiency, generally\napplicable to the market congestion caused by demand surges. Finally, we apply\ntime series analysis to research the interaction of Non-Fungible token (NFT)\ndrops and market congestion using Facebook Prophet, an open-source algorithm\nfor generating time-series models. Our study identified NFT drops as a unique\nsource of market congestion -- holiday effects -- beyond trend and season\neffects. Finally, we envision three future research directions of TFM.\n"
    },
    {
        "paper_id": 2305.02587,
        "authors": "Ke (Amy) Ma, Sophie Yanying Sheng, Haitian Xie",
        "title": "Employer Reputation and the Labor Market: Evidence from Glassdoor.com\n  and Dice.com",
        "comments": "data access needs to be updated",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  How does employer reputation affect the labor market? We investigate this\nquestion using a novel dataset combining reviews from Glassdoor.com and job\napplications data from Dice.com. Labor market institutions such as\nGlassdoor.com crowd-sources information about employers to alleviate\ninformation problems faced by workers when choosing an employer. Raw\ncrowd-sourced employer ratings are rounded when displayed to job seekers. By\nexploiting the rounding threshold, we identify the causal impact of Glassdoor\nratings using a regression discontinuity framework. We document the effects of\nsuch ratings on both the demand and supply sides of the labor market. We find\nthat displayed employer reputation affects an employer's ability to attract\nworkers, especially when the displayed rating is \"sticky.\" Employers respond to\nhaving a rating above the rounding threshold by posting more new positions and\nre-activating more job postings. The effects are the strongest for private,\nsmaller, and less established firms, suggesting that online reputation is a\nsubstitute for other types of reputation.\n"
    },
    {
        "paper_id": 2305.02823,
        "authors": "Leland Bybee",
        "title": "Surveying Generative AI's Economic Expectations",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  I introduce a survey of economic expectations formed by querying a large\nlanguage model (LLM)'s expectations of various financial and macroeconomic\nvariables based on a sample of news articles from the Wall Street Journal\nbetween 1984 and 2021. I find the resulting expectations closely match existing\nsurveys including the Survey of Professional Forecasters (SPF), the American\nAssociation of Individual Investors, and the Duke CFO Survey. Importantly, I\ndocument that LLM based expectations match many of the deviations from\nfull-information rational expectations exhibited in these existing survey\nseries. The LLM's macroeconomic expectations exhibit under-reaction commonly\nfound in consensus SPF forecasts. Additionally, its return expectations are\nextrapolative, disconnected from objective measures of expected returns, and\nnegatively correlated with future realized returns. Finally, using a sample of\narticles outside of the LLM's training period I find that the correlation with\nexisting survey measures persists -- indicating these results do not reflect\nmemorization but generalization on the part of the LLM. My results provide\nevidence for the potential of LLMs to help us better understand human beliefs\nand navigate possible models of nonrational expectations.\n"
    },
    {
        "paper_id": 2305.03224,
        "authors": "Tianqi Pang and Kehui Tan and Chenyou Fan",
        "title": "Carbon Price Forecasting with Quantile Regression and Feature Selection",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Carbon futures has recently emerged as a novel financial asset in the trading\nmarkets such as the European Union and China. Monitoring the trend of the\ncarbon price has become critical for both national policy-making as well as\nindustrial manufacturing planning. However, various geopolitical, social, and\neconomic factors can impose substantial influence on the carbon price. Due to\nits volatility and non-linearity, predicting accurate carbon prices is\ngenerally a difficult task. In this study, we propose to improve carbon price\nforecasting with several novel practices. First, we collect various influencing\nfactors, including commodity prices, export volumes such as oil and natural\ngas, and prosperity indices. Then we select the most significant factors and\ndisclose their optimal grouping for explainability. Finally, we use the Sparse\nQuantile Group Lasso and Adaptive Sparse Quantile Group Lasso for robust price\npredictions. We demonstrate through extensive experimental studies that our\nproposed methods outperform existing ones. Also, our quantile predictions\nprovide a complete profile of future prices at different levels, which better\ndescribes the distributions of the carbon market.\n"
    },
    {
        "paper_id": 2305.03468,
        "authors": "Atilla Aras",
        "title": "Empirical Evidence for the New Definitions in Financial Markets and\n  Equity Premium Puzzle",
        "comments": "21 pages, 3 tables, 1 figure; 1 new theorem and 4 new definitions are\n  added",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  This study presents empirical evidence to support the validity of new\ndefinitions in financial markets. The author develops a new method to determine\ninvestors' risk attitudes in financial markets. The risk attitudes of investors\nin US financial markets from 1889-1978 are analyzed and the results indicate\nthat equity investors who invested in the composite S&P 500 index were\nrisk-averse in 1977. Conversely, risk-free asset investors who invested in US\nTreasury bills were found to exhibit not enough risk-loving behavior, which can\nbe considered a type of risk-averse behavior. These findings suggest that the\nnew definitions in financial markets accurately reflect the behavior of\ninvestors and should be considered in investment strategies.\n"
    },
    {
        "paper_id": 2305.03565,
        "authors": "Lorenz Riess, Mathias Beiglb\\\"ock, Johannes Temme, Andreas Wolf, Julio\n  Backhoff",
        "title": "The geometry of financial institutions -- Wasserstein clustering of\n  financial data",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  The increasing availability of granular and big data on various objects of\ninterest has made it necessary to develop methods for condensing this\ninformation into a representative and intelligible map. Financial regulation is\na field that exemplifies this need, as regulators require diverse and often\nhighly granular data from financial institutions to monitor and assess their\nactivities. However, processing and analyzing such data can be a daunting task,\nespecially given the challenges of dealing with missing values and identifying\nclusters based on specific features.\n  To address these challenges, we propose a variant of Lloyd's algorithm that\napplies to probability distributions and uses generalized Wasserstein\nbarycenters to construct a metric space which represents given data on various\nobjects in condensed form. By applying our method to the financial regulation\ncontext, we demonstrate its usefulness in dealing with the specific challenges\nfaced by regulators in this domain. We believe that our approach can also be\napplied more generally to other fields where large and complex data sets need\nto be represented in concise form.\n"
    },
    {
        "paper_id": 2305.03644,
        "authors": "Andrew Kloosterman and Peter Troyan",
        "title": "Rankings-Dependent Preferences: A Real Goods Matching Experiment",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  We investigate whether preferences for objects received via a matching\nmechanism are influenced by how highly agents rank them in their reported rank\norder list. We hypothesize that all else equal, agents receive greater utility\nfor the same object when they rank it higher. The addition of\nrankings-dependent utility implies that it may not be a dominant strategy to\nsubmit truthful preferences to a strategyproof mechanism, and that\nnon-strategyproof mechanisms that give more agents objects they \\emph{report}\nas higher ranked may increase market welfare. We test these hypotheses with a\nmatching experiment in a strategyproof mechanism, the random serial\ndictatorship, and a non-strategyproof mechanism, the Boston mechanism. A novel\nfeature of our experimental design is that the objects allocated in the\nmatching markets are real goods, which allows us to directly measure\nrankings-dependence by eliciting values for goods both inside and outside of\nthe mechanism. The experimental results are mixed, with stronger evidence for\nrankings-dependence in the RSD treatment than the Boston treatment. We find no\ndifferences between the two mechanisms for the rates of truth-telling and the\nfinal welfare.\n"
    },
    {
        "paper_id": 2305.04137,
        "authors": "Carsten H. Chong and Viktor Todorov",
        "title": "Volatility of Volatility and Leverage Effect from Options",
        "comments": null,
        "journal-ref": "Journal of Econometrics, Volume 240, Issue 1, 105669, 2024",
        "doi": "10.1016/j.jeconom.2024.105669",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We propose model-free (nonparametric) estimators of the volatility of\nvolatility and leverage effect using high-frequency observations of short-dated\noptions. At each point in time, we integrate available options into estimates\nof the conditional characteristic function of the price increment until the\noptions' expiration and we use these estimates to recover spot volatility. Our\nvolatility of volatility estimator is then formed from the sample variance and\nfirst-order autocovariance of the spot volatility increments, with the latter\ncorrecting for the bias in the former due to option observation errors. The\nleverage effect estimator is the sample covariance between price increments and\nthe estimated volatility increments. The rate of convergence of the estimators\ndepends on the diffusive innovations in the latent volatility process as well\nas on the observation error in the options with strikes in the vicinity of the\ncurrent spot price. Feasible inference is developed in a way that does not\nrequire prior knowledge of the source of estimation error that is\nasymptotically dominating.\n"
    },
    {
        "paper_id": 2305.04216,
        "authors": "Jie Dong",
        "title": "Study on the Identification of Financial Risk Path Under the Digital\n  Transformation of Enterprise Based on DEMATEL-ISM-MICMAC",
        "comments": "20 pages, 6 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Digital transformation challenges financial management while reducing costs\nand increasing efficiency for enterprises in various countries. Identifying the\ntransmission paths of enterprise financial risks in the context of digital\ntransformation is an urgent problem to be solved. This paper constructs a\nsystem of influencing factors of corporate financial risks in the new era\nthrough literature research. It proposes a path identification method of\nfinancial risks in the context of the digital transformation of enterprises\nbased on DEMATEL-ISM-MICMAC. This paper explores the intrinsic association\namong the influencing factors of corporate financial risks, identifies the key\ninfluencing factors, sorts out the hierarchical structure of the influencing\nfactor system, and analyses the dependency and driving relationships among the\nfactors in this system. The results show that: (1) The political and economic\nenvironment being not optimistic will limit the enterprise's operating ability,\nthus directly leading to the change of the enterprise's asset and liability\nstructure and working capital stock. (2) The enterprise's unreasonable talent\ntraining and incentive mechanism will limit the enterprise's technological\ninnovation ability and cause a shortage of digitally literate financial\ntalents, which eventually leads to the vulnerability of the enterprise's\nfinancial management. This study provides a theoretical reference for\nenterprises to develop risk management strategies and ideas for future academic\nresearch in digital finance.\n"
    },
    {
        "paper_id": 2305.04248,
        "authors": "Allister Loder, Fabienne Cantner, Lennart Adenaw, Markus B. Siewert,\n  Sebastian Goerg, Klaus Bogenberger",
        "title": "A nation-wide experiment, part II: the introduction of a\n  49-Euro-per-month travel pass in Germany -- An empirical study on this fare\n  innovation",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
        "abstract": "  In a response to the 2022 cost-of-living crisis in Europe, the German\ngovernment implemented a three-month fuel excise tax cut and a public transport\ntravel pass for 9 Euro per month valid on all local and regional services.\nFollowing this period, a public debate immediately emerged on a successor to\nthe so-called \"9-Euro-Ticket\", leading to the political decision of introducing\na similar ticket priced at 49 Euro per month in May 2023, the so-called\n\"Deutschlandticket\". We observe this introduction of the new public transport\nticket with a sample of 818 participants using a smartphone-based travel diary\nwith passive tracking and a two-wave survey. The sample comprises 510 remaining\nparticipants of our initial \"9-Euro-Ticket study from 2022 and 308 participants\nrecruited in March and early April 2023. In this report we report on the status\nof the panel before the introduction of the \"Deutschlandticket\".\n"
    },
    {
        "paper_id": 2305.04348,
        "authors": "Jinkai Li",
        "title": "A Scoping Review of Internal Migration and Left-behind Children's\n  Wellbeing in China",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Children's well-being of immigrants is facing several challenges related to\nphysical, mental, and educational risks, which may obstacle human capital\naccumulation and further development. In rural China, due to the restriction of\nHukou registration system, nearly 9 million left-behind children (LBC) are in\nlack of parental care and supervision in 2020 when their parents internally\nmigrate out for work. Through the systematic scoping review, this study\nprovides a comprehensive literature summary and concludes the overall negative\neffects of parental migration on LBC's physical, mental (especially for\nleft-behind girls), and educational outcomes (especially for left-behind boys).\nNoticeably, both parents' and mother's migration may exacerbate LBC's\ndisadvantages. Furthermore, remittance from migrants and more family-level and\nsocial support may help mitigate the negative influence. Finally, we put\nforward theoretical and realistic implications which may shed light on\npotential research directions. Further studies, especially quantitative\nstudies, are needed to conduct a longitudinal survey, combine the ongoing Hukou\nreform in China, and simultaneously focus on left-behind children and migrant\nchildren.\n"
    },
    {
        "paper_id": 2305.045,
        "authors": "Takeshi Kato",
        "title": "Well-being policy evaluation methodology based on WE pluralism",
        "comments": "18 pages, 6 figures",
        "journal-ref": "Contemporary and Applied Philosophy 2023, 14: 159-175",
        "doi": "10.14989/285977",
        "license": "http://creativecommons.org/licenses/by-sa/4.0/",
        "abstract": "  Methodologies for evaluating and selecting policies that contribute to the\nwell-being of diverse populations need clarification. To bridge the gap between\nobjective indicators and policies related to well-being, this study shifts from\nconstitutive pluralism based on objective indicators to conceptual pluralism\nthat emphasizes subjective context, develops from subject-object pluralism\nthrough individual-group pluralism to WE pluralism, and presents a new policy\nevaluation method that combines joint fact-finding based on policy plurality.\nFirst, to evaluate policies involving diverse stakeholders, I develop from\nindividual subjectivity-objectivity to individual subjectivity and group\nintersubjectivity, and then move to a narrow-wide WE pluralism in the gradation\nof I-family-community-municipality-nation-world. Additionally, by referring to\nsome functional forms of well-being, I formulate the dependence of well-being\non narrow-wide WE. Finally, given that policies themselves have a plurality of\nsocial, ecological, and economic values, I define a set of policies for each of\nthe narrow-wide WE and consider a mapping between the two to provide an\nevaluation basis. Furthermore, by combining well-being and joint fact-finding\non the narrow-wide WE consensus, the policy evaluation method is formulated.\nThe fact-value combined parameter system, combined policy-making approach, and\ncombined impact evaluation are disclosed as examples of implementation. This\npaper contributes to the realization of a well-being society by bridging\nphilosophical theory and policies based on WE pluralism and presenting a new\nmethod of policy evaluation based on subjective context and consensus building.\n"
    },
    {
        "paper_id": 2305.04703,
        "authors": "Jones Pontoh",
        "title": "Why Students Trade? The Analysis of Young Investors behavior",
        "comments": "6 pages, 1 table",
        "journal-ref": "International Research Journal of Economics and Management\n  Studies, Vol. 2, No. 2, pp. 87-92, 2023",
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  Interestingly the numbers of young traders in Jakarta Stock Exchange had been\nincreasing in recent years. Even in the middle of the global crisis caused by\ncovid19 pandemic, in December 2021 according to KSEI, Individual investors were\ndominated by young investors. Data presented by KSEI showed that 60 percent of\nthe investors listed in Indonesian Stock Exchange were young investors. Other\ndata shows that 28 percent of the investors listed were shockingly students. It\nwas interesting to study the behavior of young and Rookie investors at the\nbranch of stock market in Manado State University. Basically, they varied in\nhow to make decision to trade on the stock exchange. The problems were\ndiscussed by qualitative approach. Descriptive analysis was conducted prior to\ninterviews. Data will be collected through data observation techniques and\ninterviews. The study succeeded in investigating the investment behavior of\nyoung or Rookie investors at Manado State University in accordance with\ninvestment decision making and the perception of behavioral control. The\nperception of behavioral control greatly influenced investors decision making.\nStudents were greatly influenced by lecturer, friends and more experienced\ninvestors. The results of the interview provide information that before they\ndetermine their behavior, first they do stock analysis, both technical and\nfundamental analysis. These facts shows that students investors were well\nliterate.\n"
    },
    {
        "paper_id": 2305.04748,
        "authors": "Snezhana Kirusheva and Thomas S. Salisbury",
        "title": "A greedy algorithm for habit formation under multiplicative utility",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  We consider the problem of optimizing lifetime consumption under a habit\nformation model, both with and without an exogenous pension. Unlike much of the\nexisting literature, we apply a power utility to the ratio of consumption to\nhabit, rather than to their difference. The martingale/duality method becomes\nintractable in this setting, so we develop a greedy version of this method that\nis solvable using Monte Carlo simulation. We investigate the behaviour of the\ngreedy solution, and explore what parameter values make the greedy solution a\ngood approximation to the optimal one.\n"
    },
    {
        "paper_id": 2305.04801,
        "authors": "Ali Shirazi, Fereshteh Sadeghi Naieni Fard",
        "title": "Financial Hedging and Risk Compression, A journey from linear regression\n  to neural network",
        "comments": "15 pages, 9 Figures, 2 Tables",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-sa/4.0/",
        "abstract": "  Finding the hedge ratios for a portfolio and risk compression is the same\nmathematical problem. Traditionally, regression is used for this purpose.\nHowever, regression has its own limitations. For example, in a regression\nmodel, we can't use highly correlated independent variables due to\nmulticollinearity issue and instability in the results. A regression model\ncannot also consider the cost of hedging in the hedge ratios estimation. We\nhave introduced several methods that address the linear regression limitation\nwhile achieving better performance. These models, in general, fall into two\ncategories: Regularization Techniques and Common Factor Analyses. In\nregularization techniques, we minimize the variance of hedged portfolio profit\nand loss (PnL) and the hedge ratio sizes, which helps reduce the cost of\nhedging. The regularization techniques methods could also consider the cost of\nhedging as a function of the cost of funding, market condition, and liquidity.\nIn common factor analyses, we first map variables into common factors and then\nfind the hedge ratios so that the hedged portfolio doesn't have any exposure to\nthe factors. We can use linear or nonlinear factors construction. We are\nintroducing a modified beta variational autoencoder that constructs common\nfactors nonlinearly to compute hedges. Finally, we introduce a comparison\nmethod and generate numerical results for an example.\n"
    },
    {
        "paper_id": 2305.04811,
        "authors": "Cheng Zhang, Nilam Nur Amir Sjarif, Roslina Ibrahim",
        "title": "Deep learning models for price forecasting of financial time series: A\n  review of recent advancements: 2020-2022",
        "comments": "37 pages, 19 figures, 5 tables",
        "journal-ref": null,
        "doi": "10.1002/widm.1519",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Accurately predicting the prices of financial time series is essential and\nchallenging for the financial sector. Owing to recent advancements in deep\nlearning techniques, deep learning models are gradually replacing traditional\nstatistical and machine learning models as the first choice for price\nforecasting tasks. This shift in model selection has led to a notable rise in\nresearch related to applying deep learning models to price forecasting,\nresulting in a rapid accumulation of new knowledge. Therefore, we conducted a\nliterature review of relevant studies over the past three years with a view to\naiding researchers and practitioners in the field. This review delves deeply\ninto deep learning-based forecasting models, presenting information on model\narchitectures, practical applications, and their respective advantages and\ndisadvantages. In particular, detailed information is provided on advanced\nmodels for price forecasting, such as Transformers, generative adversarial\nnetworks (GANs), graph neural networks (GNNs), and deep quantum neural networks\n(DQNNs). The present contribution also includes potential directions for future\nresearch, such as examining the effectiveness of deep learning models with\ncomplex structures for price forecasting, extending from point prediction to\ninterval prediction using deep learning models, scrutinising the reliability\nand validity of decomposition ensembles, and exploring the influence of data\nvolume on model performance.\n"
    },
    {
        "paper_id": 2305.04838,
        "authors": "Kai Ren",
        "title": "An Empirical Study of Capital Asset Pricing Model based on Chinese\n  A-share Trading Data",
        "comments": "28 pages, in Chinese language, 3 figures, 12 tables",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/publicdomain/zero/1.0/",
        "abstract": "  This paper presents an empirical analysis of the capital asset pricing model\nusing trading data for the Chinese A-share market from 2000 to 2019. Firstly,\nthe standard CAPM is tested using a Fama-MacBetch regression and although the\nresults successfully test the three core hypotheses, the resulting beta risk\ndoes not have a significant impact on returns. Secondly, the Fama-French\nthree-factor model, which uses a combination of market, size and value factors\nto price capital assets, is analysed, showing that it is able to capture most\nof the variation in A-share market returns, with an adjusted R-squared greater\nthan 0.88 for the 25 portfolios constructed. Finally, the paper takes into\naccount the \"shell value contamination\" problem caused by IPO regulation in the\nChinese stock market, and minimises its impact by excluding stocks in the\nlowest 30% of the market capitalisation, which allows some of the anomalous\nresults in the three-factor model to be effectively corrected. Although this\npaper does not present a more innovative approach, it is unique in its data\nselection and presents a detailed presentation of the data processing and\nregression analysis process, which 1) illustrates the applicability of capital\nasset pricing models in the Chinese market; and 2) provides a set of open\nsource materials for the basic learning of capital asset pricing models.\n"
    },
    {
        "paper_id": 2305.04865,
        "authors": "Zlata Tabachov\\'a, Christian Diem, Andr\\'as Borsos, Csaba Burger,\n  Stefan Thurner",
        "title": "Estimating the impact of supply chain network contagion on financial\n  stability",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  Realistic credit risk assessment, the estimation of losses from\ncounterparty's failure, is central for the financial stability. Credit risk\nmodels focus on the financial conditions of borrowers and only marginally\nconsider other risks from the real economy, supply chains in particular. Recent\npandemics, geopolitical instabilities, and natural disasters demonstrated that\nsupply chain shocks do contribute to large financial losses. Based on a unique\nnation-wide micro-dataset, containing practically all supply chain relations of\nall Hungarian firms, together with their bank loans, we estimate how\nfirm-failures affect the supply chain network, leading to potentially\nadditional firm defaults and additional financial losses. Within a multi-layer\nnetwork framework we define a financial systemic risk index (FSRI) for every\nfirm, quantifying these expected financial losses caused by its own- and all\nthe secondary defaulting loans caused by supply chain network (SCN) shock\npropagation. We find a small fraction of firms carrying substantial financial\nsystemic risk, affecting up to 16% of the banking system's overall equity.\nThese losses are predominantly caused by SCN contagion. For every bank we\ncalculate the expected loss (EL), value at risk (VaR) and expected shortfall\n(ES), with and without accounting for SCN contagion. We find that SCN contagion\namplifies the EL, VaR, and ES by a factor of 4.3, 4.5, and 3.2, respectively.\nThese findings indicate that for a more complete picture of financial stability\nand realistic credit risk assessment, SCN contagion needs to be considered.\nThis newly quantified contagion channel is of potential relevance for\nregulators' future systemic risk assessments.\n"
    },
    {
        "paper_id": 2305.04884,
        "authors": "Marcell T. Kurbucz, P\\'eter P\\'osfay, Antal Jakov\\'ac",
        "title": "Predicting the Price Movement of Cryptocurrencies Using Linear Law-based\n  Transformation",
        "comments": "Manuscript: 9 pages, 1 figure, 1 table; Supplementary material: 33\n  pages, 64 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  The aim of this paper is to investigate the effect of a novel method called\nlinear law-based feature space transformation (LLT) on the accuracy of intraday\nprice movement prediction of cryptocurrencies. To do this, the 1-minute\ninterval price data of Bitcoin, Ethereum, Binance Coin, and Ripple between 1\nJanuary 2019 and 22 October 2022 were collected from the Binance cryptocurrency\nexchange. Then, 14-hour nonoverlapping time windows were applied to sample the\nprice data. The classification was based on the first 12 hours, and the two\nclasses were determined based on whether the closing price rose or fell after\nthe next 2 hours. These price data were first transformed with the LLT, then\nthey were classified by traditional machine learning algorithms with 10-fold\ncross-validation. Based on the results, LLT greatly increased the accuracy for\nall cryptocurrencies, which emphasizes the potential of the LLT algorithm in\npredicting price movements.\n"
    },
    {
        "paper_id": 2305.04967,
        "authors": "Ashish Dhiman",
        "title": "UQ for Credit Risk Management: A deep evidence regression approach",
        "comments": "9 pages, plus references",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Machine Learning has invariantly found its way into various Credit Risk\napplications. Due to the intrinsic nature of Credit Risk, quantifying the\nuncertainty of the predicted risk metrics is essential, and applying\nuncertainty-aware deep learning models to credit risk settings can be very\nhelpful. In this work, we have explored the application of a scalable UQ-aware\ndeep learning technique, Deep Evidence Regression and applied it to predicting\nLoss Given Default. We contribute to the literature by extending the Deep\nEvidence Regression methodology to learning target variables generated by a\nWeibull process and provide the relevant learning framework. We demonstrate the\napplication of our approach to both simulated and real-world data.\n"
    },
    {
        "paper_id": 2305.05516,
        "authors": "Fulin Guo",
        "title": "GPT in Game Theory Experiments",
        "comments": "updated to use GPT-4 instead of GPT-3.5 and added reasoning analysis",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper explores the use of Generative Pre-trained Transformers (GPT) in\nstrategic game experiments, specifically the ultimatum game and the prisoner's\ndilemma. I designed prompts and architectures to enable GPT to understand the\ngame rules and to generate both its choices and the reasoning behind decisions.\nThe key findings show that GPT exhibits behaviours similar to human responses,\nsuch as making positive offers and rejecting unfair ones in the ultimatum game,\nalong with conditional cooperation in the prisoner's dilemma. The study\nexplores how prompting GPT with traits of fairness concern or selfishness\ninfluences its decisions. Notably, the \"fair\" GPT in the ultimatum game tends\nto make higher offers and reject offers more frequently compared to the\n\"selfish\" GPT. In the prisoner's dilemma, high cooperation rates are maintained\nonly when both GPT players are \"fair\". The reasoning statements GPT produces\nduring gameplay reveal the underlying logic of certain intriguing patterns\nobserved in the games. Overall, this research shows the potential of GPT as a\nvaluable tool in social science research, especially in experimental studies\nand social simulations.\n"
    },
    {
        "paper_id": 2305.05663,
        "authors": "S. Gerber, H. Markowitz, P. Ernst, Y. Miao, B. Javid, and P. Sargen",
        "title": "Proofs that the Gerber Statistic is Positive Semidefinite",
        "comments": "5 pages, 0 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  In this brief note, we prove that both forms of the Gerber statistic\nintroduced in Gerber et al. (2022) are positive semi-definite.\n"
    },
    {
        "paper_id": 2305.05751,
        "authors": "Stanis{\\l}aw Dro\\.zd\\.z and Jaros{\\l}aw Kwapie\\'n and Marcin\n  W\\k{a}torek",
        "title": "What is mature and what is still emerging in the cryptocurrency market?",
        "comments": null,
        "journal-ref": "Entropy 2023, 25(5), 772",
        "doi": "10.3390/e25050772",
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  In relation to the traditional financial markets, the cryptocurrency market\nis a recent invention and the trading dynamics of all its components are\nreadily recorded and stored. This fact opens up a unique opportunity to follow\nthe multidimensional trajectory of its development since inception up to the\npresent time. Several main characteristics commonly recognized as financial\nstylized facts of mature markets were quantitatively studied here. In\nparticular, it is shown that the return distributions, volatility clustering\neffects, and even temporal multifractal correlations for a few\nhighest-capitalization cryptocurrencies largely follow those of the\nwell-established financial markets. The smaller cryptocurrencies are somewhat\ndeficient in this regard, however. They are also not as highly cross-correlated\namong themselves and with other financial markets as the large\ncryptocurrencies. Quite generally, the volume V impact on price changes R\nappears to be much stronger on the cryptocurrency market than in the mature\nstock markets, and scales as $R(V) \\sim V^{\\alpha}$ with $\\alpha \\gtrsim 1$.\n"
    },
    {
        "paper_id": 2305.05762,
        "authors": "Ignacio Escanuela Romana and Clara Escanuela Nieves",
        "title": "A spectral approach to stock market performance",
        "comments": "26 pages, 13 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We pose the estimation and predictability of stock market performance. Three\ncases are taken: US, Japan, Germany, the monthly index of the value of realized\ninvestment in stocks, prices plus the value of dividend payments (OECD data).\nOnce deflated and trend removed, harmonic analysis is applied. The series are\ntaken with and without the periods with evidence of exogenous shocks. The\nseries are erratic and the random walk hypothesis is reasonably falsified. The\nestimation reveals relevant hidden periodicities, which approximate stock value\nmovements. From July 2008 onwards, it is successfully analyzed whether the\nsubsequent fall in share value would have been predictable. Again, the data are\nirregular and scattered, but the sum of the first five harmonics in relevance\nanticipates the fall in stock market values that followed.\n"
    },
    {
        "paper_id": 2305.05998,
        "authors": "Koichiro Moriya and Akihiko Noda",
        "title": "On the Time-Varying Structure of the Arbitrage Pricing Theory using the\n  Japanese Sector Indices",
        "comments": "18 pages, 2 tables, 2 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  This paper is the first study to examine the time instability of the APT in\nthe Japanese stock market. In particular, we measure how changes in each risk\nfactor affect the stock risk premiums to investigate the validity of the APT\nover time, applying the rolling window method to Fama and MacBeth's (1973)\ntwo-step regression and Kamstra and Shi's (2023) generalized GRS test. We\nsummarize our empirical results as follows: (1) the changes in monetary policy\nby major central banks greatly affect the validity of the APT in Japan, and (2)\nthe time-varying estimates of the risk premiums for each factor are also\nunstable over time, and they are affected by the business cycle and economic\ncrises. Therefore, we conclude that the validity of the APT as an appropriate\nmodel to explain the Japanese sector index is not stable over time.\n"
    },
    {
        "paper_id": 2305.06215,
        "authors": "Mariano Zeron, Meng Wu, Ignacio Ruiz",
        "title": "The FRTB-IMA computational challenge for Equity Autocallables",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  When the Orthogonal Chebyshev Sliding Technique was introduced it was applied\nto a portfolio of swaps and swaptions within the context of the FRTB-IMA\ncapital calculation. The computational cost associated to the computation of\nthe ES values - an essential component of the capital caluclation under\nFRTB-IMA - was reduced by more than $90\\%$ while passing PLA tests.\n  This paper extends the use of the Orthogonal Chebyshev Sliding Technique to\nportfolios of equity autocallables defined over a range of spot underlyings.\nResults are very positive as computational reductions are of about $90\\%$ with\npassing PLA metrics.\n  Since equity autocallables are a commonly traded exotic trade type, with\nsignificant FRTB-IMA computational costs, the extension presented in this paper\nconstitutes an imporant step forward in tackling the computational challenges\nassociated to an efficient FRTB-IMA implementation.\n"
    },
    {
        "paper_id": 2305.06284,
        "authors": "Laura Garcia-Herrero, Stevo Lavrnic, Valentina Guerrieri, Attilio\n  Toscano, Mirco Milani, Giuseppe Luigi Cirelli, Matteo Vittuari",
        "title": "Cost-benefit of green infrastructures for water management: A\n  sustainability assessment of full-scale constructed wetlands in Northern and\n  Southern Italy",
        "comments": null,
        "journal-ref": "Ecological Engineering 2022",
        "doi": "10.1016/j.ecoleng.2022.106797",
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  Sustainable water management has become an urgent challenge due to irregular\nwater availability patterns and water quality issues. The effect of climate\nchange exacerbates this phenomenon in water-scarce areas, such as the\nMediterranean region, stimulating the implementation of solutions aiming to\nmitigate or improve environmental, social, and economic conditions. A novel\nsolution inspired by nature, technology-oriented, explored in the past years,\nis constructed wetlands. Commonly applied for different types of wastewater due\nto its low cost and simple maintenance, they are considered a promising\nsolution to remove pollutants while creating an improved ecosystem by\nincreasing biodiversity around them. This research aims to assess the\nsustainability of two typologies of constructed wetlands in two Italian areas:\nSicily, with a vertical subsurface flow constructed wetland, and Emilia\nRomagna, with a surface flow constructed wetland. The assessment is performed\nby applying a cost-benefit analysis combining primary and secondary data\nsources. The analysis considered the market and non-market values in both\nproposed scenarios to establish the feasibility of the two options and identify\nthe most convenient one. Results show that both constructed wetlands bring more\nbenefits (benefits-cost ratio, BCR) than costs (BCR > 0). In the case of\nSicily, the BCR is lower (1) in the constructed wetland scenario, while in its\nabsence it is almost double. If other ecosystem services are included the\nconstructed wetland scenario reach a BCR of 4 and a ROI of 5, showing a better\nperformance from a costing perspective than the absence one. In Emilia Romagna,\nthe constructed wetland scenario shows a high BCR (10) and ROI (9), while the\nscenario in absence has obtained a negative present value indicating that the\ncost do not cover the benefits expected.\n"
    },
    {
        "paper_id": 2305.06337,
        "authors": "Kamil Fortuna, Janusz Szwabi\\'nski",
        "title": "The Unified Framework for Modelling Credit Cycles with Marshall-Walras\n  Price Formation Process And Systemic Risk Assessment",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Systemic risk is a rapidly developing area of research. Classical financial\nmodels often do not adequately reflect the phenomena of bubbles, crises, and\ntransitions between them during credit cycles. To study very improbable events,\nsystemic risk methodologies utilise advanced mathematical and computational\ntools, such as complex systems, chaos theory, and Monte Carlo simulations. In\nthis paper, a relatively simple mathematical formalism is applied to provide a\nunified framework for modeling credit cycles and systemic risk assessment. The\nproposed model is analyzed in detail to assess whether it can reflect very\ndifferent states of the economy. Basing on those results, measures of systemic\nrisk are constructed to provide information regarding the stability of the\nsystem. The formalism is then applied to describe the full credit cycle with\nthe explanation of causal relationships between the phases expressed in terms\nof parameters derived from real-world quantities. The framework can be\nnaturally interpreted and understood with respect to different economic\nsituations and easily incorporated into the analysis and decision-making\nprocess based on classical models, significantly enhancing their quality and\nflexibility.\n"
    },
    {
        "paper_id": 2305.06704,
        "authors": "Yichi Zhang, Mihai Cucuringu, Alexander Y. Shestopaloff, Stefan Zohren",
        "title": "Robust Detection of Lead-Lag Relationships in Lagged Multi-Factor Models",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In multivariate time series systems, key insights can be obtained by\ndiscovering lead-lag relationships inherent in the data, which refer to the\ndependence between two time series shifted in time relative to one another, and\nwhich can be leveraged for the purposes of control, forecasting or clustering.\nWe develop a clustering-driven methodology for robust detection of lead-lag\nrelationships in lagged multi-factor models. Within our framework, the\nenvisioned pipeline takes as input a set of time series, and creates an\nenlarged universe of extracted subsequence time series from each input time\nseries, via a sliding window approach. This is then followed by an application\nof various clustering techniques, (such as k-means++ and spectral clustering),\nemploying a variety of pairwise similarity measures, including nonlinear ones.\nOnce the clusters have been extracted, lead-lag estimates across clusters are\nrobustly aggregated to enhance the identification of the consistent\nrelationships in the original universe. We establish connections to the\nmultireference alignment problem for both the homogeneous and heterogeneous\nsettings. Since multivariate time series are ubiquitous in a wide range of\ndomains, we demonstrate that our method is not only able to robustly detect\nlead-lag relationships in financial markets, but can also yield insightful\nresults when applied to an environmental data set.\n"
    },
    {
        "paper_id": 2305.06805,
        "authors": "Ludovic Gouden\\`ege, Andrea Molent, Antonino Zanette",
        "title": "Backward Hedging for American Options with Transaction Costs",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  In this article, we introduce an algorithm called Backward Hedging, designed\nfor hedging European and American options while considering transaction costs.\nThe optimal strategy is determined by minimizing an appropriate loss function,\nwhich is based on either a risk measure or the mean squared error of the\nhedging strategy at maturity. The proposed algorithm moves backward in time,\ndetermining, for each time-step and different market states, the optimal\nhedging strategy that minimizes the loss function at the time the option is\nexercised, by assuming that the strategy used in the future for hedging the\nliability is the one determined at the previous steps of the algorithm. The\napproach avoids machine learning and instead relies on classic optimization\ntechniques, Monte Carlo simulations, and interpolations on a grid. Comparisons\nwith the Deep Hedging algorithm in various numerical experiments showcase the\nefficiency and accuracy of the proposed method.\n"
    },
    {
        "paper_id": 2305.06888,
        "authors": "Lorenzo Lucchini, Ollin Langle-Chimal, Lorenzo Candeago, Lucio Melito,\n  Alex Chunet, Aleister Montfort, Bruno Lepri, Nancy Lozano-Gracia, and Samuel\n  P. Fraiberger",
        "title": "Socioeconomic disparities in mobility behavior during the COVID-19\n  pandemic in developing countries",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
        "abstract": "  Mobile phone data have played a key role in quantifying human mobility during\nthe COVID-19 pandemic. Existing studies on mobility patterns have primarily\nfocused on regional aggregates in high-income countries, obfuscating the\naccentuated impact of the pandemic on the most vulnerable populations. By\ncombining geolocation data from mobile phones and population census for 6\nmiddle-income countries across 3 continents between March and December 2020, we\nuncovered common disparities in the behavioral response to the pandemic across\nsocioeconomic groups. When the pandemic hit, urban users living in low-wealth\nneighborhoods were less likely to respond by self-isolating at home, relocating\nto rural areas, or refraining from commuting to work. The gap in the behavioral\nresponses between socioeconomic groups persisted during the entire observation\nperiod. Among low-wealth users, those who used to commute to work in\nhigh-wealth neighborhoods pre-pandemic were particularly at risk, facing both\nthe reduction in activity in high-wealth neighborhood and being more likely to\nbe affected by public transport closures due to their longer commute. While\nconfinement policies were predominantly country-wide, these results suggest a\nrole for place-based policies informed by mobility data to target aid to the\nmost vulnerable.\n"
    },
    {
        "paper_id": 2305.06961,
        "authors": "Masood Tadi, Ji\\v{r}\\'i Witzany",
        "title": "Copula-Based Trading of Cointegrated Cryptocurrency Pairs",
        "comments": "29 pages, 3 figures, 10 tables",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  This research introduces a novel pairs trading strategy based on copulas for\ncointegrated pairs of cryptocurrencies. To identify the most suitable pairs,\nthe study employs linear and non-linear cointegration tests along with a\ncorrelation coefficient measure and fits different copula families to generate\ntrading signals formulated from a reference asset for analyzing the mispricing\nindex. The strategy's performance is then evaluated by conducting back-testing\nfor various triggers of opening positions, assessing its returns and risks. The\nfindings indicate that the proposed method outperforms buy-and-hold trading\nstrategies in terms of both profitability and risk-adjusted returns.\n"
    },
    {
        "paper_id": 2305.07166,
        "authors": "Mengge Li, Shuaijie Qian and Chao Zhou",
        "title": "Robust Equilibrium Strategy for Mean-Variance Portfolio Selection",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The classical mean-variance portfolio selection problem induces\ntime-inconsistent (precommited) strategies (see Zhou and Li (2000)). To\novercome this time-inconsistency, Basak and Chabakauri (2010) introduce the\ngame theoretical approach and look for (sub-game perfect Nash) equilibrium\nstrategies, which is solved from the corresponding partial differential\nequations (PDE) system. In their model, the investor perfectly knows the drift\nand volatility of the assets. However, in reality investors only have an\nestimate on them, e.g, a 95% confidence interval. In this case, some literature\n(e.g., Pham, Wei and Zhou (2022)) derives the optimal precommited strategy\nunder the worst parameters, which is the robust control. The relation between\nthe equilibrium strategy and the PDE system has not been justified when\nincorporating robust control. In this paper, we consider a general dynamic\nmean-variance framework and propose a novel definition of the robust\nequilibrium strategy. Under our definition, a classical solution to the\ncorresponding PDE system implies a robust equilibrium strategy. We then\nexplicitly solve for some special examples.\n"
    },
    {
        "paper_id": 2305.07179,
        "authors": "Amine Ouazad and Matthew E. Kahn",
        "title": "Mortgage Securitization Dynamics in the Aftermath of Natural Disasters:\n  A Reply",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Climate change poses new risks for real estate assets. Given that the\nmajority of home buyers use a loan to pay for their homes and the majority of\nthese loans are purchased by the Government Sponsored Enterprises (GSEs), it is\nimportant to understand how rising natural disaster risk affects the mortgage\nfinance market. The climate securitization hypothesis (CSH) posits that, in the\naftermath of natural disasters, lenders strategically react to the GSEs\nconforming loan securitization rules that create incentives that foster both\nmoral hazard and adverse selection effects. The climate risks bundled into GSE\nmortgage-backed securities emerge because of the complex securitization chain\nthat creates weak monitoring and screening incentives. We survey the recent\ntheoretical literature and empirical literature exploring screening incentive\neffects. Using regression discontinuity methods, we test key hypotheses\npresented in the securitization literature with a focus on securitization\ndynamics immediately after major hurricanes. Our evidence supports the CSH. We\naddress the data construction issues posed by LaCour-Little et. al. and show\nthat their concerns do not affect our main results. Under the current rules of\nthe game, climate risks exacerbates the established lemons problem commonly\nfound in loan securitization markets.\n"
    },
    {
        "paper_id": 2305.07318,
        "authors": "Peiyu Jing, Ravi Seshadri, Takanori Sakai, Ali Shamshiripour, Andre\n  Romano Alho, Antonios Lentzakis, Moshe E. Ben-Akiva",
        "title": "Evaluating congestion pricing schemes using agent-based passenger and\n  freight microsimulation",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  The distributional impacts of congestion pricing have been widely studied in\nthe literature and the evidence on this is mixed. Some studies find that\npricing is regressive whereas others suggest that it can be progressive or\nneutral depending on the specific spatial characteristics of the urban region,\nexisting activity and travel patterns, and the design of the pricing scheme.\nMoreover, the welfare and distributional impacts of pricing have largely been\nstudied in the context of passenger travel whereas freight has received\nrelatively less attention. In this paper, we examine the impacts of several\nthird-best congestion pricing schemes on both passenger transport and freight\nin an integrated manner using a large-scale microsimulator (SimMobility) that\nexplicitly simulates the behavioral decisions of the entire population of\nindividuals and business establishments, dynamic multimodal network\nperformance, and their interactions. Through simulations of a prototypical\nNorth American city, we find that a distance-based pricing scheme yields the\nlargest welfare gains, although the gains are a modest fraction of toll\nrevenues (around 30\\%). In the absence of revenue recycling or redistribution,\ndistance-based and cordon-based schemes are found to be particularly\nregressive. On average, lower income individuals lose as a result of the\nscheme, whereas higher income individuals gain. A similar trend is observed in\nthe context of shippers -- small establishments having lower shipment values\nlose on average whereas larger establishments with higher shipment values gain.\nWe perform a detailed spatial analysis of distributional outcomes, and examine\nthe impacts on network performance, activity generation, mode and departure\ntime choices, and logistics operations.\n"
    },
    {
        "paper_id": 2305.07352,
        "authors": "Stephan Leitner",
        "title": "Building resilient organizations: The roles of top-down vs. bottom-up\n  organizing",
        "comments": "14 pages, 1 figure, 3 tables",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Organizations face numerous challenges posed by unexpected events such as\nenergy price hikes, pandemic disruptions, terrorist attacks, and natural\ndisasters, and the factors that contribute to organizational success in dealing\nwith such disruptions often remain unclear. This paper analyzes the roles of\ntop-down and bottom-up organizational structures in promoting organizational\nresilience. To do so, an agent-based model of stylized organizations is\nintroduced that features learning, adaptation, different modes of organizing,\nand environmental disruptions. The results indicate that bottom-up designed\norganizations tend to have a higher ability to absorb the effects of\nenvironmental disruptions, and situations are identified in which either\ntop-down or bottom-up designed organizations have an advantage in recovering\nfrom shocks.\n"
    },
    {
        "paper_id": 2305.07362,
        "authors": "Matthias Raddant and Martin Bertau and Gerald Steiner",
        "title": "The use of trade data in the analysis of global phosphate flows",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper we present a new method to trace the flows of phosphate from\nthe countries where it is mined to the countries where it is used in\nagricultural production. We achieve this by combining data on phosphate rock\nmining with data on fertilizer use and data on international trade of\nphosphate-related products. We show that by making adjustments to data on net\nexports and by estimating weighting factors we can derive the matrix of\nphosphate flows on the country level to a large degree and thus contribute to\nthe accuracy of material flow analyses, a results that is important for\nimproving environmental accounting, not only for phosphorus but for many other\nresources.\n"
    },
    {
        "paper_id": 2305.07466,
        "authors": "Nadeem Malibari, Iyad Katib and Rashid Mehmood",
        "title": "Systematic Review on Reinforcement Learning in the Field of Fintech",
        "comments": "31 pages, 15 figures, 7 tables",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Applications of Reinforcement Learning in the Finance Technology (Fintech)\nhave acquired a lot of admiration lately. Undoubtedly Reinforcement Learning,\nthrough its vast competence and proficiency, has aided remarkable results in\nthe field of Fintech. The objective of this systematic survey is to perform an\nexploratory study on a correlation between reinforcement learning and Fintech\nto highlight the prediction accuracy, complexity, scalability, risks,\nprofitability and performance. Major uses of reinforcement learning in finance\nor Fintech include portfolio optimization, credit risk reduction, investment\ncapital management, profit maximization, effective recommendation systems, and\nbetter price setting strategies. Several studies have addressed the actual\ncontribution of reinforcement learning to the performance of financial\ninstitutions. The latest studies included in this survey are publications from\n2018 onward. The survey is conducted using PRISMA technique which focuses on\nthe reporting of reviews and is based on a checklist and four-phase flow\ndiagram. The conducted survey indicates that the performance of RL-based\nstrategies in Fintech fields proves to perform considerably better than other\nstate-of-the-art algorithms. The present work discusses the use of\nreinforcement learning algorithms in diverse decision-making challenges in\nFintech and concludes that the organizations dealing with finance can benefit\ngreatly from Robo-advising, smart order channelling, market making, hedging and\noptions pricing, portfolio optimization, and optimal execution.\n"
    },
    {
        "paper_id": 2305.07559,
        "authors": "Christopher J. Cho, Timothy J. Norman, Manuel Nunes",
        "title": "PRIME: A Price-Reverting Impact Model of a cryptocurrency Exchange",
        "comments": "Pre-print for the Cryptocurrency Research Conference 2023",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  In a financial exchange, market impact is a measure of the price change of an\nasset following a transaction. This is an important element of market\nmicrostructure, which determines the behaviour of the market following a trade.\nIn this paper, we first provide a discussion on the market impact observed in\nthe BTC/USD Futures market, then we present a novel multi-agent market\nsimulation that can follow an underlying price series, whilst maintaining the\nability to reproduce the market impact observed in the market in an explainable\nmanner. This simulation of the financial exchange allows the model to interact\nrealistically with market participants, helping its users better estimate\nmarket slippage as well as the knock-on consequences of their market actions.\nIn turn, it allows various stakeholders such as industrial practitioners,\ngovernments and regulators to test their market hypotheses, without deploying\ncapital or destabilising the system.\n"
    },
    {
        "paper_id": 2305.0797,
        "authors": "Steve Phelps and Yvan I. Russell",
        "title": "The Machine Psychology of Cooperation: Can GPT models operationalise\n  prompts for altruism, cooperation, competitiveness and selfishness in\n  economic games?",
        "comments": "38 pages, 11 figures. For associated code see\n  https://github.com/phelps-sg/llm-cooperation",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  We investigated the capability of the GPT-3.5 large language model (LLM) to\noperationalize natural language descriptions of cooperative, competitive,\naltruistic, and self-interested behavior in two social dilemmas: the repeated\nPrisoners Dilemma and the one-shot Dictator Game. Using a within-subject\nexperimental design, we used a prompt to describe the task environment using a\nsimilar protocol to that used in experimental psychology studies with human\nsubjects. We tested our research question by manipulating the part of our\nprompt which was used to create a simulated persona with different cooperative\nand competitive stances. We then assessed the resulting simulacras' level of\ncooperation in each social dilemma, taking into account the effect of different\npartner conditions for the repeated game. Our results provide evidence that\nLLMs can, to some extent, translate natural language descriptions of different\ncooperative stances into corresponding descriptions of appropriate task\nbehaviour, particularly in the one-shot game. There is some evidence of\nbehaviour resembling conditional reciprocity for the cooperative simulacra in\nthe repeated game, and for the later version of the model there is evidence of\naltruistic behaviour. Our study has potential implications for using LLM\nchatbots in task environments that involve cooperation, e.g. using chatbots as\nmediators and facilitators in public-goods negotiations.\n"
    },
    {
        "paper_id": 2305.07972,
        "authors": "Agam Shah and Suvan Paturi and Sudheer Chava",
        "title": "Trillion Dollar Words: A New Financial Dataset, Task & Market Analysis",
        "comments": "ACL 2023 (main)",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
        "abstract": "  Monetary policy pronouncements by Federal Open Market Committee (FOMC) are a\nmajor driver of financial market returns. We construct the largest tokenized\nand annotated dataset of FOMC speeches, meeting minutes, and press conference\ntranscripts in order to understand how monetary policy influences financial\nmarkets. In this study, we develop a novel task of hawkish-dovish\nclassification and benchmark various pre-trained language models on the\nproposed dataset. Using the best-performing model (RoBERTa-large), we construct\na measure of monetary policy stance for the FOMC document release days. To\nevaluate the constructed measure, we study its impact on the treasury market,\nstock market, and macroeconomic indicators. Our dataset, models, and code are\npublicly available on Huggingface and GitHub under CC BY-NC 4.0 license.\n"
    },
    {
        "paper_id": 2305.08056,
        "authors": "Ke Wan, Yiwen Liu",
        "title": "Hybrid Quantum Algorithms integrating QAOA, Penalty Dephasing and Zeno\n  Effect for Solving Binary Optimization Problems with Multiple Constraints",
        "comments": "21 pages, 15 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
        "abstract": "  When tackling binary optimization problems using quantum algorithms, the\nconventional Ising representation and Quantum Approximate Optimization\nAlgorithm (QAOA) encounter difficulties in efficiently handling errors for\nlarge-scale problems involving multiple constraints. To address these\nchallenges, this paper presents a hybrid framework that combines the use of\nstandard Ising Hamiltonians to solve a subset of the constraints, while\nemploying non-Ising formulations to represent and address the remaining\nconstraints. The resolution of these non-Ising constraints is achieved through\neither penalty dephasing or the quantum Zeno effect. This innovative approach\nleads to a collection of quantum circuits with adaptable structures, depending\non the chosen representation for each constraint. Furthermore, this paper\nintroduces a novel technique that utilizes the quantum Zeno effect by\nfrequently measuring the constraint flag, enabling the resolution of any\noptimization constraint. Theoretical properties of these algorithms are\ndiscussed, and their performance in addressing practical aircraft loading\nproblems is highly promising, showcasing significant potential for a wide range\nof industrial applications.\n"
    },
    {
        "paper_id": 2305.08241,
        "authors": "William H. Press",
        "title": "NYSE Price Correlations Are Abitrageable Over Hours and Predictable Over\n  Years",
        "comments": "48 pages, 21 figures, 2 tables",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Trade prices of about 1000 New York Stock Exchange-listed stocks are studied\nat one-minute time resolution over the continuous five year period 2018--2022.\nFor each stock, in dollar-volume-weighted transaction time, the discrepancy\nfrom a Brownian-motion martingale is measured on timescales of minutes to\nseveral days. The result is well fit by a power-law shot-noise (or Gaussian)\nprocess with Hurst exponent 0.465, that is, slightly mean-reverting. As a\ncheck, we execute an arbitrage strategy on simulated Hurst-exponent data, and a\ncomparable strategy in backtesting on the actual data, obtaining similar\nresults (annualized returns $\\sim 60$\\% if zero transaction costs). Next\nexamining the cross-correlation structure of the $\\sim 1000$ stocks, we find\nthat, counterintuitively, correlations increase with time lag in the range\nstudied. We show that this behavior that can be quantitatively explained if the\nmean-reverting Hurst component of each stock is uncorrelated, i.e., does not\nshare that stock's overall correlation with other stocks. Overall, we find that\n$\\approx 45$\\% of a stock's 1-hour returns variance is explained by its\nparticular correlations to other stocks, but that most of this is simply\nexplained by the movement of all stocks together. Unexpectedly, the fraction of\nvariance explained is greatest when price volatility is high, for example\nduring COVID-19 year 2020. An arbitrage strategy with cross-correlations does\nsignificantly better than without (annualized returns $\\sim 100$\\% if zero\ntransaction costs). Measured correlations from any single year in 2018--2022\nare about equally good in predicting all the other years, indicating that an\noverall correlation structure is persistent over the whole period.\n"
    },
    {
        "paper_id": 2305.08268,
        "authors": "Tomohiro Hirano, Alexis Akira Toda",
        "title": "Bubble Necessity Theorem",
        "comments": null,
        "journal-ref": null,
        "doi": "10.1086/732528",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Asset price bubbles are situations where asset prices exceed the fundamental\nvalues defined by the present value of dividends. This paper presents a\nconceptually new perspective: the necessity of bubbles. We establish the Bubble\nNecessity Theorem in a plausible general class of economic models: with faster\nlong-run economic growth ($G$) than dividend growth ($G_d$) and counterfactual\nlong-run autarky interest rate ($R$) below dividend growth, all equilibria are\nbubbly with non-negligible bubble sizes relative to the economy. This bubble\nnecessity condition naturally arises in economies with sufficiently strong\nsavings motives and multiple factors or sectors with uneven productivity\ngrowth.\n"
    },
    {
        "paper_id": 2305.08524,
        "authors": "Linyi Yang, Yingpeng Ma, Yue Zhang",
        "title": "Measuring Consistency in Text-based Financial Forecasting Models",
        "comments": "Accepted to ACL 2023 Main Conference",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  Financial forecasting has been an important and active area of machine\nlearning research, as even the most modest advantage in predictive accuracy can\nbe parlayed into significant financial gains. Recent advances in natural\nlanguage processing (NLP) bring the opportunity to leverage textual data, such\nas earnings reports of publicly traded companies, to predict the return rate\nfor an asset. However, when dealing with such a sensitive task, the consistency\nof models -- their invariance under meaning-preserving alternations in input --\nis a crucial property for building user trust. Despite this, current financial\nforecasting methods do not consider consistency. To address this problem, we\npropose FinTrust, an evaluation tool that assesses logical consistency in\nfinancial text. Using FinTrust, we show that the consistency of\nstate-of-the-art NLP models for financial forecasting is poor. Our analysis of\nthe performance degradation caused by meaning-preserving alternations suggests\nthat current text-based methods are not suitable for robustly predicting market\ninformation. All resources are available at\nhttps://github.com/yingpengma/fintrust.\n"
    },
    {
        "paper_id": 2305.0853,
        "authors": "Maxime Markov, Vladimir Markov",
        "title": "Portfolio Optimization Rules beyond the Mean-Variance Approach",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  In this paper, we revisit the relationship between investors' utility\nfunctions and portfolio allocation rules. We derive portfolio allocation rules\nfor asymmetric Laplace distributed $ALD(\\mu,\\sigma,\\kappa)$ returns and compare\nthem with the mean-variance approach, which is based on Gaussian returns. We\nreveal that in the limit of small $\\frac{\\mu}{\\sigma}$, the Markowitz\ncontribution is accompanied by a skewness term. We also obtain the allocation\nrules when the expected return is a random normal variable in an average and\nworst-case scenarios, which allows us to take into account uncertainty of the\npredicted returns. An optimal worst-case scenario solution smoothly\napproximates between equal weights and minimum variance portfolio, presenting\nan attractive convex alternative to the risk parity portfolio. We address the\nissue of handling singular covariance matrices by imposing conditional\nindependence structure on the precision matrix directly. Finally, utilizing a\nmicroscopic portfolio model with random drift and analytical expression for the\nexpected utility function with log-normal distributed cross-sectional returns,\nwe demonstrate the influence of model parameters on portfolio construction.\nThis comprehensive approach enhances allocation weight stability, mitigates\ninstabilities associated with the mean-variance approach, and can prove\nvaluable for both short-term traders and long-term investors.\n"
    },
    {
        "paper_id": 2305.0874,
        "authors": "Sheng Xiang, Dawei Cheng, Chencheng Shang, Ying Zhang, Yuqi Liang",
        "title": "Temporal and Heterogeneous Graph Neural Network for Financial Time\n  Series Prediction",
        "comments": "10 pages, 6 figures, ACM CIKM'22, Code:\n  https://github.com/CharlieSCC/alpha/tree/main/alpha/model/THGNN",
        "journal-ref": null,
        "doi": "10.1145/3511808.3557089",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The price movement prediction of stock market has been a classical yet\nchallenging problem, with the attention of both economists and computer\nscientists. In recent years, graph neural network has significantly improved\nthe prediction performance by employing deep learning on company relations.\nHowever, existing relation graphs are usually constructed by handcraft human\nlabeling or nature language processing, which are suffering from heavy resource\nrequirement and low accuracy. Besides, they cannot effectively response to the\ndynamic changes in relation graphs. Therefore, in this paper, we propose a\ntemporal and heterogeneous graph neural network-based (THGNN) approach to learn\nthe dynamic relations among price movements in financial time series. In\nparticular, we first generate the company relation graph for each trading day\naccording to their historic price. Then we leverage a transformer encoder to\nencode the price movement information into temporal representations. Afterward,\nwe propose a heterogeneous graph attention network to jointly optimize the\nembeddings of the financial time series data by transformer encoder and infer\nthe probability of target movements. Finally, we conduct extensive experiments\non the stock market in the United States and China. The results demonstrate the\neffectiveness and superior performance of our proposed methods compared with\nstate-of-the-art baselines. Moreover, we also deploy the proposed THGNN in a\nreal-world quantitative algorithm trading system, the accumulated portfolio\nreturn obtained by our method significantly outperforms other baselines.\n"
    },
    {
        "paper_id": 2305.08778,
        "authors": "Jia Xu and Longbing Cao",
        "title": "Copula Variational LSTM for High-dimensional Cross-market Multivariate\n  Dependence Modeling",
        "comments": "15 pages, 7 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We address an important yet challenging problem - modeling high-dimensional\ndependencies across multivariates such as financial indicators in heterogeneous\nmarkets. In reality, a market couples and influences others over time, and the\nfinancial variables of a market are also coupled. We make the first attempt to\nintegrate variational sequential neural learning with copula-based dependence\nmodeling to characterize both temporal observable and latent variable-based\ndependence degrees and structures across non-normal multivariates. Our\nvariational neural network WPVC-VLSTM models variational sequential dependence\ndegrees and structures across multivariate time series by variational long\nshort-term memory networks and regular vine copula. The regular vine copula\nmodels nonnormal and long-range distributional couplings across multiple\ndynamic variables. WPVC-VLSTM is verified in terms of both technical\nsignificance and portfolio forecasting performance. It outperforms benchmarks\nincluding linear models, stochastic volatility models, deep neural networks,\nand variational recurrent networks in cross-market portfolio forecasting.\n"
    },
    {
        "paper_id": 2305.08958,
        "authors": "Giampaolo Bonomi and Ali Uppal",
        "title": "Kites and Quails: Monetary Policy and Communication with Strategic\n  Financial Markets",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We propose a model to study the consequences of including financial stability\namong the central bank's objectives when market players are strategic, and\nsurprises compromise their stability. In this setup, central banks underreact\nto economic shocks, a prediction consistent with the Federal Reserve's behavior\nduring the 2023 banking crisis. Moreover, policymakers' stability concerns bias\ninvestors' choices, inducing inefficiency. If the central bank has private\ninformation about its policy intentions, the equilibrium forward guidance\nentails an information loss, highlighting a trade-off between stabilizing\nmarkets through policy and communication. A \"kitish\" central banker, who puts\nless weight on stability, reduces these inefficiencies.\n"
    },
    {
        "paper_id": 2305.09046,
        "authors": "James Chok and Geoffrey M. Vasil",
        "title": "Convex optimization over a probability simplex",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  We propose a new iteration scheme, the Cauchy-Simplex, to optimize convex\nproblems over the probability simplex $\\{w\\in\\mathbb{R}^n\\ |\\ \\sum_i w_i=1\\\n\\textrm{and}\\ w_i\\geq0\\}$. Other works have taken steps to enforce positivity\nor unit normalization automatically but never simultaneously within a unified\nsetting. This paper presents a natural framework for manifestly requiring the\nprobability condition. Specifically, we map the simplex to the positive\nquadrant of a unit sphere, envisage gradient descent in latent variables, and\nmap the result back in a way that only depends on the simplex variable.\nMoreover, proving rigorous convergence results in this formulation leads\ninherently to tools from information theory (e.g. cross entropy and KL\ndivergence). Each iteration of the Cauchy-Simplex consists of simple\noperations, making it well-suited for high-dimensional problems. We prove that\nit has a convergence rate of ${O}(1/T)$ for convex functions, and numerical\nexperiments of projection onto convex hulls show faster convergence than\nsimilar algorithms. Finally, we apply our algorithm to online learning problems\nand prove the convergence of the average regret for (1) Prediction with expert\nadvice and (2) Universal Portfolios.\n"
    },
    {
        "paper_id": 2305.09097,
        "authors": "Baishuai Zuo, Chuancun Yin, Jing Yao",
        "title": "Multivariate range Value-at-Risk and covariance risk measures for\n  elliptical and log-elliptical distributions",
        "comments": "26pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper, we propose the multivariate range Value-at-Risk (MRVaR) and\nthe multivariate range covariance (MRCov) as two risk measures and explore\ntheir desirable properties in risk management. In particular, we explain that\nsuch range-based risk measures are appropriate for risk management of\nregulation and investment purposes. The multivariate range correlation matrix\n(MRCorr) is introduced accordingly. To facilitate analytical analyses, we\nderive explicit expressions of the MRVaR and the MRCov in the context of the\nmultivariate (log-)elliptical distribution family. Frequently-used cases in\nindustry, such as normal, student-$t$, logistic, Laplace, and Pearson type VII\ndistributions, are presented with numerical examples. As an application, we\npropose a range-based mean-variance framework of optimal portfolio selection.\nWe calculate the range-based efficient frontiers of the optimal portfolios\nbased on real data of stocks' returns. Both the numerical examples and the\nefficient frontiers demonstrate consistences with the desirable properties of\nthe range-based risk measures.\n"
    },
    {
        "paper_id": 2305.09166,
        "authors": "Jiawei Huo",
        "title": "Finite Difference Solution Ansatz approach in Least-Squares Monte Carlo",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  This article presents a simple but effective and efficient approach to\nimprove the accuracy and stability of Least-Squares Monte Carlo for\nAmerican-style option pricing as well as expected exposure calculation in\nvaluation adjustments. The key idea is to construct the ansatz of conditional\nexpected continuation payoff using the finite difference solution from one\ndimension, to be used in linear regression. This approach bridges between\nsolving backward partial differential equations and Monte Carlo simulation,\naiming at achieving the best of both worlds. Independent of model settings, the\nansatz is proved to serve as a control variate to reduce the least-squares\nerrors. We illustrate the technique with realistic examples including Bermudan\noptions, worst of issuer callable notes and expected positive exposure on\nEuropean options. The method can be considered as a generic numerical scheme\nacross various asset classes, in particular, as an accurate method for pricing\nand risk-managing American-style derivatives under arbitrary dimensions.\n"
    },
    {
        "paper_id": 2305.09352,
        "authors": "Tatsuki Inoue",
        "title": "Health Impacts of Public Pawnshops in Industrializing Tokyo",
        "comments": "39pages, 7 tables, 3 figures, Appendicies",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/publicdomain/zero/1.0/",
        "abstract": "  This study is the first to investigate whether pawnshops, financial\ninstitutions for low-income populations, have contributed to the decline in\nmortality in the early twentieth century. Using ward-level panel data from\nTokyo City, this study revealed that the popularity of public pawnshops was\nassociated with a 4% and 5% decrease in infant mortality and fetal death rates,\nrespectively, during 1927-1935. The historical context implies that the\npotential channels of the relationships were improving nutrition and hygiene\nand covering childbirth costs. Moreover, a cost-benefit calculation highlighted\nthat the establishment of public pawnshops was a cost-effective public\ninvestment for better public health. Contrarily, for-profit private pawnshops\nshowed no significant association with health improvements.\n"
    },
    {
        "paper_id": 2305.09471,
        "authors": "Felix Fie{\\ss}inger and Mitja Stadje",
        "title": "Time-Consistent Asset Allocation for Risk Measures in a L\\'evy Market",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Focusing on gains & losses relative to a risk-free benchmark instead of\nterminal wealth, we consider an asset allocation problem to maximize\ntime-consistently a mean-risk reward function with a general risk measure which\nis i) law-invariant, ii) cash- or shift-invariant, and iii) positively\nhomogeneous, and possibly plugged into a general function. Examples include\n(relative) Value at Risk, coherent risk measures, variance, and generalized\ndeviation risk measures. We model the market via a generalized version of the\nmulti-dimensional Black-Scholes model using $\\alpha$-stable L\\'evy processes\nand give supplementary results for the classical Black-Scholes model. The\noptimal solution to this problem is a Nash subgame equilibrium given by the\nsolution of an extended Hamilton-Jacobi-Bellman equation. Moreover, we show\nthat the optimal solution is deterministic under appropriate assumptions.\n"
    },
    {
        "paper_id": 2305.09472,
        "authors": "Huansang Xu, Ruyi Liu and Marek Rutkowski",
        "title": "Equity Protection Swaps: A New Type of Investment Insurance for Holders\n  of Superannuation Accounts",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/publicdomain/zero/1.0/",
        "abstract": "  We propose to develop a new class of investment insurance products for\nholders of superannuation accounts in Australia, which we tentatively call\nequity protection swaps (EPSs). An EPS is a standalone financial derivative,\nwhich is reminiscent of a total return swap but also shares some features with\nthe variable annuity known as the registered index-linked annuity (RILA). The\nbuyer of an EPS obtains partial protection against losses on a reference\nportfolio and, in exchange, agrees to share portfolio gains with the insurance\nprovider if the realized return on a reference portfolio is above a\npredetermined threshold. Formally, a generic EPS consists of protection and fee\nlegs with participation rates agreed upon by the provider and holder. A general\nfair pricing formula for an EPS is obtained by considering a static hedging\nstrategy based on traded European options. It is argued that to make the\ncontract appealing to holders, the provider should select appropriate\nprotection and fee rates that make the fair premium at the contract's inception\nequal to zero. A numerical study based on the Black-Scholes model and empirical\ntests based on market data for S\\&P~500 and S&P/ASX~200 indices for 2020-2022\ndemonstrates the benefits of an EPS as an efficient investment insurance tool\nfor superannuation accounts.\n"
    },
    {
        "paper_id": 2305.09473,
        "authors": "Jonathan A. Jensen",
        "title": "Searching for the \"Holy Grail\" of sponsorship-linked marketing: A\n  generalizable sponsorship ROI model",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Marketers routinely allocate a significant portion of their budget to\nsponsorship. However, isolating the return on investment from such efforts has\nremained a challenge. Thus, a dataset of more than 5,800 sponsorships is\nanalyzed using survival analysis approaches that utilizes the sponsor's renewal\nof the sponsorship as a proxy for positive ROI. In addition to its contribution\nto the sponsorship-linked marketing literature, the resulting model can be\nutilized by managers to generate predicted values relative to the sponsor's\nprobability of renewal and the ultimate duration of time the sponsor will\nremain with the property, representing a novel managerial contribution.\n"
    },
    {
        "paper_id": 2305.09474,
        "authors": "Jungyeon Park, Est\\^ev\\~ao Alvarenga, Jooyoung Jeon, Ran Li, Fotios\n  Petropoulos, Hokyun Kim and Kwangwon Ahn",
        "title": "Probabilistic Forecast-based Portfolio Optimization of Electricity\n  Demand at Low Aggregation Levels",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  In the effort to achieve carbon neutrality through a decentralized\nelectricity market, accurate short-term load forecasting at low aggregation\nlevels has become increasingly crucial for various market participants'\nstrategies. Accurate probabilistic forecasts at low aggregation levels can\nimprove peer-to-peer energy sharing, demand response, and the operation of\nreliable distribution networks. However, these applications require not only\nprobabilistic demand forecasts, which involve quantification of the forecast\nuncertainty, but also determining which consumers to include in the aggregation\nto meet electricity supply at the forecast lead time. While research papers\nhave been proposed on the supply side, no similar research has been conducted\non the demand side. This paper presents a method for creating a portfolio that\noptimally aggregates demand for a given energy demand, minimizing forecast\ninaccuracy of overall low-level aggregation. Using probabilistic load forecasts\nproduced by either ARMA-GARCH models or kernel density estimation (KDE), we\npropose three approaches to creating a portfolio of residential households'\ndemand: Forecast Validated, Seasonal Residual, and Seasonal Similarity. An\nevaluation of probabilistic load forecasts demonstrates that all three\napproaches enhance the accuracy of forecasts produced by random portfolios,\nwith the Seasonal Residual approach for Korea and Ireland outperforming the\nothers in terms of both accuracy and computational efficiency.\n"
    },
    {
        "paper_id": 2305.09479,
        "authors": "Naixin Zhu",
        "title": "Dissertation on Applied Microeconomics of Freemium Pricing Strategies in\n  Mobile App Market",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  In my dissertation, I will analyze how the product market position of a\nmobile app affects its pricing strategies, which in turn impacts an app's\nmonetization process. Using natural language processing and k-mean clustering\non apps' text descriptions, I created a new variable that measures the\ndistinctiveness of an app as compared to its peers. I created four pricing\nvariables, price, cumulative installs, and indicators of whether an app\ncontains in-app ads and purchases. I found that the effect differs for\nsuccessful apps and less successful apps. I measure the success here using\ncumulative installs and the firms that developed the apps. Based on third-party\nrankings and the shape of the distribution of installs, I set two thresholds\nand divided apps into market-leading and market-follower apps. The\nmarket-leading sub-sample consists of apps with high cumulative installs or\ndeveloped by prestigious firms, and the market-follower sub-sample consists of\nthe rest of the apps. I found that the impact of being niche is smaller in the\nmarket-leading apps because of their relatively higher heterogeneity. In\naddition, being niche also impact utility apps differently from hedonic apps or\napps with two-sided market characteristics. For the special gaming category,\nbeing niche has some effect but is smaller than in the market follower\nsub-sample. My research provides novel empirical evidence of digital products\nto various strands of theoretical research, including the optimal\ndistinctiveness theory, product differentiation, price discrimination in two or\nmulti-sided markets, and consumer psychology.\n"
    },
    {
        "paper_id": 2305.09485,
        "authors": "Niklas Mueller, Steffen Klug, Andreas Koenig, Alexander Kathan, Lukas\n  Christ, Bjoern Schuller, Shahin Amiriparian",
        "title": "Executive Voiced Laughter and Social Approval: An Explorative Machine\n  Learning Study",
        "comments": "Method section needs to be updated",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We study voiced laughter in executive communication and its effect on social\napproval. Integrating research on laughter, affect-as-information, and\ninfomediaries' social evaluations of firms, we hypothesize that voiced laughter\nin executive communication positively affects social approval, defined as\naudience perceptions of affinity towards an organization. We surmise that the\neffect of laughter is especially strong for joint laughter, i.e., the number of\ninstances in a given communication venue for which the focal executive and the\naudience laugh simultaneously. Finally, combining the notions of\naffect-as-information and negativity bias in human cognition, we hypothesize\nthat the positive effect of laughter on social approval increases with bad\norganizational performance. We find partial support for our ideas when testing\nthem on panel data comprising 902 German Bundesliga soccer press conferences\nand media tenor, applying state-of-the-art machine learning approaches for\nlaughter detection as well as sentiment analysis. Our findings contribute to\nresearch at the nexus of executive communication, strategic leadership, and\nsocial evaluations, especially by introducing laughter as a highly\nconsequential potential, but understudied social lubricant at the\nexecutive-infomediary interface. Our research is unique by focusing on\nreflexive microprocesses of social evaluations, rather than the\ninfomediary-routines perspectives in infomediaries' evaluations. We also make\nmethodological contributions.\n"
    },
    {
        "paper_id": 2305.09783,
        "authors": "Benjamin Fan, Edward Qiao, Anran Jiao, Zhouzhou Gu, Wenhao Li, Lu Lu",
        "title": "Deep Learning for Solving and Estimating Dynamic Macro-Finance Models",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  We develop a methodology that utilizes deep learning to simultaneously solve\nand estimate canonical continuous-time general equilibrium models in financial\neconomics. We illustrate our method in two examples: (1) industrial dynamics of\nfirms and (2) macroeconomic models with financial frictions. Through these\napplications, we illustrate the advantages of our method: generality,\nsimultaneous solution and estimation, leveraging the state-of-art\nmachine-learning techniques, and handling large state space. The method is\nversatile and can be applied to a vast variety of problems.\n"
    },
    {
        "paper_id": 2305.09837,
        "authors": "Jessie Henshaw",
        "title": "Emergent Growth of System Self-Organization & Self-Control",
        "comments": "14 pages, 7 figures, 2023 pending SRBS review",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  In physics, I noticed subjects not explained by formulas were often not\nstudied, like how uncontrolled growth systems changed form. Weather,\nbusinesses, societies, environments, communities, cultures, groups,\nrelationships, lives, and livelihoods all do it following some variation of an\n'S' curve. It is a slow-fast-slow process of self-animated contextual energy\nsystem development. Making working systems, they seem to develop by \"find and\nconnect\" in three stages, starting small to first A) freely build designs of\ngrowing power, then B) diversify, adapt, respond, and harmonize with others,\nthen C) take on one or more roles in their climax environments. It is a life\ncurve of tremendous syntropic success that then ends with decline. Life is\nparticularly risky for small start-ups, but many do succeed. Many powerful\ncivilizations have emerged, some never growing up but growing as endless\nstartups, only to become fragile, fail, and vanish. That looks like a choice.\n"
    },
    {
        "paper_id": 2305.09917,
        "authors": "Abhijit Chakraborty, Tetsuo Hatsuda and Yuichi Ikeda",
        "title": "Embedding and correlation tensor for XRP transaction networks",
        "comments": "8 pages, 7 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Cryptoassets are growing rapidly worldwide. One of the large cap cryptoassets\nis XRP. In this article, we focus on analyzing transaction data for the\n2017-2018 period that consist one of the significant XRP market price bursts.\nWe construct weekly weighted directed networks of XRP transactions. These\nweekly networks are embedded on continuous vector space using a network\nembedding technique that encodes structural regularities present in the network\nstructure in terms of node vectors. Using a suitable time window we calculate a\ncorrelation tensor. A double singular value decomposition of the correlation\ntensor provides key insights about the system. The significance of the\ncorrelation tensor is captured using a randomized correlation tensor. We\npresent a detailed dependence of correlation tensor on model parameters.\n"
    },
    {
        "paper_id": 2305.10164,
        "authors": "John Geanakoplos and Herakles Polemarchakis",
        "title": "Rational Dialogues",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Any finite conversation can be rationalized.\n"
    },
    {
        "paper_id": 2305.10165,
        "authors": "Aviad Heifetz, Enrico Minelli, Herakles Polemarchakis",
        "title": "Affective interdependence and welfare",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Purely affective interaction allows the welfare of an individual to depend on\nher own actions and on the profile of welfare levels of others. Under an\nassumption on the structure of mutual affection that we interpret as\n\"non-explosive mutual affection,\" we show that equilibria of simultaneous-move\naffective interaction are Pareto optimal independently of whether or not an\ninduced standard game exists. Moreover, if purely affective interaction induces\na standard game, then an equilibrium profile of actions is a Nash equilibrium\nof the game, and this Nash equilibrium and Pareto optimal profile of strategies\nis locally dominant.\n"
    },
    {
        "paper_id": 2305.10234,
        "authors": "Anastasija Nikiforova, Nina Rizun, Magdalena Ciesielska, Charalampos\n  Alexopoulos, Andrea Mileti\\v{c}",
        "title": "Towards High-Value Datasets determination for data-driven development: a\n  systematic literature review",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  The OGD is seen as a political and socio-economic phenomenon that promises to\npromote civic engagement and stimulate public sector innovations in various\nareas of public life. To bring the expected benefits, data must be reused and\ntransformed into value-added products or services. This, in turn, sets another\nprecondition for data that are expected to not only be available and comply\nwith open data principles, but also be of value, i.e., of interest for reuse by\nthe end-user. This refers to the notion of 'high-value dataset' (HVD),\nrecognized by the European Data Portal as a key trend in the OGD area in 2022.\nWhile there is a progress in this direction, e.g., the Open Data Directive,\nincl. identifying 6 key categories, a list of HVDs and arrangements for their\npublication and re-use, they can be seen as 'core' / 'base' datasets aimed at\nincreasing interoperability of public sector data with a high priority,\ncontributing to the development of a more mature OGD initiative. Depending on\nthe specifics of a region and country - geographical location, social,\nenvironmental, economic issues, cultural characteristics, (under)developed\nsectors and market specificities, more datasets can be recognized as of high\nvalue for a particular country. However, there is no standardized approach to\nassist chief data officers in this. In this paper, we present a systematic\nreview of existing literature on the HVD determination, which is expected to\nform an initial knowledge base for this process, incl. used approaches and\nindicators to determine them, data, stakeholders.\n"
    },
    {
        "paper_id": 2305.10239,
        "authors": "Lane P. Hughston and Leandro S\\'anchez-Betancourt",
        "title": "Valuation of a Financial Claim Contingent on the Outcome of a Quantum\n  Measurement",
        "comments": "27 pages, 1 figure",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  We consider a rational agent who at time $0$ enters into a financial contract\nfor which the payout is determined by a quantum measurement at some time $T>0$.\nThe state of the quantum system is given in the Heisenberg representation by a\nknown density matrix $\\hat p$. How much will the agent be willing to pay at\ntime $0$ to enter into such a contract? In the case of a finite dimensional\nHilbert space, each such claim is represented by an observable $\\hat X_T$ where\nthe eigenvalues of $\\hat X_T$ determine the amount paid if the corresponding\noutcome is obtained in the measurement. We prove, under reasonable axioms, that\nthere exists a pricing state $\\hat q$ which is equivalent to the physical state\n$\\hat p$ on null spaces such that the pricing function $\\Pi_{0T}$ takes the\nform $\\Pi_{0T}(\\hat X_T) = P_{0T}\\,{\\rm tr} ( \\hat q \\hat X_T) $ for any claim\n$\\hat X_T$, where $P_{0T}$ is the one-period discount factor. By \"equivalent\"\nwe mean that $\\hat p$ and $\\hat q$ share the same null space: thus, for any\n$|\\xi \\rangle \\in \\mathcal H$ one has $\\langle \\bar \\xi | \\hat p | \\xi \\rangle\n= 0$ if and only if $\\langle \\bar \\xi | \\hat q | \\xi \\rangle = 0$. We introduce\na class of optimization problems and solve for the optimal contract payout\nstructure for a claim based on a given measurement. Then we consider the\nimplications of the Kochen-Specker theorem in such a setting and we look at the\nproblem of forming portfolios of such contracts. Finally, we consider\nmulti-period contracts.\n"
    },
    {
        "paper_id": 2305.10624,
        "authors": "Nihar Shah, Lucas Baker, Suraj Srinivasan, Alex Toberoff",
        "title": "A Short Note on Setting Swap Parameters",
        "comments": "3 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  This short note illustrates the theoretical solution to a trader determining\nhow to optimally swap her wealth into a target asset through on-chain\noperations. It offers the framework to solve optimal slippage parameters and\noptimal trade size.\n"
    },
    {
        "paper_id": 2305.10678,
        "authors": "Qian Li and Li Wang",
        "title": "Option pricing under jump diffusion model",
        "comments": "14 pages, 4 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We provide an European option pricing formula written in the form of an\ninfinite series of Black Scholes type terms under double Levy jumps model,\nwhere both the interest rate and underlying price are driven by Levy process.\nThe series solution converges with a radius of convergence, and it is\ncomplemented by some numerical experiments to demonstrate its speed of\nconvergence.\n"
    },
    {
        "paper_id": 2305.10693,
        "authors": "Jingjing Guo",
        "title": "Gated Deeper Models are Effective Factor Learners",
        "comments": "7 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
        "abstract": "  Precisely forecasting the excess returns of an asset (e.g., Tesla stock) is\nbeneficial to all investors. However, the unpredictability of market dynamics,\ninfluenced by human behaviors, makes this a challenging task. In prior\nresearch, researcher have manually crafted among of factors as signals to guide\ntheir investing process. In contrast, this paper view this problem in a\ndifferent perspective that we align deep learning model to combine those human\ndesigned factors to predict the trend of excess returns. To this end, we\npresent a 5-layer deep neural network that generates more meaningful factors in\na 2048-dimensional space. Modern network design techniques are utilized to\nenhance robustness training and reduce overfitting. Additionally, we propose a\ngated network that dynamically filters out noise-learned features, resulting in\nimproved performance. We evaluate our model over 2,000 stocks from the China\nmarket with their recent three years records. The experimental results show\nthat the proposed gated activation layer and the deep neural network could\neffectively overcome the problem. Specifically, the proposed gated activation\nlayer and deep neural network contribute to the superior performance of our\nmodel. In summary, the proposed model exhibits promising results and could\npotentially benefit investors seeking to optimize their investment strategies.\n"
    },
    {
        "paper_id": 2305.10695,
        "authors": "Lars Tyge Nielsen",
        "title": "A Counterexample in Ito Integration Theory",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Ito's Lemma implies that if $W$ is a Wiener process and $f$ is a twice\ncontinuously differentiable function, then the process $f(W)$ is the sum of a\ntime integral and an Ito integral. The Ito integrand is not necessarily locally\nsquare integrable. This note provides a counterexample.\n"
    },
    {
        "paper_id": 2305.10725,
        "authors": "Svetlana Boyarchenko and Sergei Levendorski\\u{i}",
        "title": "Efficient inverse $Z$-transform: sufficient conditions",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We derive several sets of sufficient conditions for applicability of the new\nefficient numerical realization of the inverse $Z$-transform. For large $n$,\nthe complexity of the new scheme is dozens of times smaller than the complexity\nof the trapezoid rule. As applications, pricing of European options and single\nbarrier options with discrete monitoring are considered; applications to more\ngeneral options with barrier-lookback features are outlined. In the case of\nsectorial transition operators, hence, for symmetric L\\'evy models, the proof\nis straightforward. In the case of non-symmetric L\\'evy models, we construct a\nnon-linear deformation of the dual space, which makes the transition operator\nsectorial, with an arbitrary small opening angle, and justify the new\nrealization. We impose mild conditions which are satisfied for wide classes of\nnon-symmetric Stieltjes-L\\'evy processes.\n"
    },
    {
        "paper_id": 2305.10849,
        "authors": "Alexander Gairat and Vadim Shcherbakov",
        "title": "Extreme ATM skew in a local volatility model with discontinuity: joint\n  density approach",
        "comments": "To appear in Finance and Stochastics",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  This paper concerns a local volatility model in which volatility takes two\npossible values, and the specific value depends on whether the underlying price\nis above or below a given threshold value. The model is known, and a number of\nresults have been obtained for it. In particular, option pricing formulas and a\npower law behaviour of the implied volatility skew have been established in the\ncase when the threshold is taken at the money. In this paper we derive an\nalternative representation of option pricing formulas. In addition, we obtain\nan approximation of option prices by the corresponding Black-Scholes prices.\nUsing this approximation streamlines obtaining the aforementioned behaviour of\nthe skew. Our approach is based on the natural relationship of the model with\nSkew Brownian motion and consists of the systematic use of the joint\ndistribution of this stochastic process and some of its functionals.\n"
    },
    {
        "paper_id": 2305.10885,
        "authors": "Yun Liao, Ruihui Xu",
        "title": "Super-efficiency of Listed Banks in China and Determinants Analysis\n  (2006-2021)",
        "comments": "JEL Classifications: G21; D21; D57; C23",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  This study employs the annual unbalanced panel data of 42 listed banks in\nChina from 2006 to 2021, adopts the non-radial and non-oriented\nsuper-efficiency Data envelopment analysis (Super-SBM-UND-VRS based DEA) model\nconsidering NPL as undesired output. Our results show that the profitability\nsuper-efficiency of State-owned banks and Rural/City Commercial Banks is better\nthan that of Joint-stock Banks. In terms of intermediary efficiency(deposit and\nloan), state-owned banks have advantage on other two type of banks. The\ndeterminants analysis shows that all type of banks significantly benefits from\nthe decrease of ownership concentration which support reformation and IPO.\nRegional commercial banks significantly benefit from the decrease of customer\nconcentration and the increase of reserves. On the other hand, State-owned\nbanks should increase its loan to deposit ratio while joint-stock banks should\ndo the opposite.\n"
    },
    {
        "paper_id": 2305.10911,
        "authors": "Francesco Cesarone, Rosella Giacometti, Jacopo Maria Ricci",
        "title": "Non-parametric cumulants approach for outlier detection of multivariate\n  financial data",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  In this paper, we propose an outlier detection algorithm for multivariate\ndata based on their projections on the directions that maximize the Cumulant\nGenerating Function (CGF). We prove that CGF is a convex function, and we\ncharacterize the CGF maximization problem on the unit n-circle as a concave\nminimization problem. Then, we show that the CGF maximization approach can be\ninterpreted as an extension of the standard principal component technique.\nTherefore, for validation and testing, we provide a thorough comparison of our\nmethodology with two other projection-based approaches both on artificial and\nreal-world financial data. Finally, we apply our method as an early detector\nfor financial crises.\n"
    },
    {
        "paper_id": 2305.11298,
        "authors": "Sumanjay Dutta, Shashi Jain",
        "title": "Precision versus Shrinkage: A Comparative Analysis of Covariance\n  Estimation Methods for Portfolio Allocation",
        "comments": "arXiv admin note: text overlap with arXiv:1905.01282, arXiv:1410.8504\n  by other authors",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  In this paper, we perform a comprehensive study of different covariance and\nprecision matrix estimation methods in the context of minimum variance\nportfolio allocation. The set of models studied by us can be broadly\ncategorized as: Gaussian Graphical Model (GGM) based methods, Shrinkage\nMethods, Thresholding and Random Matrix Theory (RMT) based methods. Among\nthese, GGM methods estimate the precision matrix directly while the other\napproaches estimate the covariance matrix. We perform a synthetic experiment to\nstudy the network learning and sample complexity performance of GGM methods.\nThereafter, we compare all the covariance and precision matrix estimation\nmethods in terms of their predictive ability for daily, weekly and monthly\nhorizons. We consider portfolio risk as an indicator of estimation error and\nemploy it as a loss function for comparison of the methods under consideration.\nWe find that GGM methods outperform shrinkage and other approaches. Our\nobservations for the performance of GGM methods are consistent with the\nsynthetic experiment. We also propose a new criterion for the hyperparameter\ntuning of GGM methods. Our tuning approach outperforms the existing methodology\nin the synthetic setup. We further perform an empirical experiment where we\nstudy the properties of the estimated precision matrix. The properties of the\nestimated precision matrices calculated using our tuning approach are in\nagreement with the algorithm performances observed in the synthetic experiment\nand the empirical experiment for predictive ability performance comparison.\nApart from this, we perform another synthetic experiment which demonstrates the\ndirect relation between estimation error of the precision matrix and portfolio\nrisk.\n"
    },
    {
        "paper_id": 2305.11319,
        "authors": "Sebastian Jaimungal, Silvana M. Pesenti, Yuri F. Saporito, Rodrigo S.\n  Targino",
        "title": "Risk Budgeting Allocation for Dynamic Risk Measures",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-sa/4.0/",
        "abstract": "  We define and develop an approach for risk budgeting allocation - a risk\ndiversification portfolio strategy - where risk is measured using a dynamic\ntime-consistent risk measure. For this, we introduce a notion of dynamic risk\ncontributions that generalise the classical Euler contributions and which allow\nus to obtain dynamic risk contributions in a recursive manner. We prove that,\nfor the class of coherent dynamic distortion risk measures, the risk allocation\nproblem may be recast as a sequence of strictly convex optimisation problems.\nMoreover, we show that self-financing dynamic risk budgeting strategies with\ninitial wealth of 1 are scaled versions of the solution of the sequence of\nconvex optimisation problems. Furthermore, we develop an actor-critic approach,\nleveraging the elicitability of dynamic risk measures, to solve for risk\nbudgeting strategies using deep learning.\n"
    },
    {
        "paper_id": 2305.11375,
        "authors": "Benedict Guttman-Kenney, Jesse Leary, Neil Stewart",
        "title": "Weighing Anchor on Credit Card Debt",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  We find it is common for consumers who are not in financial distress to make\ncredit card payments at or close to the minimum. This pattern is difficult to\nreconcile with economic factors but can be explained by minimum payment\ninformation presented to consumers acting as an anchor that weighs payments\ndown. Building on Stewart (2009), we conduct a hypothetical credit card payment\nexperiment to test an intervention to de-anchor payment choices. This\nintervention effectively stops consumers selecting payments at the contractual\nminimum. It also increases their average payments, as well as shifting the\ndistribution of payments. By de-anchoring choices from the minimum, consumers\nincreasingly choose the full payment amount - which potentially seems to act as\na target payment for consumers. We innovate by linking the experimental\nresponses to survey responses on financial distress and to actual credit card\npayment behaviours. We find that the intervention largely increases payments\nmade by less financially-distressed consumers. We are also able to evaluate the\npotential external validity of our experiment and find that hypothetical\nresponses are closely related to consumers' actual credit card payments.\n"
    },
    {
        "paper_id": 2305.11406,
        "authors": "Eric Budish, Ruiquan Gao, Abraham Othman, Aviad Rubinstein, Qianfan\n  Zhang",
        "title": "Practical algorithms and experimentally validated incentives for\n  equilibrium-based fair division (A-CEEI)",
        "comments": "To appear in EC 2023",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Approximate Competitive Equilibrium from Equal Incomes (A-CEEI) is an\nequilibrium-based solution concept for fair division of discrete items to\nagents with combinatorial demands. In theory, it is known that in\nasymptotically large markets:\n  1. For incentives, the A-CEEI mechanism is Envy-Free-but-for-Tie-Breaking\n(EF-TB), which implies that it is Strategyproof-in-the-Large (SP-L).\n  2. From a computational perspective, computing the equilibrium solution is\nunfortunately a computationally intractable problem (in the worst-case,\nassuming $\\textsf{PPAD}\\ne \\textsf{FP}$).\n  We develop a new heuristic algorithm that outperforms the previous\nstate-of-the-art by multiple orders of magnitude. This new, faster algorithm\nlets us perform experiments on real-world inputs for the first time. We\ndiscover that with real-world preferences, even in a realistic implementation\nthat satisfies the EF-TB and SP-L properties, agents may have surprisingly\nsimple and plausible deviations from truthful reporting of preferences. To this\nend, we propose a novel strengthening of EF-TB, which dramatically reduces the\npotential for strategic deviations from truthful reporting in our experiments.\n  A (variant of) our algorithm is now in production: on real course allocation\nproblems it is much faster, has zero clearing error, and has stronger incentive\nproperties than the prior state-of-the-art implementation.\n"
    },
    {
        "paper_id": 2305.11519,
        "authors": "Nikodem Tomczak",
        "title": "Artificial intelligence moral agent as Adam Smith's impartial spectator",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Adam Smith developed a version of moral philosophy where better decisions are\nmade by interrogating an impartial spectator within us. We discuss the\npossibility of using an external non-human-based substitute tool that would\naugment our internal mental processes and play the role of the impartial\nspectator. Such tool would have more knowledge about the world, be more\nimpartial, and would provide a more encompassing perspective on moral\nassessment.\n"
    },
    {
        "paper_id": 2305.11523,
        "authors": "Jonas Tallberg, Magnus Lundgren, Johannes Geith",
        "title": "AI Regulation in the European Union: Examining Non-State Actor\n  Preferences",
        "comments": null,
        "journal-ref": "Bus. Polit. 26 (2024) 218-239",
        "doi": "10.1017/bap.2023.36",
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  As the development and use of artificial intelligence (AI) continues to grow,\npolicymakers are increasingly grappling with the question of how to regulate\nthis technology. The most far-reaching international initiative is the European\nUnion (EU) AI Act, which aims to establish the first comprehensive, binding\nframework for regulating AI. In this article, we offer the first systematic\nanalysis of non-state actor preferences toward international regulation of AI,\nfocusing on the case of the EU AI Act. Theoretically, we develop an argument\nabout the regulatory preferences of business actors and other non-state actors\nunder varying conditions of AI sector competitiveness. Empirically, we test\nthese expectations using data from public consultations on European AI\nregulation. Our findings are threefold. First, all types of non-state actors\nexpress concerns about AI and support regulation in some form. Second, there\nare nonetheless significant differences across actor types, with business\nactors being less concerned about the downsides of AI and more in favor of lax\nregulation than other non-state actors. Third, these differences are more\npronounced in countries with stronger commercial AI sectors. Our findings shed\nnew light on non-state actor preferences toward AI regulation and point to\nchallenges for policymakers balancing competing interests in society.\n"
    },
    {
        "paper_id": 2305.11528,
        "authors": "Jonas Tallberg, Eva Erman, Markus Furendal, Johannes Geith, Mark\n  Klamberg, and Magnus Lundgren",
        "title": "The Global Governance of Artificial Intelligence: Next Steps for\n  Empirical and Normative Research",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Artificial intelligence (AI) represents a technological upheaval with the\npotential to change human society. Because of its transformative potential, AI\nis increasingly becoming subject to regulatory initiatives at the global level.\nYet, so far, scholarship in political science and international relations has\nfocused more on AI applications than on the emerging architecture of global AI\nregulation. The purpose of this article is to outline an agenda for research\ninto the global governance of AI. The article distinguishes between two broad\nperspectives: an empirical approach, aimed at mapping and explaining global AI\ngovernance; and a normative approach, aimed at developing and applying\nstandards for appropriate global AI governance. The two approaches offer\nquestions, concepts, and theories that are helpful in gaining an understanding\nof the emerging global governance of AI. Conversely, exploring AI as a\nregulatory issue offers a critical opportunity to refine existing general\napproaches to the study of global governance.\n"
    },
    {
        "paper_id": 2305.11581,
        "authors": "Alexandra Brintrup, George Baryannis, Ashutosh Tiwari, Svetan Ratchev,\n  Giovanna Martinez-Arellano, Jatinder Singh",
        "title": "Trustworthy, responsible, ethical AI in manufacturing and supply chains:\n  synthesis and emerging research questions",
        "comments": "Pre-print under peer-review",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  While the increased use of AI in the manufacturing sector has been widely\nnoted, there is little understanding on the risks that it may raise in a\nmanufacturing organisation. Although various high level frameworks and\ndefinitions have been proposed to consolidate potential risks, practitioners\nstruggle with understanding and implementing them.\n  This lack of understanding exposes manufacturing to a multitude of risks,\nincluding the organisation, its workers, as well as suppliers and clients. In\nthis paper, we explore and interpret the applicability of responsible, ethical,\nand trustworthy AI within the context of manufacturing. We then use a broadened\nadaptation of a machine learning lifecycle to discuss, through the use of\nillustrative examples, how each step may result in a given AI trustworthiness\nconcern. We additionally propose a number of research questions to the\nmanufacturing research community, in order to help guide future research so\nthat the economic and societal benefits envisaged by AI in manufacturing are\ndelivered safely and responsibly.\n"
    },
    {
        "paper_id": 2305.11873,
        "authors": "Pawe{\\l} Niszczota, Paul Conway",
        "title": "Judgments of research co-created by generative AI: experimental evidence",
        "comments": "10 pages, 2 tables, 1 figure",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  The introduction of ChatGPT has fuelled a public debate on the use of\ngenerative AI (large language models; LLMs), including its use by researchers.\nIn the current work, we test whether delegating parts of the research process\nto LLMs leads people to distrust and devalue researchers and scientific output.\nParticipants (N=402) considered a researcher who delegates elements of the\nresearch process to a PhD student or LLM, and rated (1) moral acceptability,\n(2) trust in the scientist to oversee future projects, and (3) the accuracy and\nquality of the output. People judged delegating to an LLM as less acceptable\nthan delegating to a human (d = -0.78). Delegation to an LLM also decreased\ntrust to oversee future research projects (d = -0.80), and people thought the\nresults would be less accurate and of lower quality (d = -0.85). We discuss how\nthis devaluation might transfer into the underreporting of generative AI use.\n"
    },
    {
        "paper_id": 2305.119,
        "authors": "Samir Huseynov",
        "title": "ChatGPT and the Labor Market: Unraveling the Effect of AI Discussions on\n  Students' Earnings Expectations",
        "comments": "Working paper",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  This paper investigates the causal impact of negatively and positively toned\nChatGPT Artificial Intelligence (AI) discussions on US students' anticipated\nlabor market outcomes. Our findings reveal students reduce their confidence\nregarding their future earnings prospects after exposure to AI debates, and\nthis effect is more pronounced after reading discussion excerpts with a\nnegative tone. Unlike STEM majors, students in Non-STEM fields show asymmetric\nand pessimistic belief changes, suggesting that they might feel more vulnerable\nto emerging AI technologies. Pessimistic belief updates regarding future\nearnings are also prevalent among non-male students, indicating widespread AI\nconcerns among vulnerable student subgroups. Educators, administrators, and\npolicymakers may regularly engage with students to address their concerns and\nenhance educational curricula to better prepare them for a future that AI will\ninevitably shape.\n"
    },
    {
        "paper_id": 2305.12142,
        "authors": "Kai Ren",
        "title": "Study on Intelligent Forecasting of Credit Bond Default Risk",
        "comments": "20 pages, 9 figures, 2 tables",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Credit risk in the China's bond market has become increasingly evident,\ncreating a progressively escalating risk of default for credit bond investors.\nGiven the current incomplete and inaccurate bond information disclosure, timely\ntracking and forecasting the individual credit bond default risks have become\nessential to maintain market stability and ensure healthy development. This\npaper proposes an Intelligent Forecasting Framework for Default Risk that\nprovides precise day-by-day default risk prediction. In this framework, we\nfirst summarize the factors that impact credit bond defaults and construct a\nrisk index system. Then, we employ a combined default probability annotation\nmethod based on the evolutionary characteristics of bond default risk. The\nmethod considers the weighted average of Variational Bayesian Gaussian Mixture\nestimation, Market Index estimation, and Default Trend Backward estimation for\ndaily default risk annotation of matured or defaulted bonds according to the\nrisk index system. Moreover, to mine time-series correlation and\ncross-sectional index correlation features efficiently, an intelligent\nprediction model for Chinese credit bond default risk is designed using the\nConvLSTM neural network and trained with structured feature data. The\nexperiments demonstrate that the predicted individual bond risk is slightly\nhigher and substantially more responsive to fluctuations than the risk\nindicated by authoritative ratings, thereby improving on the inadequacies of\ninflated and untimely bond ratings. Consequently, this study's findings offer\nmultiple insights for regulators, issuers, and investors.\n"
    },
    {
        "paper_id": 2305.12179,
        "authors": "Marzio Di Vece, Frank P. Pijpers and Diego Garlaschelli",
        "title": "Commodity-specific triads in the Dutch inter-industry production network",
        "comments": "20 pages, 6 figures",
        "journal-ref": "Sci Rep 14, 3625 (2024)",
        "doi": "10.1038/s41598-024-53655-3",
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Triadic motifs are the smallest building blocks of higher-order interactions\nin complex networks and can be detected as over-occurrences with respect to\nnull models with only pair-wise interactions. Recently, the motif structure of\nproduction networks has attracted attention in light of its possible role in\nthe propagation of economic shocks. However, its characterization at the level\nof individual commodities is still poorly understood. Here we analyze both\nbinary and weighted triadic motifs in the Dutch inter-industry production\nnetwork disaggregated at the level of 187 commodity groups, which Statistics\nNetherlands reconstructed from National Accounts registers, surveys and known\nempirical data. We introduce appropriate null models that filter out node\nheterogeneity and the strong effects of link reciprocity and find that, while\nthe aggregate network that overlays all products is characterized by a\nmultitude of triadic motifs, most single-product layers feature no significant\nmotif, and roughly $85\\%$ of the layers feature only two motifs or less. This\nresult paves the way for identifying a simple `triadic fingerprint' of each\ncommodity and for reconstructing most product-specific networks from partial\ninformation in a pairwise fashion by controlling for their reciprocity\nstructure. We discuss how these results can help statistical bureaus identify\nfine-grained information in structural analyses of interest for policymakers.\n"
    },
    {
        "paper_id": 2305.12192,
        "authors": "Giampiero M. Gallo, Demetrio Lacava, Edoardo Otranto",
        "title": "Volatility jumps and the classification of monetary policy announcements",
        "comments": "22 pages, 5 tables, 6 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Central Banks interventions are frequent in response to exogenous events with\ndirect implications on financial market volatility. In this paper, we introduce\nthe Asymmetric Jump Multiplicative Error Model (AJM), which accounts for a\nspecific jump component of volatility within an intradaily framework. Taking\nthe Federal Reserve (Fed) as a reference, we propose a new model-based\nclassification of monetary announcements based on their impact on the jump\ncomponent of volatility. Focusing on a short window following each Fed's\ncommunication, we isolate the impact of monetary announcements from any\ncontamination carried by relevant events that may occur within the same\nannouncement day.\n"
    },
    {
        "paper_id": 2305.12264,
        "authors": "Masanori Hirano, Kentaro Imajo, Kentaro Minami, Takuya Shimada",
        "title": "Efficient Learning of Nested Deep Hedging using Multiple Options",
        "comments": "8 pages. 8 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Deep hedging is a framework for hedging derivatives in the presence of market\nfrictions. In this study, we focus on the problem of hedging a given target\noption by using multiple options. To extend the deep hedging framework to this\nsetting, the options used as hedging instruments also have to be priced during\ntraining. While one might use classical pricing model such as the Black-Scholes\nformula, ignoring frictions can offer arbitrage opportunities which are\nundesirable for deep hedging learning. The goal of this study is to develop a\nnested deep hedging method. That is, we develop a fully-deep approach of deep\nhedging in which the hedging instruments are also priced by deep neural\nnetworks that are aware of frictions. However, since the prices of hedging\ninstruments have to be calculated under many different conditions, the entire\nlearning process can be computationally intractable. To overcome this problem,\nwe propose an efficient learning method for nested deep hedging. Our method\nconsists of three techniques to circumvent computational intractability, each\nof which reduces redundant computations during training. We show through\nexperiments that the Black-Scholes pricing of hedge instruments can admit\nsignificant arbitrage opportunities, which are not observed when the pricing is\nperformed by deep hedging. We also demonstrate that our proposed method\nsuccessfully reduces the hedging risks compared to a baseline method that does\nnot use options as hedging instruments.\n"
    },
    {
        "paper_id": 2305.12318,
        "authors": "Arnaud Cedric Kamkoum",
        "title": "The Federal Reserve's Response to the Global Financial Crisis and Its\n  Long-Term Impact: An Interrupted Time-Series Natural Experimental Analysis",
        "comments": "The data and R codes for all the figures and tables (model estimation\n  results) in this study can be freely downloaded from Data and Code for \"The\n  Federal Reserve's Response to the Global Financial Crisis and Its Long-Term\n  Impact: An Interrupted Time-Series Natural Experimental Analysis,\" by A. C.\n  Kamkoum, 2023, OSF, Center for Open Science\n  (https://doi.org/10.17605/osf.io/t7ezj). CC BY 4.0",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  This paper examines the monetary policies the Federal Reserve implemented in\nresponse to the Global Financial Crisis. More specifically, it analyzes the\nFederal Reserve's quantitative easing (QE) programs, liquidity facilities, and\nforward guidance operations conducted from 2007 to 2018. The essay's detailed\nexamination of these policies culminates in an interrupted time-series (ITS)\nanalysis of the long-term causal effects of the QE programs on U.S. inflation\nand real GDP. The results of this formal design-based natural experimental\napproach show that the QE operations positively affected U.S. real GDP but did\nnot significantly impact U.S. inflation. Specifically, it is found that, for\nthe 2011Q2-2018Q4 post-QE period, real GDP per capita in the U.S. increased by\nan average of 231 dollars per quarter relative to how it would have changed had\nthe QE programs not been conducted. Moreover, the results show that, in 2018Q4,\nten years after the beginning of the QE programs, real GDP per capita in the\nU.S. was 14% higher relative to what it would have been during that quarter had\nthere not been the QE programs. These findings contradict Williamson's (2017)\ninformal natural experimental evidence and confirm the conclusions of VARs and\nnew Keynesian DSGE models that the Federal Reserve's QE policies positively\naffected U.S. real GDP. The results suggest that the current U.S. and worldwide\nhigh inflation rates are likely not because of the QE programs implemented in\nresponse to the financial crisis that accompanied the COVID-19 pandemic. They\nare likely due to the unprecedentedly large fiscal stimulus packages used, the\npeculiar nature of the financial downturn itself, the negative supply shocks\nfrom the war in Ukraine, or a combination of these factors. This paper is the\nfirst study to measure the macroeconomic effects of QE using a design-based\nnatural experimental approach.\n"
    },
    {
        "paper_id": 2305.12364,
        "authors": "Taeisha Nundlall, Terence L Van Zyl",
        "title": "Machine Learning for Socially Responsible Portfolio Optimisation",
        "comments": null,
        "journal-ref": null,
        "doi": "10.1145/3596947.3596966",
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  Socially responsible investors build investment portfolios intending to\nincite social and environmental advancement alongside a financial return.\nAlthough Mean-Variance (MV) models successfully generate the highest possible\nreturn based on an investor's risk tolerance, MV models do not make provisions\nfor additional constraints relevant to socially responsible (SR) investors. In\nresponse to this problem, the MV model must consider Environmental, Social, and\nGovernance (ESG) scores in optimisation. Based on the prominent MV model, this\nstudy implements portfolio optimisation for socially responsible investors. The\namended MV model allows SR investors to enter markets with competitive SR\nportfolios despite facing a trade-off between their investment Sharpe Ratio and\nthe average ESG score of the portfolio.\n"
    },
    {
        "paper_id": 2305.12539,
        "authors": "Peyman Alipour, Ali Foroush Bastani",
        "title": "Value-at-Risk-Based Portfolio Insurance: Performance Evaluation and\n  Benchmarking Against CPPI in a Markov-Modulated Regime-Switching Market",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Designing dynamic portfolio insurance strategies under market conditions\nswitching between two or more regimes is a challenging task in financial\neconomics. Recently, a promising approach employing the value-at-risk (VaR)\nmeasure to assign weights to risky and riskless assets has been proposed in\n[Jiang C., Ma Y. and An Y. \"The effectiveness of the VaR-based portfolio\ninsurance strategy: An empirical analysis\" , International Review of Financial\nAnalysis 18(4) (2009): 185-197]. In their study, the risky asset follows a\ngeometric Brownian motion with constant drift and diffusion coefficients. In\nthis paper, we first extend their idea to a regime-switching framework in which\nthe expected return of the risky asset and its volatility depend on an\nunobservable Markovian term which describes the cyclical nature of asset\nreturns in modern financial markets. We then analyze and compare the resulting\nVaR-based portfolio insurance (VBPI) strategy with the well-known constant\nproportion portfolio insurance (CPPI) strategy. In this respect, we employ a\nvariety of performance evaluation criteria such as Sharpe, Omega and Kappa\nratios to compare the two methods. Our results indicate that the CPPI strategy\nhas a better risk-return tradeoff in most of the scenarios analyzed and\nmaintains a relatively stable return profile for the resulting portfolio at the\nmaturity.\n"
    },
    {
        "paper_id": 2305.12632,
        "authors": "Masato Hisakado and Takuya Kaneko",
        "title": "Deformation of Marchenko-Pastur distribution for the correlated time\n  series",
        "comments": "20 pages, 5 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/publicdomain/zero/1.0/",
        "abstract": "  We study the eigenvalue of the Wishart matrix which is created form the time\nseries with the temporal correlation. When there is no correlation, the\neigenvalue distribution of the Wishart matrix is known as the Marchenko-Pastur\ndistribution (MPD) in the double scaling limit. When there is the temporal\ncorrelation, the eigenvalue distribution converges to the deformed MPD which\nhas longer tail and higher peak than the MPD. We discuss the moments of the\ndistribution and the convergence to the deformed MPD. We show the second moment\nincreases as the temporal correlation increases. When the temporal correlation\nis the power decay, we can confirm the phase transition. When $\\gamma>1/2$\nwhich is the power index, the second moment of the distribution is finite and\nthe largest eigenvalue is finite. On the other hand, when $\\gamma\\leq 1/2$, the\nsecond moment is infinite and the maximum eigenvalue is infinite.\n"
    },
    {
        "paper_id": 2305.12739,
        "authors": "Aman Saggu, Lennart Ante",
        "title": "The Influence of ChatGPT on Artificial Intelligence Related Crypto\n  Assets: Evidence from a Synthetic Control Analysis",
        "comments": "26 pages, 4 tables, 1 figure, 2 appendix figures",
        "journal-ref": "Finance Research Letters, 103993 (2023)",
        "doi": "10.1016/j.frl.2023.103993",
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  The introduction of OpenAI's large language model, ChatGPT, catalyzed\ninvestor attention towards artificial intelligence (AI) technologies, including\nAI-related crypto assets not directly related to ChatGPT. Utilizing the\nsynthetic difference-in-difference methodology, we identify significant\n'ChatGPT effects' with returns of AI-related crypto assets experiencing average\nreturns ranging between 10.7% and 15.6% (35.5% to 41.3%) in the one-month\n(two-month) period after the ChatGPT launch. Furthermore, Google search\nvolumes, a proxy for attention to AI, emerged as critical pricing indicators\nfor AI-related crypto post-launch. We conclude that investors perceived\nAI-assets as possessing heightened potential or value after the launch,\nresulting in higher market valuations.\n"
    },
    {
        "paper_id": 2305.12763,
        "authors": "Yiting Chen, Tracy Xiao Liu, You Shan, and Songfa Zhong",
        "title": "The Emergence of Economic Rationality of GPT",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  As large language models (LLMs) like GPT become increasingly prevalent, it is\nessential that we assess their capabilities beyond language processing. This\npaper examines the economic rationality of GPT by instructing it to make\nbudgetary decisions in four domains: risk, time, social, and food preferences.\nWe measure economic rationality by assessing the consistency of GPT's decisions\nwith utility maximization in classic revealed preference theory. We find that\nGPT's decisions are largely rational in each domain and demonstrate higher\nrationality score than those of human subjects in a parallel experiment and in\nthe literature. Moreover, the estimated preference parameters of GPT are\nslightly different from human subjects and exhibit a lower degree of\nheterogeneity. We also find that the rationality scores are robust to the\ndegree of randomness and demographic settings such as age and gender, but are\nsensitive to contexts based on the language frames of the choice situations.\nThese results suggest the potential of LLMs to make good decisions and the need\nto further understand their capabilities, limitations, and underlying\nmechanisms.\n"
    },
    {
        "paper_id": 2305.12826,
        "authors": "Abdulnasser Hatemi-J and Alan Mustafa",
        "title": "A Simulation Package in VBA to Support Finance Students for Constructing\n  Optimal Portfolios",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper introduces a software component created in Visual Basic for\nApplications (VBA) that can be applied for creating an optimal portfolio using\ntwo different methods. The first method is the seminal approach of Markowitz\nthat is based on finding budget shares via the minimization of the variance of\nthe underlying portfolio. The second method is developed by El-Khatib and\nHatemi-J, which combines risk and return directly in the optimization problem\nand yields budget shares that lead to maximizing the risk adjusted return of\nthe portfolio. This approach is consistent with the expectation of rational\ninvestors since these investors consider both risk and return as the\nfundamental basis for selection of the investment assets. Our package offers\nanother advantage that is usually neglected in the literature, which is the\nnumber of assets that should be included in the portfolio. The common practice\nis to assume that the number of assets is given exogenously when the portfolio\nis constructed. However, the current software component constructs all possible\ncombinations and thus the investor can figure out empirically which portfolio\nis the best one among all portfolios considered. The software is consumer\nfriendly via a graphical user interface. An application is also provided to\ndemonstrate how the software can be used using real-time series data for\nseveral assets.\n"
    },
    {
        "paper_id": 2305.12857,
        "authors": "Stefania Miricola, Armando Rungi, Gianluca Santoni",
        "title": "Ownership Chains in Multinational Enterprises",
        "comments": "35 pages, 5 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  In this contribution, we investigate the role of ownership chains developed\nby multinational enterprises across different national borders. First, we\ndocument that parent companies control a majority (58%) of foreign subsidiaries\nthrough indirect control relationships involving at least two countries along\nan ownership chain. Therefore, we hypothesize that locations along ownership\nchains are driven by the existence of communication costs to transmit\nmanagement decisions. In line with motivating evidence, we develop a\ntheoretical model for competition on corporate control that considers the\npossibility that parent companies in the origin countries can delegate their\nmonitoring activities in final subsidiaries to middlemen subsidiaries that are\nlocated in intermediate jurisdictions. Our model returns us a two-step\nempirical strategy with two gravity equations: i) a triangular gravity for\nestablishing a middleman by the parent, conditional on final investments'\nlocations; ii) a classical gravity for the location of final investments. First\nestimates confirm the predictions that ease of communication at the country\nlevel shapes the heterogeneous locations of subsidiaries along global ownership\nchains.\n"
    },
    {
        "paper_id": 2305.13104,
        "authors": "M. R. Ibrahim, D. U. Muhammad, B. Muhammad, J. O. Alaezi and J.\n  Agidani",
        "title": "The Key to Organizational and construction Excellence: A Study of Total\n  Quality Management",
        "comments": "19 pages 3 Postscript figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/publicdomain/zero/1.0/",
        "abstract": "  This study examines the impact of Total Quality Management (TQM) practices on\norganizational outcomes. Results show a significant relationship between TQM\npractices such as top executive commitment, education and teaching, process\ncontrol, and continuous progress, and how they can be leveraged to enhance\nperformance outcomes.\n"
    },
    {
        "paper_id": 2305.13121,
        "authors": "M. R. Ibrahim",
        "title": "The Missing Link: Exploring the Relationship Between Transformational\n  Leadership and Change in team members in Construction",
        "comments": "25 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This study aimed to investigate how transformational leadership affects team\nprocesses, mediated by change in team members. A self-administered\nquestionnaire was distributed to construction project team members in Abuja and\nKaduna, and statistical analysis revealed a significant positive relationship\nbetween transformational leadership and team processes, transformational\nleadership and change in team members, changes in team members and team\nprocesses, and changes in team members mediating the relationship between\ntransformational leadership and team processes. Future studies should consider\ncultural differences.\n"
    },
    {
        "paper_id": 2305.13123,
        "authors": "Matthieu Garcin",
        "title": "Complexity measure, kernel density estimation, bandwidth selection, and\n  the efficient market hypothesis",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-sa/4.0/",
        "abstract": "  We are interested in the nonparametric estimation of the probability density\nof price returns, using the kernel approach. The output of the method heavily\nrelies on the selection of a bandwidth parameter. Many selection methods have\nbeen proposed in the statistical literature. We put forward an alternative\nselection method based on a criterion coming from information theory and from\nthe physics of complex systems: the bandwidth to be selected maximizes a new\nmeasure of complexity, with the aim of avoiding both overfitting and\nunderfitting. We review existing methods of bandwidth selection and show that\nthey lead to contradictory conclusions regarding the complexity of the\nprobability distribution of price returns. This has also some striking\nconsequences in the evaluation of the relevance of the efficient market\nhypothesis. We apply these methods to real financial data, focusing on the\nBitcoin.\n"
    },
    {
        "paper_id": 2305.13227,
        "authors": "Orhan Koc",
        "title": "Trustless Price Feeds of Cryptocurrencies: Pathfinder",
        "comments": "4 pages, 4 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  Price feeds of securities is a critical component for many financial\nservices, allowing for collateral liquidation, margin trading, derivative\npricing and more. With the advent of blockchain technology, value in reporting\naccurate prices without a third party has become apparent. There have been many\nattempts at trying to calculate prices without a third party, in which each of\nthese attempts have resulted in being exploited by an exploiter artificially\ninflating the price. The industry has then shifted to a more centralized\ndesign, fetching price data from multiple centralized sources and then applying\nstatistical methods to reach a consensus price. Even though this strategy is\nsecure compared to reading from a single source, enough number of sources need\nto report to be able to apply statistical methods. As more sources participate\nin reporting the price, the feed gets more secure with the slowest feed\nbecoming the bottleneck for query response time, introducing a tradeoff between\nsecurity and speed. This paper provides the design and implementation details\nof a novel method to algorithmically compute security prices in a way that\nartificially inflating targeted pools has no effect on the reported price of\nthe queried asset. We hypothesize that the proposed algorithm can report\naccurate prices given a set of possibly dishonest sources.\n"
    },
    {
        "paper_id": 2305.13253,
        "authors": "Yekimov Sergiy",
        "title": "Study of the problem of reducing greenhouse gas emissions in\n  agricultural production Czech Republic",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-sa/4.0/",
        "abstract": "  Agricultural production is the main source of greenhouse gas emissions, and\ntherefore it has a great influence on the dynamics of changes in global\nwarming. The article investigated the problems faced by Czech agricultural\nproducers on the way to reduce greenhouse gas emissions. The author analyzed\nthe dynamics of greenhouse gas emissions by various branches of agriculture for\nthe period 2000-2015. The author proposed the coefficient t -covariances to\ndetermine the interdependence of the given tabular macroeconomic values. This\nindicator allows you to analyze the interdependence of macroeconomic variables\nthat do not have a normal distribution. In the context of the globalization of\nthe economy and the need to combat global warming in each country, it makes\nsense to produce primarily agricultural products that provide maximum added\nvalue with maximum greenhouse gas emissions.\n"
    },
    {
        "paper_id": 2305.13475,
        "authors": "F. Lillo, G. Livieri, S. Marmi, A. Solomko, S. Vaienti",
        "title": "Unimodal maps perturbed by heteroscedastic noise: an application to a\n  financial systems",
        "comments": "31 pages, 8 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  We investigate and prove the mathematical properties of a general class of\none-dimensional unimodal smooth maps perturbed with a heteroscedastic noise.\nSpecifically, we investigate the stability of the associated Markov chain, show\nthe weak convergence of the unique stationary measure to the invariant measure\nof the map, and show that the average Lyapunov exponent depends continuously on\nthe Markov chain parameters. Representing the Markov chain in terms of random\ntransformation enables us to state and prove the Central Limit Theorem, the\nlarge deviation principle, and the Berry-Ess\\`een inequality. We perform a\nmultifractal analysis for the invariant and the stationary measures, and we\nprove Gumbel's law for the Markov chain with an extreme index equal to 1. In\naddition, we present an example linked to the financial concept of systemic\nrisk and leverage cycle, and we use the model to investigate the finite sample\nproperties of our asymptotic results.\n"
    },
    {
        "paper_id": 2305.13532,
        "authors": "Simerjot Kaur, Andrea Stefanucci, Sameena Shah",
        "title": "InProC: Industry and Product/Service Code Classification",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Determining industry and product/service codes for a company is an important\nreal-world task and is typically very expensive as it involves manual curation\nof data about the companies. Building an AI agent that can predict these codes\nautomatically can significantly help reduce costs, and eliminate human biases\nand errors. However, unavailability of labeled datasets as well as the need for\nhigh precision results within the financial domain makes this a challenging\nproblem. In this work, we propose a hierarchical multi-class industry code\nclassifier with a targeted multi-label product/service code classifier\nleveraging advances in unsupervised representation learning techniques. We\ndemonstrate how a high quality industry and product/service code classification\nsystem can be built using extremely limited labeled dataset. We evaluate our\napproach on a dataset of more than 20,000 companies and achieved a\nclassification accuracy of more than 92\\%. Additionally, we also compared our\napproach with a dataset of 350 manually labeled product/service codes provided\nby Subject Matter Experts (SMEs) and obtained an accuracy of more than 96\\%\nresulting in real-life adoption within the financial domain.\n"
    },
    {
        "paper_id": 2305.13791,
        "authors": "Fabien Le Floc'h",
        "title": "The Quadratic Local Variance Gamma Model: an arbitrage-free\n  interpolation of class $\\mathcal{C}^3$ for option prices",
        "comments": "arXiv admin note: text overlap with arXiv:2004.08650",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  This paper generalizes the local variance gamma model of Carr and Nadtochiy,\nto a piecewise quadratic local variance function. The formulation encompasses\nthe piecewise linear Bachelier and piecewise linear Black local variance gamma\nmodels. The quadratic local variance function results in an arbitrage-free\ninterpolation of class $\\mathcal{C}^3$. The increased smoothness over the\npiecewise-constant and piecewise-linear representation allows to reduce the\nnumber of knots when interpolating raw market quotes, thus providing an\ninteresting alternative to regularization while reducing the computational\ncost.\n"
    },
    {
        "paper_id": 2305.1393,
        "authors": "Kian Tehranian",
        "title": "Monetary Policy & Stock Market",
        "comments": "Full dataset is provided in the appendix. 45 Pages, 17 Tables",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-sa/4.0/",
        "abstract": "  This paper assesses the link between central bank's policy rate, inflation\nrate and output gap through Taylor rule equation in both United States and\nUnited Kingdom from 1990 to 2020. Also, it analyses the relationship between\nmonetary policy and asset price volatility using an augmented Taylor rule.\nAccording to the literature, there has been a discussion about the utility of\nusing asset prices to evaluate central bank monetary policy decisions. First, I\nderive the equation coefficients and examine the stability of the relationship\nover the shocking period. Test the model with actual data to see its\nrobustness. I add asset price to the equation in the next step, and then test\nthe relationship by Normality, Newey-West, and GMM estimator tests. Lastly, I\nconduct comparison between USA and UK results to find out which country's\npolicy decisions can be explained better through Taylor rule.\n"
    },
    {
        "paper_id": 2305.14368,
        "authors": "Harsimrat Kaeley, Ye Qiao, Nader Bagherzadeh",
        "title": "Support for Stock Trend Prediction Using Transformers and Sentiment\n  Analysis",
        "comments": "8 pages, 3 figures, 1 table. To be published in IISES 18th Economics\n  & Finance Conference, London",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Stock trend analysis has been an influential time-series prediction topic due\nto its lucrative and inherently chaotic nature. Many models looking to\naccurately predict the trend of stocks have been based on Recurrent Neural\nNetworks (RNNs). However, due to the limitations of RNNs, such as gradient\nvanish and long-term dependencies being lost as sequence length increases, in\nthis paper we develop a Transformer based model that uses technical stock data\nand sentiment analysis to conduct accurate stock trend prediction over long\ntime windows. This paper also introduces a novel dataset containing daily\ntechnical stock data and top news headline data spanning almost three years.\nStock prediction based solely on technical data can suffer from lag caused by\nthe inability of stock indicators to effectively factor in breaking market\nnews. The use of sentiment analysis on top headlines can help account for\nunforeseen shifts in market conditions caused by news coverage. We measure the\nperformance of our model against RNNs over sequence lengths spanning 5 business\ndays to 30 business days to mimic different length trading strategies. This\nreveals an improvement in directional accuracy over RNNs as sequence length is\nincreased, with the largest improvement being close to 18.63% at 30 business\ndays.\n"
    },
    {
        "paper_id": 2305.14378,
        "authors": "Aadhitya A, Rajapriya R, Vineetha R S, Anurag M Bagde",
        "title": "Predicting Stock Market Time-Series Data using CNN-LSTM Neural Network\n  Model",
        "comments": "8 pages, 9 figures, 5 tables",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  Stock market is often important as it represents the ownership claims on\nbusinesses. Without sufficient stocks, a company cannot perform well in\nfinance. Predicting a stock market performance of a company is nearly hard\nbecause every time the prices of a company stock keeps changing and not\nconstant. So, its complex to determine the stock data. But if the previous\nperformance of a company in stock market is known, then we can track the data\nand provide predictions to stockholders in order to wisely take decisions on\nhandling the stocks to a company. To handle this, many machine learning models\nhave been invented but they didn't succeed due to many reasons like absence of\nadvanced libraries, inaccuracy of model when made to train with real time data\nand much more. So, to track the patterns and the features of data, a CNN-LSTM\nNeural Network can be made. Recently, CNN is now used in Natural Language\nProcessing (NLP) based applications, so by identifying the features from stock\ndata and converting them into tensors, we can obtain the features and then send\nit to LSTM neural network to find the patterns and thereby predicting the stock\nmarket for given period of time. The accuracy of the CNN-LSTM NN model is found\nto be high even when allowed to train on real-time stock market data. This\npaper describes about the features of the custom CNN-LSTM model, experiments we\nmade with the model (like training with stock market datasets, performance\ncomparison with other models) and the end product we obtained at final stage.\n"
    },
    {
        "paper_id": 2305.14382,
        "authors": "Yuze Lu, Hailong Zhang, Qiwen Guo",
        "title": "Stock and market index prediction using Informer network",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Applications of deep learning in financial market prediction has attracted\nhuge attention from investors and researchers. In particular, intra-day\nprediction at the minute scale, the dramatically fluctuating volume and stock\nprices within short time periods have posed a great challenge for the\nconvergence of networks result. Informer is a more novel network, improved on\nTransformer with smaller computational complexity, longer prediction length and\nglobal time stamp features. We have designed three experiments to compare\nInformer with the commonly used networks LSTM, Transformer and BERT on 1-minute\nand 5-minute frequencies for four different stocks/ market indices. The\nprediction results are measured by three evaluation criteria: MAE, RMSE and\nMAPE. Informer has obtained best performance among all the networks on every\ndataset. Network without the global time stamp mechanism has significantly\nlower prediction effect compared to the complete Informer; it is evident that\nthis mechanism grants the time series to the characteristics and substantially\nimproves the prediction accuracy of the networks. Finally, transfer learning\ncapability experiment is conducted, Informer also achieves a good performance.\nInformer has good robustness and improved performance in market prediction,\nwhich can be exactly adapted to real trading.\n"
    },
    {
        "paper_id": 2305.14478,
        "authors": "Lars Vilhuber",
        "title": "Reproducibility and Transparency versus Privacy and Confidentiality:\n  Reflections from a Data Editor",
        "comments": null,
        "journal-ref": null,
        "doi": "10.1016/j.jeconom.2023.05.001",
        "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
        "abstract": "  Transparency and reproducibility are often seen in opposition to privacy and\nconfidentiality. Data that need to be kept confidential are seen as an\nimpediment to reproducibility, and privacy would seem to inhibit transparency.\nI bring a more nuanced view to the discussion, and show, using examples from\nover 1,000 reproducibility assessments, that confidential data can very well be\nused in reproducible and transparent research. The key insight is that access\nto most confidential data, while tedious, is open to hundreds if not thousands\nof researchers. In cases where few researchers can consider accessing such data\nin the future, reproducibility services, such as those provided by some\njournals, can provide some evidence for effective reproducibility even when the\nsame data may not be available for future research.\n"
    },
    {
        "paper_id": 2305.14604,
        "authors": "Jason Milionis, Ciamac C. Moallemi, Tim Roughgarden",
        "title": "Automated Market Making and Arbitrage Profits in the Presence of Fees",
        "comments": "27 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We consider the impact of trading fees on the profits of arbitrageurs trading\nagainst an automated marker marker (AMM) or, equivalently, on the adverse\nselection incurred by liquidity providers due to arbitrage. We extend the model\nof Milionis et al. [2022] for a general class of two asset AMMs to both\nintroduce fees and discrete Poisson block generation times. In our setting, we\nare able to compute the expected instantaneous rate of arbitrage profit in\nclosed form. When the fees are low, in the fast block asymptotic regime, the\nimpact of fees takes a particularly simple form: fees simply scale down\narbitrage profits by the fraction of time that an arriving arbitrageur finds a\nprofitable trade.\n"
    },
    {
        "paper_id": 2305.14605,
        "authors": "Francisco Rodr\\'iguez",
        "title": "Estimating causal effects of sanctions impacts: what role for\n  country-level studies?",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  This article reviews recent advances in addressing empirical identification\nissues in cross-country and country-level studies and their implications for\nthe identification of the effectiveness and consequences of economic sanctions.\nI argue that, given the difficulties in assessing causal relationships in\ncross-national data, country-level case studies can serve as a useful and\ninformative complement to cross-national regression studies. However, I also\nwarn that case studies pose a set of additional potential empirical pitfalls\nwhich can obfuscate rather than clarify the identification of causal mechanisms\nat work. Therefore, the most sensible way to read case study evidence is as a\ncomplement rather than as a substitute to cross-national research.\n"
    },
    {
        "paper_id": 2305.14672,
        "authors": "Xinmei Yang and Abhishek Arora and Shao-Yu Jheng and Melissa Dell",
        "title": "Quantifying Character Similarity with Vision Transformers",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Record linkage is a bedrock of quantitative social science, as analyses often\nrequire linking data from multiple, noisy sources. Off-the-shelf string\nmatching methods are widely used, as they are straightforward and cheap to\nimplement and scale. Not all character substitutions are equally probable, and\nfor some settings there are widely used handcrafted lists denoting which string\nsubstitutions are more likely, that improve the accuracy of string matching.\nHowever, such lists do not exist for many settings, skewing research with\nlinked datasets towards a few high-resource contexts that are not\nrepresentative of the diversity of human societies. This study develops an\nextensible way to measure character substitution costs for OCR'ed documents, by\nemploying large-scale self-supervised training of vision transformers (ViT)\nwith augmented digital fonts. For each language written with the CJK script, we\ncontrastively learn a metric space where different augmentations of the same\ncharacter are represented nearby. In this space, homoglyphic characters - those\nwith similar appearance such as ``O'' and ``0'' - have similar vector\nrepresentations. Using the cosine distance between characters' representations\nas the substitution cost in an edit distance matching algorithm significantly\nimproves record linkage compared to other widely used string matching methods,\nas OCR errors tend to be homoglyphic in nature. Homoglyphs can plausibly\ncapture character visual similarity across any script, including low-resource\nsettings. We illustrate this by creating homoglyph sets for 3,000 year old\nancient Chinese characters, which are highly pictorial. Fascinatingly, a ViT is\nable to capture relationships in how different abstract concepts were\nconceptualized by ancient societies, that have been noted in the archaeological\nliterature.\n"
    },
    {
        "paper_id": 2305.14698,
        "authors": "Dorothy Kronick and Francisco Rodr\\'iguez",
        "title": "Political Conflict and Economic Growth in Post-Independence Venezuela",
        "comments": "28 pages, 3 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Venezuela has suffered three economic catastrophes since independence: one\neach in the nineteenth, twentieth, and twenty-first centuries. Prominent\nexplanations for this trilogy point to the interaction of class conflict and\nresource dependence. We turn attention to intra-class conflict, arguing that\nthe most destructive policy choices stemmed not from the rich defending\nthemselves against the masses but rather from pitched battles among elites.\nOthers posit that Venezuelan political institutions failed to sustain growth\nbecause they were insufficiently inclusive; we suggest in addition that they\ninadequately mediated intra-elite conflict.\n"
    },
    {
        "paper_id": 2305.15364,
        "authors": "Hanchao Liu, Dena Firoozi, Mich\\`ele Breton",
        "title": "LQG Risk-Sensitive Single-Agent and Major-Minor Mean Field Game Systems:\n  A Variational Framework",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We develop a variational approach to address risk-sensitive optimal control\nproblems with an exponential-of-integral cost functional in a general\nlinear-quadratic-Gaussian (LQG) single-agent setup, offering new insights into\nsuch problems. Our analysis leads to the derivation of a nonlinear necessary\nand sufficient condition of optimality, expressed in terms of martingale\nprocesses. Subject to specific conditions, we find an equivalent risk-neutral\nmeasure, under which a linear state feedback form can be obtained for the\noptimal control. It is then shown that the obtained feedback control is\nconsistent with the imposed condition and remains optimal under the original\nmeasure. Building upon this development, we (i) propose a variational framework\nfor general LQG risk-sensitive mean-field games (MFGs) and (ii) advance the LQG\nrisk-sensitive MFG theory by incorporating a major agent in the framework. The\nmajor agent interacts with a large number of minor agents, and unlike the minor\nagents, its influence on the system remains significant even with an increasing\nnumber of minor agents. We derive the Markovian closed-loop best-response\nstrategies of agents in the limiting case where the number of agents goes to\ninfinity. We establish that the set of obtained best-response strategies yields\na Nash equilibrium in the limiting case and an $\\varepsilon$-Nash equilibrium\nin the finite-player case. We conclude the paper by presenting illustrative\nnumerical experiments.\n"
    },
    {
        "paper_id": 2305.15821,
        "authors": "Hong Guo, Jianwu Lin and Fanlin Huang",
        "title": "Market Making with Deep Reinforcement Learning from Limit Order Books",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Market making (MM) is an important research topic in quantitative finance,\nthe agent needs to continuously optimize ask and bid quotes to provide\nliquidity and make profits. The limit order book (LOB) contains information on\nall active limit orders, which is an essential basis for decision-making. The\nmodeling of evolving, high-dimensional and low signal-to-noise ratio LOB data\nis a critical challenge. Traditional MM strategy relied on strong assumptions\nsuch as price process, order arrival process, etc. Previous reinforcement\nlearning (RL) works handcrafted market features, which is insufficient to\nrepresent the market. This paper proposes a RL agent for market making with LOB\ndata. We leverage a neural network with convolutional filters and attention\nmechanism (Attn-LOB) for feature extraction from LOB. We design a new\ncontinuous action space and a hybrid reward function for the MM task. Finally,\nwe conduct comprehensive experiments on latency and interpretability, showing\nthat our agent has good applicability.\n"
    },
    {
        "paper_id": 2305.16088,
        "authors": "Saeed Nosratabadi, Thabit Atobishi and Szilard HegedHus",
        "title": "Social Sustainability of Digital Transformation: Empirical Evidence from\n  EU-27 Countries",
        "comments": null,
        "journal-ref": "Administrative Sciences, 2023",
        "doi": "10.3390/admsci13050126",
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  In the EU-27 countries, the importance of social sustainability of digital\ntransformation (SOSDIT) is heightened by the need to balance economic growth\nwith social cohesion. By prioritizing SOSDIT, the EU can ensure that its\ncitizens are not left behind in the digital transformation process and that\ntechnology serves the needs of all Europeans. Therefore, the current study\naimed firstly to evaluate the SOSDIT of EU-27 countries and then to model its\nimportance in reaching sustainable development goals (SDGs). The current study,\nusing structural equation modeling, provided quantitative empirical evidence\nthat digital transformation in Finland, the Netherlands, and Denmark are\nrespectively most socially sustainable. It is also found that SOSDIT leads the\ncountries to have a higher performance in reaching SDGs. Finally, the study\nprovided evidence implying the inverse relationship between the Gini\ncoefficient and reaching SDGs. In other words, the higher the Gini coefficient\nof a country, the lower its performance in reaching SDGs. The findings of this\nstudy contribute to the literature of sustainability and digitalization. It\nalso provides empirical evidence regarding the SOSDIT level of EU-27 countries\nthat can be a foundation for the development of policies to improve the\nsustainability of digital transformation. According to the findings, this study\nprovides practical recommendations for countries to ensure that their digital\ntransformation is sustainable and has a positive impact on society.\n"
    },
    {
        "paper_id": 2305.16095,
        "authors": "Parvaneh Bahrami, Saeed Nosratabadi, Khodayar Palouzian and Szilard\n  Hegedus",
        "title": "Modeling the Impact of Mentoring on Women's Work-LifeBalance: A Grounded\n  Theory Approach",
        "comments": null,
        "journal-ref": "Administrative Sciences, 2023",
        "doi": "10.3390/admsci13010006",
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  The purpose of this study was to model the impact of mentoring on women's\nwork-life balance. Indeed, this study considered mentoring as a solution to\ncreate a work-life balance of women. For this purpose, semi-structured\ninterviews with both mentors and mentees of Tehran Municipality were conducted\nand the collected data were analyzed using constructivist grounded theory.\nFindings provided a model of how mentoring affects women's work-life balance.\nAccording to this model, role management is the key criterion for work-life\nbalancing among women. In this model, antecedents of role management and the\ncontextual factors affecting role management, the constraints of mentoring in\nthe organization, as well as the consequences of effective mentoring in the\norganization are described. The findings of this research contribute to the\nmentoring literature as well as to the role management literature and provide\nrecommendations for organizations and for future research.\n"
    },
    {
        "paper_id": 2305.16152,
        "authors": "Areski Cousin (IRMA), J\\'er\\^ome Lelong (DAO), Tom Picard (DAO)",
        "title": "Mean-variance dynamic portfolio allocation with transaction costs: a\n  Wiener chaos expansion approach",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper studies the multi-period mean-variance portfolio allocation\nproblem with transaction costs. Many methods have been proposed these last\nyears to challenge the famous uni-period Markowitz strategy.But these methods\ncannot integrate transaction costs or become computationally heavy and hardly\napplicable. In this paper, we try to tackle this allocation problem by\nproposing an innovative approach which relies on representing the set of\nadmissible portfolios by a finite dimensional Wiener chaos expansion. This\nnumerical method is able to find an optimal strategy for the allocation problem\nsubject to transaction costs. To complete the study, the link between optimal\nportfolios submitted to transaction costs and the underlying risk aversion is\ninvestigated. Then a competitive and compliant benchmark based on the\nsequential uni-period Markowitz strategy is built to highlight the efficiency\nof our approach.\n"
    },
    {
        "paper_id": 2305.16164,
        "authors": "Travis Adams, Andrea Ajello, Diego Silva, Francisco Vazquez-Grande",
        "title": "More than Words: Twitter Chatter and Financial Market Sentiment",
        "comments": "29 pages, 6 Tables, 12 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  We build a new measure of credit and financial market sentiment using Natural\nLanguage Processing on Twitter data. We find that the Twitter Financial\nSentiment Index (TFSI) correlates highly with corporate bond spreads and other\nprice- and survey-based measures of financial conditions. We document that\novernight Twitter financial sentiment helps predict next day stock market\nreturns. Most notably, we show that the index contains information that helps\nforecast changes in the U.S. monetary policy stance: a deterioration in Twitter\nfinancial sentiment the day ahead of an FOMC statement release predicts the\nsize of restrictive monetary policy shocks. Finally, we document that sentiment\nworsens in response to an unexpected tightening of monetary policy.\n"
    },
    {
        "paper_id": 2305.16238,
        "authors": "Suchismita Banerjee",
        "title": "Role of Neighbouring Wealth Preference in Kinetic Exchange model of\n  market",
        "comments": "13 pages, 8 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The kinetic exchange model has gained popularity in the field of statistical\nmechanics for investigating wealth interaction. Traditionally, kinetic exchange\nmodels have been studied without considering preferential interactions.\nHowever, in this study, we introduce two types of preferential interactions to\nexplore wealth dynamics and its associated distributions. In the first\npreference, one agent is randomly selected, while the other agent is chosen\nrandomly with wealth just above or below the first agent. Through this\npreference, we observe the emergence of a quasi-oligarchic society, where the\nmajority of the wealth cycles around the hand of very few agents. For the\nsecond preference, we impose a constraint on the difference in pre-interaction\nwealth between the two agents. This preference leads to the segregation of\nsociety into two distinct economic classes. To investigate these phenomena, we\nconducted extensive Monte Carlo simulations, enabling us to characterize the\nbehavior of wealth distributions in these two scenarios. Our findings shed\nlight on the dynamics of wealth accumulation and distribution within\npreferential interactions in the context of the kinetic exchange model.\n"
    },
    {
        "paper_id": 2305.16255,
        "authors": "Paul Ghelasi, Florian Ziel",
        "title": "Hierarchical forecasting for aggregated curves with an application to\n  day-ahead electricity price auctions",
        "comments": "34 pages, 6 figures. International Journal of Forecasting (2022)",
        "journal-ref": null,
        "doi": "10.1016/j.ijforecast.2022.11.004",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Aggregated curves are common structures in economics and finance, and the\nmost prominent examples are supply and demand curves. In this study, we exploit\nthe fact that all aggregated curves have an intrinsic hierarchical structure,\nand thus hierarchical reconciliation methods can be used to improve the\nforecast accuracy. We provide an in-depth theory on how aggregated curves can\nbe constructed or deconstructed, and conclude that these methods are equivalent\nunder weak assumptions. We consider multiple reconciliation methods for\naggregated curves, including previously established bottom-up, top-down, and\nlinear optimal reconciliation approaches. We also present a new benchmark\nreconciliation method called 'aggregated-down' with similar complexity to\nbottom-up and top-down approaches, but it tends to provide better accuracy in\nthis setup. We conducted an empirical forecasting study on the German day-ahead\npower auction market by predicting the demand and supply curves, where their\nequilibrium determines the electricity price for the next day. Our results\ndemonstrate that hierarchical reconciliation methods can be used to improve the\nforecasting accuracy of aggregated curves.\n"
    },
    {
        "paper_id": 2305.16274,
        "authors": "Zacharia Issa and Blanka Horvath and Maud Lemercier and Cristopher\n  Salvi",
        "title": "Non-adversarial training of Neural SDEs with signature kernel scores",
        "comments": "Code available at https://github.com/issaz/sigker-nsdes/",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Neural SDEs are continuous-time generative models for sequential data.\nState-of-the-art performance for irregular time series generation has been\npreviously obtained by training these models adversarially as GANs. However, as\ntypical for GAN architectures, training is notoriously unstable, often suffers\nfrom mode collapse, and requires specialised techniques such as weight clipping\nand gradient penalty to mitigate these issues. In this paper, we introduce a\nnovel class of scoring rules on pathspace based on signature kernels and use\nthem as objective for training Neural SDEs non-adversarially. By showing strict\nproperness of such kernel scores and consistency of the corresponding\nestimators, we provide existence and uniqueness guarantees for the minimiser.\nWith this formulation, evaluating the generator-discriminator pair amounts to\nsolving a system of linear path-dependent PDEs which allows for\nmemory-efficient adjoint-based backpropagation. Moreover, because the proposed\nkernel scores are well-defined for paths with values in infinite dimensional\nspaces of functions, our framework can be easily extended to generate\nspatiotemporal data. Our procedure permits conditioning on a rich variety of\nmarket conditions and significantly outperforms alternative ways of training\nNeural SDEs on a variety of tasks including the simulation of rough volatility\nmodels, the conditional probabilistic forecasts of real-world forex pairs where\nthe conditioning variable is an observed past trajectory, and the mesh-free\ngeneration of limit order book dynamics.\n"
    },
    {
        "paper_id": 2305.16364,
        "authors": "Zikai Wei, Bo Dai, Dahua Lin",
        "title": "E2EAI: End-to-End Deep Learning Framework for Active Investing",
        "comments": "12 pages, 3 figures, Factoring Investing, Portfolio Management",
        "journal-ref": null,
        "doi": "10.1145/3604237.3626848",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Active investing aims to construct a portfolio of assets that are believed to\nbe relatively profitable in the markets, with one popular method being to\nconstruct a portfolio via factor-based strategies. In recent years, there have\nbeen increasing efforts to apply deep learning to pursue \"deep factors'' with\nmore active returns or promising pipelines for asset trends prediction.\nHowever, the question of how to construct an active investment portfolio via an\nend-to-end deep learning framework (E2E) is still open and rarely addressed in\nexisting works. In this paper, we are the first to propose an E2E that covers\nalmost the entire process of factor investing through factor selection, factor\ncombination, stock selection, and portfolio construction. Extensive experiments\non real stock market data demonstrate the effectiveness of our end-to-end deep\nleaning framework in active investing.\n"
    },
    {
        "paper_id": 2305.16377,
        "authors": "Tijs W. Alleman and Koen Schoors and Jan M. Baetens",
        "title": "Validating a dynamic input-output model for the propagation of supply\n  and demand shocks during the COVID-19 pandemic in Belgium",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  This work validates a dynamic production network model, used to quantify the\nimpact of economic shocks caused by COVID-19 in the UK, using data for Belgium.\nBecause the model was published early during the 2020 COVID-19 pandemic, it\nrelied on several assumptions regarding the magnitude of the observed economic\nshocks, for which more accurate data have become available in the meantime. We\nrefined the propagated shocks to align with observed data collected during the\npandemic and calibrated some less well-informed parameters using 115 economic\ntime series. The refined model effectively captures the evolution of GDP,\nrevenue, and employment during the COVID-19 pandemic in Belgium at both\nindividual economic activity and aggregate levels. However, the reduction in\nbusiness-to-business demand is overestimated, revealing structural shortcomings\nin accounting for businesses' motivations to sustain trade despite the\npandemic's induced shocks. We confirm that the relaxation of the stringent\nLeontief production function by a survey on the criticality of inputs\nsignificantly improved the model's accuracy. However, despite a large dataset,\ndistinguishing between varying degrees of relaxation proved challenging.\nOverall, this work demonstrates the model's validity in assessing the impact of\neconomic shocks caused by an epidemic in Belgium.\n"
    },
    {
        "paper_id": 2305.16434,
        "authors": "Irena Barja\\v{s}i\\'c, Stefano Battiston, Vinko Zlati\\'c",
        "title": "Credit Valuation Adjustment in Financial Networks",
        "comments": "31 pages, 7 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Credit Valuation Adjustment captures the difference in the value of\nderivative contracts when the counterparty default probability is taken into\naccount. However, in the context of a network of contracts, the default\nprobability of a direct counterparty can depend substantially on the default\nprobabilities of indirect counterparties. We develop a model to clarify when\nand how these network effects matter for CVA, in particular in the presence of\ncorrelation among counterparties defaults. We provide an approximate analytical\nsolution for the default probabilities. This solution allows for identifying\nconditions on key parameters such as network degree, leverage and correlation,\nwhere network effects yield large differences in CVA (e.g. above 50%), and thus\nrelevant for practical applications. Moreover, we find evidence that network\neffects induce a multi-modal distribution of CVA values.\n"
    },
    {
        "paper_id": 2305.16632,
        "authors": "Chniguir Mounira and Henchiri Jamel Eddine",
        "title": "Causality between investor sentiment and the shares return on the\n  Moroccan and Tunisian financial markets",
        "comments": "15 pages, 16 tables",
        "journal-ref": "CHNIGUIR M. & HENCHIRI J.E.(2022) Causality between investor\n  sentiment and the shares return on the Moroccan and Tunisian financial\n  markets, Revue du controle, de la comptabilite et de laudit Volume 6: numero\n  4 pp: 350 -364",
        "doi": "10.5281/zenodo.7780959",
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  This paper aims to test the relationship between investor sentiment and the\nprofitability of stocks listed on two emergent financial markets, the Moroccan\nand Tunisian ones. Two indirect measures of investor sentiment are used, SENT\nand ARMS. These sentiment indicators show that there is an important\nrelationship between the stocks returns and investor sentiment. Indeed, the\nresults of modeling investor sentiment by past observations show that sentiment\nhas weak memory; on the other hand, series of changes in sentiment have\nsignificant memory. The results of the Granger causality test between stock\nreturn and investor sentiment show us that profitability causes investor\nsentiment and not the other way around for the two financial markets\nstudied.Thanks to four autoregressive relationships estimated between investor\nsentiment, change in sentiment, stock return and change in stock return, we\nfind firstly that the returns predict the changes in sentiments which confirms\nwith our hypothesis and secondly, the variation in profitability negatively\naffects investor sentiment.We conclude that whatever sentiment measure is used\nthere is a positive and significant relationship between investor sentiment and\nprofitability, but sentiment cannot be predicted from our various variables.\n"
    },
    {
        "paper_id": 2305.16633,
        "authors": "Agam Shah and Sudheer Chava",
        "title": "Zero is Not Hero Yet: Benchmarking Zero-Shot Performance of LLMs for\n  Financial Tasks",
        "comments": "Working Paper",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Recently large language models (LLMs) like ChatGPT have shown impressive\nperformance on many natural language processing tasks with zero-shot. In this\npaper, we investigate the effectiveness of zero-shot LLMs in the financial\ndomain. We compare the performance of ChatGPT along with some open-source\ngenerative LLMs in zero-shot mode with RoBERTa fine-tuned on annotated data. We\naddress three inter-related research questions on data annotation, performance\ngaps, and the feasibility of employing generative models in the finance domain.\nOur findings demonstrate that ChatGPT performs well even without labeled data\nbut fine-tuned models generally outperform it. Our research also highlights how\nannotating with generative models can be time-intensive. Our codebase is\npublicly available on GitHub under CC BY-NC 4.0 license.\n"
    },
    {
        "paper_id": 2305.16712,
        "authors": "Shashwat Mishra and Rishabh Raj and Siddhartha P. Chakrabarty",
        "title": "Green portfolio optimization: A scenario analysis and stress testing\n  based novel approach for sustainable investing in the paradigm Indian markets",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  In this article, we present a novel approach for the construction of an\nenvironment-friendly green portfolio using the ESG ratings, and application of\nthe modern portfolio theory to present what we call as the ``green efficient\nfrontier'' (wherein the environmental score is included as a third dimension to\nthe traditional mean-variance framework). Based on the prevailing action levels\nand policies, as well as additional market information, scenario analyses and\nstress testing are conducted to anticipate the future performance of the green\nportfolio in varying circumstances. The performance of the green portfolio is\nevaluated against the market returns in order to highlight the importance of\nsustainable investing and recognizing climate risk as a significant risk factor\nin financial analysis.\n"
    },
    {
        "paper_id": 2305.16842,
        "authors": "Germ\\`a Coenders (1) and N\\'uria Arimany Serrat (2) ((1) University of\n  Girona, (2) University of Vic - Central University of Catalonia)",
        "title": "Accounting statement analysis at industry level. A gentle introduction\n  to the compositional approach",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Compositional data are contemporarily defined as positive vectors, the ratios\namong whose elements are of interest to the researcher. Financial statement\nanalysis by means of accounting ratios fulfils this definition to the letter.\nCompositional data analysis solves the major problems in statistical analysis\nof standard financial ratios at industry level, such as skewness,\nnon-normality, non-linearity and dependence of the results on the choice of\nwhich accounting figure goes to the numerator and to the denominator of the\nratio. In spite of this, compositional applications to financial statement\nanalysis are still rare. In this article, we present some transformations\nwithin compositional data analysis that are particularly useful for financial\nstatement analysis. We show how to compute industry or sub-industry means of\nstandard financial ratios from a compositional perspective. We show how to\nvisualise firms in an industry with a compositional biplot, to classify them\nwith compositional cluster analysis and to relate financial and non-financial\nindicators with compositional regression models. We show an application to the\naccounting statements of Spanish wineries using DuPont analysis, and a\nstep-by-step tutorial to the compositional freeware CoDaPack.\n"
    },
    {
        "paper_id": 2305.16872,
        "authors": "Joshua Gans and Abhishek Nagaraj",
        "title": "The Economics of Augmented and Virtual Reality",
        "comments": "13 pages, 1 table",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper explores the economics of Augmented Reality (AR) and Virtual\nReality (VR) technologies within decision-making contexts. Two metrics are\nproposed: Context Entropy, the informational complexity of an environment, and\nContext Immersivity, the value from full immersion. The analysis suggests that\nAR technologies assist in understanding complex contexts, while VR technologies\nprovide access to distant, risky, or expensive environments. The paper provides\na framework for assessing the value of AR and VR applications in various\nbusiness sectors by evaluating the pre-existing context entropy and context\nimmersivity. The goal is to identify areas where immersive technologies can\nsignificantly impact and distinguish those that may be overhyped.\n"
    },
    {
        "paper_id": 2305.16915,
        "authors": "Victor Le Coz, Iacopo Mastromatteo, Damien Challet and Michael\n  Benzaquen",
        "title": "When is cross impact relevant?",
        "comments": "17 pages, 21 figures, 1 table",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Trading pressure from one asset can move the price of another, a phenomenon\nreferred to as cross impact. Using tick-by-tick data spanning 5 years for 500\nassets listed in the United States, we identify the features that make\ncross-impact relevant to explain the variance of price returns. We show that\nprice formation occurs endogenously within highly liquid assets. Then, trades\nin these assets influence the prices of less liquid correlated products, with\nan impact velocity constrained by their minimum trading frequency. We\ninvestigate the implications of such a multidimensional price formation\nmechanism on interest rate markets. We find that the 10-year bond future serves\nas the primary liquidity reservoir, influencing the prices of cash bonds and\nfutures contracts within the interest rate curve. Such behaviour challenges the\nvalidity of the theory in Financial Economics that regards long-term rates as\nagents anticipations of future short term rates.\n"
    },
    {
        "paper_id": 2305.17177,
        "authors": "John C. Stevenson",
        "title": "Local Sharing and Sociality Effects on Wealth Inequality in a Simple\n  Artificial Society",
        "comments": "2 tables, 1 algorithm PDL, and 8 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Redistribution of resources within a group as a method to reduce wealth\ninequality is a current area of debate. The evolutionary path to or away from\nwealth sharing is also a subject of active research. In order to investigate\neffects and evolution of wealth sharing, societies are simulated using a\nminimal model of a complex adapting system. These simulations demonstrate, for\nthis artificial foraging society, that local sharing of resources reduces the\neconomy's total wealth and increases wealth inequality. Evolutionary pressures\nstrongly select against local sharing, whether globally or within a\nindividual's clan, and select for asocial behaviors. By holding constant the\ngene for sharing resources among neighbors, from rich to poor, either with\neveryone or only within members of the same clan, social behavior is selected\nbut total wealth and mean age are substantially reduced relative to non-sharing\nsocieties. The Gini coefficient is shown to be ineffective in measuring these\nchanges in total wealth and wealth distributions, and, therefore, individual\nwell-being. Only with sociality do strategies emerge that allow sharing clans\nto exclude or coexist with non-sharing clans. These strategies are based on\nspatial effects, emphasizing the importance of modeling movement mediated\ncommunity assembly and coexistence as well as sociality.\n"
    },
    {
        "paper_id": 2305.17285,
        "authors": "Andrea Gabrielli, Valentina Macchiati, Diego Garlaschelli",
        "title": "Critical density for network reconstruction",
        "comments": null,
        "journal-ref": "In: Cantone, D., Pulvirenti, A. (eds) From Computational Logic to\n  Computational Biology. Lecture Notes in Computer Science, vol 14070.\n  Springer, Cham (2024)",
        "doi": "10.1007/978-3-031-55248-9_11",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The structure of many financial networks is protected by privacy and has to\nbe inferred from aggregate observables. Here we consider one of the most\nsuccessful network reconstruction methods, producing random graphs with desired\nlink density and where the observed constraints (related to the market size of\neach node) are replicated as averages over the graph ensemble, but not in\nindividual realizations. We show that there is a minimum critical link density\nbelow which the method exhibits an `unreconstructability' phase where at least\none of the constraints, while still reproduced on average, is far from its\nexpected value in typical individual realizations. We establish the scaling of\nthe critical density for various theoretical and empirical distributions of\ninterbank assets and liabilities, showing that the threshold differs from the\ncritical densities for the onset of the giant component and of the unique\ncomponent in the graph. We also find that, while dense networks are always\nreconstructable, sparse networks are unreconstructable if their structure is\nhomogeneous, while they can display a crossover to reconstructability if they\nhave an appropriate core-periphery or heterogeneous structure. Since the\nreconstructability of interbank networks is related to market clearing, our\nresults suggest that central bank interventions aimed at lowering the density\nof links should take network structure into account to avoid unintentional\nliquidity crises where the supply and demand of all financial institutions\ncannot be matched simultaneously.\n"
    },
    {
        "paper_id": 2305.17344,
        "authors": "Div Bhagia",
        "title": "Duration Dependence and Heterogeneity: Learning from Early Notice of\n  Layoff",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper presents a novel approach to distinguish the impact of\nduration-dependent forces and adverse selection on the exit rate from\nunemployment by leveraging variation in the length of layoff notices. I\nformulate a Mixed Hazard model in discrete time and specify the conditions\nunder which variation in notice length enables the identification of structural\nduration dependence while allowing for arbitrary heterogeneity across workers.\nUtilizing data from the Displaced Worker Supplement (DWS), I employ the\nGeneralized Method of Moments (GMM) to estimate the model. According to the\nestimates, the decline in the exit rate over the first 48 weeks of unemployment\nis largely due to the worsening composition of surviving jobseekers.\nFurthermore, I find that an individual's likelihood of exiting unemployment\ndecreases initially, then increases until unemployment benefits run out, and\nremains steady thereafter. These findings are consistent with a standard search\nmodel where returns to search decline early in the spell.\n"
    },
    {
        "paper_id": 2305.17419,
        "authors": "Ben Moews",
        "title": "On random number generators and practical market efficiency",
        "comments": "Accepted for publication in Journal of the Operational Research\n  Society",
        "journal-ref": null,
        "doi": "10.1080/01605682.2023.2219292",
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Modern mainstream financial theory is underpinned by the efficient market\nhypothesis, which posits the rapid incorporation of relevant information into\nasset pricing. Limited prior studies in the operational research literature\nhave investigated tests designed for random number generators to check for\nthese informational efficiencies. Treating binary daily returns as a hardware\nrandom number generator analogue, tests of overlapping permutations have\nindicated that these time series feature idiosyncratic recurrent patterns.\nContrary to prior studies, we split our analysis into two streams at the annual\nand company level, and investigate longer-term efficiency over a larger time\nframe for Nasdaq-listed public companies to diminish the effects of trading\nnoise and allow the market to realistically digest new information. Our results\ndemonstrate that information efficiency varies across years and reflects\nlarge-scale market impacts such as financial crises. We also show the proximity\nto results of a well-tested pseudo-random number generator, discuss the\ndistinction between theoretical and practical market efficiency, and find that\nthe statistical qualification of stock-separated returns in support of the\nefficient market hypothesis is dependent on the driving factor of small\ninefficient subsets that skew market assessments.\n"
    },
    {
        "paper_id": 2305.17457,
        "authors": "Elias Zavitsanos, Dimitris Mavroeidis, Konstantinos Bougiatiotis,\n  Eirini Spyropoulou, Lefteris Loukas, Georgios Paliouras",
        "title": "Financial misstatement detection: a realistic evaluation",
        "comments": "9 pages, ICAIF2021",
        "journal-ref": "Proceedings of the Second ACM International Conference on AI in\n  Finance, no 34, 2021",
        "doi": "10.1145/3490354.3494453",
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  In this work, we examine the evaluation process for the task of detecting\nfinancial reports with a high risk of containing a misstatement. This task is\noften referred to, in the literature, as ``misstatement detection in financial\nreports''. We provide an extensive review of the related literature. We propose\na new, realistic evaluation framework for the task which, unlike a large part\nof the previous work: (a) focuses on the misstatement class and its rarity, (b)\nconsiders the dimension of time when splitting data into training and test and\n(c) considers the fact that misstatements can take a long time to detect. Most\nimportantly, we show that the evaluation process significantly affects system\nperformance, and we analyze the performance of different models and feature\ntypes in the new realistic framework.\n"
    },
    {
        "paper_id": 2305.17474,
        "authors": "Goran Petrevski",
        "title": "Macroeconomic Effects of Inflation Targeting: A Survey of the Empirical\n  Literature",
        "comments": "109 pages, a survey of the empirical literature on macroeconomic\n  effects of inflation targeting",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
        "abstract": "  This paper surveys the empirical literature of inflation targeting. The main\nfindings from our review are the following: there is robust empirical evidence\nthat larger and more developed countries are more likely to adopt the IT\nregime; the introduction of this regime is conditional on previous\ndisinflation, greater exchange rate flexibility, central bank independence, and\nhigher level of financial development; the empirical evidence has failed to\nprovide convincing evidence that IT itself may serve as an effective tool for\nstabilizing inflation expectations and for reducing inflation persistence; the\nempirical research focused on advanced economies has failed to provide\nconvincing evidence on the beneficial effects of IT on inflation performance,\nwhile there is some evidence that the gains from the IT regime may have been\nmore prevalent in the emerging market economies; there is not convincing\nevidence that IT is associated with either higher output growth or lower output\nvariability; the empirical research suggests that IT may have differential\neffects on exchange-rate volatility in advanced economies versus EMEs; although\nthe empirical evidence on the impact of IT on fiscal policy is quite limited,\nit supports the idea that IT indeed improves fiscal discipline; the empirical\nsupport to the proposition that IT is associated with lower disinflation costs\nseems to be rather weak. Therefore, the accumulated empirical literature\nimplies that IT does not produce superior macroeconomic benefits in comparison\nwith the alternative monetary strategies or, at most, they are quite modest.\n"
    },
    {
        "paper_id": 2305.17523,
        "authors": "Jaydip Sen, Aditya Jaiswal, Anshuman Pathak, Atish Kumar Majee,\n  Kushagra Kumar, Manas Kumar Sarkar, and Soubhik Maji",
        "title": "A Comparative Analysis of Portfolio Optimization Using Mean-Variance,\n  Hierarchical Risk Parity, and Reinforcement Learning Approaches on the Indian\n  Stock Market",
        "comments": "The report is 52 pages long. It is based on the capstone project done\n  in the post graduate course of data science in Praxis Business School,\n  Kolkata, India, of the Autumn Batch, 2022",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper presents a comparative analysis of the performances of three\nportfolio optimization approaches. Three approaches of portfolio optimization\nthat are considered in this work are the mean-variance portfolio (MVP),\nhierarchical risk parity (HRP) portfolio, and reinforcement learning-based\nportfolio. The portfolios are trained and tested over several stock data and\ntheir performances are compared on their annual returns, annual risks, and\nSharpe ratios. In the reinforcement learning-based portfolio design approach,\nthe deep Q learning technique has been utilized. Due to the large number of\npossible states, the construction of the Q-table is done using a deep neural\nnetwork. The historical prices of the 50 premier stocks from the Indian stock\nmarket, known as the NIFTY50 stocks, and several stocks from 10 important\nsectors of the Indian stock market are used to create the environment for\ntraining the agent.\n"
    },
    {
        "paper_id": 2305.17741,
        "authors": "David McCune and Adam Graham-Squire",
        "title": "Monotonicity Anomalies in Scottish Local Government Elections",
        "comments": "42 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Single Transferable Vote (STV) is a voting method used to elect multiple\ncandidates in ranked-choice elections. One weakness of STV is that it fails\nmultiple fairness criteria related to monotonicity and no show paradoxes. We\nanalyze 1,079 local government STV elections in Scotland to estimate the\nfrequency of such monotonicity anomalies in real-world elections, and compare\nour results with prior empirical and theoretical research about the rates at\nwhich such anomalies occur. In 62 of the 1079 elections we found some kind of\nmonotonicity anomaly. We generally find that the rates of anomalies are similar\nto prior empirical research and much lower than what most theoretical research\nhas found. The STV anomalies we find are the first of their kind to be\ndocumented in real-world multiwinner elections.\n"
    },
    {
        "paper_id": 2305.1783,
        "authors": "Yuanyuan Chang, Dena Firoozi, David Benatia",
        "title": "Large Banks and Systemic Risk: Insights from a Mean-Field Game Model",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper presents a dynamic game framework to analyze the role of large\nbanks in the interbank market. By extending existing models, we incorporate a\nmajor bank as a dynamic decision-maker interacting with multiple small banks.\nUsing the mean field game methodology and convex analysis, best-response\ntrading strategies are derived, leading to an approximate equilibrium for the\ninterbank market. We investigate the influence of the large bank on the market\nstability by examining individual default probabilities and systemic risk,\nthrough the use of Monte Carlo simulations. Our findings reveal that, when the\nsize of the major bank is not excessively large, it can positively contribute\nto market stability. However, there is also the potential for negative\nspillover effects in the event of default, leading to an increase in systemic\nrisk. The magnitude of this impact is further influenced by the size and\ntrading rate of the major bank. Overall, this study provides valuable insights\ninto the management of systemic risk in the interbank market.\n"
    },
    {
        "paper_id": 2305.17844,
        "authors": "Qasim Ajao, and Lanre Sadeeq",
        "title": "An Approximate Feasibility Assessment of Electric Vehicles Adoption in\n  Nigeria: Forecast 2030",
        "comments": "14 pages, 10 figures, Submitted to The Institute of Electrical and\n  Electronics Engineers (IEEE)",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Efforts toward building a sustainable future have underscored the importance\nof collective responsibility among state and non-state actors, corporations,\nand individuals to achieve climate goals. International initiatives, including\nthe Sustainable Development Goals and the Paris Agreement, emphasize the need\nfor immediate action from all stakeholders. This paper presents a feasibility\nassessment focused on the opportunities within Nigeria's Electric Vehicle Value\nChain, aiming to enhance public understanding of the country's renewable energy\nsector. As petroleum currently fulfills over 95% of global transportation\nneeds, energy companies must diversify their portfolios and integrate various\nrenewable energy sources to transition toward a sustainable future. The\nshifting investor sentiment away from traditional fossil fuel industries\nfurther highlights the imperative of incorporating renewables. To facilitate\nsignificant progress in the renewable energy sector, it is vital to establish\nplatforms that support the growth and diversification of industry players, with\nknowledge sharing playing a pivotal role. This feasibility assessment serves as\nan initial reference for individuals and businesses seeking technically and\neconomically viable opportunities within the sector.\n"
    },
    {
        "paper_id": 2305.17881,
        "authors": "Yi Huang, Wei Zhu, Duan Li, Shushang Zhu, Shikun Wang",
        "title": "Integrating Different Informations for Portfolio Selection",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Following the idea of Bayesian learning via Gaussian mixture model, we\norganically combine the backward-looking information contained in the\nhistorical data and the forward-looking information implied by the market\nportfolio, which is affected by heterogeneous expectations and noisy trading\nbehavior. The proposed combined estimation adaptively harmonizes these two\ntypes of information based on the degree of market efficiency and responds\nquickly at turning points of the market. Both simulation experiments and a\nglobal empirical test confirm that the approach is a flexible and robust\nforecasting tool and is applicable to various capital markets with different\ndegrees of efficiency.\n"
    },
    {
        "paper_id": 2305.18136,
        "authors": "Yan Dolinsky and Or Zuk",
        "title": "Exponential Utility Maximization in a Discrete Time Gaussian Framework",
        "comments": "2 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  The aim of this short note is to present a solution to the discrete time\nexponential utility maximization problem in a case where the underlying asset\nhas a multivariate normal distribution. In addition to the usual setting\nconsidered in Mathematical Finance, we also consider an investor who is\ninformed about the risky asset's price changes with a delay. Our method of\nsolution is based on the theory developed in [4] and guessing the optimal\nportfolio.\n"
    },
    {
        "paper_id": 2305.18941,
        "authors": "Louis Abraham",
        "title": "A Game of Competition for Risk",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  In this study, we present models where participants strategically select\ntheir risk levels and earn corresponding rewards, mirroring real-world\ncompetition across various sectors. Our analysis starts with a normal form game\ninvolving two players in a continuous action space, confirming the existence\nand uniqueness of a Nash equilibrium and providing an analytical solution. We\nthen extend this analysis to multi-player scenarios, introducing a new\nnumerical algorithm for its calculation. A key novelty of our work lies in\nusing regret minimization algorithms to solve continuous games through\ndiscretization. This groundbreaking approach enables us to incorporate\nadditional real-world factors like market frictions and risk correlations among\nfirms. We also experimentally validate that the Nash equilibrium in our model\nalso serves as a correlated equilibrium. Our findings illuminate how market\nfrictions and risk correlations affect strategic risk-taking. We also explore\nhow policy measures can impact risk-taking and its associated rewards, with our\nmodel providing broader applicability than the Diamond-Dybvig framework. We\nmake our methodology and open-source code available at\nhttps://github.com/louisabraham/cfrgame\n  Finally, we contribute methodologically by advocating the use of algorithms\nin economics, shifting focus from finite games to games with continuous action\nsets. Our study provides a solid framework for analyzing strategic interactions\nin continuous action games, emphasizing the importance of market frictions,\nrisk correlations, and policy measures in strategic risk-taking dynamics.\n"
    },
    {
        "paper_id": 2305.18949,
        "authors": "Andreas Bjerre-Nielsen, Lykke Sterll Christensen, Mikkel H{\\o}st\n  Gandil, Hans Henrik Sievertsen",
        "title": "Playing the system: address manipulation and access to schools",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
        "abstract": "  Strategic incentives may lead to inefficient and unequal provision of public\nservices. A prominent example is school admissions. Existing research shows\nthat applicants \"play the system\" by submitting school rankings strategically.\nWe investigate whether applicants also play the system by manipulating their\neligibility at schools. We analyze this applicant deception in a theoretical\nmodel and provide testable predictions for commonly-used admission procedures.\nWe confirm these model predictions empirically by analyzing the implementation\nof two reforms. First, we find that the introduction of a residence-based\nschool-admission criterion in Denmark caused address changes to increase by\nmore than 100% before the high-school application deadline. This increase\noccurred only in areas where the incentive to manipulate is high-powered.\nSecond, to assess whether this behavior reflects actual address changes, we\nstudy a second reform that required applicants to provide additional proof of\nplace of residence to approve an address change. The second reform\nsignificantly reduced address changes around the school application deadline,\nsuggesting that the observed increase in address changes mainly reflects\nmanipulation. The manipulation is driven by applicants from more affluent\nhouseholds and their behavior affects non-manipulating applicants.\nCounter-factual simulations show that among students not enrolling in their\nfirst listed school, more than 25% would have been offered a place in the\nabsence of address manipulation and their peer GPA is 0.2SD lower due to the\nmanipulative behavior of other applicants. Our findings show that popular\nschool choice systems give applicants the incentive to play the system with\nreal implications for non-strategic applicants.\n"
    },
    {
        "paper_id": 2305.18991,
        "authors": "Andrew J. Patton and Yasin Simsek",
        "title": "Generalized Autoregressive Score Trees and Forests",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We propose methods to improve the forecasts from generalized autoregressive\nscore (GAS) models (Creal et. al, 2013; Harvey, 2013) by localizing their\nparameters using decision trees and random forests. These methods avoid the\ncurse of dimensionality faced by kernel-based approaches, and allow one to draw\non information from multiple state variables simultaneously. We apply the new\nmodels to four distinct empirical analyses, and in all applications the\nproposed new methods significantly outperform the baseline GAS model. In our\napplications to stock return volatility and density prediction, the optimal GAS\ntree model reveals a leverage effect and a variance risk premium effect. Our\nstudy of stock-bond dependence finds evidence of a flight-to-quality effect in\nthe optimal GAS forest forecasts, while our analysis of high-frequency trade\ndurations uncovers a volume-volatility effect.\n"
    },
    {
        "paper_id": 2305.19499,
        "authors": "Shumin Ma, Zhiri Yuan, Qi Wu, Yiyan Huang, Xixu Hu, Cheuk Hang Leung,\n  Dongdong Wang, Zhixiang Huang",
        "title": "Deep into The Domain Shift: Transfer Learning through Dependence\n  Regularization",
        "comments": "15 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/publicdomain/zero/1.0/",
        "abstract": "  Classical Domain Adaptation methods acquire transferability by regularizing\nthe overall distributional discrepancies between features in the source domain\n(labeled) and features in the target domain (unlabeled). They often do not\ndifferentiate whether the domain differences come from the marginals or the\ndependence structures. In many business and financial applications, the\nlabeling function usually has different sensitivities to the changes in the\nmarginals versus changes in the dependence structures. Measuring the overall\ndistributional differences will not be discriminative enough in acquiring\ntransferability. Without the needed structural resolution, the learned transfer\nis less optimal. This paper proposes a new domain adaptation approach in which\none can measure the differences in the internal dependence structure separately\nfrom those in the marginals. By optimizing the relative weights among them, the\nnew regularization strategy greatly relaxes the rigidness of the existing\napproaches. It allows a learning machine to pay special attention to places\nwhere the differences matter the most. Experiments on three real-world datasets\nshow that the improvements are quite notable and robust compared to various\nbenchmark domain adaptation models.\n"
    },
    {
        "paper_id": 2305.19708,
        "authors": "Battulga Gankhuu",
        "title": "Parameter Estimation Methods of Required Rate of Return",
        "comments": "29 pages, 1 figures, and 2 tables. arXiv admin note: substantial text\n  overlap with arXiv:2206.09657",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  In this study, we introduce new estimation methods for the required rate of\nreturns on equity and liabilities of private and public companies using the\nstochastic dividend discount model (DDM). To estimate the required rate of\nreturn on equity, we use the maximum likelihood method, the Bayesian method,\nand the Kalman filtering. We also provide a method that evaluates the market\nvalues of liabilities. We apply the model to a set of firms from the S\\&P 500\nindex using historical dividend and price data over a 32--year period. Overall,\nthe suggested methods can be used to estimate the required rate of returns.\n"
    },
    {
        "paper_id": 2305.19865,
        "authors": "Deepesh Singh, Gopikrishnan Muraleedharan, Boxiang Fu, Chen-Mou Cheng,\n  Nicolas Roussy Newton, Peter P. Rohde, Gavin K. Brennen",
        "title": "Proof-of-work consensus by quantum sampling",
        "comments": "21 pages, 6 figures (v2 fixes typos, add references)",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Since its advent in 2011, boson-sampling has been a preferred candidate for\ndemonstrating quantum advantage because of its simplicity and near-term\nrequirements compared to other quantum algorithms. We propose to use a variant,\ncalled coarse-grained boson-sampling (CGBS), as a quantum Proof-of-Work (PoW)\nscheme for blockchain consensus. The users perform boson-sampling using input\nstates that depend on the current block information, and commit their samples\nto the network. Afterward, CGBS strategies are determined which can be used to\nboth validate samples and to reward successful miners. By combining rewards to\nminers committing honest samples together with penalties to miners committing\ndishonest samples, a Nash equilibrium is found that incentivizes honest nodes.\nThe scheme works for both Fock state boson sampling and Gaussian boson sampling\nand provides dramatic speedup and energy savings relative to computation by\nclassical hardware.\n"
    },
    {
        "paper_id": 2305.20067,
        "authors": "Fabrizio Cipollini, Giampiero M. Gallo, Alessandro Palandri",
        "title": "Modeling and evaluating conditional quantile dynamics in VaR forecasts",
        "comments": "37 pages, 5 figures, 8 tables",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  We focus on the time-varying modeling of VaR at a given coverage $\\tau$,\nassessing whether the quantiles of the distribution of the returns standardized\nby their conditional means and standard deviations exhibit predictable\ndynamics. Models are evaluated via simulation, determining the merits of the\nasymmetric Mean Absolute Deviation as a loss function to rank forecast\nperformances. The empirical application on the Fama-French 25 value-weighted\nportfolios with a moving forecast window shows substantial improvements in\nforecasting conditional quantiles by keeping the predicted quantile unchanged\nunless the empirical frequency of violations falls outside a data-driven\ninterval around $\\tau$.\n"
    },
    {
        "paper_id": 2305.20078,
        "authors": "Adam Graham-Squire and David McCune",
        "title": "Paradoxical Oddities in Two Multiwinner Elections from Scotland",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Ranked-choice voting anomalies such as monotonicity paradoxes have been\nextensively studied through creating hypothetical examples and generating\nelections under various models of voter behavior. However, very few real-world\nexamples of such voting paradoxes have been found and analyzed. We investigate\ntwo single-transferable vote elections from Scotland that demonstrate upward\nmonotonicity, downward monotonicity, no-show, and committee size paradoxes.\nThese paradoxes are rarely observed in real-world elections, and this article\nis the first case study of such paradoxes in multiwinner elections.\n"
    },
    {
        "paper_id": 2306.00002,
        "authors": "Richard S.J. Tol",
        "title": "The climate niche of Homo Sapiens",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  I propose the Dominicy-Hill-Worton estimator to estimate the current climate\nniche of Homo Sapiens and our croplands. I use this to extrapolate the degree\nof unprecedentedness of future climates. Worton's peeled hull is a\nnon-parametric, N-dimensional generalization of order statistics. Dominicy and\ncolleagues show that Hill's estimator of the tail-index can be applied to any\nhomogeneous function of multivariate order statistics. I apply the\nDominicy-Hill estimator to transects through Worton's peels. I find a thick\ntail for low temperatures and a thin tail for high ones. That is, warming is\nmore worrying than cooling. Similarly, wettening is more worrying than drying.\nFurthermore, temperature changes are more important than changes in\nprecipitation. The results are not affected by income, population density, or\ntime. I replace the Hill estimator by the QQ one and correct it for\ntop-censoring. The qualitative results are unaffected.\n"
    },
    {
        "paper_id": 2306.00093,
        "authors": "Vygintas Gontis",
        "title": "Discrete $q$-exponential limit order cancellation time distribution",
        "comments": "12 pages, 6 figures",
        "journal-ref": "Fractal and Fractional 7 (8), jul 2023, p. 581",
        "doi": "10.3390/fractalfract7080581",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Modeling financial markets based on empirical data poses challenges in\nselecting the most appropriate models. Despite the abundance of empirical data\navailable, researchers often face difficulties in identifying the best-fitting\nmodel. Long-range memory and self-similarity estimators, commonly used for this\npurpose, can yield inconsistent parameter values, as they are tailored to\nspecific time series models. In our previous work, we explored order disbalance\ntime series from the broader perspective of fractional L'{e}vy stable motion,\nrevealing a stable anti-correlation in the financial market order flow.\nHowever, a more detailed analysis of empirical data indicates the need for a\nmore specific order flow model that incorporates the power-law distribution of\nlimit order cancellation times. When considering a series in event time, the\nlimit order cancellation times follow a discrete probability mass function\nderived from the Tsallis q-exponential distribution. The combination of\npower-law distributions for limit order volumes and cancellation times\nintroduces a novel approach to modeling order disbalance in the financial\nmarkets. Moreover, this proposed model has the potential to serve as an example\nfor modeling opinion dynamics in social systems. By tailoring the model to\nincorporate the unique statistical properties of financial market data, we can\nimprove the accuracy of our predictions and gain deeper insights into the\ndynamics of these complex systems.\n"
    },
    {
        "paper_id": 2306.00132,
        "authors": "Paul Deroubaix, Takuro Kobashi, L\\'ena Gurriaran, Fouzi Benkhelifa,\n  Philippe Ciais, Katsumasa Tanaka",
        "title": "SolarEV City Concept for Paris: A promising idea?",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Urban decarbonization is one of the pillars for strategies to achieve carbon\nneutrality around the world. However, the current speed of urban\ndecarbonization is insufficient to keep pace with efforts to achieve this goal.\nRooftop PVs integrated with electric vehicles (EVs) as battery is a promising\ntechnology capable to supply CO2-free, affordable, and dispatchable electricity\nin urban environments (SolarEV City Concept). Here, we evaluated Paris, France\nfor the decarbonization potentials of rooftop PV + EV in comparison to the\nsurrounding suburban area Ile-de-France and Kyoto, Japan. We assessed various\nscenarios by calculating the energy sufficiency, self-consumption,\nself-sufficiency, cost savings, and CO2 emission reduction of the PV + EV\nsystem or PV only system. The combination of EVs with PVs by V2H or V2B systems\nat the city or region level was found to be more effective in Ile-de-France\nthan in Paris suggesting that SolarEV City is more effective for geographically\nlarger area including Paris. If implemented at a significant scale, they can\nadd substantial values to rooftop PV economics and keep a high self-consumption\nand self-sufficiency, which also allows bypassing the classical battery storage\nthat is too expensive to be profitable. Furthermore, the systems potentially\nallow rapid CO2 emissions reduction; however, with already low-carbon\nelectricity of France by nuclear power, CO2 abatement (0.020 kgCO2kWh-1\nreduction from 0.063 kgCO2kWh-1) by PV + EV system can be limited, in\ncomparison to that (0.270 kgCO2kWh-1 reduction from 0.352 kgCO2kWh-1) of Kyoto,\nalso because of the Paris low insolation and high demands in higher latitude\nwinter. While the SolarEV City Concept can help Paris to move one step closer\nto the carbon neutrality goal, there are also implementation challenges for\ninstalling PVs in Paris.\n"
    },
    {
        "paper_id": 2306.00439,
        "authors": "Evan Winter, Anupam Shah, Ujjwal Gupta, Anshul Kumar, Deepayan\n  Mohanty, Juan Carlos Uribe, Aishwary Gupta, Mini P. Thomas",
        "title": "Examination of Supernets to Facilitate International Trade for Indian\n  Exports to Brazil",
        "comments": "22 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The objective of this paper is to investigate a more efficient cross-border\npayment and document handling process for the export of Indian goods to Brazil.\nThe paper is structured into two sections: first, to explain the problems\nunique to the India-Brazil international trade corridor by highlighting the\nobstacles of compliance, speed, and payments; and second, to propose a digital\nsolution for India-brazil trade utilizing Supernets, focusing on the use case\nof Indian exports. The solution assumes that stakeholders will be onboarded as\npermissioned actors (i.e. nodes) on a Polygon Supernet. By engaging trade and\nbanking stakeholders, we ensure that the digital solution results in export\nbenefits for Indian exporters, and a lawful channel to receive hard currency\npayments. The involvement of Brazilian and Indian banks ensures that Letter of\nCredit (LC) processing time and document handling occur at the speed of\nblockchain technology. The ultimate goal is to achieve faster settlement and\nnegotiation period while maintaining a regulatory-compliant outcome, so that\nthe end result is faster and easier, yet otherwise identical to the real-world\nprocess in terms of export benefits and compliance.\n"
    },
    {
        "paper_id": 2306.00574,
        "authors": "Giacomo De Giorgi and Costanza Naguib",
        "title": "Life after (Soft) Default",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/publicdomain/zero/1.0/",
        "abstract": "  Soft default, defined as a delinquency of 90 days or more, is a relatively\ncommon event in the credit market, in 2010 such episodes affected about 3\nmillion individuals. Yet we lack a detailed understanding of what happens\nafterward. We use credit report data, on approximately 2 million individuals\nfrom 2004 to 2020, to shed light on individual trajectories after such event,\nand document enduring negative impacts. These effects persist for up to ten\nyears post-event and manifest in lower credit scores, reduced total credit\nlimits, lower homeownership rates, lower income, and relocation to less\neconomically active zip codes. It appears that those who are overextended in\ntheir mortgage lines, and with larger delinquent amounts, suffer the harshest\nconsequences.\n"
    },
    {
        "paper_id": 2306.00599,
        "authors": "Natascha Hey, Jean-Philippe Bouchaud, Iacopo Mastromatteo, Johannes\n  Muhle-Karbe, Kevin Webster",
        "title": "The Cost of Misspecifying Price Impact",
        "comments": "14 pages, 5 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Portfolio managers' orders trade off return and trading cost predictions.\nReturn predictions rely on alpha models, whereas price impact models quantify\ntrading costs. This paper studies what happens when trades are based on an\nincorrect price impact model, so that the portfolio either over- or\nunder-trades its alpha signal. We derive tractable formulas for these\nmisspecification costs and illustrate them on proprietary trading data. The\nmisspecification costs are naturally asymmetric: underestimating impact\nconcavity or impact decay shrinks profits, but overestimating concavity or\nimpact decay can even turn profits into losses.\n"
    },
    {
        "paper_id": 2306.00621,
        "authors": "Peter Bank and \\'Alvaro Cartea and Laura K\\\"orber",
        "title": "Optimal execution and speculation with trade signals",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  We propose a price impact model where changes in prices are purely driven by\nthe order flow in the market. The stochastic price impact of market orders and\nthe arrival rates of limit and market orders are functions of the market\nliquidity process which reflects the balance of the demand and supply of\nliquidity. Limit and market orders mutually excite each other so that liquidity\nis mean reverting. We use the theory of Meyer-$\\sigma$-fields to introduce a\nshort-term signal process from which a trader learns about imminent changes in\norder flow. In this setting, we examine an optimal execution problem and derive\nthe Hamilton--Jacobi--Bellman (HJB) equation for the value function. The HJB\nequation is solved numerically and we illustrate how the trader uses the signal\nto enhance the performance of execution problems and to execute speculative\nstrategies.\n"
    },
    {
        "paper_id": 2306.00718,
        "authors": "Jaros{\\l}aw W\\k{a}tr\\'obski, Aleksandra B\\k{a}czkiewicz, Iga Rudawska",
        "title": "A Strong Sustainability Paradigm Based Analytical Hierarchy Process\n  (SSP-AHP) Method to Evaluate Sustainable Healthcare Systems",
        "comments": "34 pages, 13 figures, 16 tables",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  The recent studies signify the growing concern of researchers towards\nmonitoring and measuring sustainability performance at various levels and in\nmany fields, including healthcare. However, there is no agreed approach to\nassessing the sustainability of health systems. Moreover, social indicators are\nless developed and less succinct. Therefore, the authors seek to map\nsustainable reference values in healthcare and propose a conceptual and\nstructured framework that can guide the measurement of the social\nsustainability-oriented health systems. Based on a new multi-criteria method\ncalled Strong Sustainability Paradigm based Analytical Hierarchy Process,\n(SSP-AHP), the presented approach opens the availability for systems'\ncomparison and benchmarking. The Strong Sustainability Paradigm incorporated\ninto the multi-criteria evaluation method prevents the exchangeability of\ncriteria by promoting alternatives that achieve good performance values on all\ncriteria, implying sustainability. The research results offer insights into the\ncore domains, sub-domains, and indicators supporting a more comprehensive\nassessment of the social sustainability of health systems. The framework\nconstructed in this study consists of five major areas: equity, quality,\nresponsiveness, financial coverage, and adaptability. The proposed set of\nindicators can also serve as a reference instrument, providing transparency\nabout core aspects of performance to be measured and reported, as well as\nsupporting policy-makers in decisions regarding sectoral strategies in\nhealthcare. Our findings suggest that the most socially sustainable systems are\nNordic countries. They offer a high level of social and financial protection,\nachieving very good health outcomes. On the other hand, the most unsustainable\nsystems located in central and eastern European countries.\n"
    },
    {
        "paper_id": 2306.00869,
        "authors": "Bingyou Chen, Yu Luo, Jieni Li, Yujian Li, Ying Liu, Fan Yang, Junge\n  Bo and Yanan Qiao",
        "title": "Blockchain-based Decentralized Co-governance: Innovations and Solutions\n  for Sustainable Crowdfunding",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This thesis provides an in-depth exploration of the Decentralized\nCo-governance Crowdfunding (DCC) Ecosystem, a novel solution addressing\nprevailing challenges in conventional crowdfunding methods faced by MSMEs and\ninnovative projects. Among the problems it seeks to mitigate are high\ntransaction costs, lack of transparency, fraud, and inefficient resource\nallocation. Leveraging a comprehensive review of the existing literature on\ncrowdfunding economic activities and blockchain's impact on organizational\ngovernance, we propose a transformative socio-economic model based on digital\ntokens and decentralized co-governance. This ecosystem is marked by a\ntripartite community structure - the Labor, Capital, and Governance communities\n- each contributing uniquely to the ecosystem's operation. Our research unfolds\nthe evolution of the DCC ecosystem through distinct phases, offering a novel\nunderstanding of socioeconomic dynamics in a decentralized digital world. It\nalso delves into the intricate governance mechanism of the ecosystem, ensuring\nintegrity, fairness, and a balanced distribution of value and wealth.\n"
    },
    {
        "paper_id": 2306.01284,
        "authors": "Max Sina Knicker, Karl Naumann-Woleske, Jean-Philippe Bouchaud,\n  Francesco Zamponi",
        "title": "Post-COVID Inflation & the Monetary Policy Dilemma: An Agent-Based\n  Scenario Analysis",
        "comments": "57 pages, 35 figures",
        "journal-ref": "Journal of Economic Interaction and Coordination (2024)",
        "doi": "10.1007/s11403-024-00413-3",
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  The economic shocks that followed the COVID-19 pandemic have brought to light\nthe difficulty, both for academics and policy makers, of describing and\npredicting the dynamics of inflation. This paper offers an alternative\nmodelling approach. We study the 2020-2023 period within the well-studied\nMark-0 Agent-Based Model, in which economic agents act and react according to\nplausible behavioural rules. We include a mechanism through which trust of\neconomic agents in the Central Bank can de-anchor. We investigate the influence\nof regulatory policies on inflationary dynamics resulting from three exogenous\nshocks, calibrated on those that followed the COVID-19 pandemic: a\nproduction/consumption shock due to COVID-related lockdowns, a supply-chain\nshock, and an energy price shock exacerbated by the Russian invasion of\nUkraine. By exploring the impact of these shocks under different assumptions\nabout monetary policy efficacy and transmission channels, we review various\nexplanations for the resurgence of inflation in the United States, including\ndemand-pull, cost-push, and profit-driven factors. Our main results are\nfour-fold: (i) without appropriate fiscal policy, the shocked economy can take\nyears to recover, or even tip over into a deep recession; {(ii) the success of\nmonetary policy in curbing inflation is primarily due to expectation anchoring,\nrather than to the direct economic impact of interest rate hikes; (iii)\nhowever, strong inflation anchoring is detrimental to consumption and\nunemployment, leading to a narrow window of ``optimal'' policy responses due to\nthe trade-off between inflation and unemployment;} (iv) the two most sensitive\nmodel parameters are those describing wage and price indexation. The results of\nour study have implications for Central Bank decision-making, and offers an\neasy-to-use tool that may help anticipate the consequences of different\nmonetary and fiscal policies.\n"
    },
    {
        "paper_id": 2306.01503,
        "authors": "Laurence Carassus, Johannes Wiesel",
        "title": "Strategies with minimal norm are optimal for expected utility\n  maximization under high model ambiguity",
        "comments": "We have substantially generalized our main result, Theorem 1.1. We\n  now consider general closed constraint sets $D$ and show that the optimal\n  strategies converge to the one with minimal norm. In the case $D=\\{w: <w,1>\n  \\ge a\\}$ for some $a>0$ we recover the re-weighted uniform strategy",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We investigate an expected utility maximization problem under model\nuncertainty in a one-period financial market. We capture model uncertainty by\nreplacing the baseline model $\\mathbb{P}$ with an adverse choice from a\nWasserstein ball of radius $k$ around $\\mathbb{P}$ in the space of probability\nmeasures and consider the corresponding Wasserstein distributionally robust\noptimization problem. We show that optimal solutions converge to a strategy\nwith minimal norm when uncertainty is increasingly large, i.e. when the radius\n$k$ tends to infinity.\n"
    },
    {
        "paper_id": 2306.01511,
        "authors": "Jozef Barunik, Lukas Vacha",
        "title": "The Dynamic Persistence of Economic Shocks",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper presents a model for smoothly varying heterogeneous persistence of\neconomic data. We argue that such dynamics arise naturally from the dynamic\nnature of economic shocks with various degree of persistence. The\nidentification of such dynamics from data is done using localised regressions.\nEmpirically, we identify rich persistence structures that change smoothly over\ntime in two important data sets: inflation, which plays a key role in policy\nformulation, and stock volatility, which is crucial for risk and market\nanalysis.\n"
    },
    {
        "paper_id": 2306.01552,
        "authors": "Emanuel Kohlscheen, Richhild Moessner, Daniel Rees",
        "title": "The shape of business cycles: a cross-country analysis of Friedman s\n  plucking theory",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  We test the international applicability of Friedman s famous plucking theory\nof the business cycle in 12 advanced economies between 1970 and 2021. We find\nthat in countries where labour markets are flexible (Australia, Canada, United\nKingdom and United States), unemployment rates typically return to\npre-recession levels, in line with Friedman s theory. Elsewhere, unemployment\nrates are less cyclical. Output recoveries differ less across countries, but\nmore across episodes: on average, half of the decline in GDP during a recession\npersists. In terms of sectors, declines in manufacturing are typically fully\nreversed. In contrast, construction-driven recessions, which are often\nassociated with bursting property price bubbles, tend to be persistent.\n"
    },
    {
        "paper_id": 2306.0166,
        "authors": "Julio Cezar Soares Silva, Adiel Teixeira de Almeida Filho",
        "title": "A systematic literature review on solution approaches for the index\n  tracking problem in the last decade",
        "comments": "This article has been accepted for publication in the IMA Journal of\n  Management Mathematics Published by Oxford University Press",
        "journal-ref": null,
        "doi": "10.1093/imaman/dpad007",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The passive management approach offers conservative investors a way to reduce\nrisk concerning the market. This investment strategy aims at replicating a\nspecific index, such as the NASDAQ Composite or the FTSE100 index. The problem\nis that buying all the index's assets incurs high rebalancing costs, and this\nharms future returns. The index tracking problem concerns building a portfolio\nthat follows a specific benchmark with fewer transaction costs. Since a subset\nof assets is required to solve the index problem this class of problems is\nNP-hard, and in the past years, researchers have been studying solution\napproaches to obtain tracking portfolios more practically. This work brings an\nanalysis, spanning the last decade, of the advances in mathematical approaches\nfor index tracking. The systematic literature review covered important issues,\nsuch as the most relevant research areas, solution methods, and model\nstructures. Special attention was given to the exploration and analysis of\nmetaheuristics applied to the index tracking problem.\n"
    },
    {
        "paper_id": 2306.0174,
        "authors": "Lawrence Clegg and John Cartlidge",
        "title": "Not feeling the buzz: Correction study of mispricing and inefficiency in\n  online sportsbooks",
        "comments": "24 pages, 2 figures. Revised argument; minor edits and funding\n  acknowledgement; typos corrected. This paper is a replication and correction\n  study and longer version of an accepted journal article. Replication code and\n  data are available online: https://github.com/Faxulous/notFeelingTheBuzz",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We present a replication and correction of a recent article (Ramirez, P.,\nReade, J.J., Singleton, C., Betting on a buzz: Mispricing and inefficiency in\nonline sportsbooks, International Journal of Forecasting, 39:3, 2023, pp.\n1413-1423, doi: 10.1016/j.ijforecast.2022.07.011). RRS measure profile page\nviews on Wikipedia to generate a \"buzz factor\" metric for tennis players and\nshow that it can be used to form a profitable gambling strategy by predicting\nbookmaker mispricing. Here, we use the same dataset as RRS to reproduce their\nresults exactly, thus confirming the robustness of their mispricing claim.\nHowever, we discover that the published betting results are significantly\naffected by a single bet (the \"Hercog\" bet), which returns substantial outlier\nprofits based on erroneously long odds. When this data quality issue is\nresolved, the majority of reported profits disappear and only one strategy,\nwhich bets on \"competitive\" matches, remains significantly profitable in the\noriginal out-of-sample period. While one profitable strategy offers weaker\nsupport than the original study, it still provides an indication that market\ninefficiencies may exist, as originally claimed by RRS. As an extension, we\ncontinue backtesting after 2020 on a cleaned dataset. Results show that (a) the\n\"competitive\" strategy generates no further profits, potentially suggesting\nmarkets have become more efficient, and (b) model coefficients estimated over\nthis more recent period are no longer reliable predictors of bookmaker\nmispricing. We present this work as a case study demonstrating the importance\nof replication studies in sports forecasting, and the necessity to clean data.\nWe open-source release comprehensive datasets and code.\n"
    },
    {
        "paper_id": 2306.01749,
        "authors": "Victor Medina-Olivares, Raffaella Calabrese",
        "title": "Detecting Consumers' Financial Vulnerability using Open Banking Data:\n  Evidence from UK Payday Loans",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Behind the debt trap concept is the rationale that payday loans exacerbate\nconsumers' financial vulnerability. To investigate this relationship, we\npropose a Mixed Poisson Hidden Markov approach to model the number of payday\nloans a borrower obtains in each period. Given the lack of agreement in the\nliterature on financial vulnerability, we introduce financial distress as an\nunobserved binary variable using a hidden Markov process (vulnerable and\nnon-vulnerable). Using data from 90,523 anonymised transactions for 1,817 UK\nconsumers, we find that the effect of certain time-varying covariates depends\ngreatly on the borrower's hidden state. For instance, luxury expenses and\nnon-recurring income increase the need for payday loans when financially\nvulnerable, but the opposite is true when not vulnerable. Additionally, we\ndemonstrate that almost 60\\% of payday loan borrowers remain vulnerable for 12\nor more consecutive weeks, with two-thirds experiencing consistent financial\ndifficulties. Finally, our analysis underscores the need for a nuanced approach\nto payday lending that recognises the varying levels of vulnerability among\nborrowers, which can prove helpful for policymakers and lenders to enhance\nresponsible lending practices.\n"
    },
    {
        "paper_id": 2306.01949,
        "authors": "Alexander M. Petersen and Felber Arroyave and Fabio Pammolli",
        "title": "The disruption index is biased by citation inflation",
        "comments": "10 pages, 6 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  A recent analysis of scientific publication and patent citation networks by\nPark et al. (Nature, 2023) suggests that publications and patents are becoming\nless disruptive over time. Here we show that the reported decrease in\ndisruptiveness is an artifact of systematic shifts in the structure of citation\nnetworks unrelated to innovation system capacity. Instead, the decline is\nattributable to 'citation inflation', an unavoidable characteristic of real\ncitation networks that manifests as a systematic time-dependent bias and\nrenders cross-temporal analysis challenging. One driver of citation inflation\nis the ever-increasing lengths of reference lists over time, which in turn\nincreases the density of links in citation networks, and causes the disruption\nindex to converge to 0. A second driver is attributable to shifts in the\nconstruction of reference lists, which is increasingly impacted by\nself-citations that increase in the rate of triadic closure in citation\nnetworks, and thus confounds efforts to measure disruption, which is itself a\nmeasure of triadic closure. Combined, these two systematic shifts render the\ndisruption index temporally biased, and unsuitable for cross-temporal analysis.\nThe impact of this systematic bias further stymies efforts to correlate\ndisruption to other measures that are also time-dependent, such as team size\nand citation counts. In order to demonstrate this fundamental measurement\nproblem, we present three complementary lines of critique (deductive, empirical\nand computational modeling), and also make available an ensemble of synthetic\ncitation networks that can be used to test alternative citation-based indices\nfor systematic bias.\n"
    },
    {
        "paper_id": 2306.02136,
        "authors": "Tingsong Jiang, Andy Zeng",
        "title": "Financial sentiment analysis using FinBERT with application in\n  predicting stock movement",
        "comments": "CS224U project",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/publicdomain/zero/1.0/",
        "abstract": "  We apply sentiment analysis in financial context using FinBERT, and build a\ndeep neural network model based on LSTM to predict the movement of financial\nmarket movement. We apply this model on stock news dataset, and compare its\neffectiveness to BERT, LSTM and classical ARIMA model. We find that sentiment\nis an effective factor in predicting market movement. We also propose several\nmethod to improve the model.\n"
    },
    {
        "paper_id": 2306.02148,
        "authors": "David Ardia, Keven Bluteau",
        "title": "The Role of Twitter in Cryptocurrency Pump-and-Dumps",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  We examine the influence of Twitter promotion on cryptocurrency pump-and-dump\nevents. By analyzing abnormal returns, trading volume, and tweet activity, we\nuncover that Twitter effectively garners attention for pump-and-dump schemes,\nleading to notable effects on abnormal returns before the event. Our results\nindicate that investors relying on Twitter information exhibit delayed selling\nbehavior during the post-dump phase, resulting in significant losses compared\nto other participants. These findings shed light on the pivotal role of Twitter\npromotion in cryptocurrency manipulation, offering valuable insights into\nparticipant behavior and market dynamics.\n"
    },
    {
        "paper_id": 2306.02328,
        "authors": "Zhicen Liu",
        "title": "Physical energy cost serves as the ''invisible hand'' governing economic\n  valuation: Direct evidence from biogeochemical data and the U.S. metal market",
        "comments": "5 pages,4 figures, 1 table",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Energy supply is mandatory for the production of economic value.\nNevertheless, tradition dictates that an enigmatic 'invisible hand' governs\neconomic valuation. Physical scientists have long proposed alternative but\ntestable energy cost theories of economic valuation, and have shown the gross\ncorrelation between energy consumption and economic output at the national\nlevel through input-output energy analysis. However, due to the difficulty of\nprecise energy analysis and highly complicated real markets, no decisive\nevidence directly linking energy costs to the selling prices of individual\ncommodities has yet been found. Over the past century, the US metal market has\naccumulated a huge body of price data, which for the first time ever provides\nus the opportunity to quantitatively examine the direct energy-value\ncorrelation. Here, by analyzing the market price data of 65 purified chemical\nelements (mainly metals) relative to the total energy consumption for refining\nthem from naturally occurring geochemical conditions, we found a clear\ncorrelation between the energy cost and their market prices. The underlying\nphysics we proposed has compatibility with conventional economic concepts such\nas the ratio between supply and demand or scarcity's role in economic\nvaluation. It demonstrates how energy cost serves as the 'invisible hand'\ngoverning economic valuation. Thorough understanding of this energy connection\nbetween the human economic and the Earth's biogeochemical metabolism is\nessential for improving the overall energy efficiency and furthermore the\nsustainability of the human society.\n"
    },
    {
        "paper_id": 2306.02708,
        "authors": "Ofelia Bonesini, Giorgia Callegaro, Martino Grasselli, Gilles Pag\\`es",
        "title": "From elephant to goldfish (and back): memory in stochastic Volterra\n  processes",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  We propose a new theoretical framework that exploits convolution kernels to\ntransform a Volterra path-dependent (non-Markovian) stochastic process into a\nstandard (Markovian) diffusion process. This transformation is achieved by\nembedding a Markovian \"memory process\" within the dynamics of the non-Markovian\nprocess. We discuss existence and path-wise regularity of solutions for the\nstochastic Volterra equations introduced and we provide a financial application\nto volatility modeling. We also propose a numerical scheme for simulating the\nprocesses. The numerical scheme exhibits a strong convergence rate of 1/2,\nwhich is independent of the roughness parameter of the volatility process. This\nis a significant improvement compared to Euler schemes used in similar models.\n  We propose a new theoretical framework that exploits convolution kernels to\ntransform a Volterra path-dependent (non-Markovian) stochastic process into a\nstandard (Markovian) diffusion process. This transformation is achieved by\nembedding a Markovian \"memory process\" (the goldfish) within the dynamics of\nthe non-Markovian process (the elephant). Most notably, it is also possible to\ngo back, i.e., the transformation is reversible. We discuss existence and\npath-wise regularity of solutions for the stochastic Volterra equations\nintroduced and we propose a numerical scheme for simulating the processes,\nwhich exhibits a remarkable convergence rate of $1/2$. In particular, in the\nfractional kernel case, the strong convergence rate is independent of the\nroughness parameter, which is a positive novelty in contrast with what happens\nin the available Euler schemes in the literature in rough volatility models.\n"
    },
    {
        "paper_id": 2306.02764,
        "authors": "Shiqi Gong, Shuaiqiang Liu, Danny D. Sun",
        "title": "Optimal Market Making in the Chinese Stock Market: A Stochastic Control\n  and Scenario Analysis",
        "comments": "35 pages, 7 figures, 6 tables",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Market making plays a crucial role in providing liquidity and maintaining\nstability in financial markets, making it an essential component of\nwell-functioning capital markets. Despite its importance, there is limited\nresearch on market making in the Chinese stock market, which is one of the\nlargest and most rapidly growing markets globally. To address this gap, we\nemploy an optimal market making framework with an exponential CARA-type\n(Constant Absolute Risk Aversion) utility function that accounts for various\nmarket conditions, such as price drift, volatility, and stamp duty, and is\ncapable of describing 3 major risks (i.e., inventory, execution and adverse\nselection risks) in market making practice, and provide an in-depth\nquantitative and scenario analysis of market making in the Chinese stock\nmarket. Our numerical experiments explore the impact of volatility on the\nmarket maker's inventory. Furthermore, we find that the stamp duty rate is a\ncritical factor in market making, with a negative impact on both the profit of\nthe market maker and the liquidity of the market. Additionally, our analysis\nemphasizes the significance of accurately estimating stock drift for managing\ninventory and adverse selection risks effectively and enhancing profit for the\nmarket maker. These findings offer valuable insights for both market makers and\npolicymakers in the Chinese stock market and provide directions for further\nresearch in designing effective market making strategies and policies.\n"
    },
    {
        "paper_id": 2306.02773,
        "authors": "Barry Quinn",
        "title": "Explaining AI in Finance: Past, Present, Prospects",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  This paper explores the journey of AI in finance, with a particular focus on\nthe crucial role and potential of Explainable AI (XAI). We trace AI's evolution\nfrom early statistical methods to sophisticated machine learning, highlighting\nXAI's role in popular financial applications. The paper underscores the\nsuperior interpretability of methods like Shapley values compared to\ntraditional linear regression in complex financial scenarios. It emphasizes the\nnecessity of further XAI research, given forthcoming EU regulations. The paper\ndemonstrates, through simulations, that XAI enhances trust in AI systems,\nfostering more responsible decision-making within finance.\n"
    },
    {
        "paper_id": 2306.02848,
        "authors": "Zikai Wei, Anyi Rao, Bo Dai, Dahua Lin",
        "title": "HireVAE: An Online and Adaptive Factor Model Based on Hierarchical and\n  Regime-Switch VAE",
        "comments": "Accepted to IJCAI 2023",
        "journal-ref": null,
        "doi": "10.24963/ijcai.2023/545",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Factor model is a fundamental investment tool in quantitative investment,\nwhich can be empowered by deep learning to become more flexible and efficient\nin practical complicated investing situations. However, it is still an open\nquestion to build a factor model that can conduct stock prediction in an online\nand adaptive setting, where the model can adapt itself to match the current\nmarket regime identified based on only point-in-time market information. To\ntackle this problem, we propose the first deep learning based online and\nadaptive factor model, HireVAE, at the core of which is a hierarchical latent\nspace that embeds the underlying relationship between the market situation and\nstock-wise latent factors, so that HireVAE can effectively estimate useful\nlatent factors given only historical market information and subsequently\npredict accurate stock returns. Across four commonly used real stock market\nbenchmarks, the proposed HireVAE demonstrate superior performance in terms of\nactive returns over previous methods, verifying the potential of such online\nand adaptive factor model.\n"
    },
    {
        "paper_id": 2306.02874,
        "authors": "Gregory M. Dickinson",
        "title": "Big Tech's Tightening Grip on Internet Speech",
        "comments": null,
        "journal-ref": "55 Ind. L. Rev. 101 (2022)",
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Online platforms have completely transformed American social life. They have\ndemocratized publication, overthrown old gatekeepers, and given ordinary\nAmericans a fresh voice in politics. But the system is beginning to falter.\nControl over online speech lies in the hands of a select few -- Facebook,\nGoogle, and Twitter -- who moderate content for the entire nation. It is an\nimpossible task. Americans cannot even agree among themselves what speech\nshould be permitted. And, more importantly, platforms have their own interests\nat stake: Fringe theories and ugly name-calling drive away users. Moderation is\ngood for business. But platform beautification has consequences for society's\nunpopular members, whose unsightly voices are silenced in the process. With\ncontrol over online speech so centralized, online outcasts are left with few\navenues for expression.\n  Concentrated private control over important resources is an old problem. Last\ncentury, for example, saw the rise of railroads and telephone networks. To\nensure access, such entities are treated as common carriers and required to\nprovide equal service to all comers. Perhaps the same should be true for social\nmedia. This Essay responds to recent calls from Congress, the Supreme Court,\nand academia arguing that, like common carriers, online platforms should be\nrequired to carry all lawful content. The Essay studies users' and platforms'\ncompeting expressive interests, analyzes problematic trends in platforms'\ncensorship practices, and explores the costs of common-carrier regulation\nbefore ultimately proposing market expansion and segmentation as an alternate\npathway to avoid the economic and social costs of common-carrier regulation.\n"
    },
    {
        "paper_id": 2306.02875,
        "authors": "Gregory M. Dickinson",
        "title": "Toward Textual Internet Immunity",
        "comments": null,
        "journal-ref": "33 Stan. L. & Pol'y Rev. Online 1 (2022)",
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Internet immunity doctrine is broken. Under Section 230 of the Communications\nDecency Act of 1996, online entities are absolutely immune from lawsuits\nrelated to content authored by third parties. The law has been essential to the\ninternet's development over the last twenty years, but it has not kept pace\nwith the times and is now deeply flawed. Democrats demand accountability for\nonline misinformation. Republicans decry politically motivated censorship. And\nCongress, President Biden, the Department of Justice, and the Federal\nCommunications Commission all have their own plans for reform. Absent from the\nfray, however -- until now -- has been the Supreme Court, which has never\nissued a decision interpreting Section 230. That appears poised to change,\nhowever, following Justice Thomas's statement in Malwarebytes v. Enigma in\nwhich he urges the Court to prune back decades of lower-court precedent to\ncraft a more limited immunity doctrine. This Essay discusses how courts'\nzealous enforcement of the early internet's free-information ethos gave birth\nto an expansive immunity doctrine, warns of potential pitfalls to reform, and\nexplores what a narrower, text-focused doctrine might mean for the tech\nindustry.\n"
    },
    {
        "paper_id": 2306.02876,
        "authors": "Gregory M. Dickinson",
        "title": "Rebooting Internet Immunity",
        "comments": null,
        "journal-ref": "89 Geo. Wash. L. Rev. 347 (2021)",
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  We do everything online. We shop, travel, invest, socialize, and even hold\ngarage sales. Even though we may not care whether a company operates online or\nin the physical world, however, the question has dramatic consequences for the\ncompanies themselves. Online and offline entities are governed by different\nrules. Under Section 230 of the Communications Decency Act, online entities --\nbut not physical-world entities -- are immune from lawsuits related to content\nauthored by their users or customers. As a result, online entities have been\nable to avoid claims for harms caused by their negligence and defective product\ndesigns simply because they operate online.\n  The reason for the disparate treatment is the internet's dramatic evolution\nover the last two decades. The internet of 1996 served as an information\nrepository and communications channel and was well governed by Section 230,\nwhich treats internet entities as another form of mass media: Because Facebook,\nTwitter and other online companies could not possibly review the mass of\ncontent that flows through their systems, Section 230 immunizes them from\nclaims related to user content. But content distribution is not the internet's\nonly function, and it is even less so now than it was in 1996. The internet\nalso operates as a platform for the delivery of real-world goods and services\nand requires a correspondingly diverse immunity doctrine. This Article proposes\nrefining online immunity by limiting it to claims that threaten to impose a\ncontent-moderation burden on internet defendants. Where a claim is preventable\nother than by content moderation -- for example, by redesigning an app or\nwebsite -- a plaintiff could freely seek relief, just as in the physical world.\nThis approach empowers courts to identify culpable actors in the virtual world\nand treat like conduct alike wherever it occurs.\n"
    },
    {
        "paper_id": 2306.02987,
        "authors": "Dirk Lauinger, Fran\\c{c}ois Vuille, Daniel Kuhn",
        "title": "Frequency Regulation with Storage: On Losses and Profits",
        "comments": null,
        "journal-ref": null,
        "doi": "10.1016/j.ejor.2024.03.022",
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  Low-carbon societies will need to store vast amounts of electricity to\nbalance intermittent generation from wind and solar energy, for example,\nthrough frequency regulation. Here, we derive an analytical solution to the\ndecision-making problem of storage operators who sell frequency regulation\npower to grid operators and trade electricity on day-ahead markets.\nMathematically, we treat future frequency deviation trajectories as functional\nuncertainties in a receding horizon robust optimization problem. We constrain\nthe expected terminal state-of-charge to be equal to some target to allow\nstorage operators to make good decisions not only for the present but also the\nfuture. Thanks to this constraint, the amount of electricity traded on\nday-ahead markets is an implicit function of the regulation power sold to grid\noperators. The implicit function quantifies the amount of power that needs to\nbe purchased to cover the expected energy loss that results from providing\nfrequency regulation. We show how the marginal cost associated with the\nexpected energy loss decreases with roundtrip efficiency and increases with\nfrequency deviation dispersion. We find that the profits from frequency\nregulation over the lifetime of energy-constrained storage devices are roughly\ninversely proportional to the length of time for which regulation power must be\ncommitted.\n"
    },
    {
        "paper_id": 2306.03237,
        "authors": "Ivan Arraut",
        "title": "Gauge symmetries and the Higgs mechanism in Quantum Finance",
        "comments": "6 pages",
        "journal-ref": "2023 EPL 143 42001",
        "doi": "10.1209/0295-5075/acedce",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  By using the Hamiltonian formulation, we demonstrate that the Merton-Garman\nequation emerges naturally from the Black-Scholes equation after imposing\ninvariance (symmetry) under local (gauge) transformations over changes in the\nstock price. This is the case because imposing gauge symmetry implies the\nappearance of an additional field, which corresponds to the stochastic\nvolatility. The gauge symmetry then imposes some constraints over the\nfree-parameters of the Merton-Garman Hamiltonian. Finally, we analyze how the\nstochastic volatility gets massive dynamically via Higgs mechanism.\n"
    },
    {
        "paper_id": 2306.03275,
        "authors": "Alexis Akira Toda",
        "title": "'Ergodicity Economics' is Pseudoscience",
        "comments": null,
        "journal-ref": null,
        "doi": "10.32388/ADBSXF",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In a series of papers, Ole Peters and his collaborators claim that the\n'conceptual basis of mainstream economic theory' is 'flawed' and that the\napproach they call 'ergodicity economics' gives 'reason to hope for a future\neconomic science that is more parsimonious, conceptually clearer and less\nsubjective' (Peters, 2019). This paper argues that 'ergodicity economics' is\npseudoscience because it has not produced falsifiable implications and should\nbe taken with skepticism.\n"
    },
    {
        "paper_id": 2306.03303,
        "authors": "Christa Cuchiero, Philipp Schmocker, Josef Teichmann",
        "title": "Global universal approximation of functional input maps on weighted\n  spaces",
        "comments": "60 pages, 4 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We introduce so-called functional input neural networks defined on a possibly\ninfinite dimensional weighted space with values also in a possibly infinite\ndimensional output space. To this end, we use an additive family to map the\ninput weighted space to the hidden layer, on which a non-linear scalar\nactivation function is applied to each neuron, and finally return the output\nvia some linear readouts. Relying on Stone-Weierstrass theorems on weighted\nspaces, we can prove a global universal approximation result on weighted spaces\nfor continuous functions going beyond the usual approximation on compact sets.\nThis then applies in particular to approximation of (non-anticipative) path\nspace functionals via functional input neural networks. As a further\napplication of the weighted Stone-Weierstrass theorem we prove a global\nuniversal approximation result for linear functions of the signature. We also\nintroduce the viewpoint of Gaussian process regression in this setting and\nemphasize that the reproducing kernel Hilbert space of the signature kernels\nare Cameron-Martin spaces of certain Gaussian processes. This paves a way\ntowards uncertainty quantification for signature kernel regression.\n"
    },
    {
        "paper_id": 2306.0362,
        "authors": "Reza Nematirad, Amin Ahmadisharaf, and Ali Lashgari",
        "title": "Forecasting the Performance of US Stock Market Indices During COVID-19:\n  RF vs LSTM",
        "comments": "Pennsylvania Economic Association (PEA)- June 2023",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The US stock market experienced instability following the recession\n(2007-2009). COVID-19 poses a significant challenge to US stock traders and\ninvestors. Traders and investors should keep up with the stock market. This is\nto mitigate risks and improve profits by using forecasting models that account\nfor the effects of the pandemic. With consideration of the COVID-19 pandemic\nafter the recession, two machine learning models, including Random Forest and\nLSTM are used to forecast two major US stock market indices. Data on historical\nprices after the big recession is used for developing machine learning models\nand forecasting index returns. To evaluate the model performance during\ntraining, cross-validation is used. Additionally, hyperparameter optimizing,\nregularization, such as dropouts and weight decays, and preprocessing improve\nthe performances of Machine Learning techniques. Using high-accuracy machine\nlearning techniques, traders and investors can forecast stock market behavior,\nstay ahead of their competition, and improve profitability. Keywords: COVID-19,\nLSTM, S&P500, Random Forest, Russell 2000, Forecasting, Machine Learning, Time\nSeries JEL Code: C6, C8, G4.\n"
    },
    {
        "paper_id": 2306.03763,
        "authors": "Zihan Chen, Lei Nico Zheng, Cheng Lu, Jialu Yuan, Di Zhu",
        "title": "ChatGPT Informed Graph Neural Network for Stock Movement Prediction",
        "comments": "Dataset is available at\n  [https://github.com/ZihanChen1995/ChatGPT-GNN-StockPredict]. Accepted for the\n  oral presentation at SIGKDD 2023 Workshop on Robust NLP for Finance",
        "journal-ref": null,
        "doi": "10.2139/ssrn.4464002",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  ChatGPT has demonstrated remarkable capabilities across various natural\nlanguage processing (NLP) tasks. However, its potential for inferring dynamic\nnetwork structures from temporal textual data, specifically financial news,\nremains an unexplored frontier. In this research, we introduce a novel\nframework that leverages ChatGPT's graph inference capabilities to enhance\nGraph Neural Networks (GNN). Our framework adeptly extracts evolving network\nstructures from textual data, and incorporates these networks into graph neural\nnetworks for subsequent predictive tasks. The experimental results from stock\nmovement forecasting indicate our model has consistently outperformed the\nstate-of-the-art Deep Learning-based benchmarks. Furthermore, the portfolios\nconstructed based on our model's outputs demonstrate higher annualized\ncumulative returns, alongside reduced volatility and maximum drawdown. This\nsuperior performance highlights the potential of ChatGPT for text-based network\ninferences and underscores its promising implications for the financial sector.\n"
    },
    {
        "paper_id": 2306.03822,
        "authors": "Vincent Lemaire, Gilles Pag\\`es, Christian Yeo",
        "title": "Swing contract pricing: with and without Neural Networks",
        "comments": "30 pages",
        "journal-ref": null,
        "doi": "10.3934/fmf.2024007",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We propose two parametric approaches to evaluate swing contracts with firm\nconstraints. Our objective is to define approximations for the optimal control,\nwhich represents the amounts of energy purchased throughout the contract. The\nfirst approach involves approximating the optimal control by means of an\nexplicit parametric function, where the parameters are determined using\nstochastic gradient descent based algorithms. The second approach builds on the\nfirst one, where we replace parameters in the first approach by the output of a\nneural network. Our numerical experiments demonstrate that by using Langevin\nbased algorithms, both parameterizations provide, in a short computation time,\nbetter prices compared to state-of-the-art methods.\n"
    },
    {
        "paper_id": 2306.04065,
        "authors": "Daniel Grainger",
        "title": "Sustainability criterion implied externality pricing for resource\n  extraction",
        "comments": "7 pages",
        "journal-ref": null,
        "doi": "10.1016/j.econlet.2023.111448",
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  A dynamic model is constructed that generalises the Hartwick and Van Long\n(2020) endogenous discounting setup by introducing externalities and asks what\nimplications this has for optimal natural resource extraction with constant\nconsumption. It is shown that a modified form of the Hotelling and Hartwick\nrule holds in which the externality component of price is a specific function\nof the instantaneous user costs and cross price elasticities. It is\ndemonstrated that the externality adjusted marginal user cost of remaining\nnatural reserves is equal to the marginal user cost of extracted resources\ninvested in human-made reproducible capital. This lends itself to a discrete\nform with a readily intuitive economic interpretation that illuminates the\nstepwise impact of externality pricing on optimal extraction schedules.\n"
    },
    {
        "paper_id": 2306.04158,
        "authors": "Svetlozar Rachev, Nancy Asare Nyarko, Blessing Omotade, and Peter\n  Yegon",
        "title": "Bachelier's Market Model for ESG Asset Pricing",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Environmental, Social, and Governance (ESG) finance is a cornerstone of\nmodern finance and investment, as it changes the classical return-risk view of\ninvestment by incorporating an additional dimension of investment performance:\nthe ESG score of the investment. We define the ESG price process and integrate\nit into an extension of Bachelier's market model in both discrete and\ncontinuous time, enabling option pricing valuation.\n"
    },
    {
        "paper_id": 2306.04285,
        "authors": "Jes\\'us Fern\\'andez-Villaverde and Isaiah Hull",
        "title": "Dynamic Programming on a Quantum Annealer: Solving the RBC Model",
        "comments": "46 pages, 11 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We introduce a novel approach to solving dynamic programming problems, such\nas those in many economic models, on a quantum annealer, a specialized device\nthat performs combinatorial optimization. Quantum annealers attempt to solve an\nNP-hard problem by starting in a quantum superposition of all states and\ngenerating candidate global solutions in milliseconds, irrespective of problem\nsize. Using existing quantum hardware, we achieve an order-of-magnitude\nspeed-up in solving the real business cycle model over benchmarks in the\nliterature. We also provide a detailed introduction to quantum annealing and\ndiscuss its potential use for more challenging economic problems.\n"
    },
    {
        "paper_id": 2306.04462,
        "authors": "Gregory M. Dickinson",
        "title": "An Empirical Study of Obstacle Preemption in the Supreme Court",
        "comments": null,
        "journal-ref": "89 Neb. L. Rev. 682 (2011)",
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  The Supreme Court's federal preemption decisions are notoriously\nunpredictable. Traditional left-right voting alignments break down in the face\nof competing ideological pulls. The breakdown of predictable voting blocs\nleaves the business interests most affected by federal preemption uncertain of\nthe scope of potential liability to injured third parties and unsure even of\nwhether state or federal law will be applied to future claims.\n  This empirical analysis of the Court's decisions over the last fifteen years\nsheds light on the Court's unique voting alignments in obstacle preemption\ncases. A surprising anti-obstacle preemption coalition is forming as Justice\nThomas gradually positions himself alongside the Court's liberals to form a\nfive-justice voting bloc opposing obstacle preemption.\n"
    },
    {
        "paper_id": 2306.04463,
        "authors": "Gregory M. Dickinson",
        "title": "Calibrating Chevron for Preemption",
        "comments": null,
        "journal-ref": "63 Admin. L. Rev. 667 (2011)",
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Now almost three decades since its seminal Chevron decision, the Supreme\nCourt has yet to articulate how that case's doctrine of deference to agency\nstatutory interpretations relates to one of the most compelling federalism\nissues of our time: regulatory preemption of state law. Should courts defer to\npreemptive agency interpretations under Chevron, or do preemption's federalism\nimplications demand a less deferential approach? Commentators have provided no\nshortage of possible solutions, but thus far the Court has resisted all of\nthem.\n  This Article makes two contributions to the debate. First, through a detailed\nanalysis of the Court's recent agency-preemption decisions, I trace its\nhesitancy to adopt any of the various proposed rules to its high regard for\ncongressional intent where areas of traditional state sovereignty are at risk.\nRecognizing that congressional intent to delegate preemptive authority varies\nfrom case to case, the Court has hesitated to adopt an across-the-board rule.\nAny such rule would constrain the Court and risk mismatch with congressional\nintent -- a risk it accepts under Chevron generally but which it finds\nparticularly troublesome in the delicate area of federal preemption.\n  Second, building on this previously underappreciated factor in the Court's\nanalysis, I suggest a novel solution of variable deference that avoids the\ninflexibility inherent in an across-the-board rule while providing greater\npredictability than the Court's current haphazard approach. The proposed rule\nwould grant full Chevron-style deference in those cases where congressional\ndelegative intent is most likely -- where Congress has expressly preempted some\nstate law and the agency interpretation merely resolves preemptive scope --\nwhile withholding deference in those cases where Congress has remained\ncompletely silent as to preemption and delegative intent is least likely.\n"
    },
    {
        "paper_id": 2306.04562,
        "authors": "Santiago Camara",
        "title": "International Spillovers of ECB Interest Rates: Monetary Policy &\n  Information Effects",
        "comments": "arXiv admin note: text overlap with arXiv:2211.10864",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/publicdomain/zero/1.0/",
        "abstract": "  This paper shows that disregarding the information effects around the\nEuropean Central Bank monetary policy decision announcements biases its\ninternational spillovers. Using data from 23 economies, both Emerging and\nAdvanced, I show that following an identification strategy that disentangles\npure monetary policy shocks from information effects lead to international\nspillovers on industrial production, exchange rates and equity indexes which\nare between 2 to 3 times larger in magnitude than those arising from following\nthe standard high frequency identification strategy. This bias is driven by\npure monetary policy and information effects having intuitively opposite\ninternational spillovers. Results are present for a battery of robustness\nchecks: for a sub-sample of ``close'' and ``further away'' countries, for both\nEmerging and Advanced economies, using local projection techniques and for\nalternative methods that control for ``information effects''. I argue that this\nbiases may have led a previous literature to disregard or find little\ninternational spillovers of ECB rates.\n"
    },
    {
        "paper_id": 2306.04569,
        "authors": "George Barnes, Sanjaye Ramgoolam, Michael Stephanou",
        "title": "Permutation invariant Gaussian matrix models for financial correlation\n  matrices",
        "comments": "49 pages + appendices, 8 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We construct an ensemble of correlation matrices from high-frequency foreign\nexchange market data, with one matrix for every day for 446 days. The matrices\nare symmetric and have vanishing diagonal elements after subtracting the\nidentity matrix. For this case, we construct the general permutation invariant\nGaussian matrix model, which has 4 parameters characterised using the\nrepresentation theory of symmetric groups. The permutation invariant polynomial\nfunctions of the symmetric, diagonally vanishing matrices have a basis labelled\nby undirected loop-less graphs. Using the expectation values of the general\nlinear and quadratic permutation invariant functions of the matrices in the\ndataset, the 4 parameters of the matrix model are determined. The model then\npredicts the expectation values of the cubic and quartic polynomials. These\npredictions are compared to the data to give strong evidence for a good overall\nfit of the permutation invariant Gaussian matrix model. The linear, quadratic,\ncubic and quartic polynomial functions are then used to define low-dimensional\nfeature vectors for the days associated to the matrices. These vectors, with\nchoices informed by the refined structure of small non-Gaussianities, are found\nto be effective as a tool for anomaly detection in market states: statistically\nsignificant correlations are established between atypical days as defined using\nthese feature vectors, and days with significant economic events as recognized\nin standard foreign exchange economic calendars. They are also shown to be\nuseful as a tool for ranking pairs of days in terms of their similarity,\nyielding a strongly statistically significant correlation with a ranking based\non a higher dimensional proxy for visual similarity.\n"
    },
    {
        "paper_id": 2306.04643,
        "authors": "Mingxiao Song and Yunsong Liu and Agam Shah and Sudheer Chava",
        "title": "Abnormal Trading Detection in the NFT Market",
        "comments": "The Undergraduate Consortium at KDD 2023 (KDD-UC)",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  The Non-Fungible-Token (NFT) market has experienced explosive growth in\nrecent years. According to DappRadar, the total transaction volume on OpenSea,\nthe largest NFT marketplace, reached 34.7 billion dollars in February 2023.\nHowever, the NFT market is mostly unregulated and there are significant\nconcerns about money laundering, fraud and wash trading. The lack of\nindustry-wide regulations, and the fact that amateur traders and retail\ninvestors comprise a significant fraction of the NFT market, make this market\nparticularly vulnerable to fraudulent activities. Therefore it is essential to\ninvestigate and highlight the relevant risks involved in NFT trading. In this\npaper, we attempted to uncover common fraudulent behaviors such as wash trading\nthat could mislead other traders. Using market data, we designed quantitative\nfeatures from the network, monetary, and temporal perspectives that were fed\ninto K-means clustering unsupervised learning algorithm to sort traders into\ngroups. Lastly, we discussed the clustering results' significance and how\nregulations can reduce undesired behaviors. Our work can potentially help\nregulators narrow down their search space for bad actors in the market as well\nas provide insights for amateur traders to protect themselves from unforeseen\nfrauds.\n"
    },
    {
        "paper_id": 2306.04771,
        "authors": "Gregory M. Dickinson",
        "title": "Chevron's Sliding Scale in Wyeth v. Levine, 129 S. Ct. 1187 (2009)",
        "comments": "arXiv admin note: substantial text overlap with arXiv:2306.04463",
        "journal-ref": "33 Harv. J.L. & Pub. Pol'y 1177 (2010)",
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  In Wyeth v. Levine the Supreme Court once again failed to reconcile the\ninterpretive presumption against preemption with the sometimes competing\nChevron doctrine of deference to agencies' reasonable statutory\ninterpretations. Rather than resolve the issue of which principle should govern\nwhere the two principles point toward opposite results, the Court continued its\nrecent practice of applying both principles halfheartedly, carving exceptions,\nand giving neither its proper weight.\n  This analysis situates Wyeth within the larger framework of the Court's\nrecent preemption decisions in an effort to explain the Court's hesitancy to\nresolve the conflict. The analysis concludes that the Court, motivated by its\nstrong respect for congressional intent and concern to protect federalism,\napplies both the presumption against preemption and the Chevron doctrine on a\nsliding scale. Where congressional intent to preempt is clear and vague only as\nto scope, the Court is usually quite deferential to agency determinations, but\nwhere congressional preemptive intent is unclear, agency views are accorded\nless weight.\n  The Court's variable approach to deference is defensible as necessary to\nprevent unauthorized incursion into areas of traditional state sovereignty, but\nits inherent unpredictability sows confusion among regulated parties, and the\nneed for flexibility prevents the Court from adopting any of the more\npredictable across-the-board approaches to deference proposed by the Court's\ncritics. A superior approach would combine the Court's concern for federalism\nwith the certainty of a bright-line rule by granting deference to agency views\nwhere Congress has spoken via a preemption clause of ambiguous scope and no\ndeference where Congress has remained silent.\n"
    },
    {
        "paper_id": 2306.04819,
        "authors": "Yang Hu",
        "title": "Perspectives in closed-loop supply chains network design considering\n  risk and uncertainty factors",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  Risk and uncertainty in each stage of CLSC have greatly increased the\ncomplexity and reduced process efficiency of the closed-loop networks, impeding\nthe sustainable and resilient development of industries and the circular\neconomy. Recently, increasing interest in academia have been raised on the risk\nand uncertainty analysis of closed-loop supply chain, yet there is no\ncomprehensive review paper focusing on closed-loop network design considering\nrisk and uncertainty. This paper examines previous research on the domain of\nclosed-loop network design under risk and uncertainties to provide constructive\nprospects for future study. We selected 106 papers published in the Scopus\ndatabase from the year 2004 to 2022. We analyse the source of risk and\nuncertainties of the CLSC network and identified appropriate methods for\nhandling uncertainties in addition to algorithms for solving uncertain CLSCND\nproblems. We also illustrate the evolution of objectives for designing a\nclosed-loop supply chain that is expos to risk or uncertainty, and investigate\nthe application of uncertain network design models in practical industry\nsectors. Finally, we draw proper research gaps for each category and clarify\nsome novel insights for future study. By considering the impacts of risk or\nuncertainties of different sources on closed-loop supply chain network design,\nwe can approach the economical, sustainable, social, and resilient objectives\neffectively and efficiently.\n"
    },
    {
        "paper_id": 2306.04946,
        "authors": "Syed Abul Basher, Salim Rashid, Mohammad Riad Uddin",
        "title": "Losing a Gold Mine?",
        "comments": "11 pages, 1 table",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Four rounds of surveys of slum dwellers in Dhaka city during the 2020-21\nCOVID-19 pandemic raise questions about whether the slum dwellers possess some\nform of immunity to the effects of COVID-19? If the working poor of Bangladesh\nare practically immune to COVID-19, why has this question not been more\nactively investigated? We shed light on some explanations for these pandemic\nquestions and draw attention to the role of intellectual elites and public\npolicy, suggesting modifications needed for pandemic research.\n"
    },
    {
        "paper_id": 2306.05113,
        "authors": "Andrea Bovo and Tiziano De Angelis and Jan Palczewski",
        "title": "Zero-sum stopper vs. singular-controller games with constrained control\n  directions",
        "comments": "29 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We consider a class of zero-sum stopper vs. singular-controller games in\nwhich the controller can only act on a subset $d_0<d$ of the $d$ coordinates of\na controlled diffusion. Due to the constraint on the control directions these\ngames fall outside the framework of recently studied variational methods. In\nthis paper we develop an approximation procedure, based on $L^1$-stability\nestimates for the controlled diffusion process and almost sure convergence of\nsuitable stopping times. That allows us to prove existence of the game's value\nand to obtain an optimal strategy for the stopper, under continuity and growth\nconditions on the payoff functions. This class of games is a natural extension\nof (single-agent) singular control problems, studied in the literature, with\nsimilar constraints on the admissible controls.\n"
    },
    {
        "paper_id": 2306.05433,
        "authors": "Eduardo Abi Jaber, Eyal Neuman and Moritz Vo{\\ss}",
        "title": "Equilibrium in Functional Stochastic Games with Mean-Field Interaction",
        "comments": "48 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  We consider a general class of finite-player stochastic games with mean-field\ninteraction, in which the linear-quadratic cost functional includes linear\noperators acting on controls in $L^2$. We propose a novel approach for deriving\nthe Nash equilibrium of the game semi-explicitly in terms of operator\nresolvents, by reducing the associated first order conditions to a system of\nstochastic Fredholm equations of the second kind and deriving their solution in\nsemi-explicit form. Furthermore, by proving stability results for the system of\nstochastic Fredholm equations, we derive the convergence of the equilibrium of\nthe $N$-player game to the corresponding mean-field equilibrium. As a\nby-product, we also derive an $\\varepsilon$-Nash equilibrium for the mean-field\ngame, which is valuable in this setting as we show that the conditions for\nexistence of an equilibrium in the mean-field limit are less restrictive than\nin the finite-player game. Finally, we apply our general framework to solve\nvarious examples, such as stochastic Volterra linear-quadratic games, models of\nsystemic risk and advertising with delay, and optimal liquidation games with\ntransient price impact.\n"
    },
    {
        "paper_id": 2306.05479,
        "authors": "Alvaro Arroyo, Alvaro Cartea, Fernando Moreno-Pino, Stefan Zohren",
        "title": "Deep Attentive Survival Analysis in Limit Order Books: Estimating Fill\n  Probabilities with Convolutional-Transformers",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  One of the key decisions in execution strategies is the choice between a\npassive (liquidity providing) or an aggressive (liquidity taking) order to\nexecute a trade in a limit order book (LOB). Essential to this choice is the\nfill probability of a passive limit order placed in the LOB. This paper\nproposes a deep learning method to estimate the filltimes of limit orders\nposted in different levels of the LOB. We develop a novel model for survival\nanalysis that maps time-varying features of the LOB to the distribution of\nfilltimes of limit orders. Our method is based on a convolutional-Transformer\nencoder and a monotonic neural network decoder. We use proper scoring rules to\ncompare our method with other approaches in survival analysis, and perform an\ninterpretability analysis to understand the informativeness of features used to\ncompute fill probabilities. Our method significantly outperforms those\ntypically used in survival analysis literature. Finally, we carry out a\nstatistical analysis of the fill probability of orders placed in the order book\n(e.g., within the bid-ask spread) for assets with different queue dynamics and\ntrading activity.\n"
    },
    {
        "paper_id": 2306.05568,
        "authors": "Philippe Goulet Coulombe, Maximilian Goebel",
        "title": "Maximally Machine-Learnable Portfolios",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  When it comes to stock returns, any form of predictability can bolster\nrisk-adjusted profitability. We develop a collaborative machine learning\nalgorithm that optimizes portfolio weights so that the resulting synthetic\nsecurity is maximally predictable. Precisely, we introduce MACE, a multivariate\nextension of Alternating Conditional Expectations that achieves the\naforementioned goal by wielding a Random Forest on one side of the equation,\nand a constrained Ridge Regression on the other. There are two key improvements\nwith respect to Lo and MacKinlay's original maximally predictable portfolio\napproach. First, it accommodates for any (nonlinear) forecasting algorithm and\npredictor set. Second, it handles large portfolios. We conduct exercises at the\ndaily and monthly frequency and report significant increases in predictability\nand profitability using very little conditioning information. Interestingly,\npredictability is found in bad as well as good times, and MACE successfully\nnavigates the debacle of 2022.\n"
    },
    {
        "paper_id": 2306.05667,
        "authors": "Andr\\'es Garc\\'ia-Medina and Benito Rodrigu\\'ez-Camejo",
        "title": "Random matrix theory and nested clustered portfolios on Mexican markets",
        "comments": "Submited to Revista Mexicana de F\\'isica; 11 pages; five figures, two\n  tables",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  This work aims to deal with the optimal allocation instability problem of\nMarkowitz's modern portfolio theory in high dimensionality. We propose a\ncombined strategy that considers covariance matrix estimators from Random\nMatrix Theory~(RMT) and the machine learning allocation methodology known as\nNested Clustered Optimization~(NCO). The latter methodology is modified and\nreformulated in terms of the spectral clustering algorithm and Minimum Spanning\nTree~(MST) to solve internal problems inherent to the original proposal.\nMarkowitz's classical mean-variance allocation and the modified NCO machine\nlearning approach are tested on financial instruments listed on the Mexican\nStock Exchange~(BMV) in a moving window analysis from 2018 to 2022. The\nmodified NCO algorithm achieves stable allocations by incorporating RMT\ncovariance estimators. In particular, the allocation weights are positive, and\ntheir absolute value adds up to the total capital without considering explicit\nrestrictions in the formulation. Our results suggest that can be avoided the\nrisky \\emph{short position} investment strategy by means of RMT inference and\nstatistical learning techniques.\n"
    },
    {
        "paper_id": 2306.0575,
        "authors": "Takuji Arai, Yuto Imai",
        "title": "Monte Carlo simulation for Barndorff-Nielsen and Shephard model under\n  change of measure",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The Barndorff-Nielsen and Shephard model is a representative jump-type\nstochastic volatility model. Still, no method exists to compute option prices\nnumerically for the non-martingale case with infinite active jumps. We develop\ntwo simulation methods for such a case under change of measure and conduct some\nnumerical experiments.\n"
    },
    {
        "paper_id": 2306.0577,
        "authors": "Tatsuki Inoue amd Erika Igarashi",
        "title": "The far-reaching effects of bombing on fertility in mid-20th century\n  Japan",
        "comments": "39 pages, 7 figures, 2 tables, Appendix",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  This study explores the indirect impact of war damage on postwar fertility,\nwith a specific focus on Japan's air raids during World War II. Using 1935 and\n1947 Kinki region town/village data and city air raid damage information, we\nexplored the \"far-reaching effects\" on fertility rates in nearby undamaged\nareas. Our fixed-effects model estimates show that air raids influenced postwar\nfertility within a 15-kilometer radius of the bombed cities. These impacts\nvaried based on bombing intensity, showing both positive and negative effects.\nMoreover, a deeper analysis, using the Allied flight path as a natural\nexperiment, indicates that air raid threats and associated fear led to\nincreased postwar fertility, even in undamaged areas. This study highlights the\nrelatively unexplored indirect consequences of war on fertility rates in\nneighboring regions and significantly contributes to the literature on the\nrelationship between wars and fertility.\n"
    },
    {
        "paper_id": 2306.05803,
        "authors": "Lubdhak Mondal, Udeshya Raj, Abinandhan S, Began Gowsik S, Sarwesh P\n  and Abhijeet Chandra",
        "title": "Causality between Sentiment and Cryptocurrency Prices",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  This study investigates the relationship between narratives conveyed through\nmicroblogging platforms, namely Twitter, and the value of crypto assets. Our\nstudy provides a unique technique to build narratives about cryptocurrency by\ncombining topic modelling of short texts with sentiment analysis. First, we\nused an unsupervised machine learning algorithm to discover the latent topics\nwithin the massive and noisy textual data from Twitter, and then we revealed\n4-5 cryptocurrency-related narratives, including financial investment,\ntechnological advancement related to crypto, financial and political\nregulations, crypto assets, and media coverage. In a number of situations, we\nnoticed a strong link between our narratives and crypto prices. Our work\nconnects the most recent innovation in economics, Narrative Economics, to a new\narea of study that combines topic modelling and sentiment analysis to relate\nconsumer behaviour to narratives.\n"
    },
    {
        "paper_id": 2306.0586,
        "authors": "Jessica Reale",
        "title": "Interbank Decisions and Margins of Stability: an Agent-Based Stock-Flow\n  Consistent Approach",
        "comments": "27 pages, 21 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  This study investigates the functioning of modern payment systems through the\nlens of banks' maturity mismatch practices, and it examines the effects of\nbanks' refusal to roll over short-term interbank liabilities on financial\nstability. Within an agent-based stock-flow consistent framework, banks can\nengage in two segments of the interbank market that differ in maturity,\novernight and term. We compare two interbank matching scenarios to assess how\nbank-specific maturity targets, dependent on the dictates of the Net Stable\nFunding Ratio, impact the dynamics of the interbank market and the\neffectiveness of conventional monetary policies. The findings reveal that\nmaturity misalignment between deficit and surplus banks compromises the\ninterbank market's efficiency and increases reliance on the central bank's\nstanding facilities. Monetary policy interest-rate steering practices also\nbecome less effective. The study also uncovers a dual stability-based\nconfiguration in the banking sector, resembling the segmented European\ninterbank structure. This paper suggests that heterogeneous maturity mismatches\nbetween surplus and deficit banks may result in asymmetric funding frictions\nthat might precede credit- and sovereign-risk explanations of interbank\ntensions. Also, a combined examination of macroprudential tools and\nrollover-based interbank dynamics can enhance our understanding of how\nregulatory changes impact the stability of heterogeneous banking sectors.\n"
    },
    {
        "paper_id": 2306.05867,
        "authors": "Mohammad Heydari, Matineh Moghaddam and Habibollah Danai",
        "title": "The Relationship Between Burnout Operators with the Functions of Family\n  Tehran Banking Melli Iran Bank in 2015",
        "comments": null,
        "journal-ref": "The Social Sciences, (2016), 11: 1168-1173",
        "doi": "10.36478/sscience.2016.1168.1173",
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  In this study, the relationship between burnout and family functions of the\nMelli Iran Bank staff will be studied. A number of employees within the\norganization using appropriate scientific methods as the samples were selected\nby detailed questionnaire and the appropriate data is collected burnout and\nfamily functions. The method used descriptive statistical population used for\nthis study consisted of 314 bank loan officers in branches of Melli Iran Bank\nof Tehran province and all the officials at the bank for >5 years of service at\nMelli Iran Bank branches in Tehran. They are married and men constitute the\nstudy population. The Maslach Burnout Inventory in the end internal to 0/90\nalpha emotional exhaustion, depersonalization and low personal accomplishment\nCronbach alpha of 0/79 and inventory by 0/71 within the last family to solve\nthe problem 0/70, emotional response 0/51, touch 0/70, 0/69 affective\ninvolvement, roles, 0/59, 0/68 behavior is controlled. The results indicate\nthat the hypothesis that included the relationship between burnout and 6, the\nfamily functioning, problem solving, communication, roles, affective\nresponsiveness, affective fusion there was a significant relationship between\nbehavior and the correlation was negative. The burnout is high; the functions\nwithin the family will be in trouble.\n"
    },
    {
        "paper_id": 2306.05966,
        "authors": "Mallory Dickerson, Erin Martin, David McCune",
        "title": "An Empirical Analysis of the Effect of Ballot Truncation on\n  Ranked-Choice Electoral Outcomes",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In ranked-choice elections voters cast preference ballots which provide a\nvoter's ranking of the candidates. The method of ranked-choice voting (RCV)\nchooses a winner by using voter preferences to simulate a series of runoff\nelections. Some jurisdictions which use RCV limit the number of candidates that\nvoters can rank on the ballot, imposing what we term a truncation level, which\nis the number of candidates that voters are allowed to rank. Given fixed voter\npreferences, the winner of the election can change if we impose different\ntruncation levels. We use a database of 1171 real-world ranked-choice elections\nto empirically analyze the potential effects of imposing different truncation\nlevels in ranked-choice elections. Our general finding is that if the\ntruncation level is at least three then restricting the number of candidates\nwhich can be ranked on the ballot rarely affects the election winner.\n"
    },
    {
        "paper_id": 2306.05987,
        "authors": "Ruihua Ruan, Emmanuel Bacry, Jean-Fran\\c{c}ois Muzy",
        "title": "Liquidity takers behavior representation through a contrastive learning\n  approach",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Thanks to the access to the labeled orders on the CAC40 data from Euronext,\nwe are able to analyze agents' behaviors in the market based on their placed\norders. In this study, we construct a self-supervised learning model using\ntriplet loss to effectively learn the representation of agent market orders. By\nacquiring this learned representation, various downstream tasks become\nfeasible. In this work, we utilize the K-means clustering algorithm on the\nlearned representation vectors of agent orders to identify distinct behavior\ntypes within each cluster.\n"
    },
    {
        "paper_id": 2306.06031,
        "authors": "Hongyang Yang, Xiao-Yang Liu, Christina Dan Wang",
        "title": "FinGPT: Open-Source Financial Large Language Models",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Large language models (LLMs) have shown the potential of revolutionizing\nnatural language processing tasks in diverse domains, sparking great interest\nin finance. Accessing high-quality financial data is the first challenge for\nfinancial LLMs (FinLLMs). While proprietary models like BloombergGPT have taken\nadvantage of their unique data accumulation, such privileged access calls for\nan open-source alternative to democratize Internet-scale financial data.\n  In this paper, we present an open-source large language model, FinGPT, for\nthe finance sector. Unlike proprietary models, FinGPT takes a data-centric\napproach, providing researchers and practitioners with accessible and\ntransparent resources to develop their FinLLMs. We highlight the importance of\nan automatic data curation pipeline and the lightweight low-rank adaptation\ntechnique in building FinGPT. Furthermore, we showcase several potential\napplications as stepping stones for users, such as robo-advising, algorithmic\ntrading, and low-code development. Through collaborative efforts within the\nopen-source AI4Finance community, FinGPT aims to stimulate innovation,\ndemocratize FinLLMs, and unlock new opportunities in open finance. Two\nassociated code repos are \\url{https://github.com/AI4Finance-Foundation/FinGPT}\nand \\url{https://github.com/AI4Finance-Foundation/FinNLP}\n"
    },
    {
        "paper_id": 2306.06087,
        "authors": "David Byrd",
        "title": "Learning Not to Spoof",
        "comments": null,
        "journal-ref": null,
        "doi": "10.1145/3533271.3561767",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  As intelligent trading agents based on reinforcement learning (RL) gain\nprevalence, it becomes more important to ensure that RL agents obey laws,\nregulations, and human behavioral expectations. There is substantial literature\nconcerning the aversion of obvious catastrophes like crashing a helicopter or\nbankrupting a trading account, but little around the avoidance of subtle\nnon-normative behavior for which there are examples, but no programmable\ndefinition. Such behavior may violate legal or regulatory, rather than physical\nor monetary, constraints.\n  In this article, I consider a series of experiments in which an intelligent\nstock trading agent maximizes profit but may also inadvertently learn to spoof\nthe market in which it participates. I first inject a hand-coded spoofing agent\nto a multi-agent market simulation and learn to recognize spoofing activity\nsequences. Then I replace the hand-coded spoofing trader with a simple\nprofit-maximizing RL agent and observe that it independently discovers spoofing\nas the optimal strategy. Finally, I introduce a method to incorporate the\nrecognizer as normative guide, shaping the agent's perceived rewards and\naltering its selected actions. The agent remains profitable while avoiding\nspoofing behaviors that would result in even higher profit. After presenting\nthe empirical results, I conclude with some recommendations. The method should\ngeneralize to the reduction of any unwanted behavior for which a recognizer can\nbe learned.\n"
    },
    {
        "paper_id": 2306.0615,
        "authors": "Mohammad Heydari, Matineh Moghaddam, Khadijeh Gholami and Habibollah\n  Danai",
        "title": "Investigation User Reviews FRO to Determine the Level of Customer\n  Loyalty Model Shahrvand Chain Stores",
        "comments": null,
        "journal-ref": "International Business Management (2016),10: 1914-1920",
        "doi": "10.36478/ibm.2016.1914.1920",
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  In this study, focusing on organizations in a rapid-response component model\n(FRO), the relative importance of each one, from the point of view of customers\nand their impact on the purchase of Shahrvand chain stores determined to\ndirectors and managers of the shops, according to customer needs and their\npriorities in order to satisfy the customers and take steps to strengthen their\ncompetitiveness. For this purpose, all shahrvand chain stores in Tehran\ncurrently have 10 stores in different parts of Tehran that have been studied\nare that of the 10 branches; Five branches were selected. The sampling method\nis used in this study population with a confidence level of 95% and 8% error;\n150 are more specifically typically 30 were studied in each branch. In this\nstudy, a standard questionnaire of 26 questions which is used FRO validity\nusing Cronbach's alpha values of \"0/95\" is obtained. The results showed that\neach of the six factors on customer loyalty model FRO effective Shahrvand chain\nstores. The effect of each of the six Foundation FRO customer loyalty model\nshahrvand is different chain stores.\n"
    },
    {
        "paper_id": 2306.06483,
        "authors": "Eiji Yamamura, Yoshiro Tsutsui, Fumio Ohtake",
        "title": "Surname Order and Revaccination Intentions: The Effect of Mixed-Gender\n  Lists on Gender Differences during the COVID-19 Pandemic",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This study probes the effects of Japan's traditional alphabetical\nsurname-based call system on students' experiences and long-term behavior. It\nreveals that early listed surnames enhance cognitive and non-cognitive skill\ndevelopment. The adoption of mixed-gender lists since the 1980s has amplified\nthis effect, particularly for females. Furthermore, the study uncovers a strong\ncorrelation between childhood surname order and individuals' intention for\nCOVID-19 revaccination, while changes in adulthood surnames do not exhibit the\nsame influence. The implications for societal behaviors and policy are\nsubstantial and wide-ranging.\n"
    },
    {
        "paper_id": 2306.0668,
        "authors": "Rinki Ito",
        "title": "Centrality in Production Networks and International Technology Diffusion",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  This study examines whether the structure of global value chains (GVCs)\naffects international spillovers of research and development (R&D). Although\nthe presence of ``hub'' countries in GVCs has been confirmed by previous\nstudies, the role of these hub countries in the diffusion of the technology has\nnot been analyzed. Using a sample of 21 countries and 14 manufacturing\nindustries during the period 1995-2007, I explore the role of hubs as the\nmediator of knowledge by classifying countries and industries based on a\n``centrality'' measure. I find that R&D spillovers from exporters with High\ncentrality are the largest, suggesting that hub countries play an important\nrole in both gathering and diffusing knowledge. I also find that countries with\nMiddle centrality are getting important in the diffusion of knowledge. Finally,\npositive spillover effects from own are observed only in the G5 countries.\n"
    },
    {
        "paper_id": 2306.07013,
        "authors": "Zhenglong Li, Hejun Huang, Vincent Tam",
        "title": "Combining Reinforcement Learning and Barrier Functions for Adaptive Risk\n  Management in Portfolio Optimization",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
        "abstract": "  Reinforcement learning (RL) based investment strategies have been widely\nadopted in portfolio management (PM) in recent years. Nevertheless, most\nRL-based approaches may often emphasize on pursuing returns while ignoring the\nrisks of the underlying trading strategies that may potentially lead to great\nlosses especially under high market volatility. Therefore, a risk-manageable PM\ninvestment framework integrating both RL and barrier functions (BF) is proposed\nto carefully balance the needs for high returns and acceptable risk exposure in\nPM applications. Up to our understanding, this work represents the first\nattempt to combine BF and RL for financial applications. While the involved RL\napproach may aggressively search for more profitable trading strategies, the\nBF-based risk controller will continuously monitor the market states to\ndynamically adjust the investment portfolio as a controllable measure for\navoiding potential losses particularly in downtrend markets. Additionally, two\nadaptive mechanisms are provided to dynamically adjust the impact of risk\ncontrollers such that the proposed framework can be flexibly adapted to uptrend\nand downtrend markets. The empirical results of our proposed framework clearly\nreveal such advantages against most well-known RL-based approaches on\nreal-world data sets. More importantly, our proposed framework shed lights on\nmany possible directions for future investigation.\n"
    },
    {
        "paper_id": 2306.07134,
        "authors": "Labrini Zarpala",
        "title": "Auctioning Corporate Bonds: A Uniform-Price under Investment Mandates",
        "comments": "25 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper examines how risk and budget limits on investment mandates affect\nthe bidding strategy in a uniform-price auction for issuing corporate bonds. I\nprove the existence of symmetric Bayesian Nash equilibrium and explore how the\nrisk limits imposed on the mandate may mitigate severe underpricing, as the\nsymmetric equilibrium's yield positively relates to the risk limit. Investment\nmandates with low-risk acceptance inversely affect the equilibrium bid. The\nequilibrium bid provides insights into the optimal mechanism for pricing\ncorporate bonds conveying information about the bond's valuation, market power,\nand the number of bidders. These findings contribute to auction theory and have\nimplications for empirical research in the corporate bond market.\n"
    },
    {
        "paper_id": 2306.07147,
        "authors": "Marcus Ogren",
        "title": "Candidate Incentive Distributions: How voting methods shape electoral\n  incentives",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We evaluate the tendency for different voting methods to promote political\ncompromise and reduce tensions in a society by using computer simulations to\ndetermine which voters candidates are incentivized to appeal to. We find that\nInstant Runoff Voting incentivizes candidates to appeal to a wider range of\nvoters than Plurality Voting, but that it leaves candidates far more strongly\nincentivized to appeal to their base than to voters in opposing factions. In\ncontrast, we find that Condorcet methods and STAR (Score Then Automatic Runoff)\nVoting provide the most balanced incentives; these differences between voting\nmethods become more pronounced with more candidates are in the race and less\npronounced in the presence of strategic voting. We find that the incentives\nprovided by Single Transferable Vote to appeal to opposing voters are\nnegligible, but that a tweak to the tabulation algorithm makes them\nsubstantial.\n"
    },
    {
        "paper_id": 2306.07305,
        "authors": "Shaun D'Souza, Dheeraj Shah, Amareshwar Allati, Parikshit Soni",
        "title": "Making forecasting self-learning and adaptive -- Pilot forecasting rack",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  Retail sales and price projections are typically based on time series\nforecasting. For some product categories, the accuracy of demand forecasts\nachieved is low, negatively impacting inventory, transport, and replenishment\nplanning. This paper presents our findings based on a proactive pilot exercise\nto explore ways to help retailers to improve forecast accuracy for such product\ncategories.\n  We evaluated opportunities for algorithmic interventions to improve forecast\naccuracy based on a sample product category, Knitwear. The Knitwear product\ncategory has a current demand forecast accuracy from non-AI models in the range\nof 60%. We explored how to improve the forecast accuracy using a rack approach.\nTo generate forecasts, our decision model dynamically selects the best\nalgorithm from an algorithm rack based on performance for a given state and\ncontext. Outcomes from our AI/ML forecasting model built using advanced feature\nengineering show an increase in the accuracy of demand forecast for Knitwear\nproduct category by 20%, taking the overall accuracy to 80%. Because our rack\ncomprises algorithms that cater to a range of customer data sets, the\nforecasting model can be easily tailored for specific customer contexts.\n"
    },
    {
        "paper_id": 2306.07731,
        "authors": "Christian Laudag\\'e, Florian Aichinger, Sascha Desmettre",
        "title": "A Comparative Study of Factor Models for Different Periods of the\n  Electricity Spot Price Market",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Due to major shifts in European energy supply, a structural change can be\nobserved in Austrian electricity spot price data starting from the second\nquarter of the year 2021 onward. In this work we study the performance of two\ndifferent factor models for the electricity spot price in three different time\nperiods. To this end, we consider three samples of EEX data for the Austrian\nbase load electricity spot price, one from the pre-crises from 2018 to 2021,\nthe second from the time of the crisis from 2021 to 2023 and the whole data\nfrom 2018 to 2023. For each of these samples, we investigate the fit of a\nclassical 3-factor model with a Gaussian base signal and one positive and one\nnegative jump signal and compare it with a 4-factor model to assess the effect\nof adding a second Gaussian base signal to the model. For the calibration of\nthe models we develop a tailor-made Markov Chain Monte Carlo method based on\nGibbs sampling. To evaluate the model adequacy, we provide simulations of the\nspot price as well as a posterior predictive check for the 3- and the 4-factor\nmodel. We find that the 4-factor model outperforms the 3-factor model in times\nof non-crises. In times of crises, the second Gaussian base signal does not\nlead to a better fit of the model. To the best of our knowledge, this is the\nfirst study regarding stochastic electricity spot price models in this new\nmarket environment. Hence, it serves as a solid base for future research.\n"
    },
    {
        "paper_id": 2306.07928,
        "authors": "Shuo Han, Yinan Chen, Jiacheng Liu",
        "title": "Optimizing Investment Strategies with Lazy Factor and Probability\n  Weighting: A Price Portfolio Forecasting and Mean-Variance Model with\n  Transaction Costs Approach",
        "comments": "Accepted by IEEE SMC 2023",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Market traders often engage in the frequent transaction of volatile assets to\noptimize their total return. In this study, we introduce a novel investment\nstrategy model, anchored on the 'lazy factor.' Our approach bifurcates into a\nPrice Portfolio Forecasting Model and a Mean-Variance Model with Transaction\nCosts, utilizing probability weights as the coefficients of laziness factors.\nThe Price Portfolio Forecasting Model, leveraging the EXPMA Mean Method, plots\nthe long-term price trend line and forecasts future price movements,\nincorporating the tangent slope and rate of change. For short-term investments,\nwe apply the ARIMA Model to predict ensuing prices. The Mean-Variance Model\nwith Transaction Costs employs the Monte Carlo Method to formulate the feasible\nregion. To strike an optimal balance between risk and return, equal probability\nweights are incorporated as coefficients of the laziness factor. To assess the\nefficacy of this combined strategy, we executed extensive experiments on a\nspecified dataset. Our findings underscore the model's adaptability and\ngeneralizability, indicating its potential to transform investment strategies.\n"
    },
    {
        "paper_id": 2306.07972,
        "authors": "Georgios Palaiokrassas and Sandro Scherrers and Iason Ofeidis and\n  Leandros Tassiulas",
        "title": "Leveraging Machine Learning for Multichain DeFi Fraud Detection",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Since the inception of permissionless blockchains with Bitcoin in 2008, it\nbecame apparent that their most well-suited use case is related to making the\nfinancial system and its advantages available to everyone seamlessly without\ndepending on any trusted intermediaries. Smart contracts across chains provide\nan ecosystem of decentralized finance (DeFi), where users can interact with\nlending pools, Automated Market Maker (AMM) exchanges, stablecoins,\nderivatives, etc. with a cumulative locked value which had exceeded 160B USD.\nWhile DeFi comes with high rewards, it also carries plenty of risks. Many\nfinancial crimes have occurred over the years making the early detection of\nmalicious activity an issue of high priority. The proposed framework introduces\nan effective method for extracting a set of features from different chains,\nincluding the largest one, Ethereum and it is evaluated over an extensive\ndataset we gathered with the transactions of the most widely used DeFi\nprotocols (23 in total, including Aave, Compound, Curve, Lido, and Yearn) based\non a novel dataset in collaboration with Covalent. Different Machine Learning\nmethods were employed, such as XGBoost and a Neural Network for identifying\nfraud accounts detection interacting with DeFi and we demonstrate that the\nintroduction of novel DeFi-related features, significantly improves the\nevaluation results, where Accuracy, Precision, Recall, F1-score and F2-score\nwhere utilized.\n"
    },
    {
        "paper_id": 2306.08105,
        "authors": "Vadim Zlotnikov, Jiayu Liu, Igor Halperin, Fei He, Lisa Huang",
        "title": "Model-Free Market Risk Hedging Using Crowding Networks",
        "comments": "8 pages, 6 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Crowding is widely regarded as one of the most important risk factors in\ndesigning portfolio strategies. In this paper, we analyze stock crowding using\nnetwork analysis of fund holdings, which is used to compute crowding scores for\nstocks. These scores are used to construct costless long-short portfolios,\ncomputed in a distribution-free (model-free) way and without using any\nnumerical optimization, with desirable properties of hedge portfolios. More\nspecifically, these long-short portfolios provide protection for both small and\nlarge market price fluctuations, due to their negative correlation with the\nmarket and positive convexity as a function of market returns. By adding our\nlong-short portfolio to a baseline portfolio such as a traditional 60/40\nportfolio, our method provides an alternative way to hedge portfolio risk\nincluding tail risk, which does not require costly option-based strategies or\ncomplex numerical optimization. The total cost of such hedging amounts to the\ntotal cost of rebalancing the hedge portfolio.\n"
    },
    {
        "paper_id": 2306.08157,
        "authors": "Rasoul Amirzadeh, Asef Nazari, Dhananjay Thiruvady, and Mong Shan Ee",
        "title": "Causal Feature Engineering of Price Directions of Cryptocurrencies using\n  Dynamic Bayesian Networks",
        "comments": "32 pages, 8 figures, 6 tables",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Cryptocurrencies have gained popularity across various sectors, especially in\nfinance and investment. The popularity is partly due to their unique\nspecifications originating from blockchain-related characteristics such as\nprivacy, decentralisation, and untraceability. Despite their growing\npopularity, cryptocurrencies remain a high-risk investment due to their price\nvolatility and uncertainty. The inherent volatility in cryptocurrency prices,\ncoupled with internal cryptocurrency-related factors and external influential\nglobal economic factors makes predicting their prices and price movement\ndirections challenging. Nevertheless, the knowledge obtained from predicting\nthe direction of cryptocurrency prices can provide valuable guidance for\ninvestors in making informed investment decisions. To address this issue, this\npaper proposes a dynamic Bayesian network (DBN) approach, which can model\ncomplex systems in multivariate settings, to predict the price movement\ndirection of five popular altcoins (cryptocurrencies other than Bitcoin) in the\nnext trading day. The efficacy of the proposed model in predicting\ncryptocurrency price directions is evaluated from two perspectives. Firstly,\nour proposed approach is compared to two baseline models, namely an\nauto-regressive integrated moving average and support vector regression.\nSecondly, from a feature engineering point of view, the impact of twenty-three\ndifferent features, grouped into four categories, on the DBN's prediction\nperformance is investigated. The experimental results demonstrate that the DBN\nsignificantly outperforms the baseline models. In addition, among the groups of\nfeatures, technical indicators are found to be the most effective predictors of\ncryptocurrency price directions.\n"
    },
    {
        "paper_id": 2306.08214,
        "authors": "Qiang Li",
        "title": "Response toward Public Health Policy Ambiguity and Insurance Decisions",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  Adjustments to public health policy are common. This paper investigates the\nimpact of COVID-19 policy ambiguity on specific groups' insurance consumption.\nThe results show that sensitive groups' willingness to pay (WTP) for insurance\nis 12.2% above the benchmark. Groups that have experienced income disruptions\nare more likely to suffer this. This paper offers fresh perspectives on the\neffects of pandemic control shifts.\n"
    },
    {
        "paper_id": 2306.08295,
        "authors": "Kwonhyung Lee, Yejin Lim, Sunghyun Cho",
        "title": "Intranational Skill-relevance Model of the Immigrant's Self-selection:\n  Further Evidence of the Stylized Fact from the E-9 Employment Permit System\n  (EPS)",
        "comments": "18 pages, 3 tables, 2 appendices",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  This study expands upon the foundation of 'Skill-Relevance-Self Selection'\nmodel on labor immigration, introduced by our previous study (Lee, Lim, & Cho,\n2022). In detail, we seek an empirical confirmation of the model by providing\nevidence of the attained -- however, yet to be tested -- stylized fact: 'as the\ndiscount of intranational skill-relevance by immigration is intensified, the\nwage differential of international labor immigration is diminished'. Utilizing\nthe hypothesis and data meticulously formulated and selected in consideration\nof Employment Permit System (EPS) and the typology of host nations, OLS linear\nregression results reasonably support all hypotheses with statistical\nsignificance, thereby inductively substantiating our constructed model. This\npaper contributes to existing labor immigration economics literature in three\nfollowing aspects: (1) Acknowledge the previously overlooked factor of 'skill\nrelevance discount' in labor immigration as an independent parameter, separate\nfrom the 'Moving cost' of Borjas model (1987; 1991); (2) Demonstrate the\ndown-to-earth economic implications of host nation typology, thereby\nestablishing a taxonomy in existence, rather than an ideal classification; (3)\nSeek a complementary synthesis of two grand strands of research methodology --\nthat is, deductive mathematical modeling and inductive statistical testing.\n"
    },
    {
        "paper_id": 2306.08297,
        "authors": "Allister Loder, Fabienne Cantner, Lennart Adenaw, Nico Nachtigall,\n  David Ziegler, Felix Gotzler, Markus B. Siewert, Stefan Wurster, Sebastian\n  Goerg, Markus Lienkamp, Klaus Bogenberger",
        "title": "Germany's nationwide travel experiment in 2022: public transport for 9\n  Euro per month -- First findings of an empirical study",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
        "abstract": "  In spring 2022, the German federal government agreed on a set of policy\nmeasures that aimed at reducing households' financial burden resulting from a\nrecent price increase, especially in energy and mobility. These included among\nothers, a nationwide public transport ticket for 9~Euro per month for three\nmonths in June, July, and August 2022. In transport policy research this is an\nalmost unprecedented behavioral experiment. It allows us to study not only\nbehavioral responses in mode choice and induced demand but also to assess the\neffectiveness of these instruments. We observe this natural experiment with a\nthree-wave survey and a smartphone-based travel diary with passive tracking on\nan initial sample of 2,261 participants with a focus on the Munich metropolitan\nregion. This area is chosen as it offers a variety of mode options with a dense\nand far-reaching public transport network that even provides good access to\nmany leisure destinations. The app has been providing data from 756\nparticipants until the end of September, the three-wave survey by 1,402, and\nthe app and the three waves by 637 participants. In this paper, we report on\nthe study design, the recruitment and study participation as well as the\nimpacts of the policy measures on the self-reported and app-observed travel\nbehavior; we present results on consumer choices for a successor ticket to the\n9-Euro-Ticket that started in May 2023. We find a substantial shift in the\nmodal share towards public transport from the car in our sample during the\n9-Euro-Ticket period in travel distance (around 5 %) and in trip frequency\n(around 7 %). The mobility outcomes of the 9-Euro-Ticket however provide\nevidence that cheap public transport as a policy instrument does not suffice to\nincentive sustainable travel behavior choices and that other policy instruments\nare required in addition.\n"
    },
    {
        "paper_id": 2306.08421,
        "authors": "Tobias Behrens and Gero Junike",
        "title": "Greeks' pitfalls for the COS method in the Laplace model",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  The Greeks Delta, Gamma and Speed are the first, second and third derivatives\nof a European option with respect to the current price of the underlying asset.\nThe Fourier cosine series expansion method (COS method) is a numerical method\nfor approximating the price and the Greeks of European options. We develop a\nclosed-form expression of Speed for various European options in the Laplace\nmodel and we provide sufficient conditions for the COS method to approximate\nSpeed. We show empirically that the COS method may produce numerically\nnonsensical results if theses sufficient conditions are not met.\n"
    },
    {
        "paper_id": 2306.08519,
        "authors": "Jin Hyuk Choi, Jetlir Duraj, Kim Weston",
        "title": "A multi-agent targeted trading equilibrium with transaction costs",
        "comments": "33 pages, 5 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We prove the existence of a continuous-time Radner equilibrium with multiple\nagents and transaction costs. The agents are incentivized to trade towards a\ntargeted number of shares throughout the trading period and seek to maximize\ntheir expected wealth minus a penalty for deviating from their targets. Their\nwealth is further reduced by transaction costs that are proportional to the\nnumber of stock shares traded. The agents' targeted number of shares is\npublicly known, making the resulting equilibrium fully revealing. In\nequilibrium, each agent optimally chooses to trade for an initial time interval\nbefore stopping trade. Our equilibrium construction and analysis involves\nidentifying the order in which the agents stop trade. The transaction cost\nlevel impacts the equilibrium stock price drift. We analyze the equilibrium\noutcomes and provide numerical examples.\n"
    },
    {
        "paper_id": 2306.08743,
        "authors": "Artur F. Tomeczek",
        "title": "The rise of the chaebol: A bibliometric analysis of business groups in\n  South Korea",
        "comments": "35 pages, 5 graphs, 3 tables",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  South Korea has become one of the most important economies in Asia. The\nlargest Korean multinational firms are affiliated with influential family-owned\nbusiness groups known as the chaebol. Despite the surging academic popularity\nof the chaebol, there is a considerable knowledge gap in the bibliometric\nanalysis of business groups in Korea. In an attempt to fill this gap, the\narticle aims to provide a systematic review of the chaebol and the role that\nbusiness groups have played in the economy of Korea. Three distinct\nbibliometric networks are analyzed, namely the scientific collaboration\nnetwork, bibliographic coupling network, and keyword co-occurrence network.\n"
    },
    {
        "paper_id": 2306.0876,
        "authors": "Davide Luparello",
        "title": "Do Productivity Shocks Cause Inputs Misallocation?",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Firms exhibit varying productivity levels even within narrowly defined\nindustries and face uncertainty when predicting future performance. This paper\ninvestigates the link between productivity uncertainty, heterogeneity, and\nmisallocation across all inputs. Using a model where heterogeneous firms face\nstaggered productivity shocks, creating gaps between expected and actual\nproductivity, I find a positive association between marginal revenue product\ndispersions and productivity variability. The analysis reveals that\nproductivity shocks predominantly drive marginal revenue product dispersions.\nBy comparing baseline estimates with those from the factor shares approach, I\nhighlight the limitations of the latter method in analyzing the effects of\nproductivity evolution.\n"
    },
    {
        "paper_id": 2306.08797,
        "authors": "Vitor Costa",
        "title": "Local Labor Market Effects of Mergers and Acquisitions in Developing\n  Countries: Evidence from Brazil",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-sa/4.0/",
        "abstract": "  I use matched employer-employee records merged with corporate tax information\nfrom 2003 to 2017 to estimate labor market-wide effects of mergers and\nacquisitions in Brazil. Labor markets are defined by pairs of commuting zone\nand industry sector. In the following year of a merger, market size falls by\n10.8%. The employment adjustment is concentrated in merging firms. For the\nfirms not involved in M&As, I estimate a 1.07% decline in workers earnings and\na positive, although not significant, increase in their size. Most mergers have\na predicted impact of zero points in concentration, measured by the\nHerfindahl-Hirschman Index (HHI). I spillover firms, earnings decline similarly\nfor mergers with high and low predicted changes in HHI. Contrary to the recent\nliterature on market concentration in developed economies, I find no evidence\nof oligopsonistic behavior in Brazilian labor markets.\n"
    },
    {
        "paper_id": 2306.08809,
        "authors": "Xiaoyue Li, John M. Mulvey",
        "title": "Optimal Portfolio Execution in a Regime-switching Market with Non-linear\n  Impact Costs: Combining Dynamic Program and Neural Network",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
        "abstract": "  Optimal execution of a portfolio have been a challenging problem for\ninstitutional investors. Traders face the trade-off between average trading\nprice and uncertainty, and traditional methods suffer from the curse of\ndimensionality. Here, we propose a four-step numerical framework for the\noptimal portfolio execution problem where multiple market regimes exist, with\nthe underlying regime switching based on a Markov process. The market impact\ncosts are modelled with a temporary part and a permanent part, where the former\naffects only the current trade while the latter persists. Our approach accepts\nimpact cost functions in generic forms. First, we calculate the approximated\northogonal portfolios based on estimated impact cost functions; second, we\nemploy dynamic program to learn the optimal selling schedule of each\napproximated orthogonal portfolio; third, weights of a neural network are\npre-trained with the strategy suggested by previous step; last, we train the\nneural network to optimize on the original trading model. In our experiment of\na 10-asset liquidation example with quadratic impact costs, the proposed\ncombined method provides promising selling strategy for both CRRA (constant\nrelative risk aversion) and mean-variance objectives. The running time is\nlinear in the number of risky assets in the portfolio as well as in the number\nof trading periods. Possible improvements in running time are discussed for\npotential large-scale usages.\n"
    },
    {
        "paper_id": 2306.08829,
        "authors": "Kwonhyung Lee, Yejin Lim, Sunghyun Cho",
        "title": "Migrant Laborer's Optimization Mechanism Under Employment Permit\n  System(EPS): Introducing and Analyzing 'Skill-Relevance-Self Selection' Model",
        "comments": "Language: Korean, 28 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  Migrant laborers subject to ROK's Employment Permit System(EPS) must strike a\nbalance between host country's high wage and 'Depreciation of skill-relevance\nentailed by immigration', whilst taking account of the 'migration costs'. This\nstudy modelizes the optimization mechanism of migrant workers and the firms\nhiring them -- then induces the solution of the very model, namely, 'Subgame\nPerfect Nash Equilibrium(SPNE)', by utilizing game theory's 'backward\ninduction' method. Analyzing the dynamics between variables at SPNE state, the\nattained stylized facts are what as follows; [1]Host nation's skill-relevance\nand wage differential have positive correlation. [2]Emigrating nation's\nskill-relevance and wage differential have negative correlation. Both stylized\nfacts -- [1,2] -- are operationalized into 'Host nation skill-relevance\nhypothesis(H1)' and 'Emigrating nation skill-relevance hypothesis(H2)',\nrespectively; of which are thoroughly tested by OLS linear regression analysis.\nIn all sex/gender parameters(Total/Men/Women), test results support both\nhypotheses with statistical significance, thereby inductively substantiating\nthe constructed model. This paper contributes to existing labor immigration\nliterature in three following aspects: (1)Stimulate the economic approach to\nmigrant labor analysis, and by such means, break away from the overflow of\nsociology, anthropology, political science, and jurisprudence in prior studies;\n(2)Shed a light on the EPS's microeconomic interaction process, of which was\nleft undisclosed as a 'black box'; (3)Seek a complementary synthesis of two\ngrand strands of research methodology -- that is, deductive modeling and\ninductive statistics.\n"
    },
    {
        "paper_id": 2306.09084,
        "authors": "Dan Pirjol, Lingjiong Zhu",
        "title": "Asymptotics for the Laplace transform of the time integral of the\n  geometric Brownian motion",
        "comments": "17 pages, 2 figures, 2 tables",
        "journal-ref": "Operations Research Letters 2023, Volume 51, 346-352",
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We present an asymptotic result for the Laplace transform of the time\nintegral of the geometric Brownian motion $F(\\theta,T) = \\mathbb{E}[e^{-\\theta\nX_T}]$ with $X_T = \\int_0^T e^{\\sigma W_s + ( a - \\frac12 \\sigma^2)s} ds$,\nwhich is exact in the limit $\\sigma^2 T \\to 0$ at fixed $\\sigma^2 \\theta T^2$\nand $aT$. This asymptotic result is applied to pricing zero coupon bonds in the\nDothan model of stochastic interest rates. The asymptotic result provides an\napproximation for bond prices which is in good agreement with numerical\nevaluations in a wide range of model parameters. As a side result we obtain the\nasymptotics for Asian option prices in the Black-Scholes model, taking into\naccount interest rates and dividend yield contributions in the $\\sigma^{2}T\\to\n0$ limit.\n"
    },
    {
        "paper_id": 2306.09421,
        "authors": "Jason Milionis, Xin Wan, Austin Adams",
        "title": "FLAIR: A Metric for Liquidity Provider Competitiveness in Automated\n  Market Makers",
        "comments": "16 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper aims to enhance the understanding of liquidity provider (LP)\nreturns in automated market makers (AMMs). LPs face market risk as well as\nadverse selection due to risky asset holdings in the pool that they provide\nliquidity to and the informational asymmetry between informed traders\n(arbitrageurs) and AMMs. Loss-versus-rebalancing (LVR) quantifies the adverse\nselection cost (Milionis et al., 2022a), and is a popular metric to evaluate\nthe flow toxicity to an AMM. However, individual LP returns are critically\naffected by another factor orthogonal to the above: the competitiveness among\nLPs. This work introduces a novel metric for LP competitiveness, called FLAIR\n(short for fee liquidity-adjusted instantaneous returns), that aims to\nsupplement LVR in assessments of LP performance to capture the dynamic behavior\nof LPs in a pool. Our metric reflects the characteristics of fee\nreturn-on-capital, and differentiates active liquidity provisioning strategies\nin AMMs. To illustrate how both flow toxicity, accounting for the\nsophistication of the counterparty of LPs, as well as LP competitiveness,\naccounting for the sophistication of the competition among LPs, affect\nindividual LP returns, we propose a quadrant interpretation where all of these\ncharacteristics may be readily visualized. We examine LP competitiveness in an\nex-post fashion, and show example cases in all of which our metric confirms the\nexpected nuances and intuition of competitiveness among LPs. FLAIR has\nparticular merit in empirical analyses, and is able to better inform practical\nassessments of AMM pools.\n"
    },
    {
        "paper_id": 2306.09437,
        "authors": "Pranjal Rawat",
        "title": "Designing Auctions when Algorithms Learn to Bid: The critical role of\n  Payment Rules",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  This paper examines the impact of different payment rules on efficiency when\nalgorithms learn to bid. We use a fully randomized experiment of 427 trials,\nwhere Q-learning bidders participate in up to 250,000 auctions for a commonly\nvalued item. The findings reveal that the first price auction, where winners\npay the winning bid, is susceptible to coordinated bid suppression, with\nwinning bids averaging roughly 20% below the true values. In contrast, the\nsecond price auction, where winners pay the second highest bid, aligns winning\nbids with actual values, reduces the volatility during learning and speeds up\nconvergence. Regression analysis, incorporating design elements such as payment\nrules, number of participants, algorithmic factors including the discount and\nlearning rate, asynchronous/synchronous updating, feedback, and exploration\nstrategies, discovers the critical role of payment rules on efficiency.\nFurthermore, machine learning estimators find that payment rules matter even\nmore with few bidders, high discount factors, asynchronous learning, and coarse\nbid spaces. This paper underscores the importance of auction design in\nalgorithmic bidding. It suggests that computerized auctions like Google\nAdSense, which rely on the first price auction, can mitigate the risk of\nalgorithmic collusion by adopting the second price auction.\n"
    },
    {
        "paper_id": 2306.09485,
        "authors": "Elohim Fonseca dos Reis, Alexander Teytelboym, Abeer ElBahraw, Ignacio\n  De Loizaga, Andrea Baronchelli",
        "title": "Identifying key players in dark web marketplaces",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Dark web marketplaces have been a significant outlet for illicit trade,\nserving millions of users worldwide for over a decade. However, not all users\nare the same. This paper aims to identify the key players in Bitcoin\ntransaction networks linked to dark markets and assess their role by analysing\na dataset of 40 million Bitcoin transactions involving 31 markets in the period\n2011-2021. First, we propose an algorithm that categorizes users either as\nbuyers or sellers and shows that a large fraction of the traded volume is\nconcentrated in a small group of elite market participants. Then, we\ninvestigate both market star-graphs and user-to-user networks and highlight the\nimportance of a new class of users, namely `multihomers' who operate on\nmultiple marketplaces concurrently. Specifically, we show how the networks of\nmultihomers and seller-to-seller interactions can shed light on the resilience\nof the dark market ecosystem against external shocks. Our findings suggest that\nunderstanding the behavior of key players in dark web marketplaces is critical\nto effectively disrupting illegal activities.\n"
    },
    {
        "paper_id": 2306.09678,
        "authors": "Attila Lajos Makai, Tibor D\\H{o}ry",
        "title": "Perceived university support and environment as a factor of\n  entrepreneurial intention: Evidence from Western Transdanubia Region",
        "comments": null,
        "journal-ref": null,
        "doi": "10.1371/journal.pone.0283850",
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  The exploration of entrepreneurship has become a priority for scientific\nresearch in recent years. Understanding this phenomenon is particularly\nimportant for the transformation of entrepreneurship into action, which is a\nkey factor in early-stage entrepreneurial activity. This gains particular\nrelevance in the university environment, where, in addition to the conventional\nteaching and research functions, the entrepreneurial university operation based\non open innovation, as well as the enhancement of entrepreneurial attitudes of\nresearchers and students, are receiving increased attention. This study is\nbased on a survey conducted among students attending a Hungarian university of\napplied science in Western Transdanubia Region who have demonstrated their\nexisting entrepreneurial commitment by joining a national startup training and\nincubation programme. The main research question of the study is to what extent\nstudent entrepreneurship intention is influenced by the environment of the\nentrepreneurial university ecosystem and the support services available at the\nuniversity. A further question is whether these factors are able to mitigate\nthe negative effects of internal cognitive and external barriers by enhancing\nentrepreneurial attitudes and perceived behavioural control. The relatively\nlarge number of students involved in the programme allows the data to be\nanalysed using SEM modelling. The results indicate a strong covariance between\nthe perceived university support and environment among students. Another\nobservation is the distinct effect of these institutional factors on perceived\nbehavioural control of students.\n"
    },
    {
        "paper_id": 2306.09798,
        "authors": "Erion \\c{C}ano and Xhesilda Vogli",
        "title": "CSREU: A Novel Dataset about Corporate Social Responsibility and\n  Performance Indicators",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Corporate Social Responsibility (CSR) has become an important topic that is\ngaining academic interest. This research paper presents CSREU, a new dataset\nwith attributes of 115 European companies, which includes several performance\nindicators and the respective CSR disclosure scores computed using the Global\nReporting Initiative (GRI) framework. We also examine the correlations between\nsome of the financial indicators and the CSR disclosure scores of the\ncompanies. According to our results, these correlations are weak and deeper\nanalysis is required to draw convincing conclusions about the potential impact\nof CSR disclosure on financial performance. We hope that the newly created data\nand our preliminary results will help and foster research in this field.\n"
    },
    {
        "paper_id": 2306.09862,
        "authors": "Lifan Zhao, Shuming Kong, Yanyan Shen",
        "title": "DoubleAdapt: A Meta-learning Approach to Incremental Learning for Stock\n  Trend Forecasting",
        "comments": "Accepted by KDD 2023. Code is at https://github.com/SJTU-Quant/qlib",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Stock trend forecasting is a fundamental task of quantitative investment\nwhere precise predictions of price trends are indispensable. As an online\nservice, stock data continuously arrive over time. It is practical and\nefficient to incrementally update the forecast model with the latest data which\nmay reveal some new patterns recurring in the future stock market. However,\nincremental learning for stock trend forecasting still remains under-explored\ndue to the challenge of distribution shifts (a.k.a. concept drifts). With the\nstock market dynamically evolving, the distribution of future data can slightly\nor significantly differ from incremental data, hindering the effectiveness of\nincremental updates. To address this challenge, we propose DoubleAdapt, an\nend-to-end framework with two adapters, which can effectively adapt the data\nand the model to mitigate the effects of distribution shifts. Our key insight\nis to automatically learn how to adapt stock data into a locally stationary\ndistribution in favor of profitable updates. Complemented by data adaptation,\nwe can confidently adapt the model parameters under mitigated distribution\nshifts. We cast each incremental learning task as a meta-learning task and\nautomatically optimize the adapters for desirable data adaptation and parameter\ninitialization. Experiments on real-world stock datasets demonstrate that\nDoubleAdapt achieves state-of-the-art predictive performance and shows\nconsiderable efficiency.\n"
    },
    {
        "paper_id": 2306.10031,
        "authors": "A. Ramirez-Hassan, C. Gomez, S. Velasquez, K. Tangarife",
        "title": "Marijuana on Main Streets? The Story Continues in Colombia: An\n  Endogenous Three-part Model",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Cannabis is the most common illicit drug, and understanding its demand is\nrelevant to analyze the potential implications of its legalization. This paper\nproposes an endogenous three-part model taking into account incidental\ntruncation and access restrictions to study demand for marijuana in Colombia,\nand analyze the potential effects of its legalization. Our application suggests\nthat modeling simultaneously access, intensive and extensive margin is\nrelevant, and that selection into access is important for the intensive margin.\nWe find that younger men that have consumed alcohol and cigarettes, living in a\nneighborhood with drug suppliers, and friends that consume marijuana face\nhigher probability of having access and using this drug. In addition, we find\nthat marijuana is an inelastic good (-0.45 elasticity). Our results are robust\nto different specifications and definitions. If marijuana were legalized,\nyounger individuals with a medium or low risk perception about marijuana use\nwould increase the probability of use in 3.8 percentage points, from 13.6% to\n17.4%. Overall, legalization would increase the probability of consumption in\n0.7 p.p. (2.3% to 3.0%). Different price settings suggest that annual tax\nrevenues fluctuate between USD 11.0 million and USD 54.2 million, a potential\nbenchmark is USD 32 million.\n"
    },
    {
        "paper_id": 2306.10053,
        "authors": "Seonmi Kim, Youngbin Lee, Yejin Kim, Joohwan Hong, and Yongjae Lee",
        "title": "NFTs to MARS: Multi-Attention Recommender System for NFTs",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-sa/4.0/",
        "abstract": "  Recommender systems have become essential tools for enhancing user\nexperiences across various domains. While extensive research has been conducted\non recommender systems for movies, music, and e-commerce, the rapidly growing\nand economically significant Non-Fungible Token (NFT) market remains\nunderexplored. The unique characteristics and increasing prominence of the NFT\nmarket highlight the importance of developing tailored recommender systems to\ncater to its specific needs and unlock its full potential. In this paper, we\nexamine the distinctive characteristics of NFTs and propose the first\nrecommender system specifically designed to address NFT market challenges. In\nspecific, we develop a Multi-Attention Recommender System for NFTs (NFT-MARS)\nwith three key characteristics: (1) graph attention to handle sparse user-item\ninteractions, (2) multi-modal attention to incorporate feature preference of\nusers, and (3) multi-task learning to consider the dual nature of NFTs as both\nartwork and financial assets. We demonstrate the effectiveness of NFT-MARS\ncompared to various baseline models using the actual transaction data of NFTs\ncollected directly from blockchain for four of the most popular NFT\ncollections. The source code and data are available at\nhttps://anonymous.4open.science/r/RecSys2023-93ED.\n"
    },
    {
        "paper_id": 2306.10144,
        "authors": "Simon Montfort",
        "title": "Key predictors for climate policy support and political mobilization:\n  The role of beliefs and preferences",
        "comments": null,
        "journal-ref": null,
        "doi": "10.1371/journal.pclm.0000145",
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Public support and political mobilization are two crucial factors for the\nadoption of ambitious climate policies in line with the international\ngreenhouse gas reduction targets of the Paris Agreement. Despite their compound\nimportance, they are mainly studied separately. Using a random forest\nmachine-learning model, this article investigates the relative predictive power\nof key established explanations for public support and mobilization for climate\npolicies. Predictive models may shape future research priorities and contribute\nto theoretical advancement by showing which predictors are the most and least\nimportant. The analysis is based on a pre-election conjoint survey experiment\non the Swiss CO2 Act in 2021. Results indicate that beliefs (such as the\nperceived effectiveness of policies) and policy design preferences (such as for\nsubsidies or tax-related policies) are the most important predictors while\nother established explanations, such as socio-demographics, issue salience (the\nrelative importance of issues) or political variables (such as the party\naffiliation) have relatively weak predictive power. Thus, beliefs are an\nessential factor to consider in addition to explanations that emphasize issue\nsalience and preferences driven by voters' cost-benefit considerations.\n"
    },
    {
        "paper_id": 2306.10224,
        "authors": "Alex Kim, Maximilian Muhn, Valeri Nikolaev",
        "title": "Bloated Disclosures: Can ChatGPT Help Investors Process Information?",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  Generative AI tools such as ChatGPT can fundamentally change the way\ninvestors process information. We probe the economic usefulness of these tools\nin summarizing complex corporate disclosures using the stock market as a\nlaboratory. The unconstrained summaries are remarkably shorter compared to the\noriginals, whereas their information content is amplified. When a document has\na positive (negative) sentiment, its summary becomes more positive (negative).\nImportantly, the summaries are more effective at explaining stock market\nreactions to the disclosed information. Motivated by these findings, we propose\na measure of information ``bloat.\" We show that bloated disclosure is\nassociated with adverse capital market consequences, such as lower price\nefficiency and higher information asymmetry. Finally, we show that the model is\neffective at constructing targeted summaries that identify firms'\n(non-)financial performance. Collectively, our results indicate that generative\nAI adds considerable value for investors with information processing\nconstraints.\n"
    },
    {
        "paper_id": 2306.10496,
        "authors": "Li Wang, Xing-Lu Gao, Wei-Xing Zhou (ECUST)",
        "title": "Testing for intrinsic multifractality in the global grain spot market\n  indices: A multifractal detrended fluctuation analysis",
        "comments": "23 pages including 14 figures",
        "journal-ref": "Fractals 31 (7), 2350090 (2023)",
        "doi": "10.1142/S0218348X23500901",
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  Grains account for more than 50% of the calories consumed by people\nworldwide, and military conflicts, pandemics, climate change, and soaring grain\nprices all have vital impacts on food security. However, the complex price\nbehavior of the global grain spot markets has not been well understood. A\nrecent study performed multifractal moving average analysis (MF-DMA) of the\nGrains & Oilseeds Index (GOI) and its sub-indices of wheat, maize, soyabeans,\nrice, and barley and found that only the maize and barley sub-indices exhibit\nan intrinsic multifractal nature with convincing evidence. Here, we utilize\nmultifractal fluctuation analysis (MF-DFA) to investigate the same problem.\nExtensive statistical tests confirm the presence of intrinsic multifractality\nin the maize and barley sub-indices and the absence of intrinsic\nmultifractality in the wheat and rice sub-indices. Different from the MF-DMA\nresults, the MF-DFA results suggest that there is also intrinsic\nmultifractality in the GOI and soyabeans sub-indices. Our comparative analysis\ndoes not provide conclusive information about the GOI and soyabeans and\nhighlights the high complexity of the global grain spot markets.\n"
    },
    {
        "paper_id": 2306.10582,
        "authors": "Marc Chen, Mohammad Shirazi, Peter A. Forsyth, Yuying Li",
        "title": "Machine Learning and Hamilton-Jacobi-Bellman Equation for Optimal\n  Decumulation: a Comparison Study",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  We propose a novel data-driven neural network (NN) optimization framework for\nsolving an optimal stochastic control problem under stochastic constraints.\nCustomized activation functions for the output layers of the NN are applied,\nwhich permits training via standard unconstrained optimization. The optimal\nsolution yields a multi-period asset allocation and decumulation strategy for a\nholder of a defined contribution (DC) pension plan. The objective function of\nthe optimal control problem is based on expected wealth withdrawn (EW) and\nexpected shortfall (ES) that directly targets left-tail risk. The stochastic\nbound constraints enforce a guaranteed minimum withdrawal each year. We\ndemonstrate that the data-driven approach is capable of learning a near-optimal\nsolution by benchmarking it against the numerical results from a\nHamilton-Jacobi-Bellman (HJB) Partial Differential Equation (PDE) computational\nframework.\n"
    },
    {
        "paper_id": 2306.10591,
        "authors": "Gerhard Hellstern, Vanessa Dehn, Martin Zaefferer",
        "title": "Quantum computer based Feature Selection in Machine Learning",
        "comments": "25 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  The problem of selecting an appropriate number of features in supervised\nlearning problems is investigated in this paper. Starting with common methods\nin machine learning, we treat the feature selection task as a quadratic\nunconstrained optimization problem (QUBO), which can be tackled with classical\nnumerical methods as well as within a quantum computing framework. We compare\nthe different results in small-sized problem setups. According to the results\nof our study, whether the QUBO method outperforms other feature selection\nmethods depends on the data set. In an extension to a larger data set with 27\nfeatures, we compare the convergence behavior of the QUBO methods via quantum\ncomputing with classical stochastic optimization methods. Due to persisting\nerror rates, the classical stochastic optimization methods are still superior.\n"
    },
    {
        "paper_id": 2306.10612,
        "authors": "Thomas N. Cintra, Maxwell P. Holloway",
        "title": "Detecting Depegs: Towards Safer Passive Liquidity Provision on Curve\n  Finance",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  We consider a liquidity provider's (LP's) exposure to stablecoin and liquid\nstaking derivative (LSD) depegs on Curve's StableSwap pools. We construct a\nsuite of metrics designed to detect potential asset depegs based on price and\ntrading data. Using our metrics, we fine-tune a Bayesian Online Changepoint\nDetection (BOCD) algorithm to alert LPs of potential depegs before or as they\noccur. We train and test our changepoint detection algorithm against Curve LP\ntoken prices for 13 StableSwap pools throughout 2022 and 2023, focusing on\nrelevant stablecoin and LSD depegs. We show that our model, trained on 2022 UST\ndata, is able to detect the USDC depeg in March of 2023 at 9pm UTC on March\n10th, approximately 5 hours before USDC dips below 99 cents, with few false\nalarms in the 17 months on which it is tested. Finally, we describe how this\nresearch may be used by Curve's liquidity providers, and how it may be extended\nto dynamically de-risk Curve pools by modifying parameters in anticipation of\npotential depegs. This research underpins an API developed to alert Curve LPs,\nin real-time, when their positions might be at risk.\n"
    },
    {
        "paper_id": 2306.10659,
        "authors": "Yuanda Chen, Zailei Cheng, Haixu Wang",
        "title": "Option Pricing for the Variance Gamma Model: A New Perspective",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The variance gamma model is a widely popular model for option pricing in both\nacademia and industry. In this paper, we provide a new perspective for pricing\nEuropean style options for the variance gamma model by deriving closed-form\nformulas combining the randomization method and fractional derivatives. We also\ncompare our results with various existing results in the literature by\nnumerical examples.\n"
    },
    {
        "paper_id": 2306.10752,
        "authors": "Alessandro Doldi, Marco Frittelli and Emanuela Rosazza Gianin",
        "title": "Are Shortfall Systemic Risk Measures One Dimensional?",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Shortfall systemic (multivariate) risk measures $\\rho$ defined through an\n$N$-dimensional multivariate utility function $U$ and random allocations can be\nrepresented as classical (one dimensional) shortfall risk measures associated\nto an explicitly determined $1$-dimensional function constructed from $U$. This\nfinding allows for simplifying the study of several properties of $\\rho$, such\nas dual representations, law invariance and stability.\n"
    },
    {
        "paper_id": 2306.10774,
        "authors": "Jeffrey T. Macher, Christian Rutzer, and Rolf Weder",
        "title": "The Illusive Slump of Disruptive Patents",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  Despite tremendous growth in the volume of new scientific and technological\nknowledge, the popular press has recently raised concerns that disruptive\ninnovative activity is slowing. These dire prognoses were mainly driven by Park\net al. (2023), a Nature publication that uses decades of data and millions of\nobservations coupled with a novel quantitative metric (the CD index) that\ncharacterizes innovation in science and technology as either consolidating or\ndisruptive. We challenge the Park et al. (2023) methodology and findings,\nprincipally around concerns of truncation bias and exclusion bias. We show that\n88 percent of the decrease in disruptive patents over 1980-2010 reported by the\nauthors can be explained by their truncation of all backward citations before\n1976. We also show that this truncation bias varies by technology class. We\nupdate the analysis to 2016 and account for a change in U.S. patent law that\nallows for citations to patent applications in addition to patent grants, which\nis ignored by the authors in their analysis. We show that the number of highly\ndisruptive patents has increased since 1980 -- particularly in IT technologies.\nOur results suggest caution in using the Park et al. (2023) methodology as a\nbasis for research and decision making in public policy, industry restructuring\nor firm reorganization aimed at altering the current innovation landscape.\n"
    },
    {
        "paper_id": 2306.10929,
        "authors": "Carlo Marinelli",
        "title": "On some semi-parametric estimates for European option prices",
        "comments": "10 pages",
        "journal-ref": null,
        "doi": "10.1017/jpr.2023.94",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We show that an estimate by de la Pe\\~na, Ibragimov and Jordan for\n$\\mathbb{E}(X-c)^+$, with $c$ a constant and $X$ a random variable of which the\nmean, the variance, and $\\mathbb{P}(X \\leq c)$ are known, implies an estimate\nby Scarf on the infimum of $\\mathbb{E}(X \\wedge c)$ over the set of positive\nrandom variables $X$ with fixed mean and variance. This also shows, as a\nconsequence, that the former estimate implies an estimate by Lo on European\noption prices.\n"
    },
    {
        "paper_id": 2306.1095,
        "authors": "Marc Velay, Bich-Li\\^en Doan, Arpad Rimmel, Fabrice Popineau, Fabrice\n  Daniel",
        "title": "Benchmarking Robustness of Deep Reinforcement Learning approaches to\n  Online Portfolio Management",
        "comments": "Submitted to INISTA 2023",
        "journal-ref": null,
        "doi": "10.1109/INISTA59065.2023.10310402",
        "license": "http://creativecommons.org/licenses/by-sa/4.0/",
        "abstract": "  Deep Reinforcement Learning approaches to Online Portfolio Selection have\ngrown in popularity in recent years. The sensitive nature of training\nReinforcement Learning agents implies a need for extensive efforts in market\nrepresentation, behavior objectives, and training processes, which have often\nbeen lacking in previous works. We propose a training and evaluation process to\nassess the performance of classical DRL algorithms for portfolio management. We\nfound that most Deep Reinforcement Learning algorithms were not robust, with\nstrategies generalizing poorly and degrading quickly during backtesting.\n"
    },
    {
        "paper_id": 2306.11025,
        "authors": "Xinli Yu, Zheng Chen, Yuan Ling, Shujing Dong, Zongyi Liu, Yanbin Lu",
        "title": "Temporal Data Meets LLM -- Explainable Financial Time Series Forecasting",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
        "abstract": "  This paper presents a novel study on harnessing Large Language Models' (LLMs)\noutstanding knowledge and reasoning abilities for explainable financial time\nseries forecasting. The application of machine learning models to financial\ntime series comes with several challenges, including the difficulty in\ncross-sequence reasoning and inference, the hurdle of incorporating multi-modal\nsignals from historical news, financial knowledge graphs, etc., and the issue\nof interpreting and explaining the model results. In this paper, we focus on\nNASDAQ-100 stocks, making use of publicly accessible historical stock price\ndata, company metadata, and historical economic/financial news. We conduct\nexperiments to illustrate the potential of LLMs in offering a unified solution\nto the aforementioned challenges. Our experiments include trying\nzero-shot/few-shot inference with GPT-4 and instruction-based fine-tuning with\na public LLM model Open LLaMA. We demonstrate our approach outperforms a few\nbaselines, including the widely applied classic ARMA-GARCH model and a\ngradient-boosting tree model. Through the performance comparison results and a\nfew examples, we find LLMs can make a well-thought decision by reasoning over\ninformation from both textual news and price time series and extracting\ninsights, leveraging cross-sequence information, and utilizing the inherent\nknowledge embedded within the LLM. Additionally, we show that a publicly\navailable LLM such as Open-LLaMA, after fine-tuning, can comprehend the\ninstruction to generate explainable forecasts and achieve reasonable\nperformance, albeit relatively inferior in comparison to GPT-4.\n"
    },
    {
        "paper_id": 2306.11049,
        "authors": "Mohammadhosein Bahmanpour-Khalesi and Mohammadjavad Sharifzadeh",
        "title": "Public Finance or Public Choice? The Scholastic Political Economy As an\n  Essentialist Synthesis",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Nowadays, it is thought that there are only two approaches to political\neconomy: public finance and public choice; however, this research aims to\nintroduce a new insight by investigating scholastic sources. We study the\nrelevant classic books from the thirteenth to the seventeenth centuries and\nreevaluate the scholastic literature by doctrines of public finance and public\nchoice. The findings confirm that the government is the institution for\nrealizing the common good according to scholastic attitude. Therefore,\nscholastic thinkers saw a common mission for the government based on their\nessentialist attitude toward human happiness. Social conflicts and lack of\nsocial consent are the product of diversification in ends and desires; hence,\nif the end of humans were unified, there would be no conflict of interest.\nAccordingly, if the government acts according to its assigned mission, the lack\nof public consent is not significant. Based on the scholastic point of view\nthis study introduces the third approach to political economy, which can be,\nconsider an analytical synthesis among classical doctrines.\n"
    },
    {
        "paper_id": 2306.11061,
        "authors": "Fabio Baschetti (1), Giacomo Bormetti (2), Pietro Rossi (2 and 3) ((1)\n  Scuola Normale Superiore, (2) University of Bologna, (3) Prometeia S.p.A.)",
        "title": "Deep calibration with random grids",
        "comments": "38 pages, 22 figures, and 9 tables",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We propose a neural network-based approach to calibrating stochastic\nvolatility models, which combines the pioneering grid approach by Horvath et\nal. (2021) with the pointwise two-stage calibration of Bayer et al. (2018) and\nLiu et al. (2019). Our methodology inherits robustness from the former while\nnot suffering from the need for interpolation/extrapolation techniques, a clear\nadvantage ensured by the pointwise approach. The crucial point to the entire\nprocedure is the generation of implied volatility surfaces on random grids,\nwhich one dispenses to the network in the training phase. We support the\nvalidity of our calibration technique with several empirical and Monte Carlo\nexperiments for the rough Bergomi and Heston models under a simple but\neffective parametrization of the forward variance curve. The approach paves the\nway for valuable applications in financial engineering - for instance, pricing\nunder local stochastic volatility models - and extensions to the fast-growing\nfield of path-dependent volatility models.\n"
    },
    {
        "paper_id": 2306.11158,
        "authors": "Marcos Escobar-Anel, Michel Kschonnek, Rudi Zagst",
        "title": "Mind the Cap! -- Constrained Portfolio Optimisation in Heston's\n  Stochastic Volatility Model",
        "comments": "40 pages, 5 figures",
        "journal-ref": null,
        "doi": "10.1080/14697688.2023.2271223",
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  We consider a portfolio optimisation problem for a utility-maximising\ninvestor who faces convex constraints on his portfolio allocation in Heston's\nstochastic volatility model. We apply the duality methods developed in previous\nwork to obtain a closed-form expression for the optimal portfolio allocation.\nIn doing so, we observe that allocation constraints impact the optimal\nconstrained portfolio allocation in a fundamentally different way in Heston's\nstochastic volatility model than in the Black Scholes model. In particular, the\noptimal constrained portfolio may be different from the naive capped portfolio,\nwhich caps off the optimal unconstrained portfolio at the boundaries of the\nconstraints. Despite this difference, we illustrate by way of a numerical\nanalysis that in most realistic scenarios the capped portfolio leads to slim\nannual wealth equivalent losses compared to the optimal constrained portfolio.\nDuring a financial crisis, however, a capped solution might lead to compelling\nannual wealth equivalent losses.\n"
    },
    {
        "paper_id": 2306.11376,
        "authors": "Rossana Mastrandrea, Leonardo Boncinelli and Ennio Bilancini",
        "title": "Coevolution of cognition and cooperation in structured populations under\n  reinforcement learning",
        "comments": "10 pages, 2 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  We study the evolution of behavior under reinforcement learning in a\nPrisoner's Dilemma where agents interact in a regular network and can learn\nabout whether they play one-shot or repeatedly by incurring a cost of\ndeliberation. With respect to other behavioral rules used in the literature,\n(i) we confirm the existence of a threshold value of the probability of\nrepeated interaction, switching the emergent behavior from intuitive defector\nto dual-process cooperator; (ii) we find a different role of the node degree,\nwith smaller degrees reducing the evolutionary success of dual-process\ncooperators; (iii) we observe a higher frequency of deliberation.\n"
    },
    {
        "paper_id": 2306.1147,
        "authors": "David Criens and Mikhail Urusov",
        "title": "Criteria for NUPBR, NFLVR and the existence of EMMs in integrated\n  diffusion markets",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Consider a single asset financial market whose discounted asset price process\nis a stochastic integral with respect to a continuous regular strong Markov\nsemimartingale (a so-called general diffusion semimartingale) that is\nparameterized by a scale function and a speed measure. In a previous paper, we\nestablished a characterization of the no free lunch with vanishing risk (NFLVR)\ncondition for a canonical framework of such a financial market in terms of the\nscale function and the speed measure. Ioannis Karatzas (personal communication)\nasked us whether it is also possible to prove a characterization for the weaker\nno unbounded profit with bounded risk (NUPBR) condition, which is the main\nquestion we treat in this paper. Here, we do not restrict our attention to\ncanonical frameworks but we allow a general setup with a general filtration\nthat preserves the strong Markov property. Our main results are precise\ncharacterizations of NUPBR and NFLVR which only depend on the scale function\nand the speed measure. In particular, we prove that NUPBR forces the scale\nfunction to be continuously differentiable with absolutely continuous\nderivative. The latter extends our previous result, that, in the canonical\nframework, NFLVR implies such a property, in two directions (a weaker\nno-arbitrage notion and a more general framework). We also make the surprising\nobservation that NUPBR and NFLVR are equivalent whenever finite boundary points\nare accessible for the driving diffusion.\n"
    },
    {
        "paper_id": 2306.11566,
        "authors": "Philipp Andreas Gunkel (1), Febin Kachirayil (2), Claire-Marie\n  Bergaentzl\\'e (1), Russell McKenna (2 and 3), Dogan Keles (1), and Henrik\n  Klinge Jacobsen (1) ((1) Section for Energy Economics and Modelling, DTU\n  Management, Technical University of Denmark, 2800 Kongens Lyngby, Denmark,\n  (2) Chair of Energy Systems Analysis, Institute of Energy and Process\n  Engineering, ETH Zuerich, 8092 Zuerich, Switzerland, (3) Paul Scherrer\n  Institute, Laboratory for Energy Systems Analysis, Forschungsstrasse 111,\n  5232 Villigen PSI, Switzerland)",
        "title": "Uniform taxation of electricity: incentives for flexibility and cost\n  redistribution among household categories",
        "comments": "44 pages, 15 figures, 5 tables, Under review in Energy Economics",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  Recent years have shown a rapid adoption of residential solar PV with\nincreased self-consumption and self-sufficiency levels in Europe. A major\ndriver for their economic viability is the electricity tax exemption for the\nconsumption of self-produced electricity. This leads to large residential PV\ncapacities and partially overburdened distribution grids. Furthermore, the tax\nexemption that benefits wealthy households that can afford capital-intense\ninvestments in solar panels in particular has sparked discussions about energy\nequity and the appropriate taxation level for self-consumption. This study\ninvestigates the implementation of uniform electricity taxes on all\nconsumption, irrespective of the origin of the production, by means of a case\nstudy of 155,000 hypothetical Danish prosumers. The results show that the new\ntaxation policy redistributes costs progressively across household sizes. As\nmore consumption is taxed, the tax level can be reduced by 38%, leading to 61%\nof all households seeing net savings of up to 23% off their yearly tax bill.\nHigh-occupancy houses save an average of 116 Euro per year at the expense of\nsingle households living in large dwellings who pay 55 Euro per year more.\nImplementing a uniform electricity tax in combination with a reduced overall\ntax level can (a) maintain overall tax revenues and (b) increase the\ninteraction of batteries with the grid at the expense of behind-the-meter\noperations. In the end, the implicit cross-subsidy is removed by taxing\nself-consumption uniformly, leading to a cost redistribution supporting\noccupant-dense households and encouraging the flexible behavior of prosumers.\nThis policy measure improves economic efficiency and greater use of technology\nwith positive system-wide impacts.\n"
    },
    {
        "paper_id": 2306.1158,
        "authors": "Richard Dewey, Craig Newbold",
        "title": "The Pricing And Hedging Of Constant Function Market Makers",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  We investigate the most common type of blockchain-based decentralized\nexchange, which are known as constant function market makers (CFMMs). We\nexamine the the market microstructure around CFMMs and present a model for\nvaluing the liquidity provider (LP) mechanism and estimating the value of the\nassociated derivatives. We develop a model with two types of traders that have\ndifferent information and contribute methods for simulating the behavior of\neach trader and accounting for trade PnL. We also develop ideas around the\nequilibrium distribution of fair price conditional on the arrival of traders.\nFinally, we show how these findings might be used to think about parameters for\nalternative CFMMs.\n"
    },
    {
        "paper_id": 2306.11599,
        "authors": "Francesca Biagini, Alessandro Doldi, Jean-Pierre Fouque, Marco\n  Frittelli and Thilo Meyer-Brandis",
        "title": "Collective Arbitrage and the Value of Cooperation",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We introduce the notions of Collective Arbitrage and of Collective\nSuper-replication in a discrete-time setting where agents are investing in\ntheir markets and are allowed to cooperate through exchanges. We accordingly\nestablish versions of the fundamental theorem of asset pricing and of the\npricing-hedging duality. A reduction of the price interval of the contingent\nclaims can be obtained by applying the collective super-replication price.\n"
    },
    {
        "paper_id": 2306.12119,
        "authors": "Zhi Su, Danni Wu, Zhenkun Zhou, Junran Wu, Libo Yin",
        "title": "The Impact of Customer Online Satisfaction on Stock Returns: Evidence\n  from the E-commerce Reviews in China",
        "comments": "29 pages, 9 tables",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  This paper investigates the significance of consumer opinions in relation to\nvalue in China's A-share market. By analyzing a large dataset comprising over\n18 million product reviews by customers on JD.com, we demonstrate that\nsentiments expressed in consumer reviews can influence stock returns,\nindicating that consumer opinions contain valuable information that can impact\nthe stock market. Our findings show that Customer Negative Sentiment Tendency\n(CNST) and One-Star Tendency (OST) have a negative effect on expected stock\nreturns, even after controlling for firm characteristics such as market risk,\nilliquidity, idiosyncratic volatility, and asset growth. Further analysis\nreveals that the predictive power of CNST is stronger in firms with high\nsentiment conditions, growth companies, and firms with lower accounting\ntransparency. We also find that CNST negatively predicts revenue surprises,\nearnings surprises, and cash flow shocks. These results suggest that online\nsatisfaction derived from big data analysis of customer reviews contains novel\ninformation about firms' fundamentals.\n"
    },
    {
        "paper_id": 2306.12434,
        "authors": "Aditya Pandey, Kunal Joshi",
        "title": "Using Internal Bar Strength as a Key Indicator for Trading Country ETFs",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  This report aims to investigate the effectiveness of using internal bar\nstrength (IBS) as a key indicator for trading country exchange-traded funds\n(ETFs). The study uses a quantitative approach to analyze historical price data\nfor a bucket of country ETFs over a period of 10 years and uses the idea of\nMean Reversion to create a profitable trading strategy. Our findings suggest\nthat IBS can be a useful technical indicator for predicting short-term price\nmovements in this basket of ETFs.\n"
    },
    {
        "paper_id": 2306.12439,
        "authors": "Yuxia Liu and Qi Zhang and Wei Xiao and Tianguang Chu",
        "title": "Successive one-sided Hodrick-Prescott filter with incremental filtering\n  algorithm for nonlinear economic time series",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We propose a successive one-sided Hodrick-Prescott (SOHP) filter from\nmultiple time scale decomposition perspective to derive trend estimate for a\ntime series. The idea is to apply the one-sided HP (OHP) filter recursively on\nthe updated cyclical component to extract the trend residual on multiple time\nscales, thereby to improve the trend estimate. To address the issue of\noptimization with a moving horizon as that of the SOHP filter, we present an\nincremental HP filtering algorithm, which greatly simplifies the involved\ninverse matrix operation and reduces the computational demand of the basic HP\nfiltering. Actually, the new algorithm also applies effectively to other\nHP-type filters, especially for large-size or expanding data scenario.\nNumerical examples on real economic data show the better performance of the\nSOHP filter in comparison with other known HP-type filters.\n"
    },
    {
        "paper_id": 2306.12446,
        "authors": "Wenbo Ge, Pooia Lalbakhsh, Leigh Isai, Artem Lensky, Hanna Suominen",
        "title": "Comparing Deep Learning Models for the Task of Volatility Prediction\n  Using Multivariate Data",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  This study aims to compare multiple deep learning-based forecasters for the\ntask of predicting volatility using multivariate data. The paper evaluates a\nrange of models, starting from simpler and shallower ones and progressing to\ndeeper and more complex architectures. Additionally, the performance of these\nmodels is compared against naive predictions and variations of classical GARCH\nmodels.\n  The prediction of volatility for five assets, namely S&P500, NASDAQ100, gold,\nsilver, and oil, is specifically addressed using GARCH models, Multi-Layer\nPerceptrons, Recurrent Neural Networks, Temporal Convolutional Networks, and\nthe Temporal Fusion Transformer. In the majority of cases, the Temporal Fusion\nTransformer, followed by variants of the Temporal Convolutional Network,\noutperformed classical approaches and shallow networks. These experiments were\nrepeated, and the differences observed between the competing models were found\nto be statistically significant, thus providing strong encouragement for their\npractical application.\n"
    },
    {
        "paper_id": 2306.12602,
        "authors": "Domonkos F. Vamossy",
        "title": "Social Media Emotions and IPO Returns",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  I examine potential mechanisms behind two stylized facts of initial public\nofferings (IPOs) returns. By analyzing investor emotions expressed on\nStockTwits and Twitter, I find that emotions conveyed through these social\nmedia platforms can help explain the mispricing of IPO stocks. The abundance of\ninformation and opinions shared on social media can generate hype around\ncertain stocks, leading to investors' irrational buying and selling decisions.\nThis can result in an overvaluation of the stock in the short term but often\nleads to a correction in the long term as the stock's performance fails to meet\nthe inflated expectations. In particular, I find that IPOs with high levels of\npre-IPO enthusiasm tend to have a significantly higher first-day return of\n29.73%, compared to IPOs with lower levels of pre-IPO investor enthusiasm,\nwhich have an average first-day return of 17.59%. However, this initial\nenthusiasm may be misplaced, as IPOs with high pre-IPO investor enthusiasm\ndemonstrate a much lower average long-run industry-adjusted return of -8.22%,\ncompared to IPOs with lower pre-IPO investor enthusiasm, which have an average\nlong-run industry-adjusted return of -0.14%. Diving deeper into the qualitative\naspects of investor discourse, I find that messages rich in financial language\nor that bolster prevailing information drive my results. Additionally, a trend\ntowards caution emerges among users who frequently engage with IPOs, perhaps a\nbyproduct of lessons from past endeavors. Intriguingly, firms that enjoy high\nlevels of pre-IPO optimism consistently garner post-launch enthusiasm, a trend\nat odds with their long-term under-performance.\n"
    },
    {
        "paper_id": 2306.12639,
        "authors": "Cassidy K. Buhler, Hande Y. Benson",
        "title": "Efficient Solution of Portfolio Optimization Problems via Dimension\n  Reduction and Sparsification",
        "comments": "14 pages, 3 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  The Markowitz mean-variance portfolio optimization model aims to balance\nexpected return and risk when investing. However, there is a significant\nlimitation when solving large portfolio optimization problems efficiently: the\nlarge and dense covariance matrix. Since portfolio performance can be\npotentially improved by considering a wider range of investments, it is\nimperative to be able to solve large portfolio optimization problems\nefficiently, typically in microseconds. We propose dimension reduction and\nincreased sparsity as remedies for the covariance matrix. The size reduction is\nbased on predictions from machine learning techniques and the solution to a\nlinear programming problem. We find that using the efficient frontier from the\nlinear formulation is much better at predicting the assets on the Markowitz\nefficient frontier, compared to the predictions from neural networks. Reducing\nthe covariance matrix based on these predictions decreases both runtime and\ntotal iterations. We also present a technique to sparsify the covariance matrix\nsuch that it preserves positive semi-definiteness, which improves runtime per\niteration. The methods we discuss all achieved similar portfolio expected risk\nand return as we would obtain from a full dense covariance matrix but with\nimproved optimizer performance.\n"
    },
    {
        "paper_id": 2306.12658,
        "authors": "Erhan Bayraktar, Bingyan Han",
        "title": "Fitted Value Iteration Methods for Bicausal Optimal Transport",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We develop a fitted value iteration (FVI) method to compute bicausal optimal\ntransport (OT) where couplings have an adapted structure. Based on the dynamic\nprogramming formulation, FVI adopts a function class to approximate the value\nfunctions in bicausal OT. Under the concentrability condition and approximate\ncompleteness assumption, we prove the sample complexity using (local)\nRademacher complexity. Furthermore, we demonstrate that multilayer neural\nnetworks with appropriate structures satisfy the crucial assumptions required\nin sample complexity proofs. Numerical experiments reveal that FVI outperforms\nlinear programming and adapted Sinkhorn methods in scalability as the time\nhorizon increases, while still maintaining acceptable accuracy.\n"
    },
    {
        "paper_id": 2306.12659,
        "authors": "Boyu Zhang, Hongyang Yang, Xiao-Yang Liu",
        "title": "Instruct-FinGPT: Financial Sentiment Analysis by Instruction Tuning of\n  General-Purpose Large Language Models",
        "comments": "FinLLM Symposium at IJCAI 2023",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Sentiment analysis is a vital tool for uncovering insights from financial\narticles, news, and social media, shaping our understanding of market\nmovements. Despite the impressive capabilities of large language models (LLMs)\nin financial natural language processing (NLP), they still struggle with\naccurately interpreting numerical values and grasping financial context,\nlimiting their effectiveness in predicting financial sentiment. In this paper,\nwe introduce a simple yet effective instruction tuning approach to address\nthese issues. By transforming a small portion of supervised financial sentiment\nanalysis data into instruction data and fine-tuning a general-purpose LLM with\nthis method, we achieve remarkable advancements in financial sentiment\nanalysis. In the experiment, our approach outperforms state-of-the-art\nsupervised sentiment analysis models, as well as widely used LLMs like ChatGPT\nand LLaMAs, particularly in scenarios where numerical understanding and\ncontextual comprehension are vital.\n"
    },
    {
        "paper_id": 2306.12806,
        "authors": "Andrea Coletta, Joseph Jerome, Rahul Savani, and Svitlana Vyetrenko",
        "title": "Conditional Generators for Limit Order Book Environments:\n  Explainability, Challenges, and Robustness",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Limit order books are a fundamental and widespread market mechanism. This\npaper investigates the use of conditional generative models for order book\nsimulation. For developing a trading agent, this approach has drawn recent\nattention as an alternative to traditional backtesting due to its ability to\nreact to the presence of the trading agent. Using a state-of-the-art CGAN (from\nColetta et al. (2022)), we explore its dependence upon input features, which\nhighlights both strengths and weaknesses. To do this, we use \"adversarial\nattacks\" on the model's features and its mechanism. We then show how these\ninsights can be used to improve the CGAN, both in terms of its realism and\nrobustness. We finish by laying out a roadmap for future work.\n"
    },
    {
        "paper_id": 2306.12921,
        "authors": "David Xiao",
        "title": "Generic Forward Curve Dynamics for Commodity Derivatives",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  This article presents a generic framework for modeling the dynamics of\nforward curves in commodity market as commodity derivatives are typically\ntraded by futures or forwards. We have theoretically demonstrated that\ncommodity prices are driven by multiple components. As such, the model can\nbetter capture the forward price and volatility dynamics. Empirical study shows\nthat the model prices are very close to the market prices, indicating prima\nfacie that the model performs quite well.\n"
    },
    {
        "paper_id": 2306.12924,
        "authors": "Radost Waszkiewicz and Honorata Bogusz",
        "title": "The Impact of Parenthood on Labour Market Outcomes of Women and Men in\n  Poland",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
        "abstract": "  We examine the gender gap in income in Poland in relation to parenthood\nstatus, employing the placebo event history method adapted to low-resolution\ndata (Polish Generations and Gender Survey). Our analysis reveals anticipatory\nbehavior in both women and men who expect to become parents. We observe a\ndecrease of approximately 20 percent in mothers' income post-birth. In\ncontrast, the income of fathers surpasses that of non-fathers both pre- and\npost-birth, suggesting that the fatherhood child premium may be primarily\ndriven by selection. We note an increase (decrease) in hours worked for fathers\n(mothers). Finally, we compare the gender gaps in income and wages between\nwomen and men in the sample with those in a counterfactual scenario where the\nentire population is childless. Our findings indicate no statistically\nsignificant gender gaps in the counterfactual scenario, leading us to conclude\nthat parenthood drives the gender gaps in income and wages in Poland.\n"
    },
    {
        "paper_id": 2306.12964,
        "authors": "Shuo Yu, Hongyan Xue, Xiang Ao, Feiyang Pan, Jia He, Dandan Tu, and\n  Qing He",
        "title": "Generating Synergistic Formulaic Alpha Collections via Reinforcement\n  Learning",
        "comments": "Accepted by KDD '23, ADS track",
        "journal-ref": null,
        "doi": "10.1145/3580305.3599831",
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  In the field of quantitative trading, it is common practice to transform raw\nhistorical stock data into indicative signals for the market trend. Such\nsignals are called alpha factors. Alphas in formula forms are more\ninterpretable and thus favored by practitioners concerned with risk. In\npractice, a set of formulaic alphas is often used together for better modeling\nprecision, so we need to find synergistic formulaic alpha sets that work well\ntogether. However, most traditional alpha generators mine alphas one by one\nseparately, overlooking the fact that the alphas would be combined later. In\nthis paper, we propose a new alpha-mining framework that prioritizes mining a\nsynergistic set of alphas, i.e., it directly uses the performance of the\ndownstream combination model to optimize the alpha generator. Our framework\nalso leverages the strong exploratory capabilities of reinforcement\nlearning~(RL) to better explore the vast search space of formulaic alphas. The\ncontribution to the combination models' performance is assigned to be the\nreturn used in the RL process, driving the alpha generator to find better\nalphas that improve upon the current set. Experimental evaluations on\nreal-world stock market data demonstrate both the effectiveness and the\nefficiency of our framework for stock trend forecasting. The investment\nsimulation results show that our framework is able to achieve higher returns\ncompared to previous approaches.\n"
    },
    {
        "paper_id": 2306.12965,
        "authors": "Sohum Thakkar (1), Skander Kazdaghli (1), Natansh Mathur (1 and 2),\n  Iordanis Kerenidis (1 and 2), Andr\\'e J. Ferreira-Martins (3), Samurai Brito\n  (3) ((1) QC Ware Corp, (2) IRIF - Universit\\'e Paris Cit\\'e and CNRS, (3)\n  Ita\\'u Unibanco)",
        "title": "Improved Financial Forecasting via Quantum Machine Learning",
        "comments": "The version of record of this article was submitted for publication\n  in Quantum Machine Intelligence (https://link.springer.com/journal/42484)",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Quantum algorithms have the potential to enhance machine learning across a\nvariety of domains and applications. In this work, we show how quantum machine\nlearning can be used to improve financial forecasting. First, we use classical\nand quantum Determinantal Point Processes to enhance Random Forest models for\nchurn prediction, improving precision by almost 6%. Second, we design quantum\nneural network architectures with orthogonal and compound layers for credit\nrisk assessment, which match classical performance with significantly fewer\nparameters. Our results demonstrate that leveraging quantum ideas can\neffectively enhance the performance of machine learning, both today as\nquantum-inspired classical ML solutions, and even more in the future, with the\nadvent of better quantum hardware.\n"
    },
    {
        "paper_id": 2306.12969,
        "authors": "David Noel",
        "title": "Stock Price Prediction using Dynamic Neural Networks",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper will analyze and implement a time series dynamic neural network to\npredict daily closing stock prices. Neural networks possess unsurpassed\nabilities in identifying underlying patterns in chaotic, non-linear, and\nseemingly random data, thus providing a mechanism to predict stock price\nmovements much more precisely than many current techniques. Contemporary\nmethods for stock analysis, including fundamental, technical, and regression\ntechniques, are conversed and paralleled with the performance of neural\nnetworks. Also, the Efficient Market Hypothesis (EMH) is presented and\ncontrasted with Chaos theory using neural networks. This paper will refute the\nEMH and support Chaos theory. Finally, recommendations for using neural\nnetworks in stock price prediction will be presented.\n"
    },
    {
        "paper_id": 2306.1307,
        "authors": "Hector Galindo-Silva and Guy Tchuente",
        "title": "Armed Conflict and Early Human Capital Accumulation: Evidence from\n  Cameroon's Anglophone Conflict",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  This paper examines the impact of the Anglophone Conflict in Cameroon on\nhuman capital accumulation. Using high-quality individual-level data on test\nscores and information on conflict-related violent events, a\ndifference-in-differences design is employed to estimate the conflict's causal\neffects. The results show that an increase in violent events and\nconflict-related deaths causes a significant decline in test scores in reading\nand mathematics. The conflict also leads to higher rates of teacher absenteeism\nand reduced access to electricity in schools. These findings highlight the\nadverse consequences of conflict-related violence on human capital\naccumulation, particularly within the Anglophone subsystem. The study\nemphasizes the disproportionate burden faced by Anglophone pupils due to\nlanguage-rooted tensions and segregated educational systems.\n"
    },
    {
        "paper_id": 2306.13208,
        "authors": "Anisha Ghosh, Alexandros Theloudis",
        "title": "Consumption Partial Insurance in the Presence of Tail Income Risk",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  We measure the extent of consumption insurance to income shocks accounting\nfor high-order moments of the income distribution. We derive a nonlinear\nconsumption function, in which the extent of insurance varies with the sign and\nmagnitude of income shocks. Using PSID data, we estimate an asymmetric\npass-through of bad versus good permanent shocks -- 17% of a 3 sigma negative\nshock transmits to consumption compared to 9% of an equal-sized positive shock\n-- and the pass-through increases as the shock worsens. We find similar\npatterns for transitory shocks among the least wealthy. Households are willing\nto sacrifice more than 1/8 of lifetime consumption to eliminate tail income\nrisk. Our results are consistent with surveys of consumption responses to\nhypothetical events and suggest that tail risk matters substantially for\nconsumption.\n"
    },
    {
        "paper_id": 2306.13343,
        "authors": "Julian H\\\"olzermann",
        "title": "Optimal Investment with Stochastic Interest Rates and Ambiguity",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper studies dynamic asset allocation with interest rate risk and\nseveral sources of ambiguity. The market consists of a risk-free asset, a\nzero-coupon bond (both determined by a Vasicek model), and a stock. There is\nambiguity about the risk premia, the volatilities, and the correlation. The\ninvestor's preferences display both risk aversion and ambiguity aversion. The\noptimal investment problem admits a closed-form solution. The solution shows\nthat the ambiguity only affects the speculative motives of the investor,\nrepresenting a hedge against the ambiguity, but not the hedging of interest\nrate risk. An implementation of the optimal investment strategy shows that\nambiguity aversion helps to tame the highly leveraged portfolios neglecting\nambiguity and leads to strategies that are more in line with popular investment\nadvice.\n"
    },
    {
        "paper_id": 2306.13371,
        "authors": "Xavier Brouty and Matthieu Garcin",
        "title": "Fractal properties, information theory, and market efficiency",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-sa/4.0/",
        "abstract": "  Considering that both the entropy-based market information and the Hurst\nexponent are useful tools for determining whether the efficient market\nhypothesis holds for a given asset, we study the link between the two\napproaches. We thus provide a theoretical expression for the market information\nwhen log-prices follow either a fractional Brownian motion or its stationary\nextension using the Lamperti transform. In the latter model, we show that a\nHurst exponent close to 1/2 can lead to a very high informativeness of the time\nseries, because of the stationarity mechanism. In addition, we introduce a\nmultiscale method to get a deeper interpretation of the entropy and of the\nmarket information, depending on the size of the information set. Applications\nto Bitcoin, CAC 40 index, Nikkei 225 index, and EUR/USD FX rate, using daily or\nintraday data, illustrate the methodological content.\n"
    },
    {
        "paper_id": 2306.13378,
        "authors": "Yuki Sato and Kiyoshi Kanazawa",
        "title": "Exact solution to a generalised Lillo-Mike-Farmer model with\n  heterogeneous order-splitting strategies",
        "comments": "additional theoretical analyses",
        "journal-ref": "Journal of Statistical Physics 191, 58 (2024)",
        "doi": "10.1007/s10955-024-03264-1",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The Lillo-Mike-Farmer (LMF) model is an established econophysics model\ndescribing the order-splitting behaviour of institutional investors in\nfinancial markets. In the original article (LMF, Physical Review E 71, 066122\n(2005)), LMF assumed the homogeneity of the traders' order-splitting strategy\nand derived a power-law asymptotic solution to the order-sign autocorrelation\nfunction (ACF) based on several heuristic reasonings. This report proposes a\ngeneralised LMF model by incorporating the heterogeneity of traders'\norder-splitting behaviour that is exactly solved without heuristics. We find\nthat the power-law exponent in the order-sign ACF is robust for arbitrary\nheterogeneous intensity distributions. On the other hand, the prefactor in the\nACF is very sensitive to heterogeneity in trading strategies and is shown to be\nsystematically underestimated in the original homogeneous LMF model. Our work\nhighlights that the ACF prefactor should be more carefully interpreted than the\nACF power-law exponent in data analyses.\n"
    },
    {
        "paper_id": 2306.13419,
        "authors": "Simon Hirsch and Florian Ziel",
        "title": "Multivariate Simulation-based Forecasting for Intraday Power Markets:\n  Modelling Cross-Product Price Effects",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Intraday electricity markets play an increasingly important role in balancing\nthe intermittent generation of renewable energy resources, which creates a need\nfor accurate probabilistic price forecasts. However, research to date has\nfocused on univariate approaches, while in many European intraday electricity\nmarkets all delivery periods are traded in parallel. Thus, the dependency\nstructure between different traded products and the corresponding cross-product\neffects cannot be ignored. We aim to fill this gap in the literature by using\ncopulas to model the high-dimensional intraday price return vector. We model\nthe marginal distribution as a zero-inflated Johnson's $S_U$ distribution with\nlocation, scale and shape parameters that depend on market and fundamental\ndata. The dependence structure is modelled using latent beta regression to\naccount for the particular market structure of the intraday electricity market,\nsuch as overlapping but independent trading sessions for different delivery\ndays. We allow the dependence parameter to be time-varying. We validate our\napproach in a simulation study for the German intraday electricity market and\nfind that modelling the dependence structure improves the forecasting\nperformance. Additionally, we shed light on the impact of the single intraday\ncoupling (SIDC) on the trading activity and price distribution and interpret\nour results in light of the market efficiency hypothesis. The approach is\ndirectly applicable to other European electricity markets.\n"
    },
    {
        "paper_id": 2306.13436,
        "authors": "Yichuan Tian",
        "title": "Does Environmental Attention by Governments Promote Carbon Reductions",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The carbon-reducing effect of attention is scarcer than that of material\nresources, and when the government focuses its attention on the environment,\nresources will be allocated in a direction that is conducive to reducing\ncarbon. Using panel data from 30 Chinese provinces from 2007 to 2019, this\nstudy revealed the impact of governments' environmental attention on carbon\nemissions and the synergistic mechanism between governments' environmental\nattention and informatization level. The findings suggested that (1)the\nenvironmental attention index of local governments in China showed an overall\nfluctuating upward trend; (2)governments' environmental atten-tion had the\neffect of reducing carbon emissions; (3)the emission-reducing effect of\ngovernments' environmental attention is more significant in the western region\nbut not in the central and eastern regions; (4)informatization level plays a\npositive moderating role in the relationship between governments' environmental\nattention and carbon emissions; (5)there is a significant threshold effect on\nthe carbon reduction effect of governments' environmental attention. Based on\nthe findings, this study proposed policy implications from the perspectives of\npromoting the sustainable enhancement of environmental attention, bringing\ninstitutional functions into play, emphasizing the ecological benefits and\nstrengthening the disclosure of information.\n"
    },
    {
        "paper_id": 2306.13661,
        "authors": "Joel Ong, Dorien Herremans",
        "title": "Constructing Time-Series Momentum Portfolios with Deep Multi-Task\n  Learning",
        "comments": null,
        "journal-ref": "Expert Systems with Applications Volume 230, 15 November 2023,\n  120587",
        "doi": "10.1016/j.eswa.2023.120587",
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  A diversified risk-adjusted time-series momentum (TSMOM) portfolio can\ndeliver substantial abnormal returns and offer some degree of tail risk\nprotection during extreme market events. The performance of existing TSMOM\nstrategies, however, relies not only on the quality of the momentum signal but\nalso on the efficacy of the volatility estimator. Yet many of the existing\nstudies have always considered these two factors to be independent. Inspired by\nrecent progress in Multi-Task Learning (MTL), we present a new approach using\nMTL in a deep neural network architecture that jointly learns portfolio\nconstruction and various auxiliary tasks related to volatility, such as\nforecasting realized volatility as measured by different volatility estimators.\nThrough backtesting from January 2000 to December 2020 on a diversified\nportfolio of continuous futures contracts, we demonstrate that even after\naccounting for transaction costs of up to 3 basis points, our approach\noutperforms existing TSMOM strategies. Moreover, experiments confirm that\nadding auxiliary tasks indeed boosts the portfolio's performance. These\nfindings demonstrate that MTL can be a powerful tool in finance.\n"
    },
    {
        "paper_id": 2306.13772,
        "authors": "Till Baldenius, Nicolas Koch, Hannah Klauber, Nadja Klein",
        "title": "Heat increases experienced racial segregation in the United States",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  Segregation on the basis of ethnic groups stands as a pervasive and\npersistent social challenge in many cities across the globe. Public spaces\nprovide opportunities for diverse encounters but recent research suggests\nindividuals adjust their time spent in such places to cope with extreme\ntemperatures. We evaluate to what extent such adaptation affects racial\nsegregation and thus shed light on a yet unexplored channel through which\nglobal warming might affect social welfare. We use large-scale foot traffic\ndata for millions of places in 315 US cities between 2018 and 2020 to estimate\nan index of experienced isolation in daily visits between whites and other\nethnic groups. We find that heat increases segregation. Results from panel\nregressions imply that a week with temperatures above 33{\\deg}C in a city like\nLos Angeles induces an upward shift of visit isolation by 0.7 percentage\npoints, which equals about 14% of the difference in the isolation index of Los\nAngeles to the more segregated city of Atlanta. The segregation-increasing\neffect is particularly strong for individuals living in lower-income areas and\nat places associated with leisure activities. Combining our estimates with\nclimate model projections, we find that stringent mitigation policy can have\nsignificant co-benefits in terms of cushioning increases in racial segregation\nin the future.\n"
    },
    {
        "paper_id": 2306.13858,
        "authors": "Ran Yan, Nan Zhou, Wei Feng, Minda Ma, Xiwang Xiang, Chao Mao",
        "title": "Decarbonization patterns of residential building operations in China and\n  India",
        "comments": "42 pages,11 figs",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  As the two largest emerging emitters with the highest growth in operational\ncarbon from residential buildings, the historical emission patterns and\ndecarbonization efforts of China and India warrant further exploration. This\nstudy aims to be the first to present a carbon intensity model considering\nend-use performances, assessing the operational decarbonization progress of\nresidential building in India and China over the past two decades using the\nimproved decomposing structural decomposition approach. Results indicate (1)\nthe overall operational carbon intensity increased by 1.4% and 2.5% in China\nand India, respectively, between 2000 and 2020. Household expenditure-related\nenergy intensity and emission factors were crucial in decarbonizing residential\nbuildings. (2) Building electrification played a significant role in\ndecarbonizing space cooling (-87.7 in China and -130.2 kilograms of carbon\ndioxide (kgCO2) per household in India) and appliances (-169.7 in China and\n-43.4 kgCO2 per household in India). (3) China and India collectively\ndecarbonized 1498.3 and 399.7 mega-tons of CO2 in residential building\noperations, respectively. In terms of decarbonization intensity, India (164.8\nkgCO2 per household) nearly caught up with China (182.5 kgCO2 per household) in\n2020 and is expected to surpass China in the upcoming years, given the\ncountry's robust annual growth rate of 7.3%. Overall, this study provides an\neffective data-driven tool for investigating the building decarbonization\npotential in China and India, and offers valuable insights for other emerging\neconomies seeking to decarbonize residential buildings in the forthcoming COP28\nage.\n"
    },
    {
        "paper_id": 2306.14004,
        "authors": "Alain-Philippe Fortin, Patrick Gagliardini, Olivier Scaillet",
        "title": "Latent Factor Analysis in Short Panels",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  We develop inferential tools for latent factor analysis in short panels. The\npseudo maximum likelihood setting under a large cross-sectional dimension n and\na fixed time series dimension T relies on a diagonal TxT covariance matrix of\nthe errors without imposing sphericity nor Gaussianity. We outline the\nasymptotic distributions of the latent factor and error covariance estimates as\nwell as of an asymptotically uniformly most powerful invariant (AUMPI) test for\nthe number of factors based on the likelihood ratio statistic. We derive the\nAUMPI characterization from inequalities ensuring the monotone likelihood ratio\nproperty for positive definite quadratic forms in normal variables. An\nempirical application to a large panel of monthly U.S. stock returns separates\nmonth after month systematic and idiosyncratic risks in short subperiods of\nbear vs. bull market based on the selected number of factors. We observe an\nuptrend in the paths of total and idiosyncratic volatilities while the\nsystematic risk explains a large part of the cross-sectional total variance in\nbear markets but is not driven by a single factor. Rank tests show that\nobserved factors struggle spanning latent factors with a discrepancy between\nthe dimensions of the two factor spaces decreasing over time.\n"
    },
    {
        "paper_id": 2306.14186,
        "authors": "Raffaele Sgarlato",
        "title": "Statistical electricity price forecasting: A structural approach",
        "comments": "15 pages, 9 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  The availability of historical data related to electricity day-ahead prices\nand to the underlying price formation process is limited. In addition, the\nelectricity market in Europe is facing a rapid transformation, which limits the\nrepresentativeness of older observations for predictive purposes. On the other\nhand, machine learning methods that gained traction also in the domain of\nelectricity price forecasting typically require large amounts of data. This\nstudy analyses the effectiveness of encoding well-established domain knowledge\nto mitigate the need for large training datasets. The domain knowledge is\nincorporated by imposing a structure on the price forecasting problem; the\nresulting accuracy gains are quantified in an experiment. Compared to an\n\"unstructured\" purely statistical model, it is shown that introducing\nintermediate quantity forecasts of load, renewable infeed, and cross-border\nexchange, paired with the estimation of supply curves, can result in a NRMSE\nreduction by 0.1 during daytime hours. The statistically most significant\nimprovements are achieved in the first day of the forecasting horizon when a\npurely statistical model is combined with structured models. Finally, results\nare evaluated and interpreted with regard to the dynamic market conditions\nobserved in Europe during the experiment period (from the 1st October 2022 to\nthe 30th April 2023), highlighting the adaptive nature of models that are\ntrained on shorter timescales.\n"
    },
    {
        "paper_id": 2306.14222,
        "authors": "Haohan Zhang, Fengrui Hua, Chengjin Xu, Hao Kong, Ruiting Zuo, Jian\n  Guo",
        "title": "Unveiling the Potential of Sentiment: Can Large Language Models Predict\n  Chinese Stock Price Movements?",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  The rapid advancement of Large Language Models (LLMs) has spurred discussions\nabout their potential to enhance quantitative trading strategies. LLMs excel in\nanalyzing sentiments about listed companies from financial news, providing\ncritical insights for trading decisions. However, the performance of LLMs in\nthis task varies substantially due to their inherent characteristics. This\npaper introduces a standardized experimental procedure for comprehensive\nevaluations. We detail the methodology using three distinct LLMs, each\nembodying a unique approach to performance enhancement, applied specifically to\nthe task of sentiment factor extraction from large volumes of Chinese news\nsummaries. Subsequently, we develop quantitative trading strategies using these\nsentiment factors and conduct back-tests in realistic scenarios. Our results\nwill offer perspectives about the performances of Large Language Models applied\nto extracting sentiments from Chinese news texts.\n"
    },
    {
        "paper_id": 2306.14506,
        "authors": "Martin Herdegen and Cosimo Munari",
        "title": "An elementary proof of the dual representation of Expected Shortfall",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  We provide an elementary proof of the dual representation of Expected\nShortfall on the space of integrable random variables over a general\nprobability space. Unlike the results in the extant literature, our proof only\nexploits basic properties of quantile functions and can thus be easily\nimplemented in any graduate course on risk measures. As a byproduct, we obtain\na new proof of the subadditivity of Expected Shortfall.\n"
    },
    {
        "paper_id": 2306.14602,
        "authors": "E. Al\\`os, F. Rolloos, K. Shiraya",
        "title": "A lower bound for the volatility swap in the lognormal SABR model",
        "comments": "arXiv admin note: text overlap with arXiv:1912.05383",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In the short time to maturity limit it is proved that for the conditionally\nlognormal SABR model the zero vanna implied volatility is a lower bound for the\nvolatility swap strike. The result is valid for all values of the correlation\nparameter and is a sharper lower bound than the at-the-money implied volatility\nfor correlation less than or equal to zero.\n"
    },
    {
        "paper_id": 2306.15026,
        "authors": "David Xiao",
        "title": "Valuation of Equity Linked Securities with Guaranteed Return",
        "comments": "20 pages, 4 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Equity-linked securities with a guaranteed return become very popular in\nfinancial markets ether as investment instruments or life insurance policies.\nThe contract pays off a guaranteed amount plus a payment linked to the\nperformance of a basket of equities averaged over a certain period. This paper\npresents a new model for valuing equity-linked securities. Our study shows that\nthe security price can be replicated by the sum of the guaranteed amount plus\nthe price of an Asian style option on the basket. Analytical formulas are\nderived for the security price and corresponding hedge ratios. The model\nappears to be accurate over a wide range of underlying security parameters\naccording to numerical studies. Finally, we use our model to value a segregated\nfund with a guarantee at maturity.\n"
    },
    {
        "paper_id": 2306.15033,
        "authors": "Thomas Dohmke (GitHub), Marco Iansiti (Harvard Business School and\n  Keystone.AI) and Greg Richards (Keystone.AI)",
        "title": "Sea Change in Software Development: Economic and Productivity Analysis\n  of the AI-Powered Developer Lifecycle",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  This study examines the impact of GitHub Copilot on a large sample of Copilot\nusers (n=934,533). The analysis shows that users on average accept nearly 30%\nof the suggested code, leading to increased productivity. Furthermore, our\nresearch demonstrates that the acceptance rate rises over time and is\nparticularly high among less experienced developers, providing them with\nsubstantial benefits. Additionally, our estimations indicate that the adoption\nof generative AI productivity tools could potentially contribute to a $1.5\ntrillion increase in global GDP by 2030. Moreover, our investigation sheds\nlight on the diverse contributors in the generative AI landscape, including\nmajor technology companies, startups, academia, and individual developers. The\nfindings suggest that the driving force behind generative AI software\ninnovation lies within the open-source ecosystem, particularly in the United\nStates. Remarkably, a majority of repositories on GitHub are led by individual\ndevelopers. As more developers embrace these tools and acquire proficiency in\nthe art of prompting with generative AI, it becomes evident that this novel\napproach to software development has forged a unique inextricable link between\nhumans and artificial intelligence. This symbiotic relationship has the\npotential to shape the construction of the world's software for future\ngenerations.\n"
    },
    {
        "paper_id": 2306.15524,
        "authors": "Xin Hai and Kihun Nam",
        "title": "Robust Wasserstein Optimization and its Application in Mean-CVaR",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We refer to recent inference methodology and formulate a framework for\nsolving the distributionally robust optimization problem, where the true\nprobability measure is inside a Wasserstein ball around the empirical measure\nand the radius of the Wasserstein ball is determined by the empirical data. We\ntransform the robust optimization into a non-robust optimization with a penalty\nterm and provide the selection of the Wasserstein ambiguity set's size.\nMoreover, we apply this framework to the robust mean-CVaR optimization problem\nand the numerical experiments of the US stock market show impressive results\ncompared to other popular strategies.\n"
    },
    {
        "paper_id": 2306.15526,
        "authors": "Yang Qiao, Yiping Xia, Xiang Li, Zheng Li, Yan Ge",
        "title": "Higher-order Graph Attention Network for Stock Selection with Joint\n  Analysis",
        "comments": "12 pages, 6 figures,",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Stock selection is important for investors to construct profitable\nportfolios. Graph neural networks (GNNs) are increasingly attracting\nresearchers for stock prediction due to their strong ability of relation\nmodelling and generalisation. However, the existing GNN methods only focus on\nsimple pairwise stock relation and do not capture complex higher-order\nstructures modelling relations more than two nodes. In addition, they only\nconsider factors of technical analysis and overlook factors of fundamental\nanalysis that can affect the stock trend significantly. Motivated by them, we\npropose higher-order graph attention network with joint analysis (H-GAT). H-GAT\nis able to capture higher-order structures and jointly incorporate factors of\nfundamental analysis with factors of technical analysis. Specifically, the\nsequential layer of H-GAT take both types of factors as the input of a\nlong-short term memory model. The relation embedding layer of H-GAT constructs\na higher-order graph and learn node embedding with GAT. We then predict the\nranks of stock return. Extensive experiments demonstrate the superiority of our\nH-GAT method on the profitability test and Sharp ratio over both NSDAQ and NYSE\ndatasets\n"
    },
    {
        "paper_id": 2306.15554,
        "authors": "Leilei Shi, Xinshuai Guo, Jiuchang Wei, Wei Zhang, Guocheng Wang,\n  Bing-Hong Wang",
        "title": "A Theory of Complex Adaptive Learning and a Non-Localized Wave Equation\n  in Quantum Mechanics",
        "comments": "19 pages in total, 8 figures, 1 table, and 59 references",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Complex adaptive learning is intelligent. It is adaptive, learns in feedback\nloops, and generates hidden patterns as many individuals, elements or particles\ninteract in complex adaptive systems (CAS). CAS highlights adaptation in life\nand lifeless complex systems cutting across all traditional natural and social\nsciences disciplines. However, discovering a universal law in CAS and\nunderstanding the underlying mechanism of distribution formation, such as a\nnon-Gauss distribution in complex quantum entanglement, remains highly\nchallenging. Quantifying the uncertainty of CAS by probability wave functions,\nthe authors explore the inherent logical relationship between Schr\\\"odinger's\nwave equation in quantum mechanics and Shi's trading volume-price wave equation\nin finance. Subsequently, the authors propose a non-localized wave equation in\nquantum mechanics if cumulative observable in a time interval represents\nmomentum or momentum force in Skinner-Shi (reinforcement-frequency-interaction)\ncoordinates. It reveals that the invariance of interaction as a universal law\nexists in quantum mechanics and finance. The theory shows that quantum\nentanglement is an interactively coherent state instead of a consequence of the\nsuperposition of coherent states. As a resource, quantum entanglement is\nnon-separable, steerable, and energy-consumed. The entanglement state has\nopposite states subject to interaction conservation between the momentum and\nreversal forces. Keywords: complex adaptive systems, complex adaptive learning,\ncomplex quantum systems, non-localized wave equation, interaction conservation,\ninteractively coherent entanglement PACS: 89.75.-k (Complex Systems); 89.65.Gh\n(Economics, Econophysics, Financial Markets, Business and Management); 03.65.Ud\n(Entanglement and Quantum Nonlocality)\n"
    },
    {
        "paper_id": 2306.15585,
        "authors": "Sherly Alfonso-S\\'anchez, Jes\\'us Solano, Alejandro Correa-Bahnsen,\n  Kristina P. Sendova, and Cristi\\'an Bravo",
        "title": "Optimizing Credit Limit Adjustments Under Adversarial Goals Using\n  Reinforcement Learning",
        "comments": "29 pages, 16 figures",
        "journal-ref": "Alfonso-Sanchez, S., Solano, J., Correa-Bahnsen, A., Sendova, K.\n  P., & Bravo, C. (2024). Optimizing credit limit adjustments under adversarial\n  goals using reinforcement learning. European Journal of Operational Research\n  315(2): 802-817",
        "doi": "10.1016/j.ejor.2023.12.025",
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  Reinforcement learning has been explored for many problems, from video games\nwith deterministic environments to portfolio and operations management in which\nscenarios are stochastic; however, there have been few attempts to test these\nmethods in banking problems. In this study, we sought to find and automatize an\noptimal credit card limit adjustment policy by employing reinforcement learning\ntechniques. Because of the historical data available, we considered two\npossible actions per customer, namely increasing or maintaining an individual's\ncurrent credit limit. To find this policy, we first formulated this\ndecision-making question as an optimization problem in which the expected\nprofit was maximized; therefore, we balanced two adversarial goals: maximizing\nthe portfolio's revenue and minimizing the portfolio's provisions. Second,\ngiven the particularities of our problem, we used an offline learning strategy\nto simulate the impact of the action based on historical data from a super-app\nin Latin America to train our reinforcement learning agent. Our results, based\non the proposed methodology involving synthetic experimentation, show that a\nDouble Q-learning agent with optimized hyperparameters can outperform other\nstrategies and generate a non-trivial optimal policy not only reflecting the\ncomplex nature of this decision but offering an incentive to explore\nreinforcement learning in real-world banking scenarios. Our research\nestablishes a conceptual structure for applying reinforcement learning\nframework to credit limit adjustment, presenting an objective technique to make\nthese decisions primarily based on data-driven methods rather than relying only\non expert-driven systems. We also study the use of alternative data for the\nproblem of balance prediction, as the latter is a requirement of our proposed\nmodel. We find the use of such data does not always bring prediction gains.\n"
    },
    {
        "paper_id": 2306.15807,
        "authors": "Qi Deng, Zhong-guo Zhou",
        "title": "Liquidity Premium, Liquidity-Adjusted Return and Volatility, and Extreme\n  Liquidity",
        "comments": "48 page, 7 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  We establish innovative liquidity premium measures, and construct\nliquidity-adjusted return and volatility to model assets with extreme\nliquidity, represented by a portfolio of selected crypto assets, and upon which\nwe develop a set of liquidity-adjusted ARMA-GARCH/EGARCH models. We demonstrate\nthat these models produce superior predictability at extreme liquidity to their\ntraditional counterparts. We provide empirical support by comparing the\nperformances of a series of Mean Variance portfolios.\n"
    },
    {
        "paper_id": 2306.15835,
        "authors": "Zacharia Issa, Blanka Horvath",
        "title": "Non-parametric online market regime detection and regime clustering for\n  multidimensional and path-dependent data structures",
        "comments": "65 pages, 52 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this work we present a non-parametric online market regime detection\nmethod for multidimensional data structures using a path-wise two-sample test\nderived from a maximum mean discrepancy-based similarity metric on path space\nthat uses rough path signatures as a feature map. The latter similarity metric\nhas been developed and applied as a discriminator in recent generative models\nfor small data environments, and has been optimised here to the setting where\nthe size of new incoming data is particularly small, for faster reactivity.\n  On the same principles, we also present a path-wise method for regime\nclustering which extends our previous work. The presented regime clustering\ntechniques were designed as ex-ante market analysis tools that can identify\nperiods of approximatively similar market activity, but the new results also\napply to path-wise, high dimensional-, and to non-Markovian settings as well as\nto data structures that exhibit autocorrelation.\n  We demonstrate our clustering tools on easily verifiable synthetic datasets\nof increasing complexity, and also show how the outlined regime detection\ntechniques can be used as fast on-line automatic regime change detectors or as\noutlier detection tools, including a fully automated pipeline. Finally, we\napply the fine-tuned algorithms to real-world historical data including\nhigh-dimensional baskets of equities and the recent price evolution of crypto\nassets, and we show that our methodology swiftly and accurately indicated\nhistorical periods of market turmoil.\n"
    },
    {
        "paper_id": 2306.16162,
        "authors": "R.P.Datta",
        "title": "Analysis of Indian foreign exchange markets: A Multifractal Detrended\n  Fluctuation Analysis (MFDFA) approach",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The multifractal spectra of daily foreign exchange rates for US dollar (USD),\nthe British Pound (GBP), the Euro (Euro) and the Japanese Yen (Yen) with\nrespect to the Indian Rupee are analysed for the period 6th January 1999 to\n24th July 2018. We observe that the time series of logarithmic returns of all\nthe four exchange rates exhibit features of multifractality. Next, we research\nthe source of the observed multifractality. For this, we transform the return\nseries in two ways: a) We randomly shuffle the original time series of\nlogarithmic returns and b) We apply the process of phase randomisation on the\nunchanged series. Our results indicate in the case of the US dollar the source\nof multifractality is mainly the fat tail. For the British Pound and the Euro,\nwe see the long-range correlations between the observations and the thick tails\nof the probability distribution give rise to the observed multifractal\nfeatures, while in the case of the Japanese Yen, the origin of the multifractal\nnature of the return series is mostly due to the broad tail.\n"
    },
    {
        "paper_id": 2306.16165,
        "authors": "Jean-Philippe Bouchaud, Matteo Marsili, Jean-Pierre Nadal",
        "title": "Application of spin glass ideas in social sciences, economics and\n  finance",
        "comments": "Contribution to the edited volume \"Spin Glass Theory & Far Beyond -\n  Replica Symmetry Breaking after 40 Years\", World Scientific, 2023",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  Classical economics has developed an arsenal of methods, based on the idea of\nrepresentative agents, to come up with precise numbers for next year's GDP,\ninflation and exchange rates, among (many) other things. Few, however, will\ndisagree with the fact that the economy is a complex system, with a large\nnumber of strongly heterogeneous, interacting units of different types (firms,\nbanks, households, public institutions) and different sizes.\n  Now, the main issue in economics is precisely the emergent organization,\ncooperation and coordination of such a motley crowd of micro-units. Treating\nthem as a unique ``representative'' firm or household clearly risks throwing\nthe baby with the bathwater. As we have learnt from statistical physics,\nunderstanding and characterizing such emergent properties can be difficult.\nBecause of feedback loops of different signs, heterogeneities and\nnon-linearities, the macro-properties are often hard to anticipate. In\nparticular, these situations generically lead to a very large number of\npossible equilibria, or even the lack thereof.\n  Spin-glasses and other disordered systems give a concrete example of such\ndifficulties. In order to tackle these complex situations, new theoretical and\nnumerical tools have been invented in the last 50 years, including of course\nthe replica method and replica symmetry breaking, and the cavity method, both\nstatic and dynamic. In this chapter we review the application of such ideas and\nmethods in economics and social sciences. Of particular interest are the\nproliferation (and fragility) of equilibria, the analogue of satisfiability\nphase transitions in games and random economies, and condensation (or\nconcentration) effects in opinion, wealth, etc\n"
    },
    {
        "paper_id": 2306.16208,
        "authors": "Xiaoli Wei, Xiang Yu",
        "title": "Continuous-time q-learning for mean-field control problems",
        "comments": "Keywords: Continuous-time reinforcement learning, continuous-time\n  q-function, Mckean-Vlasov control, weak martingale characterization, test\n  policies",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper studies the q-learning, recently coined as the continuous time\ncounterpart of Q-learning by Jia and Zhou (2023), for continuous time\nMckean-Vlasov control problems in the setting of entropy-regularized\nreinforcement learning. In contrast to the single agent's control problem in\nJia and Zhou (2023), the mean-field interaction of agents renders the\ndefinition of the q-function more subtle, for which we reveal that two distinct\nq-functions naturally arise: (i) the integrated q-function (denoted by $q$) as\nthe first-order approximation of the integrated Q-function introduced in Gu,\nGuo, Wei and Xu (2023), which can be learnt by a weak martingale condition\ninvolving test policies; and (ii) the essential q-function (denoted by $q_e$)\nthat is employed in the policy improvement iterations. We show that two\nq-functions are related via an integral representation under all test policies.\nBased on the weak martingale condition and our proposed searching method of\ntest policies, some model-free learning algorithms are devised. In two\nexamples, one in LQ control framework and one beyond LQ control framework, we\ncan obtain the exact parameterization of the optimal value function and\nq-functions and illustrate our algorithms with simulation experiments.\n"
    },
    {
        "paper_id": 2306.16346,
        "authors": "Claude Martini, Arianna Mingone",
        "title": "A closed form model-free approximation for the Initial Margin of option\n  portfolios",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Central clearing counterparty houses (CCPs) play a fundamental role in\nmitigating the counterparty risk for exchange traded options. CCPs cover for\npossible losses during the liquidation of a defaulting member's portfolio by\ncollecting initial margins from their members. In this article we analyze the\ncurrent state of the art in the industry for computing initial margins for\noptions, whose core component is generally based on a VaR or Expected Shortfall\nrisk measure. We derive an approximation formula for the VaR at short horizons\nin a model-free setting. This innovating formula has promising features and\nbehaves in a much more satisfactory way than the classical Filtered Historical\nSimulation-based VaR in our numerical experiments. In addition, we consider the\nneural-SDE model for normalized call prices proposed by [Cohen et al.,\narXiv:2202.07148, 2022] and obtain a quasi-explicit formula for the VaR and a\nclosed formula for the short term VaR in this model, due to its conditional\naffine structure.\n"
    },
    {
        "paper_id": 2306.16351,
        "authors": "Viktor Kuzmenko, Anton Malandii, Stan Uryasev",
        "title": "Expectile Quadrangle and Applications",
        "comments": "Incomplete result",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  The paper explores the concept of the \\emph{expectile risk measure} within\nthe framework of the Fundamental Risk Quadrangle (FRQ) theory. According to the\nFRQ theory, a quadrangle comprises four stochastic functions associated with a\nrandom variable: ``error'', ``regret'', ``risk'', and ``deviation''. These\nfunctions are interconnected through a stochastic function known as the\n``statistic''. Expectile is a risk measure that, similar to VaR (quantile) and\nCVaR (superquantile), can be employed in risk management. While quadrangles\nbased on VaR and CVaR statistics are well-established and widely used, the\npaper focuses on the recently proposed quadrangles based on expectile. The aim\nof this paper is to rigorously examine the properties of these Expectile\nQuadrangles, with particular emphasis on a quadrangle that encompasses\nexpectile as both a statistic and a measure of risk.\n"
    },
    {
        "paper_id": 2306.16422,
        "authors": "Ariel Neufeld, Julian Sester",
        "title": "Neural networks can detect model-free static arbitrage strategies",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper we demonstrate both theoretically as well as numerically that\nneural networks can detect model-free static arbitrage opportunities whenever\nthe market admits some. Due to the use of neural networks, our method can be\napplied to financial markets with a high number of traded securities and\nensures almost immediate execution of the corresponding trading strategies. To\ndemonstrate its tractability, effectiveness, and robustness we provide examples\nusing real financial data. From a technical point of view, we prove that a\nsingle neural network can approximately solve a class of convex semi-infinite\nprograms, which is the key result in order to derive our theoretical results\nthat neural networks can detect model-free static arbitrage strategies whenever\nthe financial market admits such opportunities.\n"
    },
    {
        "paper_id": 2306.16424,
        "authors": "Erik Altman, Jovan Blanu\\v{s}a, Luc von Niederh\\\"ausern, B\\'eni\n  Egressy, Andreea Anghel, Kubilay Atasu",
        "title": "Realistic Synthetic Financial Transactions for Anti-Money Laundering\n  Models",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  With the widespread digitization of finance and the increasing popularity of\ncryptocurrencies, the sophistication of fraud schemes devised by cybercriminals\nis growing. Money laundering -- the movement of illicit funds to conceal their\norigins -- can cross bank and national boundaries, producing complex\ntransaction patterns. The UN estimates 2-5\\% of global GDP or \\$0.8 - \\$2.0\ntrillion dollars are laundered globally each year. Unfortunately, real data to\ntrain machine learning models to detect laundering is generally not available,\nand previous synthetic data generators have had significant shortcomings. A\nrealistic, standardized, publicly-available benchmark is needed for comparing\nmodels and for the advancement of the area.\n  To this end, this paper contributes a synthetic financial transaction dataset\ngenerator and a set of synthetically generated AML (Anti-Money Laundering)\ndatasets. We have calibrated this agent-based generator to match real\ntransactions as closely as possible and made the datasets public. We describe\nthe generator in detail and demonstrate how the datasets generated can help\ncompare different machine learning models in terms of their AML abilities. In a\nkey way, using synthetic data in these comparisons can be even better than\nusing real data: the ground truth labels are complete, whilst many laundering\ntransactions in real data are never detected.\n"
    },
    {
        "paper_id": 2306.16522,
        "authors": "Yifan He, Yuan Hu and Svetlozar Rachev",
        "title": "The Implied Views of Bond Traders on the Spot Equity Market",
        "comments": "15 pages, 4 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  This study delves into the temporal dynamics within the equity market through\nthe lens of bond traders. Recognizing that the riskless interest rate\nfluctuates over time, we leverage the Black-Derman-Toy model to trace its\ntemporal evolution. To gain insights from a bond trader's perspective, we focus\non a specific type of bond: the zero-coupon bond. This paper introduces a\npricing algorithm for this bond and presents a formula that can be used to\nascertain its real value. By crafting an equation that juxtaposes the\ntheoretical value of a zero-coupon bond with its actual value, we can deduce\nthe risk-neutral probability. It is noteworthy that the risk-neutral\nprobability correlates with variables like the instantaneous mean return,\ninstantaneous volatility, and inherent upturn probability in the equity market.\nExamining these relationships enables us to discern the temporal shifts in\nthese parameters. Our findings suggest that the mean starts at a negative\nvalue, eventually plateauing at a consistent level. The volatility, on the\nother hand, initially has a minimal positive value, peaks swiftly, and then\nstabilizes. Lastly, the upturn probability is initially significantly high,\nplunges rapidly, and ultimately reaches equilibrium.\n"
    },
    {
        "paper_id": 2306.16525,
        "authors": "Anton Malandii, Siddhartha Gupte, Cheng Peng, Stan Uryasev",
        "title": "Divergence Based Quadrangle and Applications",
        "comments": "Incomplete result",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  This paper introduces a novel framework for assessing risk and\ndecision-making in the presence of uncertainty, the \\emph{$\\varphi$-Divergence\nQuadrangle}. This approach expands upon the traditional Risk Quadrangle, a\nmodel that quantifies uncertainty through four key components: \\emph{risk,\ndeviation, regret}, and \\emph{error}. The $\\varphi$-Divergence Quadrangle\nincorporates the $\\varphi$-divergence as a measure of the difference between\nprobability distributions, thereby providing a more nuanced understanding of\nrisk. Importantly, the $\\varphi$-Divergence Quadrangle is closely connected\nwith the distributionally robust optimization based on the $\\varphi$-divergence\napproach through the duality theory of convex functionals. To illustrate its\npracticality and versatility, several examples of the $\\varphi$-Divergence\nQuadrangle are provided, including the Quantile Quadrangle. The final portion\nof the paper outlines a case study implementing regression with the Entropic\nValue-at-Risk Quadrangle. The proposed $\\varphi$-Divergence Quadrangle presents\na refined methodology for understanding and managing risk, contributing to the\nongoing development of risk assessment and management strategies.\n"
    },
    {
        "paper_id": 2306.16553,
        "authors": "Delia Coculescu and M\\'ed\\'eric Motte and Huy\\^en Pham",
        "title": "Opinion dynamics in communities with major influencers and implicit\n  social influence via mean-field approximation",
        "comments": "42 pages, 8 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/publicdomain/zero/1.0/",
        "abstract": "  We study binary opinion formation in a large population where individuals are\ninfluenced by the opinions of other individuals. The population is\ncharacterised by the existence of (i) communities where individuals share some\nsimilar features, (ii) opinion leaders that may trigger unpredictable opinion\nshifts in the short term (iii) some degree of incomplete information in the\nobservation of the individual or public opinion processes. In this setting, we\nstudy three different approximate mechanisms: common sampling approximation,\nindependent sampling approximation, and, what will be our main focus in this\npaper, McKean-Vlasov (or mean-field) approximation. We show that all three\napproximations perform well in terms of different metrics that we introduce for\nmeasuring population level and individual level errors. In the presence of a\ncommon noise represented by the major influencers opinions processes, and\ndespite the absence of idiosyncratic noises, we derive a propagation of chaos\ntype result. For the particular case of a linear model and particular\nspecifications of the major influencers opinion dynamics, we provide additional\nanalysis, including long term behavior and fluctuations of the public opinion.\nThe theoretical results are complemented by some concrete examples and\nnumerical analysis, illustrating the formation of echo-chambers, the\npropagation of chaos, and phenomena such as snowball effect and social\ninertia.42 pages\n"
    },
    {
        "paper_id": 2306.16563,
        "authors": "Aditya Gupta and Vijay K. Tayal",
        "title": "Using Monte Carlo Methods for Retirement Simulations",
        "comments": "Conference Proceedings require that all preprints be withdrawn before\n  the paper is indexed",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Retirement prediction helps individuals and institutions make informed\nfinancial, lifestyle, and workforce decisions based on estimated retirement\nportfolios. This paper attempts to predict retirement using Monte Carlo\nsimulations, allowing one to probabilistically account for a range of\npossibilities. The authors propose a model to predict the values of the\ninvestment accounts IRA and 401(k) through the simulation of inflation rates,\ninterest rates, and other pertinent factors. They provide a user case study to\ndiscuss the implications of the proposed model.\n"
    },
    {
        "paper_id": 2306.16681,
        "authors": "Xin Hai, Gregoire Loeper, Kihun Nam",
        "title": "Data-driven Multiperiod Robust Mean-Variance Optimization",
        "comments": "37 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We study robust mean-variance optimization in multiperiod portfolio selection\nby allowing the true probability measure to be inside a Wasserstein ball\ncentered at the empirical probability measure. Given the confidence level, the\nradius of the Wasserstein ball is determined by the empirical data. The\nnumerical simulations of the US stock market provide a promising result\ncompared to other popular strategies.\n"
    },
    {
        "paper_id": 2306.16871,
        "authors": "Damir Filipovic",
        "title": "Discount Models",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Discount is the difference between the face value of a bond and its present\nvalue. I propose an arbitrage-free dynamic framework for discount models, which\nprovides an alternative to the Heath--Jarrow--Morton framework for forward\nrates. I derive general consistency conditions for factor models, and discuss\naffine term structure models in particular. There are several open problems,\nand I outline possible directions for further research.\n"
    },
    {
        "paper_id": 2306.16904,
        "authors": "Olivier Compte",
        "title": "Endogenous Barriers to Learning",
        "comments": "26 pages, 15 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Motivated by the idea that lack of experience is a source of errors but that\nexperience should reduce them, we model agents' behavior using a stochastic\nchoice model, leaving endogenous the accuracy of their choice. In some games,\nincreased accuracy is conducive to unstable best-response dynamics. We define\nthe barrier to learning as the minimum level of noise which keeps the\nbest-response dynamic stable. Using logit Quantal Response, this defines a\nlimitQR Equilibrium. We apply the concept to centipede, travelers' dilemma, and\n11-20 money-request games and to first-price and all-pay auctions, and discuss\nthe role of strategy restrictions in reducing or amplifying barriers to\nlearning.\n"
    },
    {
        "paper_id": 2306.1696,
        "authors": "Manuel Coelho, Jos\\'e Ant\\'onio Filipe, Manuel Alberto M. Ferreira",
        "title": "Sketching a Model on Fisheries Enforcement and Compliance -- A Survey",
        "comments": "9 pages and no figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Monitoring and enforcement considerations have been largely forgotten in the\nstudy of fishery management. This paper discusses this issue through a model\nformalization to show the impacts of costly, imperfect enforcement of law on\nthe behavior of fishing firms and fisheries management. Theoretical analysis\nmerges a standard bio-economic model of fisheries (Gordon-Schaefer) with Becker\ntheory of Crime and Punishment.\n"
    },
    {
        "paper_id": 2306.16982,
        "authors": "Bingyan Han, Chi Seng Pun, Hoi Ying Wong",
        "title": "Robust Time-inconsistent Linear-Quadratic Stochastic Controls: A\n  Stochastic Differential Game Approach",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper studies robust time-inconsistent (TIC) linear-quadratic stochastic\ncontrol problems, formulated by stochastic differential games. By a spike\nvariation approach, we derive sufficient conditions for achieving the Nash\nequilibrium, which corresponds to a time-consistent (TC) robust policy, under\nmild technical assumptions. To illustrate our framework, we consider two\nscenarios of robust mean-variance analysis, namely with state- and\ncontrol-dependent ambiguity aversion. We find numerically that with time\ninconsistency haunting the dynamic optimal controls, the ambiguity aversion\nenhances the effective risk aversion faster than the linear, implying that the\nambiguity in the TIC cases is more impactful than that under the TC\ncounterparts, e.g., expected utility maximization problems.\n"
    },
    {
        "paper_id": 2306.17095,
        "authors": "Marcin W\\k{a}torek and Maria Skupie\\'n and Jaros{\\l}aw Kwapie\\'n and\n  Stanis{\\l}aw Dro\\.zd\\.z",
        "title": "Decomposing cryptocurrency high-frequency price dynamics into recurring\n  and noisy components",
        "comments": null,
        "journal-ref": "Chaos 33, 083146 (2023)",
        "doi": "10.1063/5.0165635",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper investigates the temporal patterns of activity in the\ncryptocurrency market with a focus on Bitcoin, Ethereum, Dogecoin, and WINkLink\nfrom January 2020 to December 2022. Market activity measures - logarithmic\nreturns, volume, and transaction number, sampled every 10 seconds, were divided\ninto intraday and intraweek periods and then further decomposed into recurring\nand noise components via correlation matrix formalism. The key findings include\nthe distinctive market behavior from traditional stock markets due to the\nnonexistence of trade opening and closing. This was manifest in three\nenhanced-activity phases aligning with Asian, European, and U.S. trading\nsessions. An intriguing pattern of activity surge in 15-minute intervals,\nparticularly at full hours, was also noticed, implying the potential role of\nalgorithmic trading. Most notably, recurring bursts of activity in bitcoin and\nether were identified to coincide with the release times of significant U.S.\nmacroeconomic reports such as Nonfarm payrolls, Consumer Price Index data, and\nFederal Reserve statements. The most correlated daily patterns of activity\noccurred in 2022, possibly reflecting the documented correlations with U.S.\nstock indices in the same period. Factors that are external to the inner market\ndynamics are found to be responsible for the repeatable components of the\nmarket dynamics, while the internal factors appear to be substantially random,\nwhich manifests itself in a good agreement between the empirical eigenvalue\ndistributions in their bulk and the random matrix theory predictions expressed\nby the Marchenko-Pastur distribution. The findings reported support the growing\nintegration of cryptocurrencies into the global financial markets.\n"
    },
    {
        "paper_id": 2306.17178,
        "authors": "Cong Zheng and Jiafa He and Can Yang",
        "title": "Optimal Execution Using Reinforcement Learning",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This work is about optimal order execution, where a large order is split into\nseveral small orders to maximize the implementation shortfall. Based on the\ndiversity of cryptocurrency exchanges, we attempt to extract cross-exchange\nsignals by aligning data from multiple exchanges for the first time. Unlike\nmost previous studies that focused on using single-exchange information, we\ndiscuss the impact of cross-exchange signals on the agent's decision-making in\nthe optimal execution problem. Experimental results show that cross-exchange\nsignals can provide additional information for the optimal execution of\ncryptocurrency to facilitate the optimal execution process.\n"
    },
    {
        "paper_id": 2306.17179,
        "authors": "Jiafa He, Cong Zheng and Can Yang",
        "title": "Integrating Tick-level Data and Periodical Signal for High-frequency\n  Market Making",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We focus on the problem of market making in high-frequency trading. Market\nmaking is a critical function in financial markets that involves providing\nliquidity by buying and selling assets. However, the increasing complexity of\nfinancial markets and the high volume of data generated by tick-level trading\nmakes it challenging to develop effective market making strategies. To address\nthis challenge, we propose a deep reinforcement learning approach that fuses\ntick-level data with periodic prediction signals to develop a more accurate and\nrobust market making strategy. Our results of market making strategies based on\ndifferent deep reinforcement learning algorithms under the simulation scenarios\nand real data experiments in the cryptocurrency markets show that the proposed\nframework outperforms existing methods in terms of profitability and risk\nmanagement.\n"
    },
    {
        "paper_id": 2306.17309,
        "authors": "Sourav Ray, Avichai Snir, and Daniel Levy",
        "title": "Retail Pricing Format and Rigidity of Regular Prices",
        "comments": "Economica (Forthcoming), 102 pages, 59 pages paper, 43 pages\n  supplementary online web appendix",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We study the price rigidity of regular and sale prices, and how it is\naffected by pricing formats (pricing strategies). We use data from three large\nCanadian stores with different pricing formats (Every-Day-Low-Price, Hi-Lo, and\nHybrid) that are located within a 1 km radius of each other. Our data contains\nboth the actual transaction prices and actual regular prices as displayed on\nthe store shelves. We combine these data with two generated regular price\nseries (filtered prices and reference prices) and study their rigidity. Regular\nprice rigidity varies with store formats because different format stores treat\nsale prices differently, and consequently define regular prices differently.\nCorrespondingly, the meanings of price cuts and sale prices vary across store\nformats. To interpret the findings, we consider the store pricing format\ndistribution across the US.\n"
    },
    {
        "paper_id": 2306.17316,
        "authors": "Rithvik Rao, Nihar Shah",
        "title": "Triangle Fees",
        "comments": "7 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
        "abstract": "  Triangle fees are a novel fee structure for AMMs, in which marginal fees are\ndecreasing in a trade's size. That decline is proportional to the movement in\nthe AMM's implied price, i.e. for every basis point the trade moves the ratio\nof assets, the marginal fee declines by a basis point. These fees create\nincentives that protect against price staleness, while still allowing the AMM\nto earn meaningful fee revenue. Triangle fees can strictly improve the Pareto\nfrontier of price accuracy versus losses generated by the status quo of\nconstant fee mechanisms.\n"
    },
    {
        "paper_id": 2306.17341,
        "authors": "David McCune, Erin Martin, Grant Latina, Kaitlyn Simms",
        "title": "A Comparison of Sequential Ranked-Choice Voting and Single Transferable\n  Vote",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The methods of single transferable vote (STV) and sequential ranked-choice\nvoting (RCV) are different methods for electing a set of winners in multiwinner\nelections. STV is a classical voting method that has been widely used\ninternationally for many years. By contrast, sequential RCV has rarely been\nused, and only recently has seen an increase in usage as several cities in Utah\nhave adopted the method to elect city council members. We use Monte Carlo\nsimulations and a large database of real-world ranked-choice elections to\ninvestigate the behavior of sequential RCV by comparing it to STV. Our general\nfinding is that sequential RCV often produces different winner sets than STV.\nFurthermore, sequential RCV is best understood as an excellence-based method\nwhich will not produce proportional results, often at the expense of minority\ninterests.\n"
    },
    {
        "paper_id": 2306.17467,
        "authors": "Fausto Di Biase, Stefano Di Rocco, Alessandra Ortolano, Maurizio\n  Parton",
        "title": "On the Behavior of the Payoff Amounts in Simple Interest Loans in\n  Arbitrage-Free Markets",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  The Consumer Financial Protection Bureau defines the notion of payoff amount\nas the amount that has to be payed at a particular time in order to completely\npay off the debt, in case the lender intends to pay off the loan early, way\nbefore the last installment is due (CFPB 2020). This amount is well-understood\nfor loans at compound interest, but much less so when simple interest is used.\n  Recently, Aretusi and Mari (2018) have proposed a formula for the payoff\namount for loans at simple interest. We assume that the payoff amounts are\nestablished contractually at time zero, whence the requirement that no\narbitrage may arise this way\n  The first goal of this paper is to study this new formula and derive it\nwithin a model of a loan market in which loans are bought and sold at simple\ninterest, interest rates change over time, and no arbitrage opportunities\nexist.\n  The second goal is to show that this formula exhibits a behaviour rather\ndifferent from the one which occurs when compound interest is used. Indeed, we\nshow that, if the installments are constant and if the interest rate is greater\nthan a critical value (which depends on the number of installments), then the\nsequence of the payoff amounts is increasing before a certain critical time,\nand will start decreasing only thereafter. We also show that the critical value\nis decreasing as a function of the number of installments. For two\ninstallments, the critical value is equal to the golden section.\n  The third goal is to introduce a more efficient polynomial notation, which\nencodes a basic tenet of the subject: Each amount of money is embedded in a\ntime position (to wit: The time when it is due). The model of a loan market we\npropose is naturally linked to this new notation.\n"
    },
    {
        "paper_id": 2306.17742,
        "authors": "Basile Caparros, Amit Chaudhary, Olga Klein",
        "title": "Blockchain scaling and liquidity concentration on decentralized\n  exchanges",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  Liquidity providers (LPs) on decentralized exchanges (DEXs) can protect\nthemselves from adverse selection risk by updating their positions more\nfrequently. However, repositioning is costly, because LPs have to pay gas fees\nfor each update. We analyze the causal relation between repositioning and\nliquidity concentration around the market price, using the entry of blockchain\nscaling solutions, Arbitrum and Polygon, as our instruments. Lower gas fees on\nscaling solutions allow LPs to update more frequently than on Ethereum. Our\nresults demonstrate that higher repositioning intensity and precision lead to\ngreater liquidity concentration, which benefits small trades by reducing their\nslippage.\n"
    },
    {
        "paper_id": 2306.1781,
        "authors": "Emily Silcock, Melissa Dell",
        "title": "A Massive Scale Semantic Similarity Dataset of Historical English",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  A diversity of tasks use language models trained on semantic similarity data.\nWhile there are a variety of datasets that capture semantic similarity, they\nare either constructed from modern web data or are relatively small datasets\ncreated in the past decade by human annotators. This study utilizes a novel\nsource, newly digitized articles from off-copyright, local U.S. newspapers, to\nassemble a massive-scale semantic similarity dataset spanning 70 years from\n1920 to 1989 and containing nearly 400M positive semantic similarity pairs.\nHistorically, around half of articles in U.S. local newspapers came from\nnewswires like the Associated Press. While local papers reproduced articles\nfrom the newswire, they wrote their own headlines, which form abstractive\nsummaries of the associated articles. We associate articles and their headlines\nby exploiting document layouts and language understanding. We then use deep\nneural methods to detect which articles are from the same underlying source, in\nthe presence of substantial noise and abridgement. The headlines of reproduced\narticles form positive semantic similarity pairs. The resulting publicly\navailable HEADLINES dataset is significantly larger than most existing semantic\nsimilarity datasets and covers a much longer span of time. It will facilitate\nthe application of contrastively trained semantic similarity models to a\nvariety of tasks, including the study of semantic change across space and time.\n"
    },
    {
        "paper_id": 2307.00251,
        "authors": "Julia Hatamyar, Christopher F. Parmeter",
        "title": "Local Eviction Moratoria and the Spread of COVID-19",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  At various stages during the initial onset of the COVID-19 pandemic, various\nUS states and local municipalities enacted eviction moratoria. One of the main\naims of these moratoria was to slow the spread of COVID-19 infections. We\ndeploy a semiparametric difference-in-differences approach with an event study\nspecification to test whether the lifting of these local moratoria led to an\nincrease in COVID-19 cases and deaths. Our main findings, across a range of\nspecifications, are inconclusive regarding the impact of the moratoria -\nespecially after accounting for the number of actual evictions and conducting\nthe analysis at the county level. We argue that recently developed augmented\nsynthetic control (ASCM) methods are more appropriate in this setting. Our ASCM\nresults also suggest that the lifting of eviction moratoria had little to no\nimpact on COVID-19 cases and deaths. Thus, it seems that eviction moratoria had\nlittle to no robust effect on reducing the spread of COVID-19 throwing into\nquestion its use as a non-pharmaceutical intervention.\n"
    },
    {
        "paper_id": 2307.0041,
        "authors": "Sabiou Inoua, Vernon Smith",
        "title": "A Classical Model of Speculative Asset Price Dynamics",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  In retrospect, the experimental findings on competitive market behavior\ncalled for a revival of the old, classical, view of competition as a collective\nhiggling and bargaining process (as opposed to price-taking behaviors) founded\non reservation prices (in place of the utility function). In this paper, we\nspecialize the classical methodology to deal with speculation, an important\nimpediment to price stability. The model involves typical features of a field\nor lab asset market setup and lends itself to an experimental test of its\nspecific predictions; here we use the model to explain three general stylized\nfacts, well established both empirically and experimentally: the excess,\nfat-tailed, and clustered volatility of speculative asset prices. The fat tails\nemerge in the model from the amplifying nature of speculation, leading to a\nrandom-coefficient autoregressive return process (and power-law tails); the\nvolatility clustering is due to the traders' long memory of news; bubbles are a\npersistent phenomenon in the model, and, assuming the standard lab present\nvalue pattern, the bubble size increases with the proportion of speculators and\ndecreases with the trading horizon.\n"
    },
    {
        "paper_id": 2307.00412,
        "authors": "Sabiou Inoua, Vernon Smith",
        "title": "Adam Smith's Theory of Value: A Reappraisal of Classical Price Discovery",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  The relevance of Adam Smith for understanding human morality and sociality is\nrecognized in the growing interest in his work on moral sentiments among\nscholars of various academic backgrounds. But, paradoxically, Adam Smith's\ntheory of economic value enjoys a less prominent stature today among\neconomists, who, while they view him as the 'father of modern economics',\nconsidered him more as having had the right intuitions about a market economy\nthan as having developed the right concepts and the technical tools for\nstudying it. Yet the neoclassical tradition, which replaced the classical\nschool around 1870, failed to provide a satisfactory theory of market price\nformation. Adam Smith's sketch of market price formation (Ch. VII, Book I,\nWealth of Nations), and more generally the classical view of competition as a\ncollective higgling and bargaining process, as this paper argues, offers a\nhelpful foundation on which to build a modern theory of market price formation,\ndespite any shortcomings of the original classical formulation (notably its\ninsistence on long-run, natural value). Also, with hindsight, the experimental\nmarket findings established the remarkable stability, efficiency, and\nrobustness of the old view of competition, suggesting a rehabilitation of\nclassical price discovery. This paper reappraises classical price theory as\nAdam Smith articulated it; we explicate key propositions from his price theory\nand derive them from a simple model, which is an elementary sketch of the\nauthors' more general theory of competitive market price formation.\n"
    },
    {
        "paper_id": 2307.00459,
        "authors": "Eugene W. Park",
        "title": "Principal Component Analysis and Hidden Markov Model for Forecasting\n  Stock Returns",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  This paper presents a method for predicting stock returns using principal\ncomponent analysis (PCA) and the hidden Markov model (HMM) and tests the\nresults of trading stocks based on this approach. Principal component analysis\nis applied to the covariance matrix of stock returns for companies listed in\nthe S&P 500 index, and interpreting principal components as factor returns, we\napply the HMM model on them. Then we use the transition probability matrix and\nstate conditional means to forecast the factors returns. Reverting the factor\nreturns forecasts to stock returns using eigenvectors, we obtain forecasts for\nthe stock returns. We find that, with the right hyperparameters, our model\nyields a strategy that outperforms the buy-and-hold strategy in terms of the\nannualized Sharpe ratio.\n"
    },
    {
        "paper_id": 2307.00476,
        "authors": "Juan Esteban Berger",
        "title": "Pricing European Options with Google AutoML, TensorFlow, and XGBoost",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Researchers have been using Neural Networks and other related\nmachine-learning techniques to price options since the early 1990s. After three\ndecades of improvements in machine learning techniques, computational\nprocessing power, cloud computing, and data availability, this paper is able to\nprovide a comparison of using Google Cloud's AutoML Regressor, TensorFlow\nNeural Networks, and XGBoost Gradient Boosting Decision Trees for pricing\nEuropean Options. All three types of models were able to outperform the Black\nScholes Model in terms of mean absolute error. These results showcase the\npotential of using historical data from an option's underlying asset for\npricing European options, especially when using machine learning algorithms\nthat learn complex patterns that traditional parametric models do not take into\naccount.\n"
    },
    {
        "paper_id": 2307.00571,
        "authors": "Christoph K\\\"uhn",
        "title": "The fundamental theorem of asset pricing with and without transaction\n  costs",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  We prove a version of the fundamental theorem of asset pricing (FTAP) in\ncontinuous time that is based on the strict no-arbitrage condition and that is\napplicable to both frictionless markets and markets with proportional\ntransaction costs. We consider a market with a single risky asset whose ask\nprice process is higher than or equal to its bid price process. Neither the\nconcatenation property of the set of wealth processes, that is used in the\nproof of the frictionless FTAP, nor some boundedness property of the trading\nvolume of admissible strategies usually argued with in models with a\nnonvanishing bid-ask spread need to be satisfied in our model.\n"
    },
    {
        "paper_id": 2307.00807,
        "authors": "Tongseok Lim",
        "title": "Replication of financial derivatives under extreme market models given\n  marginals",
        "comments": "An alternative title may be \"Dual attainment for the multi-period\n  vectorial martingale optimal transport problem.\"",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  The Black-Scholes-Merton model is a mathematical model for the dynamics of a\nfinancial market that includes derivative investment instruments, and its\nformula provides a theoretical price estimate of European-style options. The\nmodel's fundamental idea is to eliminate risk by hedging the option by\npurchasing and selling the underlying asset in a specific way, that is, to\nreplicate the payoff of the option with a portfolio (which continuously trades\nthe underlying) whose value at each time can be verified. One of the most\ncrucial, yet restrictive, assumptions for this task is that the market follows\na geometric Brownian motion, which has been relaxed and generalized in various\nways.\n  The concept of robust finance revolves around developing models that account\nfor uncertainties and variations in financial markets. Martingale Optimal\nTransport, which is an adaptation of the Optimal Transport theory to the robust\nfinancial framework, is one of the most prominent directions. In this paper, we\nconsider market models with arbitrarily many underlying assets whose values are\nobserved over arbitrarily many time periods, and demonstrates the existence of\na portfolio sub- or super-hedging a general path-dependent derivative security\nin terms of trading European options and underlyings, as well as the portfolio\nreplicating the derivative payoff when the market model yields the extremal\nprice of the derivative given marginal distributions of the underlyings. In\nmathematical terms, this paper resolves the question of dual attainment for the\nmulti-period vectorial martingale optimal transport problem.\n"
    },
    {
        "paper_id": 2307.01085,
        "authors": "Arnau Quera-Bofarull, Joel Dyer, Anisoara Calinescu, Michael\n  Wooldridge",
        "title": "Some challenges of calibrating differentiable agent-based models",
        "comments": "Accepted at the ICML 2023 Differentiable Almost Everything Workshop",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Agent-based models (ABMs) are a promising approach to modelling and reasoning\nabout complex systems, yet their application in practice is impeded by their\ncomplexity, discrete nature, and the difficulty of performing parameter\ninference and optimisation tasks. This in turn has sparked interest in the\nconstruction of differentiable ABMs as a strategy for combatting these\ndifficulties, yet a number of challenges remain. In this paper, we discuss and\npresent experiments that highlight some of these challenges, along with\npotential solutions.\n"
    },
    {
        "paper_id": 2307.01155,
        "authors": "Abha Naik, Esra Yeniaras, Gerhard Hellstern, Grishma Prasad, Sanjay\n  Kumar Lalta Prasad Vishwakarma",
        "title": "From Portfolio Optimization to Quantum Blockchain and Security: A\n  Systematic Review of Quantum Computing in Finance",
        "comments": "64 pages. arXiv admin note: text overlap with arXiv:2211.13191 by\n  other authors",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  In this paper, we provide an overview of the recent work in the quantum\nfinance realm from various perspectives. The applications in consideration are\nPortfolio Optimization, Fraud Detection, and Monte Carlo methods for derivative\npricing and risk calculation. Furthermore, we give a comprehensive overview of\nthe applications of quantum computing in the field of blockchain technology\nwhich is a main concept in fintech. In that sense, we first introduce the\ngeneral overview of blockchain with its main cryptographic primitives such as\ndigital signature algorithms, hash functions, and random number generators as\nwell as the security vulnerabilities of blockchain technologies after the merge\nof quantum computers considering Shor's quantum factoring and Grover's quantum\nsearch algorithms. We then discuss the privacy preserving quantum-resistant\nblockchain systems via threshold signatures, ring signatures, and\nzero-knowledge proof systems i.e. ZK-SNARKs in quantum resistant blockchains.\nAfter emphasizing the difference between the quantum-resistant blockchain and\nquantum-safe blockchain we mention the security countermeasures to take against\nthe possible quantumized attacks aiming these systems. We finalize our\ndiscussion with quantum blockchain, efficient quantum mining and necessary\ninfrastructures for constructing such systems based on quantum computing. This\nreview has the intention to be a bridge to fill the gap between quantum\ncomputing and one of its most prominent application realms: Finance. We provide\nthe state-of-the-art results in the intersection of finance and quantum\ntechnology for both industrial practitioners and academicians.\n"
    },
    {
        "paper_id": 2307.01319,
        "authors": "Marcel Nutz, Andr\\'es Riveros Valdevenito",
        "title": "On the Guyon-Lekeufack Volatility Model",
        "comments": "To appear in 'Finance&Stochastics'",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Guyon and Lekeufack recently proposed a path-dependent volatility model and\ndocumented its excellent performance in fitting market data and capturing\nstylized facts. The instantaneous volatility is modeled as a linear combination\nof two processes, one is an integral of weighted past price returns and the\nother is the square-root of an integral of weighted past squared volatility.\nEach of the weightings is built using two exponential kernels reflecting long\nand short memory. Mathematically, the model is a coupled system of four\nstochastic differential equations. Our main result is the wellposedness of this\nsystem: the model has a unique strong (non-explosive) solution for all\nparameter values. We also study the positivity of the resulting volatility\nprocess and the martingale property of the associated exponential price\nprocess.\n"
    },
    {
        "paper_id": 2307.01404,
        "authors": "Deepthi Kolady, Amrit Dumre, Weiwei Zhang, Kaiqun Fu, Marcia O'Leary,\n  Laura Rose",
        "title": "Social media use among American Indians in South Dakota: Preferences and\n  perceptions",
        "comments": "20 pages, 6 figures, 2 Tables, Appendix Tables (7)",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Social media use data is widely being used in health, psychology, and\nmarketing research to analyze human behavior. However, we have very limited\nknowledge on social media use among American Indians. In this context, this\nstudy was designed to assess preferences and perceptions of social media use\namong American Indians during COVID-19. We collected data from American Indians\nin South Dakota using online survey. Results show that Facebook, YouTube,\nTikTok, Instagram and Snapchat are the most preferred social media platforms.\nMost of the participants reported that the use of social media increased\ntremendously during COVID-19 and had perceptions of more negative effects than\npositive effects. Hate/harassment/extremism, misinformation/made up news, and\npeople getting one point of view were the top reasons for negative effects.\n"
    },
    {
        "paper_id": 2307.01443,
        "authors": "John Bistline, Geoffrey Blanford, Maxwell Brown, Dallas Burtraw, Maya\n  Domeshek, Jamil Farbes, Allen Fawcett, Anne Hamilton, Jesse Jenkins, Ryan\n  Jones, Ben King, Hannah Kolus, John Larsen, Amanda Levin, Megan Mahajan, Cara\n  Marcy, Erin Mayfield, James McFarland, Haewon McJeon, Robbie Orvis, Neha\n  Patankar, Kevin Rennert, Christopher Roney, Nicholas Roy, Greg Schivley,\n  Daniel Steinberg, Nadejda Victor, Shelley Wenzel, John Weyant, Ryan Wiser,\n  Mei Yuan, Alicia Zhao",
        "title": "Emissions and Energy Impacts of the Inflation Reduction Act",
        "comments": null,
        "journal-ref": "Science, 380(6652): 1324-1327 (2023)",
        "doi": "10.1126/science.adg3781",
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  If goals set under the Paris Agreement are met, the world may hold warming\nwell below 2 C; however, parties are not on track to deliver these commitments,\nincreasing focus on policy implementation to close the gap between ambition and\naction. Recently, the US government passed its most prominent piece of climate\nlegislation to date, the Inflation Reduction Act of 2022 (IRA), designed to\ninvest in a wide range of programs that, among other provisions, incentivize\nclean energy and carbon management, encourage electrification and efficiency\nmeasures, reduce methane emissions, promote domestic supply chains, and address\nenvironmental justice concerns. IRA's scope and complexity make modeling\nimportant to understand impacts on emissions and energy systems. We leverage\nresults from nine independent, state-of-the-art models to examine potential\nimplications of key IRA provisions, showing economy wide emissions reductions\nbetween 43-48% below 2005 by 2035.\n"
    },
    {
        "paper_id": 2307.01599,
        "authors": "Zhenhan Huang and Fumihide Tanaka",
        "title": "A Scalable Reinforcement Learning-based System Using On-Chain Data for\n  Cryptocurrency Portfolio Management",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  On-chain data (metrics) of blockchain networks, akin to company fundamentals,\nprovide crucial and comprehensive insights into the networks. Despite their\ninformative nature, on-chain data have not been utilized in reinforcement\nlearning (RL)-based systems for cryptocurrency (crypto) portfolio management\n(PM). An intriguing subject is the extent to which the utilization of on-chain\ndata can enhance an RL-based system's return performance compared to baselines.\nTherefore, in this study, we propose CryptoRLPM, a novel RL-based system\nincorporating on-chain data for end-to-end crypto PM. CryptoRLPM consists of\nfive units, spanning from information comprehension to trading order execution.\nIn CryptoRLPM, the on-chain data are tested and specified for each crypto to\nsolve the issue of ineffectiveness of metrics. Moreover, the scalable nature of\nCryptoRLPM allows changes in the portfolios' cryptos at any time. Backtesting\nresults on three portfolios indicate that CryptoRLPM outperforms all the\nbaselines in terms of accumulated rate of return (ARR), daily rate of return\n(DRR), and Sortino ratio (SR). Particularly, when compared to Bitcoin,\nCryptoRLPM enhances the ARR, DRR, and SR by at least 83.14%, 0.5603%, and\n2.1767 respectively.\n"
    },
    {
        "paper_id": 2307.01719,
        "authors": "Yong Zheng, Kumar Neelotpal Shukla, Jasmine Xu, David (Xuejun) Wang,\n  Michael O'Leary",
        "title": "MOPO-LSI: A User Guide",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  MOPO-LSI is an open-source Multi-Objective Portfolio Optimization Library for\nSustainable Investments. This document provides a user guide for MOPO-LSI\nversion 1.0, including problem setup, workflow and the hyper-parameters in\nconfigurations.\n"
    },
    {
        "paper_id": 2307.01779,
        "authors": "Giuseppe Cavaliere, Thomas Mikosch, Anders Rahbek and Frederik Vilandt",
        "title": "Asymptotics for the Generalized Autoregressive Conditional Duration\n  Model",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  Engle and Russell (1998, Econometrica, 66:1127--1162) apply results from the\nGARCH literature to prove consistency and asymptotic normality of the\n(exponential) QMLE for the generalized autoregressive conditional duration\n(ACD) model, the so-called ACD(1,1), under the assumption of strict\nstationarity and ergodicity. The GARCH results, however, do not account for the\nfact that the number of durations over a given observation period is random.\nThus, in contrast with Engle and Russell (1998), we show that strict\nstationarity and ergodicity alone are not sufficient for consistency and\nasymptotic normality, and provide additional sufficient conditions to account\nfor the random number of durations. In particular, we argue that the durations\nneed to satisfy the stronger requirement that they have finite mean.\n"
    },
    {
        "paper_id": 2307.01814,
        "authors": "Zhou Fang, Haiqing Xu",
        "title": "Market Making of Options via Reinforcement Learning",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Market making of options with different maturities and strikes is a\nchallenging problem due to its high dimensional nature. In this paper, we\npropose a novel approach that combines a stochastic policy and reinforcement\nlearning-inspired techniques to determine the optimal policy for posting\nbid-ask spreads for an options market maker who trades options with different\nmaturities and strikes. When the arrival of market orders is linearly inverse\nto the spreads, the optimal policy is normally distributed.\n"
    },
    {
        "paper_id": 2307.01816,
        "authors": "Zhou Fang, Haiqing Xu",
        "title": "Over-the-Counter Market Making via Reinforcement Learning",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The over-the-counter (OTC) market is characterized by a unique feature that\nallows market makers to adjust bid-ask spreads based on order size. However,\nthis flexibility introduces complexity, transforming the market-making problem\ninto a high-dimensional stochastic control problem that presents significant\nchallenges. To address this, this paper proposes an innovative solution\nutilizing reinforcement learning techniques to tackle the OTC market-making\nproblem. By assuming a linear inverse relationship between market order arrival\nintensity and bid-ask spreads, we demonstrate the optimal policy for bid-ask\nspreads follows a Gaussian distribution. We apply two reinforcement learning\nalgorithms to conduct a numerical analysis, revealing the resulting return\ndistribution and bid-ask spreads under different time and inventory levels.\n"
    },
    {
        "paper_id": 2307.01986,
        "authors": "Qian Lei and Chi Seng Pun",
        "title": "On the Well-posedness of Hamilton-Jacobi-Bellman Equations of the\n  Equilibrium Type",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper studies the well-posedness of a class of nonlocal parabolic\npartial differential equations (PDEs), or equivalently equilibrium\nHamilton-Jacobi-Bellman equations, which has a strong tie with the\ncharacterization of the equilibrium strategies and the associated value\nfunctions for time-inconsistent stochastic control problems. Specifically, we\nconsider nonlocality in both time and space, which allows for modelling of the\nstochastic control problems with initial-time-and-state dependent objective\nfunctionals. We leverage the method of continuity to show the global\nwell-posedness within our proposed Banach space with our established Schauder\nprior estimate for the linearized nonlocal PDE. Then, we adopt a linearization\nmethod and Banach's fixed point arguments to show the local well-posedness of\nthe nonlocal fully nonlinear case, while the global well-posedness is\nattainable provided that a very sharp a-priori estimate is available. On top of\nthe well-posedness results, we also provide a probabilistic representation of\nthe solutions to the nonlocal fully nonlinear PDEs and an estimate on the\ndifference between the value functions of sophisticated and na\\\"{i}ve\ncontrollers. Finally, we give a financial example of time inconsistency that is\nproven to be globally solvable.\n"
    },
    {
        "paper_id": 2307.02154,
        "authors": "Cees Diks, Bram Wouters",
        "title": "Noise reduction for functional time series",
        "comments": "48 pages; working paper",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  A novel method for noise reduction in the setting of curve time series with\nerror contamination is proposed, based on extending the framework of functional\nprincipal component analysis (FPCA). We employ the underlying,\nfinite-dimensional dynamics of the functional time series to separate the\nserially dependent dynamical part of the observed curves from the noise. Upon\nidentifying the subspaces of the signal and idiosyncratic components, we\nconstruct a projection of the observed curve time series along the noise\nsubspace, resulting in an estimate of the underlying denoised curves. This\nprojection is optimal in the sense that it minimizes the mean integrated\nsquared error. By applying our method to similated and real data, we show the\ndenoising estimator is consistent and outperforms existing denoising\ntechniques. Furthermore, we show it can be used as a pre-processing step to\nimprove forecasting.\n"
    },
    {
        "paper_id": 2307.02178,
        "authors": "Shuaijie Qian, Chen Yang",
        "title": "Non-Concave Utility Maximization with Transaction Costs",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper studies a finite-horizon portfolio selection problem with\nnon-concave terminal utility and proportional transaction costs. The commonly\nused concavification principle for terminal value is no longer valid here, and\nwe establish a proper theoretical characterization of this problem. We first\ngive the asymptotic terminal behavior of the value function, which implies any\ntransaction close to maturity only provides a marginal contribution to the\nutility. After that, the theoretical foundation is established in terms of a\nnovel definition of the viscosity solution incorporating our asymptotic\nterminal condition. Via numerical analyses, we find that the introduction of\ntransaction costs into non-concave utility maximization problems can prevent\nthe portfolio from unbounded leverage and make a large short position in stock\noptimal despite a positive risk premium and symmetric transaction costs.\n"
    },
    {
        "paper_id": 2307.0231,
        "authors": "Yannick Limmer and Blanka Horvath",
        "title": "Robust Hedging GANs",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  The availability of deep hedging has opened new horizons for solving hedging\nproblems under a large variety of realistic market conditions. At the same\ntime, any model - be it a traditional stochastic model or a market generator -\nis at best an approximation of market reality, prone to model-misspecification\nand estimation errors. This raises the question, how to furnish a modelling\nsetup with tools that can address the risk of discrepancy between anticipated\ndistribution and market reality, in an automated way. Automated robustification\nis currently attracting increased attention in numerous investment problems,\nbut it is a delicate task due to its imminent implications on risk management.\nHence, it is beyond doubt that more activity can be anticipated on this topic\nto converge towards a consensus on best practices.\n  This paper presents a natural extension of the original deep hedging\nframework to address uncertainty in the data generating process via an\nadversarial approach inspired by GANs to automate robustification in our\nhedging objective. This is achieved through an interplay of three modular\ncomponents: (i) a (deep) hedging engine, (ii) a data-generating process (that\nis model agnostic permitting a large variety of classical models as well as\nmachine learning-based market generators), and (iii) a notion of distance on\nmodel space to measure deviations between our market prognosis and reality. We\ndo not restrict the ambiguity set to a region around a reference model, but\ninstead penalize deviations from the anticipated distribution. Our suggested\nchoice for each component is motivated by model agnosticism, allowing a\nseamless transition between settings. Since all individual components are\nalready used in practice, we believe that our framework is easily adaptable to\nexisting functional settings.\n"
    },
    {
        "paper_id": 2307.02375,
        "authors": "Ioanna-Yvonni Tsaknaki, Fabrizio Lillo, Piero Mazzarisi",
        "title": "Online Learning of Order Flow and Market Impact with Bayesian\n  Change-Point Detection Methods",
        "comments": "25 pages, 10 figures, 4 tables. This paper is funded by European\n  Union Next Generation EU with the grant PNRR IR0000013 SoBigData.it",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Financial order flow exhibits a remarkable level of persistence, wherein buy\n(sell) trades are often followed by subsequent buy (sell) trades over extended\nperiods. This persistence can be attributed to the division and gradual\nexecution of large orders. Consequently, distinct order flow regimes might\nemerge, which can be identified through suitable time series models applied to\nmarket data. In this paper, we propose the use of Bayesian online change-point\ndetection (BOCPD) methods to identify regime shifts in real-time and enable\nonline predictions of order flow and market impact. To enhance the\neffectiveness of our approach, we have developed a novel BOCPD method using a\nscore-driven approach. This method accommodates temporal correlations and\ntime-varying parameters within each regime. Through empirical application to\nNASDAQ data, we have found that: (i) Our newly proposed model demonstrates\nsuperior out-of-sample predictive performance compared to existing models that\nassume i.i.d. behavior within each regime; (ii) When examining the residuals,\nour model demonstrates good specification in terms of both distributional\nassumptions and temporal correlations; (iii) Within a given regime, the price\ndynamics exhibit a concave relationship with respect to time and volume,\nmirroring the characteristics of actual large orders; (iv) By incorporating\nregime information, our model produces more accurate online predictions of\norder flow and market impact compared to models that do not consider regimes.\n"
    },
    {
        "paper_id": 2307.02455,
        "authors": "Shun Zhang",
        "title": "Policy Expectation Counts? The Impact of China's Delayed Retirement\n  Announcement on Urban Households Savings Rates",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This article examines the impact of China's delayed retirement announcement\non households' savings behavior using data from China Family Panel Studies\n(CFPS). The article finds that treated households, on average, experience an 8%\nincrease in savings rates as a result of the policy announcement. This\nestimation is both significant and robust. Different types of households\nexhibit varying degrees of responsiveness to the policy announcement, with\nhigher-income households showing a greater impact. The increase in household\nsavings can be attributed to negative perceptions about future pension income.\n"
    },
    {
        "paper_id": 2307.0247,
        "authors": "Victor M. Yakovenko",
        "title": "Statistical Physics Perspective on Economic Inequality",
        "comments": "13 pages, 7 figures, an invited chapter for the Handbook of\n  Complexity Economics to be published by Routledge in 2023",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  This article is a supplement to my main contribution to the Routledge\nHandbook of Complexity Economics (2023). On the basis of three recent papers,\nit presents an unconventional perspective on economic inequality from a\nstatistical physics point of view. One section demonstrates empirical evidence\nfor the exponential distribution of income in 67 countries around the world.\nThe exponential distribution was not familiar to mainstream economists until it\nwas introduced by physicists by analogy with the Boltzmann-Gibbs distribution\nof energy and subsequently confirmed in empirical data for many countries.\nAnother section reviews the two-class structure of income distribution in the\nUSA. While the exponential law describes the majority of population (the lower\nclass), the top tail of income distribution (the upper class) is characterized\nby the Pareto power law, and there is no clearly defined middle class in\nbetween. As a result, the whole distribution can be very well fitted by using\nonly three parameters. Historical evolution of these parameters and inequality\ntrends are analyzed from 1983 to 2018. Finally, global inequality in energy\nconsumption and CO2 emissions per capita is studied using the empirical data\nfrom 1980 to 2017. Global inequality, as measured by the Gini coefficient G,\nhas been decreasing until around 2010, but then saturated at the level G=0.5.\nThe saturation at this level was theoretically predicted on the basis of the\nmaximal entropy principle, well before the slowdown of the global inequality\ndecrease became visible in the data. This effect is attributed to accelerated\nmixing of the world economy due to globalization, which brings it to the state\nof maximal entropy and thus results in global economic stagnation. This\nobservation has profound consequences for social and geopolitical stability and\nthe efforts to deal with the climate change.\n"
    },
    {
        "paper_id": 2307.02512,
        "authors": "Hsin-Lun Li",
        "title": "Application of the Deffuant model in money exchange",
        "comments": "5 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  A money transfer involves a buyer and a seller. A buyer buys goods or\nservices from a seller. The money the buyer decreases is the same as that the\nseller increases. At each time step, a pair of socially connected agents are\nselected and transact in agreed money. We evolve the Deffuant model to a money\nexchange system and study circumstances under which asymptotic stability holds,\nor equal wealth can be achieved.\n"
    },
    {
        "paper_id": 2307.02582,
        "authors": "Xiyue Han and Alexander Schied",
        "title": "Estimating the roughness exponent of stochastic volatility from discrete\n  observations of the realized variance",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We consider the problem of estimating the roughness of the volatility in a\nstochastic volatility model that arises as a nonlinear function of fractional\nBrownian motion with drift. To this end, we introduce a new estimator that\nmeasures the so-called roughness exponent of a continuous trajectory, based on\ndiscrete observations of its antiderivative. We provide conditions on the\nunderlying trajectory under which our estimator converges in a strictly\npathwise sense. Then we verify that these conditions are satisfied by almost\nevery sample path of fractional Brownian motion (with drift). As a consequence,\nwe obtain strong consistency theorems in the context of a large class of rough\nvolatility models. Numerical simulations show that our estimation procedure\nperforms well after passing to a scale-invariant modification of our estimator.\n"
    },
    {
        "paper_id": 2307.02627,
        "authors": "Jacqueline Harding",
        "title": "Proxy Selection in Transitive Proxy Voting",
        "comments": null,
        "journal-ref": "Social Choice and Welfare 58, 69-99 (2022)",
        "doi": "10.1007/s00355-021-01345-8",
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  Transitive proxy voting (or \"liquid democracy\") is a novel form of collective\ndecision making, often framed as an attractive hybrid of direct and\nrepresentative democracy. Although the ideas behind liquid democracy have\ngarnered widespread support, there have been relatively few attempts to model\nit formally. This paper makes three main contributions. First, it proposes a\nnew social choice-theoretic model of liquid democracy, which is distinguished\nby taking a richer formal perspective on the process by which a voter chooses a\nproxy. Second, it examines the model from an axiomatic perspective, proving (a)\na proxy vote analogue of May's Theorem and (b) an impossibility result\nconcerning monotonicity properties in a proxy vote setting. Third, it explores\nthe topic of manipulation in transitive proxy votes. Two forms of manipulation\nspecific to the proxy vote setting are defined, and it is shown that\nmanipulation occurs in strictly more cases in proxy votes than in classical\nvotes.\n"
    },
    {
        "paper_id": 2307.02713,
        "authors": "Aziz Guergachi and Javid Hakim",
        "title": "A Simple Linear Algebraic Approach to Capture the Dynamics of the\n  Circular Flow of Income",
        "comments": "This article presents only the model and its equations. The\n  implications of this model, its applications and methods to make effective\n  use of it will be published in separate papers",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  This article has one single purpose: introduce a new and simple, yet highly\ninsightful approach to capture, fully and quantitatively, the dynamics of the\ncircular flow of income in economies. The proposed approach relies mostly on\nbasic linear algebraic concepts and has deep implications for the disciplines\nof economics, physics and econophysics.\n"
    },
    {
        "paper_id": 2307.02918,
        "authors": "Gast\\'on P. Fern\\'andez",
        "title": "Does personality affect the allocation of resources within households?",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  This paper examines whether personality influences the allocation of\nresources within households. To do so, I model households as couples who make\nPareto-efficient allocations and divide resources according to a distribution\nfunction. Using a sample of Dutch couples from the LISS survey with detailed\ninformation on consumption, labor supply, and personality traits at the\nindividual level, I find that personality affects intrahousehold allocations\nthrough two channels. Firstly, the level of these traits acts as preference\nfactors that shape individual tastes for consumed goods and leisure time.\nSecondly, by testing distribution factor proportionality and the exclusion\nrestriction of a conditional demand system, I observe that differences in\npersonality between spouses act as distribution factors. Specifically, these\ndifferences in personality impact the allocation of resources by affecting the\nbargaining process within households. For example, women who are relatively\nmore conscientious, have higher self-esteem, and engage more cognitively than\ntheir male partners receive a larger share of intrafamily resources.\n"
    },
    {
        "paper_id": 2307.0309,
        "authors": "Francesco Della Corte, Gian Paolo Clemente, Nino Savelli",
        "title": "A cohort-based Partial Internal Model for demographic risk",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  We investigate the quantification of demographic risk in a framework\nconsistent with the market-consistent valuation imposed by Solvency II. We\nprovide compact formulas for evaluating inflows and outflows of a portfolio of\ninsurance policies based on a cohort approach. In this context, we maintain the\nhighest level of generality in order to consider both traditional policies and\nequity-linked policies: therefore, we propose a market-consistent valuation of\nthe liabilities. In the second step we evaluate the Solvency Capital\nRequirement of the idiosyncratic risk, linked to accidental mortality, and the\nsystematic risk one, also known as trend risk, proposing a formal closed\nformula for the former and an algorithm for the latter. We show that accidental\nvolatility depends on the intrinsic characteristics of the policies of the\ncohort (Sums-at-Risk), on the age of the policyholders and on the variability\nof the sums insured; trend risk depends both on accidental volatility and on\nthe longevity forecasting model used.\n"
    },
    {
        "paper_id": 2307.03391,
        "authors": "Chi-Lin Li and Chung-Han Hsieh",
        "title": "On Unified Adaptive Portfolio Management",
        "comments": "22 pages, 7 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper introduces a unified framework for adaptive portfolio management,\nintegrating dynamic Black-Litterman (BL) optimization with the general factor\nmodel, Elastic Net regression, and mean-variance portfolio optimization, which\nallows us to generate investors views and mitigate potential estimation errors\nsystematically. Specifically, we propose an innovative dynamic sliding window\nalgorithm to respond to the constantly changing market conditions. This\nalgorithm allows for the flexible window size adjustment based on market\nvolatility, generating robust estimates for factor modeling, time-varying BL\nestimations, and optimal portfolio weights. Through extensive ten-year\nempirical studies using the top 100 capitalized assets in the S&P 500 index,\naccounting for turnover transaction costs, we demonstrate that this combined\napproach leads to computational advantages and promising trading performances.\n"
    },
    {
        "paper_id": 2307.03447,
        "authors": "Roger J. A. Laeven, Emanuela Rosazza Gianin, Marco Zullino",
        "title": "Dynamic Return and Star-Shaped Risk Measures via BSDEs",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper establishes characterization results for dynamic return and\nstar-shaped risk measures induced via backward stochastic differential\nequations (BSDEs). We first characterize a general family of static star-shaped\nfunctionals in a locally convex Fr\\'echet lattice. Next, employing the\nPasch-Hausdorff envelope, we build a suitable family of convex drivers of BSDEs\ninducing a corresponding family of dynamic convex risk measures of which the\ndynamic return and star-shaped risk measures emerge as the essential minimum.\nFurthermore, we prove that if the set of star-shaped supersolutions of a BSDE\nis not empty, then there exists, for each terminal condition, at least one\nconvex BSDE with a non-empty set of supersolutions, yielding the minimal\nstar-shaped supersolution. We illustrate our theoretical results in a few\nexamples and demonstrate their usefulness in two applications, to capital\nallocation and portfolio choice.\n"
    },
    {
        "paper_id": 2307.03499,
        "authors": "\\'Alvaro Cartea, Fay\\c{c}al Drissi, Marcello Monga",
        "title": "Decentralised Finance and Automated Market Making: Execution and\n  Speculation",
        "comments": "Paper in [SSRN 4144743]",
        "journal-ref": null,
        "doi": "10.2139/ssrn.4144743",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Automated market makers (AMMs) are a new prototype of decentralised exchanges\nwhich are revolutionising market interactions. The majority of AMMs are\nconstant product markets (CPMs) where exchange rates are set by a trading\nfunction. This work studies optimal trading and statistical arbitrage in CPMs\nwhere balancing exchange rate risk and execution costs is key. Empirical\nevidence shows that execution costs are accurately estimated by the convexity\nof the trading function. These convexity costs are linear in the trade size and\nare nonlinear in the depth of liquidity and in the exchange rate. We develop\nmodels for when exchange rates form in a competing centralised exchange, in a\nCPM, or in both venues. Finally, we derive computationally efficient strategies\nthat account for stochastic convexity costs and we showcase their out-of-sample\nperformance.\n"
    },
    {
        "paper_id": 2307.03693,
        "authors": "Jiong Liu, M. Dashti Moghaddam, R. A. Serota",
        "title": "Are there Dragon Kings in the Stock Market?",
        "comments": "20 pages, 15 figues",
        "journal-ref": "Foundations 2024, 4(1), 91-113",
        "doi": "10.3390/foundations4010008",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We undertake a systematic study of historic market volatility spanning\nroughly five preceding decades. We focus specifically on the time series of\nrealized volatility (RV) of the S&P500 index and its distribution function. As\nexpected, the largest values of RV coincide with the largest economic upheavals\nof the period: Savings and Loan Crisis, Tech Bubble, Financial Crisis and Covid\nPandemic. We address the question of whether these values belong to one of the\nthree categories: Black Swans (BS), that is they lie on scale-free, power-law\ntails of the distribution; Dragon Kings (DK), defined as statistically\nsignificant upward deviations from BS; or Negative Dragons Kings (nDK), defined\nas statistically significant downward deviations from BS. In analyzing the\ntails of the distribution with RV > 40, we observe the appearance of\n\"potential\" DK which eventually terminate in an abrupt plunge to nDK. This\nphenomenon becomes more pronounced with the increase of the number of days over\nwhich the average RV is calculated -- here from daily, n=1, to \"monthly,\" n=21.\nWe fit the entire distribution with a modified Generalized Beta (mGB)\ndistribution function, which terminates at a finite value of the variable but\nexhibits a long power-law stretch prior to that, as well as Generalized Beta\nPrime (GB2) distribution function, which has a power-law tail. We also fit the\ntails directly with a straight line on a log-log scale. In order to ascertain\nBS, DK or nDK behavior, all fits include their confidence intervals and\np-values are evaluated for the data points to check if they can come from the\nrespective distributions.\n"
    },
    {
        "paper_id": 2307.03808,
        "authors": "Aditya Ramji, Hanif Tayarani",
        "title": "A Regional Analysis of Electric LDV Portfolio Choices by Vehicle\n  Manufacturers",
        "comments": "12 pages, 18 figures, 2 tables",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Global light duty electric vehicle (EV) sales exceeded 10.5 million units in\n2022, with a year-on-year growth of 55%, but these trends differ regionally.\nDespite the robust growth, upfront purchase price remains a challenge for\nconsumers in different regions, and thus, OEMs make technology choices to\nrespond to market needs. This paper examines the electrification portfolio\nchoices of three major automotive manufacturers (OEMs) in different regions of\nthe world, including Europe, Americas, Asia Pacific, and Africa/Middle-East.\nThe analysis focuses on trends in dominant segments for Battery Electric\nVehicles (BEV) and Plug-in Hybrid Electric Vehicles (PHEV), as well as battery\nchemistry choices. Regional differences show a trend towards SUVs for both BEVs\nand PHEVs. Tesla's dominance in the BEV market influences battery chemistry\nchoices. Average battery sizes for BEVs remain similar in Europe and Americas,\nbut lower in Asia Pacific and Africa/Middle East.\n"
    },
    {
        "paper_id": 2307.03927,
        "authors": "Michael Multerer, Paul Schneider, Rohan Sen",
        "title": "Fast Empirical Scenarios",
        "comments": "22 pages, 7 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We seek to extract a small number of representative scenarios from large and\nhigh-dimensional panel data that are consistent with sample moments. Among two\nnovel algorithms, the first identifies scenarios that have not been observed\nbefore, and comes with a scenario-based representation of covariance matrices.\nThe second proposal picks important data points from states of the world that\nhave already realized, and are consistent with higher-order sample moment\ninformation. Both algorithms are efficient to compute, and lend themselves to\nconsistent scenario-based modeling and high-dimensional numerical integration.\nExtensive numerical benchmarking studies and an application in portfolio\noptimization favor the proposed algorithms.\n"
    },
    {
        "paper_id": 2307.03935,
        "authors": "Colin Chan",
        "title": "dYdX: Liquidity Providers' Incentive Programme Review",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Liquidity providers are currently incentivised to provide liquidity through\nthe LP Incentives Programme on dYdX. Based on the various parameters -\nmakerVolume, depths and spreads, they are rewarded accordingly based on their\nactivities. Given the maturity of the BTC and ETH markets, alongside other\naltcoins which enjoy a consistent amount of liquidity, this paper aims to\nupdate the formula to encourage more active and efficient liquidity, improving\nthe overall trading experience. In this research, I begin by providing a basic\nunderstanding of spread management, before introducing the methodology with the\nvarious metrics and conditions. This includes gathering orderbooks on a minute\ninterval and reconstructing the depths based on historical trades to establish\nan upper bound. I end off by providing recommendations to update the maxSpread\nparameter and alternative mechanisms/solutions to improve the existing market\nstructures.\n"
    },
    {
        "paper_id": 2307.04045,
        "authors": "Alexander Nikiporenko",
        "title": "Time-limited Metaheuristics for Cardinality-constrained Portfolio\n  Optimisation",
        "comments": "51 pages, 8 tables, 3 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  A financial portfolio contains assets that offer a return with a certain\nlevel of risk. To maximise returns or minimise risk, the portfolio must be\noptimised - the ideal combination of optimal quantities of assets must be\nfound. The number of possible combinations is vast. Furthermore, to make the\nproblem realistic, constraints can be imposed on the number of assets held in\nthe portfolio and the maximum proportion of the portfolio that can be allocated\nto an asset. This problem is unsolvable using quadratic programming, which\nmeans that the optimal solution cannot be calculated. A group of algorithms,\ncalled metaheuristics, can find near-optimal solutions in a practical computing\ntime. These algorithms have been successfully used in constrained portfolio\noptimisation. However, in past studies the computation time of metaheuristics\nis not limited, which means that the results differ in both quality and\ncomputation time, and cannot be easily compared. This study proposes a\ndifferent way of testing metaheuristics, limiting their computation time to a\ncertain duration, yielding results that differ only in quality. Given that in\nsome use cases the priority is the quality of the solution and in others the\nspeed, time limits of 1, 5 and 25 seconds were tested. Three metaheuristics -\nsimulated annealing, tabu search, and genetic algorithm - were evaluated on\nfive sets of historical market data with different numbers of assets. Although\nthe metaheuristics could not find a competitive solution in 1 second, simulated\nannealing found a near-optimal solution in 5 seconds in all but one dataset.\nThe lowest quality solutions were obtained by genetic algorithm.\n"
    },
    {
        "paper_id": 2307.04059,
        "authors": "Nancy Asare Nyarko, Bhathiya Divelgama, Jagdish Gnawali, Blessing\n  Omotade, Svetlozar Rachev, Peter Yegon",
        "title": "Exploring Dynamic Asset Pricing within Bachelier Market Model",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  This paper delves into the dynamics of asset pricing within Bachelier market\nmodel, elucidating the representation of risky asset price dynamics and the\ndefinition of riskless assets.\n"
    },
    {
        "paper_id": 2307.0414,
        "authors": "Kostyantyn Anatolievich Malyshenko, Majid Mohammad Shafiee, Vadim\n  Anatolievich Malyshenko and Marina Viktorovna Anashkina",
        "title": "Dynamics of the securities market in the information asymmetry context:\n  developing a methodology for emerging securities markets",
        "comments": "lobal Business and Economics Review, Vol. 25, No. 2, 2021",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  Developing a system of indicators that reflects the degree to which the\nsecurities market fulfils its key functions, is essential to assess the level\nof its development. In the conditions of asymmetric information it can also\nprovide effective policies for securities market development. This paper is\naimed to develop a set of indicators to assess the securities market\nperformance, especially in the asymmetric information context. To this goal, we\nselected the Russian securities market as a case of asymmetric information\ncontext, in comparison with other post-Soviet countries, to investigate its\nsuccess and failure in fulfilling its key functions. Regarding this, we\ndeveloped research hypotheses and we conducted a normative research method,\nbased on an ideal model of market functioning that is used as a criterion for\ntesting the hypotheses. The results offer an original scale for assessing the\nperformance of securities market of its functions. The findings also help\npractitioners with effective policy making in securities market regulation and\nits development toward its ideal state. The key contribution of this research\nis in developing a new scale for determining the performance and efficiency of\nthe securities market, based on the conditions of information asymmetry.\n"
    },
    {
        "paper_id": 2307.0451,
        "authors": "Christian Yeo",
        "title": "An analysis of least squares regression and neural networks\n  approximation for the pricing of swing options",
        "comments": "35 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Least Squares regression was first introduced for the pricing of\nAmerican-style options, but it has since been expanded to include swing options\npricing. The swing options price may be viewed as a solution to a Backward\nDynamic Programming Principle, which involves a conditional expectation known\nas the continuation value. The approximation of the continuation value using\nleast squares regression involves two levels of approximation. First, the\ncontinuation value is replaced by an orthogonal projection over a subspace\nspanned by a finite set of $m$ squared-integrable functions (regression\nfunctions) yielding a first approximation $V^m$ of the swing value function. In\nthis paper, we prove that, with well-chosen regression functions, $V^m$\nconverges to the swing actual price $V$ as $m \\to + \\infty$. A similar result\nis proved when the regression functions are replaced by neural networks. For\nboth methods (least squares or neural networks), we analyze the second level of\napproximation involving practical computation of the swing price using Monte\nCarlo simulations and yielding an approximation $V^{m, N}$ (where $N$ denotes\nthe Monte Carlo sample size). Especially, we prove that $V^{m, N} \\to V^m$ as\n$N \\to + \\infty$ for both methods and using Hilbert basis in the least squares\nregression. Besides, a convergence rate of order\n$\\mathcal{O}\\big(\\frac{1}{\\sqrt{N}} \\big)$ is proved in the least squares case.\nSeveral convergence results in this paper are based on the continuity of the\nswing value function with respect to cumulative consumption, which is also\nproved in the paper and has not been yet explored in the literature before for\nthe best of our knowledge.\n"
    },
    {
        "paper_id": 2307.04647,
        "authors": "Samuel Solgon Santos, Marlon Ruoso Moresco, Marcelo Brutti Righi,\n  Eduardo de Oliveira Horta",
        "title": "A note on the induction of comonotonic additive risk measures from\n  acceptance sets",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  We present simple general conditions on the acceptance sets under which their\ninduced monetary risk and deviation measures are comonotonic additive. We show\nthat acceptance sets induce comonotonic additive risk measures if and only if\nthe acceptance sets and their complements are stable under convex combinations\nof comonotonic random variables. A generalization of this result applies to\nrisk measures that are additive for random variables with a priori specified\ndependence structures, e.g., perfectly correlated, uncorrelated, or independent\nrandom variables.\n"
    },
    {
        "paper_id": 2307.04676,
        "authors": "Anand Deo, Karthyek Murthy",
        "title": "Importance Sampling for Minimization of Tail Risks: A Tutorial",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  This paper provides an introductory overview of how one may employ importance\nsampling effectively as a tool for solving stochastic optimization formulations\nincorporating tail risk measures such as Conditional Value-at-Risk.\nApproximating the tail risk measure by its sample average approximation, while\nappealing due to its simplicity and universality in use, requires a large\nnumber of samples to be able to arrive at risk-minimizing decisions with high\nconfidence. This is primarily due to the rarity with which the relevant tail\nevents get observed in the samples. In simulation, Importance Sampling is among\nthe most prominent methods for substantially reducing the sample requirement\nwhile estimating probabilities of rare events. Can importance sampling be used\nfor optimization as well? If so, what are the ingredients required for making\nimportance sampling an effective tool for optimization formulations involving\nrare events? This tutorial aims to provide an introductory overview of the two\nkey ingredients in this regard, namely, (i) how one may arrive at an importance\nsampling change of measure prescription at every decision, and (ii) the\nprominent techniques available for integrating such a prescription within a\nsolution paradigm for stochastic optimization formulations.\n"
    },
    {
        "paper_id": 2307.04754,
        "authors": "Francesco Cordoni and Alessio Sancetta",
        "title": "Action-State Dependent Dynamic Model Selection",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  A model among many may only be best under certain states of the world.\nSwitching from a model to another can also be costly. Finding a procedure to\ndynamically choose a model in these circumstances requires to solve a complex\nestimation procedure and a dynamic programming problem. A Reinforcement\nlearning algorithm is used to approximate and estimate from the data the\noptimal solution to this dynamic programming problem. The algorithm is shown to\nconsistently estimate the optimal policy that may choose different models based\non a set of covariates. A typical example is the one of switching between\ndifferent portfolio models under rebalancing costs, using macroeconomic\ninformation. Using a set of macroeconomic variables and price data, an\nempirical application to the aforementioned portfolio problem shows superior\nperformance to choosing the best portfolio model with hindsight.\n"
    },
    {
        "paper_id": 2307.04863,
        "authors": "Timoth\\'ee Fabre and Vincent Ragel",
        "title": "Tackling the Problem of State Dependent Execution Probability: Empirical\n  Evidence and Order Placement",
        "comments": "22 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Order placement tactics play a crucial role in high-frequency trading\nalgorithms and their design is based on understanding the dynamics of the order\nbook. Using high quality high-frequency data and survival analysis, we exhibit\nstrong state dependence properties of the fill probability function. We define\na set of microstructure features and train a multi-layer perceptron to infer\nthe fill probability function. A weighting method is applied to the loss\nfunction such that the model learns from censored data. By comparing numerical\nresults obtained on both digital asset centralized exchanges (CEXs) and stock\nmarkets, we are able to analyze dissimilarities between the fill probability of\nsmall tick crypto pairs and large tick assets -- large, relative to cryptos.\nThe practical use of this model is illustrated with a fixed time horizon\nexecution problem in which both the decision to post a limit order or to\nimmediately execute and the optimal distance of placement are characterized. We\ndiscuss the importance of accurately estimating the clean-up cost that occurs\nin the case of a non-execution and we show it can be well approximated by a\nsmooth function of market features. We finally assess the performance of our\nmodel with a backtesting approach that avoids the insertion of hypothetical\norders and makes possible to test the order placement algorithm with orders\nthat realistically impact the price formation process.\n"
    },
    {
        "paper_id": 2307.04879,
        "authors": "Johannes Treutlein",
        "title": "Modeling evidential cooperation in large worlds",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Evidential cooperation in large worlds (ECL) refers to the idea that humans\nand other agents can benefit by cooperating with similar agents with differing\nvalues in causally disconnected parts of a large universe. Cooperating provides\nagents with evidence that other similar agents are likely to cooperate too,\nresulting in gains from trade for all. This could be a crucial consideration\nfor altruists.\n  I develop a game-theoretic model of ECL as an incomplete information\nbargaining problem. The model incorporates uncertainty about others' value\nsystems and empirical situations, and addresses the problem of selecting a\ncompromise outcome. Using the model, I investigate issues with ECL and outline\nopen technical and philosophical questions.\n  I show that all cooperators must maximize the same weighted sum of utility\nfunctions to reach a Pareto optimal outcome. However, I argue against selecting\na compromise outcome implicitly by normalizing utility functions. I review\nbargaining theory and argue that the Nash bargaining solution could be a\nrelevant Schelling point. I introduce dependency equilibria (Spohn 2007), an\nequilibrium concept suitable for ECL, and generalize a folk theorem showing\nthat the Nash bargaining solution is a dependency equilibrium. I discuss gains\nfrom trade given uncertain beliefs about other agents and analyze how these\ngains decrease in several toy examples as the belief in another agent\ndecreases.\n  Finally, I discuss open issues in my model. First, the Nash bargaining\nsolution is sometimes not coalitionally stable, meaning that a subset of\ncooperators can unilaterally improve payoffs by deviating from the compromise.\nI investigate conditions under which stable payoff vectors exist. Second, I\ndiscuss how to model agents' default actions without ECL.\n"
    },
    {
        "paper_id": 2307.04953,
        "authors": "Alejandro Rodriguez Dominguez, Irving Ramirez Carrillo, David Parraga\n  Riquelme",
        "title": "Measuring Cause-Effect with the Variability of the Largest Eigenvalue",
        "comments": "10 pages, 11 Figures. arXiv admin note: text overlap with\n  arXiv:0910.1205 by other authors",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
        "abstract": "  We present a method to test and monitor structural relationships between time\nvariables. The distribution of the first eigenvalue for lagged correlation\nmatrices (Tracy-Widom distribution) is used to test structural time\nrelationships between variables against the alternative hypothesis\n(Independence). This distribution studies the asymptotic dynamics of the\nlargest eigenvalue as a function of the lag in lagged correlation matrices. By\nanalyzing the time series of the standard deviation of the greatest eigenvalue\nfor $2\\times 2$ correlation matrices with different lags we can analyze\ndeviations from the Tracy-Widom distribution to test structural relationships\nbetween these two time variables. These relationships can be related to\ncausality. We use the standard deviation of the explanatory power of the first\neigenvalue at different lags as a proxy for testing and monitoring structural\ncausal relationships. The method is applied to analyse causal dependencies\nbetween daily monetary flows in a retail brokerage business allowing to control\nfor liquidity risks.\n"
    },
    {
        "paper_id": 2307.04986,
        "authors": "Ross Williams, Niyousha Hosseinichimeh, Aritra Majumdar, Navid\n  Ghaffarzadegan",
        "title": "Epidemic Modeling with Generative Agents",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  This study offers a new paradigm of individual-level modeling to address the\ngrand challenge of incorporating human behavior in epidemic models. Using\ngenerative artificial intelligence in an agent-based epidemic model, each agent\nis empowered to make its own reasonings and decisions via connecting to a large\nlanguage model such as ChatGPT. Through various simulation experiments, we\npresent compelling evidence that generative agents mimic real-world behaviors\nsuch as quarantining when sick and self-isolation when cases rise.\nCollectively, the agents demonstrate patterns akin to multiple waves observed\nin recent pandemics followed by an endemic period. Moreover, the agents\nsuccessfully flatten the epidemic curve. This study creates potential to\nimprove dynamic system modeling by offering a way to represent human brain,\nreasoning, and decision making.\n"
    },
    {
        "paper_id": 2307.05048,
        "authors": "Jaydip Sen, Subhasis Dasgupta",
        "title": "Portfolio Optimization: A Comparative Study",
        "comments": "This is the preprint of the book chapter accepted for publication in\n  the book titled \"Deep Learning - Recent Finding and Researches\" edited by\n  Manuel Dom\\'inguez-Morales. The book is scheduled to be be published by\n  IntechOpen, London, UK in January 2024. This is not the final version of the\n  chapter",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Portfolio optimization has been an area that has attracted considerable\nattention from the financial research community. Designing a profitable\nportfolio is a challenging task involving precise forecasting of future stock\nreturns and risks. This chapter presents a comparative study of three portfolio\ndesign approaches, the mean-variance portfolio (MVP), hierarchical risk parity\n(HRP)-based portfolio, and autoencoder-based portfolio. These three approaches\nto portfolio design are applied to the historical prices of stocks chosen from\nten thematic sectors listed on the National Stock Exchange (NSE) of India. The\nportfolios are designed using the stock price data from January 1, 2018, to\nDecember 31, 2021, and their performances are tested on the out-of-sample data\nfrom January 1, 2022, to December 31, 2022. Extensive results are analyzed on\nthe performance of the portfolios. It is observed that the performance of the\nMVP portfolio is the best on the out-of-sample data for the risk-adjusted\nreturns. However, the autoencoder portfolios outperformed their counterparts on\nannual returns.\n"
    },
    {
        "paper_id": 2307.05121,
        "authors": "Yue Tian, Guanjun Liu",
        "title": "Transaction Fraud Detection via Spatial-Temporal-Aware Graph Transformer",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  How to obtain informative representations of transactions and then perform\nthe identification of fraudulent transactions is a crucial part of ensuring\nfinancial security. Recent studies apply Graph Neural Networks (GNNs) to the\ntransaction fraud detection problem. Nevertheless, they encounter challenges in\neffectively learning spatial-temporal information due to structural\nlimitations. Moreover, few prior GNN-based detectors have recognized the\nsignificance of incorporating global information, which encompasses similar\nbehavioral patterns and offers valuable insights for discriminative\nrepresentation learning. Therefore, we propose a novel heterogeneous graph\nneural network called Spatial-Temporal-Aware Graph Transformer (STA-GT) for\ntransaction fraud detection problems. Specifically, we design a temporal\nencoding strategy to capture temporal dependencies and incorporate it into the\ngraph neural network framework, enhancing spatial-temporal information modeling\nand improving expressive ability. Furthermore, we introduce a transformer\nmodule to learn local and global information. Pairwise node-node interactions\novercome the limitation of the GNN structure and build up the interactions with\nthe target node and long-distance ones. Experimental results on two financial\ndatasets compared to general GNN models and GNN-based fraud detectors\ndemonstrate that our proposed method STA-GT is effective on the transaction\nfraud detection task.\n"
    },
    {
        "paper_id": 2307.05391,
        "authors": "Ali Lashgari",
        "title": "Harnessing the Potential of Volatility: Advancing GDP Prediction",
        "comments": "Pennsylvania Economic Association (PEA)",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper presents a novel machine learning approach to GDP prediction that\nincorporates volatility as a model weight. The proposed method is specifically\ndesigned to identify and select the most relevant macroeconomic variables for\naccurate GDP prediction, while taking into account unexpected shocks or events\nthat may impact the economy. The proposed method's effectiveness is tested on\nreal-world data and compared to previous techniques used for GDP forecasting,\nsuch as Lasso and Adaptive Lasso. The findings show that the\nVolatility-weighted Lasso method outperforms other methods in terms of accuracy\nand robustness, providing policymakers and analysts with a valuable tool for\nmaking informed decisions in a rapidly changing economic environment. This\nstudy demonstrates how data-driven approaches can help us better understand\neconomic fluctuations and support more effective economic policymaking.\n  Keywords: GDP prediction, Lasso, Volatility, Regularization, Macroeconomics\nVariable Selection, Machine Learning JEL codes: C22, C53, E37.\n"
    },
    {
        "paper_id": 2307.0547,
        "authors": "Mansur Arief, Yan Akhra, Iwan Vanany",
        "title": "A Robust and Efficient Optimization Model for Electric Vehicle Charging\n  Stations in Developing Countries under Electricity Uncertainty",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The rising demand for electric vehicles (EVs) worldwide necessitates the\ndevelopment of robust and accessible charging infrastructure, particularly in\ndeveloping countries where electricity disruptions pose a significant\nchallenge. Earlier charging infrastructure optimization studies do not\nrigorously address such service disruption characteristics, resulting in\nsuboptimal infrastructure designs. To address this issue, we propose an\nefficient simulation-based optimization model that estimates candidate\nstations' service reliability and incorporates it into the objective function\nand constraints. We employ the control variates (CV) variance reduction\ntechnique to enhance simulation efficiency. Our model provides a highly robust\nsolution that buffers against uncertain electricity disruptions, even when\ncandidate station service reliability is subject to underestimation or\noverestimation. Using a dataset from Surabaya, Indonesia, our numerical\nexperiment demonstrates that the proposed model achieves a 13% higher average\nobjective value compared to the non-robust solution. Furthermore, the CV\ntechnique successfully reduces the simulation sample size up to 10 times\ncompared to Monte Carlo, allowing the model to solve efficiently using a\nstandard MIP solver. Our study provides a robust and efficient solution for\ndesigning EV charging infrastructure that can thrive even in developing\ncountries with uncertain electricity disruptions.\n"
    },
    {
        "paper_id": 2307.05522,
        "authors": "Tom Liu, Stephen Roberts, Stefan Zohren",
        "title": "Deep Inception Networks: A General End-to-End Framework for Multi-asset\n  Quantitative Strategies",
        "comments": "17 pages, 12 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We introduce Deep Inception Networks (DINs), a family of Deep Learning models\nthat provide a general framework for end-to-end systematic trading strategies.\nDINs extract time series (TS) and cross sectional (CS) features directly from\ndaily price returns. This removes the need for handcrafted features, and allows\nthe model to learn from TS and CS information simultaneously. DINs benefit from\na fully data-driven approach to feature extraction, whilst avoiding\noverfitting. Extending prior work on Deep Momentum Networks, DIN models\ndirectly output position sizes that optimise Sharpe ratio, but for the entire\nportfolio instead of individual assets. We propose a novel loss term to balance\nturnover regularisation against increased systemic risk from high correlation\nto the overall market. Using futures data, we show that DIN models outperform\ntraditional TS and CS benchmarks, are robust to a range of transaction costs\nand perform consistently across random seeds. To balance the general nature of\nDIN models, we provide examples of how attention and Variable Selection\nNetworks can aid the interpretability of investment decisions. These\nmodel-specific methods are particularly useful when the dimensionality of the\ninput is high and variable importance fluctuates dynamically over time.\nFinally, we compare the performance of DIN models on other asset classes, and\nshow how the space of potential features can be customised.\n"
    },
    {
        "paper_id": 2307.05581,
        "authors": "Sedar Olmez, Akhil Ahmed, Keith Kam, Zhe Feng, Alan Tua",
        "title": "Exploring the Dynamics of the Specialty Insurance Market Using a Novel\n  Discrete Event Simulation Framework: a Lloyd's of London Case Study",
        "comments": "27 Pages, 12 Images and 14 Tables",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
        "abstract": "  This research presents a novel Discrete Event Simulation (DES) of the Lloyd's\nof London specialty insurance market, exploring complex market dynamics that\nhave not been previously studied quantitatively. The proof-of-concept model\nallows for the simulation of various scenarios that capture important market\nphenomena such as the underwriting cycle, the impact of risk syndication, and\nthe importance of appropriate exposure management. Despite minimal calibration,\nour model has shown that it is a valuable tool for understanding and analysing\nthe Lloyd's of London specialty insurance market, particularly in terms of\nidentifying areas for further investigation for regulators and participants of\nthe market alike. The results generate the expected behaviours that, syndicates\n(insurers) are less likely to go insolvent if they adopt sophisticated exposure\nmanagement practices, catastrophe events lead to more defined patterns of\ncyclicality and cause syndicates to substantially increase their premiums\noffered. Lastly, syndication enhances the accuracy of actuarial price estimates\nand narrows the divergence among syndicates. Overall, this research offers a\nnew perspective on the Lloyd's of London market and demonstrates the potential\nof individual-based modelling (IBM) for understanding complex financial\nsystems.\n"
    },
    {
        "paper_id": 2307.05719,
        "authors": "Pawe{\\l} Sakowski, Rafa{\\l} Sieradzki and Robert \\'Slepaczuk",
        "title": "Systemic risk indicator based on implied and realized volatility",
        "comments": "28 pages, 12 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  We propose a new measure of systemic risk to analyze the impact of the major\nfinancial market turmoils in the stock markets from 2000 to 2023 in the USA,\nEurope, Brazil, and Japan. Our Implied Volatility Realized Volatility Systemic\nRisk Indicator (IVRVSRI) shows that the reaction of stock markets varies across\ndifferent geographical locations and the persistence of the shocks depends on\nthe historical volatility and long-term average volatility level in a given\nmarket. The methodology applied is based on the logic that the simpler is\nalways better than the more complex if it leads to the same results. Such an\napproach significantly limits model risk and substantially decreases\ncomputational burden. Robustness checks show that IVRVSRI is a precise and\nvalid measure of the current systemic risk in the stock markets. Moreover, it\ncan be used for other types of assets and high-frequency data. The forecasting\nability of various SRIs (including CATFIN, CISS, IVRVSRI, SRISK, and Cleveland\nFED) with regard to weekly returns of S&P 500 index is evaluated based on the\nsimple linear, quasi-quantile, and quantile regressions. We show that IVRVSRI\nhas the strongest predicting power among them.\n"
    },
    {
        "paper_id": 2307.05843,
        "authors": "Rich Ryan",
        "title": "Responses of Unemployment to Productivity Changes for a General Matching\n  Technology",
        "comments": "For replication files, see\n  https://github.com/richryan/fundamentalSurplusGeneralMatch",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Workers separate from jobs, search for jobs, accept jobs, and fund\nconsumption with their wages. Firms recruit workers to fill vacancies. Search\nfrictions prevent firms from instantly hiring available workers. Unemployment\npersists. These features are described by the Diamond-Mortensen-Pissarides\nmodeling framework. In this class of models, how unemployment responds to\nproductivity changes depends on resources that can be allocated to job\ncreation. Yet, this characterization has been made when matching is\nparameterized by a Cobb-Douglas technology. For a canonical DMP model, I (1)\ndemonstrate that a unique steady-state equilibrium will exist as long as the\ninitial vacancy yields a positive surplus; (2) characterize responses of\nunemployment to productivity changes for a general matching technology; and (3)\nshow how a matching technology that is not Cobb-Douglas implies unemployment\nresponds more to productivity changes, which is independent of resources\navailable for job creation, a feature that will be of interest to\nbusiness-cycle researchers.\n"
    },
    {
        "paper_id": 2307.06339,
        "authors": "Kosuke Tatsumura, Ryo Hidaka, Jun Nakayama, Tomoya Kashimata, and\n  Masaya Yamasaki",
        "title": "Real-time Trading System based on Selections of Potentially Profitable,\n  Uncorrelated, and Balanced Stocks by NP-hard Combinatorial Optimization",
        "comments": "12 pages, 5 figures. arXiv admin note: text overlap with\n  arXiv:2307.05923",
        "journal-ref": "IEEE Access 11, pp. 120023 - 1200336 (2023)",
        "doi": "10.1109/ACCESS.2023.3326816",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Financial portfolio construction problems are often formulated as quadratic\nand discrete (combinatorial) optimization that belong to the nondeterministic\npolynomial time (NP)-hard class in computational complexity theory. Ising\nmachines are hardware devices that work in quantum-mechanical/quantum-inspired\nprinciples for quickly solving NP-hard optimization problems, which potentially\nenable making trading decisions based on NP-hard optimization in the time\nconstraints for high-speed trading strategies. Here we report a real-time stock\ntrading system that determines long(buying)/short(selling) positions through\nNP-hard portfolio optimization for improving the Sharpe ratio using an embedded\nIsing machine based on a quantum-inspired algorithm called simulated\nbifurcation. The Ising machine selects a balanced (delta-neutral) group of\nstocks from an $N$-stock universe according to an objective function involving\nmaximizing instantaneous expected returns defined as deviations from\nvolume-weighted average prices and minimizing the summation of statistical\ncorrelation factors (for diversification). It has been demonstrated in the\nTokyo Stock Exchange that the trading strategy based on NP-hard portfolio\noptimization for $N$=128 is executable with the FPGA (field-programmable gate\narray)-based trading system with a response latency of 164 $\\mu$s.\n"
    },
    {
        "paper_id": 2307.064,
        "authors": "Beatrice Foroni, Luca Merlo, Lea Petrella",
        "title": "Quantile and expectile copula-based hidden Markov regression models for\n  the analysis of the cryptocurrency market",
        "comments": "35 pages, 6 figures. arXiv admin note: text overlap with\n  arXiv:2301.09722",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  The role of cryptocurrencies within the financial systems has been expanding\nrapidly in recent years among investors and institutions. It is therefore\ncrucial to investigate the phenomena and develop statistical methods able to\ncapture their interrelationships, the links with other global systems, and, at\nthe same time, the serial heterogeneity. For these reasons, this paper\nintroduces hidden Markov regression models for jointly estimating quantiles and\nexpectiles of cryptocurrency returns using regime-switching copulas. The\nproposed approach allows us to focus on extreme returns and describe their\ntemporal evolution by introducing time-dependent coefficients evolving\naccording to a latent Markov chain. Moreover to model their time-varying\ndependence structure, we consider elliptical copula functions defined by\nstate-specific parameters. Maximum likelihood estimates are obtained via an\nExpectation-Maximization algorithm. The empirical analysis investigates the\nrelationship between daily returns of five cryptocurrencies and major world\nmarket indices.\n"
    },
    {
        "paper_id": 2307.0645,
        "authors": "Robert Balkin and Hector D. Ceniceros and Ruimeng Hu",
        "title": "Stochastic Delay Differential Games: Financial Modeling and Machine\n  Learning Algorithms",
        "comments": "29 pages, 8 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  In this paper, we propose a numerical methodology for finding the closed-loop\nNash equilibrium of stochastic delay differential games through deep learning.\nThese games are prevalent in finance and economics where multi-agent\ninteraction and delayed effects are often desired features in a model, but are\nintroduced at the expense of increased dimensionality of the problem. This\nincreased dimensionality is especially significant as that arising from the\nnumber of players is coupled with the potential infinite dimensionality caused\nby the delay. Our approach involves parameterizing the controls of each player\nusing distinct recurrent neural networks. These recurrent neural network-based\ncontrols are then trained using a modified version of Brown's fictitious play,\nincorporating deep learning techniques. To evaluate the effectiveness of our\nmethodology, we test it on finance-related problems with known solutions.\nFurthermore, we also develop new problems and derive their analytical Nash\nequilibrium solutions, which serve as additional benchmarks for assessing the\nperformance of our proposed deep learning approach.\n"
    },
    {
        "paper_id": 2307.066,
        "authors": "Zhu Bangyuan",
        "title": "Critical comparisons on deep learning approaches for foreign exchange\n  rate prediction",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  In a natural market environment, the price prediction model needs to be\nupdated in real time according to the data obtained by the system to ensure the\naccuracy of the prediction. In order to improve the user experience of the\nsystem, the price prediction function needs to use the fastest training model\nand the model prediction fitting effect of the best network as a predictive\nmodel. We conduct research on the fundamental theories of RNN, LSTM, and BP\nneural networks, analyse their respective characteristics, and discuss their\nadvantages and disadvantages to provide a reference for the selection of\nprice-prediction models.\n"
    },
    {
        "paper_id": 2307.06684,
        "authors": "Susan Athey and Lisa K. Simon and Oskar N. Skans and Johan Vikstrom\n  and Yaroslav Yakymovych",
        "title": "The Heterogeneous Earnings Impact of Job Loss Across Workers,\n  Establishments, and Markets",
        "comments": "Version 2 adds out-of-sample estimates using closures after the end\n  of the training sample period, robustness checks on heterogeneity within and\n  across establishments (related to AKM etc), results on Swedish insurance\n  policies across the CATE-distribution",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Using generalized random forests and rich Swedish administrative data, we\nshow that the earnings effects of job displacement due to establishment\nclosures are extremely heterogeneous across and within (observable) worker\ntypes, establishments, and markets. The decile with the largest predicted\neffects loses 50 percent of annual earnings the year after displacement and\nlosses accumulate to 200 percent over 7 years. The least affected decile\nexperiences only marginal losses of 6 percent in the year after displacement.\nPrior to displacement workers in the most affected decile were lower paid and\nhad negative earnings trajectories. Workers with large predicted effects are\nmore sensitive to adverse market conditions than other workers. When\nrestricting attention to simple targeting rules, the subgroup consisting of\nolder workers in routine-task intensive jobs has the highest predictable\neffects of displacement.\n"
    },
    {
        "paper_id": 2307.0701,
        "authors": "Guillermo Alonso Alvarez and Sergey Nadtochiy",
        "title": "Optimal contract design via relaxation: application to the problem of\n  brokerage fee for a client with private signal",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  In this paper we show how the relaxation techniques can be used to establish\nthe existence of an optimal contract in presence of information asymmetry. The\nmethod we illustrate was initially motivated by the problem of designing\noptimal brokerage fees, but it does apply to other optimal contract problems,\nin which (i) the agent controls linearly the drift of a diffusion process, (ii)\nthe direct dependence of the principal's and the agent's objectives on the\nstrategy of the agent is of a special form, and (iii) the space of admissible\ncontracts is compact. This method is then applied to establish existence of an\noptimal brokerage fee in a market model with a private trading signal observed\nby the broker's client but not by the broker.\n"
    },
    {
        "paper_id": 2307.07015,
        "authors": "Carl F. Mela, Jason M.T. Roos, Tulio Sousa",
        "title": "Advertiser Learning in Direct Advertising Markets",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Direct buy advertisers procure advertising inventory at fixed rates from\npublishers and ad networks. Such advertisers face the complex task of choosing\nads amongst myriad new publisher sites. We offer evidence that advertisers do\nnot excel at making these choices. Instead, they try many sites before settling\non a favored set, consistent with advertiser learning. We subsequently model\nadvertiser demand for publisher inventory wherein advertisers learn about\nadvertising efficacy across publishers' sites. Results suggest that advertisers\nspend considerable resources advertising on sites they eventually abandon--in\npart because their prior beliefs about advertising efficacy on those sites are\ntoo optimistic. The median advertiser's expected CTR at a new site is 0.23%,\nfive times higher than the true median CTR of 0.045%.\n  We consider how an ad network's pooling of advertiser information remediates\nthis problem. As ads with similar visual elements garner similar CTRs, the\nnetwork's pooling of information enables advertisers to better predict ad\nperformance at new sites. Counterfactual analyses indicate that gains from\npooling advertiser information are substantial: over six months, we estimate a\nmedian advertiser welfare gain of \\$2,756 (a 15.5% increase) and a median\npublisher revenue gain of \\$9,618 (a 63.9% increase).\n"
    },
    {
        "paper_id": 2307.07024,
        "authors": "David Evangelista, Yuri Thamsten",
        "title": "Approximately optimal trade execution strategies under fast\n  mean-reversion",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In a fixed time horizon, appropriately executing a large amount of a\nparticular asset -- meaning a considerable portion of the volume traded within\nthis frame -- is challenging. Especially for illiquid or even highly liquid but\nalso highly volatile ones, the role of \"market quality\" is quite relevant in\nproperly designing execution strategies. Here, we model it by considering\nuncertain volatility and liquidity; hence, moments of high or low price impact\nand risk vary randomly throughout the trading period. We work under the central\nassumption: although there are these uncertain variations, we assume they occur\nin a fast mean-reverting fashion. We thus employ singular perturbation\narguments to study approximations to the optimal strategies in this framework.\nBy using high-frequency data, we provide estimation methods for our model in\nface of microstructure noise, as well as numerically assess all of our results.\n"
    },
    {
        "paper_id": 2307.07037,
        "authors": "ATM Omor Faruq",
        "title": "The Determinants of Foreign Direct Investment (FDI) A Panel Data\n  Analysis for the Emerging Asian Economies",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper, we explore the economic, institutional, and\npolitical/governmental factors in attracting Foreign Direct Investment (FDI)\ninflows in the emerging twenty-four Asian economies. To examine the significant\ndeterminants of FDI, the study uses panel data for a period of seventeen years\n(2002-2018). The panel methodology enables us to deal with endogeneity and\nother issues. Multiple regression models are done for empirical evidence. The\nstudy focuses on a holistic approach and considers different variables under\nthree broad areas: economic, institutional, and political aspects. The\nvariables include Market Size, Trade Openness, Inflation, Natural Resource,\nLending Rate, Capital Formation as economic factors and Business Regulatory\nEnvironment and Business Disclosure Index as institutional factors and\nPolitical Stability, Government Effectiveness, and Rule of Law as political\nfactors. The empirical findings show most of the economic factors significantly\naffect FDI inflows whereas Business Disclosure is the only important\ninstitutional variable. Moreover, political stability has a significant\npositive impact in attracting foreign capital flow though the impact of\ngovernment effectiveness is found insignificant. Overall, the economic factors\nprevail strongly compared to institutional and political factors.\n"
    },
    {
        "paper_id": 2307.07103,
        "authors": "Qi Chen Hong-tao Wang and Chao Guo",
        "title": "A Hamiltonian Approach to Barrier Option Pricing Under Vasicek Model",
        "comments": "18 pages,5 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Hamiltonian approach in quantum theory provides a new thinking for option\npricing with stochastic interest rates. For barrier options, the option price\nchanging process is similar to the infinite high barrier scattering problem in\nquantum mechanics; for double barrier options, the option price changing\nprocess is analogous to a particle moving in a infinite square potential well.\nUsing Hamiltonian approach, the expressions of pricing kernels and option\nprices under Vasicek stochastic interest rate model could be derived. Numerical\nresults of options price as functions of underlying prices are also shown.\n"
    },
    {
        "paper_id": 2307.07657,
        "authors": "Laurens Van Mieghem, Antonis Papapantoleon, Jonas Papazoglou-Hennig",
        "title": "Machine learning for option pricing: an empirical investigation of\n  network architectures",
        "comments": "29 pages, 21 figures, 13 tables",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  We consider the supervised learning problem of learning the price of an\noption or the implied volatility given appropriate input data (model\nparameters) and corresponding output data (option prices or implied\nvolatilities). The majority of articles in this literature considers a (plain)\nfeed forward neural network architecture in order to connect the neurons used\nfor learning the function mapping inputs to outputs. In this article, motivated\nby methods in image classification and recent advances in machine learning\nmethods for PDEs, we investigate empirically whether and how the choice of\nnetwork architecture affects the accuracy and training time of a machine\nlearning algorithm. We find that for option pricing problems, where we focus on\nthe Black--Scholes and the Heston model, the generalized highway network\narchitecture outperforms all other variants, when considering the mean squared\nerror and the training time as criteria. Moreover, for the computation of the\nimplied volatility, after a necessary transformation, a variant of the DGM\narchitecture outperforms all other variants, when considering again the mean\nsquared error and the training time as criteria.\n"
    },
    {
        "paper_id": 2307.07689,
        "authors": "Zhaoxing Gao and Ruey S. Tsay",
        "title": "Supervised Dynamic PCA: Linear Dynamic Forecasting with Many Predictors",
        "comments": "58 pages, 7 figures",
        "journal-ref": "Journal of the American Statistical Association, 2024",
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper proposes a novel dynamic forecasting method using a new supervised\nPrincipal Component Analysis (PCA) when a large number of predictors are\navailable. The new supervised PCA provides an effective way to bridge the gap\nbetween predictors and the target variable of interest by scaling and combining\nthe predictors and their lagged values, resulting in an effective dynamic\nforecasting. Unlike the traditional diffusion-index approach, which does not\nlearn the relationships between the predictors and the target variable before\nconducting PCA, we first re-scale each predictor according to their\nsignificance in forecasting the targeted variable in a dynamic fashion, and a\nPCA is then applied to a re-scaled and additive panel, which establishes a\nconnection between the predictability of the PCA factors and the target\nvariable. Furthermore, we also propose to use penalized methods such as the\nLASSO approach to select the significant factors that have superior predictive\npower over the others. Theoretically, we show that our estimators are\nconsistent and outperform the traditional methods in prediction under some mild\nconditions. We conduct extensive simulations to verify that the proposed method\nproduces satisfactory forecasting results and outperforms most of the existing\nmethods using the traditional PCA. A real example of predicting U.S.\nmacroeconomic variables using a large number of predictors showcases that our\nmethod fares better than most of the existing ones in applications. The\nproposed method thus provides a comprehensive and effective approach for\ndynamic forecasting in high-dimensional data analysis.\n"
    },
    {
        "paper_id": 2307.07694,
        "authors": "Chung I Lu",
        "title": "Evaluation of Deep Reinforcement Learning Algorithms for Portfolio\n  Optimisation",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
        "abstract": "  We evaluate benchmark deep reinforcement learning (DRL) algorithms on the\ntask of portfolio optimisation under a simulator. The simulator is based on\ncorrelated geometric Brownian motion (GBM) with the Bertsimas-Lo (BL) market\nimpact model. Using the Kelly criterion (log utility) as the objective, we can\nanalytically derive the optimal policy without market impact and use it as an\nupper bound to measure performance when including market impact. We found that\nthe off-policy algorithms DDPG, TD3 and SAC were unable to learn the right Q\nfunction due to the noisy rewards and therefore perform poorly. The on-policy\nalgorithms PPO and A2C, with the use of generalised advantage estimation (GAE),\nwere able to deal with the noise and derive a close to optimal policy. The\nclipping variant of PPO was found to be important in preventing the policy from\ndeviating from the optimal once converged. In a more challenging environment\nwhere we have regime changes in the GBM parameters, we found that PPO, combined\nwith a hidden Markov model (HMM) to learn and predict the regime context, is\nable to learn different policies adapted to each regime. Overall, we find that\nthe sample complexity of these algorithms is too high, requiring more than 2m\nsteps to learn a good policy in the simplest setting, which is equivalent to\nalmost 8,000 years of daily prices.\n"
    },
    {
        "paper_id": 2307.07811,
        "authors": "Kamer Ali Yuksel",
        "title": "Generative Meta-Learning Robust Quality-Diversity Portfolio",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  This paper proposes a novel meta-learning approach to optimize a robust\nportfolio ensemble. The method uses a deep generative model to generate diverse\nand high-quality sub-portfolios combined to form the ensemble portfolio. The\ngenerative model consists of a convolutional layer, a stateful LSTM module, and\na dense network. During training, the model takes a randomly sampled batch of\nGaussian noise and outputs a population of solutions, which are then evaluated\nusing the objective function of the problem. The weights of the model are\nupdated using a gradient-based optimizer. The convolutional layer transforms\nthe noise into a desired distribution in latent space, while the LSTM module\nadds dependence between generations. The dense network decodes the population\nof solutions. The proposed method balances maximizing the performance of the\nsub-portfolios with minimizing their maximum correlation, resulting in a robust\nensemble portfolio against systematic shocks. The approach was effective in\nexperiments where stochastic rewards were present. Moreover, the results (Fig.\n1) demonstrated that the ensemble portfolio obtained by taking the average of\nthe generated sub-portfolio weights was robust and generalized well. The\nproposed method can be applied to problems where diversity is desired among\nco-optimized solutions for a robust ensemble. The source-codes and the dataset\nare in the supplementary material.\n"
    },
    {
        "paper_id": 2307.07867,
        "authors": "Nick P. Petropoulos",
        "title": "Adjusting the nuclear reactor's neutron transport and diffusion theory\n  for an alternative description and modelling of postage or supplies delivery\n  processes",
        "comments": "25 pages, 2 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  There seems to exist significant similarities between a reactor system and a\nsupply chain from collection to delivery. In the reactor case, neutrons are\ncontinuously produced and absorbed in nuclear fuel. In a supply system case,\nitems are continuously collected and continuously delivered to destinations.\nStable reactor operation is ensured by keeping the ratio of neutrons produced\nto neutrons absorbed in the reactor equal to one. Profitable and qualitative\nsupply operation is ensured by keeping the ratio of items delivered to items\ncollected as close to unity as possible. The analogy between the two systems is\nobvious. This text, which is provided as is and has not undergone any peer\nreview process, proposes transferring parts of the nuclear reactor's neutron\ntransport and diffusion theory to deterministically model supply processes. To\nthis end a set of assumptions and definitions are provided as needed along with\nthe introduction of reactions or interactions like collections, deliveries, and\nlosses occurring into the supply chain. The interaction rates are calculated\nwith the method used in reactors employing analogy factors and interactors with\nwhich the items in the chain interact. The main aim is to describe losses\nescape in steady state and in parallel estimate the analogy factors and\noptimize for the correct selection of the interactors pool. The model as\nproposed seems to be a tool for a different insight method into supply\nproblems. The model, if proven and applied, is discussed to be a strong\noptimization tool, which could deterministically pinpoint flaws in existing\nsupply systems or stochastically efficiently organize proposed supplied chains.\n"
    },
    {
        "paper_id": 2307.07868,
        "authors": "Varun Sangwan, Vishesh Kumar Singh, Bibin Christopher V",
        "title": "Contrasting the efficiency of stock price prediction models using\n  various types of LSTM models aided with sentiment analysis",
        "comments": "8 Pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/publicdomain/zero/1.0/",
        "abstract": "  Our research aims to find the best model that uses companies projections and\nsector performances and how the given company fares accordingly to correctly\npredict equity share prices for both short and long term goals.\n"
    },
    {
        "paper_id": 2307.07888,
        "authors": "Gregory M. Dickinson",
        "title": "Privately Policing Dark Patterns",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
        "abstract": "  Lawmakers around the country are crafting new laws to target \"dark patterns\"\n-- user interface designs that trick or coerce users into enabling cell phone\nlocation tracking, sharing browsing data, initiating automatic billing, or\nmaking whatever other choices their designers prefer. Dark patterns pose a\nserious problem. In their most aggressive forms, they interfere with human\nautonomy, undermine customers' evaluation and selection of products, and\ndistort online markets for goods and services. Yet crafting legislation is a\nmajor challenge: Persuasion and deception are difficult to distinguish, and\nshifting tech trends present an ever-moving target. To address these\nchallenges, this Article proposes leveraging state private law to define and\ntrack dark patterns as they evolve. Judge-crafted decisional law can respond\nquickly to new techniques, flexibly define the boundary between permissible and\nimpermissible designs, and bolster state and federal regulatory enforcement\nefforts by quickly identifying those designs that most undermine user autonomy.\n"
    },
    {
        "paper_id": 2307.08049,
        "authors": "Catherine E.A. Mulligan and Phil Godsiff",
        "title": "Datalism and Data Monopolies in the Era of A.I.: A Research Agenda",
        "comments": "17 pages, 4 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  The increasing use of data in various parts of the economic and social\nsystems is creating a new form of monopoly: data monopolies. We illustrate that\nthe companies using these strategies, Datalists, are challenging the existing\ndefinitions used within Monopoly Capital Theory (MCT). Datalists are pursuing a\ndifferent type of monopoly control than traditional multinational corporations.\nThey are pursuing monopolistic control over data to feed their productive\nprocesses, increasingly controlled by algorithms and Artificial Intelligence\n(AI). These productive processes use information about humans and the creative\noutputs of humans as the inputs but do not classify those humans as employees,\nso they are not paid or credited for their labour. This paper provides an\noverview of this evolution and its impact on monopoly theory. It concludes with\nan outline for a research agenda for economics in this space.\n"
    },
    {
        "paper_id": 2307.08465,
        "authors": "Sergey Yekimov",
        "title": "The Chebyshev Polynomials Of The First Kind For Analysis Rates Shares Of\n  Enterprises",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Chebyshev polynomials of the first kind have long been used to approximate\nexperimental data in solving various technical problems. Within the framework\nof this study, the dynamics of shares of eight Czech enterprises was analyzed\nby the Chebyshev polynomial decomposition: CEZ A.S. (CEZP), Colt CZ Group SE\n(CZG), Erste Bank (ERST), Komercni Banka (BKOM), Moneta Money Bank A.S.\n(MONET), Photon (PENP), Vienna insurance group (VIGR) in 2021. An investor,\nwhen making a decision to purchase a security , is guided largely by an\nheuristic approach . And variance and correlation are not observed by human\nsenses. The vectors of decomposition of time series of exchange values of\nsecurities allow analyzing the dynamics of exchange values of securities more\neffectively if their dynamics does not correspond to the normal distribution\nlaw. The proposed model allows analyzing the dynamics of the exchange value of\na securities portfolio without calculating variance and correlation. This model\ncan be useful if the dynamics of the exchange values of securities does not\nobey, due to certain circumstances, the normal law of distribution.\n"
    },
    {
        "paper_id": 2307.08564,
        "authors": "Andrea Baronchelli",
        "title": "Shaping New Norms for AI",
        "comments": null,
        "journal-ref": "Philosophical Transactions of the Royal Society B, 379(1897),\n  20230028 (2024)",
        "doi": "10.1098/rstb.2023.0028",
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  As Artificial Intelligence (AI) becomes increasingly integrated into our\nlives, the need for new norms is urgent. However, AI evolves at a much faster\npace than the characteristic time of norm formation, posing an unprecedented\nchallenge to our societies. This paper examines possible criticalities of the\nprocesses of norm formation surrounding AI. Thus, it focuses on how new norms\ncan be established, rather than on what these norms should be. It distinguishes\ndifferent scenarios based on the centralisation or decentralisation of the norm\nformation process, analysing the cases where new norms are shaped by formal\nauthorities, informal institutions, or emerge spontaneously in a bottom-up\nfashion. On the latter point, the paper reports a conversation with ChatGPT in\nwhich the LLM discusses some of the emerging norms it has observed. Far from\nseeking exhaustiveness, this article aims to offer readers interpretive tools\nto understand society's response to the growing pervasiveness of AI. An outlook\non how AI could influence the formation of future social norms emphasises the\nimportance for open societies to anchor their formal deliberation process in an\nopen, inclusive, and transparent public discourse.\n"
    },
    {
        "paper_id": 2307.08612,
        "authors": "Jessica Morales Herrera and Ra\\'ul Salgado-Garc\\'ia",
        "title": "Trend patterns statistics for assessing irreversibility in\n  cryptocurrencies: time-asymmetry versus inefficiency",
        "comments": "24 pages, 7 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  In this paper, we present a measure of time irreversibility using trend\npattern statistics. We define the irreversibility index as the Kullback-Leibler\ndivergence between the distribution of uptrends subsequences (increasing\ntrends) and the corresponding downtrends subsequences distribution (decreasing\ntrends) in a time series. We use this index to analyze the degree of\nirreversibility in log return series over time, specifically focusing on five\ncryptocurrencies: Bitcoin, Ethereum, Ripple, Litecoin, and Bitcoin Cash. Our\nanalysis reveals a strong indication of irreversibility in all these\ncryptocurrencies and the characteristic evolves over time. We additionally\nevaluate the market efficiency for these cryptocurrencies based on a recently\nproposed information-theoretic measure. By comparing inefficiency and\nirreversibility, we explore the relationship between these statistical\nfeatures. This comparison provides insight into the non-trivial relationship\nbetween inefficiency and irreversibility.\n"
    },
    {
        "paper_id": 2307.08616,
        "authors": "Rafael Ramos Tubino, Remy Cazabet, Natkamon Tovanich, and Celine\n  Robardet",
        "title": "Temporal and Geographical Analysis of Real Economic Activities in the\n  Bitcoin Blockchain",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We study the real economic activity in the Bitcoin blockchain that involves\ntransactions from/to retail users rather than between organizations such as\nmarketplaces, exchanges, or other services. We first introduce a heuristic\nmethod to classify Bitcoin players into three main categories: Frequent\nReceivers (FR), Neighbors of FR, and Others. We show that most real\ntransactions involve Frequent Receivers, representing a small fraction of the\ntotal value exchanged according to the blockchain, but a significant fraction\nof all payments, raising concerns about the centralization of the Bitcoin\necosystem. We also conduct a weekly pattern analysis of activity, providing\ninsights into the geographical location of Bitcoin users and allowing us to\nquantify the bias of a well-known dataset for actor identification.\n"
    },
    {
        "paper_id": 2307.08628,
        "authors": "Michele Azzone, Roberto Baviera",
        "title": "Is (independent) subordination relevant in option pricing?",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Monroe (1978) demonstrates that any local semimartingale can be represented\nas a time-changed Brownian Motion (BM). A natural question arises: does this\nrepresentation theorem hold when the BM and the time-change are independent? We\nprove that a local semimartingale is not equivalent to a BM with a time-change\nthat is independent from the BM. Our result is obtained utilizing a class of\nadditive processes: the additive normal tempered stable (ATS). This class of\nprocesses exhibits an exceptional ability to accurately calibrate the equity\nvolatility surface. We notice that the sub-class of additive processes that can\nbe obtained with an independent additive subordination is incompatible with\nmarket data and shows significantly worse calibration performances than the\nATS, especially on short time maturities. These results have been observed\nevery business day in a semester on a dataset of S&P 500 and EURO STOXX 50\noptions.\n"
    },
    {
        "paper_id": 2307.08649,
        "authors": "Lili Wang, Chenghan Huang, Chongyang Gao, Weicheng Ma, and Soroush\n  Vosoughi",
        "title": "Joint Latent Topic Discovery and Expectation Modeling for Financial\n  Markets",
        "comments": "In Advances in Knowledge Discovery and Data Mining 2023 (PAKDD 2023)",
        "journal-ref": null,
        "doi": "10.1007/978-3-031-33380-4_4",
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  In the pursuit of accurate and scalable quantitative methods for financial\nmarket analysis, the focus has shifted from individual stock models to those\ncapturing interrelations between companies and their stocks. However, current\nrelational stock methods are limited by their reliance on predefined stock\nrelationships and the exclusive consideration of immediate effects. To address\nthese limitations, we present a groundbreaking framework for financial market\nanalysis. This approach, to our knowledge, is the first to jointly model\ninvestor expectations and automatically mine latent stock relationships.\nComprehensive experiments conducted on China's CSI 300, one of the world's\nlargest markets, demonstrate that our model consistently achieves an annual\nreturn exceeding 10%. This performance surpasses existing benchmarks, setting a\nnew state-of-the-art standard in stock return prediction and multiyear trading\nsimulations (i.e., backtesting).\n"
    },
    {
        "paper_id": 2307.0865,
        "authors": "Supawich Puengdang, Worawate Ausawalaithong, Phiratath Nopratanawong,\n  Narongdech Keeratipranon, Chayut Wongkamthong",
        "title": "Thailand Asset Value Estimation Using Aerial or Satellite Imagery",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Real estate is a critical sector in Thailand's economy, which has led to a\ngrowing demand for a more accurate land price prediction approach. Traditional\nmethods of land price prediction, such as the weighted quality score (WQS), are\nlimited due to their reliance on subjective criteria and their lack of\nconsideration for spatial variables. In this study, we utilize aerial or\nsatellite imageries from Google Map API to enhance land price prediction models\nfrom the dataset provided by Kasikorn Business Technology Group (KBTG). We\npropose a similarity-based asset valuation model that uses a Siamese-inspired\nNeural Network with pretrained EfficientNet architecture to assess the\nsimilarity between pairs of lands. By ensembling deep learning and tree-based\nmodels, we achieve an area under the ROC curve (AUC) of approximately 0.81,\noutperforming the baseline model that used only tabular data. The appraisal\nprices of nearby lands with similarity scores higher than a predefined\nthreshold were used for weighted averaging to predict the reasonable price of\nthe land in question. At 20\\% mean absolute percentage error (MAPE), we improve\nthe recall from 59.26\\% to 69.55\\%, indicating a more accurate and reliable\napproach to predicting land prices. Our model, which is empowered by a more\ncomprehensive view of land use and environmental factors from aerial or\nsatellite imageries, provides a more precise, data-driven, and adaptive\napproach for land valuation in Thailand.\n"
    },
    {
        "paper_id": 2307.08651,
        "authors": "Ehsan Azmoodeh and Ozan H\\\"ur",
        "title": "Multi-fractional Stochastic Dominance: Mathematical Foundations",
        "comments": "Some parts of the manuscript under further investigation",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  In the landmark article \\cite{Muller}, M\\\"uller et. al. introduced the notion\nof fractional stochastic dominance (SD) to interpolate between first and second\nSD relations. In this article, we introduce a novel family of\n\\textit{multi-fractional} stochastic orders that generalizes fractional SD in a\nnatural manner. The family of multi-fractional SD is parametrized by an\narbitrary non-decreasing function $\\gamma$ ranging between $0$ and $1$ which\nprovides the feature of local interpolation rather than a global one. We show\nthat the multi-fractional $(1+\\gamma)$-SD is generated by a class of increasing\nutility functions allowing local non-concavity where the steepness of the\nnon-concavity depends on its location and it is controlled by function\n$\\gamma$. We also introduced the notion of \\text{local greediness} that allows\nus, among other things, to systematically study multi-fractional utility class.\nThe multi-fractional utility class is well-suited for representing a decision\nmaker's preferences in terms of risk aversion and greediness at a local level.\nSeveral basic properties as well as illustrating examples are presented.\n"
    },
    {
        "paper_id": 2307.08665,
        "authors": "Nelson Kyakutwika and Bruce Bartlett",
        "title": "Bayesian Forecasting of Stock Returns on the JSE using Simultaneous\n  Graphical Dynamic Linear Models",
        "comments": "28 pages, 3 figures, 8 tables, Submitted to Investment Analysts\n  Journal",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Cross-series dependencies are crucial in obtaining accurate forecasts when\nforecasting a multivariate time series. Simultaneous Graphical Dynamic Linear\nModels (SGDLMs) are Bayesian models that elegantly capture cross-series\ndependencies. This study forecasts returns of a 40-dimensional time series of\nstock data from the Johannesburg Stock Exchange (JSE) using SGDLMs. The SGDLM\napproach involves constructing a customised dynamic linear model (DLM) for each\nunivariate time series. At each time point, the DLMs are recoupled using\nimportance sampling and decoupled using mean-field variational Bayes. Our\nresults suggest that SGDLMs forecast stock data on the JSE accurately and\nrespond to market gyrations effectively.\n"
    },
    {
        "paper_id": 2307.08666,
        "authors": "Alexis Rodriguez Carranza, Jos\\'e Luis Ponte Bejarano, Juan Carlos\n  Ponte Bejarano, Segundo Eloy Soto Abanto",
        "title": "Shannon entropy to quantify complexity in the financial market",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  In this paper we study the complexity in the information traffic that occurs\nin the peruvian financial market, using the Shannon entropy. Different series\nof prices of shares traded on the Lima stock exchange are used to reconstruct\nthe unknown dynamics. We present numerical simulations on the reconstructed\ndynamics and we calculate the Shannon entropy to measure its complexity\n"
    },
    {
        "paper_id": 2307.08675,
        "authors": "Yifan He and Svetlozar Rachev",
        "title": "Exploring Implied Certainty Equivalent Rates in Financial Markets:\n  Empirical Analysis and Application to the Electric Vehicle Industry",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  In this paper, we mainly study the impact of the implied certainty equivalent\nrate on investment in financial markets. First, we derived the mathematical\nexpression of the implied certainty equivalent rate by using put-call parity,\nand then we selected some company stocks and options; we considered the\nbest-performing and worst-performing company stocks and options from the\nbeginning of 2023 to the present for empirical research. By visualizing the\nrelationship between the time to maturity, moneyness, and implied certainty\nequivalent rate of these options, we have obtained a universal conclusion -- a\npositive implied certainty equivalent rate is more suitable for investment than\na negative implied certainty equivalent rate, but for a positive implied\ncertainty equivalent rate, a larger value also means a higher investment risk.\nNext, we applied these results to the electric vehicle industry, and by\ncomparing several well-known US electric vehicle production companies, we\nfurther strengthened our conclusions. Finally, we give a warning concerning\nrisk, that is, investment in the financial market should not focus solely on\nthe implied certainty equivalent rate, because investment is not an easy task,\nand many factors need to be considered, including some factors that are\ndifficult to predict with models.\n"
    },
    {
        "paper_id": 2307.08768,
        "authors": "Hamed Amini, Maxim Bichuch, Zachary Feinstein",
        "title": "Decentralized Prediction Markets and Sports Books",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Prediction markets allow traders to bet on potential future outcomes. These\nmarkets exist for weather, political, sports, and economic forecasting. Within\nthis work we consider a decentralized framework for prediction markets using\nautomated market makers (AMMs). Specifically, we construct a liquidity-based\nAMM structure for prediction markets that, under reasonable axioms on the\nunderlying utility function, satisfy meaningful financial properties on the\ncost of betting and the resulting pricing oracle. Importantly, we study how\nliquidity can be pooled or withdrawn from the AMM and the resulting\nimplications to the market behavior. In considering this decentralized\nframework, we additionally propose financially meaningful fees that can be\ncollected for trading to compensate the liquidity providers for their vital\nmarket function.\n"
    },
    {
        "paper_id": 2307.08853,
        "authors": "Apostolos Ampountolas",
        "title": "Comparative Analysis of Machine Learning, Hybrid, and Deep Learning\n  Forecasting Models Evidence from European Financial Markets and Bitcoins",
        "comments": null,
        "journal-ref": "Forecasting 2023",
        "doi": "10.3390/forecast5020026",
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  This study analyzes the transmission of market uncertainty on key European\nfinancial markets and the cryptocurrency market over an extended period,\nencompassing the pre, during, and post-pandemic periods. Daily financial market\nindices and price observations are used to assess the forecasting models. We\ncompare statistical, machine learning, and deep learning forecasting models to\nevaluate the financial markets, such as the ARIMA, hybrid ETS-ANN, and kNN\npredictive models. The study results indicate that predicting financial market\nfluctuations is challenging, and the accuracy levels are generally low in\nseveral instances. ARIMA and hybrid ETS-ANN models perform better over extended\nperiods compared to the kNN model, with ARIMA being the best-performing model\nin 2018-2021 and the hybrid ETS-ANN model being the best-performing model in\nmost of the other subperiods. Still, the kNN model outperforms the others in\nseveral periods, depending on the observed accuracy measure. Researchers have\nadvocated using parametric and non-parametric modeling combinations to generate\nbetter results. In this study, the results suggest that the hybrid ETS-ANN\nmodel is the best-performing model despite its moderate level of accuracy.\nThus, the hybrid ETS-ANN model is a promising financial time series forecasting\napproach. The findings offer financial analysts an additional source that can\nprovide valuable insights for investment decisions.\n"
    },
    {
        "paper_id": 2307.08861,
        "authors": "Mikhail V. Sokolov",
        "title": "An effective interest rate cap: a clarification",
        "comments": "34 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The national legislation of many countries imposes restrictions on lending\nrates known as interest rate caps. In most cases, the effective (rather than\nnominal) interest rate is restricted, which includes all commissions and fees\nassociated with a loan. Typically, the generic wording of this restriction is\nambiguous in three respects. First, the literature provides several\nnonequivalent concepts of internal rate of return (IRR). Since the effective\ninterest rate is the IRR of the cash flow stream of a loan, the wording should\nspecify which concept of IRR is used. Second, most definitions of IRR are\npartial in the sense that there are cash flow streams that have no IRR. Thus,\nthe wording is vague for loans that have no IRR. Third, when loan advances and\nrepayments alternate in time, the respective roles of the borrower and lender\ncan be blurred. In this case, it is unclear to which of the parties of the loan\ncontract the regulatory measure should be addressed and, if it treats the\ncontract as illegal, which party should be brought to justice. This paper aims\nto resolve these ambiguities. We start by clarifying the concept of IRR. We\naxiomatize the conventional definition of IRR (as a unique root of the IRR\npolynomial) and show that any extension to a larger domain necessarily violates\na natural axiom. Given this result, we show how to derive an effective interest\nrate cap. We prove that there is a unique solution consistent with a set of\nreasonable conditions. Finally, we suggest a way to avoid the problem of\nidentification of the roles of the loan contract parties by imposing both a\nfloor and a ceiling on lending rates.\n"
    },
    {
        "paper_id": 2307.08869,
        "authors": "Hector Galindo-Silva and Paula Herrera-Id\\'arraga",
        "title": "Culture, Gender, and Labor Force Participation: Evidence from Colombia",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  This study investigates the impact of integrating gender equality into the\nColombian constitution of 1991 on attitudes towards gender equality,\nexperiences of gender-based discrimination, and labor market participation.\nUsing a difference-in-discontinuities framework, we compare individuals exposed\nto mandatory high school courses on the Constitution with those who were not\nexposed. Our findings show a significant increase in labor market\nparticipation, primarily driven by women. Exposure to these courses also shapes\nattitudes towards gender equality, with men demonstrating greater support.\nWomen report experiencing less gender-based discrimination. Importantly, our\nresults suggest that women's increased labor market participation is unlikely\ndue to reduced barriers from male partners. A disparity in opinions regarding\ntraditional gender norms concerning household domains is observed between men\nand women, highlighting an ongoing power struggle within the home. However, the\npresence of a younger woman in the household appears to influence men's more\npositive view of gender equality, potentially indicating a desire to empower\nyounger women in their future lives. These findings highlight the crucial role\nof cultural shocks and the constitutional inclusion of women's rights in\nshaping labor market dynamics.\n"
    },
    {
        "paper_id": 2307.08968,
        "authors": "Anton Bobrov and James Traina",
        "title": "The Beginning of the Trend: Interest Rates, Profits, and Markups",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Recent highly cited research uses time-series evidence to argue the decline\nin interest rates led to a large rise in economic profits and markups. We show\nthe size of these estimates is sensitive to the sample start date: The rise in\nmarkups from 1984 to 2019 is 14% larger than from 1980 to 2019, a difference\namounting to a $3000 change in income per worker in 2019. The sensitivity comes\nfrom a peak in interest rates in 1984, during a period of heightened\nvolatility. Our results imply researchers should justify their time-series\nselection and incorporate sensitivity checks in their analysis.\n"
    },
    {
        "paper_id": 2307.09035,
        "authors": "Shun-Yang Lee, Julian Runge, Daniel Yoo, Yakov Bart, Anett Gyurak,\n  J.W. Schneider",
        "title": "COVID-19 Demand Shocks Revisited: Did Advertising Technology Help\n  Mitigate Adverse Consequences for Small and Midsize Businesses?",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Research has investigated the impact of the COVID-19 pandemic on business\nperformance and survival, indicating particularly adverse effects for small and\nmidsize businesses (SMBs). Yet only limited work has examined whether and how\nonline advertising technology may have helped shape these outcomes,\nparticularly for SMBs. The aim of this study is to address this gap. By\nconstructing and analyzing a novel data set of more than 60,000 businesses in\n49 countries, we examine the impact of government lockdowns on business\nsurvival. Using discrete-time survival models with instrumental variables and\nstaggered difference-in-differences estimators, we find that government\nlockdowns increased the likelihood of SMB closure around the world but that use\nof online advertising technology attenuates this adverse effect. The findings\nshow heterogeneity in country, industry, and business size, which we discuss\nand is consistent with theoretical expectations.\n"
    },
    {
        "paper_id": 2307.09077,
        "authors": "Luca Mucciante and Alessio Sancetta",
        "title": "Estimation of an Order Book Dependent Hawkes Process for Large Datasets",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  A point process for event arrivals in high frequency trading is presented.\nThe intensity is the product of a Hawkes process and high dimensional functions\nof covariates derived from the order book. Conditions for stationarity of the\nprocess are stated. An algorithm is presented to estimate the model even in the\npresence of billions of data points, possibly mapping covariates into a high\ndimensional space. The large sample size can be common for high frequency data\napplications using multiple liquid instruments. Convergence of the algorithm is\nshown, consistency results under weak conditions is established, and a test\nstatistic to assess out of sample performance of different model specifications\nis suggested. The methodology is applied to the study of four stocks that trade\non the New York Stock Exchange (NYSE). The out of sample testing procedure\nsuggests that capturing the nonlinearity of the order book information adds\nvalue to the self exciting nature of high frequency trading events.\n"
    },
    {
        "paper_id": 2307.09137,
        "authors": "Apostolos Ampountolas",
        "title": "The Effect of COVID-19 on Cryptocurrencies and the Stock Market\n  Volatility -- A Two-Stage DCC-EGARCH Model Analysis",
        "comments": null,
        "journal-ref": "Journal of Risk and Financial Management 2023",
        "doi": "10.3390/jrfm16010025",
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  This research examines the correlations between the return volatility of\ncryptocurrencies, global stock market indices, and the spillover effects of the\nCOVID-19 pandemic. For this purpose, we employed a two-stage multivariate\nvolatility exponential GARCH (EGARCH) model with an integrated dynamic\nconditional correlation (DCC) approach to measure the impact on the financial\nportfolio returns from 2019 to 2020. Moreover, we used value-at-risk (VaR) and\nvalue-at-risk measurements based on the Cornish-Fisher expansion (CFVaR). The\nempirical results show significant long- and short-term spillover effects. The\ntwo-stage multivariate EGARCH model's results show that the conditional\nvolatilities of both asset portfolios surge more after positive news and\nrespond well to previous shocks. As a result, financial assets have low\nunconditional volatility and the lowest risk when there are no external\ninterruptions. Despite the financial assets' sensitivity to shocks, they\nexhibit some resistance to fluctuations in market confidence. The VaR\nperformance comparison results with the assets portfolios differ. During the\nCOVID-19 outbreak, the Dow (DJI) index reports VaR's highest loss, followed by\nthe S&P500. Conversely, the CFVaR reports negative risk results for the entire\ncryptocurrency portfolio during the pandemic, except for the Ethereum (ETH).\n"
    },
    {
        "paper_id": 2307.09216,
        "authors": "Peter Bank, Christian Bayer, Peter K. Friz, Luca Pelizzari",
        "title": "Rough PDEs for local stochastic volatility models",
        "comments": "36 pages, 2 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this work, we introduce a novel pricing methodology in general, possibly\nnon-Markovian local stochastic volatility (LSV) models. We observe that by\nconditioning the LSV dynamics on the Brownian motion that drives the\nvolatility, one obtains a time-inhomogeneous Markov process. Using tools from\nrough path theory, we describe how to precisely understand the conditional LSV\ndynamics and reveal their Markovian nature. The latter allows us to connect the\nconditional dynamics to so-called rough partial differential equations (RPDEs),\nthrough a Feynman-Kac type of formula. In terms of European pricing,\nconditional on realizations of one Brownian motion, we can compute conditional\noption prices by solving the corresponding linear RPDEs, and then average over\nall samples to find unconditional prices. Our approach depends only minimally\non the specification of the volatility, making it applicable for a wide range\nof classical and rough LSV models, and it establishes a PDE pricing method for\nnon-Markovian models. Finally, we present a first glimpse at numerical methods\nfor RPDEs and apply them to price European options in several rough LSV models.\n"
    },
    {
        "paper_id": 2307.09251,
        "authors": "Natalia Zdanowska",
        "title": "Socio-spatial Inequalities in a Context of \"Great Economic Wealth\". Case\n  study of neighbourhoods of Luxembourg City",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  In spite of being one of the smallest and wealthiest countries in the\nEuropean Union in terms of GDP per capita, Luxembourg is facing socio-economic\nchallenges due to recent rapid urban transformations. This article contributes\nby approaching this phenomenon at the most granular and rarely analysed\ngeographical level - the neighbourhoods of the capital, Luxembourg City. Based\non collected empirical data covering various socio-demographic dimensions for\n2020-2021, an ascending hierarchical classification on principal components is\nset out to establish neighbourhoods' socio-spatial patterns. In addition, Chi2\ntests are carried out to examine residents' socio-demographic characteristics\nand determine income inequalities in neighbourhoods. The results reveal a clear\nsocio-spatial divide along a north-west south-east axis. Moreover, classical\nfactors such as gender or citizenship differences are revealed to be poorly\ndeterminant of income inequalities compared with the proportion of social\nbenefits recipients and single residents.\n"
    }
]