[
    {
        "paper_id": 2206.13012,
        "authors": "Pascal Michaillat, Emmanuel Saez",
        "title": "$u^* = \\sqrt{uv}$",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  This paper aims to compute the unemployment rate $u^*$ that is consistent\nwith full employment in the United States. First, it argues that the most\nappropriate economic translation of the legal notion of full employment is\nsocial efficiency. Here efficiency requires to minimize the nonproductive use\nof labor -- both unemployment and recruiting. The nonproductive use of labor is\nmeasured by the number of jobseekers and vacancies, $u + v$. Through the\nBeveridge curve, the numbers of vacancies and jobseekers are inversely related,\n$uv = \\text{constant}$. With such symmetry the labor market is efficient when\nthere are as many jobseekers as vacancies ($u = v$), inefficiently tight when\nthere are more vacancies than jobseekers ($v > u$), and inefficiently slack\nwhen there are more jobseekers than vacancies ($u > v$). Accordingly, the\nfull-employment rate of unemployment (FERU) is the geometric average of the\nunemployment and vacancy rates: $u^* = \\sqrt{uv}$. Between 1930 and 2023, the\nFERU averages $4.1\\%$ and is quite stable -- it always remains between $2.5\\%$\nand $6.6\\%$.\n"
    },
    {
        "paper_id": 2206.13237,
        "authors": "Sebastian Frischbier, Jawad Tahir, Christoph Doblander, Arne Hormann,\n  Ruben Mayer, Hans-Arno Jacobsen",
        "title": "The DEBS 2022 Grand Challenge: Detecting Trading Trends in Financial\n  Tick Data",
        "comments": "Author's version of the work, definitive Version of Record published\n  in the proceedings of The 16th ACM International Conference on Distributed\n  and Event-based Systems (DEBS '22); 7 pages, 7 figures",
        "journal-ref": null,
        "doi": "10.1145/3524860.3539645",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The DEBS Grand Challenge (GC) is an annual programming competition open to\npractitioners from both academia and industry. The GC 2022 edition focuses on\nreal-time complex event processing of high-volume tick data provided by Infront\nFinancial Technology GmbH. The goal of the challenge is to efficiently compute\nspecific trend indicators and detect patterns in these indicators like those\nused by real-life traders to decide on buying or selling in financial markets.\nThe data set Trading Data used for benchmarking contains 289 million tick\nevents from approximately 5500+ financial instruments that had been traded on\nthe three major exchanges Amsterdam (NL), Paris (FR), and Frankfurt am Main\n(GER) over the course of a full week in 2021. The data set is made publicly\navailable. In addition to correctness and performance, submissions must\nexplicitly focus on reusability and practicability. Hence, participants must\naddress specific nonfunctional requirements and are asked to build upon\nopen-source platforms. This paper describes the required scenario and the data\nset Trading Data, defines the queries of the problem statement, and explains\nthe enhancements made to the evaluation platform Challenger that handles data\ndistribution, dynamic subscriptions, and remote evaluation of the submissions.\n"
    },
    {
        "paper_id": 2206.13341,
        "authors": "Lijun Bo, Shihua Wang, Xiang Yu",
        "title": "A mean field game approach to equilibrium consumption under external\n  habit formation",
        "comments": "Keywords: Catching up with the Joneses, linear habit formation,\n  multiplicative habit formation, mean field equilibrium, approximate Nash\n  equilibrium",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper studies the equilibrium consumption under external habit formation\nin a large population of agents. We first formulate problems under two types of\nconventional habit formation preferences, namely linear and multiplicative\nexternal habit formation, in a mean field game framework. In a log-normal\nmarket model with the asset specialization, we characterize one mean field\nequilibrium in analytical form in each problem, allowing us to understand some\nquantitative properties of the equilibrium strategy and conclude some financial\nimplications caused by consumption habits from a mean-field perspective. In\neach problem with n agents, we construct an approximate Nash equilibrium for\nthe n-player game using the obtained mean field equilibrium when n is\nsufficiently large. The explicit convergence order in each problem can also be\nobtained.\n"
    },
    {
        "paper_id": 2206.13489,
        "authors": "Meena Jagadeesan, Nikhil Garg, Jacob Steinhardt",
        "title": "Supply-Side Equilibria in Recommender Systems",
        "comments": "Appeared at NeurIPS 2023; this is the full version",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Algorithmic recommender systems such as Spotify and Netflix affect not only\nconsumer behavior but also producer incentives. Producers seek to create\ncontent that will be shown by the recommendation algorithm, which can impact\nboth the diversity and quality of their content. In this work, we investigate\nthe resulting supply-side equilibria in personalized content recommender\nsystems. We model users and content as $D$-dimensional vectors, the\nrecommendation algorithm as showing each user the content with highest dot\nproduct, and producers as maximizing the number of users who are recommended\ntheir content minus the cost of production. Two key features of our model are\nthat the producer decision space is multi-dimensional and the user base is\nheterogeneous, which contrasts with classical low-dimensional models.\n  Multi-dimensionality and heterogeneity create the potential for\nspecialization, where different producers create different types of content at\nequilibrium. Using a duality argument, we derive necessary and sufficient\nconditions for whether specialization occurs: these conditions depend on the\nextent to which users are heterogeneous and to which producers can perform well\non all dimensions at once without incurring a high cost. Then, we characterize\nthe distribution of content at equilibrium in concrete settings with two\npopulations of users. Lastly, we show that specialization can enable producers\nto achieve positive profit at equilibrium, which means that specialization can\nreduce the competitiveness of the marketplace. At a conceptual level, our\nanalysis of supply-side competition takes a step towards elucidating how\npersonalized recommendations shape the marketplace of digital goods, and\ntowards understanding what new phenomena arise in multi-dimensional competitive\nsettings.\n"
    },
    {
        "paper_id": 2206.13641,
        "authors": "Pablo Bra\\~nas-Garza, Lorenzo Ductor, Jarom\\'ir Kov\\'ar\\'ik",
        "title": "The role of unobservable characteristics in friendship network formation",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Inbreeding homophily is a prevalent feature of human social networks with\nimportant individual and group-level social, economic, and health consequences.\nThe literature has proposed an overwhelming number of dimensions along which\nhuman relationships might sort, without proposing a unified\nempirically-grounded framework for their categorization. We exploit rich data\non a sample of University freshmen with very similar characteristic - age, race\nand education- and contrast the relative importance of observable vs.\nunobservables characteristics in their friendship formation. We employ Bayesian\nModel Averaging, a methodology explicitly designed to target model uncertainty\nand to assess the robustness of each candidate attribute while predicting\nfriendships. We show that, while observable features such as assignment of\nstudents to sections, gender, and smoking are robust key determinants of\nwhether two individuals befriend each other, unobservable attributes, such as\npersonality, cognitive abilities, economic preferences, or socio-economic\naspects, are largely sensible to the model specification, and are not important\npredictors of friendships.\n"
    },
    {
        "paper_id": 2206.13652,
        "authors": "Michele Belot and Guglielmo Briscese",
        "title": "Reducing Polarization on Abortion, Guns and Immigration: An Experimental\n  Study",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  We study individuals' willingness to engage with others who hold opposite\nviews on polarizing policies. A representative sample of 2,507 Americans are\ngiven the opportunity to listen to recordings of fellow countrymen and women\nexpressing their views on immigration, abortion laws and gun ownership laws. We\nfind that most Americans (more than two-thirds) are willing to listen to a view\nopposite to theirs, and a fraction (ten percent) reports changing their views\nas a result. We also test whether emphasizing having common grounds with those\nwho think differently helps bridging views. We identify principles the vast\nmajority of people agree upon: (1) a set of fundamental human rights, and (2) a\nset of simple behavioral etiquette rules. A random subsample of people are made\nexplicitly aware they share common views, either on human rights (one-third of\nthe sample) or etiquette rules (another one-third of the sample), before they\nhave the opportunity to listen to different views. We find that the treatments\ninduce people to adjust their views towards the center on abortion and\nimmigration, relative to a control group, thus reducing polarization.\n"
    },
    {
        "paper_id": 2206.13675,
        "authors": "Ashani Amarasinghe, Paul A. Raschky",
        "title": "Competing for Attention -- The Effect of Talk Radio on Elections and\n  Political Polarization in the US",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper studies the effects of talk radio, specifically the Rush Limbaugh\nShow, on electoral outcomes and attitude polarization in the U.S. We propose a\nnovel identification strategy that considers the radio space in each county as\na market where multiple stations are competing for listeners' attention. Our\nmeasure of competition is a spatial Herfindahl-Hirschman Index (HHI) in radio\nfrequencies. To address endogeneity concerns, we exploit the variation in\ncompetition based on accidental frequency overlaps in a county, conditional on\nthe overall level of radio frequency competition. We find that counties with\nhigher exposure to the Rush Limbaugh Show have a systematically higher vote\nshare for Donald Trump in the 2016 and 2020 U.S. presidential elections.\nCombining our county-level Rush Limbaugh Show exposure measure with individual\nsurvey data reveals that self-identifying Republicans in counties with higher\nexposure to the Show express more conservative political views, while\nself-identifying Democrats in these same counties express more moderate\npolitical views. Taken together, these findings provide some of the first\ninsights on the effects of contemporary talk radio on political outcomes, both\nat the aggregate and individual level.\n"
    },
    {
        "paper_id": 2206.13679,
        "authors": "Xia Han and Liyuan Lin and Ruodu Wang",
        "title": "Diversification quotients: Quantifying diversification via risk measures",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We establish the first axiomatic theory for diversification indices using six\nintuitive axioms: non-negativity, location invariance, scale invariance,\nrationality, normalization, and continuity. The unique class of indices\nsatisfying these axioms, called the diversification quotients (DQs), are\ndefined based on a parametric family of risk measures. A further axiom of\nportfolio convexity pins down DQ based on coherent risk measures. DQ has many\nattractive properties, and it can address several theoretical and practical\nlimitations of existing indices. In particular, for the popular risk measures\nValue-at-Risk and Expected Shortfall, the corresponding DQ admits simple\nformulas and it is efficient to optimize in portfolio selection. Moreover, it\ncan properly capture tail heaviness and common shocks, which are neglected by\ntraditional diversification indices. When illustrated with financial data, DQ\nis intuitive to interpret, and its performance is competitive against other\ndiversification indices.\n"
    },
    {
        "paper_id": 2206.13751,
        "authors": "Matthew Ferranti",
        "title": "Estimating the Currency Composition of Foreign Exchange Reserves",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Central banks manage about \\$12 trillion in foreign exchange reserves,\ninfluencing global exchange rates and asset prices. However, some of the\nlargest holders of reserves report minimal information about their currency\ncomposition, hindering empirical analysis. I describe a Hidden Markov Model to\nestimate the composition of a central bank's reserves by relating the\nfluctuation in the portfolio's valuation to the exchange rates of major reserve\ncurrencies. I apply the model to China and Singapore, two countries that\ncollectively hold about \\$3.4 trillion in reserves and conceal their\ncomposition. I find that both China's reserve composition likely resembles the\nglobal average, while Singapore probably holds fewer US dollars.\n"
    },
    {
        "paper_id": 2206.1386,
        "authors": "Anish Rai, Salam Rabindrajit Luwang, Md Nurujjaman, Chittaranjan Hens,\n  Pratyay Kuila and Kanish Debnath",
        "title": "Detection and Forecasting of Extreme event in Stock Price Triggered by\n  Fundamental, Technical, and External Factors",
        "comments": "13 pages",
        "journal-ref": null,
        "doi": "10.1016/j.chaos.2023.113716",
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  The sporadic large fluctuations are seen in the stock market due to changes\nin fundamental parameters, technical setups, and external factors. These large\nfluctuations are termed as Extreme Events (EE). The EEs may be positive or\nnegative depending on the impact of these factors. During such events, the\nstock price time series is found to be nonstationary. Hence, the Hilbert-Huang\ntransformation (HHT) is used to identify EEs based on their high instantaneous\nenergy ($IE$) concentration. The analysis shows that the $IE$ concentration in\nthe stock price is very high during both positive and negative EE with\n$IE>E_{\\mu}+4\\sigma,$ where $E_{\\mu}$ and $\\sigma$ are the mean energy and\nstandard deviation of energy, respectively. Further, support vector regression\nis used to predict the stock price during an EE, with the close price being the\nmost helpful input than the open-high-low-close (OHLC) inputs. The maximum\nprediction accuracy for one step using close price and OHLC prices are 95.98\\%\nand 95.64\\% respectively. Whereas, for the two steps prediction, the accuracies\nare 94.09\\% and 93.58\\% respectively. The EEs found from the predicted time\nseries shows similar statistical characteristics that were obtained from the\noriginal data. The analysis emphasizes the importance of monitoring factors\nthat lead to EEs for a compelling entry or exit strategy as investors can gain\nor lose significant amounts of capital due to these events.\n"
    },
    {
        "paper_id": 2206.13895,
        "authors": "Alessio Ciullo, Eric Strobl, Simona Meiler, Olivia Martius, David N.\n  Bresch",
        "title": "Increasing countries financial resilience through global catastrophe\n  risk pooling",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Extreme weather events can have severe impacts on national economies, leading\nthe recovery of low- to middle-income countries to become reliant on foreign\nfinancial aid. Foreign aid, however, is slow and uncertain. Therefore, the\nSendai Framework and the Paris Agreement advocate for more resilient financial\ninstruments like sovereign catastrophe risk pools. Existing pools, however,\nmight not fully exploit financial resilience potentials because they were not\ndesigned with the goal of maximizing risk diversification and they pool risk\nonly regionally. To address this, we introduce a method that forms pools\nmaximizing risk diversification and which selects countries with low bilateral\ncorrelations or low shares in the pool risk. We apply the method to explore the\nbenefits of global pooling with respect to regional pooling. We find that\nglobal pooling increases risk diversification, it lowers countries shares in\nthe pool risk and it increases the number of countries profiting from risk\npooling.\n"
    },
    {
        "paper_id": 2206.13913,
        "authors": "Stefan Tappe",
        "title": "Invariant cones for jump-diffusions in infinite dimensions",
        "comments": "46 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper we provide sufficient conditions for stochastic invariance of\nclosed convex cones for stochastic partial differential equations (SPDEs) of\njump-diffusion type, and clarify when these conditions are necessary. Our\nresults apply to the positive cone of abstract $L^2$-spaces. Furthermore, we\npresent a series of applications, where we investigate SPDEs arising in natural\nsciences and economics.\n"
    },
    {
        "paper_id": 2206.14015,
        "authors": "David Criens and Lars Niemann",
        "title": "Robust utility maximization with nonlinear continuous semimartingales",
        "comments": "To appear in \"Mathematics and Financial Economics\"",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper we study a robust utility maximization problem in continuous\ntime under model uncertainty. The model uncertainty is governed by a continuous\nsemimartingale with uncertain local characteristics. Here, the differential\ncharacteristics are prescribed by a set-valued function that depends on time\nand path. We show that the robust utility maximization problem is in duality\nwith a conjugate problem, and we study the existence of optimal portfolios for\nlogarithmic, exponential and power utilities.\n"
    },
    {
        "paper_id": 2206.14114,
        "authors": "Mathieu Rosenbaum and Jianfei Zhang",
        "title": "On the universality of the volatility formation process: when machine\n  learning and rough volatility agree",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We train an LSTM network based on a pooled dataset made of hundreds of liquid\nstocks aiming to forecast the next daily realized volatility for all stocks.\nShowing the consistent outperformance of this universal LSTM relative to other\nasset-specific parametric models, we uncover nonparametric evidences of a\nuniversal volatility formation mechanism across assets relating past market\nrealizations, including daily returns and volatilities, to current\nvolatilities. A parsimonious parametric forecasting device combining the rough\nfractional stochastic volatility and quadratic rough Heston models with fixed\nparameters results in the same level of performance as the universal LSTM,\nwhich confirms the universality of the volatility formation process from a\nparametric perspective.\n"
    },
    {
        "paper_id": 2206.1413,
        "authors": "Yuchao Fan",
        "title": "Dissecting the dot-com bubble in the 1990s NASDAQ",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  In this paper, I revisit Phillips, Wu and Yu's seminal 2011 paper on testing\nfor the dot-com bubble. I apply recent advancements of their methods to\nindividual Nasdaq stocks and use a novel specification for fundamentals. To\naddress a divide in the literature, I generate a detailed sectoral breakdown of\nthe dot-com bubble. I find that it comprised multiple overlapping episodes of\nexuberance and that there were indeed two starting dates for internet\nexuberance.\n"
    },
    {
        "paper_id": 2206.14267,
        "authors": "Frensi Zejnullahu, Maurice Moser, Joerg Osterrieder",
        "title": "Applications of Reinforcement Learning in Finance -- Trading with a\n  Double Deep Q-Network",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  This paper presents a Double Deep Q-Network algorithm for trading single\nassets, namely the E-mini S&P 500 continuous futures contract. We use a proven\nsetup as the foundation for our environment with multiple extensions. The\nfeatures of our trading agent are constantly being expanded to include\nadditional assets such as commodities, resulting in four models. We also\nrespond to environmental conditions, including costs and crises. Our trading\nagent is first trained for a specific time period and tested on new data and\ncompared with the long-and-hold strategy as a benchmark (market). We analyze\nthe differences between the various models and the in-sample/out-of-sample\nperformance with respect to the environment. The experimental results show that\nthe trading agent follows an appropriate behavior. It can adjust its policy to\ndifferent circumstances, such as more extensive use of the neutral position\nwhen trading costs are present. Furthermore, the net asset value exceeded that\nof the benchmark, and the agent outperformed the market in the test set. We\nprovide initial insights into the behavior of an agent in a financial domain\nusing a DDQN algorithm. The results of this study can be used for further\ndevelopment.\n"
    },
    {
        "paper_id": 2206.14275,
        "authors": "Timo Dimitriadis, Yannick Hoga",
        "title": "Dynamic CoVaR Modeling",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The popular systemic risk measure CoVaR (conditional Value-at-Risk) is widely\nused in economics and finance. Formally, it is defined as a large quantile of\none variable (e.g., losses in the financial system) conditional on some other\nvariable (e.g., losses in a bank's shares) being in distress. In this article,\nwe propose joint dynamic forecasting models for the Value-at-Risk (VaR) and\nCoVaR. We also introduce a two-step M-estimator for the model parameters\ndrawing on recently proposed bivariate scoring functions for the pair (VaR,\nCoVaR). We prove consistency and asymptotic normality of our parameter\nestimator and analyze its finite-sample properties in simulations. Finally, we\napply a specific subclass of our dynamic forecasting models, which we call\nCoCAViaR models, to log-returns of large US banks. It is shown that our\nCoCAViaR models generate CoVaR predictions that are superior to forecasts\nissued from current benchmark models.\n"
    },
    {
        "paper_id": 2206.14321,
        "authors": "David R. Johnson, Nathan B. Geldner, Jing Liu, Uris Lantz Baldos,\n  Thomas Hertel",
        "title": "Reducing US Biofuels Requirements Mitigates Short-term Impacts of Global\n  Population and Income Growth on Agricultural Environmental Outcomes",
        "comments": "16 pages, 7 figures, 2 tables",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
        "abstract": "  Biobased energy, particularly corn starch-based ethanol and other liquid\nrenewable fuels, are a major element of federal and state energy policies in\nthe United States. These policies are motivated by energy security and climate\nchange mitigation objectives, but corn ethanol does not substantially reduce\ngreenhouse gas emissions when compared to petroleum-based fuels. Corn\nproduction also imposes substantial negative externalities (e.g., nitrogen\nleaching, higher food prices, water scarcity, and indirect land use change). In\nthis paper, we utilize a partial equilibrium model of corn-soy production and\ntrade to analyze the potential of reduced US demand for corn as a biobased\nenergy feedstock to mitigate increases in nitrogen leaching, crop production\nand land use associated with growing global populations and income from 2020 to\n2050. We estimate that a 23% demand reduction would sustain land use and\nnitrogen leaching below 2020 levels through the year 2025, and a 41% reduction\nwould do so through 2030. Outcomes are similar across major watersheds where\ncorn and soy are intensively farmed.\n"
    },
    {
        "paper_id": 2206.14452,
        "authors": "Yiqi Deng and Siu Ming Yiu",
        "title": "Deep Multiple Instance Learning For Forecasting Stock Trends Using\n  Financial News",
        "comments": "17 pages, 4 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  A major source of information can be taken from financial news articles,\nwhich have some correlations about the fluctuation of stock trends. In this\npaper, we investigate the influences of financial news on the stock trends,\nfrom a multi-instance view. The intuition behind this is based on the news\nuncertainty of varying intervals of news occurrences and the lack of annotation\nin every single financial news. Under the scenario of Multiple Instance\nLearning (MIL) where training instances are arranged in bags, and a label is\nassigned for the entire bag instead of instances, we develop a flexible and\nadaptive multi-instance learning model and evaluate its ability in directional\nmovement forecast of Standard & Poors 500 index on financial news dataset.\nSpecifically, we treat each trading day as one bag, with certain amounts of\nnews happening on each trading day as instances in each bag. Experiment results\ndemonstrate that our proposed multi-instance-based framework gains outstanding\nresults in terms of the accuracy of trend prediction, compared with other\nstate-of-art approaches and baselines.\n"
    },
    {
        "paper_id": 2206.14508,
        "authors": "Dar\\'io Blanco-Fern\\'andez, Stephan Leitner, Alexandra Rausch",
        "title": "The benefits of coordination in (over)adaptive virtual teams",
        "comments": "12 pages, 4 figures, submitted to SSC2022",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  The emergence of new organizational forms--such as virtual teams--has brought\nforward some challenges for teams. One of the most relevant challenges is\ncoordinating the decisions of team members who work from different time zones.\nIntuition suggests that task performance should improve if the team members'\ndecisions are coordinated. However, previous research suggests that the effect\nof coordination on task performance is ambiguous. Specifically, the effect of\ncoordination on task performance depends on aspects such as the team members'\nlearning and the changes in team composition over time. This paper aims to\nunderstand how individual learning and team composition moderate the\nrelationship between coordination and task performance. We implement an\nagent-based modeling approach based on the NK-framework to fulfill our research\nobjective. Our results suggest that both factors have moderating effects.\nSpecifically, we find that excessively increasing individual learning is\nharmful for the task performance of fully autonomous teams, but less\ndetrimental for teams that coordinate their decisions. In addition, we find\nthat teams that coordinate their decisions benefit from changing their\ncomposition in the short-term, but fully autonomous teams do not. In\nconclusion, teams that coordinate their decisions benefit more from individual\nlearning and dynamic composition than teams that do not coordinate.\nNevertheless, we should note that the existence of moderating effects does not\nimply that coordination improves task performance. Whether coordination\nimproves task performance depends on the interdependencies between the team\nmembers' decisions.\n"
    },
    {
        "paper_id": 2206.14548,
        "authors": "Pavel Ciaian and Andrej Cupak and Pirmin Fessler and d'Artis Kancs",
        "title": "Environmental-Social-Governance Preferences and Investments in\n  Crypto-Assets",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Individuals invest in Environmental-Social-Governance (ESG)-assets not only\nbecause of (higher) expected returns but also driven by ethical and social\nconsiderations. Less is known about ESG-conscious investor subjective beliefs\nabout crypto-assets and how do these compare to traditional assets.\nControversies surrounding the ESG footprint of certain crypto-asset classes -\nmainly on grounds of their energy-intensive crypto mining - offer a potentially\ninformative object of inquiry. Leveraging a unique representative household\nfinance survey for the Austrian population, we examine whether investors' ESG\npreferences can explain cross-sectional differences in individual portfolio\nexposure to crypto-assets. We find a strong association between investors' ESG\npreferences and the crypto-investment exposure. The ESG-conscious investor\nattention is higher for crypto-assets compared to traditional asset classes\nsuch as bonds and shares.\n"
    },
    {
        "paper_id": 2206.14612,
        "authors": "Damian Clarke and Pilar Larroulet and Daniel Paila\\~nir and Daniela\n  Quintana",
        "title": "Schools as a Safety-net: The Impact of School Closures and Reopenings on\n  Rates of Reporting of Violence Against Children",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  Ongoing school closures and gradual reopenings have been occurring since the\nbeginning of the COVID-19 pandemic. One substantial cost of school closure is\nbreakdown in channels of reporting of violence against children, in which\nschools play a considerable role. There is, however, little evidence\ndocumenting how widespread such a breakdown in reporting of violence against\nchildren has been, and scant evidence exists about potential recovery in\nreporting as schools re-open. We study all formal criminal reports of violence\nagainst children occurring in Chile up to December 2021, covering physical,\npsychological, and sexual violence. This is combined with administrative\nrecords of school re-opening, attendance, and epidemiological and public health\nmeasures. We observe sharp declines in violence reporting at the moment of\nschool closure across all classes of violence studied. Estimated reporting\ndeclines range from -17% (rape), to -43% (sexual abuse). While reports rise\nwith school re-opening, recovery of reporting rates is slow. Conservative\nprojections suggest that reporting gaps remained into the final quarter of\n2021, nearly two years after initial school closures. Our estimates suggest\nthat school closure and incomplete re-opening resulted in around 2,800\n`missing' reports of intra-family violence, 2,000 missing reports of sexual\nassault, and 230 missing reports of rape against children, equivalent to\nbetween 10-25 weeks of reporting in baseline periods. The immediate and longer\nterm impacts of school closures account for between 40-70% of `missing' reports\nin the post-COVID period.\n"
    },
    {
        "paper_id": 2206.14666,
        "authors": "Anthony Coache, Sebastian Jaimungal, \\'Alvaro Cartea",
        "title": "Conditionally Elicitable Dynamic Risk Measures for Deep Reinforcement\n  Learning",
        "comments": "41 pages, 7 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-sa/4.0/",
        "abstract": "  We propose a novel framework to solve risk-sensitive reinforcement learning\n(RL) problems where the agent optimises time-consistent dynamic spectral risk\nmeasures. Based on the notion of conditional elicitability, our methodology\nconstructs (strictly consistent) scoring functions that are used as penalizers\nin the estimation procedure. Our contribution is threefold: we (i) devise an\nefficient approach to estimate a class of dynamic spectral risk measures with\ndeep neural networks, (ii) prove that these dynamic spectral risk measures may\nbe approximated to any arbitrary accuracy using deep neural networks, and (iii)\ndevelop a risk-sensitive actor-critic algorithm that uses full episodes and\ndoes not require any additional nested transitions. We compare our conceptually\nimproved reinforcement learning algorithm with the nested simulation approach\nand illustrate its performance in two settings: statistical arbitrage and\nportfolio allocation on both simulated and real data.\n"
    },
    {
        "paper_id": 2206.1481,
        "authors": "Jeonggil Song",
        "title": "Predicting Economic Welfare with Images on Wealth",
        "comments": "13 pages, 6 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Using images containing information on wealth, this research investigates\nthat pictures are capable of reliably predicting the economic prosperity of\nhouseholds. Without surveys on wealth-related information and human-made\nstandard of wealth quality that the traditional wealth-based approach relied\non, this novel approach makes use of only images posted on Dollar Street as\ninput data on household wealth across 66 countries and predicts the consumption\nor income level of each household using the Convolutional Neural Network (CNN)\nmethod. The best result predicts the log of consumption level with root mean\nsquared error of 0.66 and R-squared of 0.80 in CNN regression problem. In\naddition, this simple model also performs well in classifying extreme poverty\nwith an accuracy of 0.87 and F-beta score of 0.86. Since the model shows a\nhigher performance in the extreme poverty classification when I applied the\ndifferent threshold of poverty lines to countries by their income group, it is\nsuggested that the decision of the World Bank to define poverty lines\ndifferently by income group was valid.\n"
    },
    {
        "paper_id": 2206.14844,
        "authors": "Sebastian Jaimungal, Silvana M. Pesenti, Leandro S\\'anchez-Betancourt",
        "title": "Minimal Kullback-Leibler Divergence for Constrained L\\'evy-It\\^o\n  Processes",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Given an n-dimensional stochastic process X driven by P-Brownian motions and\nPoisson random measures, we seek the probability measure Q, with minimal\nrelative entropy to P, such that the Q-expectations of some terminal and\nrunning costs are constrained. We prove existence and uniqueness of the optimal\nprobability measure, derive the explicit form of the measure change, and\ncharacterise the optimal drift and compensator adjustments under the optimal\nmeasure. We provide an analytical solution for Value-at-Risk (quantile)\nconstraints, discuss how to perturb a Brownian motion to have arbitrary\nvariance, and show that pinned measures arise as a limiting case of optimal\nmeasures. The results are illustrated in a risk management setting -- including\nan algorithm to simulate under the optimal measure -- where an agent seeks to\nanswer the question: what dynamics are induced by a perturbation of the\nValue-at-Risk and the average time spent below a barrier on the reference\nprocess?\n"
    },
    {
        "paper_id": 2206.14876,
        "authors": "Jimei Shen, Yihan Mo, Christopher Plimpton, Mustafa Kaan Basaran",
        "title": "AI in Asset Management and Rebellion Research",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  On October 30th, 2021, Rebellion Research's CEO announced in a Q3 2021 Letter\nto Investors that Rebellion's AI Global Equity strategy returned +6.8% gross\nfor the first three quarters of 2021. \"It's no surprise\", Alex told us, \"Our\nMachine Learning global strategy has a history of outperforming the S&P 500 for\n14 years\". In 2021, Rebellion's brokerage accounts can be opened in over 70\ncountries, and Rebellion's research covers over 50 countries. Besides being an\nAI asset management company, Rebellion also defines itself as a top-tier,\nglobal machine learning think tank. Alex planned to build a Rebellion ML & AI\necosystem. Should Rebellion stay in the asset management area or jump into\nother areas? How could the Rebellion strategically move towards a more broad\narea? What were Rebellion's new or alternative business models?\n"
    },
    {
        "paper_id": 2206.14922,
        "authors": "Jason Poulos",
        "title": "Gender gaps in frontier entrepreneurship? Evidence from 1901 Oklahoma\n  land lottery winners",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The paper investigates gender differences in entrepreneurship by exploiting a\nlarge-scale land lottery in Oklahoma at the turn of the 20$^{\\text{th}}$\ncentury. Lottery winners claimed land in the order in which their names were\ndrawn, so the draw number is an approximate rank ordering of lottery wealth.\nThis mechanism allows for the estimation of a dose-response function, which\nrelates each draw number to the expected outcome under each draw. I estimate\ndose-response functions on a linked dataset of lottery winners and land patent\nrecords, and find the probability of purchasing land from the government to be\ndecreasing as a function of lottery wealth, which is evidence for the presence\nof liquidity constraints. I find female winners were more effective in\nleveraging lottery wealth to purchase additional land, as evidenced by\nsignificantly higher median dose-responses compared to those of male winners.\nFor a sample of winners linked to the 1910 Census, I find that male winners\nhave higher median dose-responses compared to female winners in terms of farm\nor home ownership. These results suggest that liquidity constraints may have\nbeen more binding for female entrepreneurs in the market economy.\n"
    },
    {
        "paper_id": 2206.14932,
        "authors": "Luyao Zhang, Tianyu Wu, Saad Lahrichi, Carlos-Gustavo Salas-Flores,\n  Jiayi Li",
        "title": "A Data Science Pipeline for Algorithmic Trading: A Comparative Study of\n  Applications for Finance and Cryptoeconomics",
        "comments": "Accepted at: The First International Symposium on Recent Advances of\n  Blockchain Evolution: Architecture, Intelligence, Incentive, and Applications",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Recent advances in Artificial Intelligence (AI) have made algorithmic trading\nplay a central role in finance. However, current research and applications are\ndisconnected information islands. We propose a generally applicable pipeline\nfor designing, programming, and evaluating the algorithmic trading of stock and\ncrypto assets. Moreover, we demonstrate how our data science pipeline works\nwith respect to four conventional algorithms: the moving average crossover,\nvolume-weighted average price, sentiment analysis, and statistical arbitrage\nalgorithms. Our study offers a systematic way to program, evaluate, and compare\ndifferent trading strategies. Furthermore, we implement our algorithms through\nobject-oriented programming in Python3, which serves as open-source software\nfor future academic research and applications.\n"
    },
    {
        "paper_id": 2206.15023,
        "authors": "Patrick Vu",
        "title": "Can the Replication Rate Tell Us About Publication Bias?",
        "comments": "main text (excluding appendix): 19 pages, 2 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  A leading explanation for widespread replication failures is publication\nbias. I show in a simple model of selective publication that, contrary to\ncommon perceptions, the replication rate is unaffected by the suppression of\ninsignificant results in the publication process. I show further that the\nexpected replication rate falls below intended power owing to issues with\ncommon power calculations. I empirically calibrate a model of selective\npublication and find that power issues alone can explain the entirety of the\ngap between the replication rate and intended power in experimental economics.\nIn psychology, these issues explain two-thirds of the gap.\n"
    },
    {
        "paper_id": 2206.15096,
        "authors": "Andreas Bjerre-Nielsen and Emil Chrisander",
        "title": "Voluntary Information Disclosure in Centralized Matching: Efficiency\n  Gains and Strategic Properties",
        "comments": "28 pages, 5 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
        "abstract": "  Information frictions can harm the welfare of participants in two-sided\nmatching markets. Consider a centralized admission, where colleges cannot\nobserve students' preparedness for success in a particular major or degree\nprogram. Colleges choose between using simple, cheap admission criteria, e.g.,\nhigh school grades as a proxy for preparedness, or screening all applications,\nwhich is time-consuming for both students and colleges. To address issues of\nfairness and welfare, we introduce two novel mechanisms that allow students to\ndisclose private information voluntarily and thus only require partial\nscreening. The mechanisms are based on Deferred Acceptance and preserve its\ncore strategic properties of credible preference revelation, particularly\nordinal strategy-proofness. In addition, we demonstrate conditions for which\ncardinal welfare improves for market participants compared to not screening.\nIntuitively, students and colleges benefit from voluntary information\ndisclosure if public information about students correlates weakly with\nstudents' private information and the cost of processing disclosed information\nis sufficiently low. Finally, we present empirical evidence from the Danish\nhigher education system that supports critical features of our model. Our work\nhas policy implications for the mechanism design of large two-sided markets\nwhere information frictions are inherent.\n"
    },
    {
        "paper_id": 2206.15098,
        "authors": "Ingrid Haegele",
        "title": "Talent Hoarding in Organizations",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Most organizations rely on managers to identify talented workers. However,\nmanagers who are evaluated on team performance have an incentive to hoard\nworkers. This study provides the first empirical evidence of talent hoarding\nusing personnel records and survey evidence from a large manufacturing firm.\nTalent hoarding is reported by three-fourths of managers, is detectable in\nmanagerial decisions, and occurs more frequently when hoarding incentives are\nstronger. Using quasi-random variation in exposure to talent hoarding, I\ndemonstrate that hoarding deters workers from applying to new positions,\ninhibiting worker career progression and altering the allocation of talent in\nthe firm.\n"
    },
    {
        "paper_id": 2206.15365,
        "authors": "Andrew Y. Chen",
        "title": "Most claimed statistical findings in cross-sectional return\n  predictability are likely true",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  I develop simple and intuitive bounds for the false discovery rate (FDR) in\ncross-sectional return predictability publications. The bounds can be\ncalculated by plugging in summary statistics from previous papers and reliably\nbound the FDR in simulations that closely mimic cross-predictor correlations.\nMost bounds find that at least 75% of findings are true. The tightest bound\nfinds that at least 91% of findings are true. Surprisingly, the estimates in\nHarvey, Liu, and Zhu (2016) imply a similar FDR. I explain how Harvey et al.'s\nconclusion that most findings are false stems from misinterpreting\n``insignificant factor'' as ``false discovery.''\n"
    },
    {
        "paper_id": 2207.00436,
        "authors": "Jonghun Kwak, Jungyu Ahn, Jinho Lee, Sungwoo Park",
        "title": "Shai-am: A Machine Learning Platform for Investment Strategies",
        "comments": "5 pages, 2 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The finance industry has adopted machine learning (ML) as a form of\nquantitative research to support better investment decisions, yet there are\nseveral challenges often overlooked in practice. (1) ML code tends to be\nunstructured and ad hoc, which hinders cooperation with others. (2) Resource\nrequirements and dependencies vary depending on which algorithm is used, so a\nflexible and scalable system is needed. (3) It is difficult for domain experts\nin traditional finance to apply their experience and knowledge in ML-based\nstrategies unless they acquire expertise in recent technologies. This paper\npresents Shai-am, an ML platform integrated with our own Python framework. The\nplatform leverages existing modern open-source technologies, managing\ncontainerized pipelines for ML-based strategies with unified interfaces to\nsolve the aforementioned issues. Each strategy implements the interface defined\nin the core framework. The framework is designed to enhance reusability and\nreadability, facilitating collaborative work in quantitative research. Shai-am\naims to be a pure AI asset manager for solving various tasks in financial\nmarkets.\n"
    },
    {
        "paper_id": 2207.00446,
        "authors": "Guanxing Fu, Ulrich Horst, Xiaonyu Xia",
        "title": "A Mean-Field Control Problem of Optimal Portfolio Liquidation with\n  Semimartingale Strategies",
        "comments": "34 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We consider a mean-field control problem with c\\`adl\\`ag semimartingale\nstrategies arising in portfolio liquidation models with transient market impact\nand self-exciting order flow. We show that the value function depends on the\nstate process only through its law, and that it is of linear-quadratic form and\nthat its coefficients satisfy a coupled system of non-standard Riccati-type\nequations. The Riccati equations are obtained heuristically by passing to the\ncontinuous-time limit from a sequence of discrete-time models. A sophisticated\ntransformation shows that the system can be brought into standard Riccati form\nfrom which we deduce the existence of a global solution. Our analysis shows\nthat the optimal strategy jumps only at the beginning and the end of the\ntrading period.\n"
    },
    {
        "paper_id": 2207.00453,
        "authors": "Matteo Gardini and Piergiacomo Sabino",
        "title": "Exchange option pricing under variance gamma-like models",
        "comments": "26 pages, 2 figures, 7 tables",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  In this article we focus on the pricing of exchange options when the dynamic\nof logprices follows either the well-known variance gamma or the recent\nvariance gamma++ process introduced in Gardini et al [19]. In particular, for\nthe former model we can derive a Margrabe's type formula whereas, for the\nlatter one we can write an \"integral free\" formula. Furthermore, we show how to\nconstruct a general multidimensional versions of the variance gamma++ processes\npreserving both the mathematical and numerical tractability. Finally we apply\nthe derived models to German and French energy power markets: we calibrate\ntheir parameters using real market data and we accordingly evaluate exchange\noptions with the derived closed formulas, Fourier based methods and Monte Carlo\ntechniques.\n"
    },
    {
        "paper_id": 2207.00493,
        "authors": "Weilong Fu, Ali Hirsa, J\\\"org Osterrieder",
        "title": "Simulating financial time series using attention",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Financial time series simulation is a central topic since it extends the\nlimited real data for training and evaluation of trading strategies. It is also\nchallenging because of the complex statistical properties of the real financial\ndata. We introduce two generative adversarial networks (GANs), which utilize\nthe convolutional networks with attention and the transformers, for financial\ntime series simulation. The GANs learn the statistical properties in a\ndata-driven manner and the attention mechanism helps to replicate the\nlong-range dependencies. The proposed GANs are tested on the S&P 500 index and\noption data, examined by scores based on the stylized facts and are compared\nwith the pure convolutional GAN, i.e. QuantGAN. The attention-based GANs not\nonly reproduce the stylized facts, but also smooth the autocorrelation of\nreturns.\n"
    },
    {
        "paper_id": 2207.00524,
        "authors": "Weilong Fu, Ali Hirsa",
        "title": "Solving barrier options under stochastic volatility using deep learning",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We develop an unsupervised deep learning method to solve the barrier options\nunder the Bergomi model. The neural networks serve as the approximate option\nsurfaces and are trained to satisfy the PDE as well as the boundary conditions.\nTwo singular terms are added to the neural networks to deal with the non-smooth\nand discontinuous payoff at the strike and barrier levels so that the neural\nnetworks can replicate the asymptotic behaviors of barrier options at short\nmaturities. After that, vanilla options and barrier options are priced in a\nsingle framework. Also, neural networks are employed to deal with the high\ndimensionality of the function input in the Bergomi model. Once trained, the\nneural network solution yields fast and accurate option values.\n"
    },
    {
        "paper_id": 2207.00666,
        "authors": "Anthony Enisan Akinlo and Segun Michael Ojo",
        "title": "Economic Consequences of the COVID-19 Pandemic on Sub-Saharan Africa: A\n  historical perspective",
        "comments": "21 pages, 4 figures",
        "journal-ref": "AJSD 11, 2, 2021, 102-119",
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  This paper examined the economic consequences of the COVID-19 pandemic on\nsub-Saharan Africa (SSA) using the historical approach and analysing the policy\nresponses of the region to past crises and their economic consequences. The\nstudy employed the manufacturing-value-added share of GDP as a performance\nindicator. The analysis shows that the wrong policy interventions to past\ncrises led the sub-Saharan African sub-region into its deplorable economic\nsituation. The study observed that the region leapfrogged prematurely to import\nsubstitution, export promotion, and global value chains. Based on these\nexperiences, the region should adopt a gradual approach in responding to the\nCOVID-19 economic consequences. The sub-region should first address relevant\nareas of sustainability, including proactive investment in research and\ndevelopment to develop homegrown technology, upgrade essential infrastructural\nfacilities, develop security infrastructure, and strengthen the financial\nsector.\n"
    },
    {
        "paper_id": 2207.00713,
        "authors": "Yanwei Jia and Xun Yu Zhou",
        "title": "q-Learning in Continuous Time",
        "comments": "64 pages, 4 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  We study the continuous-time counterpart of Q-learning for reinforcement\nlearning (RL) under the entropy-regularized, exploratory diffusion process\nformulation introduced by Wang et al. (2020). As the conventional (big)\nQ-function collapses in continuous time, we consider its first-order\napproximation and coin the term ``(little) q-function\". This function is\nrelated to the instantaneous advantage rate function as well as the\nHamiltonian. We develop a ``q-learning\" theory around the q-function that is\nindependent of time discretization. Given a stochastic policy, we jointly\ncharacterize the associated q-function and value function by martingale\nconditions of certain stochastic processes, in both on-policy and off-policy\nsettings. We then apply the theory to devise different actor-critic algorithms\nfor solving underlying RL problems, depending on whether or not the density\nfunction of the Gibbs measure generated from the q-function can be computed\nexplicitly. One of our algorithms interprets the well-known Q-learning\nalgorithm SARSA, and another recovers a policy gradient (PG) based\ncontinuous-time algorithm proposed in Jia and Zhou (2022b). Finally, we conduct\nsimulation experiments to compare the performance of our algorithms with those\nof PG-based algorithms in Jia and Zhou (2022b) and time-discretized\nconventional Q-learning algorithms.\n"
    },
    {
        "paper_id": 2207.00715,
        "authors": "Suresh N, Pooja M",
        "title": "A study on Determinants of Dividend Policy and its Impact on Financial\n  Performances: A Panel Data Analysis for Indian Listed Firms",
        "comments": null,
        "journal-ref": "Journal of Seybold Report, 2020 15 2791 2799",
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Determination of the correct mix of dividend and retained earnings and its\neffect on profitability has been a subject of controversy in financial\nmanagement literature. This paper seeks to contribute to the ongoing debate by\nexamining the relationship between dividend payout policy and the financial\nperformance of 60 firms listed on the National Stock Exchange between\n2009-2018. The Return on Assets (ROA) served as a surrogate for the dependent\nvariable, profitability, while the Dividend Pay-out ratio proxied for dividend\npolicy and was the only explanatory variable. Control variables include firm\nsize, asset tangibility, and leverage. Regression result reveals a positive and\nsignificant relationship between dividend payout policy (DPO) and firm\nperformance (ROA). It is recommended that companies should endeavor to put in\nplace a robust dividend payout policy that would encourage investment in\nprojects that give positive Net Present Value.\n"
    },
    {
        "paper_id": 2207.00716,
        "authors": "Nandini E.S, Sudharani R, Suresh N",
        "title": "A Study on Impact of Environmental Accounting on Profitability of\n  Companies listed in Bombay Stock Exchange",
        "comments": null,
        "journal-ref": "Bulletin Monumental 21 2020 46 51",
        "doi": "10.37896/BMJ21.08/2905",
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  The study focuses on the Impact of Environmental Accounting on the\nProfitability of Companies listed on the Bombay Stock Exchange. The study has\nconsidered the Amount spent on Environmental protection as an Independent\nvariable and Return on Capital Employed, Return on Assets, Return on Net\nworth/equity, Net Profit Margin, and Dividend per Share as the Dependent\nvariable. The present study is to analyses the relationship between Amounts\nspent on Environmental protection costs and Return on Capital Employed, Return\non Assets, Return on Net worth/equity, Net Profit Margin, and Dividend per\nShare. The data is collected from 18 companies listed on the Bombay Stock\nExchange for 10 years from the Annual reports of companies. The data collected\nwere analysed using Panel data Regression in E-Views. Results revealed that\nthere is a significant Relationship between Environmental protection Cost and\nReturn on Capital Employed, Return on Assets, Return on Net worth/equity, Net\nProfit Margin, and Dividend per Share. The study shows that Environmental\naccounting impact positively on Firms profitability.\n"
    },
    {
        "paper_id": 2207.0072,
        "authors": "P.Aishwarya, Sudharani R, Suresh N",
        "title": "A Study on Impact of Capital Structure on Profitability of Companies\n  Listed in Indian Stock Exchange with respect to Automobile Industry",
        "comments": null,
        "journal-ref": "Journal of Seybold Report, 2020",
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Current research helps in understanding both positive and negative impacts of\ncapital structure on profits of Indian automobile companies by using variables\nlike Return on Capital Employed, Return on Long Term Funds, Return on Net\nWorth, Gross Profit Margin, and Operating Profit, and Return on Asset. The\nstudy hypothesized that RoCE, RoLT, and RoNW have a positive effect and GP, OP\nand ROA have a negative impact on debt-equity and interest coverage ratios i.e\ncapital structure of the companies. Also, the study proves that the\nrelationship between profitability and capital structure variables is strongly\nsignificant. The hypothesis was tested by using fixed effect and random effect\nmodels by considering 10 years of data (from 2010-2019) from 17 automobile\ncompanies. The result of the study recommends that the firms can improve their\nperformance by using an optimal capital structure. Also, a fair mix of debt and\nequity should be established to ensure that the firm maintains capital\nadequacy. Firms can thus be able to meet their financial compulsions and\ninvestments that can promise attractive returns.\n"
    },
    {
        "paper_id": 2207.00722,
        "authors": "Anil S, Sudharani R, Suresh N",
        "title": "A Study on the Impact of Human Resource Accounting on Firms Value with\n  Respect to Companies Listed in National Stock Exchange",
        "comments": null,
        "journal-ref": "BULLETIN MONUMENTAL 21 2020 38 44",
        "doi": "10.37896/BMJ21.09/2962",
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  The study focuses on the Impact of Employment Benefit Cots on the\nProfitability of Companies listed in the National Stock Exchange. The study has\nconsidered the Amount spent on Employment Benefit Cots as an Independent\nvariable and Profit after tax, Total Assets, Return on Equity, and Return on\nAsset and Debt equity Ration as the Dependent variable. The present study is to\nanalyses the relationship between Employment Benefit Cots and Profit after tax,\nTotal Assets, Return on Equity, Return on Asset, and Debt equity Ration. The\ndata is collected from 20 companies listed on the National Stock Exchange for\n10 years from the Annual reports of companies. The data collected were analyzed\nusing Panel data Regression in E-Views. Results revealed that there is a\nsignificant Relationship between Employment Benefit Cots and Profit after tax,\nTotal Assets, Return on Equity, Return on Asset, and Debt equity Ration. The\nstudy shows that Employment Benefit Cots impact positively on Firms\nprofitability.\n"
    },
    {
        "paper_id": 2207.00739,
        "authors": "Yichen Feng, Ming Min, Jean-Pierre Fouque",
        "title": "Deep Learning for Systemic Risk Measures",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The aim of this paper is to study a new methodological framework for systemic\nrisk measures by applying deep learning method as a tool to compute the optimal\nstrategy of capital allocations. Under this new framework, systemic risk\nmeasures can be interpreted as the minimal amount of cash that secures the\naggregated system by allocating capital to the single institutions before\naggregating the individual risks. This problem has no explicit solution except\nin very limited situations. Deep learning is increasingly receiving attention\nin financial modelings and risk management and we propose our deep learning\nbased algorithms to solve both the primal and dual problems of the risk\nmeasures, and thus to learn the fair risk allocations. In particular, our\nmethod for the dual problem involves the training philosophy inspired by the\nwell-known Generative Adversarial Networks (GAN) approach and a newly designed\ndirect estimation of Radon-Nikodym derivative. We close the paper with\nsubstantial numerical studies of the subject and provide interpretations of the\nrisk allocations associated to the systemic risk measures. In the particular\ncase of exponential preferences, numerical experiments demonstrate excellent\nperformance of the proposed algorithm, when compared with the optimal explicit\nsolution as a benchmark.\n"
    },
    {
        "paper_id": 2207.00773,
        "authors": "Oladapo Fapetu, Segun Michael Ojo, Adekunle Alexander Balogun and\n  Adeoba Adepoju Asaolu",
        "title": "Capital Market Performance and Macroeconomic Dynamics in Nigeria",
        "comments": "pages 9, 2 figures",
        "journal-ref": "FUOYE Journal of Finance and Contemporary Issues Vol 1, Issue 1,\n  2021, 38-48",
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  The study examined the relationship between capital market performance and\nthe macroeconomic dynamics in Nigeria, and it utilized secondary data spanning\n1993 to 2020. The data was analyzed using vector error correction model (VECM)\ntechnology. The result revealed a significant long run relationship between\ncapital market performance and macroeconomic dynamics in Nigeria. We observed\nlong run causality running from the exchange rate, inflation, money supply, and\nunemployment rate to capital market performance indicator in Nigeria. The\nresult supports the Arbitrage Pricing Theory (APT) proposition in the Nigerian\ncontext. The theory stipulates that the linear relationship between an asset\nexpected returns and the macroeconomic factors whose dynamics affect the asset\nrisk can forecast an asset's returns. In other words, the result of this study\nsupports the proposition that the dynamics in the exchange rate, inflation,\nmoney supply, and unemployment rate influence the capital market performance.\nThe study validates the recommendations of Arbitrage Pricing Theory (APT) in\nNigeria.\n"
    },
    {
        "paper_id": 2207.00862,
        "authors": "Georgios I. Papayiannis",
        "title": "Static Hedging of Freight Risk under Model Uncertainty",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Freight rate derivatives constitute a very popular financial tool in shipping\nindustry, that allows to the market participants and the individuals operating\nin the field, to reassure their financial positions against the risk occurred\nby the volatility of the freight rates. The special structure of the shipping\nmarket attracted the interest of both academics and practitioners, since\npricing of the related traded options which are written on non-storable assets\n(i.e. the freight service) is not a trivial task. Management of freight risk is\nof major importance to preserve the viability of shipping operations,\nespecially in periods where shocks appear in the world economy, which\nintroduces uncertainty in the freight rate prices. In practice, the reduction\nof freight risk is almost exclusively performed by constructing hedging\nportfolios relying on freight rate options. These portfolios needs to be robust\nto the market uncertainties, i.e. to choose the portfolio which returns will be\nas less as it gets affected by the market changes. Especially, for time periods\nwhere the future states of the market (even in short term) are extremely\nambiguous, i.e. there are a number of different scenarios that can occur, it is\nof great importance for the firms to decide robustly to these uncertainties. In\nthis work, a framework for the robust treatment of model uncertainty in (a)\nmodeling the freight rates dynamics employing the notion of Wasserstein\nbarycenter and (b) in choosing the optimal hedging strategy for freight risk\nmanagement, is proposed. A carefully designed simulation study in the discussed\nhedging problem, employing standard modelling approaches in freight rates\nliterature, illustrates the capabilities of the proposed method with very\nsatisfactory results in approximating the optimal strategy even in high noise\ncases.\n"
    },
    {
        "paper_id": 2207.00932,
        "authors": "Hans Buehler, Phillip Murray, Ben Wood",
        "title": "Deep Bellman Hedging",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-sa/4.0/",
        "abstract": "  We present an actor-critic-type reinforcement learning algorithm for solving\nthe problem of hedging a portfolio of financial instruments such as securities\nand over-the-counter derivatives using purely historic data. The key\ncharacteristics of our approach are: the ability to hedge with derivatives such\nas forwards, swaps, futures, options; incorporation of trading frictions such\nas trading cost and liquidity constraints; applicability for any reasonable\nportfolio of financial instruments; realistic, continuous state and action\nspaces; and formal risk-adjusted return objectives. Most importantly, the\ntrained model provides an optimal hedge for arbitrary initial portfolios and\nmarket states without the need for re-training. We also prove existence of\nfinite solutions to our Bellman equation, and show the relation to our vanilla\nDeep Hedging approach\n"
    },
    {
        "paper_id": 2207.00949,
        "authors": "Brendan K. Beare, Juwon Seo and Zhongxi Zheng",
        "title": "Stochastic arbitrage with market index options",
        "comments": "19 pages, 7 figures, 7 tables",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Opportunities for stochastic arbitrage in an options market arise when it is\npossible to construct a portfolio of options which provides a positive option\npremium and which, when combined with a direct investment in the underlying\nasset, generates a payoff which stochastically dominates the payoff from the\ndirect investment in the underlying asset. We provide linear and mixed-integer\nlinear programs for computing the stochastic arbitrage opportunity providing\nthe maximum option premium to an investor. We apply our programs to 18 years of\ndata on monthly put and call options on the Standard & Poors 500 index,\nconfining attention to options with moderate moneyness, and using two\nspecifications of the underlying asset return distribution, one symmetric and\none skewed. The pricing of market index options with moderate moneyness appears\nto be broadly consistent with our skewed specification of market returns.\n"
    },
    {
        "paper_id": 2207.0101,
        "authors": "Menna Hassan, Nourhan Sakr and Arthur Charpentier",
        "title": "Government Intervention in Catastrophe Insurance Markets: A\n  Reinforcement Learning Approach",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  This paper designs a sequential repeated game of a micro-founded society with\nthree types of agents: individuals, insurers, and a government. Nascent to\neconomics literature, we use Reinforcement Learning (RL), closely related to\nmulti-armed bandit problems, to learn the welfare impact of a set of proposed\npolicy interventions per $1 spent on them. The paper rigorously discusses the\ndesirability of the proposed interventions by comparing them against each other\non a case-by-case basis. The paper provides a framework for algorithmic policy\nevaluation using calibrated theoretical models which can assist in feasibility\nstudies.\n"
    },
    {
        "paper_id": 2207.01137,
        "authors": "Eleanor Loh, Jalaj Khandelwal, Brian Regan, Duncan A. Little",
        "title": "Promotheus: An End-to-End Machine Learning Framework for Optimizing\n  Markdown in Online Fashion E-commerce",
        "comments": "11 pages; Accepted at KDD 2022",
        "journal-ref": null,
        "doi": "10.1145/3534678.3539148",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Managing discount promotional events (\"markdown\") is a significant part of\nrunning an e-commerce business, and inefficiencies here can significantly\nhamper a retailer's profitability. Traditional approaches for tackling this\nproblem rely heavily on price elasticity modelling. However, the partial\ninformation nature of price elasticity modelling, together with the\nnon-negotiable responsibility for protecting profitability, mean that machine\nlearning practitioners must often go through great lengths to define strategies\nfor measuring offline model quality. In the face of this, many retailers fall\nback on rule-based methods, thus forgoing significant gains in profitability\nthat can be captured by machine learning. In this paper, we introduce two novel\nend-to-end markdown management systems for optimising markdown at different\nstages of a retailer's journey. The first system, \"Ithax\", enacts a rational\nsupply-side pricing strategy without demand estimation, and can be usefully\ndeployed as a \"cold start\" solution to collect markdown data while maintaining\nrevenue control. The second system, \"Promotheus\", presents a full framework for\nmarkdown optimization with price elasticity. We describe in detail the specific\nmodelling and validation procedures that, within our experience, have been\ncrucial to building a system that performs robustly in the real world. Both\nmarkdown systems achieve superior profitability compared to decisions made by\nour experienced operations teams in a controlled online test, with improvements\nof 86% (Promotheus) and 79% (Ithax) relative to manual strategies. These\nsystems have been deployed to manage markdown at ASOS.com, and both systems can\nbe fruitfully deployed for price optimization across a wide variety of retail\ne-commerce settings.\n"
    },
    {
        "paper_id": 2207.01151,
        "authors": "Di Zhang, Qiang Niu, Youzhou Zhou",
        "title": "Modeling Randomly Walking Volatility with Chained Gamma Distributions",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Volatility clustering is a common phenomenon in financial time series.\nTypically, linear models can be used to describe the temporal autocorrelation\nof the (logarithmic) variance of returns. Considering the difficulty in\nestimating this model, we construct a Dynamic Bayesian Network, which utilizes\nthe conjugate prior relation of normal-gamma and gamma-gamma, so that its\nposterior form locally remains unchanged at each node. This makes it possible\nto find approximate solutions using variational methods quickly. Furthermore,\nwe ensure that the volatility expressed by the model is an independent\nincremental process after inserting dummy gamma nodes between adjacent time\nsteps. We have found that this model has two advantages: 1) It can be proved\nthat it can express heavier tails than Gaussians, i.e., have positive excess\nkurtosis, compared to popular linear models. 2) If the variational\ninference(VI) is used for state estimation, it runs much faster than Monte\nCarlo(MC) methods since the calculation of the posterior uses only basic\narithmetic operations. And its convergence process is deterministic.\n  We tested the model, named Gam-Chain, using recent Crypto, Nasdaq, and Forex\nrecords of varying resolutions. The results show that: 1) In the same case of\nusing MC, this model can achieve comparable state estimation results with the\nregular lognormal chain. 2) In the case of only using VI, this model can obtain\naccuracy that are slightly worse than MC, but still acceptable in practice; 3)\nOnly using VI, the running time of Gam-Chain, in general case, can be reduced\nto below 5% of that based on the lognormal chain via MC.\n"
    },
    {
        "paper_id": 2207.01187,
        "authors": "Jinho Lee, Sungwoo Park, Jungyu Ahn, Jonghun Kwak",
        "title": "ETF Portfolio Construction via Neural Network trained on Financial\n  Statement Data",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Recently, the application of advanced machine learning methods for asset\nmanagement has become one of the most intriguing topics. Unfortunately, the\napplication of these methods, such as deep neural networks, is difficult due to\nthe data shortage problem. To address this issue, we propose a novel approach\nusing neural networks to construct a portfolio of exchange traded funds (ETFs)\nbased on the financial statement data of their components. Although a number of\nETFs and ETF-managed portfolios have emerged in the past few decades, the\nability to apply neural networks to manage ETF portfolios is limited since the\nnumber and historical existence of ETFs are relatively smaller and shorter,\nrespectively, than those of individual stocks. Therefore, we use the data of\nindividual stocks to train our neural networks to predict the future\nperformance of individual stocks and use these predictions and the portfolio\ndeposit file (PDF) to construct a portfolio of ETFs. Multiple experiments have\nbeen performed, and we have found that our proposed method outperforms the\nbaselines. We believe that our approach can be more beneficial when managing\nrecently listed ETFs, such as thematic ETFs, of which there is relatively\nlimited historical data for training advanced machine learning methods.\n"
    },
    {
        "paper_id": 2207.01235,
        "authors": "Johannes Wiesel, Erica Zhang",
        "title": "An optimal transport based characterization of convex order",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  For probability measures $\\mu,\\nu$ and $\\rho$ define the cost functionals\n\\begin{align*} C(\\mu,\\rho):=\\sup_{\\pi\\in \\Pi(\\mu,\\rho)} \\int \\langle\nx,y\\rangle\\, \\pi(dx,dy),\\quad C(\\nu,\\rho):=\\sup_{\\pi\\in \\Pi(\\nu,\\rho)} \\int\n\\langle x,y\\rangle\\, \\pi(dx,dy), \\end{align*} where $\\langle\\cdot,\n\\cdot\\rangle$ denotes the scalar product and $\\Pi(\\cdot,\\cdot)$ is the set of\ncouplings. We show that two probability measures $\\mu$ and $\\nu$ on\n$\\mathbb{R}^d$ with finite first moments are in convex order (i.e.\n$\\mu\\preceq_c\\nu$) iff $C(\\mu,\\rho)\\le C(\\nu,\\rho)$ holds for all probability\nmeasures $\\rho$ on $\\mathbb{R}^d$ with bounded support. This generalizes a\nresult by Carlier. Our proof relies on a quantitative bound for the infimum of\n$\\int f\\,d\\nu -\\int f\\,d\\mu$ over all $1$-Lipschitz functions $f$, which is\nobtained through optimal transport duality and Brenier's theorem. Building on\nthis result, we derive new proofs of well-known one-dimensional\ncharacterizations of convex order. We also describe new computational methods\nfor investigating convex order and applications to model-independent arbitrage\nstrategies in mathematical finance.\n"
    },
    {
        "paper_id": 2207.01277,
        "authors": "Kenji Kubo, Koichi Miyamoto, Kosuke Mitarai, Keisuke Fujii",
        "title": "Pricing multi-asset derivatives by variational quantum algorithms",
        "comments": "18 pages, 4 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Pricing a multi-asset derivative is an important problem in financial\nengineering, both theoretically and practically. Although it is suitable to\nnumerically solve partial differential equations to calculate the prices of\ncertain types of derivatives, the computational complexity increases\nexponentially as the number of underlying assets increases in some classical\nmethods, such as the finite difference method. Therefore, there are efforts to\nreduce the computational complexity by using quantum computation. However, when\nsolving with naive quantum algorithms, the target derivative price is embedded\nin the amplitude of one basis of the quantum state, and so an exponential\ncomplexity is required to obtain the solution. To avoid the bottleneck, the\nprevious study~[Miyamoto and Kubo, IEEE Transactions on Quantum Engineering,\n\\textbf{3}, 1--25 (2022)] utilizes the fact that the present price of a\nderivative can be obtained by its discounted expected value at any future point\nin time and shows that the quantum algorithm can reduce the complexity. In this\npaper, to make the algorithm feasible to run on a small quantum computer, we\nuse variational quantum simulation to solve the Black-Scholes equation and\ncompute the derivative price from the inner product between the solution and a\nprobability distribution. This avoids the measurement bottleneck of the naive\napproach and would provide quantum speedup even in noisy quantum computers. We\nalso conduct numerical experiments to validate our method. Our method will be\nan important breakthrough in derivative pricing using small-scale quantum\ncomputers.\n"
    },
    {
        "paper_id": 2207.01402,
        "authors": "Elvys Linhares Pontes and Mohamed Benjannet and Jose G. Moreno and\n  Antoine Doucet",
        "title": "Using contextual sentence analysis models to recognize ESG concepts",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  This paper summarizes the joint participation of the Trading Central Labs and\nthe L3i laboratory of the University of La Rochelle on both sub-tasks of the\nShared Task FinSim-4 evaluation campaign. The first sub-task aims to enrich the\n'Fortia ESG taxonomy' with new lexicon entries while the second one aims to\nclassify sentences to either 'sustainable' or 'unsustainable' with respect to\nESG (Environment, Social and Governance) related factors. For the first\nsub-task, we proposed a model based on pre-trained Sentence-BERT models to\nproject sentences and concepts in a common space in order to better represent\nESG concepts. The official task results show that our system yields a\nsignificant performance improvement compared to the baseline and outperforms\nall other submissions on the first sub-task. For the second sub-task, we\ncombine the RoBERTa model with a feed-forward multi-layer perceptron in order\nto extract the context of sentences and classify them. Our model achieved high\naccuracy scores (over 92%) and was ranked among the top 5 systems.\n"
    },
    {
        "paper_id": 2207.01536,
        "authors": "Caroline Hillairet, Sarah Kaakai and Mohamed Mrad",
        "title": "Time-consistent pension policy with minimum guarantee and sustainability\n  constraint",
        "comments": "36 pages",
        "journal-ref": null,
        "doi": "10.3934/puqr.2024003",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper proposes and investigates an optimal pair investment/pension\npolicy for a pay-as-you-go (PAYG) pension scheme. The social planner can invest\nin a buffer fund in order to guarantee a minimal pension amount. The model aims\nat taking into account complex dynamic phenomena such as the demographic risk\nand its evolution over time, the time and age dependence of agents preferences,\nand financial risks. The preference criterion of the social planner is modeled\nby a consistent dynamic utility defined on a stochastic domain, which\nincorporates the heterogeneity of overlapping generations and its evolution\nover time. The preference criterion and the optimization problem also\nincorporate sustainability, adequacy and fairness constraints. The paper\ndesigns and solves the social planner's dynamic decision criterion, and\ncomputes the optimal investment/pension policy in a general framework. A\ndetailed analysis for the case of dynamic power utilities is provided.\n"
    },
    {
        "paper_id": 2207.01558,
        "authors": "Hao Tang and Wenxun Wu and Xian-Min Jin",
        "title": "Quantum Computation for Pricing Caps using the LIBOR Market Model",
        "comments": "18 pages, 5 figures. We demonstrate a useful quantum computing\n  application on pricing interest rate derivatives",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
        "abstract": "  The LIBOR Market Model (LMM) is a widely used model for pricing interest rate\nderivatives. While the Black-Scholes model is well-known for pricing stock\nderivatives such as stock options, a larger portion of derivatives are based on\ninterest rates instead of stocks. Pricing interest rate derivatives used to be\nchallenging, as their previous models employed either the instantaneous\ninterest or forward rate that could not be directly observed in the market.\nThis has been much improved since LMM was raised, as it uses directly\nobservable interbank offered rates and is expected to be more precise.\nRecently, quantum computing has been used to speed up option pricing tasks, but\nrarely on structured interest rate derivatives. Given the size of the interest\nrate derivatives market and the widespread use of LMM, we employ quantum\ncomputing to price an interest rate derivative, caps, based on the LMM. As caps\npricing relates to path-dependent Monte Carlo iterations for different tenors,\nwhich is common for many complex structured derivatives, we developed our\nhybrid classical-quantum approach that applies the quantum amplitude estimation\nalgorithm to estimate the expectation for the last tenor. We show that our\nhybrid approach still shows better convergence than pure classical Monte Carlo\nmethods, providing a useful case study for quantum computing with a greater\ndiversity of derivatives.\n"
    },
    {
        "paper_id": 2207.01793,
        "authors": "Yuan Liang, Bingjie Yu, Xiaojian Zhang, Yi Lu, Linchuan Yang",
        "title": "The Short-term Impact of Congestion Taxes on Ridesourcing Demand and\n  Traffic Congestion: Evidence from Chicago",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Ridesourcing is popular in many cities. Despite its theoretical benefits, a\nlarge body of studies have claimed that ridesourcing also brings (negative)\nexternalities (e.g., inducing trips and aggravating traffic congestion).\nTherefore, many cities are planning to enact or have already enacted policies\nto regulate its use. However, these policies' effectiveness or impact on\nridesourcing demand and traffic congestion is uncertain. To this end, this\nstudy applies difference-in-differences (i.e., a regression-based causal\ninference approach) to empirically evaluate the effects of the congestion tax\npolicy on ridesourcing demand and traffic congestion in Chicago. It shows that\nthis congestion tax policy significantly curtails overall ridesourcing demand\nbut marginally alleviates traffic congestion. The results are robust to the\nchoice of time windows and data sets, additional control variables, alternative\nmodel specifications, alternative control groups, and alternative modeling\napproaches (i.e., regression discontinuity in time). Moreover, considerable\nheterogeneity exists. For example, the policy notably reduces ridesourcing\ndemand with short travel distances, but such an impact is gradually attenuated\nas the distance increases.\n"
    },
    {
        "paper_id": 2207.01939,
        "authors": "Cassandra Milbradt and D\\\"orte Kreher",
        "title": "A cross-border market model with limited transmission capacities",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We develop a cross-border market model between two countries in which the\ntransmission capacities that enable transactions between market participants of\ndifferent countries are limited. Starting from two reduced-form representations\nof national limit order book dynamics, we allow incoming market orders to be\nmatched with standing volumes of the foreign market, resulting in cross-border\ntrades. Since the transmission capacities in our model are limited, our model\nalternates between regimes in which cross-border trades are possible and\nregimes in which incoming market orders can only be matched against limit\norders of the same origin. We derive a high-frequency approximation of our\nmicroscopic model, assuming that the size of an individual order converges to\nzero while the order arrival rate tends to infinity. If transmission capacities\nare available, the limit process behaves as follows: the volume dynamics is a\nfour-dimensional linear Brownian motion in the positive orthant with oblique\nreflection at the axes. Each time two queues simultaneously hit zero, the\nprocess is reinitialized. The capacity turns out to be a continuous process of\nfinite variation. The analytic tractability of the limiting dynamics allows us\nto compute key quantities of interest like the distribution of the duration\nuntil the next price change. Additionally, we study the effect of cross-border\ntrading on price stability through a simulation study.\n"
    },
    {
        "paper_id": 2207.02064,
        "authors": "John Nay",
        "title": "Climate-Contingent Finance",
        "comments": "Forthcoming in Berkeley Business Law Journal",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Climate adaptation could yield significant benefits. However, the uncertainty\nof which future climate scenarios will occur decreases the feasibility of\nproactively adapting. Climate adaptation projects could be underwritten by\nbenefits paid for in the climate scenarios that each adaptation project is\ndesigned to address because other entities would like to hedge the financial\nrisk of those scenarios. Because the return on investment is a function of the\nlevel of climate change, it is optimal for the adapting entity to finance\nadaptation with repayment as a function of the climate. It is also optimal for\nentities with more financial downside under a more extreme climate to serve as\nan investing counterparty because they can obtain higher than market rates of\nreturn when they need it most.\n  In this way, parties proactively adapting would reduce the risk they\nover-prepare, while their investors would reduce the risk they under-prepare.\nThis is superior to typical insurance because, by investing in\nclimate-contingent mechanisms, investors are not merely financially hedging but\nalso outright preventing physical damage, and therefore creating economic\nvalue. This coordinates capital through time and place according to parties'\nrisk reduction capabilities and financial profiles, while also providing a\ndiversifying investment return.\n  Climate-contingent finance can be generalized to any situation where entities\nshare exposure to a risk where they lack direct control over whether it occurs\n(e.g., climate change, or a natural pandemic), and one type of entity can take\nproactive actions to benefit from addressing the effects of the risk if it\noccurs (e.g., through innovating on crops that would do well under extreme\nclimate change or vaccination technology that could address particular viruses)\nwith funding from another type of entity that seeks a targeted return to\nameliorate the downside.\n"
    },
    {
        "paper_id": 2207.02134,
        "authors": "Charl Maree and Christian W. Omlin",
        "title": "Balancing Profit, Risk, and Sustainability for Portfolio Management",
        "comments": null,
        "journal-ref": "IEEE CIFEr (2022)",
        "doi": "10.1109/CIFEr52523.2022.9776048",
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Stock portfolio optimization is the process of continuous reallocation of\nfunds to a selection of stocks. This is a particularly well-suited problem for\nreinforcement learning, as daily rewards are compounding and objective\nfunctions may include more than just profit, e.g., risk and sustainability. We\ndeveloped a novel utility function with the Sharpe ratio representing risk and\nthe environmental, social, and governance score (ESG) representing\nsustainability. We show that a state-of-the-art policy gradient method -\nmulti-agent deep deterministic policy gradients (MADDPG) - fails to find the\noptimum policy due to flat policy gradients and we therefore replaced gradient\ndescent with a genetic algorithm for parameter optimization. We show that our\nsystem outperforms MADDPG while improving on deep Q-learning approaches by\nallowing for continuous action spaces. Crucially, by incorporating risk and\nsustainability criteria in the utility function, we improve on the\nstate-of-the-art in reinforcement learning for portfolio optimization; risk and\nsustainability are essential in any modern trading strategy and we propose a\nsystem that does not merely report these metrics, but that actively optimizes\nthe portfolio to improve on them.\n"
    },
    {
        "paper_id": 2207.02151,
        "authors": "Rahul Tongia",
        "title": "Balancing India's 2030 Electricity Grid Needs Management of Time\n  Granularity and Uncertainty: Insights from a Parametric Model",
        "comments": "Trans Indian Natl. Acad. Eng. 2022",
        "journal-ref": null,
        "doi": "10.1007/s41403-022-00350-2",
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  With some of the world's most ambitious renewable energy (RE) growth targets,\nespecially when normalized for scale, India aims more than quadrupling wind and\nsolar by 2030. Simultaneously, coal dominates the electricity grid, providing\nroughly three-quarters of electricity today. We present results from the first\nof a kind model to handle high uncertainty, which uses parametric analysis\ninstead of stochastic analysis for grid balancing based on economic despatch\nthrough 2030, covering 30-minute resolution granularity at a national level.\nThe model assumes a range of growing demand, supply options, prices, and other\nuncertain inputs. It calculates the lowest cost portfolio across a spectrum of\nparametric uncertainty. We apply simplifications to handle the intersection of\ncapacity planning with optimized despatch. Our results indicate that very high\nRE scenarios are cost-effective, even if a measurable fraction would be surplus\nand thus discarded (\"curtailed\"). We find that high RE without storage as well\nas existing slack in coal- and gas-powered capacity are insufficient to meet\nrising demand on a real-time basis, especially adding time-of-day balancing.\nStorage technologies prove valuable but remain expensive compared to the 2019\nportfolio mix, due to issues of duty cycling like seasonal variability, not\nmerely inherent high capital costs. However, examining alternatives to\nbatteries for future growth finds all solutions for peaking power are even more\nexpensive. For balancing at peak times, a smarter grid that applies demand\nresponse may be cost-effective. We also find the need for more sophisticated\nmodelling with higher stochasticity across annual timeframes (especially year\non year changes in wind output, rainfall, and demand) along with uncertainty on\nsupply and load profiles (shapes).\n"
    },
    {
        "paper_id": 2207.02359,
        "authors": "Svetlana Boyarchenko and Sergei Levendorski\\u{i}",
        "title": "L\\'evy models amenable to efficient calculations",
        "comments": "46 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In our previous publications (IJTAF 2019, Math. Finance 2020), we introduced\na general class of SINH-regular processes and demonstrated that efficient\nnumerical methods for the evaluation of the Wiener-Hopf factors and various\nprobability distributions (prices of options of several types) in L\\'evy models\ncan be developed using only a few general properties of the characteristic\nexponent $\\psi$. Essentially all popular L\\'evy processes enjoy these\nproperties. In the present paper, we define classes of Stieltjes-L\\'evy\nprocesses (SL-processes) as processes with completely monotone L\\'evy densities\nof positive and negative jumps, and signed Stieltjes-L\\'evy processes\n(sSL-processes) as processes with densities representable as differences of\ncompletely monotone densities. We demonstrate that 1) all crucial properties of\n$\\psi$ are consequences of the representation\n$\\psi(\\xi)=(a^+_2\\xi^2-ia^+_1\\xi)ST(\\cG_+)(-i\\xi)+(a^-_2\\xi^2+ia^-_1\\xi)ST(\\cG_-)(i\\xi)+(\\sg^2/2)\\xi^2-i\\mu\\xi$,\nwhere $ST(\\cG)$ is the Stieltjes transform of the (signed) Stieltjes measure\n$\\cG$ and $a^\\pm_j\\ge 0$; 2) essentially all popular processes other than\nMerton's model and Meixner processes areSL-processes; 3) Meixner processes are\nsSL-processes; 4) under a natural symmetry condition, essentially all popular\nclasses of L\\'evy processes are SL- or sSL-subordinated Brownian motion.\n"
    },
    {
        "paper_id": 2207.02379,
        "authors": "Kyle R. Myers",
        "title": "Some Tradeoffs of Competition in Grant Contests",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  When funding public goods, resources are often allocated via mechanisms that\nresemble contests, especially in the case of research grants. A common critique\nof these contests is that they induce ``too much'' effort from participants.\nThis need not be true if the effort in the contest is itself directed towards\nthe public good. This papers analyzes survey data on scientists' time use and\nfinds that scientists allocate their time in a way that is consistent with\nfundraising effort (e.g., grant writing) having inherent scientific value --\nscientists who spend more time fundraising do not spent significantly less time\non research even after conditioning on confounding factors. Theoretical models\nof contests are used to show that the presence of such a positive effort\nexternality, where scientists generate social value when pursuing grants,\nchanges the relationship between competition and the aggregate productivity of\na grant contest. Ensuring that scientists exert socially valuable effort to\nobtain grants is increasingly important as grant contests become more\ncompetitive.\n"
    },
    {
        "paper_id": 2207.02458,
        "authors": "Jungyu Ahn, Sungwoo Park, Jiwoon Kim, Ju-hong Lee",
        "title": "Reinforcement Learning Portfolio Manager Framework with Monte Carlo\n  Simulation",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Asset allocation using reinforcement learning has advantages such as\nflexibility in goal setting and utilization of various information. However,\nexisting asset allocation methods do not consider the following viewpoints in\nsolving the asset allocation problem. First, State design without considering\nportfolio management and financial market characteristics. Second, Model\nOverfitting. Third, Model training design without considering the statistical\nstructure of financial time series data. To solve the problem of the existing\nasset allocation method using reinforcement learning, we propose a new\nreinforcement learning asset allocation method. First, the state of the\nportfolio managed by the model is considered as the state of the reinforcement\nlearning agent. Second, Monte Carlo simulation data are used to increase\ntraining data complexity to prevent model overfitting. These data can have\ndifferent patterns, which can increase the complexity of the data. Third, Monte\nCarlo simulation data are created considering various statistical structures of\nfinancial markets. We define the statistical structure of the financial market\nas the correlation matrix of the assets constituting the financial market. We\nshow experimentally that our method outperforms the benchmark at several test\nintervals.\n"
    },
    {
        "paper_id": 2207.02799,
        "authors": "Mathias Lindholm, Ronald Richman, Andreas Tsanakas, Mario V.\n  W\\\"uthrich",
        "title": "A multi-task network approach for calculating discrimination-free\n  insurance prices",
        "comments": "23 pages, 7 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  In applications of predictive modeling, such as insurance pricing, indirect\nor proxy discrimination is an issue of major concern. Namely, there exists the\npossibility that protected policyholder characteristics are implicitly inferred\nfrom non-protected ones by predictive models, and are thus having an\nundesirable (or illegal) impact on prices. A technical solution to this problem\nrelies on building a best-estimate model using all policyholder characteristics\n(including protected ones) and then averaging out the protected characteristics\nfor calculating individual prices. However, such approaches require full\nknowledge of policyholders' protected characteristics, which may in itself be\nproblematic. Here, we address this issue by using a multi-task neural network\narchitecture for claim predictions, which can be trained using only partial\ninformation on protected characteristics, and it produces prices that are free\nfrom proxy discrimination. We demonstrate the use of the proposed model and we\nfind that its predictive accuracy is comparable to a conventional feedforward\nneural network (on full information). However, this multi-task network has\nclearly superior performance in the case of partially missing policyholder\ninformation.\n"
    },
    {
        "paper_id": 2207.02832,
        "authors": "Grzegorz Marcjasz, Micha{\\l} Narajewski, Rafa{\\l} Weron and Florian\n  Ziel",
        "title": "Distributional neural networks for electricity price forecasting",
        "comments": null,
        "journal-ref": "Enrgy Economics, 125 (2023) 106843",
        "doi": "10.1016/j.eneco.2023.106843",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We present a novel approach to probabilistic electricity price forecasting\nwhich utilizes distributional neural networks. The model structure is based on\na deep neural network that contains a so-called probability layer. The\nnetwork's output is a parametric distribution with 2 (normal) or 4 (Johnson's\nSU) parameters. In a forecasting study involving day-ahead electricity prices\nin the German market, our approach significantly outperforms state-of-the-art\nbenchmarks, including LASSO-estimated regressions and deep neural networks\ncombined with Quantile Regression Averaging. The obtained results not only\nemphasize the importance of higher moments when modeling volatile electricity\nprices, but also -- given that probabilistic forecasting is the essence of risk\nmanagement -- provide important implications for managing portfolios in the\npower sector.\n"
    },
    {
        "paper_id": 2207.02858,
        "authors": "Svetlana Boyarchenko and Sergei Levendorski\\u{i}",
        "title": "Efficient inverse $Z$-transform and pricing barrier and lookback options\n  with discrete monitoring",
        "comments": "arXiv admin note: text overlap with arXiv:2207.02793",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We prove simple general formulas for expectations of functions of a random\nwalk and its running extremum. Under additional conditions, we derive\nanalytical formulas using the inverse $Z$-transform, the Fourier/Laplace\ninversion and Wiener-Hopf factorization, and discuss efficient numerical\nmethods for realization of these formulas. As applications, the cumulative\nprobability distribution function of the process and its running maximum and\nthe price of the option to exchange the power of a stock for its maximum are\ncalculated. The most efficient numerical methods use a new efficient numerical\nrealization of the inverse $Z$-transform, the sinh-acceleration technique and\nsimplified trapezoid rule. The program in Matlab running on a Mac with moderate\ncharacteristics achieves the precision E-10 and better in several dozen of\nmilliseconds, and E-14 - in a fraction of a isecond.\n"
    },
    {
        "paper_id": 2207.02896,
        "authors": "Honggao Cao",
        "title": "Mortgage-Rate-Adjusted Home Prices",
        "comments": "11 pages, 4 tables, 3 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
        "abstract": "  In this paper, we investigate the impact of mortgage rates on home prices,\nand how the impact may be used to help property purchase discussions at\nindividual buyer level and to adjust home price indices across time. A\nmortgage-rate-adjusted \"effective price\" is derived to measure near term\nproperty price in the presence of (expected) mortgage rate changes. A\nprice-mortgage rate neutrality line is then constructed based on the \"effective\nprice\" to help differentiate various market scenarios in the near term, which\ncan be used by prospective buyers in their \"to-buy or not-to-buy\"\ndeliberations. At the market level, effective home prices allow for\nneutralization of mortgage rates on the movement of the housing market. An\napplication of the neutralization strategy to the Case-Shiller Home Price Index\n(HPI) indicates that the U.S. housing market has been considerably affected by\nthe dynamics of mortgage rates in a long run. But mortgage rates do no appear a\nprimary driver of the extraordinary home price increase during the COVID-19\npandemic.\n"
    },
    {
        "paper_id": 2207.02902,
        "authors": "M. Lunkenheimer, A. Kracklauer, G. Klinkova, M. Grabinski",
        "title": "Homo economicus to model human behavior is ethically doubtful and\n  mathematically inconsistent",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  In many models in economics or business a dominantly self-interested homo\neconomicus is assumed. Unfortunately (or fortunately), humans are in general\nnot homines economici as e.g. the ultimatum game shows. This leads to the fact\nthat all these models are at least doubtful. Moreover, economists started to\nset a quantitative value for the feeling of social justice, altruism, or envy\nand the like to execute utilitarian calculation. Besides being ethically\ndoubtful, it delivers an explanation in hindsight with little predicting power.\nWe use examples from game theory to show its arbitrariness. It is even possible\nthat a stable Nash equilibrium can be calculated while it does not exist at\nall, due to the wide differences in human values. Finally, we show that\nassigned numbers for envy or altruism and the like do not build a field (in a\nmathematical sense). As there is no homomorphism to real numbers or a subset of\nit, any calculation is generally invalid or arbitrary. There is no (easy) way\nto fix the problem. One has to go back to ethical concepts like the categorical\nimperative or use at most semi quantitative approaches like considering knaves\nand knights. Mathematically one can only speculate whether e.g. surreal numbers\ncan make ethics calculable.\n"
    },
    {
        "paper_id": 2207.02947,
        "authors": "J. Cerda-Hernandez and A. Sikov and A. Ramos",
        "title": "An optimal investment strategy aimed at maximizing the expected utility\n  across all intermediate capital levels",
        "comments": "24 pages, 3 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This study investigates an optimal investment problem for an insurance\ncompany operating under the Cramer-Lundberg risk model, where investments are\nmade in both a risky asset and a risk-free asset. In contrast to other\nliterature that focuses on optimal investment and/or reinsurance strategies to\nmaximize the expected utility of terminal wealth within a given time horizon,\nthis work considers the expected value of utility accumulation across all\nintermediate capital levels of the insurer. By employing the Dynamic\nProgramming Principle, we prove a verification theorem, in order to show that\nany solution to the Hamilton-Jacobi-Bellman (HJB) equation solves our\noptimization problem. Subject to some regularity conditions on the solution of\nthe HJB equation, we establish the existence of the optimal investment\nstrategy. Finally, to illustrate the applicability of the theoretical findings,\nwe present numerical examples.\n"
    },
    {
        "paper_id": 2207.02948,
        "authors": "Carole Bernard and Gero Junike and Thibaut Lux and Steven Vanduffel",
        "title": "Cost-efficient Payoffs under Model Ambiguity",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Dybvig (1988a,b) solves in a complete market setting the problem of finding a\npayoff that is cheapest possible in reaching a given target distribution\n(\"cost-efficient payoff\"). In the presence of ambiguity, the distribution of a\npayoff is, however, no longer known with certainty. We study the problem of\nfinding the cheapest possible payoff whose worst-case distribution\nstochastically dominates a given target distribution (\"robust cost-efficient\npayoff\") and determine solutions under certain conditions. We study the link\nbetween \"robust cost-efficiency\" and the maxmin expected utility setting of\nGilboa and Schmeidler, as well as more generally with robust preferences in a\npossibly non-expected utility setting. Specifically, we show that solutions to\nmaxmin robust expected utility are necessarily robust cost-efficient. We\nillustrate our study with examples involving uncertainty both on the drift and\non the volatility of the risky asset.\n"
    },
    {
        "paper_id": 2207.02989,
        "authors": "Mnacho Echenim, Emmanuel Gobet and Anne-Claire Maurice",
        "title": "Unbiasing and robustifying implied volatility calibration in a\n  cryptocurrency market with large bid-ask spreads and missing quotes",
        "comments": "36 pages, 39 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
        "abstract": "  We design a novel calibration procedure that is designed to handle the\nspecific characteristics of options on cryptocurrency markets, namely large\nbid-ask spreads and the possibility of missing or incoherent prices in the\nconsidered data sets. We show that this calibration procedure is significantly\nmore robust and accurate than the standard one based on trade and mid-prices.\n"
    },
    {
        "paper_id": 2207.03194,
        "authors": "Tomokatsu Onaga, Fabio Caccioli, Teruyoshi Kobayashi",
        "title": "Financial fire sales as continuous-state complex contagion",
        "comments": "13 pages, 9 figures + SI",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Trading activities in financial systems create various channels through which\nsystemic risk can propagate. An important contagion channel is financial fire\nsales, where a bank failure causes asset prices to fall due to asset\nliquidation, which in turn drives further bank defaults, triggering the next\nrounds of liquidation. This process can be considered as complex contagion, yet\nit cannot be modeled using the conventional binary-state contagion models\nbecause there is a continuum of states representing asset prices. Here, we\ndevelop a threshold model of continuous-state cascades in which the states of\neach node are represented by real values. We show that the solution of a\nmulti-state contagion model, for which the continuous states are discretized,\naccurately replicates the simulated continuous state distribution as long as\nthe number of states is moderately large. This discretization approach allows\nus to exploit the power of approximate master equations (AME) to trace the\ntrajectory of the fraction of defaulted banks and obtain the distribution of\nasset prices that characterize the dynamics of fire sales on asset-bank\nbipartite networks. We examine the accuracy of the proposed method using real\ndata on asset-holding relationships in exchange-traded funds (ETFs).\n"
    },
    {
        "paper_id": 2207.03221,
        "authors": "M. Shadmangohar and S. M. S. Movahed",
        "title": "Clustering of Excursion Sets in Financial Market",
        "comments": "42 pages, 15 figures and one table. Comments are welcome",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Relying on the excursion set theory, we compute the number density of local\nextrema and crossing statistics versus the threshold for the stock market\nindices. Comparing the number density of excursion sets calculated numerically\nwith the theoretical prediction for the Gaussian process confirmed that all\ndata sets used in this paper have a surplus (almost lack) value of local\nextrema (up-crossing) density at low (high) thresholds almost around the mean\nvalue implying universal properties for stock indices. We estimate the\nclustering of geometrical measures based on the excess probability of finding\nthe pairs of excursion sets, which clarify well statistical coherency between\nmarkets located in the same geographical region. The cross-correlation of\nexcursion sets between various markets is also considered to construct the\nmatrix of agglomerative hierarchical clustering. Our results demonstrate that\nthe peak statistics is more capable of capturing blocks. Incorporating the\npartitioning approach, we implement the Singular Value Decomposition on the\nmatrix containing the maximum value of unweighted Two-Point Correlation\nFunction of peaks and up-crossing to compute the similarity measure. Our\nresults support that excursion sets are more sensitive than standard measures\nto elucidate the existence of {\\it a priori} crisis.\n"
    },
    {
        "paper_id": 2207.03352,
        "authors": "Joseph Jerome, Gregory Palmer, and Rahul Savani",
        "title": "Market Making with Scaled Beta Policies",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper introduces a new representation for the actions of a market maker\nin an order-driven market. This representation uses scaled beta distributions,\nand generalises three approaches taken in the artificial intelligence for\nmarket making literature: single price-level selection, ladder strategies and\n\"market making at the touch\". Ladder strategies place uniform volume across an\ninterval of contiguous prices. Scaled beta distribution based policies\ngeneralise these, allowing volume to be skewed across the price interval. We\ndemonstrate that this flexibility is useful for inventory management, one of\nthe key challenges faced by a market maker.\n  In this paper, we conduct three main experiments: first, we compare our more\nflexible beta-based actions with the special case of ladder strategies; then,\nwe investigate the performance of simple fixed distributions; and finally, we\ndevise and evaluate a simple and intuitive dynamic control policy that adjusts\nactions in a continuous manner depending on the signed inventory that the\nmarket maker has acquired. All empirical evaluations use a high-fidelity limit\norder book simulator based on historical data with 50 levels on each side.\n"
    },
    {
        "paper_id": 2207.03438,
        "authors": "Paolo Guasoni, Yu-Jui Huang",
        "title": "Minimizing the Repayment Cost of Federal Student Loans",
        "comments": null,
        "journal-ref": "SIAM Review, Vol. 64 (2022), No. 3, pp. 689-709",
        "doi": "10.1137/22M1505840",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Federal student loans are fixed-rate debt contracts with three main special\nfeatures: (i) borrowers can use income-driven schemes to make payments\nproportional to their income above subsistence, (ii) after several years of\ngood standing, the remaining balance is forgiven but taxed as ordinary income,\nand (iii) accrued interest is simple, i.e., not capitalized. For a very small\nloan, the cost-minimizing repayment strategy dictates maximum payments until\nfull repayment, forgoing both income-driven schemes and forgiveness. For a very\nlarge loan, the minimal payments allowed by income-driven schemes are optimal.\nFor intermediate balances, the optimal repayment strategy may entail an initial\nperiod of minimum payments to exploit the non-capitalization of accrued\ninterest, but when the principal is being reimbursed maximal payments always\nprecede minimum payments. Income-driven schemes and simple accrued interest\nmostly benefit borrowers with very large balances.\n"
    },
    {
        "paper_id": 2207.03672,
        "authors": "Zhaojia Huang, Liang Zhang, Tianhao Zhi",
        "title": "The Future of Traditional Fuel Vehicles (TFV) and New Energy Vehicles\n  (NEV): Creative Destruction or Co-existence?",
        "comments": "24 pages, 9 figures, paper accepted for the 19th International\n  Schumpeter Society (ISS) conference, 8-10 July, 2022",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  There is a rapid development and commercialization of new Energy Vehicles\n(NEV) in recent years. Although traditional fuel vehicles (TFV) still occupy a\nmajority share of the market, it is generally believed that NEV is more\nefficient, more environmental friendly, and has a greater potential of a\nSchumpeterian \"creative destruction\" that may lead to a paradigm shift in auto\nproduction and consumption. However, less is discussed regarding the potential\nenvironmental impact of NEV production and future uncertainty in R&D bottleneck\nof NEV technology and innovation. This paper aims to propose a modelling\nframework based on Lux (1995) that investigates the long-term dynamics of TFV\nand NEV, along with their associated environmental externality. We argue that\nenvironmental and technological policies will play a critical role in\ndetermining its future development. It is of vital importance to constantly\nmonitor the potential environmental impact of both sectors and support the R&D\nof critical NEV technology, as well as curbing its negative externality in a\npreemptive manner.\n"
    },
    {
        "paper_id": 2207.0371,
        "authors": "Francesca Biagini and Georg Bollweg and Katharina Oberpriller",
        "title": "Non-linear Affine Processes with Jumps",
        "comments": "31 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We present a probabilistic construction of $\\mathbb{R}^d$-valued non-linear\naffine processes with jumps. Given a set $\\Theta$ of affine parameters, we\ndefine a family of sublinear expectations on the Skorokhod space under which\nthe canonical process $X$ is a (sublinear) Markov process with a non-linear\ngenerator. This yields a tractable model for Knightian uncertainty for which\nthe sublinear expectation of a Markovian functional can be calculated via a\npartial integro-differential equation.\n"
    },
    {
        "paper_id": 2207.03816,
        "authors": "Chiara Dal Bianco and Andrea Moro",
        "title": "The welfare effects of nonlinear health dynamics",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We generate a continuous measure of health to estimate a non-parametric model\nof health dynamics, showing that adverse health shocks are highly persistent\nwhen suffered by people in poor health. Canonical models cannot account for\nthis pattern. We incorporate this health dynamic into a life-cycle model of\nconsumption, savings, and labor force participation. After estimating the model\nparameters, we simulate the effects of health shocks on economic outcomes. We\nfind that bad health shocks have long-term adverse economic effects that are\nmore extreme for those in poor health. Furthermore, bad health shocks also\nincrease the disparity of asset accumulation among this group of people. A\ncanonical model of health dynamics would not reveal these effects.\n"
    },
    {
        "paper_id": 2207.03883,
        "authors": "Kevin Kamm",
        "title": "An introduction to rating triggers for collateral-inclusive XVA in an\n  ICTMC framework",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  In this paper, we model the rating process of an entity as a piecewise\nhomogeneous continuous time Markov chain. We focus specifically on calibrating\nthe model to both historical data (rating transition matrices) and market data\n(CDS quotes), relying on a simple change of measure to switch from the\nhistorical probability to the risk-neutral one. We overcome some of the\nimperfections of the data by proposing a novel calibration procedure, which\nleads to an improvement of the entire scheme. We apply our model to compute\nbilateral credit and debit valuation adjustments of a netting set under a CSA\nwith thresholds depending on ratings of the two parties.\n"
    },
    {
        "paper_id": 2207.04004,
        "authors": "Tomas Scagliarini, Giuseppe Pappalardo, Alessio Emanuele Biondo,\n  Alessandro Pluchino, Andrea Rapisarda, Sebastiano Stramaglia",
        "title": "Pairwise and high-order dependencies in the cryptocurrency trading\n  network",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  In this paper we analyse the effects of information flows in cryptocurrency\nmarkets. We first define a cryptocurrency trading network, i.e. the network\nmade using cryptocurrencies as nodes and the Granger causality among their\nweekly log returns as links, later we analyse its evolution over time. In\nparticular, with reference to years 2020 and 2021, we study the logarithmic US\ndollar price returns of the cryptocurrency trading network using both pairwise\nand high-order statistical dependencies, quantified by Granger causality and\nO-information, respectively. With reference to the former, we find that it\nshows peaks in correspondence of important events, like e.g., Covid-19 pandemic\nturbulence or occasional sudden prices rise. The corresponding network\nstructure is rather stable, across weekly time windows in the period considered\nand the coins are the most influential nodes in the network. In the pairwise\ndescription of the network, stable coins seem to play a marginal role whereas,\nturning high-order dependencies, they appear in the highest number of\nsynergistic information circuits, thus proving that they play a major role for\nhigh order effects. With reference to redundancy and synergy with the time\nevolution of the total transactions in US dollars, we find that their large\nvolume in the first semester of 2021 seems to have triggered a transition in\nthe cryptocurrency network toward a more complex dynamical landscape. Our\nresults show that pairwise and high-order descriptions of complex financial\nsystems provide complementary information for cryptocurrency analysis.\n"
    },
    {
        "paper_id": 2207.041,
        "authors": "Alexander Barzykin, Philippe Bergault, Olivier Gu\\'eant",
        "title": "Dealing with multi-currency inventory risk in FX cash markets",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  In FX cash markets, market makers provide liquidity to clients for a wide\nvariety of currency pairs. Because of flow uncertainty and market volatility,\nthey face inventory risk. To mitigate this risk, they typically skew their\nprices to attract or divert the flow and trade with their peers on the\ndealer-to-dealer segment of the market for hedging purposes. This paper offers\na mathematical framework to FX dealers willing to maximize their expected\nprofit while controlling their inventory risk. Approximation techniques are\nproposed which make the framework scalable to any number of currency pairs.\n"
    },
    {
        "paper_id": 2207.04368,
        "authors": "Jerinsh Jeyapaulraj, Dhruv Desai, Peter Chu, Dhagash Mehta, Stefano\n  Pasquali, Philip Sommer",
        "title": "Supervised similarity learning for corporate bonds using Random Forest\n  proximities",
        "comments": "A few minor typos corrected, 1 figure added. Conclusions unchanged.\n  Matching with the accepted version",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Financial literature consists of ample research on similarity and comparison\nof financial assets and securities such as stocks, bonds, mutual funds, etc.\nHowever, going beyond correlations or aggregate statistics has been arduous\nsince financial datasets are noisy, lack useful features, have missing data and\noften lack ground truth or annotated labels. However, though similarity\nextrapolated from these traditional models heuristically may work well on an\naggregate level, such as risk management when looking at large portfolios, they\noften fail when used for portfolio construction and trading which require a\nlocal and dynamic measure of similarity on top of global measure. In this paper\nwe propose a supervised similarity framework for corporate bonds which allows\nfor inference based on both local and global measures. From a machine learning\nperspective, this paper emphasis that random forest (RF), which is usually\nviewed as a supervised learning algorithm, can also be used as a similarity\nlearning (more specifically, a distance metric learning) algorithm. In\naddition, this framework proposes a novel metric to evaluate similarities, and\nanalyses other metrics which further demonstrate that RF outperforms all other\nmethods experimented with, in this work.\n"
    },
    {
        "paper_id": 2207.04441,
        "authors": "Richard S.J. Tol",
        "title": "Nobel begets Nobel",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  I construct the professor-student network for laureates of and candidates for\nthe Nobel Prize in Economics. I study the effect of proximity to previous\nNobelists on winning the Nobel Prize. Conditional on being Nobel-worthy,\nstudents and grandstudents of Nobel laureates are significantly less likely to\nwin. Professors and fellow students of Nobel Prize winners, however, are\nsignificantly more likely to win.\n"
    },
    {
        "paper_id": 2207.0448,
        "authors": "Katherine Hoffmann Pham and Junpei Komiyama",
        "title": "Strategic Choices of Migrants and Smugglers in the Central Mediterranean\n  Sea",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The sea crossing from Libya to Italy is one of the world's most dangerous and\npolitically contentious migration routes, and yet over half a million people\nhave attempted the crossing since 2014. Leveraging data on aggregate migration\nflows and individual migration incidents, we estimate how migrants and\nsmugglers have reacted to changes in border enforcement, namely the rise in\ninterceptions by the Libyan Coast Guard starting in 2017 and the corresponding\ndecrease in the probability of rescue at sea. We find support for a deterrence\neffect in which attempted crossings along the Central Mediterranean route\ndeclined, and a diversion effect in which some migrants substituted to the\nWestern Mediterranean route. At the same time, smugglers adapted their tactics.\nUsing a strategic model of the smuggler's choice of boat size, we estimate how\nsmugglers trade off between the short-run payoffs to launching overcrowded\nboats and the long-run costs of making less successful crossing attempts under\ndifferent levels of enforcement. Taken together, these analyses shed light on\nhow the integration of incident- and flow-level datasets can inform ongoing\nmigration policy debates and identify potential consequences of changing\nenforcement regimes.\n"
    },
    {
        "paper_id": 2207.04496,
        "authors": "Ziheng Wang and Justin Sirignano",
        "title": "A Forward Propagation Algorithm for Online Optimization of Nonlinear\n  Stochastic Differential Equations",
        "comments": "arXiv admin note: substantial text overlap with arXiv:2202.06637",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Optimizing over the stationary distribution of stochastic differential\nequations (SDEs) is computationally challenging. A new forward propagation\nalgorithm has been recently proposed for the online optimization of SDEs. The\nalgorithm solves an SDE, derived using forward differentiation, which provides\na stochastic estimate for the gradient. The algorithm continuously updates the\nSDE model's parameters and the gradient estimate simultaneously. This paper\nstudies the convergence of the forward propagation algorithm for nonlinear\ndissipative SDEs. We leverage the ergodicity of this class of nonlinear SDEs to\ncharacterize the convergence rate of the transition semi-group and its\nderivatives. Then, we prove bounds on the solution of a Poisson partial\ndifferential equation (PDE) for the expected time integral of the algorithm's\nstochastic fluctuations around the direction of steepest descent. We then\nre-write the algorithm using the PDE solution, which allows us to characterize\nthe parameter evolution around the direction of steepest descent. Our main\nresult is a convergence theorem for the forward propagation algorithm for\nnonlinear dissipative SDEs.\n"
    },
    {
        "paper_id": 2207.04595,
        "authors": "Giuseppe Storti, Chao Wang",
        "title": "A semi-parametric marginalized dynamic conditional correlation framework",
        "comments": "35 pages, 6 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  We develop a novel multivariate semi-parametric framework for joint portfolio\nValue-at-Risk and Expected Shortfall forecasting. Unlike existing univariate\nsemi-parametric approaches, the proposed framework explicitly models the\ndependence structure among portfolio asset returns through a marginalized\ndynamic conditional correlation (DCC) parameterization. To estimate the model,\na two-step procedure based on the minimization of a strictly consistent scoring\nfunction derived from the Asymmetric Laplace distribution is developed. This\nprocedure allows to simultaneously estimate the marginalized DCC parameters and\nthe portfolio risk factors. The performance of the proposed model in risk\nforecasting and portfolio allocation is evaluated by means of a forecasting\nstudy on the components of the Dow Jones index for an out-of-sample period from\nDecember 2016 to September 2021. The empirical results support effectiveness of\nthe proposed framework compared to a variety of existing approaches.\n"
    },
    {
        "paper_id": 2207.04794,
        "authors": "Bartosz Uniejewski and Katarzyna Maciejowska",
        "title": "LASSO Principal Component Averaging -- a fully automated approach for\n  point forecast pooling",
        "comments": "Forthcoming in International Journal of Forecasting",
        "journal-ref": "International Journal of Forecasting 39 (2023) 1839-1852",
        "doi": "10.1016/j.ijforecast.2022.09.004",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper develops a novel, fully automated forecast averaging scheme, which\ncombines LASSO estimation method with Principal Component Averaging (PCA).\nLASSO-PCA (LPCA) explores a pool of predictions based on a single model but\ncalibrated to windows of different sizes. It uses information criteria to\nselect tuning parameters and hence reduces the impact of researchers' at hock\ndecisions. The method is applied to average predictions of hourly day-ahead\nelectricity prices over 650 point forecasts obtained with various lengths of\ncalibration windows. It is evaluated on four European and American markets with\nalmost two and a half year of out-of-sample period and compared to other semi-\nand fully automated methods, such as simple mean, AW/WAW, LASSO and PCA. The\nresults indicate that the LASSO averaging is very efficient in terms of\nforecast error reduction, whereas PCA method is robust to the selection of the\nspecification parameter. LPCA inherits the advantages of both methods and\noutperforms other approaches in terms of MAE, remaining insensitive the the\nchoice of a tuning parameter.\n"
    },
    {
        "paper_id": 2207.04856,
        "authors": "Philipp Brunner, Igor Letina and Armin Schmutzler",
        "title": "Research Joint Ventures: The Role of Financial Constraints",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper provides a novel theory of research joint ventures for financially\nconstrained firms. When firms choose R&D portfolios, an RJV can help to\ncoordinate research efforts, reducing investments in duplicate projects. This\ncan free up resources, increase the variety of pursued projects and thereby\nincrease the probability of discovering the innovation. RJVs improve innovation\noutcomes when market competition is weak and external financing conditions are\nbad. An RJV may increase the innovation probability and nevertheless lower\ntotal R&D costs. RJVs that increase innovation also increase consumer surplus\nand tend to be profitable, but innovation-reducing RJVs also exist. Finally, we\ncompare RJVs to innovation-enhancing mergers.\n"
    },
    {
        "paper_id": 2207.04867,
        "authors": "Vassilis Polimenis",
        "title": "The Lepto-Variance of Stock Returns",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  The Regression Tree (RT) sorts the samples using a specific feature and finds\nthe split point that produces the maximum variance reduction from a node to its\nchildren. Our key observation is that the best factor to use (in terms of MSE\ndrop) is always the target itself, as this most clearly separates the target.\nThus using the target as the splitting factor provides an upper bound on MSE\ndrop (or lower bound on the residual children MSE). Based on this observation,\nwe define the k-bit lepto-variance ${\\lambda}k^2$ of a target variable (or\nequivalently the lepto-variance at a specific depth k) as the variance that\ncannot be removed by any regression tree of a depth equal to k. As the upper\nbound performance for any feature, we believe ${\\lambda}k^2$ to be an\ninteresting statistical concept related to the underlying structure of the\nsample as it quantifies the resolving power of the RT for the sample. The max\nvariance that may be explained using RTs of depth up to k is called the sample\nk-bit macro-variance. At any depth, total sample variance is thus decomposed\ninto lepto-variance ${\\lambda}^2$ and macro-variance ${\\mu}^2$. We demonstrate\nthe concept, by performing 1- and 2-bit RT based lepto-structure analysis for\ndaily IBM stock returns.\n"
    },
    {
        "paper_id": 2207.04882,
        "authors": "Nabil Kahouadji",
        "title": "Variations on two-parameter families of forecasting functions:\n  seasonal/nonseasonal Models, comparison to the exponential smoothing and\n  ARIMA models, and applications to stock market data",
        "comments": "47 pages, 16 figures, 38 tables",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We introduce twenty four two-parameter families of advanced time series\nforecasting functions using a new and nonparametric approach. We also introduce\nthe concept of powering and derive nonseasonal and seasonal models with\nexamples in education, sales, finance and economy. We compare the performance\nof our twenty four models to both Holt--Winters and ARIMA models for both\nnonseasonal and seasonal times series. We show in particular that our models\nnot only do not require a decomposition of a seasonal time series into trend,\nseasonal and random components, but leads also to substantially lower sum of\nabsolute error and a higher number of closer forecasts than both Holt--Winters\nand ARIMA models. Finally, we apply and compare the performance of our twenty\nfour models using five-year stock market data of 467 companies of the S&P500.\n"
    },
    {
        "paper_id": 2207.04887,
        "authors": "Jun Lu, Minhui Wu",
        "title": "A note on VIX for postprocessing quantitative strategies",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  In this note, we introduce how to use Volatility Index (VIX) for\npostprocessing quantitative strategies so as to increase the Sharpe ratio and\nreduce trading risks. The signal from this procedure is an indicator of trading\nor not on a daily basis. Finally, we analyze this procedure on SH510300 and\nSH510050 assets. The strategies are evaluated by measurements of Sharpe ratio,\nmax drawdown, and Calmar ratio. However, there is always a risk of loss in\ntrading. The results from the tests are just examples of how the method works;\nno claim is made on the suggestion of real market positions.\n"
    },
    {
        "paper_id": 2207.0493,
        "authors": "Fabien Le Floc'h",
        "title": "Roughness of the Implied Volatility",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  The measures of roughness of the volatility in the litterature are based on\nthe realized volatility of high frequency data. Some authors show that this\nleads to a biased estimate, and does not necessarily indicate roughness of the\nunderlying volatility process. Here, we attempt to measure the roughness of the\nimplied volatility of short term options, as well as of the VIX index, and\nevaluate whether they may be more appropriate proxies of the underlying instant\nvolatility.\n"
    },
    {
        "paper_id": 2207.04959,
        "authors": "Dimitrios Vamvourellis, Mate Attila Toth, Dhruv Desai, Dhagash Mehta,\n  Stefano Pasquali",
        "title": "Learning Mutual Fund Categorization using Natural Language Processing",
        "comments": "8 pages, 5 figures, 2-column format",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Categorization of mutual funds or Exchange-Traded-funds (ETFs) have long\nserved the financial analysts to perform peer analysis for various purposes\nstarting from competitor analysis, to quantifying portfolio diversification.\nThe categorization methodology usually relies on fund composition data in the\nstructured format extracted from the Form N-1A. Here, we initiate a study to\nlearn the categorization system directly from the unstructured data as depicted\nin the forms using natural language processing (NLP). Positing as a multi-class\nclassification problem with the input data being only the investment strategy\ndescription as reported in the form and the target variable being the Lipper\nGlobal categories, and using various NLP models, we show that the\ncategorization system can indeed be learned with high accuracy. We discuss\nimplications and applications of our findings as well as limitations of\nexisting pre-trained architectures in applying them to learn fund\ncategorization.\n"
    },
    {
        "paper_id": 2207.04992,
        "authors": "Benedikt Janzen",
        "title": "Temperature and Mental Health: Evidence from Helpline Calls",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  This paper studies the short-term effects of ambient temperature on mental\nhealth using data on nearly half a million helpline calls in Germany.\nLeveraging location-based routing of helpline calls and random day-to-day\nweather fluctuations, I find a negative effect of temperature extremes on\nmental health as revealed by an increase in the demand for telephone counseling\nservices. On days with an average temperature above 25{\\deg}C (77{\\deg}F) and\nbelow 0{\\deg}C (32{\\deg}F), call volume is 3.4 and 5.1 percent higher,\nrespectively, than on mid-temperature days. Mechanism analysis reveals\npronounced adverse effects of cold temperatures on social and psychological\nwell-being and of hot temperatures on psychological well-being and violence.\nMore broadly, the findings of this work contribute to our understanding of how\nchanging climatic conditions will affect population mental health and\nassociated social costs in the near future.\n"
    },
    {
        "paper_id": 2207.05169,
        "authors": "Andr\\'es C\\'ardenas, Sergio Pulido, Rafael Serrano",
        "title": "Existence of optimal controls for stochastic Volterra equations",
        "comments": "28 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We provide sufficient conditions that guarantee the existence of relaxed\noptimal controls in the weak formulation of stochastic control problems for\nstochastic Volterra equations (SVEs). Our study can be applied to rough\nprocesses that arise when the kernel appearing in the controlled SVE is\nsingular at zero. The existence of relaxed optimal policies relies on the\ninteraction between integrability hypotheses on the kernel and growth\nconditions on the running cost functional and the coefficients of the\ncontrolled SVEs. Under classical convexity assumptions, we can also deduce the\nexistence of optimal strict controls.\n"
    },
    {
        "paper_id": 2207.05346,
        "authors": "Tomoya Mori, Takashi Akamatsu, Yuki Takayama, Minoru Osawa",
        "title": "Origin of power laws and their spatial fractal structure for city-size\n  distributions",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  City-size distributions follow an approximate power law in various countries\ndespite high volatility in relative city sizes over time. Our empirical\nevidence for the United States and Japan indicates that the scaling law stems\nfrom a spatial fractal structure owing to the coordination of industrial\nlocations. While the locations of individual industries change considerably\nover time, there is a persistent pattern in that more localized industries at a\ngiven time are found only in larger cities. The spatial organization of cities\nexhibits a hierarchical structure in which larger cities are spaced apart to\nserve as centers for surrounding smaller cities, generating a recursive pattern\nacross different spatial scales. In our theoretical replication of the observed\nregularities, diversity in scale economy among industries induces diversity in\ntheir location pattern, which translates into diversity in city size via\nspatial coordination of industries and population. The city-size power law is a\ngeneric feature of Monte-Carlo samples of stationary states resulting from the\nspontaneous spatial fractal structure in the hypothetical economy. The\nidentified regularities reveal constraints on feasible urban planning at each\nregional scale. The success or failure of place-based policies designed to take\nadvantage of individual cities' characteristics should depend on their spatial\nrelationships with other cities, subject to the nationwide spatial fractal\nstructure.\n"
    },
    {
        "paper_id": 2207.05394,
        "authors": "Luca Barbaglia, Christophe Croux, Ines Wilms",
        "title": "Detecting Anti-dumping Circumvention: A Network Approach",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Despite the increasing integration of the global economic system,\nanti-dumping measures are a common tool used by governments to protect their\nnational economy. In this paper, we propose a methodology to detect cases of\nanti-dumping circumvention through re-routing trade via a third country. Based\non the observed full network of trade flows, we propose a measure to proxy the\nevasion of an anti-dumping duty for a subset of trade flows directed to the\nEuropean Union, and look for possible cases of circumvention of an active\nanti-dumping duty. Using panel regression, we are able correctly classify 86%\nof the trade flows, on which an investigation of anti-dumping circumvention has\nbeen opened by the European authorities.\n"
    },
    {
        "paper_id": 2207.05701,
        "authors": "Jun Lu, Shao Yi",
        "title": "Autoencoding Conditional GAN for Portfolio Allocation Diversification",
        "comments": null,
        "journal-ref": "Applied Economics and Finance 9 (3), 55-68, 2022",
        "doi": "10.11114/aef.v9i3.5610",
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Over the decades, the Markowitz framework has been used extensively in\nportfolio analysis though it puts too much emphasis on the analysis of the\nmarket uncertainty rather than on the trend prediction. While generative\nadversarial network (GAN) and conditional GAN (CGAN) have been explored to\ngenerate financial time series and extract features that can help portfolio\nanalysis. The limitation of the CGAN framework stands in putting too much\nemphasis on generating series rather than keeping features that can help this\ngenerator. In this paper, we introduce an autoencoding CGAN (ACGAN) based on\ndeep generative models that learns the internal trend of historical data while\nmodeling market uncertainty and future trends. We evaluate the model on several\nreal-world datasets from both the US and Europe markets, and show that the\nproposed ACGAN model leads to better portfolio allocation and generates series\nthat are closer to true data compared to the existing Markowitz and CGAN\napproaches.\n"
    },
    {
        "paper_id": 2207.05939,
        "authors": "Kyungsub Lee",
        "title": "Application of Hawkes volatility in the observation of filtered\n  high-frequency price process in tick structures",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  The Hawkes model is suitable for describing self and mutually exciting random\nevents. In addition, the exponential decay in the Hawkes process allows us to\ncalculate the moment properties in the model. However, due to the complexity of\nthe model and formula, few studies have been conducted on the performance of\nHawkes volatility. In this study, we derived a variance formula that is\ndirectly applicable under the general settings of both unmarked and marked\nHawkes models for tick-level price dynamics. In the marked model, the linear\nimpact function and possible dependency between the marks and underlying\nprocesses are considered. The Hawkes volatility is applied to the mid-price\nprocess filtered at 0.1-second intervals to show reliable results; furthermore,\nintraday estimation is expected to have high utilization in real-time risk\nmanagement. We also note the increasing predictive power of intraday Hawkes\nvolatility over time and examine the relationship between futures and stock\nvolatilities.\n"
    },
    {
        "paper_id": 2207.06076,
        "authors": "Jussi T. S. Heikkila",
        "title": "Journal of Economic Literature codes classification system (JEL)",
        "comments": null,
        "journal-ref": "Heikkila, Jussi T. S. 2022. Journal of Economic Literature codes\n  classification system (JEL). ISKO Encyclopedia of Knowledge Organization,\n  eds. Birger Hjorland and Claudio Gnoli. jel",
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
        "abstract": "  The Journal of Economic Literature codes classification system (JEL)\npublished by the American Economic Association (AEA) is the de facto standard\nclassification system for research literature in economics. The JEL\nclassification system is used to classify articles, dissertations, books, book\nreviews, and working papers in EconLit, a database maintained by the AEA. Over\ntime, it has evolved and extended to a system with over 850 subclasses. This\npaper reviews the history and development of the JEL classification system,\ndescribes the current version, and provides a selective overview of its uses\nand applications in research. The JEL codes classification system has been\nadopted by several publishers, and their instructions are reviewed. There are\ninteresting avenues for future research as the JEL classification system has\nbeen surprisingly little used in existing bibliometric and scientometric\nresearch as well as in library classification systems.\n"
    },
    {
        "paper_id": 2207.06273,
        "authors": "Jos\\'e Pombal, Andr\\'e F. Cruz, Jo\\~ao Bravo, Pedro Saleiro, M\\'ario\n  A.T. Figueiredo, Pedro Bizarro",
        "title": "Understanding Unfairness in Fraud Detection through Model and Data Bias\n  Interactions",
        "comments": "KDD'22 Workshop on Machine Learning in Finance",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In recent years, machine learning algorithms have become ubiquitous in a\nmultitude of high-stakes decision-making applications. The unparalleled ability\nof machine learning algorithms to learn patterns from data also enables them to\nincorporate biases embedded within. A biased model can then make decisions that\ndisproportionately harm certain groups in society -- limiting their access to\nfinancial services, for example. The awareness of this problem has given rise\nto the field of Fair ML, which focuses on studying, measuring, and mitigating\nunfairness in algorithmic prediction, with respect to a set of protected groups\n(e.g., race or gender). However, the underlying causes for algorithmic\nunfairness still remain elusive, with researchers divided between blaming\neither the ML algorithms or the data they are trained on. In this work, we\nmaintain that algorithmic unfairness stems from interactions between models and\nbiases in the data, rather than from isolated contributions of either of them.\nTo this end, we propose a taxonomy to characterize data bias and we study a set\nof hypotheses regarding the fairness-accuracy trade-offs that fairness-blind ML\nalgorithms exhibit under different data bias settings. On our real-world\naccount-opening fraud use case, we find that each setting entails specific\ntrade-offs, affecting fairness in expected value and variance -- the latter\noften going unnoticed. Moreover, we show how algorithms compare differently in\nterms of accuracy and fairness, depending on the biases affecting the data.\nFinally, we note that under specific data bias conditions, simple\npre-processing interventions can successfully balance group-wise error rates,\nwhile the same techniques fail in more complex settings.\n"
    },
    {
        "paper_id": 2207.06285,
        "authors": "Kanis Saengchote, Talis Putni\\c{n}\\v{s}, Krislert Samphantharak",
        "title": "Does DeFi remove the need for trust? Evidence from a natural experiment\n  in stablecoin lending",
        "comments": "20 pages, 7 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  Decentralized Finance (DeFi) is built on a fundamentally different paradigm:\nrather than having to trust individuals and institutions, participants in DeFi\npotentially only have to trust computer code that is enforced by a\ndecentralized network of computers. We examine a natural experiment that\nexogenously stress tests this alternative paradigm by revealing the identities\nof individuals associated with a DeFi protocol, including a convicted criminal.\nWe find that, in practice, DeFi does not (yet) fully remove the need for trust\nin individuals. Our findings suggest that that because smart contracts are\nincomplete, they are subject to run risk (Allen and Gale, 2004) and personal\ncharacter and trust of individuals are still relevant in this alternative\nfinancial system.\n"
    },
    {
        "paper_id": 2207.06293,
        "authors": "Zhaoqi Zang, Richard Batley, Xiangdong Xu, David Z.W. Wang",
        "title": "On the value of distribution tail in the valuation of travel time\n  variability",
        "comments": null,
        "journal-ref": "2024",
        "doi": "10.1016/j.tre.2024.103695",
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  Extensive empirical studies show that the long distribution tail of travel\ntime and the corresponding unexpected delay can have much more serious\nconsequences than expected or moderate delay. However, the unexpected delay due\nto the distribution tail of travel time has received limited attention in\nrecent studies of the valuation of travel time variability. As a complement to\ncurrent valuation research, this paper proposes the concept of the value of\ntravel time distribution tail, which quantifies the value that travelers place\non reducing the unexpected delay for hedging against travel time variability.\nMethodologically, we define the summation of all unexpected delays as the\nunreliability area to quantify travel time distribution tail and show that it\nis a key element of two well-defined measures accounting for unreliable aspects\nof travel time. We then formally derive the value of distribution tail, show\nthat it is distinct from the more established value of reliability (VOR), and\ncombine it and the VOR in an overall value of travel time variability (VOV). We\nprove theoretically that the VOV exhibits diminishing marginal benefit in terms\nof the traveler's punctuality requirements under a validity condition. This\nimplies that it may be economically inefficient for travelers to blindly pursue\na higher probability of not being late. We then proceed to develop the concept\nof the travel time variability ratio, which gives the implicit cost of the\npunctuality requirement imposed on any given trip. Numerical examples reveal\nthat the cost of travel time distribution tail can account for more than 10% of\nthe trip cost, such that its omission could introduce non-trivial bias into\nroute choice models and transportation appraisal more generally.\n"
    },
    {
        "paper_id": 2207.06396,
        "authors": "Ioan Alexandru Puiu and Raphael Andreas Hauser",
        "title": "On Market Clearing of Day Ahead Auctions for European Power Markets:\n  Cost Minimisation versus Social Welfare Maximisation",
        "comments": "Corrected minor typos, 25 pages, 8 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  For the case of inflexible demand and considering network constraints, we\nintroduce a Cost Minimisation (CM) based market clearing mechanism, and a model\nrepresenting the standard Social Welfare Maximisation mechanism used in\nEuropean Day Ahead Electricity Markets. Since the CM model corresponds to a\nmore challenging optimisation problem, we propose four numerical algorithms\nthat leverage the problem structure, each with different trade-offs between\ncomputational cost and convergence guarantees. These algorithms are evaluated\non synthetic data to provide some intuition of their performance. We also\nprovide strong (but partial) analytical results to facilitate efficient\nsolution of the CM problem, which call for the introduction of a new concept:\noptimal zonal stack curves, and these results are used to devise one of the\nfour solution algorithms. An evaluation of the CM and SWM models and their\ncomparison is performed, under the assumption of truthful bidding, on the real\nworld data of Central Western European Day Ahead Power Market during the period\nof 2019-2020. We show that the SWM model we introduce gives a good\nrepresentation of the historical time series of the real prices. Further, the\nCM reduces the market power of producers, as generally this results in\ndecreased zonal prices and always decreases the total cost of electricity\nprocurement when compared to the currently employed SWM.\n"
    },
    {
        "paper_id": 2207.06544,
        "authors": "Gregory Benton, Wesley J. Maddox, Andrew Gordon Wilson",
        "title": "Volatility Based Kernels and Moving Average Means for Accurate\n  Forecasting with Gaussian Processes",
        "comments": "ICML 2022. Code available at https://github.com/g-benton/Volt",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  A broad class of stochastic volatility models are defined by systems of\nstochastic differential equations. While these models have seen widespread\nsuccess in domains such as finance and statistical climatology, they typically\nlack an ability to condition on historical data to produce a true posterior\ndistribution. To address this fundamental limitation, we show how to re-cast a\nclass of stochastic volatility models as a hierarchical Gaussian process (GP)\nmodel with specialized covariance functions. This GP model retains the\ninductive biases of the stochastic volatility model while providing the\nposterior predictive distribution given by GP inference. Within this framework,\nwe take inspiration from well studied domains to introduce a new class of\nmodels, Volt and Magpie, that significantly outperform baselines in stock and\nwind speed forecasting, and naturally extend to the multitask setting.\n"
    },
    {
        "paper_id": 2207.06605,
        "authors": "Shaswat Mohanty, Anirudh Vijay, Nandagopan Gopakumar",
        "title": "StockBot: Using LSTMs to Predict Stock Prices",
        "comments": "14 pages, 7 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  The evaluation of the financial markets to predict their behaviour have been\nattempted using a number of approaches, to make smart and profitable investment\ndecisions. Owing to the highly non-linear trends and inter-dependencies, it is\noften difficult to develop a statistical approach that elucidates the market\nbehaviour entirely. To this end, we present a long-short term memory (LSTM)\nbased model that leverages the sequential structure of the time-series data to\nprovide an accurate market forecast. We then develop a decision making StockBot\nthat buys/sells stocks at the end of the day with the goal of maximizing\nprofits. We successfully demonstrate an accurate prediction model, as a result\nof which our StockBot can outpace the market and can strategize for gains that\nare ~15 times higher than the most aggressive ETFs in the market.\n"
    },
    {
        "paper_id": 2207.06963,
        "authors": "N. Suresh and N.R. Bharathi",
        "title": "Effect of Demonetisation of on Indian High Denomination Currencies on\n  Indian Stock Market and its Relationship with Foreign Exchange Rate",
        "comments": "7 pages, 5 Figures, published in International Business Management in\n  2018",
        "journal-ref": "International Business Management 2018 12 205 211",
        "doi": "10.36478/ibm.2018.205.211",
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  This study examines the impact of the foreign exchange rate, i.e., US Dollar\nto Indian Rupee (USD/INR) on the Indian Stock Market Index (Nifty 50) during\nthe demonetization of high denomination Indian currencies. A daily rate of\nreturn of Foreign exchange rate (USD/INR) and the Indian Stock Market Index\n(Nifty 50) were considered for the study. The Dummy variable was used to\nmeasure the effect of demonetization during Nov/Dec 2016. The period of study\nwas restricted to 243 days from 1st April 2016 to 31st March 2017. The study\nreveals that there was an upward trend observed in the Indian Stock Market and\nthe Indian currency was strengthened with the decrease in the Foreign exchange\nrate (USD/INR).\n"
    },
    {
        "paper_id": 2207.07008,
        "authors": "Will Wolf, Aaron Henry, Hamza Al Fadel, Xavier Quintuna, Julian Gay",
        "title": "Scoring Aave Accounts for Creditworthiness",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Scoring the creditworthiness of accounts that interact with decentralized\nfinancial (DeFi) protocols remains an important yet unsolved problem. In this\npaper, we propose a credit scoring system for those accounts that have\ninteracted with the Aave v2 liquidity protocol. The key component of this\nsystem is a tree-based binary classifier that predicts \"position delinquency.\"\nTo the community, we provide our method, results, and the (abridged) dataset on\nwhich this system is built.\n"
    },
    {
        "paper_id": 2207.07183,
        "authors": "Bhaskarjit Sarmah, Nayana Nair, Dhagash Mehta, Stefano Pasquali",
        "title": "Learning Embedded Representation of the Stock Correlation Matrix using\n  Graph Machine Learning",
        "comments": "8 pages, 2 column format, 3 figure, 7 tables",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Understanding non-linear relationships among financial instruments has\nvarious applications in investment processes ranging from risk management,\nportfolio construction and trading strategies. Here, we focus on\ninterconnectedness among stocks based on their correlation matrix which we\nrepresent as a network with the nodes representing individual stocks and the\nweighted links between pairs of nodes representing the corresponding pair-wise\ncorrelation coefficients. The traditional network science techniques, which are\nextensively utilized in financial literature, require handcrafted features such\nas centrality measures to understand such correlation networks. However,\nmanually enlisting all such handcrafted features may quickly turn out to be a\ndaunting task. Instead, we propose a new approach for studying nuances and\nrelationships within the correlation network in an algorithmic way using a\ngraph machine learning algorithm called Node2Vec. In particular, the algorithm\ncompresses the network into a lower dimensional continuous space, called an\nembedding, where pairs of nodes that are identified as similar by the algorithm\nare placed closer to each other. By using log returns of S&P 500 stock data, we\nshow that our proposed algorithm can learn such an embedding from its\ncorrelation network. We define various domain specific quantitative (and\nobjective) and qualitative metrics that are inspired by metrics used in the\nfield of Natural Language Processing (NLP) to evaluate the embeddings in order\nto identify the optimal one. Further, we discuss various applications of the\nembeddings in investment management.\n"
    },
    {
        "paper_id": 2207.07222,
        "authors": "Fatemeh Nosrat",
        "title": "Assortment Optimization with Customer Choice Modeling in a Crowdfunding\n  Setting",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Crowdfunding, which is the act of raising funds from a large number of\npeople's contributions, is among the most popular research topics in economic\ntheory. Due to the fact that crowdfunding platforms (CFPs) have facilitated the\nprocess of raising funds by offering several features, we should take their\nexistence and survival in the marketplace into account. In this study, we\ninvestigated the significant role of platform features in a customer behavioral\nchoice model. In particular, we proposed a multinomial logit model to describe\nthe customers' (backers') behavior in a crowdfunding setting. We proceed by\ndiscussing the revenue-sharing model in these platforms. For this purpose, we\nconclude that an assortment optimization problem could be of major importance\nin order to maximize the platforms' revenue. We were able to derive a\nreasonable amount of data in some cases and implement two well-known machine\nlearning methods such as multivariate regression and classification problems to\npredict the best assortments the platform could offer to every arriving\ncustomer. We compared the results of these two methods and investigated how\nwell they perform in all cases.\n"
    },
    {
        "paper_id": 2207.07227,
        "authors": "S. Meghna, N. Suresh, J.C. Usha",
        "title": "A Study on Impact of Dividend Policy on Initial Public Offering Price\n  Performance",
        "comments": "7 PAGES 6 FIGURES 6 TABLES PUBLISHED IN Journal of Engineering and\n  Applied Sciences",
        "journal-ref": "Journal of Engineering and Applied Sciences 14 4501 4507 2019",
        "doi": "10.36478/jeasci.2019.4501.4507",
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  This study examines the impact of dividend policy on the performance of\ninitial public offerings in India. The period of study is from the year\n2011-2014. Monthly returns of the IPOs issued in the considered period and the\nIndian Stock Market Index (Nifty 50) were considered for the long-run\nperformance study. The methodological tools used are long-run performance\nstatistics and the GARCH model. The Dummy variable was used to measure the\neffect of dividends on the IPOs. The study reveals that the dividend policy has\nno significant effect on the stock prices of IPO.\n"
    },
    {
        "paper_id": 2207.0724,
        "authors": "Kate R. Schneider, Luc Christiaensen, Patrick Webb, William A. Masters",
        "title": "Assessing the Affordability of Nutrient-Adequate Diets",
        "comments": "33 pages, 4 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  The cost and affordability of least-cost healthy diets by time and place are\nincreasingly used as a proxy for access to nutrient-adequate diets. Recent work\nhas focused on the nutrient requirements of individuals, although most food and\nanti-poverty programs target whole households. This raises the question of how\nthe cost of a nutrient-adequate diet can be measured for an entire household.\nThis study identifies upper and lower bounds on the feasibility, cost, and\naffordability of meeting all household members' nutrient requirements using\n2013-2017 survey data from Malawi. Findings show only a minority of households\ncan afford the nutrient-adequate diet at either bound, with 20% of households\nable to afford the (upper bound) shared diets and 38% the individualized (lower\nbound) diets. Individualized diets are more frequently feasible with locally\navailable foods (90% vs. 60% of the time) and exhibit more moderate seasonal\nfluctuation. To meet all members' needs, a shared diet requires a more\nnutrient-dense combination of foods that is more costly and exhibits more\nseasonality in diet cost than any one food group or the individualized diets.\nThe findings further help adjudicate the extent to which nutritional behavioral\nchange programs versus broader agricultural and food policies can be relied\nupon to improve individual access to healthy diets.\n"
    },
    {
        "paper_id": 2207.07315,
        "authors": "Natkamon Tovanich, R\\'emy Cazabet",
        "title": "Pattern Analysis of Money Flow in the Bitcoin Blockchain",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Bitcoin is the first and highest valued cryptocurrency that stores\ntransactions in a publicly distributed ledger called the blockchain.\nUnderstanding the activity and behavior of Bitcoin actors is a crucial research\ntopic as they are pseudonymous in the transaction network. In this article, we\npropose a method based on taint analysis to extract taint flows --dynamic\nnetworks representing the sequence of Bitcoins transferred from an initial\nsource to other actors until dissolution. Then, we apply graph embedding\nmethods to characterize taint flows. We evaluate our embedding method with\ntaint flows from top mining pools and show that it can classify mining pools\nwith high accuracy. We also found that taint flows from the same period show\nhigh similarity. Our work proves that tracing the money flows can be a\npromising approach to classifying source actors and characterizing different\nmoney flow patterns\n"
    },
    {
        "paper_id": 2207.07467,
        "authors": "Phillip Murray, Ben Wood, Hans Buehler, Magnus Wiese, Mikko S.\n  Pakkanen",
        "title": "Deep Hedging: Continuous Reinforcement Learning for Hedging of General\n  Portfolios across Multiple Risk Aversions",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We present a method for finding optimal hedging policies for arbitrary\ninitial portfolios and market states. We develop a novel actor-critic algorithm\nfor solving general risk-averse stochastic control problems and use it to learn\nhedging strategies across multiple risk aversion levels simultaneously. We\ndemonstrate the effectiveness of the approach with a numerical example in a\nstochastic volatility environment.\n"
    },
    {
        "paper_id": 2207.0749,
        "authors": "Jane (Xue) Tan, Yong Tan",
        "title": "Crypto Rewards in Fundraising: Evidence from Crypto Donations to Ukraine",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Extrinsic incentives such as a conditional thank-you gift have shown both\npositive and negative impacts on charitable fundraising. Leveraging the crypto\ndonations to a Ukrainian fundraising plea that accepts Ether (i.e., the\ncurrency of the Ethereum blockchain) and Bitcoin (i.e., the currency of the\nBitcoin blockchain) over a seven-day period, we analyze the impact of crypto\nrewards that lasted for more than 24 hours. Crypto rewards are newly minted\ntokens that are usually valueless initially and grow in value if the\ncorresponding cause is well received. Separately, we find that crypto rewards\nhave a positive impact on the donation count but a negative impact on the\naverage donation size for donations from both blockchains. Comparatively, we\nfurther find that the crypto rewards lead to an 812.48% stronger donation count\nincrease for Ethereum than Bitcoin, given that the crypto rewards are more\nlikely to be issued on the Ethereum blockchain, which has higher\nprogrammability to support smart contracts. We also find a 30.1% stronger\ndecrease in average donation amount from Ethereum for small donations ($\\leq\n\\$250$); the rewards pose similar impacts on the average donation size for the\ntwo blockchains for large donations ($>\\$250$). Our study is the first work to\nlook into crypto rewards as incentives for fundraising. Our findings indicate\nthat the positive effect of crypto rewards is more likely to manifest in\ndonation count, and the negative effect of crypto rewards is more likely to\nmanifest in donation size.\n"
    },
    {
        "paper_id": 2207.07538,
        "authors": "Thomas Meissner, David Albrecht",
        "title": "Debt Aversion: Theory and Measurement",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  Debt aversion can have severe adverse effects on financial decision-making.\nWe propose a model of debt aversion, and design an experiment involving real\ndebt and saving contracts, to elicit and jointly estimate debt aversion with\npreferences over time, risk and losses. Structural estimations reveal that the\nvast majority of participants (89%) are debt averse, and that this has a strong\nimpact on choice. We estimate the \"borrowing premium\" - the compensation a debt\naverse person would require to accept getting into debt - to be around 16% of\nthe principal for our average participant.\n"
    },
    {
        "paper_id": 2207.07574,
        "authors": "Indrajit Saha and Veeraruna Kavitha",
        "title": "Systemic-risk and evolutionary stable strategies in a financial network",
        "comments": "37 pages, 3 figures and 4 tables. Accepted for publication in Dynamic\n  Games and Applications. arXiv admin note: text overlap with arXiv:2003.00886",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We consider a financial network represented at any time instance by a random\nliability graph which evolves over time. The agents connect through credit\ninstruments borrowed from each other or through direct lending, and these\ncreate the liability edges. These random edges are modified (locally) by the\nagents over time, as they learn from their experiences and (possibly imperfect)\nobservations. The settlement of the liabilities of various agents at the end of\nthe contract period (at any time instance) can be expressed as solutions of\nrandom fixed point equations. Our first step is to derive the solutions of\nthese equations (asymptotically and one for each time instance), using a recent\nresult on random fixed point equations. The agents, at any time instance, adapt\none of the two available strategies, risky or less risky investments, with an\naim to maximize their returns. We aim to study the emerging strategies of such\nreplicator dynamics that drives the financial network. We theoretically reduce\nthe analysis of the complex system to that of an appropriate ordinary\ndifferential equation (ODE). Using the attractors of the resulting ODE we show\nthat the replicator dynamics converges to one of the two pure evolutionary\nstable strategies (all risky or all less risky agents); one can have mixed\nlimit only when the observations are imperfect. We verify our theoretical\nfindings using exhaustive Monte Carlo simulations. The dynamics avoid the\nemergence of the systemic-risk regime (where majority default). However, if all\nthe agents blindly adapt risky strategy it can lead to systemic risk regime.\n"
    },
    {
        "paper_id": 2207.07578,
        "authors": "Shuo Sun, Rundong Wang, Bo An",
        "title": "Quantitative Stock Investment by Routing Uncertainty-Aware Trading\n  Experts: A Multi-Task Learning Approach",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Quantitative investment is a fundamental financial task that highly relies on\naccurate stock prediction and profitable investment decision making. Despite\nrecent advances in deep learning (DL) have shown stellar performance on\ncapturing trading opportunities in the stochastic stock market, we observe that\nthe performance of existing DL methods is sensitive to random seeds and network\ninitialization. To design more profitable DL methods, we analyze this\nphenomenon and find two major limitations of existing works. First, there is a\nnoticeable gap between accurate financial predictions and profitable investment\nstrategies. Second, investment decisions are made based on only one individual\npredictor without consideration of model uncertainty, which is inconsistent\nwith the workflow in real-world trading firms. To tackle these two limitations,\nwe first reformulate quantitative investment as a multi-task learning problem.\nLater on, we propose AlphaMix, a novel two-stage mixture-of-experts (MoE)\nframework for quantitative investment to mimic the efficient bottom-up trading\nstrategy design workflow of successful trading firms. In Stage one, multiple\nindependent trading experts are jointly optimized with an individual\nuncertainty-aware loss function. In Stage two, we train neural routers\n(corresponding to the role of a portfolio manager) to dynamically deploy these\nexperts on an as-needed basis. AlphaMix is also a universal framework that is\napplicable to various backbone network architectures with consistent\nperformance gains. Through extensive experiments on long-term real-world data\nspanning over five years on two of the most influential financial markets (US\nand China), we demonstrate that AlphaMix significantly outperforms many\nstate-of-the-art baselines in terms of four financial criteria.\n"
    },
    {
        "paper_id": 2207.07767,
        "authors": "Eric Luxenberg and Stephen Boyd and Mykel Kochenderfer and Misha van\n  Beek and Wen Cao and Steven Diamond and Alex Ulitsky and Kunal Menda and Vidy\n  Vairavamurthy",
        "title": "Strategic Asset Allocation with Illiquid Alternatives",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We address the problem of strategic asset allocation (SAA) with portfolios\nthat include illiquid alternative asset classes. The main challenge in\nportfolio construction with illiquid asset classes is that we do not have\ndirect control over our positions, as we do in liquid asset classes. Instead we\ncan only make commitments; the position builds up over time as capital calls\ncome in, and reduces over time as distributions occur, neither of which the\ninvestor has direct control over. The effect on positions of our commitments is\nsubject to a delay, typically of a few years, and is also unknown or\nstochastic. A further challenge is the requirement that we can meet the capital\ncalls, with very high probability, with our liquid assets.\n  We formulate the illiquid dynamics as a random linear system, and propose a\nconvex optimization based model predictive control (MPC) policy for allocating\nliquid assets and making new illiquid commitments in each period. Despite the\nchallenges of time delay and uncertainty, we show that this policy attains\nperformance surprisingly close to a fictional setting where we pretend the\nilliquid asset classes are completely liquid, and we can arbitrarily and\nimmediately adjust our positions. In this paper we focus on the growth problem,\nwith no external liabilities or income, but the method is readily extended to\nhandle this case.\n"
    },
    {
        "paper_id": 2207.07848,
        "authors": "Xiaoshan Chen, Xun Li, Fahuai Yi, Xiang Yu",
        "title": "Optimal consumption under a drawdown constraint over a finite horizon",
        "comments": "Keywords: Optimal consumption, drawdown constraint, parabolic\n  variational inequality, gradient constraint, free boundary",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper studies a finite horizon utility maximization problem on excessive\nconsumption under a drawdown constraint up to the bankruptcy time. Our control\nproblem is an extension of the one considered in Bahman et al. (2019) to the\nmodel with a finite horizon and also an extension of the one considered in Jeon\nand Oh (2022) to the model with zero interest rate. Contrary to Bahman et al.\n(2019), we encounter a parabolic nonlinear HJB variational inequality with a\ngradient constraint, in which some time-dependent free boundaries complicate\nthe analysis significantly. Meanwhile, our methodology is built on technical\nPDE arguments, which differs substantially from the martingale approach in Jeon\nand Oh (2022). Using dual transform and considering some auxiliary parabolic\nvariational inequalities with both gradient and function constraints, we\nestablish the existence and uniqueness of the classical solution to the HJB\nvariational inequality and characterize all associated free boundaries in\nanalytical form. Consequently, the piecewise optimal feedback controls and some\ntime-dependent thresholds of the wealth variable for different control\nexpressions can be obtained.\n"
    },
    {
        "paper_id": 2207.07985,
        "authors": "Xize Wang (National University of Singapore), Tao Liu (Peking\n  University)",
        "title": "Home-made blues: Residential crowding and mental health in Beijing,\n  China",
        "comments": null,
        "journal-ref": "Urban Studies (2022)",
        "doi": "10.1177/00420980221101707",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Although residential crowding has many well-being implications, its\nconnection to mental health is yet to be widely examined. Using survey data\nfrom 1613 residents in Beijing, China, we find that living in a crowded place -\nmeasured by both square metres per person and persons per bedroom - is\nsignificantly associated with a higher risk of depression. We test for the\nmechanisms of such associations and find that the residential\ncrowding-depression link arises through increased living space-specific stress\nrather than increased life stress. We also identify the following subgroups\nthat have relatively stronger residential crowding-depression associations:\nfemales, those living with children, those not living with parents, and those\nliving in non-market housing units. Our findings show that inequality in living\nspace among urban residents not only is an important social justice issue but\nalso has health implications.\n"
    },
    {
        "paper_id": 2207.0799,
        "authors": "Xize Wang (National University of Singapore), Tao Liu (Peking\n  University)",
        "title": "The Roads One Must Walk Down: Commute and Depression for Beijing's\n  Residents",
        "comments": null,
        "journal-ref": "Transp. Res. Part D: Transp. Environ., 109 (2022), Article 103316",
        "doi": "10.1016/j.trd.2022.103316",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  As a vital aspect of individual's quality of life, mental health has been\nincluded as an important component of the U.N. Sustainable Development Goals.\nThis study focuses on a specific aspect of mental health: depression, and\nexamines its relationship with commute patterns. Using survey data from 1,528\nresidents in Beijing, China, we find that every 10 additional minutes of\ncommute time is associated with 1.1% higher likelihood of depression. We test\nfor the mechanisms of the commute-depression link and find that commute is\nassociated with depression as a direct stressor rather than triggering higher\nwork stress. When decomposing commute time into mode-specific time, we found\nthat time on mopeds/motorcycles has the strongest association with depression.\nMoreover, the commute-depression associations are stronger for older workers\nand blue-collar workers. Hence, policies that could reduce commute time,\nencourage work from home, improve job-housing balance or increase\nmotorcyclists' safety would help promote mental health.\n"
    },
    {
        "paper_id": 2207.08053,
        "authors": "Bijesh Mishra",
        "title": "Adoption of Sustainable Agricultural Practices among Kentucky Farmers\n  and Their Perception about Farm Sustainability",
        "comments": "125 Pages, MS thesis",
        "journal-ref": null,
        "doi": "10.1007/s00267-018-1109-3",
        "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
        "abstract": "  The purpose of this research was to identify commonly adopted SAPs and their\nadoption among Kentucky farmers. The specific objectives were to explore\nfarmers' Perceptions about farm and farming practice sustainability, to\nidentify predictors of SAPs adoption using farm attributes, farmers' attitudes\nand behaviors, socioeconomic and demographic factors, and knowledge, and to\nevaluate adoption barriers of SAPs among Kentucky Farmers. Farmers generally\nperceive that their farm and farming activities attain the objectives of\nsustainable agriculture. Inadequate knowledge, perceived difficulty of\nimplementation, lack of market, negative attitude about technologies, and lack\nof technologies were major adoption barriers of SAPs in Kentucky.\n"
    },
    {
        "paper_id": 2207.08282,
        "authors": "Huaxin Wang-Lu and Octasiano Miguel Valerio Mendoza",
        "title": "Job Prospects and Labour Mobility in China",
        "comments": null,
        "journal-ref": null,
        "doi": "10.1080/09638199.2022.2157463",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  China's structural changes have brought new challenges to its regional\nemployment structures, entailing labour redistribution. By now Chinese research\non migration decisions with a forward-looking stand and on bilateral\nlongitudinal determinants at the prefecture city level is almost non-existent.\nThis paper investigates the effects of sector-based job prospects on individual\nmigration decisions across prefecture boundaries. To this end, we created a\nproxy variable for job prospects, compiled a unique quasi-panel of 66,427\nindividuals from 283 cities during 1997--2017, introduced reference-dependence\nto the random utility maximisation model of migration in a sequential setting,\nderived empirical specifications with theoretical micro-foundations, and\napplied various monadic and dyadic fixed effects to address multilateral\nresistance to migration. Multilevel logit models and two-step system GMM\nestimation were adopted for the robustness check. Our primary findings are that\na 10% increase in the ratio of sector-based job prospects in cities of\ndestination to cities of origin raises the probability of migration by\n1.281--2.185 percentage points, and the effects tend to be stronger when the\nscale of the ratio is larger. Having a family migration network causes an\nincrease of approximately 6 percentage points in migratory probabilities.\nFurther, labour migrants are more likely to be male, unmarried, younger, or\nmore educated. Our results suggest that the ongoing industrial reform in China\ninfluences labour mobility between cities, providing important insights for\nregional policymakers to prevent brain drain and to attract relevant talent.\n"
    },
    {
        "paper_id": 2207.08613,
        "authors": "Marcelo Brutti Righi and Marlon Ruoso Moresco",
        "title": "Star-Shaped deviations",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We propose the Star-Shaped deviation measures in the same vein as Star-Shaped\nrisk measures and Star-Shaped acceptability indexes. We characterize\nStar-Shaped deviation measures through Star-Shaped acceptance sets and as the\nminimum of a family of Convex deviation measures. We also expose an interplay\nbetween Star-Shaped risk measures and deviation measures.\n"
    },
    {
        "paper_id": 2207.08941,
        "authors": "Carolina E S Mattsson, Teodoro Criscione, Frank W Takes",
        "title": "Circulation of a digital community currency",
        "comments": null,
        "journal-ref": "Scientific Reports 13, 5864 (2023)",
        "doi": "10.1038/s41598-023-33184-1",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Circulation is the characteristic feature of successful currency systems,\nfrom community currencies to cryptocurrencies to national currencies. In this\npaper, we propose a network analysis approach especially suited for studying\ncirculation given a system's digital transaction records. Sarafu is a digital\ncommunity currency that was active in Kenya over a period that saw considerable\neconomic disruption due to the COVID-19 pandemic. We represent its circulation\nas a network of monetary flow among the 40,000 Sarafu users. Network flow\nanalysis reveals that circulation was highly modular, geographically localized,\nand occurring among users with diverse livelihoods. Across localized\nsub-populations, network cycle analysis supports the intuitive notion that\ncirculation requires cycles. Moreover, the sub-networks underlying circulation\nare consistently degree disassortative and we find evidence of preferential\nattachment. Community-based institutions often take on the role of local hubs,\nand network centrality measures confirm the importance of early adopters and of\nwomen's participation. This work demonstrates that networks of monetary flow\nenable the study of circulation within currency systems at a striking level of\ndetail, and our findings can be used to inform the development of community\ncurrencies in marginalized areas.\n"
    },
    {
        "paper_id": 2207.09036,
        "authors": "David Roodman",
        "title": "Schooling and Labor Market Consequences of School Construction in\n  Indonesia: Comment",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
        "abstract": "  Duflo (2001) exploits a 1970s schooling expansion to estimate the returns to\nschooling in Indonesia. Under the study's difference-in-differences (DID)\ndesign, two patterns in the data--shallower pay scales for younger workers and\nnegative selection in treatment--can violate the parallel trends assumption and\nupward-bias results. In response, I follow up later, test for trend breaks\ntimed to the intervention, and perform changes-in-changes (CIC). I also correct\ndata errors, cluster variance estimates, incorporate survey weights to correct\nfor endogenous sampling, and test for (and detect) instrument weakness. Weak\nidentification-robust inference yields positive but imprecise estimates. CIC\nestimates also tilt positive.\n"
    },
    {
        "paper_id": 2207.09843,
        "authors": "Jakob Schwerter, Nicolai Netz, and Nicolas H\\\"ubner",
        "title": "Do school reforms shape study behavior at university? Evidence from an\n  instructional time reform",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  Early-life environments can have long-lasting developmental effects.\nInterestingly, research on how school reforms affect later-life study behavior\nhas hardly adopted this perspective. Therefore, we investigated a staggered\nschool reform that reduced the number of school years and increased weekly\ninstructional time for secondary school students in most German federal states.\nWe analyzed this quasi-experiment in a difference-in-differences framework\nusing representative large-scale survey data on 71,426 students who attended\nuniversity between 1998 and 2016. We found negative effects of reform exposure\non hours spent attending classes and on self-study, and a larger time gap\nbetween school completion and higher education entry. Our results support the\nview that research should examine unintended long-term effects of school\nreforms on individual life courses.\n"
    },
    {
        "paper_id": 2207.09951,
        "authors": "Bruno Ga\\v{s}perov, Zvonko Kostanj\\v{c}ar",
        "title": "Deep Reinforcement Learning for Market Making Under a Hawkes\n  Process-Based Limit Order Book Model",
        "comments": "6 pages, 4 figures",
        "journal-ref": "IEEE Control Systems Letters 6 (2022): 2485-2490",
        "doi": "10.1109/LCSYS.2022.3166446",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The stochastic control problem of optimal market making is among the central\nproblems in quantitative finance. In this paper, a deep reinforcement\nlearning-based controller is trained on a weakly consistent, multivariate\nHawkes process-based limit order book simulator to obtain market making\ncontrols. The proposed approach leverages the advantages of Monte Carlo\nbacktesting and contributes to the line of research on market making under\nweakly consistent limit order book models. The ensuing deep reinforcement\nlearning controller is compared to multiple market making benchmarks, with the\nresults indicating its superior performance with respect to various risk-reward\nmetrics, even under significant transaction costs.\n"
    },
    {
        "paper_id": 2207.1006,
        "authors": "Karel in 't Hout, Pieter Lamotte",
        "title": "Efficient numerical valuation of European options under the two-asset\n  Kou jump-diffusion model",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper concerns the numerical solution of the two-dimensional\ntime-dependent partial integro-differential equation (PIDE) that holds for the\nvalues of European-style options under the two-asset Kou jump-diffusion model.\nA main feature of this equation is the presence of a nonlocal double integral\nterm. For its numerical evaluation, we extend a highly efficient algorithm\nderived by Toivanen (2008) in the case of the one-dimensional Kou integral. The\nacquired algorithm for the two-dimensional Kou integral has optimal\ncomputational cost: the number of basic arithmetic operations is directly\nproportional to the number of spatial grid points in the semidiscretization.\nFor the effective discretization in time, we study seven contemporary operator\nsplitting schemes of the implicit-explicit (IMEX) and the alternating direction\nimplicit (ADI) kind. All these schemes allow for a convenient, explicit\ntreatment of the integral term. We analyze their (von Neumann) stability. By\nample numerical experiments for put-on-the-average option values, the actual\nconvergence behavior as well as the mutual performance of the seven operator\nsplitting schemes are investigated. Moreover, the Greeks Delta and Gamma are\nconsidered.\n"
    },
    {
        "paper_id": 2207.10071,
        "authors": "Jun-Cheng Chen, Cong-Xiao Chen, Li-Juan Duan, Zhi Cai",
        "title": "DDPG based on multi-scale strokes for financial time series trading\n  strategy",
        "comments": "10 pages,5 figures,to be published in 2022 8th International\n  Conference on Computer Technology Applications conference",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  With the development of artificial intelligence,more and more financial\npractitioners apply deep reinforcement learning to financial trading\nstrategies.However,It is difficult to extract accurate features due to the\ncharacteristics of considerable noise,highly non-stationary,and non-linearity\nof single-scale time series,which makes it hard to obtain high returns.In this\npaper,we extract a multi-scale feature matrix on multiple time scales of\nfinancial time series,according to the classic financial theory-Chan Theory,and\nput forward to an approach of multi-scale stroke deep deterministic policy\ngradient reinforcement learning model(MSSDDPG)to search for the optimal trading\nstrategy.We carried out experiments on the datasets of the Dow Jones,S&P 500 of\nU.S. stocks, and China's CSI 300,SSE Composite,evaluate the performance of our\napproach compared with turtle trading strategy, Deep\nQ-learning(DQN)reinforcement learning strategy,and deep deterministic policy\ngradient (DDPG) reinforcement learning strategy.The result shows that our\napproach gets the best performance in China CSI 300,SSE Composite,and get an\noutstanding result in Dow Jones,S&P 500 of U.S.\n"
    },
    {
        "paper_id": 2207.1037,
        "authors": "Elisa Al\\`os, Frido Rolloos, and Kenichiro Shiraya",
        "title": "Forward start volatility swaps in rough volatility models",
        "comments": "arXiv admin note: text overlap with arXiv:1912.05383",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper shows the relationship between the forward start volatility swap\nprice and the forward start zero vanna implied volatility of forward start\noptions in rough volatility models. It is shown that in the short\ntime-to-maturity limit the approximation error in the leading term of the\ncorrelated case with $H\\in(0,\\frac12)$ does not depend on the time to forward\nstart date, but only on the difference between the maturity date and forward\nstart date and on the Hurst parameter $H$.\n"
    },
    {
        "paper_id": 2207.10373,
        "authors": "Griselda Deelstra, Lech A. Grzelak, Felix L. Wolf",
        "title": "Sensitivities and Hedging of the Collateral Choice Option",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The collateral choice option allows a collateral-posting party the\nopportunity to change the type of security in which the collateral is\ndeposited. Due to non-zero collateral basis spreads, this optionality\nsignificantly impacts asset valuation. Because of the complexity of valuing the\noption, many practitioners resort to deterministic assumptions on the\ncollateral rates. In this article, we focus on a valuation model of the\ncollateral choice option based on stochastic dynamics. Intrinsic differences in\nthe resulting collateral choice option valuation and its implications for\ncollateral management are presented. We obtain sensitivities of the collateral\nchoice option price under both the deterministic and the stochastic model, and\nwe show that the stochastic model attributes risks to all involved collateral\ncurrencies. Besides an inability to capture volatility effects, the\ndeterministic model exhibits a digital structure in which only the\ncheapest-to-deliver currency influences the valuation at a given time. We\nfurther consider hedging an asset with the collateral choice option by a\nportfolio of domestic and foreign zero-coupon bonds that do not carry the\ncollateral choice option. We propose static hedging strategies based on the\ncrossing times of the deterministic model and based on variance-minimization\nunder the stochastic model. We show how the weights of this model can be\nexplicitly determined with the semi-analytical common factor approach and we\nshow in numerical experiments that this strategy offers good hedging\nperformance under minimized variance.\n"
    },
    {
        "paper_id": 2207.10476,
        "authors": "Andrey Shternshis, Piero Mazzarisi, Stefano Marmi",
        "title": "Efficiency of the Moscow Stock Exchange before 2022",
        "comments": "21 pages, 5 figures",
        "journal-ref": "Shternshis, A.; Mazzarisi, P.; Marmi, S. Efficiency of the Moscow\n  Stock Exchange before 2022. Entropy 2022, 24, 1184",
        "doi": "10.3390/e24091184",
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  This paper investigates the degree of efficiency for the Moscow Stock\nExchange. A market is called efficient if prices of its assets fully reflect\nall available information. We show that the degree of market efficiency is\nsignificantly low for most of the months from 2012 to 2021. We calculate the\ndegree of market efficiency by (i) filtering out regularities in financial data\nand (ii) computing the Shannon entropy of the filtered return time series. We\nhave developed a simple method for estimating volatility and price staleness in\nempirical data, in order to filter out such regularity patterns from return\ntime series. The resulting financial time series of stocks' returns are then\nclustered into different groups according to some entropy measures. In\nparticular, we use the Kullback-Leibler distance and a novel entropy metric\ncapturing the co-movements between pairs of stocks. By using Monte Carlo\nsimulations, we are then able to identify the time periods of market\ninefficiency for a group of 18 stocks. The inefficiency of the Moscow Stock\nExchange that we have detected is a signal of the possibility of devising\nprofitable strategies, net of transaction costs. The deviation from the\nefficient behavior for a stock strongly depends on the industrial sector it\nbelongs.\n"
    },
    {
        "paper_id": 2207.10539,
        "authors": "Weronika Ormaniec, Marcin Pitera, Sajad Safarveisi, Thorsten Schmidt",
        "title": "Estimating value at risk: LSTM vs. GARCH",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Estimating value-at-risk on time series data with possibly heteroscedastic\ndynamics is a highly challenging task. Typically, we face a small data problem\nin combination with a high degree of non-linearity, causing difficulties for\nboth classical and machine-learning estimation algorithms. In this paper, we\npropose a novel value-at-risk estimator using a long short-term memory (LSTM)\nneural network and compare its performance to benchmark GARCH estimators.\n  Our results indicate that even for a relatively short time series, the LSTM\ncould be used to refine or monitor risk estimation processes and correctly\nidentify the underlying risk dynamics in a non-parametric fashion. We evaluate\nthe estimator on both simulated and market data with a focus on\nheteroscedasticity, finding that LSTM exhibits a similar performance to GARCH\nestimators on simulated data, whereas on real market data it is more sensitive\ntowards increasing or decreasing volatility and outperforms all existing\nestimators of value-at-risk in terms of exception rate and mean quantile score.\n"
    },
    {
        "paper_id": 2207.10577,
        "authors": "Xize Wang (University of Southern California), Greg Lindsey\n  (University of Minnesota), Jessica E. Schoner (University of Minnesota), and\n  Andrew Harrison (San Francisco Municipal Transportation Agency)",
        "title": "Modeling Bike Share Station Activity: Effects of Nearby Businesses and\n  Jobs on Trips to and from Stations",
        "comments": null,
        "journal-ref": "Journal of Urban Planning and Development 142(1) (2016) 04015001",
        "doi": "10.1061/(ASCE)UP.1943-5444.0000273",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The purpose of this research is to identify correlates of bike station\nactivity for Nice Ride Minnesota, a bike share system in Minneapolis - St. Paul\nMetropolitan Area in Minnesota. We obtained the number of trips to and from\neach of the 116 bike share stations operating in 2011 from Nice Ride Minnesota.\nData for independent variables included in models come from a variety of\nsources; including the 2010 US Census, the Metropolitan Council, a regional\nplanning agency, and the cities of Minneapolis and St. Paul. We use log-linear\nand negative binomial regression models to evaluate the marginal effects of\nthese factors on average daily station trips. Our models have high goodness of\nfit, and each of 13 independent variables is significant at the 10% level or\nhigher. The number of trips at Nice Ride stations is associated with\nneighborhood socio demographics (i.e., age and race), proximity to the central\nbusiness district, proximity to water, accessibility to trails, distance to\nother bike share stations, and measures of economic activity. Analysts can use\nthese results to optimize bike share operations, locate new stations, and\nevaluate the potential of new bike share programs.\n"
    },
    {
        "paper_id": 2207.10705,
        "authors": "Kara Karpman, Samriddha Lahiry, Diganta Mukherjee, and Sumanta Basu",
        "title": "Exploring Financial Networks Using Quantile Regression and Granger\n  Causality",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In the post-crisis era, financial regulators and policymakers are\nincreasingly interested in data-driven tools to measure systemic risk and to\nidentify systemically important firms. Granger Causality (GC) based techniques\nto build networks among financial firms using time series of their stock\nreturns have received significant attention in recent years. Existing GC\nnetwork methods model conditional means, and do not distinguish between\nconnectivity in lower and upper tails of the return distribution - an aspect\ncrucial for systemic risk analysis. We propose statistical methods that measure\nconnectivity in the financial sector using system-wide tail-based analysis and\nis able to distinguish between connectivity in lower and upper tails of the\nreturn distribution. This is achieved using bivariate and multivariate GC\nanalysis based on regular and Lasso penalized quantile regressions, an approach\nwe call quantile Granger causality (QGC). By considering centrality measures of\nthese financial networks, we can assess the build-up of systemic risk and\nidentify risk propagation channels. We provide an asymptotic theory of QGC\nestimators under a quantile vector autoregressive model, and show its benefit\nover regular GC analysis on simulated data. We apply our method to the monthly\nstock returns of large U.S. firms and demonstrate that lower tail based\nnetworks can detect systemically risky periods in historical data with higher\naccuracy than mean-based networks. In a similar analysis of large Indian banks,\nwe find that upper and lower tail networks convey different information and\nhave the potential to distinguish between periods of high connectivity that are\ngoverned by positive vs negative news in the market.\n"
    },
    {
        "paper_id": 2207.10709,
        "authors": "Marc Mukendi Mpanda",
        "title": "Malliavin differentiability of fractional Heston-type model and\n  applications to option pricing",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper defines fractional Heston-type (fHt) model as an arbitrage-free\nfinancial market model with the infinitesimal return volatility described by\nthe square of a single stochastic equation with respect to fractional Brownian\nmotion with Hurst parameter H in (0, 1). We extend the idea of Alos and [Alos,\nE., & Ewald, C. O. (2008). Malliavin differentiability of the Heston volatility\nand applications to option pricing. Advances in Applied Probability, 40(1),\n144-162.] to prove that fHt model is Malliavin differentiable and deduce an\nexpression of expected payoff function having discontinuity of any kind. Some\nsimulations of stock price process and option prices are performed.\n"
    },
    {
        "paper_id": 2207.10838,
        "authors": "Tianchen Zhao and Chuhao Sun and Asaf Cohen and James Stokes and\n  Shravan Veerapaneni",
        "title": "Quantum-inspired variational algorithms for partial differential\n  equations: Application to financial derivative pricing",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Variational quantum Monte Carlo (VMC) combined with neural-network quantum\nstates offers a novel angle of attack on the curse-of-dimensionality\nencountered in a particular class of partial differential equations (PDEs);\nnamely, the real- and imaginary time-dependent Schr\\\"odinger equation. In this\npaper, we present a simple generalization of VMC applicable to arbitrary\ntime-dependent PDEs, showcasing the technique in the multi-asset Black-Scholes\nPDE for pricing European options contingent on many correlated underlying\nassets.\n"
    },
    {
        "paper_id": 2207.11152,
        "authors": "Feiyang Pan, Tongzhe Zhang, Ling Luo, Jia He, Shuoling Liu",
        "title": "Learn Continuously, Act Discretely: Hybrid Action-Space Reinforcement\n  Learning For Optimal Execution",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Optimal execution is a sequential decision-making problem for cost-saving in\nalgorithmic trading. Studies have found that reinforcement learning (RL) can\nhelp decide the order-splitting sizes. However, a problem remains unsolved: how\nto place limit orders at appropriate limit prices? The key challenge lies in\nthe \"continuous-discrete duality\" of the action space. On the one hand, the\ncontinuous action space using percentage changes in prices is preferred for\ngeneralization. On the other hand, the trader eventually needs to choose limit\nprices discretely due to the existence of the tick size, which requires\nspecialization for every single stock with different characteristics (e.g., the\nliquidity and the price range). So we need continuous control for\ngeneralization and discrete control for specialization. To this end, we propose\na hybrid RL method to combine the advantages of both of them. We first use a\ncontinuous control agent to scope an action subset, then deploy a fine-grained\nagent to choose a specific limit price. Extensive experiments show that our\nmethod has higher sample efficiency and better training stability than existing\nRL algorithms and significantly outperforms previous learning-based methods for\norder execution.\n"
    },
    {
        "paper_id": 2207.11292,
        "authors": "Jamaal Ahmad, Mogens Bladt",
        "title": "Phase-type representations of stochastic interest rates with\n  applications to life insurance",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The purpose of the present paper is to incorporate stochastic interest rates\ninto a matrix-approach to multi-state life insurance, where formulas for\nreserves, moments of future payments and equivalence premiums can be obtained\nas explicit formulas in terms of product integrals or matrix exponentials. To\nthis end we consider the Markovian interest model, where the rates are\npiecewise deterministic (or even constant) in the different states of a Markov\njump process, and which is shown to integrate naturally into the matrix\nframework. The discounting factor then becomes the price of a zero-coupon bond\nwhich may or may not be correlated with the biometric insurance process.\nAnother nice feature about the Markovian interest model is that the price of\nthe bond coincides with the survival function of a phase-type distributed\nrandom variable. This, in particular, allows for calibrating the Markovian\ninterest rate models using a maximum likelihood approach to observed data\n(prices) or to theoretical models like e.g. a Vasicek model. Due to the\ndenseness of phase-type distributions, we can approximate the price behaviour\nof any zero-coupon bond with interest rates bounded from below by choosing the\nnumber of possible interest rate values sufficiently large. For observed data\nmodels with few data points, lower dimensions will usually suffice, while for\ntheoretical models the dimensionality is only a computational issue.\n"
    },
    {
        "paper_id": 2207.11486,
        "authors": "Stefanos Bennett, Jase Clarkson",
        "title": "Time Series Prediction under Distribution Shift using Differentiable\n  Forgetting",
        "comments": "ICML Principles of Distribution Shift 2022 Workshop",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Time series prediction is often complicated by distribution shift which\ndemands adaptive models to accommodate time-varying distributions. We frame\ntime series prediction under distribution shift as a weighted empirical risk\nminimisation problem. The weighting of previous observations in the empirical\nrisk is determined by a forgetting mechanism which controls the trade-off\nbetween the relevancy and effective sample size that is used for the estimation\nof the predictive model. In contrast to previous work, we propose a\ngradient-based learning method for the parameters of the forgetting mechanism.\nThis speeds up optimisation and therefore allows more expressive forgetting\nmechanisms.\n"
    },
    {
        "paper_id": 2207.11491,
        "authors": "Gianmarco Ricciardi, Guido Montagna, Guido Caldarelli, Giulio Cimini",
        "title": "Dimensional Reduction of Solvency Contagion Dynamics on Financial\n  Networks",
        "comments": null,
        "journal-ref": "Physica A: Statistical Mechanics and its Applications 630, 129287\n  (2023)",
        "doi": "10.1016/j.physa.2023.129287",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Modelling systems with networks has been a powerful approach to tame the\ncomplexity of several phenomena. Unfortunately, such an approach is often made\ndifficult by the large number of variables to take into consideration. Methods\nof dimensional reduction are useful tools to rescale a complex dynamical\nnetwork down to a low-dimensional effective system and thus to capture the\nglobal features of the dynamics. Here we study the application of the\ndegree-weighted and spectral reduction methods to an important class of\ndynamical processes on networks: the propagation of credit shocks within an\ninterbank network, modelled according to the DebtRank algorithm. In particular\nwe introduce an effective version of the dynamics, characterised by functions\nwith continuous derivatives that can be handled by the dimensional reduction.\nWe test the reduction methods against the full dynamical system in different\ninterbank market settings: homogeneous and heterogeneous networks generated\nfrom state-of-the-art reconstruction methods as well as networks derived from\nempirical e-MID data. Our results indicate that, for proper choices of the bank\ndefault probability, reduction methods are able to provide reliable estimates\nof systemic risk in the market, with the spectral reduction better handling\nheterogeneous networks. Finally we provide new physical insights on the nature\nand working principles of dimensional reduction methods.\n"
    },
    {
        "paper_id": 2207.11545,
        "authors": "Hanzhao Wang, Xiaocheng Li, Kalyan Talluri",
        "title": "Learning to Sell a Focal-ancillary Combination",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  A number of products are sold in the following sequence: First a focal\nproduct is shown, and if the customer purchases, one or more ancillary products\nare displayed for purchase. A prominent example is the sale of an airline\nticket, where first the flight is shown, and when chosen, a number of\nancillaries such as cabin or hold bag options, seat selection, insurance etc.\nare presented. The firm has to decide on a sale format -- whether to sell them\nin sequence unbundled, or together as a bundle -- and how to price the focal\nand ancillary products, separately or as a bundle. Since the ancillary is\nconsidered by the customer only after the purchase of the focal product, the\nsale strategy chosen by the firm creates an information and learning dependency\nbetween the products: for instance, offering only a bundle would preclude\nlearning customers' valuation for the focal and ancillary products\nindividually. In this paper we study learning strategies for such focal and\nancillary item combinations under the following scenarios: (a) pure unbundling\nto all customers, (b) personalized mechanism, where, depending on some observed\nfeatures of the customers, the two products are presented and priced as a\nbundle or in sequence, (c) initially unbundling (for all customers), and switch\nto bundling (if more profitable) permanently once during the horizon. We design\npricing and decisions algorithms for all three scenarios, with regret upper\nbounded by $O(d \\sqrt{T} \\log T)$, and an optimal switching time for the third\nscenario.\n"
    },
    {
        "paper_id": 2207.11546,
        "authors": "D Reshma, Sudharani R, Suresh N",
        "title": "A Study on Impact of Downsizing on Profitability of Construction\n  Industries listed in Bombay Stock Exchange (BSE) India",
        "comments": "10 pages, 10 tables, Published in Journal of Seybold Report",
        "journal-ref": "Journal of Seybold Report 2020 15 7 91 100",
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  The study investigates the impact of downsizing layoffs on the profitability\nof construction industries listed in BSE India. In India, construction\nindustries have adopted downsizing long back in the organization to improve the\nfirms performance. For the purpose of the study, Secondary data of 15\nConstruction companies listed in BSE India have been considered for a period of\n10 years from FY.2010 to FY2019. Data has been taken from the companys official\nwebsite. The variable considered for the analysis is Other Expenses, Returns on\nNet Worth, Employee Expenses, Number of Employees, and Profit Per Employee. The\nstudy has used the Co-integration test to see co-integration between the\nvariables, Ordinary Least Square (OLS) and Vector Auto Regression (VAR) the\nmodel used for estimating the impact of downsizing on the profitability of\nconstruction companies. OLS and VAR model has been used to draw a conclusion\nbased on the P values and R square. From the result, it can be concluded that,\nExpect Profit Per Employees are the downsizing variable that has no significant\nimpact on the profitability of the firms performance. Whereas the other\nDownsizing variables Employee Expenses and the Number of Employee has a\nsignificant impact on the profitability of the firms performance\n"
    },
    {
        "paper_id": 2207.11568,
        "authors": "Jose Cruz, Maria Grossinho, Daniel Sevcovic, Cyril Izuchukwu Udeani",
        "title": "Linear and Nonlinear Partial Integro-Differential Equations arising from\n  Finance",
        "comments": "arXiv admin note: text overlap with arXiv:2104.06115,\n  arXiv:2106.10498, arXiv:1901.06467",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  The purpose of this review paper is to present our recent results on\nnonlinear and nonlocal mathematical models arising from modern financial\nmathematics. It is based on our four papers written jointly by J. Cruz, M.\nGrossinho, D. Sevcovic, and C. Udeani, as well as parts of PhD thesis by J.\nCruz. We investigated linear and nonlinear partial integro-differential\nequations (PIDEs) arising from option pricing and portfolio selection problems\nand studied the systematic relationships between the PIDEs with option pricing\ntheory and Black--Scholes models. First, we relax the liquid and complete\nmarket assumptions and extend the models that study the market's illiquidity to\nthe case where the underlying asset price follows a L\\'evy stochastic process\nwith jumps. Then, we establish the corresponding PIDE for option pricing under\nsuitable assumptions. The qualitative properties of solutions to nonlocal\nlinear and nonlinear PIDE are presented using the theory of abstract semilinear\nparabolic equation in the scale of Bessel potential spaces. The existence and\nuniqueness of solutions to the PIDE for a general class of the so-called\nadmissible L\\'evy measures satisfying suitable growth conditions at infinity\nand origin are also established in the multidimensional space.\n"
    },
    {
        "paper_id": 2207.11577,
        "authors": "Mostafa Shabani, Dat Thanh Tran, Juho Kanniainen, Alexandros Iosifidis",
        "title": "Augmented Bilinear Network for Incremental Multi-Stock Time-Series\n  Classification",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Deep Learning models have become dominant in tackling financial time-series\nanalysis problems, overturning conventional machine learning and statistical\nmethods. Most often, a model trained for one market or security cannot be\ndirectly applied to another market or security due to differences inherent in\nthe market conditions. In addition, as the market evolves through time, it is\nnecessary to update the existing models or train new ones when new data is made\navailable. This scenario, which is inherent in most financial forecasting\napplications, naturally raises the following research question: How to\nefficiently adapt a pre-trained model to a new set of data while retaining\nperformance on the old data, especially when the old data is not accessible? In\nthis paper, we propose a method to efficiently retain the knowledge available\nin a neural network pre-trained on a set of securities and adapt it to achieve\nhigh performance in new ones. In our method, the prior knowledge encoded in a\npre-trained neural network is maintained by keeping existing connections fixed,\nand this knowledge is adjusted for the new securities by a set of augmented\nconnections, which are optimized using the new data. The auxiliary connections\nare constrained to be of low rank. This not only allows us to rapidly optimize\nfor the new task but also reduces the storage and run-time complexity during\nthe deployment phase. The efficiency of our approach is empirically validated\nin the stock mid-price movement prediction problem using a large-scale limit\norder book dataset. Experimental results show that our approach enhances\nprediction performance as well as reduces the overall number of network\nparameters.\n"
    },
    {
        "paper_id": 2207.11636,
        "authors": "Sergio Correia, Stephan Luck, and Emil Verner",
        "title": "Pandemics Depress the Economy, Public Health Interventions Do Not:\n  Evidence from the 1918 Flu",
        "comments": null,
        "journal-ref": null,
        "doi": "10.1017/S0022050722000407",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We study the impact of non-pharmaceutical interventions (NPIs) on mortality\nand economic activity across U.S. cities during the 1918 Flu Pandemic. The\ncombination of fast and stringent NPIs reduced peak mortality by 50% and\ncumulative excess mortality by 24% to 34%. However, while the pandemic itself\nwas associated with short-run economic disruptions, we find that these\ndisruptions were similar across cities with strict and lenient NPIs. NPIs also\ndid not worsen medium-run economic outcomes. Our findings indicate that NPIs\ncan reduce disease transmission without further depressing economic activity, a\nfinding also reflected in discussions in contemporary newspapers.\n"
    },
    {
        "paper_id": 2207.11835,
        "authors": "Kshitij Kulkarni and Theo Diamandis and Tarun Chitra",
        "title": "Towards a Theory of Maximal Extractable Value I: Constant Function\n  Market Makers",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Maximal Extractable Value (MEV) refers to excess value captured by miners (or\nvalidators) from users in a cryptocurrency network. This excess value often\ncomes from reordering users' transactions to maximize fees or from inserting\nnew transactions that front-run users' transactions. One of the most common\ntypes of MEV involves a `sandwich attack' against a user trading on a constant\nfunction market maker (CFMM), which is a popular class of automated market\nmaker. We analyze game theoretic properties of MEV in CFMMs that we call\n\\textit{routing} and \\textit{reordering} MEV. In the case of routing, we\npresent examples where the existence of MEV both degrades and,\ncounterintuitively, \\emph{improves} the quality of routing. We construct an\nanalogue of the price of anarchy for this setting and demonstrate that if the\nimpact of a sandwich attack is localized in a suitable sense, then the price of\nanarchy is constant. In the case of reordering, we show conditions when the\nmaximum price impact caused by the reordering of sandwich attacks in a sequence\nof trades, relative to the average price, impact is $O(\\log n)$ in the number\nof user trades. Combined, our results suggest methods that both MEV searchers\nand CFMM designers can utilize for estimating costs and profits of MEV.\n"
    },
    {
        "paper_id": 2207.12199,
        "authors": "Richard S.J. Tol",
        "title": "A meta-analysis of the total economic impact of climate change",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Earlier meta-analyses of the economic impact of climate change are updated\nwith more data, with three new results: (1) The central estimate of the\neconomic impact of global warming is always negative. (2) The confidence\ninterval about the estimates is much wider. (3) Elicitation methods are most\npessimistic, econometric studies most optimistic. Two previous results remain:\n(4) The uncertainty about the impact is skewed towards negative surprises. (5)\nPoorer countries are much more vulnerable than richer ones. A meta-analysis of\nthe impact of weather shocks reveals that studies, which relate economic growth\nto temperature levels, cannot agree on the sign of the impact whereas studies,\nwhich make economic growth a function of temperature change do agree on the\nsign but differ an order of magnitude in effect size. The former studies posit\nthat climate change has a permanent effect on economic growth, the latter that\nthe effect is transient. The impact on economic growth implied by studies of\nthe impact of climate change is close to the growth impact estimated as a\nfunction of weather shocks. The social cost of carbon shows a similar pattern\nto the total impact estimates, but with more emphasis on the impacts of\nmoderate warming in the near and medium term.\n"
    },
    {
        "paper_id": 2207.12255,
        "authors": "Igor Sadoune, Andrea Lodi, Marcelin Joanis",
        "title": "Implementing a Hierarchical Deep Learning Approach for Simulating\n  Multi-Level Auction Data",
        "comments": null,
        "journal-ref": null,
        "doi": "10.1007/S10614-024-10622-4",
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  We present a deep learning solution to address the challenges of simulating\nrealistic synthetic first-price sealed-bid auction data. The complexities\nencountered in this type of auction data include high-cardinality discrete\nfeature spaces and a multilevel structure arising from multiple bids associated\nwith a single auction instance. Our methodology combines deep generative\nmodeling (DGM) with an artificial learner that predicts the conditional bid\ndistribution based on auction characteristics, contributing to advancements in\nsimulation-based research. This approach lays the groundwork for creating\nrealistic auction environments suitable for agent-based learning and modeling\napplications. Our contribution is twofold: we introduce a comprehensive\nmethodology for simulating multilevel discrete auction data, and we underscore\nthe potential of DGM as a powerful instrument for refining simulation\ntechniques and fostering the development of economic models grounded in\ngenerative AI.\n"
    },
    {
        "paper_id": 2207.12492,
        "authors": "Christopher Boudreaux, Anand Jha, and Monica Escaleras",
        "title": "Natural Disasters, Entrepreneurship Activity, and the Moderating Role of\n  Country Governance",
        "comments": "49 pages, 4 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  The purpose of this paper is to investigate if a country quality of\ngovernance moderates the effect of natural disasters on startup activity within\nthat country. We test our hypotheses using a panel of 95 countries from 2006 to\n2016. Our findings suggest that natural disasters discourage startup activity\nin countries that have low quality governance but encourage startup activity in\ncountries that have high quality governance. Moreover, our estimates reveal\nthat natural disasters effects on startup activity persist for the short term\n(1-3 years) but not the long term. Our findings provide new insights into how\nnatural disasters affect entrepreneurship activity and highlight the importance\nof country governance during these events.\n"
    },
    {
        "paper_id": 2207.12494,
        "authors": "Sergio Ocampo, Raphael Schoenle, Dominic A. Smith",
        "title": "Extending the Range of Robust PCE Inflation Measures",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  We evaluate the forecasting performance of a wide set of robust inflation\nmeasures between 1960 and 2022, including official median and trimmed-mean\npersonal-consumption-expenditure inflation. When trimming out different\nexpenditure categories with the highest and lowest inflation rates, we find\nthat the optimal trim points vary widely across time and also depend on the\nchoice of target; optimal trims are higher when targeting future trend\ninflation or for a 1970s-1980s subsample. Surprisingly, there are no grounds to\nselect a single series on the basis of forecasting performance. A wide range of\ntrims-including those of the official robust measures-have an average\nprediction error that makes them statistically indistinguishable from the\nbest-performing trim. Despite indistinguishable average errors, these trims\nimply different predictions for trend inflation in any given month, within a\nrange of 0.5 to 1 percentage points, suggesting the use of a set of\nnear-optimal trims.\n"
    },
    {
        "paper_id": 2207.12581,
        "authors": "Wenpin Tang and David D. Yao",
        "title": "Trading under the Proof-of-Stake Protocol -- a Continuous-Time Control\n  Approach",
        "comments": "24 pages, 4 figures. This paper is published by\n  https://onlinelibrary.wiley.com/doi/full/10.1111/mafi.12403",
        "journal-ref": null,
        "doi": "10.1111/mafi.12403",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We develop a continuous-time control approach to optimal trading in a\nProof-of-Stake (PoS) blockchain, formulated as a consumption-investment problem\nthat aims to strike the optimal balance between a participant's (or agent's)\nutility from holding/trading stakes and utility from consumption. We present\nsolutions via dynamic programming and the Hamilton-Jacobi-Bellman (HJB)\nequations. When the utility functions are linear or convex, we derive\nclose-form solutions and show that the bang-bang strategy is optimal (i.e.,\nalways buy or sell at full capacity). Furthermore, we bring out the explicit\nconnection between the rate of return in trading/holding stakes and the\nparticipant's risk-adjusted valuation of the stakes. In particular, we show\nwhen a participant is risk-neutral or risk-seeking, corresponding to the\nrisk-adjusted valuation being a martingale or a sub-martingale, the optimal\nstrategy must be to either buy all the time, sell all the time, or first buy\nthen sell, and with both buying and selling executed at full capacity. We also\npropose a risk-control version of the consumption-investment problem; and for a\nspecial case, the ''stake-parity'' problem, we show a mean-reverting strategy\nis optimal.\n"
    },
    {
        "paper_id": 2207.12631,
        "authors": "Christian Kurniawan, Xiyu Deng, Adhiraj Chakraborty, Assane Gueye,\n  Niangjun Chen, and Yorie Nakahira",
        "title": "A Learning and Control Perspective for Microfinance",
        "comments": "37 pages, 12 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Microfinance, despite its significant potential for poverty reduction, is\nfacing sustainability hardships due to high default rates. Although many\nmethods in regular finance can estimate credit scores and default\nprobabilities, these methods are not directly applicable to microfinance due to\nthe following unique characteristics: a) under-explored (developing) areas such\nas rural Africa do not have sufficient prior loan data for microfinance\ninstitutions (MFIs) to establish a credit scoring system; b) microfinance\napplicants may have difficulty providing sufficient information for MFIs to\naccurately predict default probabilities; and c) many MFIs use group liability\n(instead of collateral) to secure repayment. Here, we present a novel\ncontrol-theoretic model of microfinance that accounts for these\ncharacteristics. We construct an algorithm to learn microfinance decision\npolicies that achieve financial inclusion, fairness, social welfare, and\nsustainability. We characterize the convergence conditions to Pareto-optimum\nand the convergence speeds. We demonstrate, in numerous real and synthetic\ndatasets, that the proposed method accounts for the complexities induced by\ngroup liability to produce robust decisions before sufficient loans are given\nto establish credit scoring systems and for applicants whose default\nprobability cannot be accurately estimated due to missing information. To the\nbest of our knowledge, this paper is the first to connect microfinance and\ncontrol theory. We envision that the connection will enable safe learning and\ncontrol techniques to help modernize microfinance and alleviate poverty.\n"
    },
    {
        "paper_id": 2207.12834,
        "authors": "Ziqiao Ao, Gergely Horvath, Chunyuan Sheng, Yifan Song, Yutong Sun",
        "title": "Skill requirements in job advertisements: A comparison of\n  skill-categorization methods based on explanatory power in wage regressions",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper, we compare different methods to extract skill requirements\nfrom job advertisements. We consider three top-down methods that are based on\nexpert-created dictionaries of keywords, and a bottom-up method of unsupervised\ntopic modeling, the Latent Dirichlet Allocation (LDA) model. We measure the\nskill requirements based on these methods using a U.K. dataset of job\nadvertisements that contains over 1 million entries. We estimate the returns of\nthe identified skills using wage regressions. Finally, we compare the different\nmethods by the wage variation they can explain, assuming that better-identified\nskills will explain a higher fraction of the wage variation in the labor\nmarket. We find that the top-down methods perform worse than the LDA model, as\nthey can explain only about 20% of the wage variation, while the LDA model\nexplains about 45% of it.\n"
    },
    {
        "paper_id": 2207.13033,
        "authors": "Ganesh Karapakula",
        "title": "An Axiomatic Framework for Cost-Benefit Analysis",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In recent years, the Marginal Value of Public Funds (MVPF) has become a\npopular tool for conducting cost-benefit analysis; the MVPF relies on the ratio\nof willingness-to-pay for a policy divided by its net fiscal cost. The MVPF\ngives policymakers important information about the equity-efficiency trade-off\nthat is not necessarily conveyed by absolute welfare measures. However, I show\nin this paper that the usefulness of MVPF for comparative welfare analysis is\nlimited, because it suffers from several empirically important economic\nparadoxes and statistical irregularities. There are also several practical\nissues in using the MVPF to aggregate welfare across policies or across\npopulation subgroups. To address these problems, I develop a new axiomatic\nframework to construct a measure that quantifies the equity-efficiency\ntrade-off in a better way. I do so without compromising on the core features of\nthe MVPF: its unit-free property, and the main preference orderings underlying\nit. My axiomatic framework delivers a unique (econo)metric that I call the\nRelative Policy Value (RPV), which can be weighted to conduct both comparative\nand absolute welfare analyses (or a hybrid combination thereof) and to\nintuitively aggregate welfare (without encountering the issues in MVPF-based\naggregation). I also propose computationally convenient methods to make\nuniformly valid statistical inferences on welfare measures. After reanalyzing\nseveral government policies using my new econometric methods, I conclude that\nthere is substantial economic and statistical uncertainty about welfare of some\npolicies that were previously reported to have very high or even \"precisely\nestimated infinite\" MVPF values.\n"
    },
    {
        "paper_id": 2207.13071,
        "authors": "Andrew Y. Chen and Jack McCoy",
        "title": "Missing Values Handling for Machine Learning Portfolios",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  We characterize the structure and origins of missingness for 159\ncross-sectional return predictors and study missing value handling for\nportfolios constructed using machine learning. Simply imputing with\ncross-sectional means performs well compared to rigorous\nexpectation-maximization methods. This stems from three facts about predictor\ndata: (1) missingness occurs in large blocks organized by time, (2)\ncross-sectional correlations are small, and (3) missingness tends to occur in\nblocks organized by the underlying data source. As a result, observed data\nprovide little information about missing data. Sophisticated imputations\nintroduce estimation noise that can lead to underperformance if machine\nlearning is not carefully applied.\n"
    },
    {
        "paper_id": 2207.13123,
        "authors": "Shruthi B.C., N. Suresh",
        "title": "Indian Derivatives Market Evolution and Challenge",
        "comments": "11 pages, 11 Graphs, Published in International Journal of\n  Engineering and Management Research (IJEMR)",
        "journal-ref": "International Journal of Engineering and Management Research\n  (IJEMR) 3 11 1 11 2013",
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  The study is conducted to establish the framework for comparing the relation\nin performance of the derivatives of BSE and NSE in India and to analyse the\nrelationship of derivatives with the cash market and the market volatility. The\nexchange-traded equity derivatives were considered for the study. It was found\nthat the performance of derivatives in NSE is a lot higher than in BSE and the\nNSE is on par with the global exchanges compared to BSE in terms of the number\nof contracts traded for Stock Index Options and Futures and also Stock Futures.\nHence derivatives market needs to strengthen further with the number of\ncontracts traded and turnover in all the derivative instruments with more\nstrong regulations and a robust framework protecting the interest of the\ninvestors. This study enables Derivative Industry to progress towards its goals\nand objectives in a more efficient way.\n"
    },
    {
        "paper_id": 2207.13136,
        "authors": "Christa Cuchiero, Guido Gazzani, Sara Svaluto-Ferro",
        "title": "Signature-based models: theory and calibration",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  We consider asset price models whose dynamics are described by linear\nfunctions of the (time extended) signature of a primary underlying process,\nwhich can range from a (market-inferred) Brownian motion to a general\nmultidimensional continuous semimartingale. The framework is universal in the\nsense that classical models can be approximated arbitrarily well and that the\nmodel's parameters can be learned from all sources of available data by simple\nmethods. We provide conditions guaranteeing absence of arbitrage as well as\ntractable option pricing formulas for so-called sig-payoffs, exploiting the\npolynomial nature of generic primary processes. One of our main focus lies on\ncalibration, where we consider both time-series and implied volatility surface\ndata, generated from classical stochastic volatility models and also from\nS&P500 index market data. For both tasks the linearity of the model turns out\nto be the crucial tractability feature which allows to get fast and accurate\ncalibrations results.\n"
    },
    {
        "paper_id": 2207.13319,
        "authors": "Paul Glasserman and Mike Li",
        "title": "Should Bank Stress Tests Be Fair?",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Regulatory stress tests have become one of the main tools for setting capital\nrequirements at the largest U.S. banks. The Federal Reserve uses confidential\nmodels to evaluate bank-specific outcomes for bank-specific portfolios in\nshared stress scenarios. As a matter of policy, the same models are used for\nall banks, despite considerable heterogeneity across institutions; individual\nbanks have contended that some models are not suited to their businesses.\nMotivated by this debate, we ask, what is a fair aggregation of individually\ntailored models into a common model? We argue that simply pooling data across\nbanks treats banks equally but is subject to two deficiencies: it may distort\nthe impact of legitimate portfolio features, and it is vulnerable to implicit\nmisdirection of legitimate information to infer bank identity. We compare\nvarious notions of regression fairness to address these deficiencies,\nconsidering both forecast accuracy and equal treatment. In the setting of\nlinear models, we argue for estimating and then discarding centered bank fixed\neffects as preferable to simply ignoring differences across banks. We present\nevidence that the overall impact can be material. We also discuss extensions to\nnonlinear models.\n"
    },
    {
        "paper_id": 2207.1335,
        "authors": "Benedikt Geuchen and Katharina Oberpriller and Thorsten Schmidt",
        "title": "Affine models with path-dependence under parameter uncertainty and their\n  application in finance",
        "comments": "24 pages, 2 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this work we consider one-dimensional generalized affine processes under\nthe paradigm of Knightian uncertainty (so-called non-linear generalized affine\nmodels). This extends and generalizes previous results in Fadina et al. (2019)\nand L\\\"utkebohmert et al. (2022). In particular, we study the case when the\npayoff is allowed to depend on the path, like it is the case for barrier\noptions or Asian options. To this end, we develop the path-dependent setting\nfor the value function which we do by relying on functional It\\^o calculus. We\nestablish a dynamic programming principle which then leads to a functional\nnon-linear Kolmogorov equation describing the evolution of the value function.\nWhile for Asian options, the valuation can be traced back to PDE methods, this\nis no longer possible for more complicated payoffs like barrier options. To\nhandle such payoffs in an efficient manner, we approximate the functional\nderivatives with deep neural networks and show that the numerical valuation\nunder parameter uncertainty is highly tractable. Finally, we consider the\napplication to structural modelling of credit and counterparty risk, where both\nparameter uncertainty and path-dependence are crucial and the approach proposed\nhere opens the door to efficient numerical methods in this field.\n"
    },
    {
        "paper_id": 2207.13444,
        "authors": "Ganapathy G Gangadharan, N.Suresh",
        "title": "Interrogation of A Bubble in the Indian Market",
        "comments": "7 pages, 1 figure, 1 table, published in Journal of Management",
        "journal-ref": "Journal of Management 6 64 70 2019",
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Emerging markets such as India provide investors with returns far greater\nthan those in developed markets; taking the average returns from the period\n1995 to 2014 the returns are 4.714% to 3.276% of the developed market. The\nmajority of emerging markets commenced joining with the capital market of the\nworld, thus allowing a huge inflow of capital which in turn paved the path for\neconomic growth. Even though the emerging markets provide high returns these\nmay also be an indication of a bubble formation. Detection of a bubble is a\ntedious task primarily due to the fundamental value of the security being\nuncertain, and the randomness of the fundamentals of the market makes detecting\nbubbles an arduous task. Ratios that foretold the financial crisis of 2007-\nMarket Capitalization to GDP, Price to Earnings Ratio, Price to Book Value,\nTobins Q. Data is collected from 1999-2000 from various Indian indices such as\nNIFTY 50, NIFTY NEXT 50, NIFTY BANK, NIFTY 500 S and PBSE SENSEX, S and P BSE\n100. The paper utilizes the ratios mentioned above to detect and backtrack\nvarious bubble episodes in the Indian market; the methodology used is the\nPhilips et al 2015 right-tailed unit test. The paper is also inclined to take\nsteps to mitigate the effects of a bubble by amending the financial policies\nand the monetary liquidity of the financial system.\n"
    },
    {
        "paper_id": 2207.13573,
        "authors": "Martin Keller-Ressel",
        "title": "Bartlett's Delta revisited: Variance-optimal hedging in the lognormal\n  SABR and in the rough Bergomi model",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We derive analytic expressions for the variance-optimal hedging strategy and\nits mean-square hedging error in the lognormal SABR and in the rough Bergomi\nmodel. In the SABR model, we show that the variance-optimal hedging strategy\ncoincides with the Delta adjustment of Bartlett [Wilmott magazine 4/6 (2006)].\nWe show both mathematically and in simulation that the efficiency of the\nvariance-optimal strategy (in comparison to simple Delta hedging) depends\nstrongly on the leverage parameter rho and - in a weaker sense - also on the\nroughness parameter H of the model, and give a precise quantification of this\ndependency.\n"
    },
    {
        "paper_id": 2207.13914,
        "authors": "Antonio Briola, David Vidal-Tom\\'as, Yuanrong Wang, Tomaso Aste",
        "title": "Anatomy of a Stablecoin's failure: the Terra-Luna case",
        "comments": "17 pages, 7 figures, 6 tables, 1 appendix",
        "journal-ref": null,
        "doi": "10.1016/j.frl.2022.103358",
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  We quantitatively describe the main events that led to the Terra project's\nfailure in May 2022. We first review, in a systematic way, news from\nheterogeneous social media sources; we discuss the fragility of the Terra\nproject and its vicious dependence on the Anchor protocol. We hence identify\nthe crash's trigger events, analysing hourly and transaction data for Bitcoin,\nLuna, and TerraUSD. Finally, using state-of-the-art techniques from network\nscience, we study the evolution of dependency structures for 61 highly\ncapitalised cryptocurrencies during the down-market and we also highlight the\nabsence of herding behaviour analysing cross-sectional absolute deviation of\nreturns.\n"
    },
    {
        "paper_id": 2207.14379,
        "authors": "Chinonso Nwankwo, Weizhong Dai",
        "title": "Sixth-Order Compact Differencing with Staggered Boundary Schemes and\n  3(2) Bogacki-Shampine Pairs for Pricing Free-Boundary Options",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We propose a stable sixth-order compact finite difference scheme with a\ndynamic fifth-order staggered boundary scheme and 3(2) R-K Bogacki and Shampine\nadaptive time stepping for pricing American style options. To locate, fix and\ncompute the free-boundary simultaneously with option and delta sensitivity, we\nintroduce a Landau transformation. Furthermore, we remove the convective term\nin the pricing model which could further introduce errors. Hence, an efficient\nsixth-order compact scheme can easily be implemented. The main challenge in\ncoupling the sixth order compact scheme in discrete form is to efficiently\naccount for the near-boundary scheme. In this work, we introduce novel fifth-\nand sixth-order Dirichlet near-boundary schemes suitable for solving our model.\nThe optimal exercise boundary and other boundary values are approximated using\na high-order analytical approximation obtained from a novel fifth-order\nstaggered boundary scheme. Furthermore, we investigate the smoothness of the\nfirst and second derivatives of the optimal exercise boundary which is obtained\nfrom this high-order analytical approximation. Coupled with the 3(2) RK-Bogacki\nand Shampine time integration method, the interior values are then approximated\nusing the sixth order compact operator. The expected convergence rate is\nobtained, and our present numerical scheme is very fast and gives highly\naccurate approximations with very coarse grids.\n"
    },
    {
        "paper_id": 2207.14724,
        "authors": "Richard S. J. Tol",
        "title": "The IPCC and the challenge of ex post policy evaluation",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The IPCC started at a time when climate policy was an aspiration for the\nfuture. The research assessed in the early IPCC reports was necessarily about\npotential climate policies, always stylized and often optimized. The IPCC has\ncontinued on this path, even though there is now a considerable literature\nstudying actual climate policy, in all its infuriating detail, warts and all.\nFour case studies suggest that the IPCC, in its current form, will not be able\nto successfully switch from ex ante to ex post policy evaluation. This\ntransition is key as AR7 will most likely have to confront the failure to meet\nthe 1.5K target. The four cases are as follows. (1) The scenarios first build\nand later endorsed by the IPCC all project a peaceful future with steady if not\nrapid economic growth everywhere, more closely resembling political manifestos\nthan facts on the ground. (2) Successive IPCC reports have studiously avoided\ndiscussing the voluminous literature suggesting that political targets for\ngreenhouse gas emission reduction are far from optimal, although a central part\nof that work was awarded the Nobel Prize in 2018. (3) IPCC AR5 found it\nimpossible to acknowledge that the international climate policy negotiations\nfrom COP1 (Berlin) to COP19 (Warsaw) were bound to fail, just months before the\nradical overhaul at COP20 (Lima) proved that point. (4) IPCC AR6 by and large\nomitted the nascent literature on \\textit{ex post} climate policy evaluation.\nTogether, these cases suggest that the IPCC finds self-criticism difficult and\nis too close to policy makers to criticize past and current policy mistakes.\nOne solution would be to move control over the IPCC to the national authorities\non research and higher education.\n"
    },
    {
        "paper_id": 2207.14775,
        "authors": "Ricardo A. Pasquini",
        "title": "Optimal Allocation of Limited Funds in Quadratic Funding",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We examine the allocation of a limited pool of matching funds to public good\nprojects using Quadratic Funding. In particular, we consider a variation of the\nCapital Constrained Quadratic Funding (CQF) mechanism proposed by Buterin,\nHitzig and Weyl (2019) where only funds in the matching pool are distributed\namong projects. We show that this mechanism achieves a socially optimal\nallocation of limited funds.\n"
    },
    {
        "paper_id": 2207.14793,
        "authors": "Zhenyu Cui, Anne MacKay and Marie-Claude Vachon",
        "title": "Analysis of VIX-linked fee incentives in variable annuities via\n  continuous-time Markov chain approximation",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  We consider the pricing of variable annuities (VAs) with general fee\nstructures under popular stochastic volatility models such as Heston,\nHull-White, Scott, $\\alpha$-Hypergeometric, $3/2$, and $4/2$ models. In\nparticular, we analyze the impact of different VIX-linked fee structures on the\noptimal surrender strategy of a VA contract with guaranteed minimum maturity\nbenefit (GMMB). Under the assumption that the VA contract can be surrendered\nbefore maturity, the pricing of a VA contract corresponds to an optimal\nstopping problem with an unbounded, time-dependent, and discontinuous payoff\nfunction. We develop efficient algorithms for the pricing of VA contracts using\na two-layer continuous-time Markov chain approximation for the fund value\nprocess. When the contract is kept until maturity and under a general fee\nstructure, we show that the value of the contract can be approximated by a\nclosed-form matrix expression. We also provide a quick and simple way to\ndetermine the value of early surrenders via a recursive algorithm and give an\neasy procedure to approximate the optimal surrender surface. We show\nnumerically that the optimal surrender strategy is more robust to changes in\nthe volatility of the account value when the fee is linked to the VIX index.\n"
    },
    {
        "paper_id": 2208.00181,
        "authors": "Paola Deriu, Fabrizio Lillo, Piero Mazzarisi, Francesca Medda, Adele\n  Ravagnani, Antonio Russo",
        "title": "How Covid mobility restrictions modified the population of investors in\n  Italian stock markets",
        "comments": "25 pages, 16 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper investigates how Covid mobility restrictions impacted the\npopulation of investors of the Italian stock market. The analysis tracks the\ntrading activity of individual investors in Italian stocks in the period\nJanuary 2019-September 2021, investigating how their composition and the\ntrading activity changed around the Covid-19 lockdown period (March 9 - May 19,\n2020) and more generally in the period of the pandemic. The results pinpoint\nthat the lockdown restriction was accompanied by a surge in interest toward\nstock market, as testified by the trading volume by households. Given the\ngenerically falling prices during the lockdown, the households, which are\ntypically contrarian, were net buyers, even if less than expected from their\ntrading activity in 2019. This can be explained by the arrival, during the\nlockdown, of a group of about 185k new investors (i.e. which had never traded\nsince January 2019) which were on average ten year younger and with a larger\nfraction of males than the pre-lockdown investors. By looking at the gross P&L,\nthere is clear evidence that these new investors were more skilled in trading.\nThere are thus indications that the lockdown, and more generally the Covid\npandemic, created a sort of regime change in the population of financial\ninvestors.\n"
    },
    {
        "paper_id": 2208.00765,
        "authors": "Leonardo Kanashiro Felizardo and Elia Matsumoto and Emilio\n  Del-Moral-Hernandez",
        "title": "Solving the optimal stopping problem with reinforcement learning: an\n  application in financial option exercise",
        "comments": "8 pages, 6 figures, WCCI2022 IEEE WORLD CONGRESS ON COMPUTATIONAL\n  INTELLIGENCE",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The optimal stopping problem is a category of decision problems with a\nspecific constrained configuration. It is relevant to various real-world\napplications such as finance and management. To solve the optimal stopping\nproblem, state-of-the-art algorithms in dynamic programming, such as the\nleast-squares Monte Carlo (LSMC), are employed. This type of algorithm relies\non path simulations using only the last price of the underlying asset as a\nstate representation. Also, the LSMC was thinking for option valuation where\nrisk-neutral probabilities can be employed to account for uncertainty. However,\nthe general optimal stopping problem goals may not fit the requirements of the\nLSMC showing auto-correlated prices. We employ a data-driven method that uses\nMonte Carlo simulation to train and test artificial neural networks (ANN) to\nsolve the optimal stopping problem. Using ANN to solve decision problems is not\nentirely new. We propose a different architecture that uses convolutional\nneural networks (CNN) to deal with the dimensionality problem that arises when\nwe transform the whole history of prices into a Markovian state. We present\nexperiments that indicate that our proposed architecture improves results over\nthe previous implementations under specific simulated time series function\nsets. Lastly, we employ our proposed method to compare the optimal exercise of\nthe financial options problem with the LSMC algorithm. Our experiments show\nthat our method can capture more accurate exercise opportunities when compared\nto the LSMC. We have outstandingly higher (above 974\\% improvement) expected\npayoff from these exercise policies under the many Monte Carlo simulations that\nused the real-world return database on the out-of-sample (test) data.\n"
    },
    {
        "paper_id": 2208.0083,
        "authors": "Carsten Chong and Viktor Todorov",
        "title": "Short-time expansion of characteristic functions in a rough volatility\n  setting with applications",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We derive a higher-order asymptotic expansion of the conditional\ncharacteristic function of the increment of an It\\^o semimartingale over a\nshrinking time interval. The spot characteristics of the It\\^o semimartingale\nare allowed to have dynamics of general form. In particular, their paths can be\nrough, that is, exhibit local behavior like that of a fractional Brownian\nmotion, while at the same time have jumps with arbitrary degree of activity.\nThe expansion result shows the distinct roles played by the different features\nof the spot characteristics dynamics. As an application of our result, we\nconstruct a nonparametric estimator of the Hurst parameter of the diffusive\nvolatility process from portfolios of short-dated options written on an\nunderlying asset.\n"
    },
    {
        "paper_id": 2208.00907,
        "authors": "Josef Taalbi",
        "title": "Long-run patterns in the discovery of the adjacent possible",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
        "abstract": "  The notion of the \"adjacent possible\" has been advanced to theorize the\ngeneration of novelty across many different research domains. This study is an\nattempt to examine in what way the notion can be made empirically useful for\ninnovation studies. A theoretical framework is construed based on the notion of\ninnovation a search process of recombining knowledge to discover the \"adjacent\npossible\". The framework makes testable predictions about the rate of\ninnovation, the distribution of innovations across organizations, and the rate\nof diversification or product portfolios. The empirical section examines how\nwell this framework predicts long-run patterns of new product introductions in\nSweden, 1908-2016 and examines the long-run evolution of the product space of\nSwedish organizations. The results suggest that, remarkably, the rate of\ninnovation depends linearly on cumulative innovations, which explains\nadvantages of incumbent firms, but excludes the emergence of \"winner takes all\"\ndistributions. The results also suggest that the rate of development of new\ntypes of products follows \"Heaps' law\", where the share of new product types\nwithin organizations declines over time. The topology of the Swedish product\nspace carries information about future product diversifications, suggesting\nthat the adjacent possible is not altogether `\"unprestatable\".\n"
    },
    {
        "paper_id": 2208.00952,
        "authors": "Beatrice Franzolini, Alexandros Beskos, Maria De Iorio, Warrick\n  Poklewski Koziell and Karolina Grzeszkiewicz",
        "title": "Change point detection in dynamic Gaussian graphical models: the impact\n  of COVID-19 pandemic on the US stock market",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
        "abstract": "  Reliable estimates of volatility and correlation are fundamental in economics\nand finance for understanding the impact of macroeconomics events on the market\nand guiding future investments and policies. Dependence across financial\nreturns is likely to be subject to sudden structural changes, especially in\ncorrespondence with major global events, such as the COVID-19 pandemic. In this\nwork, we are interested in capturing abrupt changes over time in the dependence\nacross US industry stock portfolios, over a time horizon that covers the\nCOVID-19 pandemic. The selected stocks give a comprehensive picture of the US\nstock market. To this end, we develop a Bayesian multivariate stochastic\nvolatility model based on a time-varying sequence of graphs capturing the\nevolution of the dependence structure. The model builds on the Gaussian\ngraphical models and the random change points literature. In particular, we\ntreat the number, the position of change points, and the graphs as object of\nposterior inference, allowing for sparsity in graph recovery and change point\ndetection. The high dimension of the parameter space poses complex\ncomputational challenges. However, the model admits a hidden Markov model\nformulation. This leads to the development of an efficient computational\nstrategy, based on a combination of sequential Monte-Carlo and Markov chain\nMonte-Carlo techniques. Model and computational development are widely\napplicable, beyond the scope of the application of interest in this work.\n"
    },
    {
        "paper_id": 2208.01167,
        "authors": "Dillon Bowen",
        "title": "Simple models predict behavior at least as well as behavioral scientists",
        "comments": "21 pages, 4 figures, 2 tables",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  How accurately can behavioral scientists predict behavior? To answer this\nquestion, we analyzed data from five studies in which 640 professional\nbehavioral scientists predicted the results of one or more behavioral science\nexperiments. We compared the behavioral scientists' predictions to random\nchance, linear models, and simple heuristics like \"behavioral interventions\nhave no effect\" and \"all published psychology research is false.\" We find that\nbehavioral scientists are consistently no better than - and often worse than -\nthese simple heuristics and models. Behavioral scientists' predictions are not\nonly noisy but also biased. They systematically overestimate how well\nbehavioral science \"works\": overestimating the effectiveness of behavioral\ninterventions, the impact of psychological phenomena like time discounting, and\nthe replicability of published psychology research.\n"
    },
    {
        "paper_id": 2208.0127,
        "authors": "Koichiro Moriya, Akihiko Noda",
        "title": "Time Instability of the Fama-French Multifactor Models: An International\n  Evidence",
        "comments": "48 pages, 24 figures, 10 table",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  This paper investigates the time-varying structure of Fama and French's\n(1993; 2015) multi-factor models using Fama and MacBeth's (1973) two-step\nestimation based on the rolling window method. In particular, we employ the\ngeneralized GRS statistics proposed by Kamstra and Shi (2024) to examine\nwhether the validity of the risk factors (or factor redundancy) in the FF3 and\nFF5 models remains stable over time, and investigate whether the manner of\nportfolio sorting affects the time stability of the validity of the risk\nfactors. In addition, we examine whether the similar results are obtained even\nwhen we use different datasets by country and region. First, we find that the\neffectiveness of factors in the FF3 and FF5 models is not stable over time in\nall countries. Second, the effectiveness of factors is also affected by the\nmanner of portfolio sorting. Third, the validity of the FF3, FF5, and their\nnested models do not remain stable over time except for Japan. This suggests\nthat the efficient market hypothesis is supported in the Japanese stock market.\nFinally, the factor redundancy varies over time and is affected by the manner\nof portfolio sorting mainly in the U.S. and Europe.\n"
    },
    {
        "paper_id": 2208.01289,
        "authors": "Alberto Manzano, Emanuele Nastasi, Andrea Pallavicini, Carlos\n  V\\'azquez",
        "title": "Pricing commodity index options",
        "comments": "22 pages, 3 Figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  We present a stochastic local volatility model for derivative contracts on\ncommodity futures. The aim of the model is to be able to recover the prices of\nderivative claims both on futures contracts and on indices on futures\nstrategies. Numerical examples for calibration and pricing are provided for the\nS&P GSCI Crude Oil excess-return index.\n"
    },
    {
        "paper_id": 2208.01353,
        "authors": "Elisa Al\\`os, Eulalia Nualart and Makar Pravosud",
        "title": "On the implied volatility of Asian options under stochastic volatility\n  models",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  In this paper we study the short-time behavior of the at-the-money implied\nvolatility for arithmetic Asian options with fixed strike price. The asset\nprice is assumed to follow the Black-Scholes model with a general stochastic\nvolatility process. Using techniques of the Malliavin calculus such as the\nanticipating Ito's formula we first compute the level of the implied volatility\nof the option when the maturity converges to zero. Then, we find and short\nmaturity asymptotic formula for the skew of the implied volatility that depends\non the roughness of the volatility model. We apply our general results to the\nSABR model and the rough Bergomi model, and provide some numerical simulations\nthat confirm the accurateness of the asymptotic formula for the skew.\n"
    },
    {
        "paper_id": 2208.01433,
        "authors": "Chunmeng Yang, Siqi Bu, Yi Fan, Wayne Xinwei Wan, Ruoheng Wang, Aoife\n  Foley",
        "title": "Review of Energy Transition Policies in Singapore, London, and\n  California",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  The paper contains the online supplementary materials for \"Data-Driven\nPrediction and Evaluation on Future Impact of Energy Transition Policies in\nSmart Regions\". We review the renewable energy development and policies in the\nthree metropolitan cities/regions over recent decades. Depending on the\ngeographic variations in the types and quantities of renewable energy resources\nand the levels of policymakers' commitment to carbon neutrality, we classify\nSingapore, London, and California as case studies at the primary, intermediate,\nand advanced stages of the renewable energy transition, respectively.\n"
    },
    {
        "paper_id": 2208.01445,
        "authors": "Marcin W\\k{a}torek and Jaros{\\l}aw Kwapie\\'n and Stanis{\\l}aw\n  Dro\\.zd\\.z",
        "title": "Multifractal cross-correlations of bitcoin and ether trading\n  characteristics in the post-COVID-19 time",
        "comments": null,
        "journal-ref": "Future Internet 2022, 14(7), 215",
        "doi": "10.3390/fi14070215",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Unlike price fluctuations, the temporal structure of cryptocurrency trading\nhas seldom been a subject of systematic study. In order to fill this gap, we\nanalyse detrended correlations of the price returns, the average number of\ntrades in time unit, and the traded volume based on high-frequency data\nrepresenting two major cryptocurrencies: bitcoin and ether. We apply the\nmultifractal detrended cross-correlation analysis, which is considered the most\nreliable method for identifying nonlinear correlations in time series. We find\nthat all the quantities considered in our study show an unambiguous\nmultifractal structure from both the univariate (auto-correlation) and\nbivariate (cross-correlation) perspectives. We looked at the bitcoin--ether\ncross-correlations in simultaneously recorded signals, as well as in\ntime-lagged signals, in which a time series for one of the cryptocurrencies is\nshifted with respect to the other. Such a shift suppresses the\ncross-correlations partially for short time scales, but does not remove them\ncompletely. We did not observe any qualitative asymmetry in the results for the\ntwo choices of a leading asset. The cross-correlations for the simultaneous and\nlagged time series became the same in magnitude for the sufficiently long\nscales.\n"
    },
    {
        "paper_id": 2208.01467,
        "authors": "Victor Sellemi",
        "title": "Risk in Network Economies",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Economic models with input-output networks assume that firm or sector (unit)\ngrowth is driven by a weighted sum of trade partners' growth and an\nindependently-drawn idiosyncratic shock. I show that the idiosyncratic risk\nassumption in a broad class of network models implicitly generates restrictions\non the network weights which are unrealistic. When allowing for correlated\nshocks, units are exposed to an additional risk term which captures the ability\nto substitute away from supply and demand shocks propagating through the\nnetwork. I provide empirical evidence that changes in substitutability between\ntrade partners are inversely related to changes in the panel of realized\nindustry variance. Moreover, I find that supply-side (demand-side)\nsubstitutability is closely related to technological (product) dispersion of a\nunit's suppliers (customers). To synthesize these results, I propose a\nproduction-based asset pricing model in which supply chain substitutability is\na function of dispersion in product/technology space and correlation in supply\nand demand shocks is driven by shared customers and suppliers between firms.\nThe model predicts that assets which are positively exposed to average\npropagation of upstream and downstream shocks are useful hedges and thus earn\nlower average risk premia. Consistently, I find that estimated upstream\n(downstream) propagation factors earn return spreads of -11.4% (-4.2%) and are\nnegatively associated with aggregate consumption, output, and dividend growth.\n"
    },
    {
        "paper_id": 2208.01538,
        "authors": "Elroi Hadad, Haim Kedar-Levy",
        "title": "The Impact of Retail Investors Sentiment on Conditional Volatility of\n  Stocks and Bonds",
        "comments": "32 Pages, 4 Tables",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-sa/4.0/",
        "abstract": "  We measure bond and stock conditional return volatility as a function of\nchanges in sentiment, proxied by six indicators from the Tel Aviv Stock\nExchange. We find that changes in sentiment affect conditional volatilities at\ndifferent magnitudes and often in an opposite manner in the two markets,\nsubject to market states. We are the first to measure bonds conditional\nvolatility of retail investors sentiment thanks to a unique dataset of\ncorporate bond returns from a limit-order-book with highly active retail\ntraders. This market structure differs from the prevalent OTC platforms, where\ninstitutional investors are active yet less prone to sentiment.\n"
    },
    {
        "paper_id": 2208.01791,
        "authors": "Gabriele Borg, Diego Gentile Passaro, Santiago Hermo",
        "title": "From Workplace to Residence: The Spillover Effects of Minimum Wage\n  Policies on Local Housing Markets",
        "comments": "This article was previously circulated under the title \"Minimum Wage\n  as a Place-Based Policy: Evidence from US Housing Rental Markets.\"",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  The recent rise of sub-national minimum wage (MW) policies in the US has\nresulted in significant dispersion of MW levels within urban areas. In this\npaper, we study the spillover effects of these policies on local rental markets\nthrough commuting. To do so, for each USPS ZIP code we construct a \"workplace\"\nMW measure based on the location of its resident's jobs, and use it to estimate\nthe effect of MW policies on rents. We use a novel identification strategy that\nexploits the fine timing of differential changes in the workplace MW across ZIP\ncodes that share the same \"residence\" MW, defined as the same location's MW.\nOur baseline results imply that a 10 percent increase in the workplace MW\nincreases rents at residence ZIP codes by 0.69 percent. To illustrate the\nimportance of commuting patterns, we use our estimates and a simple model to\nsimulate the impact of federal and city counterfactual MW policies. The\nsimulations suggest that landlords pocket approximately 10 cents of each dollar\ngenerated by the MW across directly and indirectly affected areas, though the\nincidence on landlords varies systematically across space.\n"
    },
    {
        "paper_id": 2208.01969,
        "authors": "Dan Ben-Moshe and David Genesove",
        "title": "Regulation and Frontier Housing Supply",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Regulation is a major driver of housing supply, yet often not easily\nobserved. Using only apartment prices and building heights, we estimate\n$\\textit{frontier costs}$, defined as housing production costs absent\nregulation. Identification uses conditions on the support of supply and demand\nshocks without recourse to instrumental variables. In an application to Israeli\nresidential construction, we find on average 43% of housing price ascribable to\nregulation, but with substantial dispersion, and with higher rates in areas\nthat are higher priced, denser, and closer to city centers. We also find\neconomies of scale in frontier costs at low building heights. This estimation\ntakes into account measurement error, which includes random unobserved\nstructural quality. When allowing structural quality to vary with amenities\n(locational quality), and assuming weak complementarity (the return in price on\nstructural quality is nondecreasing in amenities) among buildings within 1km,\nwe bound mean regulation from below by 19% of prices.\n"
    },
    {
        "paper_id": 2208.01974,
        "authors": "Battulga Gankhuu",
        "title": "Merton's Default Risk Model for Private Company",
        "comments": "15 pages. arXiv admin note: substantial text overlap with\n  arXiv:2206.09666",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Because the asset value of a private company does not observable except in\nquarterly reports, the structural model has not been developed for a private\ncompany. For this reason, this paper attempt to develop the Merton's structural\nmodel for the private company by using the dividend discount model (DDM). In\nthis paper, we obtain closed--form formulas of risk--neutral equity and\nliability values and default probability for the private company. Also, the\npaper provides ML estimators and the EM algorithm of our model's parameters.\n"
    },
    {
        "paper_id": 2208.02073,
        "authors": "Guido Ascari, Sophocles Mavroeidis, Nigel McClung",
        "title": "Coherence without Rationality at the Zero Lower Bound",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Standard rational expectations models with an occasionally binding zero lower\nbound constraint either admit no solutions (incoherence) or multiple solutions\n(incompleteness). This paper shows that deviations from full-information\nrational expectations mitigate concerns about incoherence and incompleteness.\nModels with no rational expectations equilibria admit self-confirming\nequilibria involving the use of simple mis-specified forecasting models.\nCompleteness and coherence is restored if expectations are adaptive or if\nagents are less forward-looking due to some information or behavioral friction.\nIn the case of incompleteness, the E-stability criterion selects an\nequilibrium.\n"
    },
    {
        "paper_id": 2208.02098,
        "authors": "Giuseppe Cavaliere, Thomas Mikosch, Anders Rahbek and Frederik Vilandt",
        "title": "The Econometrics of Financial Duration Modeling",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
        "abstract": "  We establish new results for estimation and inference in financial durations\nmodels, where events are observed over a given time span, such as a trading\nday, or a week. For the classical autoregressive conditional duration (ACD)\nmodels by Engle and Russell (1998, Econometrica 66, 1127-1162), we show that\nthe large sample behavior of likelihood estimators is highly sensitive to the\ntail behavior of the financial durations. In particular, even under\nstationarity, asymptotic normality breaks down for tail indices smaller than\none or, equivalently, when the clustering behaviour of the observed events is\nsuch that the unconditional distribution of the durations has no finite mean.\nInstead, we find that estimators are mixed Gaussian and have non-standard rates\nof convergence. The results are based on exploiting the crucial fact that for\nduration data the number of observations within any given time span is random.\nOur results apply to general econometric models where the number of observed\nevents is random.\n"
    },
    {
        "paper_id": 2208.02154,
        "authors": "Phillip Sherlock, Herman T. Knopf, Robert Chapman, Maya Schreiber,\n  Courtney K. Blackwell",
        "title": "Child Care Provider Survival Analysis",
        "comments": "20 pages, 2 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  The aggregate ability of child care providers to meet local demand for child\ncare is linked to employment rates in many sectors of the economy. Amid growing\nconcern regarding child care provider sustainability due to the COVID-19\npandemic, state and local governments have received large amounts of new\nfunding to better support provider stability. In response to this new funding\naimed at bolstering the child care market in Florida, this study was devised as\nan exploratory investigation into features of child care providers that lead to\nbusiness longevity. In this study we used optimal survival trees, a machine\nlearning technique designed to better understand which providers are expected\nto remain operational for longer periods of time, supporting stabilization of\nthe child care market. This tree-based survival analysis detects and describes\ncomplex interactions between provider characteristics that lead to differences\nin expected business survival rates. Results show that small providers who are\nreligiously affiliated, and all providers who are serving children in Florida's\nuniversal Prekindergarten program and/or children using child care subsidy, are\nlikely to have the longest expected survival rates.\n"
    },
    {
        "paper_id": 2208.02219,
        "authors": "Yining Liu, Yanfeng Ouyang",
        "title": "Planning ride-pooling services with detour restrictions for spatially\n  heterogeneous demand: A multi-zone queuing network approach",
        "comments": null,
        "journal-ref": null,
        "doi": "10.1016/j.trb.2023.102779",
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  This study presents a multi-zone queuing network model for steady-state\nride-pooling operations that serve heterogeneous demand, and then builds upon\nthis model to optimize the design of ride-pooling services. Spatial\nheterogeneity is addressed by partitioning the study region into a set of\nrelatively homogeneous zones, and a set of criteria are imposed to avoid\nsignificant detours among matched passengers. A generalized multi-zone queuing\nnetwork model is then developed to describe how vehicles' states transition\nwithin each zone and across neighboring zones, and how passengers are served by\nidle or partially occupied vehicles. A large system of equations is constructed\nbased on the queuing network model to analytically evaluate steady-state system\nperformance. Then, we formulate a constrained nonlinear program to optimize the\ndesign of ride-pooling services, such as zone-level vehicle deployment, vehicle\nrouting paths, and vehicle rebalancing operations. A customized solution\napproach is also proposed to decompose and solve the optimization problem. The\nproposed model and solution approach are applied to a hypothetical case and a\nreal-world Chicago case study, so as to demonstrate their applicability and to\ndraw insights. Agent-based simulations are also used to corroborate results\nfrom the proposed analytical model. These numerical examples not only reveal\ninteresting insights on how ride-pooling services serve heterogeneous demand,\nbut also highlight the importance of addressing demand heterogeneity when\ndesigning ride-pooling services.\n"
    },
    {
        "paper_id": 2208.02293,
        "authors": "Christa Cuchiero, Francesca Primavera, Sara Svaluto-Ferro",
        "title": "Universal approximation theorems for continuous functions of c\\`adl\\`ag\n  paths and L\\'evy-type signature models",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  We prove a universal approximation theorem that allows to approximate\ncontinuous functionals of c\\`adl\\`ag (rough) paths uniformly in time and on\ncompact sets of paths via linear functionals of their time-extended signature.\nOur main motivation to treat this question comes from signature-based models\nfor finance that allow for the inclusion of jumps. Indeed, as an important\napplication, we define a new class of universal signature models based on an\naugmented L\\'evy process, which we call L\\'evy-type signature models. They\nextend continuous signature models for asset prices as proposed e.g. by Arribas\net al.(2020) in several directions, while still preserving universality and\ntractability properties. To analyze this, we first show that the signature\nprocess of a generic multivariate L\\'evy process is a polynomial process on the\nextended tensor algebra and then use this for pricing and hedging approaches\nwithin L\\'evy-type signature models.\n"
    },
    {
        "paper_id": 2208.02364,
        "authors": "Xi-Ning Zhuang, Zhao-Yun Chen, Cheng Xue, Yu-Chun Wu, Guo-Ping Guo",
        "title": "Quantum Encoding and Analysis on Continuous Time Stochastic Process with\n  Financial Applications",
        "comments": "37 pages, 15 figures",
        "journal-ref": "Quantum 7, 1127 (2023)",
        "doi": "10.22331/q-2023-10-03-1127",
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  The continuous time stochastic process is a mainstream mathematical\ninstrument modeling the random world with a wide range of applications\ninvolving finance, statistics, physics, and time series analysis, while the\nsimulation and analysis of the continuous time stochastic process is a\nchallenging problem for classical computers. In this work, a general framework\nis established to prepare the path of a continuous time stochastic process in a\nquantum computer efficiently. The storage and computation resource is\nexponentially reduced on the key parameter of holding time, as the qubit number\nand the circuit depth are both optimized via our compressed state preparation\nmethod. The desired information, including the path-dependent and\nhistory-sensitive information that is essential for financial problems, can be\nextracted efficiently from the compressed sampling path, and admits a further\nquadratic speed-up. Moreover, this extraction method is more sensitive to those\ndiscontinuous jumps capturing extreme market events. Two applications of option\npricing in Merton jump diffusion model and ruin probability computing in the\ncollective risk model are given.\n"
    },
    {
        "paper_id": 2208.02409,
        "authors": "Yuchao Dong",
        "title": "Randomized Optimal Stopping Problem in Continuous time and Reinforcement\n  Learning Algorithm",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  In this paper, we study the optimal stopping problem in the so-called\nexploratory framework, in which the agent takes actions randomly conditioning\non current state and an entropy-regularized term is added to the reward\nfunctional. Such a transformation reduces the optimal stopping problem to a\nstandard optimal control problem. We derive the related HJB equation and prove\nits solvability. Furthermore, we give a convergence rate of policy iteration\nand the comparison to classical optimal stopping problem. Based on the\ntheoretical analysis, a reinforcement learning algorithm is designed and\nnumerical results are demonstrated for several models.\n"
    },
    {
        "paper_id": 2208.02573,
        "authors": "Constantinos Kardaras, Hyeng Keun Koo, Johannes Ruf",
        "title": "Estimation of growth in fund models",
        "comments": "40 pages, 5 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Fund models are statistical descriptions of markets where all asset returns\nare spanned by the returns of a lower-dimensional collection of funds, modulo\northogonal noise. Equivalently, they may be characterised as models where the\nglobal growth-optimal portfolio only involves investment in the aforementioned\nfunds. The loss of growth due to estimation error in fund models under local\nfrequentist estimation is determined entirely by the number of funds.\nFurthermore, under a general filtering framework for Bayesian estimation, the\nloss of growth increases as the investment universe does. A shrinkage method\nthat targets maximal growth with the least amount of deviation is proposed.\nEmpirical evidence suggests that shrinkage gives a stable estimate that more\nclosely follows growth potential than an unrestricted Bayesian estimate.\n"
    },
    {
        "paper_id": 2208.02609,
        "authors": "Zied Chaieb, Djibril Gueye",
        "title": "Pricing zero-coupon CAT bonds using the enlargement of ltration theory:\n  a general framework",
        "comments": "Journal of Mathematical Finance, Scientific Research, In press",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The main goal of this paper is to use the enlargement of ltration framework\nfor pricing zerocoupon CAT bonds. For this purpose, we develop two models where\nthe trigger event time is perfectly covered by an increasing sequence of\nstopping times with respect to a reference ltration. Hence, depending on the\nnature of these stopping times the trigger event time can be either accessible\nor totally inaccessible. When some of these stopping times are not predictable,\nthe trigger event time is totally inaccessible, and very nice mathematical\ncomputations can be derived. When the stopping times are predictable, the\ntrigger event time is accessible, and this case would be a meaningful choice\nfor Model 1 from a practical point of view since features like seasonality are\nalready captured by some quantities such as the stochastic intensity of the\nPoisson process. We compute the main tools for pricing the zero-coupon CAT bond\nand show that our constructions are more general than some existing models in\nthe literature. We obtain some closed-form prices of zero-coupon CAT bonds in\nModel 2 so we give a numerical illustrative example for this latter.\n"
    },
    {
        "paper_id": 2208.02659,
        "authors": "Lorenzo Mercuri, Andrea Perchiazzo, Edit Rroji",
        "title": "A Hawkes model with CARMA(p,q) intensity",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  In this paper we introduce a new model named CARMA(p,q)-Hawkes process as the\nHawkes model with exponential kernel implies a strictly decreasing behaviour of\nthe autocorrelation function and empirically evidences reject the monotonicity\nassumption on the autocorrelation function. The proposed model is a Hawkes\nprocess where the intensity follows a Continuous Time Autoregressive Moving\nAverage (CARMA) process and specifically is able to reproduce more realistic\ndependence structures. We also study the conditions of stationarity and\npositivity for the intensity and the strong mixing property for the increments.\nFurthermore we compute the likelihood, present a simulation method and discuss\nan estimation method based on the autocorrelation function. A simulation and\nestimation exercise highlights the main features of the CARMA(p,q)-Hawkes.\n"
    },
    {
        "paper_id": 2208.03135,
        "authors": "Fanwei Zhu, Wendong Xiao, Yao Yu, Ziyi Wang, Zulong Chen, Quan Lu,\n  Zemin Liu, Minghui Wu and Shenghua Ni",
        "title": "Modeling Price Elasticity for Occupancy Prediction in Hotel Dynamic\n  Pricing",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Demand estimation plays an important role in dynamic pricing where the\noptimal price can be obtained via maximizing the revenue based on the demand\ncurve. In online hotel booking platform, the demand or occupancy of rooms\nvaries across room-types and changes over time, and thus it is challenging to\nget an accurate occupancy estimate. In this paper, we propose a novel hotel\ndemand function that explicitly models the price elasticity of demand for\noccupancy prediction, and design a price elasticity prediction model to learn\nthe dynamic price elasticity coefficient from a variety of affecting factors.\nOur model is composed of carefully designed elasticity learning modules to\nalleviate the endogeneity problem, and trained in a multi-task framework to\ntackle the data sparseness. We conduct comprehensive experiments on real-world\ndatasets and validate the superiority of our method over the state-of-the-art\nbaselines for both occupancy prediction and dynamic pricing.\n"
    },
    {
        "paper_id": 2208.03164,
        "authors": "Lucio Fiorin",
        "title": "Estimation of Historical volatility and Allocation strategies using\n  Variance Swaps",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  In this memorie de fin d'etudes, we review some techniques to estimate\nhistorical volatility and to price Variance Swaps\n"
    },
    {
        "paper_id": 2208.03318,
        "authors": "Adam Khakhar and Xi Chen",
        "title": "Delta Hedging Liquidity Positions on Automated Market Makers",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Liquidity Providers on Automated Market Makers generate millions of USD in\ntransaction fees daily. However, the net value of a Liquidity Position is\nvulnerable to price changes in the underlying assets in the pool. The dominant\nmeasure of loss in a Liquidity Position is Impermanent Loss. Impermanent Loss\nfor Constant Function Market Makers has been widely studied. We propose a new\nmetric to measure Liquidity Position PNL based on price movement from the\nunderlying assets. We show how this new metric more appropriately measures the\nchange in the net value of a Liquidity Position as a function of price movement\nin the underlying assets. Our second contribution is an algorithm to delta\nhedge arbitrary Liquidity Positions on both uniform liquidity Automated Market\nMakers (such as Uniswap v2) and concentrated liquidity Automated Market Makers\n(such as Uniswap v3) via a combination of derivatives.\n"
    },
    {
        "paper_id": 2208.03456,
        "authors": "Krishnadas M., K. P. Harikrishnan, G. Ambika",
        "title": "Recurrence measures and transitions in stock market dynamics",
        "comments": "24 pages, 14 figures",
        "journal-ref": null,
        "doi": "10.1016/j.physa.2022.128240",
        "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
        "abstract": "  The financial markets are understood as complex dynamical systems whose\ndynamics is analysed mostly using nonstationary and brief data sets that\nusually come from stock markets. For such data sets, a reliable method of\nanalysis is based on recurrence plots and recurrence networks, constructed from\nthe data sets over the period of study. In this study, we do a comprehensive\nanalysis of the complexity of the underlying dynamics of 26 markets around the\nglobe using recurrence based measures. We also examine trends in the nature of\ntransitions as revealed from these measures by the sliding window analysis\nalong the time series during the global financial crisis of 2008 and compare\nthat with changes during the most recent pandemic related lock down. We show\nthat the measures derived from recurrence patterns can be used to capture the\nnature of transitions in stock market dynamics. Our study reveals that the\nchanges around 2008 indicate stochasticity driven transition, which is\ndifferent from the transition during the pandemic.\n"
    },
    {
        "paper_id": 2208.03564,
        "authors": "Daniel Levy and Avichai Snir",
        "title": "Potterian Economics",
        "comments": "32 pages",
        "journal-ref": "Oxford Open Economics, Vol. 1, Issue 1, 1-32, 2022",
        "doi": "10.1093/ooec/odac004",
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  Recent studies in psychology and neuroscience offer systematic evidence that\nfictional works exert a surprisingly strong influence on readers and have the\npower to shape their opinions and worldviews. Building on these findings, we\nstudy what we term Potterian economics, the economic ideas, insights, and\nstructure, found in Harry Potter books, to assess how the books might affect\neconomic literacy. A conservative estimate suggests that more than 7.3 percent\nof the world population has read the Harry Potter books, and millions more have\nseen their movie adaptations. These extraordinary figures underscore the\nimportance of the messages the books convey. We explore the Potterian economic\nmodel and compare it to professional economic models to assess the consistency\nof the Potterian economic principles with the existing economic models. We find\nthat some of the principles of Potterian economics are consistent with\neconomists models. Many other principles, however, are distorted and contain\nnumerous inaccuracies, contradicting professional economists views and\ninsights. We conclude that Potterian economics can teach us about the formation\nand dissemination of folk economics, the intuitive notions of naive individuals\nwho see market transactions as a zero-sum game, who care about distribution but\nfail to understand incentives and efficiency, and who think of prices as\nallocating wealth but not resources or their efficient use.\n"
    },
    {
        "paper_id": 2208.03568,
        "authors": "Kara Karpman, Sumanta Basu, David Easley",
        "title": "Learning Financial Networks with High-frequency Trade Data",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Financial networks are typically estimated by applying standard time series\nanalyses to price-based economic variables collected at low-frequency (e.g.,\ndaily or monthly stock returns or realized volatility). These networks are used\nfor risk monitoring and for studying information flows in financial markets.\nHigh-frequency intraday trade data sets may provide additional insights into\nnetwork linkages by leveraging high-resolution information. However, such data\nsets pose significant modeling challenges due to their asynchronous nature,\nnonlinear dynamics, and nonstationarity. To tackle these challenges, we\nestimate financial networks using random forests. The edges in our network are\ndetermined by using microstructure measures of one firm to forecast the sign of\nthe change in a market measure (either realized volatility or returns kurtosis)\nof another firm. We first investigate the evolution of network connectivity in\nthe period leading up to the U.S. financial crisis of 2007-09. We find that the\nnetworks have the highest density in 2007, with high degree connectivity\nassociated with Lehman Brothers in 2006. A second analysis into the nature of\nlinkages among firms suggests that larger firms tend to offer better predictive\npower than smaller firms, a finding qualitatively consistent with prior works\nin the market microstructure literature.\n"
    },
    {
        "paper_id": 2208.03815,
        "authors": "Francetic Igor",
        "title": "Selection on moral hazard in the Swiss market for mandatory health\n  insurance: Empirical evidence from Swiss Household Panel data",
        "comments": "24 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Selection on moral hazard represents the tendency to select a specific health\ninsurance coverage depending on the heterogeneity in utilisation ''slopes''. I\nuse data from the Swiss Household Panel and from publicly available regulatory\ndata to explore the extent of selection on slopes in the Swiss managed\ncompetition system. I estimate responses in terms of (log) doctor visits to\nlowest and highest deductible levels using Roy-type models, identifying\nmarginal treatment effects with local instrumental variables. The response to\nhigh coverage plans (i.e. plans with the lowest deductible level) among high\nmoral hazard types is 25-35 percent higher than average.\n"
    },
    {
        "paper_id": 2208.03939,
        "authors": "Johannes Assefa and Philipp Harms",
        "title": "Cylindrical stochastic integration and applications to financial term\n  structure modeling",
        "comments": "19 pages, 2 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We develop a novel - cylindrical - solution concept for stochastic evolution\nequations. Our motivation is to establish a Heath-Jarrow-Morton framework\ncapable of analysing financial term structures with discontinuities, overcoming\ndeep stochastic-analytic limitations posed by mild or weak solution concepts.\nOur cylindrical approach, which we investigate in full generality, bypasses\nthese difficulties and nicely mirrors the structure of a large financial\nmarket.\n"
    },
    {
        "paper_id": 2208.04117,
        "authors": "Darija Barak, Edoardo Gallo, Ke Rong, Ke Tang, Wei Du",
        "title": "Experience of the COVID-19 pandemic in Wuhan leads to a lasting increase\n  in social distancing",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  On 11th Jan 2020, the first COVID-19 related death was confirmed in Wuhan,\nHubei. The Chinese government responded to the outbreak with a lockdown that\nimpacted most residents of Hubei province and lasted for almost three months.\nAt the time, the lockdown was the strictest both within China and worldwide.\nUsing an interactive web-based experiment conducted half a year after the\nlockdown with participants from 11 Chinese provinces, we investigate the\nbehavioral effects of this `shock' event experienced by the population of\nHubei. We find that both one's place of residence and the strictness of\nlockdown measures in their province are robust predictors of individual social\ndistancing behavior. Further, we observe that informational messages are\neffective at increasing compliance with social distancing throughout China,\nwhereas fines for noncompliance work better within Hubei province relative to\nthe rest of the country. We also report that residents of Hubei increase their\npropensity to social distance when exposed to social environments characterized\nby the presence of a superspreader, while the effect is not present outside of\nthe province. Our results appear to be specific to the context of COVID-19, and\nare not explained by general differences in risk attitudes and social\npreferences.\n"
    },
    {
        "paper_id": 2208.04205,
        "authors": "Sidharth Mallik",
        "title": "A mean-variance optimized portfolio constructed for investment in a\n  reference security, for an investor with a preference towards an accepted set\n  of securities",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  We consider a reference security, understood to be an attractive investment,\nwith the caveat that an investor is not willing to directly invest in the\nsecurity, for presence of constraints, either investor specific or pertaining\nto the security itself. The investor, however, is open to a portfolio\nconstructed with an accepted set of securities, where returns could be\nconsidered similar to the reference security. We demonstrate, under a measure\nof similarity, such a portfolio could be selected with a mean-variance\ncharacterization, as defined by Markowitz. Furthermore, we consider the\nperformance relative to the reference security, with the Sharpe Ratio. The\nobjective of the paper is to derive an optimal portfolio to address an investor\npreference for the accepted set of securities.\n"
    },
    {
        "paper_id": 2208.04382,
        "authors": "Askery Canabarro, Taysa M. Mendon\\c{c}a, Ranieri Nery, George Moreno,\n  Anton S. Albino, Gleydson F. de Jesus and Rafael Chaves",
        "title": "Quantum Finance: a tutorial on quantum computing applied to the\n  financial market",
        "comments": "In PORTUGUESE",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-sa/4.0/",
        "abstract": "  Previously only considered a frontier area of Physics, nowadays quantum\ncomputing is one of the fastest growing research field, precisely because of\nits technological applications in optimization problems, machine learning,\ninformation security and simulations. The goal of this article is to introduce\nthe fundamentals of quantum computing, focusing on a promising quantum\nalgorithm and its application to a financial market problem. More specifically,\nwe discuss the portfolio optimization problem using the \\textit{Quantum\nApproximate Optimization Algorithm} (QAOA). We not only describe the main\nconcepts involved but also consider simple practical examples, involving\nfinancial assets available on the Brazilian stock exchange, with codes, both\nclassic and quantum, freely available as a Jupyter Notebook. We also analyze in\ndetails the quality of the combinatorial portfolio optimization solutions\nthrough QAOA using SENAI/CIMATEC's ATOS QLM quantum simulator.\n"
    },
    {
        "paper_id": 2208.04685,
        "authors": "Vinay K Chaudhri",
        "title": "Computable Contracts in the Financial Services Industry",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  A computable contract is a contract that a computer can read, understand and\nexecute. The financial services industry makes extensive use of contracts, for\nexample, mortgage agreements, derivatives contracts, arbitration agreements,\netc. Most of these contracts exist as text documents, making it difficult to\nautomatically query, execute and analyze them. In this vision paper, we argue\nthat the use of computable contracts in the financial services industry will\nlead to substantial improvements in customer experience, reductions in the cost\nof doing legal transactions, make it easier to respond to changing laws, and\nprovide a much better framework for making decisions impacted by contracts.\nUsing a simple payment agreement, we illustrate a Contract Definition Language,\nsketch several use cases and discuss their benefits to the financial services\nindustry.\n"
    },
    {
        "paper_id": 2208.04813,
        "authors": "Antonio Scala and Marco Delmastro",
        "title": "The explosive value of the networks",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  Networks have always played a special role for human beings in shaping social\nrelations, forming public opinion, and driving economic equilibria. Nowadays,\nonline networked platforms dominate digital markets and capitalization\nleader-boards, while social networks drive public discussion. Despite the\nimportance of networks in many economic and social domains (economics,\nsociology, anthropology, psychology,...), the knowledge about the laws that\ndominate their dynamics is still scarce and fragmented. Here, we analyse a wide\nset of online networks (those financed by advertising) by investigating their\nvalue dynamics from several perspectives: the type of service, the geographic\nscope, the merging between networks, and the relationship between economic and\nfinancial value. The results show that the networks are dominated by strongly\nnonlinear dynamics. The existence of non-linearity is often underestimated in\nsocial sciences because it involves contexts that are difficult to deal with,\nsuch as the presence of multiple equilibria -- some of which are unstable. Yet,\nthese dynamics must be fully understood and addressed if we aim to understand\nthe recent evolution in the economic, political and social milieus, which are\nprecisely characterised by corner equilibria (e.g., polarization,\nwinner-take-all solutions, increasing inequality) and nonlinear patterns.\n"
    },
    {
        "paper_id": 2208.04843,
        "authors": "Anas Abudaqa, Mohd Faiz Hilmi, Norziani Dahalan",
        "title": "The Nexus between Job Burnout and Emotional Intelligence on Turnover\n  Intention in Oil and Gas Companies in the UAE",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  Currently, job satisfaction and turnover intentions are the significant\nissues for oil and gas companies in the United Arab Emirates (UAE). These\nissues need to be addressed soon for the performance of the oil and gas\ncompanies. Thus, the aim related to the current study is to examine the impact\nof job burnout, emotional intelligence, and job satisfaction on the turnover\nintentions of the oil and gas companies in the UAE. The goals of this research\nalso include the examination of mediating the influence of job satisfaction\nalongside the nexus of job burnout and turnover intentions of the oil and gas\ncompanies in the UAE. The questionnaire method was adopted to collect the data\nfrom the respondents, and Smart-PLS were employed to analyse the data. The\nresults show that job burnout, emotional intelligence, and job satisfaction\nhave a positive association with turnover intentions. In contrast, job\nsatisfaction positively mediates the nexus between job burnout and turnover\nintentions. These results provide the guidelines to the policymakers that they\nshould enhance their focus on job satisfaction and turnover intentions of the\nemployees that improve the firm performance.\n"
    },
    {
        "paper_id": 2208.04922,
        "authors": "Mark Whitmeyer and Kun Zhang",
        "title": "Costly Evidence and Discretionary Disclosure",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  A sender flexibly acquires evidence--which she may pay a third party to\ncertify--to disclose to a receiver. When evidence acquisition is overt, the\nreceiver observes the evidence gathering process irrespective of whether its\noutcome is certified. When acquisition is covert, the receiver does not. In\ncontrast to the case with exogenous evidence, the receiver prefers a strictly\npositive certification cost. As acquisition costs vanish, equilibria converge\nto the Pareto-worst free-learning equilibrium. The receiver always prefers\ncovert to overt evidence acquisition.\n"
    },
    {
        "paper_id": 2208.05002,
        "authors": "Anindya Bhattacharya, Anirban Kar, Sunil Kumar, Alita Nandi",
        "title": "Patronage and power in rural India: a study based on interaction\n  networks",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  This work has two intertwined components: first, as part of a research\nprogramme it introduces a new methodology for identifying `power-centres' in\nrural societies of developing countries in general and then applies that in the\nspecific context of contemporary rural India for addressing some debates on the\ndynamics of power in rural India. We identify the nature of `local' rural\ninstitutions based on primary data collected by ourselves (in 2013 and 2014).\nWe took 36 villages in the states of Maharashtra, Odisha and Uttar Pradesh - 12\nin each of these states - as the sites for our observation and data collection.\nWe quantify nature of institutions from data on the day-to-day interactions of\nhouseholds in the spheres of economy, society and politics. Our household\nsurvey shows that there is substantial variation in power structure across\nregions. We identified the presence of `local elites' in 22 villages out of 36\nsurveyed. We conducted a follow-up survey, called `elite survey', to get\ndetailed information about the identified elite households. We observe that\nlandlordism has considerably weakened, land has ceased to be the sole source of\npower and new power-centres have emerged. Despite these changes, caste,\nlandownership and patron-client relation continue to be three important pillars\nof rural power structure.\n"
    },
    {
        "paper_id": 2208.05252,
        "authors": "Sonya Ravindranath Waddell, John M. Abowd, Camille Busette, and Mark\n  Hugo Lopez",
        "title": "Measuring Race in US Economic Statistics: What Do We Know?",
        "comments": "Pre-publication version. Includes all information in the published\n  version Bus Econ (2022)",
        "journal-ref": null,
        "doi": "10.1057/s11369-022-00274-3",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This article is an edited transcript of the session of the same name at the\n38th Annual NABE Economic Policy Conference: Policy Options for Sustainable and\nInclusive Growth. The panelists are experts from government and private\nresearch organizations.\n"
    },
    {
        "paper_id": 2208.05567,
        "authors": "Catia Nicodemo, Sonia Oreffice and Climent Quintana-Domeque",
        "title": "Correlates of repeat abortions and their spacing: Evidence from registry\n  data in Spain",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Using administrative data on all induced abortions recorded in Spain in 2019,\nwe analyze the characteristics of women undergoing repeat abortions and the\nspacing between these procedures. Our findings indicate that compared to women\nexperiencing their first abortion, those who undergo repeat abortions are more\nlikely to have lower education levels, have dependent children, live alone, or\nbe foreign-born, with a non-monotonic relationship with age. We also report\nthat being less educated, not employed, having dependent children, or being\nforeign-born are all strongly related to a higher number of repeat abortions.\nLastly, we find that being less educated, foreign-born, or not employed is\ncorrelated with a shorter time interval between the last two abortions.\n"
    },
    {
        "paper_id": 2208.05656,
        "authors": "Daniel Bartl, Johannes Wiesel",
        "title": "Sensitivity of multiperiod optimization problems in adapted Wasserstein\n  distance",
        "comments": "final version, accepted for publication in SIAM J. Financial Math",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We analyze the effect of small changes in the underlying probabilistic model\non the value of multi-period stochastic optimization problems and optimal\nstopping problems. We work in finite discrete time and measure these changes\nwith the adapted Wasserstein distance. We prove explicit first-order\napproximations for both problems. Expected utility maximization is discussed as\na special case.\n"
    },
    {
        "paper_id": 2208.05826,
        "authors": "Hans Henrik Sievertsen",
        "title": "Assessments in Education",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Assessments such as standardized tests and teacher evaluations of students'\nclassroom participation are central elements of most educational systems.\nAssessments inform the student, parent, teacher, and school about the student\nlearning progress. Individuals use the information to adjust their study\nefforts and to make guide their course choice. Schools and teachers use the\ninformation to evaluate effectiveness and inputs. Assessments are also used to\nsort students into tracks, educational programmes, and on the labor market.\nPolicymakers use assessments to reward or penalise schools and parents use\nassessment results to select schools. Consequently, assessments incentivize the\nindividual, the teacher, and the school to do well.\n  Because assessments play an important role in individuals' educational\ncareers, either through the information or the incentive channel, they are also\nimportant for efficiency, equity, and well-being. The information channel is\nimportant for ensuring the most efficient human capital investments: students\nlearn about the returns and costs of effort investments and about their\nabilities and comparative advantages. However, because students are sorted into\neducational programs and on the labor market based on assessment results,\nstudents optimal educational investment might not equal their optimal human\ncapital investment because of the signaling value. Biases in assessments and\nheterogeneity in access to assessments are sources of inequality in education\naccording to gender, origin, and socioeconomic background. These sources have\nlong-running implications for equality and opportunity. Finally, because\nassessment results also carry important consequences for individuals'\neducational opportunities and on the labor market, they are a source of stress\nand reduced well-being.\n"
    },
    {
        "paper_id": 2208.06046,
        "authors": "Jason Milionis, Ciamac C. Moallemi, Tim Roughgarden, Anthony Lee Zhang",
        "title": "Automated Market Making and Loss-Versus-Rebalancing",
        "comments": "63 pages, 7 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We consider the market microstructure of automated market makers (AMMs) from\nthe perspective of liquidity providers (LPs). Our central contribution is a\n``Black-Scholes formula for AMMs''. We identify the main adverse selection cost\nincurred by LPs, which we call ``loss-versus-rebalancing'' (LVR, pronounced\n``lever''). LVR captures costs incurred by AMM LPs due to stale prices that are\npicked off by better informed arbitrageurs. We derive closed-form expressions\nfor LVR applicable to all automated market makers. Our model is quantitatively\nrealistic, matching actual LP returns empirically, and shows how CFMM protocols\ncan be redesigned to reduce or eliminate LVR.\n"
    },
    {
        "paper_id": 2208.06271,
        "authors": "Xiaoguang Ling",
        "title": "The effect of ambient air pollution on birth outcomes in Norway",
        "comments": "45 pages, 10 figures, 21 tables",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Ambient air pollution is harmful to the fetus even in countries with\nrelatively low levels of pollution. In this paper, I examine the effects of\nambient air pollution on birth outcomes in Norway. I find that prenatal\nexposure to ambient nitric oxide in the last trimester causes significant birth\nweight and birth length loss under the same sub-postcode fixed effects and\ncalendar month fixed effects, whereas other ambient air pollutants such as\nnitrogen dioxide and sulfur dioxide appear to be at safe levels for the fetus\nin Norway. In addition, the marginal adverse effect of ambient nitric oxide is\nlarger for newborns with disadvantaged parents. Both average concentrations of\nnitric oxide and occasional high concentration events can adversely affect\nbirth outcomes. The contributions of my work include: first, my finding that\nprenatal exposure to environmental nitric oxide has an adverse effect on birth\noutcomes fills a long-standing knowledge gap. Second, with the large sample\nsize and geographic division of sub-postal codes in Norway, I can control for a\nrich set of spatio-temporal fixed effects to overcome most of the endogeneity\nproblems caused by the choice of residential area and date of delivery. In\naddition, I study ambient air pollution in a low-pollution setting, which\nprovides new evidence on the health effects of low ambient air pollution.\n"
    },
    {
        "paper_id": 2208.06535,
        "authors": "Mingyu Xu, Zuo Quan Xu, Xun Yu Zhou",
        "title": "$g$-Expectation of Distributions",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We define $g$-expectation of a distribution as the infimum of the\n$g$-expectations of all the terminal random variables sharing that\ndistribution. We present two special cases for nonlinear $g$ where the\n$g$-expectation of distributions can be explicitly derived. As a related\nproblem, we introduce the notion of law-invariant $g$-expectation and provide\nits sufficient conditions. Examples of application in financial dynamic\nportfolio choice are supplied.\n"
    },
    {
        "paper_id": 2208.06549,
        "authors": "Mikl\\'os R\\'asonyi and Hasanjan Sayit",
        "title": "Exponential utility maximization in small/large financial markets",
        "comments": "27 Pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Obtaining utility maximizing optimal portfolios in closed form is a\nchallenging issue when the return vector follows a more general distribution\nthan the normal one. In this note, we give closed form expressions, in markets\nbased on finitely many assets, for optimal portfolios that maximize the\nexpected exponential utility when the return vector follows normal\nmean-variance mixture models. We then consider large financial markets based on\nnormal mean-variance mixture models also and show that, under exponential\nutility, the optimal utilities based on small markets converge to the optimal\nutility in the large financial market. This result shows, in particular, that\nto reach optimal utility level investors need to diversify their portfolios to\ninclude infinitely many assets into their portfolio and with portfolios based\non any set of only finitely many assets, they never be able to reach optimum\nlevel of utility. In this paper, we also consider portfolio optimization\nproblems with more general class of utility functions and provide an\neasy-to-implement numerical procedure for locating optimal portfolios.\nEspecially, our approach in this part of the paper reduces a high dimensional\nproblem in locating optimal portfolio into a three dimensional problem for a\ngeneral class of utility functions.\n"
    },
    {
        "paper_id": 2208.06675,
        "authors": "Luca De Benedictis, Vania Licio and Anna Pinna",
        "title": "From the historical Roman road network to modern infrastructure in Italy",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  An integrated and widespread road system, like the one built during the Roman\nEmpire in Italy, plays an important role today in facilitating the construction\nof new infrastructure. This paper investigates the historical path of Roman\nroads as main determinant of both motorways and railways in the country. The\nempirical analysis shows how the modern Italian transport infrastructure\nfollowed the path traced in ancient times by the Romans in constructing their\nroads. Being paved and connecting Italy from North to South, consular\ntrajectories lasted in time, representing the starting physical capital for\ndeveloping the new transport networks.\n"
    },
    {
        "paper_id": 2208.06928,
        "authors": "Haoying Wang, Rafael Garduno Rivera",
        "title": "The Growing US-Mexico Natural Gas Trade and Its Regional Economic\n  Impacts in Mexico",
        "comments": "32 pages, 6 tables, 5 figures, with an appendix at the end",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  With the recent administration change in Mexico, the fluctuations in national\nenergy policy have generated widespread concerns among investors and the\npublic. The debate centers around Mexico's energy dependence on the US and how\nMexico's energy development should move forward. The goal of this study is\ntwo-fold. We first review the history and background of the recent energy\nreforms in Mexico. The focus of the study is on quantifying the state-level\nregional economic impact of the growing US-Mexico natural gas trade in Mexico.\nWe examine both the quantity effect (impact of import volume) and the price\neffect (impact of natural gas price changes). Our empirical analysis adopts a\nfixed-effects regression model and the instrumental variables (IV) estimation\napproach to address spatial heterogeneities and the potential endogeneity\nassociated with natural gas import. The quantity effect analysis suggests a\nstatistically significant positive employment impact of imports in non-mining\nsectors. The impact in the mining sector, however, is insignificant. The\nstate-level average (non-mining) employment impact is 127 jobs per million MCFs\nof natural gas imported from the US. The price effect analysis suggests a\nstatistically significant positive employment impact of price increases in the\nmining sector. A one-percentage increase in natural gas price (1.82 Pesos/GJ,\nin 2015 Peso) leads to an average state-level mining employment increase of 140\n(or 2.38%). We also explored the implications of our findings for Mexico's\nenergy policy, trade policy, and energy security.\n"
    },
    {
        "paper_id": 2208.0693,
        "authors": "Amine Ouazad",
        "title": "Do Investors Hedge Against Green Swans? Option-Implied Risk Aversion to\n  Wildfires",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  Measuring beliefs about natural disasters is challenging. Deep\nout-of-the-money options allow investors to hedge at a range of strikes and\ntime horizons, thus the 3-dimensional surface of firm-level option prices\nprovides information on (i) skewed and fat-tailed beliefs about the impact of\nnatural disaster risk across space and time dimensions at daily frequency; and\n(ii) information on the covariance of wildfire-exposed stocks with investors'\nmarginal utility of wealth. Each publicly-traded company's daily surface of\noption prices is matched with its network of establishments and wildfire\nperimeters over two decades. First, wildfires affect investors' risk neutral\nprobabilities at short and long maturities; investors price asymmetric downward\ntail risk and a probability of upward jumps. The volatility smile is more\npronounced. Second, comparing risk-neutral and physical distributions reveals\nthe option-implied risk aversion with respect to wildfire-exposed stock prices.\nInvestors' marginal utility of wealth is correlated with wildfire shocks.\nOption-implied risk aversion identifies the wildfire-exposed share of\nportfolios. For risk aversions consistent with Barro (2012), equity options\nsuggest (i) investors hold larger shares of wildfire-exposed stocks than the\nmarket portfolio; or (ii) investors may have more pessimistic beliefs about\nwildfires' impacts than what observed returns suggest, such as pricing\nlow-probability unrealized downward tail risk. We calibrate options with models\nfeaturing both upward and downward risk. Results are consistent a significant\npricing of downward jumps.\n"
    },
    {
        "paper_id": 2208.06972,
        "authors": "Darwin Zhou",
        "title": "Is the NFL's franchise tag fair to players?",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  There has been a consistent criticism over the past decade of the NFL\nfranchise tag's monetary limitations due to its biased institutions in favor of\nthe team rather than the player. But the question whether the NFL's franchise\ntag is fair or unfair to players has never been systematically studied. In this\npaper, I investigate the effects of NFL players' contract extensions when on a\nfranchise tag compared to when they are not and analyze them through\nstatistical and economic lens. Through my research, I find that indeed the\ncurrent franchise tag designation is unfair to players when it comes to\ncontract extension. I then propose a solution to remedy this unfairness, that\nis, removing the opportunity to franchise tag players for multiple years, and\nadding an option for the player to either test free agency but receive zero pay\nuntil they settle on a contract (the team can also match the offer) or sign the\nfranchise tag, to provide more flexibility for the player and the team.\n"
    },
    {
        "paper_id": 2208.07158,
        "authors": "Ricard Durall",
        "title": "Asset Allocation: From Markowitz to Deep Reinforcement Learning",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/publicdomain/zero/1.0/",
        "abstract": "  Asset allocation is an investment strategy that aims to balance risk and\nreward by constantly redistributing the portfolio's assets according to certain\ngoals, risk tolerance, and investment horizon. Unfortunately, there is no\nsimple formula that can find the right allocation for every individual. As a\nresult, investors may use different asset allocations' strategy to try to\nfulfil their financial objectives. In this work, we conduct an extensive\nbenchmark study to determine the efficacy and reliability of a number of\noptimization techniques. In particular, we focus on traditional approaches\nbased on Modern Portfolio Theory, and on machine-learning approaches based on\ndeep reinforcement learning. We assess the model's performance under different\nmarket tendency, i.e., both bullish and bearish markets. For reproducibility,\nwe provide the code implementation code in this repository.\n"
    },
    {
        "paper_id": 2208.07159,
        "authors": "Jun Lu, Danny Ding",
        "title": "A Hybrid Approach on Conditional GAN for Portfolio Analysis",
        "comments": "arXiv admin note: substantial text overlap with arXiv:2207.05701",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Over the decades, the Markowitz framework has been used extensively in\nportfolio analysis though it puts too much emphasis on the analysis of the\nmarket uncertainty rather than on the trend prediction. While generative\nadversarial network (GAN), conditional GAN (CGAN), and autoencoding CGAN\n(ACGAN) have been explored to generate financial time series and extract\nfeatures that can help portfolio analysis. The limitation of the CGAN or ACGAN\nframework stands in putting too much emphasis on generating series and finding\nthe internal trends of the series rather than predicting the future trends. In\nthis paper, we introduce a hybrid approach on conditional GAN based on deep\ngenerative models that learns the internal trend of historical data while\nmodeling market uncertainty and future trends. We evaluate the model on several\nreal-world datasets from both the US and Europe markets, and show that the\nproposed HybridCGAN and HybridACGAN models lead to better portfolio allocation\ncompared to the existing Markowitz, CGAN, and ACGAN approaches.\n"
    },
    {
        "paper_id": 2208.07163,
        "authors": "Jos\\'e A. Salmer\\'on, Giulia Di Nunno and Bernardo D'Auria",
        "title": "Before and after default: information and optimal portfolio via\n  anticipating calculus",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Default risk calculus plays a crucial role in portfolio optimization when the\nrisky asset is under threat of bankruptcy. However, traditional stochastic\ncontrol techniques are not applicable in this scenario, and additional\nassumptions are required to obtain the optimal solution in a before-and-after\ndefault context. We propose an alternative approach using forward integration,\nwhich allows to avoid one of the restrictive assumptions, the Jacod density\nhypothesis. We demonstrate that, in the case of logarithmic utility, the weaker\nintensity hypothesis is the appropriate condition for optimality. Furthermore,\nwe establish the semimartingale decomposition of the risky asset in the\nfiltration that is progressively enlarged to accommodate the default process,\nunder the assumption of the existence of the optimal portfolio. This work aims\nto provide valueable insights for developing effective risk management\nstrategies when facing default risk.\n"
    },
    {
        "paper_id": 2208.07165,
        "authors": "Taylan Kabbani, Ekrem Duman",
        "title": "Deep Reinforcement Learning Approach for Trading Automation in The Stock\n  Market",
        "comments": "10 pages, 5 figures, ICANN 2022: 16. International Conference on\n  Artificial Neural Networks",
        "journal-ref": null,
        "doi": "10.1109/ACCESS.2022.3203697",
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  Deep Reinforcement Learning (DRL) algorithms can scale to previously\nintractable problems. The automation of profit generation in the stock market\nis possible using DRL, by combining the financial assets price \"prediction\"\nstep and the \"allocation\" step of the portfolio in one unified process to\nproduce fully autonomous systems capable of interacting with their environment\nto make optimal decisions through trial and error. This work represents a DRL\nmodel to generate profitable trades in the stock market, effectively overcoming\nthe limitations of supervised learning approaches. We formulate the trading\nproblem as a Partially Observed Markov Decision Process (POMDP) model,\nconsidering the constraints imposed by the stock market, such as liquidity and\ntransaction costs. We then solve the formulated POMDP problem using the Twin\nDelayed Deep Deterministic Policy Gradient (TD3) algorithm reporting a 2.68\nSharpe Ratio on unseen data set (test data). From the point of view of stock\nmarket forecasting and the intelligent decision-making mechanism, this paper\ndemonstrates the superiority of DRL in financial markets over other types of\nmachine learning and proves its credibility and advantages of strategic\ndecision-making.\n"
    },
    {
        "paper_id": 2208.07166,
        "authors": "Jaydip Sen, Arpit Awad, Aaditya Raj, Gourav Ray, Pusparna Chakraborty,\n  Sanket Das, Subhasmita Mishra",
        "title": "Stock Performance Evaluation for Portfolio Design from Different Sectors\n  of the Indian Stock Market",
        "comments": "The report is 113 pages long. The report is based on the capstone\n  project done in the post graduate course of data science in Praxis Business\n  School, Kolkata, India - Group 5 of the Autumn Batch, 2021. arXiv admin note:\n  text overlap with arXiv:2201.05570; text overlap with arXiv:2005.11417 by\n  other authors",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The stock market offers a platform where people buy and sell shares of\npublicly listed companies. Generally, stock prices are quite volatile; hence\npredicting them is a daunting task. There is still much research going to\ndevelop more accuracy in stock price prediction. Portfolio construction refers\nto the allocation of different sector stocks optimally to achieve a maximum\nreturn by taking a minimum risk. A good portfolio can help investors earn\nmaximum profit by taking a minimum risk. Beginning with Dow Jones Theory a lot\nof advancement has happened in the area of building efficient portfolios. In\nthis project, we have tried to predict the future value of a few stocks from\nsix important sectors of the Indian economy and also built a portfolio. As part\nof the project, our team has conducted a study of the performance of various\nTime series, machine learning, and deep learning models in stock price\nprediction on selected stocks from the chosen six important sectors of the\neconomy. As part of building an efficient portfolio, we have studied multiple\nportfolio optimization theories beginning with the Modern Portfolio theory. We\nhave built a minimum variance portfolio and optimal risk portfolio for all the\nsix chosen sectors by using the daily stock prices over the past five years as\ntraining data and have also conducted back testing to check the performance of\nthe portfolio. We look forward to continuing our study in the area of stock\nprice prediction and asset allocation and consider this project as the first\nstepping stone.\n"
    },
    {
        "paper_id": 2208.07168,
        "authors": "Danijel Jevtic, Romain Deleze and Joerg Osterrieder",
        "title": "AI for trading strategies",
        "comments": "Keywords: Machine Learning, Trading strategies, Financial\n  time-series, Time-series analysis, Financial data",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  In this bachelor thesis, we show how four different machine learning methods\n(Long Short-Term Memory, Random Forest, Support Vector Machine Regression, and\nk-Nearest Neighbor) perform compared to already successfully applied trading\nstrategies such as Cross Signal Trading and a conventional statistical time\nseries model ARMA-GARCH. The aim is to show that machine learning methods\nperform better than conventional methods in the crude oil market when used\ncorrectly. A more detailed performance analysis was made, showing the\nperformance of the different models in different market phases so that the\nrobustness of individual models in high and low volatility phases could be\nexamined more closely. For further investigation, these models would also have\nto be analyzed in other markets.\n"
    },
    {
        "paper_id": 2208.07222,
        "authors": "Mohd Shadab Danish, Pritam Ranjan, and Ruchi Sharma",
        "title": "Assessing the Impact of Patent Attributes on the Value of Discrete and\n  Complex Innovations",
        "comments": null,
        "journal-ref": null,
        "doi": "10.1142/S1363919622500165",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This study assesses the degree to which the social value of patents can be\nconnected to the private value of patents across discrete and complex\ninnovation. The underlying theory suggests that the social value of cumulative\npatents is less related to the private value of patents. We use the patents\napplied between 1995 to 2002 and granted on or before December 2018 from the\nIndian Patent Office (IPO). Here the patent renewal information is utilized as\na proxy for the private value of the patent. We have used a variety of logit\nregression model for the impact assessment analysis. The results reveal that\nthe technology classification (i.e., discrete versus complex innovations) plays\nan important role in patent value assessment, and some technologies are\nsignificantly different than the others even within the two broader\nclassifications. Moreover, the non-resident patents in India are more likely to\nhave a higher value than the resident patents. According to the conclusions of\nthis study, only a few technologies from the discrete and complex innovation\ncategories have some private value. There is no evidence that patent social\nvalue indicators are less useful in complicated technical classes than in\ndiscrete ones.\n"
    },
    {
        "paper_id": 2208.07232,
        "authors": "Lei Li, Zhiyuan Zhang, Ruihan Bao, Keiko Harimoto, Xu Sun",
        "title": "Distributional Correlation--Aware Knowledge Distillation for Stock\n  Trading Volume Prediction",
        "comments": "ECML-PKDD 2022, our code and data will be available at\n  https://github.com/lancopku/DCKD",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Traditional knowledge distillation in classification problems transfers the\nknowledge via class correlations in the soft label produced by teacher models,\nwhich are not available in regression problems like stock trading volume\nprediction. To remedy this, we present a novel distillation framework for\ntraining a light-weight student model to perform trading volume prediction\ngiven historical transaction data. Specifically, we turn the regression model\ninto a probabilistic forecasting model, by training models to predict a\nGaussian distribution to which the trading volume belongs. The student model\ncan thus learn from the teacher at a more informative distributional level, by\nmatching its predicted distributions to that of the teacher. Two correlational\ndistillation objectives are further introduced to encourage the student to\nproduce consistent pair-wise relationships with the teacher model. We evaluate\nthe framework on a real-world stock volume dataset with two different time\nwindow settings. Experiments demonstrate that our framework is superior to\nstrong baseline models, compressing the model size by $5\\times$ while\nmaintaining $99.6\\%$ prediction accuracy. The extensive analysis further\nreveals that our framework is more effective than vanilla distillation methods\nunder low-resource scenarios.\n"
    },
    {
        "paper_id": 2208.07248,
        "authors": "Semen Budennyy, Alexey Kazakov, Elizaveta Kovtun, Leonid Zhukov",
        "title": "New drugs and stock market: how to predict pharma market reaction to\n  clinical trial announcements",
        "comments": "17 pages, 14 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Pharmaceutical companies operate in a strictly regulated and highly risky\nenvironment in which a single slip can lead to serious financial implications.\nAccordingly, the announcements of clinical trial results tend to determine the\nfuture course of events, hence being closely monitored by the public. In this\nwork, we provide statistical evidence for the result promulgation influence on\nthe public pharma market value. Whereas most works focus on retrospective\nimpact analysis, the present research aims to predict the numerical values of\nannouncement-induced changes in stock prices. For this purpose, we develop a\npipeline that includes a BERT-based model for extracting sentiment polarity of\nannouncements, a Temporal Fusion Transformer for forecasting the expected\nreturn, a graph convolution network for capturing event relationships, and\ngradient boosting for predicting the price change. The challenge of the problem\nlies in inherently different patterns of responses to positive and negative\nannouncements, reflected in a stronger and more pronounced reaction to the\nnegative news. Moreover, such phenomenon as the drop in stocks after the\npositive announcements affirms the counterintuitiveness of the price behavior.\nImportantly, we discover two crucial factors that should be considered while\nworking within a predictive framework. The first factor is the drug portfolio\nsize of the company, indicating the greater susceptibility to an announcement\nin the case of small drug diversification. The second one is the network effect\nof the events related to the same company or nosology. All findings and\ninsights are gained on the basis of one of the biggest FDA (the Food and Drug\nAdministration) announcement datasets, consisting of 5436 clinical trial\nannouncements from 681 companies over the last five years.\n"
    },
    {
        "paper_id": 2208.07251,
        "authors": "Herv\\'e Andr\\`es (CERMICS), Alexandre Boumezoued, Benjamin Jourdain\n  (CERMICS, MATHRISK)",
        "title": "Signature-based validation of real-world economic scenarios",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Motivated by insurance applications, we propose a new approach for the\nvalidation of real-world economic scenarios. This approach is based on the\nstatistical test developed by Chevyrev and Oberhauser (2022) and relies on the\nnotions of signature and maximum mean distance. This test allows to check\nwhether two samples of stochastic processes paths come from the same\ndistribution. Our contribution is to apply this test to a variety of stochastic\nprocesses exhibiting different pathwise properties (H{\\\"o}lder regularity,\nautocorrelation, regime switches) and which are relevant for the modelling of\nstock prices and stock volatility as well as of inflation in view of actuarial\napplications.\n"
    },
    {
        "paper_id": 2208.07254,
        "authors": "Mike Kraehenbuehl, Joerg Osterrieder",
        "title": "The Efficient Market Hypothesis for Bitcoin in the context of neural\n  networks",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  This study examines the weak form of the efficient market hypothesis for\nBitcoin using a feedforward neural network. Due to the increasing popularity of\ncryptocurrencies in recent years, the question has arisen, as to whether market\ninefficiencies could be exploited in Bitcoin. Several studies we refer to here\ndiscuss this topic in the context of Bitcoin using either statistical tests or\nmachine learning methods, mostly relying exclusively on data from Bitcoin\nitself. Results regarding market efficiency vary from study to study. In this\nstudy, however, the focus is on applying various asset-related input features\nin a neural network. The aim is to investigate whether the prediction accuracy\nimproves when adding equity stock indices (S&P 500, Russell 2000), currencies\n(EURUSD), 10 Year US Treasury Note Yield as well as Gold&Silver producers index\n(XAU), in addition to using Bitcoin returns as input feature. As expected, the\nresults show that more features lead to higher training performance from 54.6%\nprediction accuracy with one feature to 61% with six features. On the test set,\nwe observe that with our neural network methodology, adding additional asset\nclasses, no increase in prediction accuracy is achieved. One feature set is\nable to partially outperform a buy-and-hold strategy, but the performance drops\nagain as soon as another feature is added. This leads us to the partial\nconclusion that weak market inefficiencies for Bitcoin cannot be detected using\nneural networks and the given asset classes as input. Therefore, based on this\nstudy, we find evidence that the Bitcoin market is efficient in the sense of\nthe efficient market hypothesis during the sample period. We encourage further\nresearch in this area, as much depends on the sample period chosen, the input\nfeatures, the model architecture, and the hyperparameters.\n"
    },
    {
        "paper_id": 2208.07305,
        "authors": "Daniel Z. Zanger",
        "title": "G3Ms:Generalized Mean Market Makers",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  In the Decentralized Finance (DeFi) setting, we present a new parametrized\nfamily of Constant Function Market Makers (CFMMs) which we call the Generalized\nMean Market Makers (G3Ms), based on the generalized means. The G3Ms are\nintermediate between the Arithmetic Mean and Geometric Mean CFMM models, which\nG3Ms incorporate as special cases. We also present an extension of the G3Ms,\nbased on the so-called Generalized f-Means, called Generalized f-Mean Market\nMakers (Gf3Ms). We show in addition that the G3Ms possess certain properties\npreferable to those exhibited by either the Arithmetic Mean CFMM or the\nGeometric Mean CFMM alone.\n"
    },
    {
        "paper_id": 2208.07533,
        "authors": "Zhanyi Jiao, Steven Kou, Yang Liu, Ruodu Wang",
        "title": "An axiomatic theory for anonymized risk sharing",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We study an axiomatic framework for anonymized risk sharing. In contrast to\ntraditional risk sharing settings, our framework requires no information on\npreferences, identities, private operations and realized losses from the\nindividual agents, and thereby it is useful for modeling risk sharing in\ndecentralized systems. Four axioms natural in such a framework -- actuarial\nfairness, risk fairness, risk anonymity, and operational anonymity -- are put\nforward and discussed. We establish the remarkable fact that the four axioms\ncharacterizes the conditional mean risk sharing rule, revealing the unique and\nprominent role of this popular risk sharing rule among all others in relevant\napplications of anonymized risk sharing. Several other properties and their\nrelations to the four axioms are studied, as well as their implications in\nrationalizing the design of some sharing mechanisms in practice.\n"
    },
    {
        "paper_id": 2208.07626,
        "authors": "Bryce McLaughlin and Jann Spiess",
        "title": "Algorithmic Assistance with Recommendation-Dependent Preferences",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  When an algorithm provides risk assessments, we typically think of them as\nhelpful inputs to human decisions, such as when risk scores are presented to\njudges or doctors. However, a decision-maker may not only react to the\ninformation provided by the algorithm. The decision-maker may also view the\nalgorithmic recommendation as a default action, making it costly for them to\ndeviate, such as when a judge is reluctant to overrule a high-risk assessment\nfor a defendant or a doctor fears the consequences of deviating from\nrecommended procedures. To address such unintended consequences of algorithmic\nassistance, we propose a principal-agent model of joint human-machine\ndecision-making. Within this model, we consider the effect and design of\nalgorithmic recommendations when they affect choices not just by shifting\nbeliefs, but also by altering preferences. We motivate this assumption from\ninstitutional factors, such as a desire to avoid audits, as well as from\nwell-established models in behavioral science that predict loss aversion\nrelative to a reference point, which here is set by the algorithm. We show that\nrecommendation-dependent preferences create inefficiencies where the\ndecision-maker is overly responsive to the recommendation. As a potential\nremedy, we discuss algorithms that strategically withhold recommendations, and\nshow how they can improve the quality of final decisions.\n"
    },
    {
        "paper_id": 2208.07694,
        "authors": "Roger J. A. Laeven, Emanuela Rosazza Gianin",
        "title": "Quasi-Logconvex Measures of Risk",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper introduces and fully characterizes the novel class of\nquasi-logconvex measures of risk, to stand on equal footing with the rich class\nof quasi-convex measures of risk. Quasi-logconvex risk measures naturally\ngeneralize logconvex return risk measures, just like quasi-convex risk measures\ngeneralize convex monetary risk measures. We establish their dual\nrepresentation and analyze their taxonomy in a few (sub)classification results.\nFurthermore, we characterize quasi-logconvex risk measures in terms of\nproperties of families of acceptance sets and provide their law-invariant\nrepresentation. Examples and applications to portfolio choice and capital\nallocation are also discussed.\n"
    },
    {
        "paper_id": 2208.07839,
        "authors": "Victor Olkhov",
        "title": "Why Economic Theories and Policies Fail? Unnoticed Variables and\n  Overlooked Economics",
        "comments": "26 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Accuracy of economic theories and efficiency of economic policy strictly\ndepend on the choice of the economic variables and processes mostly liable for\ndescription of economic reality. That states the general problem of assessment\nof any possible economic variables and processes chargeable for economic\nevolution. We show that economic variables and processes described by current\neconomic theories constitute only a negligible fraction of factors responsible\nfor economic dynamics. We consider numerous unnoted economic variables and\noverlooked economic processes those determine the states and predictions of the\nreal economics. We regard collective economic variables, collective\ntransactions and expectations, mean risks of economic variables and\ntransactions, collective velocities and flows of economic variables,\ntransactions and expectations as overlooked factors of economic evolution. We\nintroduce market-based probability of the asset price and consider unnoticed\ninfluence of market stochasticity on randomness of macroeconomic variables. We\nintroduce economic domain composed by continuous numeric risk grades and\noutline that the bounds of the economic domain result in unnoticed inherent\ncyclical motion of collective variables, transactions and expectations those\nare responsible for observed business cycles. Our treatment of unnoticed and\noverlooked factors of theoretical economics and policy decisions preserves a\nwide field of studies for many decades for academic researchers, economic\nauthorities and high-level politicians.\n"
    },
    {
        "paper_id": 2208.07926,
        "authors": "R. Maria del Rio-Chanona, Alejandro Hermida-Carrillo, Melody\n  Sepahpour-Fard, Luning Sun, Renata Topinkova, Ljubica Nedelkoska",
        "title": "Mental health concerns prelude the Great Resignation: Evidence from\n  Social Media",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  To study the causes of the 2021 Great Resignation, we use text analysis to\ninvestigate the changes in work- and quit-related posts between 2018 and 2021\non Reddit. We find that the Reddit discourse evolution resembles the dynamics\nof the U.S. quit and layoff rates. Furthermore, when the COVID-19 pandemic\nstarted, conversations related to working from home, switching jobs,\nwork-related distress, and mental health increased. We distinguish between\ngeneral work-related and specific quit-related discourse changes using a\ndifference-in-differences method. Our main finding is that mental health and\nwork-related distress topics disproportionally increased among quit-related\nposts since the onset of the pandemic, likely contributing to the Great\nResignation. Along with better labor market conditions, some relief came\nbeginning-to-mid-2021 when these concerns decreased. Our study validates the\nuse of forums such as Reddit for studying emerging economic phenomena in real\ntime, complementing traditional labor market surveys and administrative data.\n"
    },
    {
        "paper_id": 2208.08169,
        "authors": "Ivonne Schwartz and Mark Kirstein",
        "title": "Time is limited on the road to asymptopia",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  One challenge in the estimation of financial market agent-based models\n(FABMs) is to infer reliable insights using numerical simulations validated by\nonly a single observed time series. Ergodicity (besides stationarity) is a\nstrong precondition for any estimation, however it has not been systematically\nexplored and is often simply presumed. For finite-sample lengths and limited\ncomputational resources empirical estimation always takes place in\npre-asymptopia. Thus broken ergodicity must be considered the rule, but it\nremains largely unclear how to deal with the remaining uncertainty in\nnon-ergodic observables. Here we show how an understanding of the ergodic\nproperties of moment functions can help to improve the estimation of (F)ABMs.\nWe run Monte Carlo experiments and study the convergence behaviour of moment\nfunctions of two prototype models. We find infeasibly-long convergence times\nfor most. Choosing an efficient mix of ensemble size and simulated time length\nguided our estimation and might help in general.\n"
    },
    {
        "paper_id": 2208.083,
        "authors": "Tashreef Muhammad, Anika Bintee Aftab, Md. Mainul Ahsan, Maishameem\n  Meherin Muhu, Muhammad Ibrahim, Shahidul Islam Khan and Mohammad Shafiul Alam",
        "title": "Transformer-Based Deep Learning Model for Stock Price Prediction: A Case\n  Study on Bangladesh Stock Market",
        "comments": "16 Pages, 14 Figures (including some containing subfigures)",
        "journal-ref": "Preprint of an article published in [International Journal of\n  Computational Intelligence and Applications, Volume 22, No. 01, 2023]\n  \\c{opyright} [copyright World Scientific Publishing Company]\n  [https://www.worldscientific.com/worldscinet/ijcia]",
        "doi": "10.1142/S146902682350013X",
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  In modern capital market the price of a stock is often considered to be\nhighly volatile and unpredictable because of various social, financial,\npolitical and other dynamic factors. With calculated and thoughtful investment,\nstock market can ensure a handsome profit with minimal capital investment,\nwhile incorrect prediction can easily bring catastrophic financial loss to the\ninvestors. This paper introduces the application of a recently introduced\nmachine learning model - the Transformer model, to predict the future price of\nstocks of Dhaka Stock Exchange (DSE), the leading stock exchange in Bangladesh.\nThe transformer model has been widely leveraged for natural language processing\nand computer vision tasks, but, to the best of our knowledge, has never been\nused for stock price prediction task at DSE. Recently the introduction of\ntime2vec encoding to represent the time series features has made it possible to\nemploy the transformer model for the stock price prediction. This paper\nconcentrates on the application of transformer-based model to predict the price\nmovement of eight specific stocks listed in DSE based on their historical daily\nand weekly data. Our experiments demonstrate promising results and acceptable\nroot mean squared error on most of the stocks.\n"
    },
    {
        "paper_id": 2208.0843,
        "authors": "Marie Michaelides, Mathieu Pigeon and H\\'el\\`ene Cossette",
        "title": "Individual Claims Reserving using Activation Patterns",
        "comments": "European Actuarial Journal (2023)",
        "journal-ref": null,
        "doi": "10.1007/s13385-023-00355-3",
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  The occurrence of a claim often impacts not one but multiple insurance\ncoverages provided in the contract. To account for this multivariate feature,\nwe propose a new individual claims reserving model built around the activation\nof the different coverages to predict the reserve amounts. Using the framework\nof multinomial logistic regression, we model the activation of the different\ninsurance coverages for each claim and their development in the following\nyears, i.e. the activation of other coverages in the later years and all the\npossible payments that might result from them. As such, the model allows us to\ncomplete the individual development of the open claims in the portfolio. Using\na recent automobile dataset from a major Canadian insurance company, we\ndemonstrate that this approach generates accurate predictions of the total\nreserves as well as of the reserves per insurance coverage. This allows the\ninsurer to get better insights in the dynamics of his claims reserves.\n"
    },
    {
        "paper_id": 2208.08442,
        "authors": "I. Martin-de-Santos",
        "title": "Peculiaridades de la Economia islandesa en los albores del siglo XXI",
        "comments": "23 pages, in Portuguese language",
        "journal-ref": "Revista Internacional del mundo Econ\\'omico y del Derecho, 2014",
        "doi": "10.5281/zenodo.4501211",
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  Se repasa brevemente la historia y las finanzas islandesas de manera\ndiacr\\'onica. Se presenta a Islandia como basti\\'on del estallido de la crisis\nfinanciera internacional que comienza a gestarse a principios del siglo XXI y\ncuyo origen se hace evidente en la fecha simb\\'olica del a\\~no 2008. Se\nanalizan las razones fundamentales de esta crisis, centrandonos en las\nparticularidades de la estructura econ\\'omica islandesa. Se consideran las\ndiferencias y parecidos de esta situaci\\'on en relaci\\'on a algunos otros\npa\\'ises en similares circunstancias. Se estudia el caso del banco Icesave. Se\nconsidera la repercusi\\'on que la crisis experimentada por Islandia tiene en el\n\\'ambito internacional, especialmente en los inversores extranjeros y en los\nconflictos jur\\'idicos surgidos a ra\\'iz de las medidas adoptadas por el\ngobierno island\\'es para sacar al pa\\'is de la bancarrota.\n  --\n  Icelandic history and diachronically finances are briefly reviewed. Iceland\nis presented as a bastion of the outbreak of the global financial crisis begins\nto take shape in the early twenty-first century and whose origin is evident in\nthe symbolic date of 2008. The main reasons for this crisis are analyzed,\nfocusing on the particularities of Iceland's economic structure. The\ndifferences and similarities of this in relation to some other countries in\nsimilar circumstances are considered. Bank Icesave case is studied. The impact\nof the crises experienced by Iceland has in the international arena, especially\nforeign investors and legal disputes arising out of actions taken by the\nIcelandic government to pull the country out of bankruptcy is considered.\n"
    },
    {
        "paper_id": 2208.08471,
        "authors": "Yuyu Chen, Paul Embrechts, Ruodu Wang",
        "title": "An unexpected stochastic dominance: Pareto distributions, dependence,\n  and diversification",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We find the perhaps surprising inequality that the weighted average of\nindependent and identically distributed Pareto random variables with infinite\nmean is larger than one such random variable in the sense of first-order\nstochastic dominance. This result holds for more general models including\nsuper-Pareto distributions, negative dependence, and triggering events, and\nyields superadditivity of the risk measure Value-at-Risk for these models.\n"
    },
    {
        "paper_id": 2208.08496,
        "authors": "Ali Saeb",
        "title": "Stock Prices as Janardan Galton Watson Process",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Janardan (1980) introduces a class of offspring distributions that sandwich\nbetween Bernoulli and Poisson. This paper extends the Janardan Galton Watson\n(JGW) branching process as a model of stock prices. In this article, the return\nvalue over time t depends on the initial close price, which shows the number of\noffspring, has a role in the expectation of return and probability of\nextinction after the passage at time t. Suppose the number of offspring in t th\ngeneration is zero, (i.e., called extinction of model at time t) is equivalent\nwith negative return values over time [0, t]. We also introduce the Algorithm\nthat detecting the trend of stock markets.\n"
    },
    {
        "paper_id": 2208.08497,
        "authors": "Xia Han, Ruodu Wang, Xun Yu Zhou",
        "title": "Choquet regularization for reinforcement learning",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We propose \\emph{Choquet regularizers} to measure and manage the level of\nexploration for reinforcement learning (RL), and reformulate the\ncontinuous-time entropy-regularized RL problem of Wang et al. (2020, JMLR,\n21(198)) in which we replace the differential entropy used for regularization\nwith a Choquet regularizer. We derive the Hamilton--Jacobi--Bellman equation of\nthe problem, and solve it explicitly in the linear--quadratic (LQ) case via\nmaximizing statically a mean--variance constrained Choquet regularizer. Under\nthe LQ setting, we derive explicit optimal distributions for several specific\nChoquet regularizers, and conversely identify the Choquet regularizers that\ngenerate a number of broadly used exploratory samplers such as\n$\\epsilon$-greedy, exponential, uniform and Gaussian.\n"
    },
    {
        "paper_id": 2208.08746,
        "authors": "Alessio Calvelli",
        "title": "No-Arbitrage Pricing, Dynamics and Forward Prices of Collateralized\n  Derivatives",
        "comments": "Upgrades of v4: added Proposition 2.34",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper analyzes the pricing of collateralized derivatives, i.e. contracts\nwhere counterparties are not only subject to financial derivatives cash flows\nbut also to collateral cash flows arising from a collateral agreement. We do\nthis along the lines of the brilliant approach of the first part of Moreni and\nPallavicini (2017): in particular we extend their framework where underlyings\nare continuous processes driven by a Brownian vector, to a more general setup\nwhere underlyings are semimartingales (and hence jump processes). First of all,\nwe briefly derive from scratch the theoretical foundations of the main\nsubsequent achievements, i.e. the extension of the classical No-Arbitrage\ntheory to dividend paying semimartingale assets, where by dividend we mean any\ncash flow earned/paid from holding the asset. In this part we merge, in the\nsame treatment and under the same notation, the principal known results with\nsome original ones. Then we extend the approach of Moreni and Pallavicini\n(2017) in different directions and we derive not only the pricing formulae but\nalso the dynamics and forward prices of collateralized derivatives (extending\nthe achievements of the first part of Gabrielli et al. (2019)). Finally, we\nstudy some important applications (Repurchase Agreements, Securities Lending\nand Futures contracts) of previously established theoretical frameworks,\nobtaining some results that are commonly used in practitioners literature, but\noften not well understood.\n"
    },
    {
        "paper_id": 2208.09087,
        "authors": "Jorge A. Garcia and Angelos Alamanos",
        "title": "Integrated modelling approaches for sustainable agri-economic growth and\n  environmental improvement: Examples from Canada, Greece, and Ireland",
        "comments": "25 pages, 3 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Complex agricultural problems concern many countries, as the economic motives\nare increasingly higher, and at the same time the consequences from the\nirrational resources use and emissions are becoming more evident. In this work\nwe study three of the most common agricultural problems and model them through\noptimization techniques, showing ways to assess conflicting objectives together\nas a system and provide overall optimum solutions. The studied problems refer\nto: i) a water-scarce area with overexploited surface and groundwater resources\ndue to over-pumping for irrigation (Central Greece), ii) a water-abundant area\nwith issues of water quality deterioration caused by agriculture (Southern\nOntario, Canada), iii) and a case of intensified agriculture based on animal\nfarming that causes issues of water, soil quality degradation, and increased\ngreenhouse gases emissions (Central Ireland). Linear, non-linear, and Goal\nProgramming optimization techniques have been developed and applied for each\ncase to maximize farmers welfare, make a less intensive use of environmental\nresources, and control the emission of pollutants. The proposed approaches and\ntheir solutions are novel applications for each case-study, compared to the\nexisting literature and practice. Furthermore, they provide useful insights for\nmost countries facing similar problems, they are easily applicable, and\ndeveloped and solved in publicly available tools such as Python.\n"
    },
    {
        "paper_id": 2208.09156,
        "authors": "Emanuel Sommer, Karoline Bax, Claudia Czado",
        "title": "Vine Copula based portfolio level conditional risk measure forecasting",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Accurately estimating risk measures for financial portfolios is critical for\nboth financial institutions and regulators. However, many existing models\noperate at the aggregate portfolio level and thus fail to capture the complex\ncross-dependencies between portfolio components. To address this, a new\napproach is presented that uses vine copulas in combination with univariate\nARMA-GARCH models for marginal modelling to compute conditional portfolio-level\nrisk measure estimates by simulating portfolio-level forecasts conditioned on a\nstress factor. A quantile-based approach is then presented to observe the\nbehaviour of risk measures given a particular state of the conditioning\nasset(s). In a case study of Spanish equities with different stress factors,\nthe results show that the portfolio is quite robust to a sharp downturn in the\nAmerican market. At the same time, there is no evidence of this behaviour with\nrespect to the European market.\n"
    },
    {
        "paper_id": 2208.09239,
        "authors": "Antonio Cabrales, Manu Garc\\'ia, David Ramos Mu\\~noz and Angel\n  S\\'anchez",
        "title": "The Interactions of Social Norms about Climate Change: Science,\n  Institutions and Economics",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  We study the evolution of interest about climate change between different\nactors of the population, and how the interest of those actors affect one\nanother. We first document the evolution individually, and then provide a model\nof cross influences between them, that we then estimate with a VAR. We find\nlarge swings over time of said interest for the general public by creating a\nClimate Change Index for Europe and the US (CCI) using news media mentions, and\nlittle interest among economists (measured by publications in top journals of\nthe discipline). The general interest science journals and policymakers have a\nmore steady interest, although policymakers get interested much later.\n"
    },
    {
        "paper_id": 2208.09372,
        "authors": "Po-Yi Liu, Chi-Hua Wang, Henghsiu Tsai",
        "title": "Non-Stationary Dynamic Pricing Via Actor-Critic Information-Directed\n  Pricing",
        "comments": "24 pages, 13 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  This paper presents a novel non-stationary dynamic pricing algorithm design,\nwhere pricing agents face incomplete demand information and market environment\nshifts. The agents run price experiments to learn about each product's demand\ncurve and the profit-maximizing price, while being aware of market environment\nshifts to avoid high opportunity costs from offering sub-optimal prices. The\nproposed ACIDP extends information-directed sampling (IDS) algorithms from\nstatistical machine learning to include microeconomic choice theory, with a\nnovel pricing strategy auditing procedure to escape sub-optimal pricing after\nmarket environment shift. The proposed ACIDP outperforms competing bandit\nalgorithms including Upper Confidence Bound (UCB) and Thompson sampling (TS) in\na series of market environment shifts.\n"
    },
    {
        "paper_id": 2208.09642,
        "authors": "Lioba Heimbach, Eric Schertenleib, Roger Wattenhofer",
        "title": "Exploring Price Accuracy on Uniswap V3 in Times of Distress",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Financial markets have evolved over centuries, and exchanges have converged\nto rely on the order book mechanism for market making. Latency on the\nblockchain, however, has prevented decentralized exchanges (DEXes) from\nutilizing the order book mechanism and instead gave rise to the development of\nmarket designs that are better suited to a blockchain. Although the first\nwidely popularized DEX, Uniswap V2, stood out through its astonishing\nsimplicity, a recent design overhaul introduced with Uniswap V3 has introduced\nincreasing levels of complexity aiming to increase capital efficiency.\n  In this work, we empirically study the ability of Unsiwap V3 to handle\nunexpected price shocks. Our analysis finds that the prices on Uniswap V3 were\ninaccurate during the recent abrupt price drops of two stablecoins: UST and\nUSDT. We identify the lack of agility required of Unsiwap V3 liquidity\nproviders as the root cause of these worrying price inaccuracies. Additionally,\nwe outline that there are too few incentives for liquidity providers to enter\nliquidity pools, given the elevated volatility in such market conditions.\n"
    },
    {
        "paper_id": 2208.09895,
        "authors": "Michael Monoyios and Oleksii Mostovyi",
        "title": "Stability of the Epstein-Zin problem",
        "comments": "32 pages, preliminary version",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We investigate the stability of the Epstein-Zin problem with respect to small\ndistortions in the dynamics of the traded securities. We work in incomplete\nmarket model settings, where our parametrization of perturbations allows for\njoint distortions in returns and volatility of the risky assets and the\ninterest rate. Considering empirically the most relevant specifications of risk\naversion and elasticity of intertemporal substitution, we provide a condition\nthat guarantees the convexity of the domain of the underlying problem and\nresults in the existence and uniqueness of a solution to it. Then, we prove the\nconvergence of the optimal consumption streams, the associated wealth\nprocesses, the indirect utility processes, and the value functions in the limit\nwhen the model perturbations vanish.\n"
    },
    {
        "paper_id": 2208.09898,
        "authors": "William Busching, Delphine Hintz, Oleksii Mostovyi, Alexey Pozdnyakov",
        "title": "Fair pricing and hedging under small perturbations of the num\\'eraire on\n  a finite probability space",
        "comments": "20 pages, accepted in Involve",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We consider the problem of fair pricing and hedging under small perturbations\nof the num\\'eraire. We show that for replicable claims, the change of\nnum\\'eraire affects neither the fair price nor the hedging strategy. For\nnon-replicable claims, we demonstrate that is not the case. By reformulating\nthe key stochastic control problem in a more tractable form, we show that both\nthe fair price and optimal strategy are stable with respect to small\nperturbations of the num\\'eraire. Further, our approach allows for explicit\nasymptotic formulas describing the fair price and hedging strategy's leading\norder correction terms. Mathematically, our results constitute stability and\nasymptotic analysis of a stochastic control problem under certain perturbations\nof the integrator of the controlled process, where constraints make this\nproblem hard to analyze.\n"
    },
    {
        "paper_id": 2208.09968,
        "authors": "Daniel Poh, Stephen Roberts and Stefan Zohren",
        "title": "Transfer Ranking in Finance: Applications to Cross-Sectional Momentum\n  with Data Scarcity",
        "comments": "18 pages, 12 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Cross-sectional strategies are a classical and popular trading style, with\nrecent high performing variants incorporating sophisticated neural\narchitectures. While these strategies have been applied successfully to\ndata-rich settings involving mature assets with long histories, deploying them\non instruments with limited samples generally produce over-fitted models with\ndegraded performance. In this paper, we introduce Fused Encoder Networks -- a\nnovel and hybrid parameter-sharing transfer ranking model. The model fuses\ninformation extracted using an encoder-attention module operated on a source\ndataset with a similar but separate module focused on a smaller target dataset\nof interest. This mitigates the issue of models with poor generalisability that\nare a consequence of training on scarce target data. Additionally, the\nself-attention mechanism enables interactions among instruments to be accounted\nfor, not just at the loss level during model training, but also at inference\ntime. Focusing on momentum applied to the top ten cryptocurrencies by market\ncapitalisation as a demonstrative use-case, the Fused Encoder Networks\noutperforms the reference benchmarks on most performance measures, delivering a\nthree-fold boost in the Sharpe ratio over classical momentum as well as an\nimprovement of approximately 50% against the best benchmark model without\ntransaction costs. It continues outperforming baselines even after accounting\nfor the high transaction costs associated with trading cryptocurrencies.\n"
    },
    {
        "paper_id": 2208.09986,
        "authors": "Mao Fabrice Djete",
        "title": "Non--regular McKean--Vlasov equations and calibration problem in local\n  stochastic volatility models",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  In order to deal with the question of the existence of a calibrated local\nstochastic volatility model in finance, we investigate a class of\nMcKean--Vlasov equations where a minimal continuity assumption is imposed on\nthe coefficients. Namely, the drift coefficient and, in particular, the\nvolatility coefficient are not necessarily continuous in the measure variable\nfor the Wasserstein topology. In this paper, we provide an existence result and\nshow an approximation by $N$--particle system or propagation of chaos for this\ntype of McKean--Vlasov equations. As a direct result, we are able to deduce the\nexistence of a calibrated local stochastic volatility model for an appropriate\nchoice of stochastic volatility parameters. The associated propagation of chaos\nresult is also proved.\n"
    },
    {
        "paper_id": 2208.10183,
        "authors": "Claudio Fontana and Francesco Rotondi",
        "title": "Valuation of general GMWB annuities in a low interest rate environment",
        "comments": "Revised version (39 pages, 10 figures), including supplementary\n  material",
        "journal-ref": "Insurance: Mathematics and Economics, 2023, 112: 142-167",
        "doi": "10.1016/j.insmatheco.2023.07.003",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Variable annuities with Guaranteed Minimum Withdrawal Benefits (GMWB) entitle\nthe policy holder to periodic withdrawals together with a terminal payoff\nlinked to the performance of an equity fund. In this paper, we consider the\nvaluation of a general class of GMWB annuities, allowing for step-up, bonus and\nsurrender features, taking also into account mortality risk and death benefits.\nWhen dynamic withdrawals are allowed, the valuation of GMWB annuities leads to\na stochastic optimal control problem, which we address here by dynamic\nprogramming techniques. Adopting a Hull-White interest rate model, correlated\nwith the equity fund, we propose an efficient tree-based algorithm. We perform\na thorough analysis of the determinants of the market value of GMWB annuities\nand of the optimal withdrawal strategies. In particular, we study the impact of\na low/negative interest rate environment. Our findings indicate that\nlow/negative rates profoundly affect the optimal withdrawal behaviour and, in\ncombination with step-up and bonus features, increase significantly the fair\nvalues of GMWB annuities, which can only be compensated by large management\nfees.\n"
    },
    {
        "paper_id": 2208.10319,
        "authors": "Karl Friedrich Siburg, Christopher Strothmann, Gregor Wei{\\ss}",
        "title": "Comparing and quantifying tail dependence",
        "comments": "9 pages, 7 figures, 2 tables",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  We introduce a new stochastic order for the tail dependence between random\nvariables. We then study different measures of tail dependence which are\nmonotone in the proposed order, thereby extending various known tail dependence\ncoefficients from the literature. We apply our concepts in an empirical study\nwhere we investigate the tail dependence for different pairs of S&P 500 stocks\nand indices, and illustrate the advantage of our measures of tail dependence\nover the classical tail dependence coefficient.\n"
    },
    {
        "paper_id": 2208.10434,
        "authors": "Matthew Dicks, Andrew Paskaramoorthy, Tim Gebbie",
        "title": "A simple learning agent interacting with an agent-based market model",
        "comments": "18 pages, 7 figures. Accepted: Physica A",
        "journal-ref": "Physica A: Statistical Mechanics and its Applications 633 (2024)\n  129363",
        "doi": "10.1016/j.physa.2023.129363",
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  We consider the learning dynamics of a single reinforcement learning optimal\nexecution trading agent when it interacts with an event driven agent-based\nfinancial market model. Trading takes place asynchronously through a matching\nengine in event time. The optimal execution agent is considered at different\nlevels of initial order-sizes and differently sized state spaces. The resulting\nimpact on the agent-based model and market are considered using a calibration\napproach that explores changes in the empirical stylised facts and price impact\ncurves. Convergence, volume trajectory and action trace plots are used to\nvisualise the learning dynamics. Here the smaller state space agents had the\nnumber of states they visited converge much faster than the larger state space\nagents, and they were able to start learning to trade intuitively using the\nspread and volume states. We find that the moments of the model are robust to\nthe impact of the learning agents except for the Hurst exponent, which was\nlowered by the introduction of strategic order-splitting. The introduction of\nthe learning agent preserves the shape of the price impact curves but can\nreduce the trade-sign auto-correlations when their trading volumes increase.\n"
    },
    {
        "paper_id": 2208.10435,
        "authors": "Karoline Bax",
        "title": "Do diverse and inclusive workplaces benefit investors? An Empirical\n  Analysis on Europe and the United States",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  As the COVID-19 pandemic restrictions slow down, employees start to return to\ntheir offices. Hence, the discussions on optimal workplaces and issues of\ndiversity and inclusion have peaked. Previous research has shown that employees\nand companies benefit from positive workplace changes. This research questions\nwhether allowing for diversity and inclusion criteria in portfolio construction\nis beneficial to investors. By considering the new Diversity & Inclusion (D&I)\nscore by Refinitiv, I find evidence that investors might suffer lower returns\nand pay for investing in responsible (i.e., more diverse and inclusive)\nemployers in both the US and European market.\n"
    },
    {
        "paper_id": 2208.10707,
        "authors": "Boyi Jin",
        "title": "An intelligent algorithmic trading based on a risk-return reinforcement\n  learning algorithm",
        "comments": "17 page ,9 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This scientific paper propose a novel portfolio optimization model using an\nimproved deep reinforcement learning algorithm. The objective function of the\noptimization model is the weighted sum of the expectation and value at\nrisk(VaR) of portfolio cumulative return. The proposed algorithm is based on\nactor-critic architecture, in which the main task of critical network is to\nlearn the distribution of portfolio cumulative return using quantile\nregression, and actor network outputs the optimal portfolio weight by\nmaximizing the objective function mentioned above. Meanwhile, we exploit a\nlinear transformation function to realize asset short selling. Finally, A\nmulti-process method is used, called Ape-x, to accelerate the speed of deep\nreinforcement learning training. To validate our proposed approach, we conduct\nbacktesting for two representative portfolios and observe that the proposed\nmodel in this work is superior to the benchmark strategies.\n"
    },
    {
        "paper_id": 2208.10735,
        "authors": "Zhou Yang, Jing Zhang, Chao Zhou",
        "title": "Robust control problems of BSDEs coupled with value functions",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  A robust control problem is considered in this paper, where the controlled\nstochastic differential equations (SDEs) include ambiguity parameters and their\ncoefficients satisfy non-Lipschitz continuous and non-linear growth conditions,\nthe objective function is expressed as a backward stochastic differential\nequation (BSDE) with the generator depending on the value function. We\nestablish the existence and uniqueness of the value function in a proper space\nand provide a verification theorem. Moreover, we apply the results to solve two\ntypical optimal investment problems in the market with ambiguity, one of which\nis with Heston stochastic volatility model. In particular, we establish some\nsharp estimations for Heston model with ambiguity parameters.\n"
    },
    {
        "paper_id": 2208.11334,
        "authors": "Henri Arno, Klaas Mulier, Joke Baeck and Thomas Demeester",
        "title": "Next-Year Bankruptcy Prediction from Textual Data: Benchmark and\n  Baselines",
        "comments": "Presented at the 4th Workshop on Financial Technology and Natural\n  Language Processing (FinNLP) @ IJCAI-ECAI 2022 in Vienna, Austria",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Models for bankruptcy prediction are useful in several real-world scenarios,\nand multiple research contributions have been devoted to the task, based on\nstructured (numerical) as well as unstructured (textual) data. However, the\nlack of a common benchmark dataset and evaluation strategy impedes the\nobjective comparison between models. This paper introduces such a benchmark for\nthe unstructured data scenario, based on novel and established datasets, in\norder to stimulate further research into the task. We describe and evaluate\nseveral classical and neural baseline models, and discuss benefits and flaws of\ndifferent strategies. In particular, we find that a lightweight bag-of-words\nmodel based on static in-domain word representations obtains surprisingly good\nresults, especially when taking textual data from several years into account.\nThese results are critically assessed, and discussed in light of particular\naspects of the data and the task. All code to replicate the data and\nexperimental results will be released.\n"
    },
    {
        "paper_id": 2208.1138,
        "authors": "Samuel Palmer, Konstantinos Karagiannis, Adam Florence, Asier\n  Rodriguez, Roman Orus, Harish Naik, Samuel Mugel",
        "title": "Financial Index Tracking via Quantum Computing with Cardinality\n  Constraints",
        "comments": "8 pages, 8 figures, 4 tables",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this work, we demonstrate how to apply non-linear cardinality constraints,\nimportant for real-world asset management, to quantum portfolio optimization.\nThis enables us to tackle non-convex portfolio optimization problems using\nquantum annealing that would otherwise be challenging for classical algorithms.\nBeing able to use cardinality constraints for portfolio optimization opens the\ndoors to new applications for creating innovative portfolios and\nexchange-traded-funds (ETFs). We apply the methodology to the practical problem\nof enhanced index tracking and are able to construct smaller portfolios that\nsignificantly outperform the risk profile of the target index whilst retaining\nhigh degrees of tracking.\n"
    },
    {
        "paper_id": 2208.11577,
        "authors": "Yuan Liang, Quan Yuan, Daoge Wang, Yong Feng, Pengfei Xu, Jiangping\n  Zhou",
        "title": "Panacea or Placebo? Exploring Causal Effects of Nonlocal Vehicle Driving\n  Restriction Policies on Traffic Congestion Using Difference-in-differences\n  Approach",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Car dependence has been threatening transportation sustainability as it\ncontributes to congestion and associated externalities. In response, various\ntransport policies that restrict the use of private vehicle have been\nimplemented. However, empirical evaluations of such policies have been limited.\nTo assess these policies' benefits and costs, it is imperative to accurately\nevaluate how such policies affect traffic conditions. In this study, we compile\na refined spatio-temporal resolution data set of the floating-vehicle-based\ntraffic performance index to examine the effects of a recent nonlocal vehicle\ndriving restriction policy in Shanghai, one of most populous cities in the\nworld. Specifically, we explore whether and how the policy impacted traffic\nspeeds in the short term by employing a quasi-experimental\ndifference-in-differences modeling approach. We find that: (1) In the first\nmonth, the policy led to an increase of the network-level traffic speed by\n1.47% (0.352 km/h) during evening peak hours (17:00-19:00) but had no\nsignificant effects during morning peak hours (7:00-9:00). (2) The policy also\nhelped improve the network-level traffic speed in some unrestricted hours\n(6:00, 12:00, 14:00, and 20:00) although the impact was marginal. (3) The\nshort-term effects of the policy exhibited heterogeneity across traffic\nanalysis zones. The lower the metro station density, the greater the effects\nwere. We conclude that driving restrictions for non-local vehicles alone may\nnot significantly reduce congestion, and their effects can differ both\ntemporally and spatially. However, they can have potential side effects such as\nincreased purchase and usage of new energy vehicles, owners of which can obtain\na local license plate of Shanghai for free.\n"
    },
    {
        "paper_id": 2208.11606,
        "authors": "Michele Battisti and Giuseppe Maggio",
        "title": "Will the last be the first? School closures and educational outcomes",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Governments have implemented school closures and online learning as one of\nthe main tools to reduce the spread of Covid-19. Despite the potential benefits\nin terms of reduction of cases, the educational costs of these policies may be\ndramatic. This work identifies the educational costs, expressed as decrease in\ntest scores, for the whole universe of Italian students attending the 5th, 8th\nand 13th grade of the school cycle during the 2021/22 school year. The analysis\nrelies on a difference-in-difference model in relative time, where the control\ngroup is the closest generation before the Covid-19 pandemic. The results\nsuggest a national average loss between 1.6-4.1% and 0.5-2.4% in Mathematics\nand Italian test scores, respectively. After collecting the precise number of\ndays of school closures for the universe of students in Sicily, we estimate\nthat 30 additional days of closure decrease the test score by 1%. However, the\nimpact is much larger for students from high schools (1.8%) compared to\nstudents from low and middle schools (0.5%). This is likely explained by the\nlower relevance of parental inputs and higher reliance on peers inputs, within\nthe educational production function, for higher grades. Findings are also\nheterogeneous across class size and parental job conditions, pointing towards\npotential growing inequalities driven by the lack of in front teaching.\n"
    },
    {
        "paper_id": 2208.11976,
        "authors": "Xavier Brouty and Matthieu Garcin",
        "title": "A statistical test of market efficiency based on information theory",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-sa/4.0/",
        "abstract": "  We determine the amount of information contained in a time series of price\nreturns at a given time scale, by using a widespread tool of the information\ntheory, namely the Shannon entropy, applied to a symbolic representation of\nthis time series. By deriving the exact and the asymptotic distribution of this\nmarket information indicator in the case where the efficient market hypothesis\nholds, we develop a statistical test of market efficiency. We apply it to a\nreal dataset of stock indices, single stock, and cryptocurrency, for which we\nare able to determine at each date whether the efficient market hypothesis is\nto be rejected, with respect to a given confidence level.\n"
    },
    {
        "paper_id": 2208.12067,
        "authors": "Ben Duan, Yutian Li, Dawei Lu, Yang Lu, and Ran Zhang",
        "title": "Pricing Stocks with Trading Volumes",
        "comments": "major revision",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  The present paper proposes a new framework for describing the stock price\ndynamics. In the traditional geometric Brownian motion model and its variants,\nvolatility plays a vital role. The modern studies of asset pricing expand\naround volatility, trying to improve the understanding of it and remove the gap\nbetween the theory and market data. Unlike this, we propose to replace\nvolatility with trading volume in stock pricing models. This pricing strategy\nis based on two hypotheses: a price-volume relation with an idea borrowed from\nfluid flows and a white-noise hypothesis for the price rate of change (ROC)\nthat is verified via statistic testing on actual market data. The new framework\ncan be easily adopted to local volume and stochastic volume models for the\noption pricing problem, which will point out a new possible direction for this\ncentral problem in quantitative finance.\n"
    },
    {
        "paper_id": 2208.12321,
        "authors": "Ritika Sethi",
        "title": "Can Desegregation Close the Racial Gap in High School Coursework?",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper examines the interplay between desegregation, institutional bias,\nand individual behavior in education. Using a game-theoretic model that\nconsiders race-heterogeneous social incentives, the study investigates the\neffects of between-school desegregation on within-school disparities in\ncoursework. The analysis incorporates a segregation measure based on entropy\nand proposes an optimization-based approach to evaluate the impact of student\nreassignment policies. The results highlight that Black and Hispanic students\nin predominantly White schools, despite receiving less encouragement to apply\nto college, exhibit higher enrollment in college-prep coursework due to\nstronger social incentives from their classmates' coursework decisions.\n"
    },
    {
        "paper_id": 2208.12518,
        "authors": "Lech A. Grzelak",
        "title": "On Randomization of Affine Diffusion Processes with Application to\n  Pricing of Options on VIX and S&P 500",
        "comments": "7424 words, 24 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  The class of Affine (Jump) Diffusion (AD) has, due to its closed form\ncharacteristic function (ChF), gained tremendous popularity among practitioners\nand researchers. However, there is clear evidence that a linearity constraint\nis insufficient for precise and consistent option pricing. Any non-affine model\nmust pass the strict requirement of quick calibration -- which is often\nchallenging. We focus here on Randomized AD (RAnD) models, i.e., we allow for\nexogenous stochasticity of the model parameters. Randomization of a pricing\nmodel occurs outside the affine model and, therefore, forms a generalization\nthat relaxes the affinity constraints. The method is generic and can apply to\nany model parameter. It relies on the existence of moments of the so-called\nrandomizer- a random variable for the stochastic parameter. The RAnD model\nallows flexibility while benefiting from fast calibration and well-established,\nlarge-step Monte Carlo simulation, often available for AD processes. The\narticle will discuss theoretical and practical aspects of the RAnD method, like\nderivations of the corresponding ChF, simulation, and computations of\nsensitivities. We will also illustrate the advantages of the randomized\nstochastic volatility models in the consistent pricing of options on the S&P\n500 and VIX.\n"
    },
    {
        "paper_id": 2208.1253,
        "authors": "Sojung Kim, Marcel Kleiber, Stefan Weber",
        "title": "Microscopic Traffic Models, Accidents, and Insurance Losses",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  The paper develops a methodology to enable microscopic models of\ntransportation systems to be accessible for a statistical study of traffic\naccidents. Our approach is intended to permit an understanding not only of\nhistorical losses, but also of incidents that may occur in altered, potential\nfuture systems. Through such a counterfactual analysis, it is possible, from an\ninsurance, but also from an engineering perspective, to assess the impact of\nchanges in the design of vehicles and transport systems in terms of their\nimpact on road safety and functionality.\n  Structurally, we characterize the total loss distribution approximatively as\na mean-variance mixture. This also yields valuation procedures that can be used\ninstead of Monte Carlo simulation. Specifically, we construct an implementation\nbased on the open-source traffic simulator SUMO and illustrate the potential of\nthe approach in counterfactual case studies.\n"
    },
    {
        "paper_id": 2208.12541,
        "authors": "Nazrul Islam and Syed Khaled Rahman",
        "title": "Corporate Environmental Management Accounting Practicing and Reporting\n  in Bangladesh",
        "comments": "70 pages, SUST research project",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  In the management of environment the Environmental Management Accounting\n(EMA) is essential for corporate or companies because corporate sectors are the\nmain parties of environmental humiliation as they are existed in the\nenvironment and for protecting environment a branch of accounting is emerged\nwhich is called environmental management accounting. The objective of the study\nis to develop a compliance framework for EMA and appraise the ER practices in\nselected industries in Bangladesh. In conducting the study, 50 environmental\nsensitive industries were selected from DSE. A compliance checklist was\ndeveloped on 75 aspects of EMA and ER under 13 groups. In developing the\ncompliance index binary method is used i.e. 1= if ER practices; 0= if not\npractices. Further the level of EMR/ER practices have been evaluated in terms\nof selected independent variables of the company viz. total assets, total\nsales, return on equity and size of board. The study found that the\nenvironmental management accounting in the manufacturing companies is in poor\nlevel. The maximum compliance is 67% and the lowest is 20%. The TA, TS BS and\nSP have been considered to find out the explanatory variables. In most of the\ncases board size does not play significant role in the practice of EMA in the\nsampled firms.\n"
    },
    {
        "paper_id": 2208.12614,
        "authors": "Danial Saef, Yuanrong Wang, Tomaso Aste",
        "title": "Regime-based Implied Stochastic Volatility Model for Crypto Option\n  Pricing",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The increasing adoption of Digital Assets (DAs), such as Bitcoin (BTC), rises\nthe need for accurate option pricing models. Yet, existing methodologies fail\nto cope with the volatile nature of the emerging DAs. Many models have been\nproposed to address the unorthodox market dynamics and frequent disruptions in\nthe microstructure caused by the non-stationarity, and peculiar statistics, in\nDA markets. However, they are either prone to the curse of dimensionality, as\nadditional complexity is required to employ traditional theories, or they\noverfit historical patterns that may never repeat. Instead, we leverage recent\nadvances in market regime (MR) clustering with the Implied Stochastic\nVolatility Model (ISVM). Time-regime clustering is a temporal clustering\nmethod, that clusters the historic evolution of a market into different\nvolatility periods accounting for non-stationarity. ISVM can incorporate\ninvestor expectations in each of the sentiment-driven periods by using implied\nvolatility (IV) data. In this paper, we applied this integrated time-regime\nclustering and ISVM method (termed MR-ISVM) to high-frequency data on BTC\noptions at the popular trading platform Deribit. We demonstrate that MR-ISVM\ncontributes to overcome the burden of complex adaption to jumps in higher order\ncharacteristics of option pricing models. This allows us to price the market\nbased on the expectations of its participants in an adaptive fashion.\n"
    },
    {
        "paper_id": 2208.12838,
        "authors": "Zhiyi Shen",
        "title": "Out-of-Model Adjustments of Variable Annuities",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper studies the model risk of the Black-Scholes (BS) model in pricing\nand risk-managing variable annuities motivated by its wide usage in the\ninsurance industry. Specifically, we derive a model-free decomposition of the\nno-arbitrage price of the variable annuity into the BS model price in\nconjunction with three out-of-model adjustment terms. This sheds light on all\nrisk drivers behind the product, that is, spot price, realized volatility,\nfuture smile, and sub-optimal withdrawal. We further investigate the efficacy\nof the BS-based hedging strategy given the market diverges from the model\nassumptions. We disclose that the spot price risk can always be eliminated by\nthe strategy and the hedger's cumulative P\\&L exhibits gradual slippage and\ninstantaneous leakage. We finally show that the pricing, risk and hedging\nmodels can be separated from each other in managing the risks of variable\nannuities.\n"
    },
    {
        "paper_id": 2208.13254,
        "authors": "Martin Jaraiz",
        "title": "An agent-based modeling approach for real-world economic systems:\n  Example and calibration with a Social Accounting Matrix of Spain",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  The global economy is one of today's major challenges, with increasing\nrelevance in recent decades. A frequent observation by policy makers is the\nlack of tools that help at least to understand, if not predict, economic\ncrises. Currently, macroeconomic modeling is dominated by Dynamic Stochastic\nGeneral Equilibrium (DSGE) models. The limitations of DSGE in coping with the\ncomplexity of today's global economy are often recognized and are the subject\nof intense research to find possible solutions. As an alternative or complement\nto DSGE, the last two decades have seen the rise of agent-based models (ABM).\nAn attractive feature of ABM is that it can model very complex systems because\nit is a bottom-up approach that can describe the specific behavior of\nheterogeneous agents. The main obstacle, however, is the large number of\nparameters that need to be known or calibrated. To enable the use of ABM with\ndata from the real-world economy, this paper describes an agent-based\nmacroeconomic modeling approach that can read a Social Accounting Matrix (SAM)\nand deploy from scratch an economic system (labor, activity sectors operating\nas firms, a central bank, the government, external sectors...) whose structure\nand activity produce a SAM with values very close to those of the actual SAM\nsnapshot. This approach paves the way for unleashing the expected high\nperformance of ABM models to deal with the complexities of current global\nmacroeconomics, including other layers of interest like ecology, epidemiology,\nor social networks among others.\n"
    },
    {
        "paper_id": 2208.13336,
        "authors": "Guangyan Jia and Mengjin Zhao",
        "title": "On the Correspondence and the Risk Contribution for Conditional Coherent\n  and Deviation Risk Measures",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We give an axiomatic framework for conditional generalized deviation\nmeasures. Under financially reasonable assumptions, we give the correspondence\nbetween conditional coherent risk measures and generalized deviation measures.\nMoreover, we establish the notion of continuous-time risk contribution for\nconditional coherent risk measures and generalized deviation measures. With the\nhelp of the correspondence between these two different types of risk measures,\nwe give a microscopic interpretation of their risk contributions. Particularly,\nwe show that the risk contributions of time-consistent risk measures are still\ntime-consistent. We also demonstrate that the second element of the BSDE\nsolution $(Y, Z)$ associated with $g$-expectation has the meaning of risk\ncontribution.\n"
    },
    {
        "paper_id": 2208.13564,
        "authors": "Om Mane and Saravanakumar kandasamy",
        "title": "Stock Market Prediction using Natural Language Processing -- A Survey",
        "comments": "Conference Paper. arXiv admin note: text overlap with\n  arXiv:2004.01878, arXiv:2106.02522 by other authors",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  The stock market is a network which provides a platform for almost all major\neconomic transactions. While investing in the stock market is a good idea,\ninvesting in individual stocks may not be, especially for the casual investor.\nSmart stock-picking requires in-depth research and plenty of dedication.\nPredicting this stock value offers enormous arbitrage profit opportunities.\nThis attractiveness of finding a solution has prompted researchers to find a\nway past problems like volatility, seasonality, and dependence on time. This\npaper surveys recent literature in the domain of natural language processing\nand machine learning techniques used to predict stock market movements. The\nmain contributions of this paper include the sophisticated categorizations of\nmany recent articles and the illustration of the recent trends of research in\nstock market prediction and its related areas.\n"
    },
    {
        "paper_id": 2208.13654,
        "authors": "Kang Gao, Perukrishnen Vytelingum, Stephen Weston, Wayne Luk, Ce Guo",
        "title": "High-frequency financial market simulation and flash crash scenarios\n  analysis: an agent-based modelling approach",
        "comments": null,
        "journal-ref": "Journal of Artificial Societies and Social Simulation 2024 27 (2)\n  8 <http://jasss.soc.surrey.ac.uk/27/2/8.html>",
        "doi": "10.18564/jasss.5403",
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  This paper describes simulations and analysis of flash crash scenarios in an\nagent-based modelling framework. We design, implement, and assess a novel\nhigh-frequency agent-based financial market simulator that generates realistic\nmillisecond-level financial price time series for the E-Mini S&P 500 futures\nmarket. Specifically, a microstructure model of a single security traded on a\ncentral limit order book is provided, where different types of traders follow\ndifferent behavioural rules. The model is calibrated using the machine learning\nsurrogate modelling approach. Statistical test and moment coverage ratio\nresults show that the model has excellent capability of reproducing realistic\nstylised facts in financial markets. By introducing an institutional trader\nthat mimics the real-world Sell Algorithm on May 6th, 2010, the proposed\nhigh-frequency agent-based financial market simulator is used to simulate the\nFlash Crash that took place that day. We scrutinise the market dynamics during\nthe simulated flash crash and show that the simulated dynamics are consistent\nwith what happened in historical flash crash scenarios. With the help of Monte\nCarlo simulations, we discover functional relationships between the amplitude\nof the simulated 2010 Flash Crash and three conditions: the percentage of\nvolume of the Sell Algorithm, the market maker inventory limit, and the trading\nfrequency of fundamental traders. Similar analyses are carried out for mini\nflash crash events. An innovative \"Spiking Trader\" is introduced to the model,\naiming at precipitating mini flash crash events. We analyse the market dynamics\nduring the course of a typical simulated mini flash crash event and study the\nconditions affecting its characteristics. The proposed model can be used for\ntesting resiliency and robustness of trading algorithms and providing advice\nfor policymakers.\n"
    },
    {
        "paper_id": 2208.1394,
        "authors": "Keshav Agrawal, Susan Athey, Ayush Kanodia, Emil Palikot",
        "title": "Personalized Recommendations in EdTech: Evidence from a Randomized\n  Controlled Trial",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We study the impact of personalized content recommendations on the usage of\nan educational app for children. In a randomized controlled trial, we show that\nthe introduction of personalized recommendations increases the consumption of\ncontent in the personalized section of the app by approximately 60%. We further\nshow that the overall app usage increases by 14%, compared to the baseline\nsystem where human content editors select stories for all students at a given\ngrade level. The magnitude of individual gains from personalized content\nincreases with the amount of data available about a student and with\npreferences for niche content: heavy users with long histories of content\ninteractions who prefer niche content benefit more than infrequent, newer users\nwho like popular content. To facilitate the move to personalized recommendation\nsystems from a simpler system, we describe how we make important design\ndecisions, such as comparing alternative models using offline metrics and\nchoosing the right target audience.\n"
    },
    {
        "paper_id": 2208.14038,
        "authors": "S\\'andor Kuns\\'agi-M\\'at\\'e, G\\'abor F\\'ath, Istv\\'an Csabai, G\\'abor\n  Moln\\'ar-S\\'aska",
        "title": "Deep Weighted Monte Carlo: A hybrid option pricing framework using\n  neural networks",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Recent studies have demonstrated the efficiency of Variational Autoencoders\n(VAE) to compress high-dimensional implied volatility surfaces into a low\ndimensional representation. Although this method can be effectively used for\npricing vanilla options, it does not provide any explicit information about the\ndynamics of the underlying asset. In our work we present an effective way to\novercome this problem. We use a Weighted Monte Carlo approach to first generate\npaths from a simple a priori Brownian dynamics, and then calculate path weights\nto price options correctly. We develop and successfully train a neural network\nthat is able to assign these weights directly from the latent space. Combining\nthe encoder network of the VAE and this new \"weight assigner\" module, we are\nable to build a dynamic pricing framework which cleanses the volatility surface\nfrom irrelevant noise fluctuations, and then can price not just vanillas, but\nalso exotic options on this idealized vol surface. This pricing method can\nprovide relative value signals for option traders.\n"
    },
    {
        "paper_id": 2208.14106,
        "authors": "Tobias Wand, Martin He{\\ss}ler and Oliver Kamps",
        "title": "Identifying Dominant Industrial Sectors in Market States of the S&P 500\n  Financial Data",
        "comments": "18 pages and additional appendix",
        "journal-ref": null,
        "doi": "10.1088/1742-5468/accce0",
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Understanding and forecasting changing market conditions in complex economic\nsystems like the financial market is of great importance to various\nstakeholders such as financial institutions and regulatory agencies. Based on\nthe finding that the dynamics of sector correlation matrices of the S&P 500\nstock market can be described by a sequence of distinct states via a clustering\nalgorithm, we try to identify the industrial sectors dominating the correlation\nstructure of each state. For this purpose, we use a method from Explainable\nArtificial Intelligence (XAI) on daily S&P 500 stock market data from 1992 to\n2012 to assign relevance scores to every feature of each data point. To compare\nthe significance of the features for the entire data set we develop an\naggregation procedure and apply a Bayesian change point analysis to identify\nthe most significant sector correlations. We show that the correlation matrix\nof each state is dominated only by a few sector correlations. Especially the\nenergy and IT sector are identified as key factors in determining the state of\nthe economy. Additionally we show that a reduced surrogate model, using only\nthe eight sector correlations with the highest XAI-relevance, can replicate 90%\nof the cluster assignments. In general our findings imply an additional\ndimension reduction of the dynamics of the financial market.\n"
    },
    {
        "paper_id": 2208.14152,
        "authors": "Marcos Escobar-Anel, Yevhen Havrylenko, Rudi Zagst",
        "title": "Value-at-Risk constrained portfolios in incomplete markets: a dynamic\n  programming approach to Heston's model",
        "comments": "40 pages, 8 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  We solve an expected utility-maximization problem with a Value-at-risk\nconstraint on the terminal portfolio value in an incomplete financial market\ndue to stochastic volatility. To derive the optimal investment strategy, we use\nthe dynamic programming approach. We demonstrate that the value function in the\nconstrained problem can be represented as the expected modified utility\nfunction of a vega-neutral financial derivative on the optimal terminal wealth\nin the unconstrained utility-maximization problem. Via the same financial\nderivative, the optimal wealth and the optimal investment strategy in the\nconstrained problem are linked to the optimal wealth and the optimal investment\nstrategy in the unconstrained problem. In numerical studies, we substantiate\nthe impact of risk aversion levels and investment horizons on the optimal\ninvestment strategy. We observe a 20% relative difference between the\nconstrained and unconstrained allocations for average parameters in a\nlow-risk-aversion short-horizon setting.\n"
    },
    {
        "paper_id": 2208.14164,
        "authors": "Ioan Alexandru Puiu and Raphael Andreas Hauser",
        "title": "A fundamental Game Theoretic model and approximate global Nash\n  Equilibria computation for European Spot Power Markets",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Spot electricity markets are considered under a Game-Theoretic framework,\nwhere risk averse players submit orders to the market clearing mechanism to\nmaximise their own utility. Consistent with the current practice in Europe, the\nmarket clearing mechanism is modelled as a Social Welfare Maximisation problem,\nwith zonal pricing, and we consider inflexible demand, physical constraints of\nthe electricity grid, and capacity-constrained producers. A novel type of\nnon-parametric risk aversion based on a defined worst case scenario is\nintroduced, and this reduces the dimensionality of the strategy variables and\nensures boundedness of prices. By leveraging these properties we devise Jacobi\nand Gauss-Seidel iterative schemes for computation of approximate global Nash\nEquilibria, which are in contrast to derivative based local equilibria. Our\nmethodology is applied to the real world data of Central Western European (CWE)\nSpot Market during the 2019-2020 period, and offers a good representation of\nthe historical time series of prices. By also solving for the assumption of\ntruthful bidding, we devise a simple method based on hypothesis testing to\ninfer if and when producers are bidding strategically (instead of truthfully),\nand we find evidence suggesting that strategic bidding may be fairly pronounced\nin the CWE region.\n"
    },
    {
        "paper_id": 2208.14207,
        "authors": "Kang Gao, Perukrishnen Vytelingum, Stephen Weston, Wayne Luk, Ce Guo",
        "title": "Understanding intra-day price formation process by agent-based financial\n  market simulation: calibrating the extended chiarella model",
        "comments": "Published in WILMOTT Magazine: May 2022 issue. arXiv admin note: text\n  overlap with arXiv:2208.13654",
        "journal-ref": "Understanding intra-day price formation process by agent-based\n  financial market simulation: calibrating the extended chiarella model,\n  Wilmott, vol. 2022, iss. 119, p. 22-38, 2022",
        "doi": "10.1002/wilm.11014",
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  This article presents XGB-Chiarella, a powerful new approach for deploying\nagent-based models to generate realistic intra-day artificial financial price\ndata. This approach is based on agent-based models, calibrated by XGBoost\nmachine learning surrogate. Following the Extended Chiarella model, three types\nof trading agents are introduced in this agent-based model: fundamental\ntraders, momentum traders, and noise traders. In particular, XGB-Chiarella\nfocuses on configuring the simulation to accurately reflect real market\nbehaviours. Instead of using the original Expectation-Maximisation algorithm\nfor parameter estimation, the agent-based Extended Chiarella model is\ncalibrated using XGBoost machine learning surrogate. It is shown that the\nmachine learning surrogate learned in the proposed method is an accurate proxy\nof the true agent-based market simulation. The proposed calibration method is\nsuperior to the original Expectation-Maximisation parameter estimation in terms\nof the distance between historical and simulated stylised facts. With the same\nunderlying model, the proposed methodology is capable of generating realistic\nprice time series in various stocks listed at three different exchanges, which\nindicates the universality of intra-day price formation process. For the time\nscale (minutes) chosen in this paper, one agent per category is shown to be\nsufficient to capture the intra-day price formation process. The proposed\nXGB-Chiarella approach provides insights that the price formation process is\ncomprised of the interactions between momentum traders, fundamental traders,\nand noise traders. It can also be used to enhance risk management by\npractitioners.\n"
    },
    {
        "paper_id": 2208.14248,
        "authors": "Jos\\'e-Ignacio Ant\\'on (University of Salamanca), Rudolf Winter-Ebmer\n  (Johannes Kepler University Linz), Enrique Fern\\'andez-Mac\\'ias (Joint\n  Research Centre)",
        "title": "Does robotization affect job quality? Evidence from European regional\n  labour markets",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Whereas there are recent papers on the effect of robot adoption on employment\nand wages, there is no evidence on how robots affect non-monetary working\nconditions. We explore the impact of robot adoption on several domains of\nnon-monetary working conditions in Europe over the period 1995-2005 combining\ninformation from the World Robotics Survey and the European Working Conditions\nSurvey. In order to deal with the possible endogeneity of robot deployment, we\nemploy an instrumental variables strategy, using the robot exposure by sector\nin other developed countries as an instrument. Our results indicate that\nrobotization has a negative impact on the quality of work in the dimension of\nwork intensity and no relevant impact on the domains of physical environment or\nskills and discretion.\n"
    },
    {
        "paper_id": 2208.14254,
        "authors": "Emanuel Kohlscheen",
        "title": "Quantifying the Role of Interest Rates, the Dollar and Covid in Oil\n  Prices",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  This study analyses oil price movements through the lens of an agnostic\nrandom forest model, which is based on 1,000 regression trees. It shows that\nthis highly disciplined, yet flexible computational model reduces in sample\nroot mean square errors by 65% relative to a standard linear least square model\nthat uses the same set of 11 explanatory factors. In forecasting exercises the\nRMSE reduction ranges between 51% and 68%, highlighting the relevance of non\nlinearities in oil markets. The results underscore the importance of\nincorporating financial factors into oil models: US interest rates, the dollar\nand the VIX together account for 39% of the models RMSE reduction in the post\n2010 sample, rising to 48% in the post 2020 sample. If Covid 19 is also\nconsidered as a risk factor, these shares become even larger.\n"
    },
    {
        "paper_id": 2208.14267,
        "authors": "Jozef Barunik and Matej Nevrla",
        "title": "Common Idiosyncratic Quantile Risk",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  We identify a new type of risk that is characterised by commonalities in the\nquantiles of the cross-sectional distribution of asset returns. Our newly\nproposed quantile risk factor is associated with a quantile-specific risk\npremium and provides new insights into how upside and downside risks are priced\nby investors. In contrast to the previous literature, we recover the common\nstructure in cross-sectional quantiles without making confounding assumptions\nor aggregating potentially non-linear information. We discuss how the new\nquantile-based risk factor differs from popular volatility and downside risk\nfactors, and we identify where the quantile-dependent risks deserve greater\ncompensation. Quantile factors also have predictive power for aggregate market\nreturns.\n"
    },
    {
        "paper_id": 2208.14311,
        "authors": "Jonathan Berrisch, Sven Pappert, Florian Ziel, Antonia Arsova",
        "title": "Modeling Volatility and Dependence of European Carbon and Energy Prices",
        "comments": "Accepted for publication in Finance Research Letters",
        "journal-ref": null,
        "doi": "10.1016/j.frl.2022.103503",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We study the prices of European Emission Allowances (EUA), whereby we analyze\ntheir uncertainty and dependencies on related energy prices (natural gas, coal,\nand oil). We propose a probabilistic multivariate conditional time series model\nwith a VECM-Copula-GARCH structure which exploits key characteristics of the\ndata. Data are normalized with respect to inflation and carbon emissions to\nallow for proper cross-series evaluation. The forecasting performance is\nevaluated in an extensive rolling-window forecasting study, covering eight\nyears out-of-sample. We discuss our findings for both levels- and\nlog-transformed data, focusing on time-varying correlations, and in view of the\nRussian invasion of Ukraine.\n"
    },
    {
        "paper_id": 2208.14385,
        "authors": "Zheng Cao, Wenyu Du and Kirill V. Golubnichiy",
        "title": "Application of Convolutional Neural Networks with Quasi-Reversibility\n  Method Results for Option Forecasting",
        "comments": "10 pages, 2 figures, 5 tables",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  This paper presents a novel way to apply mathematical finance and machine\nlearning (ML) to forecast stock options prices. Following results from the\npaper Quasi-Reversibility Method and Neural Network Machine Learning to\nSolution of Black-Scholes Equations (appeared on the AMS Contemporary\nMathematics journal), we create and evaluate new empirical mathematical models\nfor the Black-Scholes equation to analyze data for 92,846 companies. We solve\nthe Black-Scholes (BS) equation forwards in time as an ill-posed inverse\nproblem, using the Quasi-Reversibility Method (QRM), to predict option price\nfor the future one day. For each company, we have 13 elements including stock\nand option daily prices, volatility, minimizer, etc. Because the market is so\ncomplicated that there exists no perfect model, we apply ML to train algorithms\nto make the best prediction. The current stage of research combines QRM with\nConvolutional Neural Networks (CNN), which learn information across a large\nnumber of data points simultaneously. We implement CNN to generate new results\nby validating and testing on sample market data. We test different ways of\napplying CNN and compare our CNN models with previous models to see if\nachieving a higher profit rate is possible.\n"
    },
    {
        "paper_id": 2208.1465,
        "authors": "Emanuel Kohlscheen and Richhild Moessner",
        "title": "Changing Electricity Markets: Quantifying the Price Effects of Greening\n  the Energy Matrix",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  We analyse the drivers of European Power Exchange (EPEX) wholesale\nelectricity prices between 2012 and early 2022 using machine learning. The\nagnostic random forest approach that we use is able to reduce in-sample root\nmean square errors (RMSEs) by around 50% when compared to a standard linear\nleast square model. This indicates that non-linearities and interaction effects\nare key in wholesale electricity markets. Out-of-sample prediction errors using\nmachine learning are (slightly) lower than even in-sample least square errors\nusing a least square model. The effects of efforts to limit power consumption\nand green the energy matrix on wholesale electricity prices are first order.\nCO2 permit prices strongly impact electricity prices, as do the prices of\nsource energy commodities. And carbon permit prices impact has clearly\nincreased post-2021 (particularly for baseload prices). Among energy sources,\nnatural gas has the largest effect on electricity prices. Importantly, the role\nof wind energy feed-in has slowly risen over time, and its impact is now\nroughly on par with that of coal.\n"
    },
    {
        "paper_id": 2208.14651,
        "authors": "Emanuel Kohlscheen and Richhild Moessner",
        "title": "Globalisation and the Decoupling of Inflation from Domestic Labour Costs",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  We provide novel systematic cross-country evidence that the link between\ndomestic labour markets and CPI inflation has weakened considerably in advanced\neconomies during recent decades. The central estimate is that the short-run\npass-through from domestic labour cost changes to core CPI inflation decreased\nfrom 0.25 in the 1980s to just 0.02 in the 2010s, while the long-run\npass-through fell from 0.36 to 0.03. We show that the timing of the collapse in\nthe pass-through coincides with a steep increase in import penetration from a\ngroup of 10 major manufacturing EMEs around the turn of the millennium. This\nsignals increased competition and market contestability. Besides the extent of\ntrade openness, we show that the intensity of the pass-through also depends in\na non-linear way on the average level of inflation.\n"
    },
    {
        "paper_id": 2208.14653,
        "authors": "Emanuel Kohlscheen",
        "title": "What does machine learning say about the drivers of inflation?",
        "comments": "BIS Working Paper 980 (2021)",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  This paper examines the drivers of CPI inflation through the lens of a\nsimple, but computationally intensive machine learning technique. More\nspecifically, it predicts inflation across 20 advanced countries between 2000\nand 2021, relying on 1,000 regression trees that are constructed based on six\nkey macroeconomic variables. This agnostic, purely data driven method delivers\n(relatively) good outcome prediction performance. Out of sample root mean\nsquare errors (RMSE) systematically beat even the in-sample benchmark\neconometric models. Partial effects of inflation expectations on CPI outcomes\nare also elicited in the paper. Overall, the results highlight the role of\nexpectations for inflation outcomes in advanced economies, even though their\nimportance appears to have declined somewhat during the last 10 years.\n"
    },
    {
        "paper_id": 2208.14809,
        "authors": "Marcelo Brutti Righi, Fernanda Maria M\\\"uller, Marlon Ruoso Moresco",
        "title": "A risk measurement approach from risk-averse stochastic optimization of\n  score functions",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We propose a risk measurement approach for a risk-averse stochastic problem.\nWe provide results that guarantee that our problem has a solution. We\ncharacterize and explore the properties of the argmin as a risk measure and the\nminimum as a deviation measure. We provide a connection between linear\nregression models and our framework. Based on this conception, we consider\nconditional risk and provide a connection between the minimum deviation\nportfolio and linear regression. Moreover, we also link the optimal replication\nhedging to our framework.\n"
    },
    {
        "paper_id": 2208.14902,
        "authors": "Allister Loder, Fabienne Cantner, Andrea Cadavid, Markus B. Siewert,\n  Stefan Wurster, Sebastian Goerg, Klaus Bogenberger",
        "title": "A nation-wide experiment: fuel tax cuts and almost free public transport\n  for three months in Germany -- Report 3 Second wave results",
        "comments": "arXiv admin note: text overlap with arXiv:2206.10510",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
        "abstract": "  In spring 2022, the German federal government agreed on a set of measures\nthat aimed at reducing households' financial burden resulting from a recent\nprice increase, especially in energy and mobility. These measures included\namong others, a nation-wide public transport ticket for 9\\ EUR per month and a\nfuel tax cut that reduced fuel prices by more than 15\\,\\%. In transportation\nresearch this is an almost unprecedented behavioral experiment. It allows to\nstudy not only behavioral responses in mode choice and induced demand but also\nto assess the effectiveness of transport policy instruments. We observe this\nnatural experiment with a three-wave survey and an app-based travel diary on a\nsample of hundreds of participants as well as an analysis of traffic counts. In\nthis third report, we provide first findings from the second survey, conducted\nduring the experiment.\n"
    },
    {
        "paper_id": 2208.14972,
        "authors": "Soumitra Shukla",
        "title": "Making the Elite: Top Jobs, Disparities, and Solutions",
        "comments": "Improved exposition for additional clarity",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  How do socioeconomically unequal screening practices impact access to elite\nfirms and what policies might reduce inequality? Using personnel data from\nelite U.S. and European multinational corporations recruiting from an elite\nIndian college, I show that caste disparities in hiring do not arise in many\njob search stages, including: applications, application reading, written\naptitude tests, large group debates that assess socio-emotional skills, and job\nchoices. Rather, disparities arise in the final round, comprising non-technical\npersonal interviews that screen on family background, neighborhood, and\n\"cultural fit.\" These characteristics are plausibly weakly correlated with\nproductivity (at the interview round) but strongly correlated with caste.\nEmployer willingness to pay for an advantaged caste is as large as that for a\nfull standard deviation increase in college GPA. A hiring subsidy that\neliminates the caste penalty would be more cost-effective in diversifying elite\nhiring than equalizing the caste distribution of pre-college test scores or\nenforcing hiring quotas.\n"
    },
    {
        "paper_id": 2209.00057,
        "authors": "Paul Minard",
        "title": "Molecular genetics and mid-career economic mobility",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  Reductions in the cost of genetic sequencing have enabled the construction of\nlarge datasets including both genetic and phenotypic data. Based on these\ndatasets, polygenic scores (PGSs) summarizing an individual's genetic\npropensity for educational attainment have been constructed. It is by now well\nestablished that this PGS predicts wages, income, and occupational prestige and\noccupational mobility across generations. It is unknown whether a PGS for\neducational attainment can predict upward income and occupational mobility even\nwithin the peak earning years of an individual. Using data from the Wisconsin\nLongitudinal Study (WLS), I show that: (i) a PGS for educational attainment\npredicts wage, income and occupational prestige mobility between 1974 (when\nrespondents were about 36 years of age) and 1992 (when respondents were about\n53 years of age), conditional on 1974 values of these variables and a range of\ncovariates; (ii) the effect is not mediated by parental socioeconomic status,\nis driven primarily by respondents with only a high school education, and is\nreplicated in a within sibling-pair design; (iii) conditional on 1974 outcomes,\nhigher PGS individuals surveyed in 1975 aspired to higher incomes and more\nprestigious jobs 10 years hence, an effect driven primarily by respondents with\nmore than a high school education; (iv) throughout their employment history,\nhigh PGS individuals were more likely to undertake on the job training, and\nmore likely to change job duties during tenure with an employer; and (v) though\nno more likely to change employers or industries during their careers, high PGS\nindividuals were more likely in 1974 to be working in industries which would\nexperience high wage growth in subsequent decades. These results contribute to\nour understanding of longitudinal inequality and shed light on the sources of\nheterogeneity in responses to economic shocks and policy.\n"
    },
    {
        "paper_id": 2209.00121,
        "authors": "Yang Bai",
        "title": "150 Years of Return Predictability Around the World: A Holistic View",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Using new annual data of 16 developed countries across bond, equity, and\nhousing markets, I study the return predictability using the payout-price\nratios, i.e., coupon price, dividend price, and rent price. None of the 48\ncountry-asset combinations shows consistent in-sample and out-of-sample\nperformance with positive utility gain for the mean-variance investor. Only 3\n(4/2) countries show positive economic gains in their equity (housing/bond)\nmarkets. The return predictability for the representative agents' risky asset\nportfolios and wealth portfolios is even weaker, suggesting that timing the\ninvestment return of a country using payout-price ratios will not make the\ninvestors better off. The predictive regressions based on the VAR analysis by\nCochrane (2008, 2011) suggest that 14 (5) countries have predictable payout\ngrowth in the equity (housing) markets, ex., the dividend price predicts the\ndividend growth in the US. The VAR simulation using data from all the countries\ndoes not reject the null that the dividend growth is predictable. This paper\npresents firm evidence against the return predictability based on payout\nratios.\n"
    },
    {
        "paper_id": 2209.00268,
        "authors": "Deborah Miori and Mihai Cucuringu",
        "title": "Returns-Driven Macro Regimes and Characteristic Lead-Lag Behaviour\n  between Asset Classes",
        "comments": "9 pages, 8 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  We define data-driven macroeconomic regimes by clustering the relative\nperformance in time of indices belonging to different asset classes. We then\ninvestigate lead-lag relationships within the regimes identified. Our study\nunravels market features characteristic of different windows in time and\nleverages on this knowledge to highlight market trends or risks that can be\ninformative with respect to recurrent market developments. The framework\ndeveloped also lays the foundations for multiple possible extensions.\n"
    },
    {
        "paper_id": 2209.00406,
        "authors": "Arianna Mingone",
        "title": "Smiles in delta",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Fukasawa introduced in [Fukasawa, Math Financ, 2012] two necessary conditions\nfor no butterfly arbitrage which require that the $d_1$ and $d_2$ functions of\nthe Black-Scholes formula have to be decreasing. In this article we\ncharacterize the set of smiles satisfying these conditions, using the\nparametrization of the smile in delta. We obtain a parametrization of the set\nvia one real number and three positive functions. We also show that such smiles\nand their symmetric smiles can be transformed into smiles in the strike space\nby a bijection. Our result motivates the study of the challenging question of\ncharacterizing the subset of butterfly arbitrage-free smiles.\n"
    },
    {
        "paper_id": 2209.00409,
        "authors": "Henrika Langen",
        "title": "The Impact of the #MeToo Movement on Language at Court -- A text-based\n  causal inference approach",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  This study assesses the effect of the #MeToo movement on the language used in\njudicial opinions on sexual violence related cases from 51 U.S. state and\nfederal appellate courts. The study introduces various indicators to quantify\nthe extent to which actors in courtrooms employ language that implicitly shifts\nresponsibility away from the perpetrator and onto the victim. One indicator\nmeasures how frequently the victim is mentioned as the grammatical subject, as\nresearch in the field of psychology suggests that victims are assigned more\nblame the more often they are referred to as the grammatical subject. The other\ntwo indices designed to gauge the level of victim-blaming capture the sentiment\nof and the context in sentences referencing the victim and/or perpetrator.\nAdditionally, judicial opinions are transformed into bag-of-words and tf-idf\nvectors to facilitate the examination of the evolution of language over time.\nThe causal effect of the #MeToo movement is estimated by means of a\nDifference-in-Differences approach comparing the development of the language in\nopinions on sexual offenses and other crimes against persons as well as a Panel\nEvent Study approach. The results do not clearly identify a\n#MeToo-movement-induced change in the language in court but suggest that the\nmovement may have accelerated the evolution of court language slightly, causing\nthe effect to materialize with a significant time lag. Additionally, the study\nconsiders potential effect heterogeneity with respect to the judge's gender and\npolitical affiliation. The study combines causal inference with text\nquantification methods that are commonly used for classification as well as\nwith indicators that rely on sentiment analysis, word embedding models and\ngrammatical tagging.\n"
    },
    {
        "paper_id": 2209.00534,
        "authors": "Marcel Preuss and Germ\\'an Reyes and Jason Somerville and Joy Wu",
        "title": "Inequality of Opportunity and Income Redistribution",
        "comments": "JEL codes: C91, D63",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Support for redistribution depends on whether inequality stems from\ndifferences in performance or luck, but different sources of luck may impact\nredistribution differentially. We elicit redistribution decisions from a\nU.S.-representative sample who observe worker earnings and whether luck\ninfluenced their earnings directly (\"lucky outcomes\") or indirectly by\nproviding a relative advantage (\"lucky opportunities\"). Participants\nredistribute less under lucky opportunities. When assessing the impact of\nunequal opportunities, individuals rely on a heuristic that leads them to\nunderestimate how even a small relative advantage can substantially influence\nworker earnings. Our findings highlight the role of inferential challenges in\nshaping attitudes towards inequality.\n"
    },
    {
        "paper_id": 2209.0054,
        "authors": "Oscar Calvo-Gonz\\'alez and Axel Eizmendi and Germ\\'an Reyes",
        "title": "The Shifting Attention of Political Leaders: Evidence from Two Centuries\n  of Presidential Speeches",
        "comments": "JEL codes: D78, I32, D72, N16",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  We use natural-language-processing algorithms on a novel dataset of over 900\npresidential speeches from ten Latin American countries spanning two centuries\nto study the dynamics and determinants of presidential policy priorities. We\nshow that most speech content can be characterized by a compact set of policy\nissues whose relative composition exhibited slow yet substantial shifts over\n1819-2022. Presidential attention initially centered on military interventions\nand the development of state capacity. Attention gradually evolved towards\nbuilding physical capital through investments in infrastructure and public\nservices and finally turned towards building human capital through investments\nin education, health, and social safety nets. We characterize the way in which\npresident-level characteristics, like age and gender, predict the main policy\nissues. Our findings offer novel insights into the dynamics of presidential\nattention and the factors that shape it, expanding our understanding of\npolitical agenda-setting.\n"
    },
    {
        "paper_id": 2209.0078,
        "authors": "Yoonsik Hong, Yanghoon Kim, Jeonghun Kim, Yongmin Choi",
        "title": "Index Tracking via Learning to Predict Market Sensitivities",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Index funds are substantially preferred by investors nowadays, and market\nsensitivities are instrumental in managing index funds. An index fund is a\nmutual fund aiming to track the returns of a predefined market index (e.g., the\nS&P 500). A basic strategy to manage an index fund is replicating the index's\nconstituents and weights identically, which is, however, cost-ineffective and\nimpractical. To address this issue, it is required to replicate the index\npartially with accurately predicted market sensitivities. Accordingly, we\npropose a novel partial-replication method via learning to predict market\nsensitivities. We first examine deep-learning models to predict market\nsensitivities in a supervised manner with our data-processing methods. Then, we\npropose a partial-index-tracking optimization model controlling the net\npredicted market sensitivities of the portfolios and index to be the same.\nThese processes' efficacy is corroborated by our experiments on the Korea Stock\nPrice Index 200. Our experiments show a significant reduction of the prediction\nerrors compared with historical estimations and competitive tracking errors of\nreplicating the index utilizing fewer than half of the entire constituents.\nTherefore, we show that applying deep learning to predict market sensitivities\nis promising and that our portfolio construction methods are practically\neffective. Additionally, to our knowledge, this is the first study addressing\nmarket sensitivities focused on deep learning.\n"
    },
    {
        "paper_id": 2209.00821,
        "authors": "Devang Sinha and Siddhartha P. Chakrabarty",
        "title": "Multilevel Richardson-Romberg and Importance Sampling in Derivative\n  Pricing",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  In this paper, we propose and analyze a novel combination of multilevel\nRichardson-Romberg (ML2R) and importance sampling algorithm, with the aim of\nreducing the overall computational time, while achieving desired\nroot-mean-squared error while pricing. We develop an idea to construct the\nMonte-Carlo estimator that deals with the parametric change of measure. We rely\non the Robbins-Monro algorithm with projection, in order to approximate optimal\nchange of measure parameter, for various levels of resolution in our multilevel\nalgorithm. Furthermore, we propose incorporating discretization schemes with\nhigher-order strong convergence, in order to simulate the underlying stochastic\ndifferential equations (SDEs) thereby achieving better accuracy. In order to do\nso, we study the Central Limit Theorem for the general multilevel algorithm.\nFurther, we study the asymptotic behavior of our estimator, thereby proving the\nStrong Law of Large Numbers. Finally, we present numerical results to\nsubstantiate the efficacy of our developed algorithm.\n"
    },
    {
        "paper_id": 2209.00858,
        "authors": "Mathias Lindholm, Ronald Richman, Andreas Tsanakas, Mario V.\n  W\\\"uthrich",
        "title": "A Discussion of Discrimination and Fairness in Insurance Pricing",
        "comments": "14 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Indirect discrimination is an issue of major concern in algorithmic models.\nThis is particularly the case in insurance pricing where protected policyholder\ncharacteristics are not allowed to be used for insurance pricing. Simply\ndisregarding protected policyholder information is not an appropriate solution\nbecause this still allows for the possibility of inferring the protected\ncharacteristics from the non-protected ones. This leads to so-called proxy or\nindirect discrimination. Though proxy discrimination is qualitatively different\nfrom the group fairness concepts in machine learning, these group fairness\nconcepts are proposed to 'smooth out' the impact of protected characteristics\nin the calculation of insurance prices. The purpose of this note is to share\nsome thoughts about group fairness concepts in the light of insurance pricing\nand to discuss their implications. We present a statistical model that is free\nof proxy discrimination, thus, unproblematic from an insurance pricing point of\nview. However, we find that the canonical price in this statistical model does\nnot satisfy any of the three most popular group fairness axioms. This seems\npuzzling and we welcome feedback on our example and on the usefulness of these\ngroup fairness axioms for non-discriminatory insurance pricing.\n"
    },
    {
        "paper_id": 2209.009,
        "authors": "Richard S.J. Tol",
        "title": "Costs and Benefits of the Paris Climate Targets",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The temperature targets in the Paris Agreement cannot be met without very\nrapid reduction of greenhouse gas emissions and removal of carbon dioxide from\nthe atmosphere. The latter requires large, perhaps prohibitively large\nsubsidies. The central estimate of the costs of climate policy, unrealistically\nassuming least-cost implementation, is 3.8-5.6\\% of GDP in 2100. The central\nestimate of the benefits of climate policy, unrealistically assuming constant\nvulnerability, is 2.8-3.2\\% of GDP. The uncertainty about the benefits is\nlarger than the uncertainty about the costs. The Paris targets do not pass the\ncost-benefit test unless risk aversion is high and discount rate low.\n"
    },
    {
        "paper_id": 2209.00948,
        "authors": "James T.E. Chapman and Ajit Desai",
        "title": "Macroeconomic Predictions using Payments Data and Machine Learning",
        "comments": null,
        "journal-ref": "Forecasting, 2023",
        "doi": "10.3390/forecast5040036",
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Predicting the economy's short-term dynamics -- a vital input to economic\nagents' decision-making process -- often uses lagged indicators in linear\nmodels. This is typically sufficient during normal times but could prove\ninadequate during crisis periods. This paper aims to demonstrate that\nnon-traditional and timely data such as retail and wholesale payments, with the\naid of nonlinear machine learning approaches, can provide policymakers with\nsophisticated models to accurately estimate key macroeconomic indicators in\nnear real-time. Moreover, we provide a set of econometric tools to mitigate\noverfitting and interpretability challenges in machine learning models to\nimprove their effectiveness for policy use. Our models with payments data,\nnonlinear methods, and tailored cross-validation approaches help improve\nmacroeconomic nowcasting accuracy up to 40\\% -- with higher gains during the\nCOVID-19 period. We observe that the contribution of payments data for economic\npredictions is small and linear during low and normal growth periods. However,\nthe payments data contribution is large, asymmetrical, and nonlinear during\nstrong negative or positive growth periods.\n"
    },
    {
        "paper_id": 2209.00991,
        "authors": "Qiuqi Wang, Ruodu Wang, Johanna Ziegel",
        "title": "E-backtesting",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
        "abstract": "  In the recent Basel Accords, the Expected Shortfall (ES) replaces the\nValue-at-Risk (VaR) as the standard risk measure for market risk in the banking\nsector, making it the most important risk measure in financial regulation. One\nof the most challenging tasks in risk modeling practice is to backtest ES\nforecasts provided by financial institutions. To design a model-free\nbacktesting procedure for ES, we make use of the recently developed techniques\nof e-values and e-processes. Backtest e-statistics are introduced to formulate\ne-processes for risk measure forecasts, and unique forms of backtest\ne-statistics for VaR and ES are characterized using recent results on\nidentification functions. For a given backtest e-statistic, a few criteria for\noptimally constructing the e-processes are studied. The proposed method can be\nnaturally applied to many other risk measures and statistical quantities. We\nconduct extensive simulation studies and data analysis to illustrate the\nadvantages of the model-free backtesting method, and compare it with the ones\nin the literature.\n"
    },
    {
        "paper_id": 2209.01013,
        "authors": "Wolfram Barfuss and Janusz Meylahn",
        "title": "Intrinsic fluctuations of reinforcement learning promote cooperation",
        "comments": "21 pages, 7 figures",
        "journal-ref": "Scientific Reports 13, 1309 (2023)",
        "doi": "10.1038/s41598-023-27672-7",
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  In this work, we ask for and answer what makes classical temporal-difference\nreinforcement learning with epsilon-greedy strategies cooperative. Cooperating\nin social dilemma situations is vital for animals, humans, and machines. While\nevolutionary theory revealed a range of mechanisms promoting cooperation, the\nconditions under which agents learn to cooperate are contested. Here, we\ndemonstrate which and how individual elements of the multi-agent learning\nsetting lead to cooperation. We use the iterated Prisoner's dilemma with\none-period memory as a testbed. Each of the two learning agents learns a\nstrategy that conditions the following action choices on both agents' action\nchoices of the last round. We find that next to a high caring for future\nrewards, a low exploration rate, and a small learning rate, it is primarily\nintrinsic stochastic fluctuations of the reinforcement learning process which\ndouble the final rate of cooperation to up to 80%. Thus, inherent noise is not\na necessary evil of the iterative learning process. It is a critical asset for\nthe learning of cooperation. However, we also point out the trade-off between a\nhigh likelihood of cooperative behavior and achieving this in a reasonable\namount of time. Our findings are relevant for purposefully designing\ncooperative algorithms and regulating undesired collusive effects.\n"
    },
    {
        "paper_id": 2209.01039,
        "authors": "Tommaso Luzzati, Ilaria Tucci, Pietro Guarnieri",
        "title": "Information overload and environmental degradation: learning from H.A.\n  Simon and W. Wenders",
        "comments": "Updated version of DEM Discussion papers, n. 2019/245,\n  https://econpapers.repec.org/paper/piedsedps/2019_2f245.htm",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  This paper discusses the relevance of information overload for explaining\nenvironmental degradation. Our argument goes that information overload and\ndetachment from nature, caused by energy abundance, have made individuals\nunaware of the unsustainable effects of their choices and lifestyles.\n"
    },
    {
        "paper_id": 2209.01138,
        "authors": "Biswajit Debnath, Rihab El-Hassani, Amit K Chattopadhyay, T Krishna\n  Kumar, Sadhan K Ghosh, Rahul Baidya",
        "title": "Time Evolution of a Supply Chain Network: Kinetic Modeling",
        "comments": "8 figures, 1 table, 2 appendices. arXiv admin note: text overlap with\n  arXiv:2003.00884",
        "journal-ref": "Physica A 2022",
        "doi": "10.1016/j.physa.2022.128085",
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  Resilient supply chains are often inherently dependent on the nature of their\ncomplex interconnected networks that are simultaneously multi-dimensional and\nmulti-layered. This article presents a Supply Chain Network (SCN) model that\ncan be used to regulate downstream relationships towards a sustainable SME\nusing a 4-component cost function structure - Environmental (E), Demand (D),\nEconomic (E), and Social (S). As a major generalization to the existing\npractice of using phenomenological interrelationships between the EDES cost\nkernels, we propose a complementary time varying model of a cost function,\nbased on Lagrangian mechanics (incorporating SCN constraints through Lagrange\nmultipliers), to analyze the time evolution of the SCN variables to interpret\nthe competition between economic inertia and market potential. Multicriteria\ndecision making, based on an Analytic Hierarchy Process (AHP), ranks\nperformance quality, identifying key business decision makers. The model is\nfirst solved numerically and then validated against real data pertaining to two\nSmall and Medium Enterprises (SMEs) from diverse domains, establishing the\ndomain-independent nature of the model. The results quantify how increases in a\nproduction line without appropriate consideration of market volatility can lead\nto bankruptcy, and how high transportation cost together with increased\nproduction may lead to a break-even state. The model also predicts the time it\ntakes a policy change to reinvigorate sales, thereby forecasting best practice\noperational procedure that ensures holistic sustainability on all four\nsustainability fronts.\n"
    },
    {
        "paper_id": 2209.01175,
        "authors": "Misha Teplitskiy, Soya Park, Neil Thompson, and David Karger",
        "title": "Intentional and serendipitous diffusion of ideas: Evidence from academic\n  conferences",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
        "abstract": "  This paper investigates the effects of seeing ideas presented in-person when\nthey are easily accessible online. Presentations may increase the diffusion of\nideas intentionally (when one attends the presentation of an idea of interest)\nand serendipitously (when one sees other ideas presented in the same session).\nWe measure these effects in the context of 25 computer science conferences\nusing data from the scheduling application Confer, which lets users browse\npapers, Like those of interest, and receive schedules of their presentations.\nWe address endogeneity concerns in presentation attendance by exploiting\nscheduling conflicts: when a user Likes multiple papers that are presented at\nthe same time, she cannot see them both, potentially affecting their diffusion.\nEstimates show that being able to see presentations increases citing of Liked\npapers within two years by 1.5 percentage points (62.5% boost over the baseline\ncitation rate). Attention to Liked papers also spills over to non-Liked papers\nin the same session, increasing their citing by 0.5 percentage points (125%\nboost), and this serendipitous diffusion represents 30.5% of the total effect.\nBoth diffusion types were concentrated among papers semantically close to an\nattendee's prior work, suggesting that there are inefficiencies in finding\nrelated research that conferences help overcome. Overall, even when ideas are\neasily accessible online, in-person presentations substantially increase\ndiffusion, much of it serendipitous.\n"
    },
    {
        "paper_id": 2209.01235,
        "authors": "Susan Athey, Dean Karlan, Emil Palikot, Yuan Yuan",
        "title": "Smiles in Profiles: Improving Fairness and Efficiency Using Estimates of\n  User Preferences in Online Marketplaces",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Online platforms often face challenges being both fair (i.e.,\nnon-discriminatory) and efficient (i.e., maximizing revenue). Using computer\nvision algorithms and observational data from a micro-lending marketplace, we\nfind that choices made by borrowers creating online profiles impact both of\nthese objectives. We further support this conclusion with a web-based\nrandomized survey experiment. In the experiment, we create profile images using\nGenerative Adversarial Networks that differ in a specific feature and estimate\nits impact on lender demand. We then counterfactually evaluate alternative\nplatform policies and identify particular approaches to influencing the\nchangeable profile photo features that can ameliorate the fairness-efficiency\ntension.\n"
    },
    {
        "paper_id": 2209.0133,
        "authors": "Sangita Das",
        "title": "Child labour and schooling decision of the marginal farmer households:\n  An empirical evidence from the East Medinipur district of West Bengal, India",
        "comments": "12 pages, 3 Tables. Book Title: Child at Stake Editors: Dr. Sanchari\n  Roy Mukherjee, Dr. Soma Ghosh, Dr. Selim Chisti, Dr. Sonali Mukherjee\n  (Bandyopadhyay) Publisher: New Delhi Publishers Year: June, 2022 Pages:\n  153-166 Chapter: 13 ISBN: 978-93-93878-62-5",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Based on the field investigation of West Bengal, this paper investigates\nwhether the school-aged children of the marginal farmer households are\nfull-time paid labourers or unpaid domestic labourers along with schooling or\nregular students. Probit Regression analysis is applied here to assess the\ninfluencing factors for reducing the size of the child labour force in\npractice. The result shows that the higher is the earning of the adult members\nof the households, the lower is the incidence of child labour. Moreover, the\ncredit accessibility of the mother from the Self-help group and more\nperson-days of the father in work in a reference year are also responsible for\nreducing the possibility of a child turning into labour. The study further\nsuggests that the younger age of the father, father's education, and low\noperational landholdings are positive and significant determinants to decide on\na child's education by restricting their excessive domestic work burden.\n"
    },
    {
        "paper_id": 2209.01345,
        "authors": "David Ehrlich and Nora Szech",
        "title": "How To Start a Grassroots Movement",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We study the influence of social messages that promote a digital public good,\na COVID-19 tracing app. We vary whether subjects receive a digital message from\nanother subject, and, if so, at what cost it came. Observed maximum willingness\nto invest in sending varies, from 1 cent up to 20 euros. Does this affect\nreceivers' sending behavior? Willingness to invest in sending increases when\npreviously receiving the message. Yet, cost signals have no impact. Thus,\ngrassroots movements can be started at virtually no cost. App-support matters\nnormatively as non-supporters are supposed to be punished in triage.\n"
    },
    {
        "paper_id": 2209.01378,
        "authors": "Roberto Baviera, Pietro Manzoni",
        "title": "Tree-Based Learning in RNNs for Power Consumption Forecasting",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  A Recurrent Neural Network that operates on several time lags, called an\nRNN(p), is the natural generalization of an Autoregressive ARX(p) model. It is\na powerful forecasting tool when different time scales can influence a given\nphenomenon, as it happens in the energy sector where hourly, daily, weekly and\nyearly interactions coexist. The cost-effective BPTT is the industry standard\nas learning algorithm for RNNs. We prove that, when training RNN(p) models,\nother learning algorithms turn out to be much more efficient in terms of both\ntime and space complexity. We also introduce a new learning algorithm, the Tree\nRecombined Recurrent Learning, that leverages on a tree representation of the\nunrolled network and appears to be even more effective. We present an\napplication of RNN(p) models for power consumption forecasting on the hourly\nscale: experimental results demonstrate the efficiency of the proposed\nalgorithm and the excellent predictive accuracy achieved by the selected model\nboth in point and in probabilistic forecasting of the energy consumption.\n"
    },
    {
        "paper_id": 2209.01487,
        "authors": "David J. Haw, Christian Morgenstern, Giovanni Forchini, Robert\n  Johnson, Patrick Doohan, Peter C. Smith, Katharina D. Hauck",
        "title": "Data needs for integrated economic-epidemiological models of pandemic\n  mitigation policies",
        "comments": "22 pages, 1 figure",
        "journal-ref": "Epidemics 41, 100644 (2022)",
        "doi": "10.1016/j.epidem.2022.100644",
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  The COVID-19 pandemic and the mitigation policies implemented in response to\nit have resulted in economic losses worldwide. Attempts to understand the\nrelationship between economics and epidemiology has lead to a new generation of\nintegrated mathematical models. The data needs for these models transcend those\nof the individual fields, especially where human interaction patterns are\nclosely linked with economic activity. In this article, we reflect upon\nmodelling efforts to date, discussing the data needs that they have identified,\nboth for understanding the consequences of the pandemic and policy responses to\nit through analysis of historic data and for the further development of this\nnew and exciting interdisciplinary field.\n"
    },
    {
        "paper_id": 2209.01512,
        "authors": "Carolina E.S. Mattsson, Allison Luedtke, Frank W. Takes",
        "title": "Inverse estimation of the transfer velocity of money",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Monitoring the money supply is an important prerequisite for conducting sound\nmonetary policy, yet monetary indicators are conventionally estimated in\naggregate. This paper proposes a new methodology that is able to leverage\nmicro-level transaction data from real-world payment systems. We apply a novel\ncomputational technique to measure the durations for which money is held in\nindividual accounts, and compute the transfer velocity of money from its\ninverse. Our new definition reduces to existing definitions under conventional\nassumptions. However, inverse estimation remains suitable for payment systems\nwhere the total balance fluctuates and spending patterns change in time. Our\nmethod is applied to study Sarafu, a small digital community currency in Kenya,\nwhere transaction data is available from 25 January 2020 to 15 June 2021. We\nfind that the transfer velocity of Sarafu was higher than it would seem, in\naggregate, because not all units of Sarafu remained in active circulation.\nMoreover, inverse estimation reveals strong heterogineities and enables\ncomparisons across subgroups of spenders. Some units of Sarafu were held for\nminutes, others for months, and spending patterns differed across communities\nusing Sarafu. The rate of circulation and the effective balance of Sarafu\nchanged substantially over time, as these communities experienced economic\ndisruptions related to the COVID-19 pandemic and seasonal food insecurity.\nThese findings contribute to a growing body of literature documenting the\nheterogeneous patterns underlying headline macroeconomic indicators and their\nrelevance for policy. Inverse estimation may be especially useful in studying\nthe response of spenders to targeted monetary operations.\n"
    },
    {
        "paper_id": 2209.01653,
        "authors": "Jin Hong Kuan",
        "title": "Liquidity Provision Payoff on Automated Market Makers",
        "comments": "5 pages, 2 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  The standard approach for compensating liquidity providers on many\ndecentralized exchanges (DEX) for serving as counter-party to swaps is through\ncharging a small percentage of fees. The expected payoff from the cash flow of\nthis mode of market making has yet to be mathematically formulated in terms of\nvolatility in the existing literature. We provide here a preliminary derivation\nof the payoff formula, by making the standard set of assumptions for efficient\nmarkets, namely geometric Brownian price movements and zero arbitrage. Trading\nvolume, conventionally taken as an exogenous variable for fees calculation,\nbecomes a function of volatility and available liquidity in this formulation.\nIn doing so, we show that it is a near-linear function of the volatility of the\nunderlying risky asset. Since hedging instruments with such a property are\nhighly sought after, we discuss the potential of securitizing the cash flow of\nliquidity fees to serve as a volatility product in its own right.\n"
    },
    {
        "paper_id": 2209.01805,
        "authors": "Yiyan Huang, Cheuk Hang Leung, Xing Yan, Qi Wu, Shumin Ma, Zhiri Yuan,\n  Dongdong Wang, Zhixiang Huang",
        "title": "Robust Causal Learning for the Estimation of Average Treatment Effects",
        "comments": "This paper was accepted and will be published at The 2022\n  International Joint Conference on Neural Networks (IJCNN2022). arXiv admin\n  note: substantial text overlap with arXiv:2103.11869",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Many practical decision-making problems in economics and healthcare seek to\nestimate the average treatment effect (ATE) from observational data. The\nDouble/Debiased Machine Learning (DML) is one of the prevalent methods to\nestimate ATE in the observational study. However, the DML estimators can suffer\nan error-compounding issue and even give an extreme estimate when the\npropensity scores are misspecified or very close to 0 or 1. Previous studies\nhave overcome this issue through some empirical tricks such as propensity score\ntrimming, yet none of the existing literature solves this problem from a\ntheoretical standpoint. In this paper, we propose a Robust Causal Learning\n(RCL) method to offset the deficiencies of the DML estimators. Theoretically,\nthe RCL estimators i) are as consistent and doubly robust as the DML\nestimators, and ii) can get rid of the error-compounding issue. Empirically,\nthe comprehensive experiments show that i) the RCL estimators give more stable\nestimations of the causal parameters than the DML estimators, and ii) the RCL\nestimators outperform the traditional estimators and their variants when\napplying different machine learning models on both simulation and benchmark\ndatasets.\n"
    },
    {
        "paper_id": 2209.02239,
        "authors": "Seung Hwan Kim, Jeong hwan Jeon, Anwar Aridi, Bogang Jun",
        "title": "Factors that affect the technological transition of firms toward the\n  industry 4.0 technologies",
        "comments": "37 pages, 3 figures, 7 tables",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  This research aims to identify factors that affect the technological\ntransition of firms toward industry 4.0 technologies (I4Ts) focusing on firm\ncapabilities and policy impact using relatedness and complexity measures. For\nthe analysis, a unique dataset of Korean manufacturing firms' patent and their\nfinancial and market information was used. Following the Principle of\nRelatedness, which is a recently shaped empirical principle in the field of\neconomic complexity, economic geography, and regional studies, we build a\ntechnology space and then trace each firm's footprint on the space. Using the\ntechnology space of firms, we can identify firms that successfully develop a\nnew industry 4.0 technology and examine whether their accumulated capabilities\nin their previous technology domains positively affect their technological\ndiversification and which factors play a critical role in their transition\ntowards industry 4.0. In addition, by combining data on whether the firms\nreceived government support for R&D activities, we further analyzed the role of\ngovernment policy in supporting firms' knowledge activity in new industry 4.0\ntechnologies. We found that firms with higher related technologies and more\ngovernment support are more likely to enter new I4Ts. We expect our research to\ninform policymakers who aim to diversify firms' technological capabilities into\nI4Ts.\n"
    },
    {
        "paper_id": 2209.02335,
        "authors": "Billur Aksoy, Christopher S. Carpenter, Dario Sansone",
        "title": "Understanding Labor Market Discrimination Against Transgender People:\n  Evidence from a Double List Experiment and a Survey",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Using a US nationally representative sample and a double list experiment\ndesigned to elicit views free from social desirability bias, we find that\nanti-transgender labor market attitudes are significantly underreported. After\ncorrecting for this concealment, we report that 73 percent of people would be\ncomfortable with a transgender manager and 74 percent support employment\nnon-discrimination protection for transgender people. We also show that\nrespondents severely underestimate the population level of support for\ntransgender individuals in the workplace, and we find that labor market support\nfor transgender people is significantly lower than support for gay, lesbian,\nand bisexual people. Our results provide timely evidence on workplace-related\nviews toward transgender people and help us better understand employment\ndiscrimination against them.\n"
    },
    {
        "paper_id": 2209.0234,
        "authors": "Chen Chris Gong, Falko Ueckerdt, Robert Pietzcker, Adrian Odenweller,\n  Wolf-Peter Schill, Martin Kittel, Gunnar Luderer",
        "title": "Bidirectional coupling of a long-term integrated assessment model REMIND\n  v3.0.0 with an hourly power sector model DIETER v1.0.2",
        "comments": null,
        "journal-ref": null,
        "doi": "10.5194/gmd-16-4977-2023",
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Integrated assessment models (IAMs) are a central tool for the quantitative\nanalysis of climate change mitigation strategies. However, due to their global,\ncross-sectoral and centennial scope, IAMs cannot explicitly represent the\nspatio-temporal detail required to properly analyze the key role of variable\nrenewable electricity (VRE) for decarbonizing the power sector and end-use\nelectrification. In contrast, power sector models (PSMs) incorporate high\nspatio-temporal resolutions, but tend to have narrower scopes and shorter time\nhorizons. To overcome these limitations, we present a novel methodology: an\niterative and fully automated soft-coupling framework that combines the\nstrengths of a IAM and a PSM. This framework uses the market values of power\ngeneration as well as the capture prices of demand in the PSM as price signals\nthat change the capacity and power mix of the IAM. Hence, both models make\nendogenous investment decisions, leading to a joint solution. We apply the\nmethod to Germany in a proof-of-concept study using the IAM REMIND and the PSM\nDIETER, and confirm the theoretical prediction of almost-full convergence both\nin terms of decision variables and (shadow) prices. At the end of the iterative\nprocess, the absolute model difference between the generation shares of any\ngenerator type for any year is <5% for a simple configuration (no storage, no\nflexible demand), and 6-7% for a more realistic and detailed configuration\n(with storage and flexible demand). For the simple configuration, we\nmathematically show that this coupling scheme corresponds uniquely to an\niterative mapping of the Lagrangians of two power sector optimization problems\nof different time resolutions, which can lead to a comprehensive model\nconvergence of both decision variables and (shadow) prices. Since our approach\nis based on fundamental economic principles, it is applicable also to other\nIAM-PSM pairs.\n"
    },
    {
        "paper_id": 2209.0236,
        "authors": "Leandro Lind, Rafael Cossent, and Pablo Frias",
        "title": "TSO-DSO Coordination for the Procurement of Balancing and Congestion\n  Management Services: Assessment of a meshed-to-meshed topology",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  This paper proposes a comprehensive model for different Coordination Schemes\n(CSs) for Transmission (TSO) and Distribution System Operators (DSO) in the\ncontext of distributed flexibility procurement for balancing and congestion\nmanagement. The model proposed focuses on the coordination between the EHV\n(TSO) and the HV (DSO) levels, exploring the meshed-to-meshed topology,\nincluding multiple TSO-DSO interface substations. The model is then applied to\na realistic case study in which the Swedish power system is modeled for one\nyear, considering a representation of the transmission grid together with the\nsubtransmission grid of Uppsala city. The base case scenario is then subject to\ndifferent scalability and replication scenarios. The paper corroborates the\nfinding that the Common CS leads to the least overall cost of flexibility\nprocurement. Moreover, it shows the effectiveness of the Local Flexibility\nMarket (LFM) for the DSO in the Swedish context in reducing potential penalties\nin a Multi-level CS.\n"
    },
    {
        "paper_id": 2209.02367,
        "authors": "Milad Zam and Mohammadhosein Tavakoli and Hasan Ramezanian and Amin\n  Rezasoltani",
        "title": "Assessing the different aspects of consuming fashion and the role of\n  self-confidence on the buying behaviour of fashion consumers in the clothing\n  market as a mediator",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  As a mediator variable, self-confidence is one of the most effective elements\nof the decision-making process of consumer behaviour. This research has studied\nthe effects of different aspects of consuming fashion on the self-confidence\nand behaviour of consumers in Tehran's clothing market. This study has\nconsidered the acceptance of new products, interest in mode and fashion,\nutilitarianism, and personal taste in its analysis. This research aims to\nunderstand the fashion buying behaviour amongst Iranian consumers in\nconsideration of their attitude towards self-confidence and aspects of fashion\nconsumption. The statistic sample is 400 consumers from Tehran's clothing\nmarket who have been chosen based on the random availability procedure. The\nprimary tool in this research was a questionary used to testify the assumptions\nand a model fit created by using structural equations and factor analysis. This\nresearch showed that the interest in mode and fashion, personal taste,\nutilitarianism, and new products positively impact self-confidence. In\naddition, the positive impact of self-confidence on fashion buying behaviour\nwas confirmed.\n"
    },
    {
        "paper_id": 2209.02407,
        "authors": "Ruochen Xiao, Yingying Feng, Lei Yan, Yihan Ma",
        "title": "Predict stock prices with ARIMA and LSTM",
        "comments": "arXiv admin note: text overlap with arXiv:1912.10806 by other authors",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  MAE, MSE and RMSE performance indicators are used to analyze the performance\nof different stocks predicted by LSTM and ARIMA models in this paper. 50 listed\ncompany stocks from finance.yahoo.com are selected as the research object in\nthe experiments. The dataset used in this work consists of the highest price on\ntransaction days, corresponding to the period from 01 January 2010 to 31\nDecember 2018. For LSTM model, the data from 01 January 2010 to 31 December\n2015 are selected as the training set, the data from 01 January 2016 to 31\nDecember 2017 as the validation set and the data from 01 January 2018 to 31\nDecember 2018 as the test set. In term of ARIMA model, the data from 01 January\n2016 to 31 December 2017 are selected as the training set, and the data from 01\nJanuary 2018 to 31 December 2018 as the test set. For both models, 60 days of\ndata are used to predict the next day. After analysis, it is suggested that\nboth ARIMA and LSTM models can predict stock prices, and the prediction results\nare generally consistent with the actual results;and LSTM has better\nperformance in predicting stock prices(especially in expressing stock price\nchanges), while the application of ARIMA is more convenient.\n"
    },
    {
        "paper_id": 2209.02635,
        "authors": "Patrizio Bifulco, Jochen Gl\\\"uck, Oliver Krebs, Bohdan Kukharskyy",
        "title": "Single and Attractive: Uniqueness and Stability of Economic Equilibria\n  under Monotonicity Assumptions",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper characterizes equilibrium properties of a broad class of economic\nmodels that allow multiple heterogeneous agents to interact in heterogeneous\nmanners across several markets. Our key contribution is a new theorem providing\nsufficient conditions for uniqueness and stability of equilibria in this class\nof models. To illustrate the applicability of our theorem, we characterize the\ngeneral equilibrium properties of two commonly used quantitative trade models.\nSpecifically, our analysis provides a first proof of uniqueness and stability\nof the equilibrium in multi-country trade models featuring (i) multiple\nsectors, or (ii) heterogeneity across countries in terms of their labor cost\nshares. These examples also provide a practical toolkit for future research on\nhow our theorem can be applied to establish uniqueness and stability of\nequilibria in a broad set of economic models.\n"
    },
    {
        "paper_id": 2209.02637,
        "authors": "Johannes Matzat and Aiko Schmei{\\ss}er",
        "title": "Do Unions Shape Political Ideologies at Work?",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Labor unions' greatest potential for political influence likely arises from\ntheir direct connection to millions of individuals at the workplace. There,\nthey may change the ideological positions of both unionizing workers and their\nnon-unionizing management. In this paper, we analyze the workplace-level impact\nof unionization on workers' and managers' political campaign contributions over\nthe 1980-2016 period in the United States. To do so, we link\nestablishment-level union election data with transaction-level campaign\ncontributions to federal and local candidates. In a difference-in-differences\ndesign that we validate with regression discontinuity tests and a novel\ninstrumental variables approach, we find that unionization leads to a leftward\nshift of campaign contributions. Unionization increases the support for\nDemocrats relative to Republicans not only among workers but also among\nmanagers, which speaks against an increase in political cleavages between the\ntwo groups. We provide evidence that our results are not driven by\ncompositional changes of the workforce and are weaker in states with\nRight-to-Work laws where unions can invest fewer resources in political\nactivities.\n"
    },
    {
        "paper_id": 2209.02651,
        "authors": "Ali Zeytoon-Nejad and Tanzid Hasnain",
        "title": "The Coronavirus Tradeoff -- Life vs. Economy: Handling the Tradeoff\n  Rationally and Optimally",
        "comments": null,
        "journal-ref": "Social Sciences & Humanities Open, 4(1), 100215 (2021)",
        "doi": "10.1016/j.ssaho.2021.100215",
        "license": "http://creativecommons.org/publicdomain/zero/1.0/",
        "abstract": "  The recent coronavirus outbreak has made governments face an inconvenient\ntradeoff choice, i.e. the choice between saving lives and saving the economy,\nforcing them to make immensely consequential decisions among alternative\ncourses of actions without knowing what the ultimate results would be for the\nsociety as a whole. This paper attempts to frame the coronavirus tradeoff\nproblem as an economic optimization problem and proposes mathematical\noptimization methods to make rationally optimal decisions when faced with\ntrade-off situations such as those involved in managing through the recent\ncoronavirus pandemic. The framework introduced and the method proposed in this\npaper are on the basis of the theory of rational choice at a societal level,\nwhich assumes that the government is a rational, benevolent agent that\nsystematically and purposefully takes into account the social marginal costs\nand social marginal benefits of its actions to its citizens and makes decisions\nthat maximize the society's well-being as a whole. We approach solving this\ntradeoff problem from a static as well as a dynamic point of view. Finally, we\nprovide several numerical examples clarifying how the proposed framework and\nmethods can be applied in the real-world context.\n"
    },
    {
        "paper_id": 2209.02653,
        "authors": "Ali Zeytoon-Nejad",
        "title": "Measuring Price Risk Aversion through Indirect Utility Functions: A\n  Laboratory Experiment",
        "comments": null,
        "journal-ref": "Games, 13(4), 56 (2022)",
        "doi": "10.3390/g13040056",
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  The present paper introduces a theoretical framework through which the degree\nof risk aversion with respect to uncertain prices can be measured through the\ncontext of the indirect utility function (IUF) using a lab experiment. First,\nthe paper introduces the main elements of the duality theory (DT) in economics.\nNext, it proposes the context of IUFs as a suitable framework for measuring\nprice risk aversion through varying prices as opposed to varying payoffs, which\nhas been common practice in the mainstream of experimental economics. Indeed,\nthe DT in modern microeconomics indicates that the direct utility function\n(DUF) and the IUF are dual to each other, implicitly suggesting that the degree\nof risk aversion (or risk seeking) that a given rational subject exhibits in\nthe context of the DUF must be equivalent to the degree of risk aversion (or\nrisk seeking) elicited through the context of the IUF. This paper tests the\naccuracy of this theoretical prediction through a lab experiment using a series\nof relevant statistical tests. This study uses the multiple price list (MPL)\nmethod, which has been one of the most popular sets of elicitation procedures\nin experimental economics to study risk preferences in the experimental\nlaboratory using non-interactive settings. The key findings of this study\nindicate that price risk aversion (PrRA) is statistically significantly greater\nthan payoff risk aversion (PaRA). Additionally, it is shown that the risk\npreferences elicited under the expected utility theory (EUT) are somewhat\nsubject to context. Other findings imply that the risk premium (RP), as a\nmeasure of willingness to pay for insuring an uncertain situation, is\nstatistically significantly greater for stochastic prices compared to that for\nstochastic payoffs. These results are robust across different MPL designs and\nvarious statistical tests that are utilized.\n"
    },
    {
        "paper_id": 2209.02665,
        "authors": "Seyyed Ali Zeytoon Nejad Moosavian",
        "title": "Using the Interactive Graphic Syllabus in the Teaching of Economics",
        "comments": null,
        "journal-ref": "American Journal of Business Education, 10(2), 45-64 (2017)",
        "doi": null,
        "license": "http://creativecommons.org/publicdomain/zero/1.0/",
        "abstract": "  Syllabus is essentially a concise outline of a course of study, and\nconventionally a text document. In the past few decades, however, two novel\nvariations of syllabus have emerged, namely \"the Graphic Syllabus\" and \"the\nInteractive Syllabus\". Each of these two variations of syllabus has its own\nspecial advantages. The present paper argues that there could be devised a new\ncombined version of the two mentioned variations, called \"the Interactive\nGraphic Syllabus\", which can potentially bring us the advantages of both at the\nsame time. Specifically, using a well-designed Interactive Graphic syllabus can\nbring about many advantages such as clarifying complex relationships; causing a\nbetter retention; needing less cognitive energy for interpretation; helping\ninstructors identify any snags in their course organization; capability of\nbeing integrated easily into a course management system; appealing to many of\nlearning styles and engaging students with different learning styles.\n"
    },
    {
        "paper_id": 2209.02683,
        "authors": "Seyyed Ali Zeytoon Nejad Moosavian",
        "title": "Classicals versus Keynesians: Fifty Distinctions between Two Major\n  Schools of Economic Thought",
        "comments": null,
        "journal-ref": "Journal of Economic and Social Thought, 9(2), 63-79 (2022)",
        "doi": "10.1453/jest.v9i2.2325",
        "license": "http://creativecommons.org/publicdomain/zero/1.0/",
        "abstract": "  Macroeconomics essentially discusses macroeconomic phenomena from the\nperspectives of various schools of economic thought, each of which takes\ndifferent views on how macroeconomic agents make decisions and how the\ncorresponding markets operate. Therefore, developing a clear, comprehensive\nunderstanding of how and in what ways these schools of economic thought differ\nis a key and a prerequisite for economics students to prosper academically and\nprofessionally in the discipline. This becomes even more crucial as economics\nstudents pursue their studies toward higher levels of education and graduate\nschool, during which students are expected to attain higher levels of Bloom's\ntaxonomy, including analysis, synthesis, evaluation, and creation. Teaching the\ndistinctions and similarities of the two major schools of economic thought has\nnever been an easy task to undertake in the classroom. Although the reason for\nsuch a hardship can be multi-fold, one reason has undoubtedly been students'\nlack of a holistic view on how the two mainstream economic schools of thought\ndiffer. There is strong evidence that students make smoother transition to\nhigher levels of education after building up such groundwork, on which they can\nbuild further later on (e.g. Didia and Hasnat, 1998; Marcal and Roberts, 2001;\nIslam, et al., 2008; Green, et al., 2009; White, 2016). The paper starts with a\nvisual spectrum of various schools of economic thought, and then narrows down\nthe scope to the classical and Keynesian schools, i.e. the backbone of modern\nmacroeconomics. Afterwards, a holistic table contrasts the two schools in terms\nof 50 aspects. Not only does this table help economics students enhance their\ncomprehension, retention, and critical-thinking capability, it also benefits\nmacroeconomic instructors to ...\n"
    },
    {
        "paper_id": 2209.02743,
        "authors": "Seyyed Ali Zeytoon Nejad Moosavian",
        "title": "Identifying the Effect of Parenthood on Labor Force Participation: A\n  Gender Comparison",
        "comments": null,
        "journal-ref": "Journal of Economics and Political Economy, 8(3), 207-238 (2021)",
        "doi": "10.1453/jepe.v8i3.2230",
        "license": "http://creativecommons.org/publicdomain/zero/1.0/",
        "abstract": "  Identifying the factors that influence labor force participation could\nelucidate how individuals arrive at their labor supply decisions, whose\nunderstanding is, in turn, of crucial importance in analyzing how the supply\nside of the labor market functions. This paper investigates the effect of\nparenthood status on Labor Force Participation (LFP) decisions using an\nindividual-level fixed-effects identification strategy. The differences across\nindividuals and over time in having or not having children as well as being or\nnot being in the labor force provide the variation needed to assess the\nassociation between individuals' LFP behavior and parenthood. Parenthood could\nhave different impacts on mothers than it would on fathers. In order to look at\nthe causal effect of maternity and paternity on LFP separately, the data is\ndisaggregated by gender. To this end, the effect of a change in the parenthood\nstatus can be measured using individual-level fixed-effects to account for\ntime-invariant characteristics of individuals becoming a parent. The primary\ndata source used is the National Longitudinal Surveys (NLS). Considering the\nnature of LFP variable, this paper employs Binary Response Models (BRMs) to\nestimate LFP equations using individual-level micro data. The findings of the\nstudy show that parenthood has a negative overall effect on LFP. However,\npaternity has a significant positive effect on the likelihood of being in the\nlabor force, whilst maternity has a significant negative impact of LFP. In\naddition, the results imply that the effect of parenthood on LFP has been\nfading away over time, regardless of the gender of parents. These two pieces of\nevidence precisely map onto the theoretical predictions made by the related\nmainstream economic theories (the traditional neoclassical theory of labor\nsupply as well as Becker's household production model). These results are ...\n"
    },
    {
        "paper_id": 2209.02839,
        "authors": "Seyyed Ali Zeytoon Nejad Moosavian",
        "title": "The Visual Decoding of the Wheel of Duality in Consumer Theory in Modern\n  Microeconomics",
        "comments": null,
        "journal-ref": "Applied Economics And Finance, 3(3), 288-304 (2016)",
        "doi": "10.11114/aef.v3i3.1718",
        "license": "http://creativecommons.org/publicdomain/zero/1.0/",
        "abstract": "  Duality is the heart of advanced microeconomics. It exists everywhere\nthroughout advanced microeconomics, from the beginning of consumer theory to\nthe end of production theory. The complex, circular relationships among various\ntheoretical microeconomic concepts involved in the setting of duality theory\nhave led it to be called the \"wheel of pain\" by many graduate economics\nstudents . Put simply, the main aim of this paper is to turn this \"wheel of\npain\" into a \"wheel of joy\". To be more specific, the primary purpose of this\npaper is to graphically decode the logical, complex relationships among a\nquartet of dual functions which present preferences as well as a quartet of\ndemand-related functions in a visual manner.\n"
    },
    {
        "paper_id": 2209.02841,
        "authors": "Alexandra Naumenko and Seyyed Ali Zeytoon Nejad Moosavian",
        "title": "Clarifying Theoretical Intricacies through the Use of Conceptual\n  Visualization: Case of Production Theory in Advanced Microeconomics",
        "comments": null,
        "journal-ref": "Applied Economics and Finance, 3(3), 103-122 (2016)",
        "doi": "10.11114/aef.v3i4.1781",
        "license": "http://creativecommons.org/publicdomain/zero/1.0/",
        "abstract": "  Production theory, defined as the study of the economic process of\ntransforming inputs into outputs, consists of two simultaneous economic forces:\ncost minimization and profit maximization. The cost minimization problem\ninvolves deriving conditional factor demand functions and the cost function.\nThe profit maximization problem involves deriving the output supply function,\nthe profit function, and unconditional factor demand functions. Nested within\nthe process are Shephard's lemma, Hotelling's lemmas, direct and indirect\nmathematical relations, and other elements contributing to the dynamics of the\nprocess. The intricacies and hidden underlying influences pose difficulties in\npresenting the material for an instructor, and inhibit learning by students.\nSimply put, the primary aim of this paper is to facilitate the teaching and\nlearning of the production theory realm of Economics through the use of a\nconceptual visual model. This paper proposes a pedagogical tool in the form of\na detailed graphic illustrating t he relationship between profit maximization\nand cost minimization under technical constraints, with an emphasis on the\nsimilarities and differences between the perfect competition and monopoly\ncases. The potential that such a visual has to enhance learning when\nsupplementing traditional context is discussed under the context of\ncontemporary learning literature. Embedded in the discussion is an example of\nhow we believe our model could be conceptualized and utilized in a real-world\nsetting to evaluate an industrial project with an economic point of view.\n"
    },
    {
        "paper_id": 2209.02873,
        "authors": "Anindya Goswami, Kuldip Singh Patel, and Pradeep Kumar Sahu",
        "title": "A novel difference equation approach for the stability and robustness of\n  compact schemes for variable coefficient PDEs",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Fourth-order accurate compact schemes for variable coefficient convection\ndiffusion equations are considered. A sufficient condition for the stability of\nthe fully discrete problem is derived using a difference equation based\napproach. The constant coefficient problems are considered as a special case,\nand the unconditional stability of compact schemes for such case is proved\ntheoretically. The condition number of the amplification matrix is also\nanalyzed, and an estimate for the same is derived. The examples are provided to\nsupport the assumption taken to assure stability.\n"
    },
    {
        "paper_id": 2209.03017,
        "authors": "Michael B. Giles, Abdul-Lateef Haji-Ali",
        "title": "Multilevel Path Branching for Digital Options",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We propose a new Monte Carlo-based estimator for digital options with assets\nmodelled by a stochastic differential equation (SDE). The new estimator is\nbased on repeated path splitting and relies on the correlation of approximate\npaths of the underlying SDE that share parts of a Brownian path. Combining this\nnew estimator with Multilevel Monte Carlo (MLMC) leads to an estimator with a\ncomputational complexity that is similar to the complexity of a MLMC estimator\nwhen applied to options with Lipschitz payoffs.\n  This preprint includes detailed calculations and proofs (in grey colour)\nwhich are not peer-reviewed and not included in the published article.\n"
    },
    {
        "paper_id": 2209.03046,
        "authors": "Aleksandar Keseljevic, Rok Spruk",
        "title": "Estimating the Effects of Syrian Civil War",
        "comments": "34 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  We examine the effect of civil war in Syria on economic growth, human\ndevelopment and institutional quality. Building on the synthetic control\nmethod, we estimate the missing counterfactual scenario in the hypothetical\nabsence of the armed conflict that led to unprecedented humanitarian crisis and\npopulation displacement in modern history. By matching Syrian growth and\ndevelopment trajectories with the characteristics of the donor pool of 66\ncountries with no armed internal conflict in the period 1996-2021, we estimate\na series of growth and development gaps attributed to civil war. Syrian civil\nwar appears to have had a temporary negative effect on the trajectory of\neconomic growth that almost disappeared before the onset of COVID19 pandemic.\nBy contrast, the civil war led to unprecedented losses in human development,\nrising infant mortality and rampantly deteriorating institutional quality. Down\nto the present day, each year of the conflict led to 5,700 additional\nunder-five child deaths with permanently derailed negative effect on longevity.\nThe civil war led to unprecedent and permanent deterioration in institutional\nquality indicated by pervasive weakening of the rule of law and deleterious\nimpacts on government effectiveness, civil liberties and widespread escalation\nof corruption. The estimated effects survive a battery of placebo checks.\n"
    },
    {
        "paper_id": 2209.03239,
        "authors": "Fangxuan Chen, Zhiwei Ma, Hadi Nasrabadi, Bailian Chen, Mohamed\n  Mehana, Jolante Wieke Van Wijk",
        "title": "Technical and Economic Feasibility Analysis of Underground Hydrogen\n  Storage: A Case Study in Intermountain-West Region USA",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  Hydrogen is an integral component of the current energy transition roadmap to\ndecarbonize the economy and create an environmentally-sustainable future.\nHowever, surface storage options (e.g., tanks) do not provide the required\ncapacity or durability to deploy a regional or nationwide hydrogen economy. In\nthis study, we have analyzed the techno-economic feasibility of the geologic\nstorage of hydrogen in depleted gas reservoirs, salt caverns, and aquifers in\nthe Intermountain-West (I-WEST) region. We have identified the most favorable\ncandidate sites for hydrogen storage and estimated the volumetric storage\ncapacity. Our results show that the geologic storage of hydrogen can provide at\nleast 72% of total energy consumption of I-WEST region in 2020. We also\ncalculated the capital and levelized costs of each storage option. We found\nthat a depleted gas reservoir is the most cost-effective candidate among the\nthree geologic storage options. Interestingly, the cushion gas type and volume\nplay a significant role in the storage cost when we consider hydrogen storage\nin saline aquifers. The levelized costs of hydrogen storage in depleted gas\nreservoirs, salt caverns, and saline aquifers with large-scale storage capacity\nare approximately $1.3, $2.3, and $3.4 per kg of H2, respectively. This work\nprovides essential guidance for the geologic hydrogen storage in the I-WEST\nregion.\n"
    },
    {
        "paper_id": 2209.03307,
        "authors": "Guillermo Angeris, Tarun Chitra, Alex Evans, Matthew Lorig",
        "title": "A primer on perpetuals",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We consider a continuous-time financial market with no arbitrage and no\ntransactions costs. In this setting, we introduce two types of perpetual\ncontracts, one in which the payoff to the long side is a fixed function of the\nunderlyers and the long side pays a funding rate to the short side, the other\nin which the payoff to the long side is a fixed function of the underlyers\ntimes a discount factor that changes over time but no funding payments are\nrequired. Assuming asset prices are continuous and strictly positive, we derive\nmodel-free expressions for the funding rate and discount rate of these\nperpetual contracts as well as replication strategies for the short side. When\nasset prices can jump, we derive expressions for the funding and discount\nrates, which are semi-robust in the sense that they do not depend on the\ndynamics of the volatility process of the underlying risky assets, but do\ndepend on the intensity of jumps under the market's pricing measure. When asset\nprices can jump and the volatility process is independent of the underlying\nrisky assets, we derive an explicit replication strategy for the short side of\na perpetual contract. Throughout the paper, we illustrate through examples how\nspecific perpetual contracts relate to traditional financial instruments such\nas variance swaps and leveraged exchange traded funds.\n"
    },
    {
        "paper_id": 2209.03425,
        "authors": "Ruodu Wang, Qinyu Wu",
        "title": "Quasi-convexity in mixtures for generalized rank-dependent functions",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  Quasi-convexity in probabilistic mixtures is a common and useful property in\ndecision analysis. We study a general class of non-monotone mappings, called\nthe generalized rank-dependent functions, which include the preference models\nof expected utilities, dual utilities, and rank-dependent utilities as special\ncases, as well as signed Choquet integrals used in risk management. As one of\nour main results, quasi-convex (in mixtures) signed Choquet integrals precisely\ninclude two parts: those that are convex (in mixtures) and the class of scaled\nquantile-spread mixtures, and this result leads to a full characterization of\nquasi-convexity for generalized rank-dependent functions. Seven equivalent\nconditions for quasi-convexity in mixtures are obtained for dual utilities and\nsigned Choquet integrals. We also illustrate a conflict between convexity in\nmixtures and convexity in risk pooling among constant-additive mappings.\n"
    },
    {
        "paper_id": 2209.03448,
        "authors": "Nissa Amilia, Zulkifli Palinrungi, Iwan Vanany, Mansur Arief",
        "title": "Designing an Optimized Electric Vehicle Charging Station Infrastructure\n  for Urban Area: A Case study from Indonesia",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The rapid development of electric vehicle (EV) technologies promises cleaner\nair and more efficient transportation systems, especially for polluted and\ncongested urban areas. To capitalize on this potential, the Indonesian\ngovernment has appointed PLN, its largest state-owned electricity provider, to\naccelerate the preparation of Indonesia's EV infrastructure. With a mission of\nproviding reliable, accessible, and cost-effective EV charging station\ninfrastructure throughout the country, the company is prototyping a\nlocation-optimized model to simulate how well its infrastructure design reaches\ncustomers, fulfills demands, and generates revenue. In this work, we study how\nPLN could maximize profit by optimally placing EV charging stations in urban\nareas by adopting a maximal covering location model. In our experiments, we use\ndata from Surabaya, Indonesia, and consider the two main transportation modes\nfor the locals to charge: electric motorcycles and electric cars. Numerical\nexperiments show that only four charging stations are needed to cover the whole\ncity, given the charging technology that PLN has acquired. However, consumers'\ntime-to-travel is exceptionally high (about 35 minutes), which could lead to\npoor consumer service and hindrance toward EV technologies. Sensitivity\nanalysis reveals that building more charging stations could reduce the time but\ncomes with higher costs due to extra facility installations. Adding layers of\nredundancy to buffer against outages or other disruptions also incurs higher\ncosts but could be an appealing option to design a more reliable and thriving\nEV infrastructure. The model can provide insights to decision-makers to devise\nthe most reliable and cost-effective infrastructure designs to support the\ndeployment of electric vehicles and much more advanced intelligent\ntransportation systems in the near future.\n"
    },
    {
        "paper_id": 2209.03461,
        "authors": "Eric Luxenberg and Philipp Schiele and Stephen Boyd",
        "title": "Portfolio Optimization with Cumulative Prospect Theory Utility via\n  Convex Optimization",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We consider the problem of choosing a portfolio that maximizes the cumulative\nprospect theory (CPT) utility on an empirical distribution of asset returns. We\nshow that while CPT utility is not a concave function of the portfolio weights,\nit can be expressed as a difference of two functions. The first term is the\ncomposition of a convex function with concave arguments and the second term a\ncomposition of a convex function with convex arguments. This structure allows\nus to derive a global lower bound, or minorant, on the CPT utility, which we\ncan use in a minorization-maximization (MM) algorithm for maximizing CPT\nutility. We further show that the problem is amenable to a simple\nconvex-concave (CC) procedure which iteratively maximizes a local\napproximation. Both of these methods can handle small and medium size problems,\nand complex (but convex) portfolio constraints. We also describe a simpler\nmethod that scales to larger problems, but handles only simple portfolio\nconstraints.\n"
    },
    {
        "paper_id": 2209.03892,
        "authors": "Benjamin Niswonger",
        "title": "What You See is What You Get: Local Labor Markets and Skill Acquisition",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-sa/4.0/",
        "abstract": "  This paper highlights the potential for negative dynamic consequences of\nrecent trends towards the formation of \"skill-hubs\". I first show evidence that\nskill acquisition is biased towards skills which are in demand in local labor\nmarkets. This fact along with large heterogeneity in outcomes by major and\nrecent reductions in migration rates implies a significant potential for\ninefficient skill upgrading over time. To evaluate the impact of local bias in\neducation in the context of standard models which focus on agglomeration\neffects, I develop a structural spatial model which includes educational\ninvestment. The model focuses on two sources of externalities: productivity\nthrough agglomeration and signaling. Both of these affect educational decisions\ntilting the balance of aggregate skill composition. Signaling externalities can\nprovide a substantial wedge in the response to changes in skill demand and\nskill concentration with the potential for substantial welfare gains from a\nmore equal distribution of skills.\n"
    },
    {
        "paper_id": 2209.03935,
        "authors": "Matteo Rizzato, Julien Wallart, Christophe Geissler, Nicolas Morizet,\n  Noureddine Boumlaik",
        "title": "Generative Adversarial Networks Applied to Synthetic Financial Scenarios\n  Generation",
        "comments": null,
        "journal-ref": "Physica A: Statistical Mechanics and its Applications, 2023, 623,\n  pp.128899",
        "doi": "10.1016/j.physa.2023.128899",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The finance industry is producing an increasing amount of datasets that\ninvestment professionals can consider to be influential on the price of\nfinancial assets. These datasets were initially mainly limited to exchange\ndata, namely price, capitalization and volume. Their coverage has now\nconsiderably expanded to include, for example, macroeconomic data, supply and\ndemand of commodities, balance sheet data and more recently extra-financial\ndata such as ESG scores. This broadening of the factors retained as influential\nconstitutes a serious challenge for statistical modeling. Indeed, the\ninstability of the correlations between these factors makes it practically\nimpossible to identify the joint laws needed to construct scenarios.\nFortunately, spectacular advances in Deep Learning field in recent years have\ngiven rise to GANs. GANs are a type of generative machine learning models that\nproduce new data samples with the same characteristics as a training data\ndistribution in an unsupervised way, avoiding data assumptions and human\ninduced biases. In this work, we are exploring the use of GANs for synthetic\nfinancial scenarios generation. This pilot study is the result of a\ncollaboration between Fujitsu and Advestis and it will be followed by a\nthorough exploration of the use cases that can benefit from the proposed\nsolution. We propose a GANs-based algorithm that allows the replication of\nmultivariate data representing several properties (including, but not limited\nto, price, market capitalization, ESG score, controversy score,. . .) of a set\nof stocks. This approach differs from examples in the financial literature,\nwhich are mainly focused on the reproduction of temporal asset price scenarios.\nWe also propose several metrics to evaluate the quality of the data generated\nby the GANs. This approach is well fit for the generation of scenarios, the\ntime direction simply arising as a subsequent (eventually conditioned)\ngeneration of data points drawn from the learned distribution. Our method will\nallow to simulate high dimensional scenarios (compared to $\\lesssim10$ features\ncurrently employed in most recent use cases) where network complexity is\nreduced thanks to a wisely performed feature engineering and selection.\nComplete results will be presented in a forthcoming study.\n"
    },
    {
        "paper_id": 2209.03936,
        "authors": "Alexander M. Petersen",
        "title": "Shift in house price estimates during COVID-19 reveals effect of crisis\n  on collective speculation",
        "comments": "Main manuscript: 16 pages (5 figures); Supplementary Information: 11\n  pages (11 figures, 3 Tables)",
        "journal-ref": "EPJ Data Science 13, 47 (2024)",
        "doi": "10.1140/epjds/s13688-024-00488-9",
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  We exploit a city-level panel comprised of individual house price estimates\nto estimate the impact of COVID-19 on both small and big real-estate markets in\nCalifornia USA. Descriptive analysis of spot house price estimates, including\ncontemporaneous price uncertainty and 30-day price change for individual\nproperties listed on the online real-estate platform Zillow.com, together\nfacilitate quantifying both the excess valuation and valuation confidence\nattributable to this global socio-economic shock. Our quasi-experimental\npre-/post-COVID-19 design spans several years around 2020 and leverages\ncontemporaneous price estimates of rental properties - i.e., real estate\nentering the habitation market, just not for purchase (off-market) and hence\nfree of speculation - as an appropriate counterfactual to properties listed for\nsale, which are subject to on-market speculation. Combining unit-level matching\nand multivariate difference-in-difference regression approaches, we obtain\nconsistent estimates regarding the sign and magnitude of excess price growth\nobserved after the pandemic onset. Specifically, our results indicate that\nproperties listed for sale appreciated an additional 1% per month above what\nwould be expected in the absence of the pandemic. This corresponds to an excess\nannual price growth of roughly 12.7 percentage points, which accounts for more\nthan half of the actual annual price growth in 2021 observed across the studied\nregions. Simultaneously, uncertainty in price estimates decreased, signaling\nthe irrational confidence characteristic of prior asset bubbles. We explore how\nthese two trends are related to market size, local market supply and borrowing\ncosts, which altogether lend support for the counterintuitive roles of\nuncertainty and interruptions in decision-making.\n"
    },
    {
        "paper_id": 2209.03998,
        "authors": "David Winkelmann, Frederik Tolkmitt, Matthias Ulrich, Michael R\\\"omer",
        "title": "Integrated storage assignment for an e-grocery fulfilment centre:\n  Accounting for day-of-week demand patterns",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper, we deal with a storage assignment problem arising in a\nfulfilment centre of a major European e-grocery retailer. The centre can be\ncharacterised as a hybrid warehouse consisting of a highly efficient and\npartially automated fast-picking area designed as a pick-and-pass system with\nmultiple stations, and a picker-to-parts area. The storage assignment problem\nconsidered in this paper comprises the decisions to select the products to be\nallocated to the fast-picking area, the assignment of the products to picking\nstations and the determination of a shelf within the assigned station. The\nobjective is to achieve a high level of picking efficiency while respecting\nstation workload balancing and precedence order constraints. We propose to\nsolve this three-level problem using an integrated MILP model. In computational\nexperiments with real-world data, we show that using the proposed integrated\napproach yields significantly better results than a sequential approach in\nwhich the selection of products to be included in the fast-picking area is\nsolved before assigning station and shelf. Furthermore, we provide an extension\nto the integrated storage assignment model that explicitly accounts for\nwithin-week demand variation. In a set of experiments with\nday-of-week-dependent demands we show that while a storage assignment that is\nbased on average demand figures tends to exhibit a highly imbalanced workload\non certain days of the week, the augmented model yields storage assignments\nthat are well balanced on each day of the week without compromising the quality\nof the solutions in terms of picking efficiency.\n"
    },
    {
        "paper_id": 2209.04001,
        "authors": "Ludovic Tangpi and Shichun Wang",
        "title": "Optimal Bubble Riding: A Mean Field Game with Varying Entry Times",
        "comments": "54 pages, 3 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Recent financial bubbles such as the emergence of cryptocurrencies and \"meme\nstocks\" have gained increasing attention from both retail and institutional\ninvestors. In this paper, we propose a game-theoretic model on optimal\nliquidation in the presence of an asset bubble. Our setup allows the influx of\nplayers to fuel the price of the asset. Moreover, traders will enter the market\nat possibly different times and take advantage of the uptrend at the risk of an\ninevitable crash. In particular, we consider two types of crashes: an\nendogenous burst which results from excessive selling, and an exogenous burst\nwhich cannot be anticipated and is independent from the actions of the traders.\n  The popularity of asset bubbles suggests a large-population setting, which\nnaturally leads to a mean field game (MFG) formulation. We introduce a class of\nMFGs with varying entry times. In particular, an equilibrium will depend on the\nentry-weighted average of conditional optimal strategies. To incorporate the\nexogenous burst time, we adopt the method of progressive enlargement of\nfiltrations. We prove existence of MFG equilibria using the weak formulation in\na generalized setup, and we show that the equilibrium strategy can be\ndecomposed into before-and-after-burst segments, each part containing only the\nmarket information. We also perform numerical simulations of the solution,\nwhich allow us to provide some intriguing results on the relationship between\nthe bubble burst and equilibrium strategies.\n"
    },
    {
        "paper_id": 2209.04153,
        "authors": "Aur\\'elien Alfonsi (MATHRISK, CERMICS), Bernard Lapeyre (MATHRISK,\n  CERMICS), J\\'er\\^ome Lelong (DAO)",
        "title": "How many inner simulations to compute conditional expectations with\n  least-square Monte Carlo?",
        "comments": "Methodology and Computing in Applied Probability, In press",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The problem of computing the conditional expectation E[f (Y)|X] with\nleast-square Monte-Carlo is of general importance and has been widely studied.\nTo solve this problem, it is usually assumed that one has as many samples of Y\nas of X. However, when samples are generated by computer simulation and the\nconditional law of Y given X can be simulated, it may be relevant to sample K\n$\\in$ N values of Y for each sample of X. The present work determines the\noptimal value of K for a given computational budget, as well as a way to\nestimate it. The main take away message is that the computational gain can be\nall the more important that the computational cost of sampling Y given X is\nsmall with respect to the computational cost of sampling X. Numerical\nillustrations on the optimal choice of K and on the computational gain are\ngiven on different examples including one inspired by risk management.\n"
    },
    {
        "paper_id": 2209.04264,
        "authors": "Felix Polyakov",
        "title": "Biology-inspired geometric representation of probability and\n  applications to completion and options' pricing",
        "comments": "31 pages, 19 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  Geometry constitutes a core set of intuitions present in all humans,\nregardless of their language or schooling [1]. Could brain's built in machinery\nfor processing geometric information take part in uncertainty representation?\nFor decades already traders have been citing the price of uncertainty based FX\noptional contracts in terms of implied volatility, a dummy variable related to\nthe standard deviation, instead of pricing with units of money. This work\nintroduces a methodology for geometric representation of probability in terms\nof implied volatility and attempts to find ways to approximate certain\nprobability distributions using intuitive geometric symmetry. In particular, it\nis shown how any probability distribution supported on $\\mathbb{R}_{+}$ and\nhaving finite expectation may be represented with a planar curve whose\ngeometric characteristics can be further analyzed. Log-normal distributions are\nrepresented with circles centered at the origin. Certain non-log-normal\ndistributions with bell-shaped density profiles are represented by curves that\ncan be closely approximated with circles whose centers are translated away from\nthe origin. Only three points are needed to define a circle while it represents\nthe candidate probability density approximating the distribution along the\nentire $\\mathbb{R}_{+}$. Just three numbers: scaling and translations along the\n$x$ and $y$ axes map one circle to another. It is possible to introduce\nequivalence classes whose member distributions can be obtained by transitive\nactions of geometric transformations on any of corresponding representations.\nApproximate completion of probability with non-circular shapes and cases when\nprobability is supported outside of $\\mathbb{R}_{+}$ are considered too.\nProposed completion of implied volatility is compared to the vanna-volga\nmethod.\n"
    },
    {
        "paper_id": 2209.04385,
        "authors": "Kanis Saengchote",
        "title": "Cryptocurrency bubbles, the wealth effect, and non-fungible token\n  prices: Evidence from metaverse LAND",
        "comments": "14 pages, 4 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
        "abstract": "  The rapid rise of cryptocurrency prices led to concerns (e.g. the Financial\nStability Board) that this wealth accumulation could detrimentally spill over\ninto other parts of the economy, but evidence is limited. We exploit the\ntendency for metaverses to issue their own cryptocurrencies along with\nnon-fungible tokens (NFTs) representing virtual real estate ownership (LAND) to\nprovide evidence of the wealth effect. Cryptocurrency prices and their\ncorresponding real estate prices are highly correlated (more than 0.96), and\ncryptocurrency prices Granger cause LAND prices. This metaverse bubble\nreminisces the 1920s American real estate bubble that preceded the 1929 stock\nmarket crash.\n"
    },
    {
        "paper_id": 2209.04431,
        "authors": "Iraj Daizadeh",
        "title": "Singular Secular Kuznets-like Period Realized Amid Industrial\n  Transformation in US FDA Medical Devices: A Perspective on Innovation from\n  1976 to 2020",
        "comments": null,
        "journal-ref": "Expert Review of Medical Devices 2022",
        "doi": "10.1080/17434440.2022.2139919",
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Introduction: Since inception, the United States (US) Food and Drug\nAdministration (FDA) has kept a robust record of regulated medical devices\n(MDs). Based on these data, can we gain insight into the innovation dynamics of\nthe industry, including the potential for industrial transformation? Areas\nCovered: Using Premarket Notifications (PMNs) and Approvals (PMAs) data, it is\nshown that from 1976 to 2020 the total composite (PMN + PMA) metric follows a\nsingle secular period: 20.5 years (applications peak-to-peak: 1992-2012;\ntrough: 2002) and 26.5 years (registrations peak to peak: 1992 to 2019; trough:\n2003), with a peak to trough relative percentage difference of 24% and 28%,\nrespectively. Importantly, PMNs and PMAs independently present as an inverse\nstructure. Expert Opinion: The evidence suggests: MD innovation is driven by a\nsingular secular Kutnets-like cyclic phenomenon (independent of economic\ncrises) derived from a fundamental shift from simple (PMNs) to complex (PMAs)\nMDs. Portentously, while the COVID-19 crisis may not affect the overriding\ndynamic, the anticipated yet significant (~25%) MD innovation drop may be\npotentially attenuated with attentive measures by MD stakeholders. Limitations\nof this approach and further thoughts complete this perspective.\n"
    },
    {
        "paper_id": 2209.04609,
        "authors": "Mandeep Singh Rai",
        "title": "Revisiting QUAD ambition in the Indo Pacific leveraging Space and Cyber\n  Domain",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  In the backdrop of growing power transitions and rise of strategic high tide\nin the world, the Indo Pacific Region (IPR) has emanated as an area which is\nlikely to witness increased development & enhanced security cooperation through\nmilitarization. With China trying to be at the seat of the Global leadership,\nUS & its allies in the Indo pacific are aiming at working together, finding\nmutually beneficial areas of functional cooperation and redefining the canvas\nof security. This purpose of this paper is to analyze an informal alliance,\nQuadrilateral Security dialogue (QUAD) in its present form with the\ngeostrategic landscape in Indo pacific and present recommendations to\nstrengthen collaboration & response capability.\n"
    },
    {
        "paper_id": 2209.0462,
        "authors": "Garima Agrawal, Anindya Goswami",
        "title": "A semi-Markovian approach to model the tick-by-tick dynamics of stock\n  price",
        "comments": "24 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We model the stock price dynamics through a semi-Markov process obtained\nusing a Poisson random measure. We establish the existence and uniqueness of\nthe classical solution of a non-homogeneous terminal value problem and we show\nthat the expected value of stock price at horizon can be obtained as a\nclassical solution of a linear partial differential equation that is a special\ncase of the terminal value problem studied in this paper. We further analyze\nthe market making problem using the point of view of an agent who posts the\nlimit orders at the best price available. We use the dynamic programming\nprinciple to obtain a HJB equation. In no-risk aversion case, we obtain the\nvalue function as a classical solution of a linear pde and derive the\nexpressions for optimal controls by solving the HJB equation.\n"
    },
    {
        "paper_id": 2209.04685,
        "authors": "Xiaochuan Pang, Shushang Zhu, Xueting Cui, Jiali Ma",
        "title": "Systemic Risk of Optioned Portfolios: Controllability and Optimization",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We investigate the portfolio selection problem against the systemic risk\nwhich is measured by CoVaR. We first demonstrate that the systemic risk of pure\nstock portfolios is essentially uncontrollable due to the contagion effect and\nthe seesaw effect. Next, we prove that it is necessary and sufficient to\nintroduce options to make the systemic risk controllable by the correlation\nhedging and the extreme loss hedging. In addition to systemic risk control, we\nshow that using options can also enhance return-risk performance. Then, with a\nreasonable approximation of the conditional distribution of optioned\nportfolios, we show that the portfolio optimization problem can be formulated\nas a second-order cone program (SOCP) that allows for efficient computation.\nFinally, we carry out comprehensive simulations and empirical tests to\nillustrate the theoretical findings and the performance of our method.\n"
    },
    {
        "paper_id": 2209.04686,
        "authors": "Thitithep Sitthiyot, Kanyarat Holasut",
        "title": "On the Evaluation of Skill in Binary Forecast",
        "comments": "22 pages, 1 figure, 8 tables",
        "journal-ref": "Thailand and the World Economy (2022)",
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  A good prediction is very important for scientific, economic, and\nadministrative purposes. It is therefore necessary to know whether a predictor\nis skillful enough to predict the future. Given the increased reliance on\npredictions in various disciplines, prediction skill index (PSI) is devised.\nTwenty-four numerical examples are used to demonstrate how the PSI method\nworks. The results show that the PSI awards not only the same score for random\nprediction and always predicting the same value but also nontrivial scores for\ncorrect prediction of rare or extreme events. Moreover, the PSI can distinguish\nthe difference between the perfect forecast of rare or extreme events and that\nof random events by awarding different skill scores while other conventional\nmethods cannot and award the same score. The data on growth of real gross\ndomestic product forecast of the Bank of Thailand between 2000 and 2019 are\nalso used to demonstrate how the PSI evaluates skill of the forecaster in\npractice.\n"
    },
    {
        "paper_id": 2209.04764,
        "authors": "Adam Graham-Squire and David McCune",
        "title": "A Mathematical Analysis of the 2022 Alaska Special Election for US House",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The August 2022 Alaska Special Election for US House contained many\ninteresting features from the perspective of social choice theory. This\nelection used instant runoff voting (often referred to as ranked choice voting)\nto elect a winner, and many of the weaknesses of this voting method were on\ndisplay in this election. For example, the Condorcet winner is different from\nthe instant runoff winner, and the election demonstrated a monotonicity\nparadox. The election also demonstrated a no show paradox; as far as we are\naware, this election represents the first document American ranked choice\nelection to demonstrate this paradox.\n"
    },
    {
        "paper_id": 2209.04843,
        "authors": "Darija Barak, Edoardo Gallo, Alastair Langtry",
        "title": "The economic and health impacts of contact tracing and quarantine\n  programs",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Contact tracing and quarantine programs have been one of the leading\nNon-Pharmaceutical Interventions against COVID-19. Some governments have relied\non mandatory programs, whereas others embrace a voluntary approach. However,\nthere is limited evidence on the relative effectiveness of these different\napproaches. In an interactive online experiment conducted on 731 subjects\nrepresentative of the adult US population in terms of sex and region of\nresidence, we find there is a clear ranking. A fully mandatory program is\nbetter than an optional one, and an optional system is better than no\nintervention at all. The ranking is driven by reductions in infections, while\neconomic activity stays unchanged. We also find that political conservatives\nhave higher infections and levels of economic activity, and they are less\nlikely to participate in the contact tracing program.\n"
    },
    {
        "paper_id": 2209.04976,
        "authors": "Erhan Bayraktar, Tao Chen",
        "title": "Data-Driven Nonparametric Robust Control under Dependence Uncertainty",
        "comments": "25 pages, 1 figure",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We consider a multi-period stochastic control problem where the multivariate\ndriving stochastic factor of the system has known marginal distributions but\nuncertain dependence structure. To solve the problem, we propose to implement\nthe nonparametric adaptive robust control framework. We aim to find the optimal\ncontrol against the worst-case copulae in a sequence of shrinking uncertainty\nsets which are generated from continuously observing the data. Then, we use a\nstochastic gradient descent ascent algorithm to numerically handle the\ncorresponding high dimensional dynamic inf-sup optimization problem. We present\nthe numerical results in the context of utility maximization and show that the\ncontroller benefits from knowing more information about the uncertain model.\n"
    },
    {
        "paper_id": 2209.05081,
        "authors": "Jordan Roulleau-Pasdeloup",
        "title": "Analyzing Linear DSGE models: the Method of Undetermined Markov States",
        "comments": "40 pages, 4 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  I show that a class of Linear DSGE models with one endogenous state variable\ncan be represented as a three-state Markov chain. I develop a new analytical\nsolution method based on this representation, which amounts to solving for a\nvector of Markov states and one transition probability. These two objects\nconstitute sufficient statistics to compute in closed form objects that have\nroutinely been computed numerically: impulse response function, cumulative sum,\npresent discount value multiplier. I apply the method to a standard New\nKeynesian model that features optimal monetary policy with commitment.\n"
    },
    {
        "paper_id": 2209.05089,
        "authors": "D\\'avid Csercsik and Anne Neumann",
        "title": "Solidarity in natural gas storage: A potential allocation mechanism of\n  stored quantities among several players during times of crisis",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  The recently experienced disruptions in the EU's energy supply pointed out\nthat supply crises pose a real thread and the member states must be better\nprepared do deal with the related challenges. According to the current\npractice, member states fill their gas storages independently, while it is not\nclear how solidarity could be put into practice in the future, i.e. how the\naccumulated reserves of one or more members may be potentially redistributed to\nhelp others in need. In this paper we propose some possible guidelines for a\npotential solidarity framework, and formalize a game-theoretic model in order\nto capture the basic features of the problem, considering the related\nuncertainty of the future conditions related to gas storage levels and possible\ntransmission bottlenecks as well. The proposed mechanism of supply-security\nrelated cooperation is based on voluntary participation, and may contribute to\nthe more efficient utilization of storage capacities. Via the computational\nmodel we demonstrate the operation of the proposed framework on a simple\nexample and show that under the assumption of risk-averse participants, the\nconcept exhibits potential.\n"
    },
    {
        "paper_id": 2209.05129,
        "authors": "Davood Qorbani",
        "title": "Exploitees vs. Exploiters: Dynamics of Exploitation",
        "comments": "11 pages, conference paper presented at the 40th International\n  Conference of the System Dynamics Society (2022, July 19 - 22), Frankfurt,\n  Germany. (https://proceedings.systemdynamics.org/2022/)",
        "journal-ref": "40th International Conference of the System Dynamics Society\n  (2022, July 19 - 22), Frankfurt, Germany",
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
        "abstract": "  Either saying that the market is structured to promote workforce exploitation\nin some sections or the people who participate there benefit from the existing\nstructure and exploit, exploitation happens - systematically or\nopportunistically. This research presents a perspective on how workforce\nexploitation may occur when vulnerable groups voluntarily seek employment in\nthe labor market.\n"
    },
    {
        "paper_id": 2209.05211,
        "authors": "Georgios I. Papayiannis and Athanasios N. Yannacopoulos",
        "title": "Convex Risk Measures for the Aggregation of Multiple Information Sources\n  and Applications in Insurance",
        "comments": "32 pages",
        "journal-ref": "Scandinavian Actuarial Journal, 2018:9, 792-822",
        "doi": "10.1080/03461238.2018.1461129",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We propose a novel class of convex risk measures, based on the concept of the\nFr\\'echet mean, designed in order to handle uncertainty which arises from\nmultiple information sources regarding the risk factors of interest. The\nproposed risk measures robustly characterize the exposure of the firm, by\nfiltering out appropriately the partial information available in individual\nsources into an aggregate model for the risk factors of interest. Importantly,\nthe proposed risks can be expressed in closed analytic forms allowing for\ninteresting qualitative interpretations as well as comparative statics and thus\nfacilitate their use in the everyday risk management process of the insurance\nfirms. The potential use of the proposed risk measures in insurance is\nillustrated by two concrete applications, capital risk allocation and premia\ncalculation under uncertainty.\n"
    },
    {
        "paper_id": 2209.05225,
        "authors": "Jiong Liu and R.A. Serota",
        "title": "Rethinking Generalized Beta Family of Distributions",
        "comments": "21 pages, 11 figyes, 2 tables",
        "journal-ref": "Eur. Phys. J. B (2023) 96: 24",
        "doi": "10.1140/epjb/s10051-023-00485-3",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We approach the Generalized Beta (GB) family of distributions using a\nmean-reverting stochastic differential equation (SDE) for a power of the\nvariable, whose steady-state (stationary) probability density function (PDF) is\na modified GB (mGB) distribution. The SDE approach allows for a lucid\nexplanation of Generalized Beta Prime (GB2) and Generalized Beta (GB1) limits\nof GB distribution and, further down, of Generalized Inverse Gamma (GIGa) and\nGeneralized Gamma (GGa) limits, as well as describe the transition between the\nlatter two. We provide an alternative form to the \"traditional\" GB PDF to\nunderscore that a great deal of usefulness of GB distribution lies in its\nallowing a long-range power-law behavior to be ultimately terminated at a\nfinite value. We derive the cumulative distribution function (CDF) of the\n\"traditional\" GB, which belongs to the family generated by the regularized beta\nfunction and is crucial for analysis of the tails of the distribution. We\nanalyze fifty years of historical data on realized market volatility,\nspecifically for S\\&P500, as a case study of the use of GB/mGB distributions\nand show that its behavior is consistent with that of negative Dragon Kings.\n"
    },
    {
        "paper_id": 2209.0536,
        "authors": "Bernhard K Meister",
        "title": "Meta-CTA Trading Strategies and Rational Market Failures",
        "comments": "5 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Investors trade shifting prices, portfolio values, and in turn their ability\nto borrow. Concentrated ownership, high price impact and low collateral\nrequirements are propitious for arbitrage.\n"
    },
    {
        "paper_id": 2209.05383,
        "authors": "Paul Trust, Ahmed Zahran, Rosane Minghim",
        "title": "Weak Supervision in Analysis of News: Application to Economic Policy\n  Uncertainty",
        "comments": "A key author who is the principal investigator of the work was not\n  consulted in uploading the pre-print and she requested that this publication\n  be revoked from arxiv",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  The need for timely data analysis for economic decisions has prompted most\neconomists and policy makers to search for non-traditional supplementary\nsources of data. In that context, text data is being explored to enrich\ntraditional data sources because it is easy to collect and highly abundant. Our\nwork focuses on studying the potential of textual data, in particular news\npieces, for measuring economic policy uncertainty (EPU). Economic policy\nuncertainty is defined as the public's inability to predict the outcomes of\ntheir decisions under new policies and future economic fundamentals.\nQuantifying EPU is of great importance to policy makers, economists, and\ninvestors since it influences their expectations about the future economic\nfundamentals with an impact on their policy, investment and saving decisions.\nMost of the previous work using news articles for measuring EPU are either\nmanual or based on a simple keyword search. Our work proposes a machine\nlearning based solution involving weak supervision to classify news articles\nwith regards to economic policy uncertainty. Weak supervision is shown to be an\nefficient machine learning paradigm for applying machine learning models in low\nresource settings with no or scarce training sets, leveraging domain knowledge\nand heuristics. We further generated a weak supervision based EPU index that we\nused to conduct extensive econometric analysis along with the Irish\nmacroeconomic indicators to validate whether our generated index foreshadows\nweaker macroeconomic performance\n"
    },
    {
        "paper_id": 2209.05387,
        "authors": "Valentin Zieglmeier and Maren Gierlich-Joas and Alexander Pretschner",
        "title": "Increasing Employees' Willingness to Share: Introducing Appeal\n  Strategies for People Analytics",
        "comments": "Peer-reviewed version accepted for publication in the proceedings of\n  the 13th International Conference on Software Business (ICSOB 2022)",
        "journal-ref": null,
        "doi": "10.1007/978-3-031-20706-8_15",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Increasingly digital workplaces enable advanced people analytics (PA) that\ncan improve work, but also implicate privacy risks for employees. These systems\noften depend on employees sharing their data voluntarily. Thus, to leverage the\npotential benefits of PA, companies have to manage employees' disclosure\ndecision. In literature, we identify two main strategies: increase awareness or\napply appeal strategies. While increased awareness may lead to more\nconservative data handling, appeal strategies can promote data sharing. Yet, to\nour knowledge, no systematic overview of appeal strategies for PA exists. Thus,\nwe develop an initial taxonomy of strategies based on a systematic literature\nreview and interviews with 18 experts. We describe strategies in the dimensions\nof values, benefits, and incentives. Thereby, we present concrete options to\nincrease the appeal of PA for employees.\n"
    },
    {
        "paper_id": 2209.05559,
        "authors": "Berend Jelmer Dirk Gort, Xiao-Yang Liu, Xinghang Sun, Jiechao Gao,\n  Shuaiyu Chen, Christina Dan Wang",
        "title": "Deep Reinforcement Learning for Cryptocurrency Trading: Practical\n  Approach to Address Backtest Overfitting",
        "comments": null,
        "journal-ref": null,
        "doi": "10.48550/arXiv.2209.05559",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Designing profitable and reliable trading strategies is challenging in the\nhighly volatile cryptocurrency market. Existing works applied deep\nreinforcement learning methods and optimistically reported increased profits in\nbacktesting, which may suffer from the false positive issue due to overfitting.\nIn this paper, we propose a practical approach to address backtest overfitting\nfor cryptocurrency trading using deep reinforcement learning. First, we\nformulate the detection of backtest overfitting as a hypothesis test. Then, we\ntrain the DRL agents, estimate the probability of overfitting, and reject the\noverfitted agents, increasing the chance of good trading performance. Finally,\non 10 cryptocurrencies over a testing period from 05/01/2022 to 06/27/2022\n(during which the crypto market crashed two times), we show that the less\noverfitted deep reinforcement learning agents have a higher return than that of\nmore overfitted agents, an equal weight strategy, and the S&P DBM Index (market\nbenchmark), offering confidence in possible deployment to a real market.\n"
    },
    {
        "paper_id": 2209.05592,
        "authors": "Elizabeth Kassab Sfeir",
        "title": "Impact of interpersonal influences on Employee engagement and\n  Psychological contract: Effects of guanxi, wasta, jeitinho, blat and pulling\n  strings",
        "comments": "2 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  This study puts forward a conceptual model linking interpersonal influences'\nimpact on Employee Engagement, Psychological contracts, and Human Resource\nPractices. It builds on human and social capital, as well as the social\nexchange theory (SET), projecting how interpersonal influences can impact the\npsychological contract (PC) and employee engagement (EE) of employees. This\nresearch analyzes the interpersonal influences of Wasta in the Middle East,\nGuanxi in China, Jeitinho in Brazil, Blat in Russia, and Pulling Strings in\nEngland. Interpersonal influences draw upon nepotism, favoritism, and\ncorruption in organizations in many countries. This paper draws on the\nqualitative methods of analyzing previous theories. It uses the Model Paper\nmethod of predicting relationships by examining the question of how do\ninterpersonal influences impact employee engagement and psychological\ncontract?. It is vital to track the effects of interpersonal influences on PC\nand EE, acknowledging that the employer can either empower or disengage our\nhuman capital.\n"
    },
    {
        "paper_id": 2209.05684,
        "authors": "Jieun Lee",
        "title": "Moral Hazard on Productivity Among Work-From-Home Workers Amid the\n  COVID-19 Pandemic",
        "comments": null,
        "journal-ref": null,
        "doi": "10.13140/RG.2.2.13099.52007",
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  After the outbreak of COVID 19, firms appear to monitor Work From Home (WFH)\nworkers more than ever out of anxiety that workers may shirk at home or\nimplement moral hazard at home. Using the Survey of Working Arrangements and\nAttitudes (SWAA, Barrero et al., 2021), the evidence of WFH workers' ex post\nmoral hazard as well as its specific aspects are examined. The results show\nthat the ex post moral hazard among the WFH workers is generally found.\nInterestingly, however, the moral hazard on specific type of productivity,\nefficiency, is not detected for the workers at firms with WFH friendly policy\nfor long term. Moreover, the advantages & challenges for the WFH culture report\nthat workers with health or disability issues improve their productivity,\nwhereas certain conditions specific to the WFH environment must be met.\n"
    },
    {
        "paper_id": 2209.05916,
        "authors": "Julia Hatamyar",
        "title": "Workplace Breastfeeding Legislation and Labor Market Outcomes",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  This paper studies the effects of legislation mandating workplace\nbreastfeeding amenities on various labor market outcomes. Using the Panel Study\nof Income Dynamics, I implement both a traditional fixed-effects event study\nframework and the Interaction-Weighted event study technique proposed by Sun &\nAbraham (2020). I find that workplace breastfeeding legislation increases the\nlikelihood of female labor force participation by 4.2 percentage points in the\ntwo years directly following implementation. Female labor force participation\nremains higher than before implementation in subsequent years, but the\nsignificant effect does not persist. Using the CDC's Infant Feeding Practices\nSurvey II, I then show that breastfeeding women who do not live in states with\nworkplace breastfeeding supportive legislation are 3.9 percentage points less\nlikely to work, supporting the PSID findings. The legislation mainly impacts\nwhite women. I find little effect on labor income or work intensity.\n"
    },
    {
        "paper_id": 2209.05918,
        "authors": "Aur\\'elien Alfonsi and Nerea Vadillo",
        "title": "A stochastic volatility model for the valuation of temperature\n  derivatives",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper develops a new stochastic volatility model for the temperature\nthat is a natural extension of the Ornstein-Uhlenbeck model proposed by Benth\nand Benth (2007). This model allows to be more conservative regarding extreme\nevents while keeping tractability. We give a method based on Conditional Least\nSquares to estimate the parameters on daily data and estimate our model on\neight major European cities. We then show how to calculate efficiently the\naverage payoff of weather derivatives both by Monte-Carlo and Fourier transform\ntechniques. This new model allows to better assess the risk related to\ntemperature volatility.\n"
    },
    {
        "paper_id": 2209.06276,
        "authors": "Yuan Hu and W. Brent Lindquist and Svetlozar T. Rachev",
        "title": "ESG-valued discrete option pricing in complete markets",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  We consider option pricing using replicating binomial trees, with a two fold\npurpose. The first is to introduce ESG valuation into option pricing. We\nexplore this in a number of scenarios, including enhancement of yield due to\ntrader information and the impact of the past history of a market driver. The\nsecond is to emphasize the use of discrete dynamic pricing, rather than\ncontinuum models, as the natural model that governs actual market practice. We\nfurther emphasize that discrete option pricing models must use discrete\ncompounding (such as risk-free rate compounding of $1+r_f \\Delta t$) rather\nthan continuous compounding (such as $e^{r_f \\Delta t})$.\n"
    },
    {
        "paper_id": 2209.06476,
        "authors": "D Barrera (UNIANDES), S Cr\\'epey (LPSM, UPCit\\'e), E Gobet (CMAP, X),\n  Hoang-Dung Nguyen (LPSM, UPCit\\'e), B Saadeddine (UPS)",
        "title": "Learning Value-at-Risk and Expected Shortfall",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We propose a non-asymptotic convergence analysis of a two-step approach to\nlearn a conditional value-at-risk (VaR) and expected shortfall (ES) in a\nnonparametric setting using Rademacher and Vapnik-Chervonenkis bounds. Our\napproach for the VaR is extended to the problem of learning at once multiple\nVaRs corresponding to different quantile levels. This results in efficient\nlearning schemes based on neural network quantile and least-squares\nregressions. An a posteriori Monte Carlo (non-nested) procedure is introduced\nto estimate distances to the ground-truth VaR and ES without access to the\nlatter. This is illustrated using numerical experiments in a Gaussian toy-model\nand a financial case-study where the objective is to learn a dynamic initial\nmargin.\n"
    },
    {
        "paper_id": 2209.06485,
        "authors": "Ludovic Goudenege, Andrea Molent, Antonino Zanette",
        "title": "Computing XVA for American basket derivatives by Machine Learning\n  techniques",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Total value adjustment (XVA) is the change in value to be added to the price\nof a derivative to account for the bilateral default risk and the funding\ncosts. In this paper, we compute such a premium for American basket derivatives\nwhose payoff depends on multiple underlyings. In particular, in our model,\nthose underlying are supposed to follow the multidimensional Black-Scholes\nstochastic model. In order to determine the XVA, we follow the approach\nintroduced by Burgard and Kjaer \\cite{burgard2010pde} and afterward applied by\nArregui et al. \\cite{arregui2017pde,arregui2019monte} for the one-dimensional\nAmerican derivatives. The evaluation of the XVA for basket derivatives is\nparticularly challenging as the presence of several underlings leads to a\nhigh-dimensional control problem. We tackle such an obstacle by resorting to\nGaussian Process Regression, a machine learning technique that allows one to\naddress the curse of dimensionality effectively. Moreover, the use of numerical\ntechniques, such as control variates, turns out to be a powerful tool to\nimprove the accuracy of the proposed methods. The paper includes the results of\nseveral numerical experiments that confirm the goodness of the proposed\nmethodologies.\n"
    },
    {
        "paper_id": 2209.06624,
        "authors": "Shehryar Munir, Farah Said, Umar Taj and Maida Zafar",
        "title": "Digital 'nudges' to increase childhood vaccination compliance: Evidence\n  from Pakistan",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  Pakistan has one of the lowest rates of routine childhood immunization\nworldwide, with only two-thirds of infants 2 years or younger being fully\nimmunized (Pakistan Demographic and Health Survey 2019). Government-led,\nroutine information campaigns have been disrupted over the last few years due\nto the on-going COVID-19 pandemic. We use data from a mobile-based campaign\nthat involved sending out short audio dramas emphasizing the importance of\nvaccines and parental responsibilities in Quetta, Pakistan. Five out of eleven\nareas designated by the provincial government were randomly selected to receive\nthe audio calls with a lag of 3 months and form the comparison group in our\nanalysis. We conduct a difference-in-difference analysis on data collected by\nthe provincial Department of Health in the 3-month study and find a significant\n30% increase over the comparison mean in the number of fully vaccinated\nchildren in campaign areas on average. We find evidence that suggests\nvaccination increased in UCs where vaccination centers were within a short\n30-minute travel distance, and that the campaign was successful in changing\nperceptions about vaccination and reliable sources of advice. Results highlight\nthe need for careful design and targeting of similar soft behavioral change\ncampaigns, catering to the constraints and abilities of the context.\n"
    },
    {
        "paper_id": 2209.06654,
        "authors": "Brian P. Hanley, Steve Keen",
        "title": "The Sign of Risk for Present Value of Future Losses",
        "comments": "4 pages, 1 figure",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  In the ongoing debate over discount rates and climate change, William\nNordhaus has championed a higher discount rate to account for risk. Nicholas\nStern has championed a lower rate. Here we prove that in the case of a stream\nof future losses, risk can only be represented by a lower discount rate, never\na higher one.\n"
    },
    {
        "paper_id": 2209.06902,
        "authors": "Kristian Buchardt, Christian Furrer, Oliver Lunding Sandqvist",
        "title": "Transaction time models in multi-state life insurance",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In life insurance contracts, benefits and premiums are typically paid\ncontingent on the biometric state of the insured. Due to delays between the\noccurrence, reporting, and settlement of changes to the biometric state, the\nstate process is not fully observable in real-time. This fact implies that the\nclassic multi-state models for the biometric state of the insured are not able\nto describe the development of the policy in real-time, which encompasses\nhandling of incurred-but-not-reported and reported-but-not-settled claims. We\ngive a fundamental treatment of the problem in the setting of continuous-time\nmulti-state life insurance by introducing a new class of models: transaction\ntime models. The relation between the transaction time model and the classic\nmodel is studied and a result linking the present values in the two models is\nderived. The results and their practical implications are illustrated for\ndisability coverages, where we obtain explicit expressions for the transaction\ntime reserve in specific models.\n"
    },
    {
        "paper_id": 2209.07092,
        "authors": "Kan Chen, Tuoyuan Cheng",
        "title": "Measuring Tail Risks",
        "comments": null,
        "journal-ref": null,
        "doi": "10.1016/j.jfds.2022.11.001",
        "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
        "abstract": "  Value at risk (VaR) and expected shortfall (ES) are common high\nquantile-based risk measures adopted in financial regulations and risk\nmanagement. In this paper, we propose a tail risk measure based on the most\nprobable maximum size of risk events (MPMR) that can occur over a length of\ntime. MPMR underscores the dependence of the tail risk on the risk management\ntime frame. Unlike VaR and ES, MPMR does not require specifying a confidence\nlevel. We derive the risk measure analytically for several well-known\ndistributions. In particular, for the case where the size of the risk event\nfollows a power law or Pareto distribution, we show that MPMR also scales with\nthe number of observations $n$ (or equivalently the length of the time\ninterval) by a power law, $\\text{MPMR}(n) \\propto n^{\\eta}$, where $\\eta$ is\nthe scaling exponent. The scale invariance allows for reasonable estimations of\nlong-term risks based on the extrapolation of more reliable estimations of\nshort-term risks. The scaling relationship also gives rise to a robust and\nlow-bias estimator of the tail index (TI) $\\xi$ of the size distribution, $\\xi\n= 1/\\eta$. We demonstrate the use of this risk measure for describing the tail\nrisks in financial markets as well as the risks associated with natural hazards\n(earthquakes, tsunamis, and excessive rainfall).\n"
    },
    {
        "paper_id": 2209.07335,
        "authors": "Saeed Nosratabadi, Roya Khayer Zahed, Vadim Vitalievich Ponkratov, and\n  Evgeniy Vyacheslavovich Kostyrin",
        "title": "Artificial Intelligence Models and Employee Lifecycle Management: A\n  Systematic Literature Review",
        "comments": null,
        "journal-ref": null,
        "doi": "10.2478/orga-2022-0012",
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Background/Purpose: The use of artificial intelligence (AI) models for\ndata-driven decision-making in different stages of employee lifecycle (EL)\nmanagement is increasing. However, there is no comprehensive study that\naddresses contributions of AI in EL management. Therefore, the main goal of\nthis study was to address this theoretical gap and determine the contribution\nof AI models to EL. Methods: This study applied the PRISMA method, a systematic\nliterature review model, to ensure that the maximum number of publications\nrelated to the subject can be accessed. The output of the PRISMA model led to\nthe identification of 23 related articles, and the findings of this study were\npresented based on the analysis of these articles. Results: The findings\nrevealed that AL algorithms were used in all stages of EL management (i.e.,\nrecruitment, on-boarding, employability and benefits, retention, and\noff-boarding). It was also disclosed that Random Forest, Support Vector\nMachines, Adaptive Boosting, Decision Tree, and Artificial Neural Network\nalgorithms outperform other algorithms and were the most used in the\nliterature. Conclusion: Although the use of AI models in solving EL problems is\nincreasing, research on this topic is still in its infancy stage, and more\nresearch on this topic is necessary.\n"
    },
    {
        "paper_id": 2209.07411,
        "authors": "Jeong Yin Park",
        "title": "Optimal portfolio selection of many players under relative performance\n  criteria in the market model with random coefficients",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We study the optimal portfolio selection problem under relative performance\ncriteria in the market model with random coefficients from the perspective of\nmany players game theory. We consider five random coefficients which consist of\nthree market parameters which are used in the risky asset price modeling and\ntwo preference parameters which are related to risk attitude and impact of\nrelative performance. We focus on two cases; either all agents have Constant\nAbsolute Risk Aversion (CARA) risk preferences or all agents have Constant\nRelative Risk Aversion (CRRA) risk preferences for their investment\noptimization problem. For each case, we show that the forward Nash equilibrium\nand the mean field equilibrium exist for the n-agent game and the corresponding\nmean field stochastic optimal control problem, respectively. To extend the\nn-agent game to the continuum of players game, we introduce a measure dependent\nforward relative performance process and apply an optimization over controlled\ndynamics of McKean-Vlasov type. We conclude that our optimal portfolio formulas\nextend the corresponding results of the market model with constant\ncoefficients.\n"
    },
    {
        "paper_id": 2209.07415,
        "authors": "Kerstin Awiszus, Thomas Knispel, Irina Penner, Gregor Svindland,\n  Alexander Vo{\\ss}, Stefan Weber",
        "title": "Modeling and Pricing Cyber Insurance -- Idiosyncratic, Systematic, and\n  Systemic Risks",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The paper provides a comprehensive overview of modeling and pricing cyber\ninsurance and includes clear and easily understandable explanations of the\nunderlying mathematical concepts. We distinguish three main types of cyber\nrisks: idiosyncratic, systematic, and systemic cyber risks. While for\nidiosyncratic and systematic cyber risks, classical actuarial and financial\nmathematics appear to be well-suited, systemic cyber risks require more\nsophisticated approaches that capture both network and strategic interactions.\nIn the context of pricing cyber insurance policies, issues of interdependence\narise for both systematic and systemic cyber risks; classical actuarial\nvaluation needs to be extended to include more complex methods, such as\nconcepts of risk-neutral valuation and (set-valued) monetary risk measures.\n"
    },
    {
        "paper_id": 2209.07574,
        "authors": "Mengnan Song and Jiasong Wang and Suisui Su",
        "title": "Towards a Better Microcredit Decision",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Reject inference comprises techniques to infer the possible repayment\nbehavior of rejected cases. In this paper, we model credit in a brand new view\nby capturing the sequential pattern of interactions among multiple stages of\nloan business to make better use of the underlying causal relationship.\nSpecifically, we first define 3 stages with sequential dependence throughout\nthe loan process including credit granting(AR), withdrawal application(WS) and\nrepayment commitment(GB) and integrate them into a multi-task architecture.\nInside stages, an intra-stage multi-task classification is built to meet\ndifferent business goals. Then we design an Information Corridor to express\nsequential dependence, leveraging the interaction information between customer\nand platform from former stages via a hierarchical attention module controlling\nthe content and size of the information channel. In addition, semi-supervised\nloss is introduced to deal with the unobserved instances. The proposed\nmulti-stage interaction sequence(MSIS) method is simple yet effective and\nexperimental results on a real data set from a top loan platform in China show\nthe ability to remedy the population bias and improve model generalization\nability.\n"
    },
    {
        "paper_id": 2209.07621,
        "authors": "Qi Guo, Anatoliy Swishchuk and Bruno R\\'emillard",
        "title": "Multivariate Hawkes-based Models in LOB: European, Spread and Basket\n  Option Pricing",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper, we consider pricing of European options and spread options for\nHawkes-based model for the limit order book. We introduce multivariate Hawkes\nprocess and the multivariable general compound Hawkes process. Exponential\nmultivariate general compound Hawkes processes and limit theorems for them,\nnamely, LLN and FCLT, are considered then. We also consider a special case of\none-dimensional EMGCHP and its limit theorems. Option pricing with $1D$ EGCHP\nin LOB, hedging strategies, and numerical example are presented. We also\nintroduce greeks calculations for those models. Margrabe's spread options\nvaluations with Hawkes-based models for two assets and numerical example are\npresented. Also, Margrabe's spread option pricing with two $2D$ EMGCHP and\nnumerical example are included. Basket options valuations with numerical\nexample are included. We finally discuss the implied volatility and implied\norder flow. It reveals the relationship between stock volatility and the order\nflow in the limit order book system. In this way, the Hawkes-based model can\nprovide more market forecast information than the classical Black-Scholes\nmodel.\n"
    },
    {
        "paper_id": 2209.07823,
        "authors": "Joseph Jerome, Leandro Sanchez-Betancourt, Rahul Savani, Martin\n  Herdegen",
        "title": "Model-based gym environments for limit order book trading",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Within the mathematical finance literature there is a rich catalogue of\nmathematical models for studying algorithmic trading problems -- such as\nmarket-making and optimal execution -- in limit order books. This paper\nintroduces \\mbtgym, a Python module that provides a suite of gym environments\nfor training reinforcement learning (RL) agents to solve such model-based\ntrading problems. The module is set up in an extensible way to allow the\ncombination of different aspects of different models. It supports highly\nefficient implementations of vectorized environments to allow faster training\nof RL agents. In this paper, we motivate the challenge of using RL to solve\nsuch model-based limit order book problems in mathematical finance, we explain\nthe design of our gym environment, and then demonstrate its use in solving\nstandard and non-standard problems from the literature. Finally, we lay out a\nroadmap for further development of our module, which we provide as an open\nsource repository on GitHub so that it can serve as a focal point for RL\nresearch in model-based algorithmic trading.\n"
    },
    {
        "paper_id": 2209.08211,
        "authors": "Yiang Li, Xingzuo Zhou",
        "title": "Local political control in educational policy: Evidence from\n  decentralized teacher pay reform under England's local education authorities",
        "comments": "submitted to Royal Statistical Society: Series A",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  In 2012, the School Teachers' Review Body discontinued central guidance and\nallowed school discretion in determining teachers' pay in England. Meanwhile,\nlocal education authorities (LEAs) offer non-statutory teacher pay\nrecommendations to LEA-controlled schools. This study examines how LEAs'\npolitical party control determines their guidance regarding whether to adopt\nflexible performance pay or continue seniority-based pay. A regression\ndiscontinuity design is used to address the endogeneity of political control\nand educational policy-making. We find that marginally Conservative-controlled\nLEAs are more inclined to recommend market-oriented flexible pay structures.\nThe results remain robust to alternative specifications. This study reveals\nthat politics matter in England's local educational policy-making, which has\nbroad implications for future policy.\n"
    },
    {
        "paper_id": 2209.0834,
        "authors": "Michelle Gonz\\'alez Amador, Robin Cowan, Eleonora Nillesen",
        "title": "Peer Networks and Malleability of Educational Aspirations",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  Continuing education beyond the compulsory years of schooling is one of the\nmost important choices an adolescent has to make. Higher education is\nassociated with a host of social and economic benefits both for the person and\nits community. Today, there is ample evidence that educational aspirations are\nan important determinant of said choice. We implement a multilevel, networked\nexperiment in 45 Mexican high schools and provide evidence of the malleability\nof educational aspirations. We also show there exists an interdependence of\nstudents' choices and the effect of our intervention with peer networks. We\nfind that a video intervention, which combines role models and information\nabout returns to education is successful in updating students' beliefs and\nconsequently educational aspirations.\n"
    },
    {
        "paper_id": 2209.08382,
        "authors": "Viktor Stojkoski, Philipp Koch, C\\'esar A. Hidalgo",
        "title": "Multidimensional Economic Complexity and Inclusive Green Growth",
        "comments": null,
        "journal-ref": "Communications Earth & Environment volume 4, Article number: 130\n  (2023)",
        "doi": "10.1038/s43247-023-00770-0",
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  To achieve inclusive green growth, countries need to consider a multiplicity\nof economic, social, and environmental factors. These are often captured by\nmetrics of economic complexity derived from the geography of trade, thus\nmissing key information on innovative activities. To bridge this gap, we\ncombine trade data with data on patent applications and research publications\nto build models that significantly and robustly improve the ability of economic\ncomplexity metrics to explain international variations in inclusive green\ngrowth. We show that measures of complexity built on trade and patent data\ncombine to explain future economic growth and income inequality and that\ncountries that score high in all three metrics tend to exhibit lower emission\nintensities. These findings illustrate how the geography of trade, technology,\nand research combine to explain inclusive green growth.\n"
    },
    {
        "paper_id": 2209.08521,
        "authors": "Peng Liu and Yanyan Zheng",
        "title": "Precision measurement of the return distribution property of the Chinese\n  stock market index",
        "comments": null,
        "journal-ref": "Entropy 2023, 25(1), 36",
        "doi": "10.3390/e25010036",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper systematically conducts an analysis of the composite index 1-min\ndatasets over the 17-year period (2005-2021) for both the Shanghai and Shenzhen\nstock exchanges. To reveal the difference between the Chinese and the mature\nstock markets, here we precisely measure the property of return distribution of\ncomposite index over the time scale $\\Delta t$ ranging from 1 min up to almost\n4,000 min. The main findings are as follows. (1) Return distribution presents a\nleptokurtic, fat-tailed, and almost symmetrical shape, which is similar to that\nof mature markets. (2) The central part of return distribution is well\ndescribed by the symmetrical L\\'{e}vy $\\alpha$-stable process with a stability\nparameter comparable with the value of about 1.4 extracted in the U.S. stock\nmarket. (3) Return distribution can be well described by the student's\nt-distribution within a wider return range than the L\\'{e}vy $\\alpha$-stable\ndistribution. (4) Distinctively, the stability parameter shows a potential\nchange when $\\Delta t$ increases, and thus a crossover region at 15 $< \\Delta t\n<$ 60 min is observed. This is different from the finding in the U.S. stock\nmarket where a single value of about 1.4 holds over 1 $\\le \\Delta t \\le$ 1,000\nmin. (5) The tail distribution of returns at small $\\Delta t$ decays as an\nasymptotic power-law with an exponent of about 3, which is a value widely\nexisting in mature markets. However, it decays exponentially when $\\Delta t\n\\ge$ 240 min, which is not observed in mature markets. (6) Return distributions\ngradually converge to Gaussian as $\\Delta t$ increases. This observation is\ndifferent from the finding of a critical $\\Delta t =$ 4 days in the U.S. stock\nmarket.\n"
    },
    {
        "paper_id": 2209.08574,
        "authors": "Craig McIntosh and Andrew Zeitlin",
        "title": "Skills and Liquidity Barriers to Youth Employment: Medium-term Evidence\n  from a Cash Benchmarking Experiment in Rwanda",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  We present results of an experiment benchmarking a workforce training program\nagainst cash transfers for underemployed young adults in Rwanda. 3.5 years\nafter treatment, the training program enhances productive time use and asset\ninvestment, while the cash transfers drive productive assets, livestock values,\nsavings, and subjective well-being. Both interventions have powerful effects on\nentrepreneurship. But while labor, sales, and profits all go up, the implied\nwage rate in these businesses is low. Our results suggest that credit is a\nmajor barrier to self-employment, but deeper reforms may be required to enable\nentrepreneurship to provide a transformative pathway out of poverty.\n"
    },
    {
        "paper_id": 2209.08778,
        "authors": "Frederik Bossaerts, Nitin Yadav, Peter Bossaerts, Chad Nash, Torquil\n  Todd, Torsten Rudolf, Rowena Hutchins, Anne-Louise Ponsonby, Karl Mattingly",
        "title": "Price Formation in Field Prediction Markets: the Wisdom in the Crowd",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  Prediction markets are a popular, prominent, and successful structure for a\ncollective intelligence platform. However the exact mechanism by which\ninformation known to the participating traders is incorporated into the market\nprice is unknown. Kyle (1985) detailed a model for price formation in\ncontinuous auctions with information distributed heterogeneously amongst market\nparticipants. This paper demonstrates a novel method derived from the Kyle\nmodel applied to data from a field experiment prediction market. The method is\nable to identify traders whose trades have price impact that adds a significant\namount of information to the market price. Traders who are not identified as\ninformed in aggregate have price impact consistent with noise trading. Results\nare reproduced on other prediction market datasets. Ultimately the results\nprovide strong evidence in favor of the Kyle model in a field market setting,\nand highlight an under-discussed advantage of prediction markets over\nalternative group forecasting mechanisms: that the operator of the market does\nnot need to have information on the distribution of information amongst\nparticipating traders.\n"
    },
    {
        "paper_id": 2209.08825,
        "authors": "Deborah Miori and Mihai Cucuringu",
        "title": "SEC Form 13F-HR: Statistical investigation of trading imbalances and\n  profitability analysis",
        "comments": "23 pages, 18 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  US Institutions with more than $100 million assets under management must\ndisclose part of their long positions into the SEC Form 13F-HR on a quarterly\nbasis. We consider the number of variations in holdings between consecutive\nreporting periods, and compute imbalances in buying versus selling behaviour\nfor the assets under consideration. A significant opportunity for profit arises\nif an external investor is willing to trade contrarian to the 13F filings\nimbalances. Indeed, imbalances capture the amount of information already\nconsumed in the market and the related trades tend to be inflated by crowding\nand herding. Betting on a relatively short-term movement of prices against the\nsign of imbalances results in a profitable strategy especially when using a\ntime horizon between 21 and 42 trading days (corresponding to 1-2 calendar\nmonths) after each financial quarter ends.\n"
    },
    {
        "paper_id": 2209.08848,
        "authors": "Dangxing Chen",
        "title": "Two-stage Modeling for Prediction with Confidence",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  The use of neural networks has been very successful in a wide variety of\napplications. However, it has recently been observed that it is difficult to\ngeneralize the performance of neural networks under the condition of\ndistributional shift. Several efforts have been made to identify potential\nout-of-distribution inputs. Although existing literature has made significant\nprogress with regard to images and textual data, finance has been overlooked.\nThe aim of this paper is to investigate the distribution shift in the credit\nscoring problem, one of the most important applications of finance. For the\npotential distribution shift problem, we propose a novel two-stage model. Using\nthe out-of-distribution detection method, data is first separated into\nconfident and unconfident sets. As a second step, we utilize the domain\nknowledge with a mean-variance optimization in order to provide reliable bounds\nfor unconfident samples. Using empirical results, we demonstrate that our model\noffers reliable predictions for the vast majority of datasets. It is only a\nsmall portion of the dataset that is inherently difficult to judge, and we\nleave them to the judgment of human beings. Based on the two-stage model,\nhighly confident predictions have been made and potential risks associated with\nthe model have been significantly reduced.\n"
    },
    {
        "paper_id": 2209.08967,
        "authors": "Maria Elvira Mancino, Tommaso Mariotti, Giacomo Toscano",
        "title": "Asymptotic Normality for the Fourier spot volatility estimator in the\n  presence of microstructure noise",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The main contribution of the paper is proving that the Fourier spot\nvolatility estimator introduced in [Malliavin and Mancino, 2002] is consistent\nand asymptotically efficient if the price process is contaminated by\nmicrostructure noise. Specifically, in the presence of additive microstructure\nnoise we prove a Central Limit Theorem with the optimal rate of convergence\n$n^{1/8}$. The result is obtained without the need for any manipulation of the\noriginal data or bias correction. Moreover, we complete the asymptotic theory\nfor the Fourier spot volatility estimator in the absence of noise, originally\npresented in [Mancino and Recchioni, 2015], by deriving a Central Limit Theorem\nwith the optimal convergence rate $n^{1/4}$. Finally, we propose a novel\nfeasible adaptive method for the optimal selection of the parameters involved\nin the implementation of the Fourier spot volatility estimator with noisy\nhigh-frequency data and provide support to its accuracy both numerically and\nempirically.\n"
    },
    {
        "paper_id": 2209.09157,
        "authors": "Ricardo M\\\"uller, Marco Schreyer, Timur Sattarov, Damian Borth",
        "title": "RESHAPE: Explaining Accounting Anomalies in Financial Statement Audits\n  by enhancing SHapley Additive exPlanations",
        "comments": "9 pages, 4 figures, 5 tables, preprint version, currently under\n  review",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  Detecting accounting anomalies is a recurrent challenge in financial\nstatement audits. Recently, novel methods derived from Deep-Learning (DL) have\nbeen proposed to audit the large volumes of a statement's underlying accounting\nrecords. However, due to their vast number of parameters, such models exhibit\nthe drawback of being inherently opaque. At the same time, the concealing of a\nmodel's inner workings often hinders its real-world application. This\nobservation holds particularly true in financial audits since auditors must\nreasonably explain and justify their audit decisions. Nowadays, various\nExplainable AI (XAI) techniques have been proposed to address this challenge,\ne.g., SHapley Additive exPlanations (SHAP). However, in unsupervised DL as\noften applied in financial audits, these methods explain the model output at\nthe level of encoded variables. As a result, the explanations of Autoencoder\nNeural Networks (AENNs) are often hard to comprehend by human auditors. To\nmitigate this drawback, we propose (RESHAPE), which explains the model output\non an aggregated attribute-level. In addition, we introduce an evaluation\nframework to compare the versatility of XAI methods in auditing. Our\nexperimental results show empirical evidence that RESHAPE results in versatile\nexplanations compared to state-of-the-art baselines. We envision such\nattribute-level explanations as a necessary next step in the adoption of\nunsupervised DL techniques in financial auditing.\n"
    },
    {
        "paper_id": 2209.09548,
        "authors": "Hugo Inzirillo and Ludovic De Villelongue",
        "title": "An Attention Free Long Short-Term Memory for Time Series Forecasting",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-sa/4.0/",
        "abstract": "  Deep learning is playing an increasingly important role in time series\nanalysis. We focused on time series forecasting using attention free mechanism,\na more efficient framework, and proposed a new architecture for time series\nprediction for which linear models seem to be unable to capture the time\ndependence. We proposed an architecture built using attention free LSTM layers\nthat overcome linear models for conditional variance prediction. Our findings\nconfirm the validity of our model, which also allowed to improve the prediction\ncapacity of a LSTM, while improving the efficiency of the learning task.\n"
    },
    {
        "paper_id": 2209.09649,
        "authors": "Nghia Chu, Binh Dao, Nga Pham, Huy Nguyen, Hien Tran",
        "title": "Predicting Mutual Funds' Performance using Deep Learning and Ensemble\n  Techniques",
        "comments": "16 pages, 4 figures, 4 tables",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Predicting fund performance is beneficial to both investors and fund\nmanagers, and yet is a challenging task. In this paper, we have tested whether\ndeep learning models can predict fund performance more accurately than\ntraditional statistical techniques. Fund performance is typically evaluated by\nthe Sharpe ratio, which represents the risk-adjusted performance to ensure\nmeaningful comparability across funds. We calculated the annualised Sharpe\nratios based on the monthly returns time series data for more than 600 open-end\nmutual funds investing in listed large-cap equities in the United States. We\nfind that long short-term memory (LSTM) and gated recurrent units (GRUs) deep\nlearning methods, both trained with modern Bayesian optimization, provide\nhigher accuracy in forecasting funds' Sharpe ratios than traditional\nstatistical ones. An ensemble method, which combines forecasts from LSTM and\nGRUs, achieves the best performance of all models. There is evidence to say\nthat deep learning and ensembling offer promising solutions in addressing the\nchallenge of fund performance forecasting.\n"
    },
    {
        "paper_id": 2209.09656,
        "authors": "Carlo Marinelli, Stefano d'Addona",
        "title": "Nonparametric estimates of option prices via Hermite basis functions",
        "comments": "44 pages. Final version",
        "journal-ref": null,
        "doi": "10.1007/s10436-023-00431-4",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We consider approximate pricing formulas for European options based on\napproximating the logarithmic return's density of the underlying by a linear\ncombination of rescaled Hermite polynomials. The resulting models, that can be\nseen as perturbations of the classical Black-Scholes one, are nonpararametric\nin the sense that the distribution of logarithmic returns at fixed times to\nmaturity is only assumed to have a square-integrable density. We extensively\ninvestigate the empirical performance, defined in terms of out-of-sample\nrelative pricing error, of this class of approximating models, depending on\ntheir order (that is, roughly speaking, the degree of the polynomial expansion)\nas well as on several ways to calibrate them to observed data. Empirical\nresults suggest that such approximate pricing formulas, when compared with\nsimple nonparametric estimates based on interpolation and extrapolation on the\nimplied volatility curve, perform reasonably well only for options with strike\nprice not too far apart from the strike prices of the observed sample.\n"
    },
    {
        "paper_id": 2209.09709,
        "authors": "Lin He, Zongxia Liang, Sheng Wang",
        "title": "Modern Tontine with Transaction Costs",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper, we propose a new type of reversible modern tontine with\ntransaction costs. The wealth of the retiree is divided into a bequest account\nand a tontine account. And consumption can only be withdrawn from the bequest\naccount. Each transaction between the two accounts incurs fixed and\nproportional transaction costs depending on the transaction volume. The retiree\ndynamically controls the allocation policy between the two accounts and the\nconsumption policy to maximize the consumption and bequest utilities. We\nformulate the optimization problem as a combined stochastic and impulse control\nproblem with infinite time horizon, and characterize the value function as the\nunique viscosity solution of a HJBQVI (Hamilton-Jacobi-Bellmen\nquasi-variational inequality). The numerical results exhibit the V-shaped\ntransaction region which consists of two stages. In the former stage, since\nlongevity credits increase gradually, the retiree decreases the proportion of\nwealth in the tontine account to smooth the wealth and reduce the volatility.\nHowever, in the latter stage, the retiree increases the proportion of wealth in\nthe tontine account to gamble for the considerable longevity credits.\n"
    },
    {
        "paper_id": 2209.09719,
        "authors": "Sasha Stoikov, Ivan Kosyuk",
        "title": "Valuation of Music Catalogs",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  We propose a risk neutral approach to forecast the cashflows of music\ncatalogs, based on historical revenue data. We use a discounted cashflows\nformula to produce reasonable ranges of multipliers for these assets, based on\nthe age of the catalog, the last-twelve-months revenue and the duration of the\ncontract. We compare the multipliers implied by the cashflows of top, median\nand bottom performing songs on the Royalty Exchange platform. We find that ask\nprices are close to the multipliers justified by median song cashflows. The\nbest bids are near the multipliers justified by the bottom decile of song\ncashflows.\n"
    },
    {
        "paper_id": 2209.09837,
        "authors": "Giovanni Carnazza, Pierluigi Vellucci",
        "title": "Network analysis and Eurozone trade imbalances",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  European Monetary Union continues to be characterised by significant\nmacroeconomic imbalances. Germany has shown increasing current account\nsurpluses at the expense of the other member states (especially the European\nperiphery). Since the creation of a single currency has implied the\nimpossibility of implementing competitive devaluations, trade imbalances within\na monetary union can be considered unfair behaviour. We have modelled Eurozone\ntrade flows in goods through a weighted network from 1995 to 2019. To the best\nof our knowledge, this is the first work that applies this methodology to this\nkind of data. Network analysis has allowed us to estimate a series of important\ncentrality measures. A polarisation phenomenon emerges in relation to the\ngrowth of German dominance. The common currency has then not been capable to\nremove trade asymmetry, increasing the distance between surplus and deficit\ncountries. This situation should be addressed with expansionary policies on the\ndemand side at national and supranational level.\n"
    },
    {
        "paper_id": 2209.09878,
        "authors": "Maria B. Chiarolla",
        "title": "A unifying view on the irreversible investment exercise boundary in a\n  stochastic, time-inhomogeneous capacity expansion problem",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Aiming at studying the investment exercise boundary, this paper devises a way\nto apply the Bank and El Karoui Representation Theorem to a quite general\nstochastic, continuous time capacity expansion problem with irreversible\ninvestment on the finite time interval and including a state dependent scrap\nvalue associated with the production facility at the terminal time T. The\ncapacity process is a time-inhomogeneous diffusion in which a monotone\nnon-decreasing, possibly singular, control process representing the cumulative\ninvestment enters additively. The functional to be maximized admits a\nsupergradient, hence the optimal control satisfies some first order conditions\nwhich are solved by means of the Bank and El Karoui Representation Theorem. Its\napplication in the case of non-zero scrap value at time T is not obvious and,\nas far as we know, it is new in the literature on singular stochastic control.\nIn fact, due to the scrap value, in the supergradient appears also a non\nintegral term. This challenge is overcome by suitably extending the horizon.\nThe optimal investment process is shown to become active at the so-called base\ncapacity level, given in terms of the optional solution of the Representation\nTheorem. Contrary to what happens in the no scrap value case, here the base\ncapacity depends on the initial capacity y. Hence, a priori, it is not clear if\nand how it is related to the investment exercise boundary associated to the\ncapacity expansion problem. Under the assumption of deterministic coefficients,\ndiscount factor, conversion factor, wage rate and interest rate, the investment\nboundary is shown to coincide with the base capacity. Therefore, unifying\nviews, the base capacity is deterministic and independent of y, and its\nintegral equation may be used to characterize the investment boundary, without\nany a priori regularity of it.\n"
    },
    {
        "paper_id": 2209.09956,
        "authors": "Bernhard K Meister and Henry CW Price",
        "title": "NFTs: The Game is Afoot",
        "comments": "11 pages, 1 figure",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  On the blockchain, NFT games have risen in popularity, spawning new types of\ndigital assets. We present a simplified version of well-known NFT games,\nfollowed by a discussion of issues influencing the structure and stability of\ngeneric games. Where applicable, ideas from quantitative finance are\nincorporated, suggesting various design constraints. Following that, we explain\nthree distinct methods for extracting value from NFT games. The first is to\nutilise NFT tokens as collateral outside of the game's walled garden; the\nsecond is to construct mutual beneficial games based on the participants' risk\ntolerance, and the third is to use Siegel's paradox in the case of multiple\nnumeraires.\n"
    },
    {
        "paper_id": 2209.09958,
        "authors": "Abrar Rahman, Victor Shi, Matthew Ding, Elliot Choi",
        "title": "Systematization of Knowledge: Synthetic Assets, Derivatives, and\n  On-Chain Portfolio Management",
        "comments": "45 pages, 4 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Synthetic assets are decentralized finance (DeFi) analogues of derivatives in\nthe traditional finance (TradFi) world - financial arrangements which derive\nvalue from and are directly pegged to fluctuations in the value of an\nunderlying asset (ex: futures and options). Synthetic assets occupy a unique\nniche, serving to facilitate currency exchange, giving traders a means to\nspeculate on the value of crypto assets without directly holding them, and\npowering more complex financial tools such as yield optimizers and portfolio\nmanagement suites. Unfortunately, the academic literature on this topic is\nhighly disparate and struggles to keep up with rapid changes in the space. We\npresent the first Systematization of Knowledge (SoK) in this area, focusing on\npresenting the key mechanisms, protocols, and issues in an accessible fashion\nto highlight risks for participants as well as areas of research interest. This\npaper takes a broad perspective in establishing a general framework for\nsynthetic assets, from the ideological origins of crypto to legal barriers for\nfirms in this space, encapsulating the basic mechanisms underpinning\nderivatives markets as well as presenting data-driven analyses of major\nprotocols.\n"
    },
    {
        "paper_id": 2209.1007,
        "authors": "Dangxing Chen and Weicheng Ye",
        "title": "Monotonic Neural Additive Models: Pursuing Regulated Machine Learning\n  Models for Credit Scoring",
        "comments": "to appear in the 3rd ICAIF",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  The forecasting of credit default risk has been an active research field for\nseveral decades. Historically, logistic regression has been used as a major\ntool due to its compliance with regulatory requirements: transparency,\nexplainability, and fairness. In recent years, researchers have increasingly\nused complex and advanced machine learning methods to improve prediction\naccuracy. Even though a machine learning method could potentially improve the\nmodel accuracy, it complicates simple logistic regression, deteriorates\nexplainability, and often violates fairness. In the absence of compliance with\nregulatory requirements, even highly accurate machine learning methods are\nunlikely to be accepted by companies for credit scoring. In this paper, we\nintroduce a novel class of monotonic neural additive models, which meet\nregulatory requirements by simplifying neural network architecture and\nenforcing monotonicity. By utilizing the special architectural features of the\nneural additive model, the monotonic neural additive model penalizes\nmonotonicity violations effectively. Consequently, the computational cost of\ntraining a monotonic neural additive model is similar to that of training a\nneural additive model, as a free lunch. We demonstrate through empirical\nresults that our new model is as accurate as black-box fully-connected neural\nnetworks, providing a highly accurate and regulated machine learning method.\n"
    },
    {
        "paper_id": 2209.10082,
        "authors": "Dangxing Chen and Weicheng Ye",
        "title": "Generalized Groves of Neural Additive Models: Pursuing transparent and\n  accurate machine learning models in finance",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  While machine learning methods have significantly improved model performance\nover traditional methods, their black-box structure makes it difficult for\nresearchers to interpret results. For highly regulated financial industries,\nmodel transparency is equally important to accuracy. Without understanding how\nmodels work, even highly accurate machine learning methods are unlikely to be\naccepted. We address this issue by introducing a novel class of transparent\nmachine learning models known as generalized groves of neural additive models.\nThe generalized groves of neural additive models separate features into three\ncategories: linear features, individual nonlinear features, and interacted\nnonlinear features. Additionally, interactions in the last category are only\nlocal. A stepwise selection algorithm distinguishes the linear and nonlinear\ncomponents, and interacted groups are carefully verified by applying additive\nseparation criteria. Through some empirical examples in finance, we demonstrate\nthat generalized grove of neural additive models exhibit high accuracy and\ntransparency with predominantly linear terms and only sparse nonlinear ones.\n"
    },
    {
        "paper_id": 2209.10127,
        "authors": "Dangxing Chen, Weicheng Ye, and Jiahui Ye",
        "title": "Interpretable Selective Learning in Credit Risk",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  The forecasting of the credit default risk has been an important research\nfield for several decades. Traditionally, logistic regression has been widely\nrecognized as a solution due to its accuracy and interpretability. As a recent\ntrend, researchers tend to use more complex and advanced machine learning\nmethods to improve the accuracy of the prediction. Although certain non-linear\nmachine learning methods have better predictive power, they are often\nconsidered to lack interpretability by financial regulators. Thus, they have\nnot been widely applied in credit risk assessment. We introduce a neural\nnetwork with the selective option to increase interpretability by\ndistinguishing whether the datasets can be explained by the linear models or\nnot. We find that, for most of the datasets, logistic regression will be\nsufficient, with reasonable accuracy; meanwhile, for some specific data\nportions, a shallow neural network model leads to much better accuracy without\nsignificantly sacrificing the interpretability.\n"
    },
    {
        "paper_id": 2209.10128,
        "authors": "B. Cooper Boniece, Jos\\'e E. Figueroa-L\\'opez, and Yuchen Han",
        "title": "Efficient Integrated Volatility Estimation in the Presence of Infinite\n  Variation Jumps via Debiased Truncated Realized Variations",
        "comments": "An earlier version of this manuscript was circulated under the title\n  \"Efficient Volatility Estimation for L\\'evy Processes with Jumps of Unbounded\n  Variation\". The results therein were constrained to L\\'evy processes, whereas\n  here we consider a much larger class of It\\^o semimartingales. arXiv admin\n  note: text overlap with arXiv:2202.00877",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Statistical inference for stochastic processes based on high-frequency\nobservations has been an active research area for more than two decades. One of\nthe most well-known and widely studied problems has been the estimation of the\nquadratic variation of the continuous component of an It\\^o semimartingale with\njumps. Several rate- and variance-efficient estimators have been proposed in\nthe literature when the jump component is of bounded variation. However, to\ndate, very few methods can deal with jumps of unbounded variation. By\ndeveloping new high-order expansions of the truncated moments of a locally\nstable L\\'evy process, we propose a new rate- and variance-efficient volatility\nestimator for a class of It\\^o semimartingales whose jumps behave locally like\nthose of a stable L\\'evy process with Blumenthal-Getoor index $Y\\in (1,8/5)$\n(hence, of unbounded variation). The proposed method is based on a two-step\ndebiasing procedure for the truncated realized quadratic variation of the\nprocess and can also cover the case $Y<1$. Our Monte Carlo experiments indicate\nthat the method outperforms other efficient alternatives in the literature in\nthe setting covered by our theoretical framework.\n"
    },
    {
        "paper_id": 2209.10148,
        "authors": "Kendra Walker, Ben Moscona, Kelsey Jack, Seema Jayachandran, Namrata\n  Kala, Rohini Pande, Jiani Xue, Marshall Burke",
        "title": "Detecting Crop Burning in India using Satellite Data",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Crop residue burning is a major source of air pollution in many parts of the\nworld, notably South Asia. Policymakers, practitioners and researchers have\ninvested in both measuring impacts and developing interventions to reduce\nburning. However, measuring the impacts of burning or the effectiveness of\ninterventions to reduce burning requires data on where burning occurred. These\ndata are challenging to collect in the field, both in terms of cost and\nfeasibility. We take advantage of data from ground-based monitoring of crop\nresidue burning in Punjab, India to explore whether burning can be detected\nmore effectively using accessible satellite imagery. Specifically, we used 3m\nPlanetScope data with high temporal resolution (up to daily) as well as\npublicly-available Sentinel-2 data with weekly temporal resolution but greater\ndepth of spectral information. Following an analysis of the ability of\ndifferent spectral bands and burn indices to separate burned and unburned plots\nindividually, we built a Random Forest model with those determined to provide\nthe greatest separability and evaluated model performance with ground-verified\ndata. Our overall model accuracy of 82-percent is favorable given the\nchallenges presented by the measurement. Based on insights from this process,\nwe discuss technical challenges of detecting crop residue burning from\nsatellite imagery as well as challenges to measuring impacts, both of burning\nand of policy interventions.\n"
    },
    {
        "paper_id": 2209.10166,
        "authors": "Ariel Neufeld, Philipp Schmocker",
        "title": "Chaotic Hedging with Iterated Integrals and Neural Networks",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper, we extend the Wiener-Ito chaos decomposition to the class of\ncontinuous semimartingales that are exponentially integrable, which includes in\nparticular affine and some polynomial diffusion processes. By omitting the\northogonality in the expansion, we are able to show that every $p$-integrable\nfunctional of the semimartingale, for $p \\in [1,\\infty)$, can be represented as\na sum of iterated integrals thereof. Using finitely many terms of this\nexpansion and (possibly random) neural networks for the integrands, whose\nparameters are learned in a machine learning setting, we show that every\nfinancial derivative can be approximated arbitrarily well in the $L^p$-sense.\nIn particular, for $p = 2$, we recover the optimal hedging strategy in the\nsense of quadratic hedging. Moreover, since the hedging strategy of the\napproximating option can be computed in closed form, we obtain an efficient\nalgorithm to approximately replicate any sufficiently integrable financial\nderivative within short runtime.\n"
    },
    {
        "paper_id": 2209.10206,
        "authors": "Tomoo Kikuchi and Shuige Liu",
        "title": "The Power of Non-Superpowers",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  We propose a game-theoretic model to investigate how non-superpowers with\nheterogenous preferences and endowments shape the superpower competition for a\nsphere of influence. Two superpowers play a Stackelberg game by providing club\ngoods. Their utility depends on non-superpowers who form coalitions to join a\nclub in the presence of externality. The coalition formation, which depends on\nthe characteristics of non-superpowers, influences the behavior of superpowers\nand thus the size of their clubs. Our data-based simulations of the subgame\nperfect equilbirum capture how the US-China competition depends on other\ncountries.\n"
    },
    {
        "paper_id": 2209.10256,
        "authors": "Xiaoguang Ling",
        "title": "Heterogeneous earning responses to inheritance: new event-study evidence\n  from Norway",
        "comments": "55 pages, 58 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  It has long been assumed that inheritances, particularly large ones, have a\nnegative effect on the labor supply of inheritors. Using Norwegian registry\ndata, I examine the inheritance-induced decline in inheritors' wages and\noccupational income. In contrast to prior research, my estimates allow the\ndynamic effect of inheritances on labor supply to vary among inheritor cohorts.\nThe estimation approach adopted and the 25-year long panel data make it\npossible to trace the dynamics of the effect for at least 20 years, which is\ntwice as long as the study period in previous studies. Since all observations\nin the sample are inheritors, I avoid the selection problem arising in studies\nemploying non-inheritors as controls. I find that large parental inheritances\n(more than one million Norwegian kroner) reduce annual wage and occupational\nincome by, at most, 4.3%, which is about half the decrease previously\nidentified. The magnitude of the effect increases with the size of the\ninheritance. Large inheritances also increase the probability of being\nself-employed by more than 1%, although entrepreneurship may be dampened by\ninheritances that are excessively large. The inheritance effect lasts for up to\n10 years and is heterogeneous across sexes and age groups. Male heirs are more\nlikely to reduce their labor supply after receiving the transfer. Young heirs\nare more likely to be self-employed, and their annual occupational income is,\ntherefore, less affected by inheritances in the long run; for the very young\ninheriting large amounts of wealth from their grandparents, the probability of\ntheir attaining a post-secondary education declines by 2%.\n"
    },
    {
        "paper_id": 2209.10334,
        "authors": "Yutong Lu, Gesine Reinert, Mihai Cucuringu",
        "title": "Trade Co-occurrence, Trade Flow Decomposition, and Conditional Order\n  Imbalance in Equity Markets",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The time proximity of high-frequency trades can contain a salient signal. In\nthis paper, we propose a method to classify every trade, based on its proximity\nwith other trades in the market within a short period of time, into five types.\nBy means of a suitably defined normalized order imbalance associated to each\ntype of trade, which we denote as conditional order imbalance (COI), we\ninvestigate the price impact of the decomposed trade flows. Our empirical\nfindings indicate strong positive correlations between contemporaneous returns\nand COIs. In terms of predictability, we document that associations with future\nreturns are positive for COIs of trades which are isolated from trades of\nstocks other than themselves, and negative otherwise. Furthermore, trading\nstrategies which we develop using COIs achieve conspicuous returns and Sharpe\nratios, in an extensive experimental setup on a universe of 457 stocks using\ndaily data for a period of four years.\n"
    },
    {
        "paper_id": 2209.10363,
        "authors": "Dongwei Zhao, Hao Wang, Jianwei Huang, Xiaojun Lin",
        "title": "Insurance Contract for High Renewable Energy Integration",
        "comments": "IEEE SmartGridComm 2022",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  The increasing penetration of renewable energy poses significant challenges\nto power grid reliability. There have been increasing interests in utilizing\nfinancial tools, such as insurance, to help end-users hedge the potential risk\nof lost load due to renewable energy variability. With insurance, a user pays a\npremium fee to the utility, so that he will get compensated in case his demand\nis not fully satisfied. A proper insurance design needs to resolve the\nfollowing two challenges: (i) users' reliability preference is private\ninformation; and (ii) the insurance design is tightly coupled with the\nrenewable energy investment decision. To address these challenges, we adopt the\ncontract theory to elicit users' private reliability preferences, and we study\nhow the utility can jointly optimize the insurance contract and the planning of\nrenewable energy. A key analytical challenge is that the joint optimization of\nthe insurance design and the planning of renewables is non-convex. We resolve\nthis difficulty by revealing important structural properties of the optimal\nsolution, using the help of two benchmark problems: the no-insurance benchmark\nand the social-optimum benchmark. Compared with the no-insurance benchmark, we\nprove that the social cost and users' total energy cost are always no larger\nunder the optimal contract. Simulation results show that the largest benefit of\nthe insurance contract is achieved at a medium electricity-bill price together\nwith a low type heterogeneity and a high renewable uncertainty.\n"
    },
    {
        "paper_id": 2209.10389,
        "authors": "Goran Durakovic, Pedro Crespo del Granado, Asgeir Tomasgard",
        "title": "Powering Europe with North Sea Offshore Wind: The Impact of Hydrogen\n  Investments on Grid Infrastructure and Power Prices",
        "comments": "Submitted to Energy",
        "journal-ref": null,
        "doi": "10.1016/j.energy.2022.125654",
        "license": "http://creativecommons.org/licenses/by-sa/4.0/",
        "abstract": "  Hydrogen will be a central cross-sectoral energy carrier in the\ndecarbonization of the European energy system. This paper investigates how a\nlarge-scale deployment of green hydrogen production affects the investments in\ntransmission and generation towards 2060, analyzes the North Sea area with the\nmain offshore wind projects, and assesses the development of an offshore energy\nhub. Results indicate that the hydrogen deployment has a tremendous impact on\nthe grid development in Europe and in the North Sea. Findings indicate that\ntotal power generation capacity increases around 50%. The offshore energy hub\nacts mainly as a power transmission asset, leads to a reduction in total\ngeneration capacity, and is central to unlock the offshore wind potential in\nthe North Sea. The effect of hydrogen deployment on power prices is\nmultifaceted. In regions where power prices have typically been lower than\nelsewhere in Europe, it is observed that hydrogen increases the power price\nconsiderably. However, as hydrogen flexibility relieves stress in high-demand\nperiods for the grid, power prices decrease in average for some countries. This\nsuggests that while the deployment of green hydrogen will lead to a significant\nincrease in power demand, power prices will not necessarily experience a large\nincrease.\n"
    },
    {
        "paper_id": 2209.10405,
        "authors": "Avni Singh",
        "title": "Effects of Work-From-Home on University Students and Faculty",
        "comments": "40 pages. Written during internship at IISc",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  The work-from-home policy affected people of all demographics and\nprofessions, including students and faculty at universities. After the onset of\nthe COVID-19 pandemic in 2020, institutions moved their operations online,\naffecting the motivation levels, communication abilities, and mental health of\nstudents and faculty around the world. This paper is based mainly on primary\ndata collected from students from around the world, and professors at\nuniversities in Bengaluru, India. It explores the effects of work-from-home as\na policy in terms of how it changed learning during the pandemic and how it has\npermanently altered it in a post-pandemic future. Further, it suggests and\nevaluates policies on how certain negative effects of the work-from-home policy\ncan be mitigated.\n"
    },
    {
        "paper_id": 2209.10458,
        "authors": "Adebayo Oshingbesan, Eniola Ajiboye, Peruth Kamashazi, Timothy Mbaka",
        "title": "Model-Free Reinforcement Learning for Asset Allocation",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
        "abstract": "  Asset allocation (or portfolio management) is the task of determining how to\noptimally allocate funds of a finite budget into a range of financial\ninstruments/assets such as stocks. This study investigated the performance of\nreinforcement learning (RL) when applied to portfolio management using\nmodel-free deep RL agents. We trained several RL agents on real-world stock\nprices to learn how to perform asset allocation. We compared the performance of\nthese RL agents against some baseline agents. We also compared the RL agents\namong themselves to understand which classes of agents performed better. From\nour analysis, RL agents can perform the task of portfolio management since they\nsignificantly outperformed two of the baseline agents (random allocation and\nuniform allocation). Four RL agents (A2C, SAC, PPO, and TRPO) outperformed the\nbest baseline, MPT, overall. This shows the abilities of RL agents to uncover\nmore profitable trading strategies. Furthermore, there were no significant\nperformance differences between value-based and policy-based RL agents.\nActor-critic agents performed better than other types of agents. Also,\non-policy agents performed better than off-policy agents because they are\nbetter at policy evaluation and sample efficiency is not a significant problem\nin portfolio management. This study shows that RL agents can substantially\nimprove asset allocation since they outperform strong baselines. On-policy,\nactor-critic RL agents showed the most promise based on our analysis.\n"
    },
    {
        "paper_id": 2209.10498,
        "authors": "Mandeep Singh Rai",
        "title": "International institutions and power politics in the context of Chinese\n  Belt and Road Initiative",
        "comments": "11 Pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-sa/4.0/",
        "abstract": "  The subject of international institutions and power politics continues to\noccupy a central position in the field of International Relations and to the\nworld politics. It revolves around key questions on how rising states, regional\npowers and small states leverage international institutions for achieving\nsocial, political, economic gains for themselves. Taking into account one of\nthe rising powers China and the role of international institutions in the\ncontemporary international politics, this paper aims to demonstrate, how in\npursuit of power politics, various states (Small, Regional and Great powers)\nutilise international institutions by making them adapt to the new power\nrealities critical to world politics.\n"
    },
    {
        "paper_id": 2209.10518,
        "authors": "Sam Johnston",
        "title": "Sustainable Venture Capital",
        "comments": "Masters thesis. 114 pages, 18 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-sa/4.0/",
        "abstract": "  Sustainability initiatives are set to benefit greatly from the growing\ninvolvement of venture capital, in the same way that other technological\nendeavours have been enabled and accelerated in the post-war period. With the\nspoils increasingly being shared between shareholders and other stakeholders,\nthis requires a more nuanced view than the finance-first methodologies deployed\nto date. Indeed, it is possible for a venture-backed sustainability startup to\ndeliver outstanding results to society in general without returning a cent to\ninvestors, though the most promising outcomes deliver profit with purpose,\nsatisfying all stakeholders in ways that make existing 'extractive' venture\ncapital seem hollow.\n  To explore this nascent area, a review of related research was conducted and\nsocial entrepreneurs & investors interviewed to construct a questionnaire\nassessing the interests and intentions of current & future ecosystem\nparticipants. Analysis of 114 responses received via several sampling methods\nrevealed statistically significant relationships between investing preferences\nand genders, generations, sophistication, and other variables, all the way down\nto the level of individual UN Sustainable Development Goals (SDGs).\n"
    },
    {
        "paper_id": 2209.10527,
        "authors": "Mandeep Singh Rai Misty Wyatt, Sydney Farrar, and Kristina Alabado",
        "title": "A French Connection? Recognition and Entente for the Taliban",
        "comments": "14 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  This paper will explore a way forward for French-Afghan relations post United\nNations (U.N.) occupation. The summer of 2021 proved to be very tumultuous as\nthe Taliban lay in waiting for U.N. forces to withdraw in what many would call\na hasty, poorly thought-out egress operation. The Taliban effectively retook\nall major cities, including the capital, Kabul, and found themselves isolated\nas U.N. states closed embassies and severed diplomatic relations. Now,\nAfghanistan finds itself without international aid and support and on the verge\nof an economic crisis. France now has the opportunity to initiate a leadership\nrole in the establishment and recovery of the new Afghan government.\n"
    },
    {
        "paper_id": 2209.10688,
        "authors": "Giulia Di Nunno and Yuliya Mishura and Anton Yurchenko-Tytarenko",
        "title": "Option pricing in Sandwiched Volterra Volatility model",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  We introduce a new model of financial market with stochastic volatility\ndriven by an arbitrary H\\\"older continuous Gaussian Volterra process. The\ndistinguishing feature of the model is the form of the volatility equation\nwhich ensures the solution to be ``sandwiched'' between two arbitrary H\\\"older\ncontinuous functions chosen in advance. We discuss the structure of local\nmartingale measures on this market, investigate integrability and Malliavin\ndifferentiability of prices and volatilities as well as study absolute\ncontinuity of the corresponding probability laws. Additionally, we utilize\nMalliavin calculus to develop an algorithm of pricing options with\ndiscontinuous payoffs.\n"
    },
    {
        "paper_id": 2209.1072,
        "authors": "Jayanta K. Pokharel, Erasmus Tetteh-Bator, Chris P. Tsokos",
        "title": "A Real Data-Driven Analytical Model to Predict Information Technology\n  Sector Index Price of S&P 500",
        "comments": "18 pages, 5 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
        "abstract": "  S&P 500 Index is one of the most sought after stock indices in the world. In\nparticular, Information Technology Sector of S&P 500 is the number one business\nsegment of the S&P 500 in terms of market capital, annual revenue and the\nnumber of companies (75) associated with it, and is one of the most attracting\nareas for many investors due to high percentage annual returns on investment\nover the years. A non-linear real data-driven analytical model is built to\npredict the Weekly Closing Price (WCP) of the Information Technology Sector\nIndex of S&P 500 using six financial, four economic indicators and their two\nway interactions as the attributable entities that drive the stock returns. We\nrank the statistically significant indicators and their interactions based on\nthe percentage of contribution to the $WCP$ of the Information Technology\nSector Index of the S&P 500 that provides significant information for the\nbeneficiary of the proposed predictive model. The model has the predictive\naccuracy of 99.4%, and the paper presents some intriguing findings and the\nmodel's usefulness.\n"
    },
    {
        "paper_id": 2209.10751,
        "authors": "Jacob Wallace, Paul Goldsmith-Pinkham and Jason Schwartz",
        "title": "Excess death rates for Republicans and Democrats during the COVID-19\n  pandemic",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Political affiliation has emerged as a potential risk factor for COVID-19,\namid evidence that Republican-leaning counties have had higher COVID-19 death\nrates than Democrat-leaning counties and evidence of a link between political\nparty affiliation and vaccination views. This study constructs an\nindividual-level dataset with political affiliation and excess death rates\nduring the COVID-19 pandemic via a linkage of 2017 voter registration in Ohio\nand Florida to mortality data from 2018 to 2021. We estimate substantially\nhigher excess death rates for registered Republicans when compared to\nregistered Democrats, with almost all of the difference concentrated in the\nperiod after vaccines were widely available in our study states. Overall, the\nexcess death rate for Republicans was 5.4 percentage points (pp), or 76%,\nhigher than the excess death rate for Democrats. Post-vaccines, the excess\ndeath rate gap between Republicans and Democrats widened from 1.6 pp (22% of\nthe Democrat excess death rate) to 10.4 pp (153% of the Democrat excess death\nrate). The gap in excess death rates between Republicans and Democrats is\nconcentrated in counties with low vaccination rates and only materializes after\nvaccines became widely available.\n"
    },
    {
        "paper_id": 2209.10771,
        "authors": "Soohan Kim, Seok-Bae Yun, Hyeong-Ohk Bae, Muhyun Lee, Youngjoon Hong",
        "title": "Physics-Informed Convolutional Transformer for Predicting Volatility\n  Surface",
        "comments": "Accepted for publication by Quantitative Finance",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Predicting volatility is important for asset predicting, option pricing and\nhedging strategies because it cannot be directly observed in the financial\nmarket. The Black-Scholes option pricing model is one of the most widely used\nmodels by market participants. Notwithstanding, the Black-Scholes model is\nbased on heavily criticized theoretical premises, one of which is the constant\nvolatility assumption. The dynamics of the volatility surface is difficult to\nestimate. In this paper, we establish a novel architecture based on\nphysics-informed neural networks and convolutional transformers. The\nperformance of the new architecture is directly compared to other well-known\ndeep-learning architectures, such as standard physics-informed neural networks,\nconvolutional long-short term memory (ConvLSTM), and self-attention ConvLSTM.\nNumerical evidence indicates that the proposed physics-informed convolutional\ntransformer network achieves a superior performance than other methods.\n"
    },
    {
        "paper_id": 2209.10871,
        "authors": "Alessandro Doldi and Marco Maggis",
        "title": "On Conditional Chisini Means and Risk Measures",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Given a real valued functional T on the space of bounded random variables, we\ninvestigate the problem of the existence of a conditional version of nonlinear\nmeans. We follow a seminal idea by Chisini (1929), defining a mean as the\nsolution of a functional equation induced by T. We provide sufficient\nconditions which guarantee the existence of a (unique) solution of a system of\ninfinitely many functional equations, which will provide the so called\nConditional Chisini mean. We apply our findings in characterizing the\nscalarization of conditional Risk Measures, an essential tool originally\nadopted by Detlefsen and Scandolo (2005) to deduce the robust dual\nrepresentation.\n"
    },
    {
        "paper_id": 2209.10878,
        "authors": "Eduardo Abi Jaber (X), St\\'ephane Villeneuve (TSE-R)",
        "title": "Gaussian Agency problems with memory and Linear Contracts",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Can a principal still offer optimal dynamic contracts that are linear in\nend-of-period outcomes when the agent controls a process that exhibits memory?\nWe provide a positive answer by considering a general Gaussian setting where\nthe output dynamics are not necessarily semi-martingales or Markov processes.\nWe introduce a rich class of principal-agent models that encompasses dynamic\nagency models with memory. From the mathematical point of view, we develop a\nmethodology to deal with the possible non-Markovianity and non-semimartingality\nof the control problem, which can no longer be directly solved by means of the\nusual Hamilton-Jacobi-Bellman equation. Our main contribution is to show that,\nfor one-dimensional models, this setting always allows for optimal linear\ncontracts in end-of-period observable outcomes with a deterministic optimal\nlevel of effort. In higher dimension, we show that linear contracts are still\noptimal when the effort cost function is radial and we quantify the gap between\nlinear contracts and optimal contracts for more general quadratic costs of\nefforts.\n"
    },
    {
        "paper_id": 2209.11079,
        "authors": "Pablo Bra\\~nas-Garza, Antonio Cabrales, Mar\\'ia Paz Espinosa, Diego\n  Jorrat",
        "title": "The effect of ambiguity in strategic environments: an experiment",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  We experimentally study a game in which success requires a sufficient total\ncontribution by members of a group. There are significant uncertainties\nsurrounding the chance and the total effort required for success. A theoretical\nmodel with max-min preferences towards ambiguity predicts higher contributions\nunder ambiguity than under risk. However, in a large representative sample of\nthe Spanish population (1,500 participants) we find that the ATE of ambiguity\non contributions is zero. The main significant interaction with the personal\ncharacteristics of the participants is with risk attitudes, and it increases\ncontributions. This suggests that policymakers concerned with ambiguous\nproblems (like climate change) do not need to worry excessively about\nambiguity.\n"
    },
    {
        "paper_id": 2209.1115,
        "authors": "Santiago Camara and Sebastian Ramirez Venegas",
        "title": "The Transmission of US Monetary Policy Shocks: The Role of Investment &\n  Financial Heterogeneity",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/publicdomain/zero/1.0/",
        "abstract": "  This paper studies the transmission of US monetary policy shocks into\nEmerging Markets emphasizing the role of investment and financial\nheterogeneity. First, we use a panel SVAR model to show that a US interest\ntightening leads to a persistent recession in Emerging Markets driven by a\nsharp reduction in aggregate investment. Second, we study the role of firms'\nfinancial heterogeneity in the transmission of US interest rate shocks by\nexploiting detailed balance sheet dataset from Chile. We find that more\nindebted firms experience greater drops in investment in response to a US\ntightening shock than less indebted firms. This result is at odds with recent\nevidence from US firms, even when using the same identification strategy and\neconometric methods. Third, we rationalize this finding using a stylized model\nof heterogeneous firms subject to a tightening leverage constraint. Finally, we\npresent evidence in support of this hypothesis as well as robustness checks to\nour main results. Overall, our results suggests that the transmission channel\nof US monetary policy shocks within and outside the US differ, a result novel\nto the literature.\n"
    },
    {
        "paper_id": 2209.11337,
        "authors": "Paul Bilokon and Sergei Kucherenko and Casey Williams",
        "title": "Quasi-Monte Carlo methods for calculating derivatives sensitivities on\n  the GPU",
        "comments": "26 pages, 12 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The calculation of option Greeks is vital for risk management. Traditional\npathwise and finite-difference methods work poorly for higher-order Greeks and\noptions with discontinuous payoff functions. The Quasi-Monte Carlo-based\nconditional pathwise method (QMC-CPW) for options Greeks allows the payoff\nfunction of options to be effectively smoothed, allowing for increased\nefficiency when calculating sensitivities. Also demonstrated in literature is\nthe increased computational speed gained by applying GPUs to highly\nparallelisable finance problems such as calculating Greeks. We pair QMC-CPW\nwith simulation on the GPU using the CUDA platform. We estimate the delta, vega\nand gamma Greeks of three exotic options: arithmetic Asian, binary Asian, and\nlookback. Not only are the benefits of QMC-CPW shown through variance reduction\nfactors of up to $1.0 \\times 10^{18}$, but the increased computational speed\nthrough usage of the GPU is shown as we achieve speedups over sequential CPU\nimplementations of more than $200$x for our most accurate method.\n"
    },
    {
        "paper_id": 2209.11507,
        "authors": "Soumik Ghosh and Arpan Chakraborty",
        "title": "Determinants Of Migration: Linear regression Analysis in Indian Context",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Environmental degradation, global pandemic and severing natural resource\nrelated problems cater to increase demand resulting from migration is nightmare\nfor all of us. Huge flocks of people are rushing towards to earn, to live and\nto lead a better life. This they do for their own development often ignoring\nthe environmental cost. With existing model, this paper looks at out migration\n(interstate) within India focusing on the various proximate and fundamental\ncauses relating to migration. The author deploys OLS to see those fundamental\ncauses. Obviously, these are not exhaustive cause, but definitely plays a role\nin migration decision of individual. Finally, this paper advocates for some\npolicy prescription to cope with this problem.\n"
    },
    {
        "paper_id": 2209.11686,
        "authors": "St\\'ephane Cr\\'epey (LPSM (UMR\\_8001), UPCit\\'e), Lehdili Noureddine,\n  Nisrine Madhar (LPSM (UMR\\_8001), UPCit\\'e), Maud Thomas (LPSM (UMR\\_8001),\n  SU)",
        "title": "Anomaly Detection on Financial Time Series by Principal Component\n  Analysis and Neural Networks",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  A major concern when dealing with financial time series involving a wide\nvariety ofmarket risk factors is the presence of anomalies. These induce a\nmiscalibration of the models used toquantify and manage risk, resulting in\npotential erroneous risk measures. We propose an approachthat aims to improve\nanomaly detection in financial time series, overcoming most of the\ninherentdifficulties. Valuable features are extracted from the time series by\ncompressing and reconstructingthe data through principal component analysis. We\nthen define an anomaly score using a feedforwardneural network. A time series\nis considered to be contaminated when its anomaly score exceeds agiven cutoff\nvalue. This cutoff value is not a hand-set parameter but rather is calibrated\nas a neuralnetwork parameter throughout the minimization of a customized loss\nfunction. The efficiency of theproposed approach compared to several well-known\nanomaly detection algorithms is numericallydemonstrated on both synthetic and\nreal data sets, with high and stable performance being achievedwith the PCA NN\napproach. We show that value-at-risk estimation errors are reduced when\ntheproposed anomaly detection model is used with a basic imputation approach to\ncorrect the anomaly.\n"
    },
    {
        "paper_id": 2209.11914,
        "authors": "Harry Mamaysky, Yiwen Shen, Hongyu Wu",
        "title": "Credit Information in Earnings Calls",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We develop a novel technique to extract credit-relevant information from the\ntext of quarterly earnings calls. This information is not spanned by\nfundamental or market variables and forecasts future credit spread changes. One\nreason for such forecastability is that our text-based measure predicts future\ncredit spread risk and firm profitability. More firm- and call-level complexity\nincrease the forecasting power of our measure for spread changes. Out-of-sample\nportfolio tests show the information in our measure is valuable for investors.\nBoth results suggest that investors do not fully internalize the\ncredit-relevant information contained in earnings calls.\n"
    },
    {
        "paper_id": 2209.12014,
        "authors": "Chen Zhang (SenseTime Research)",
        "title": "Asset Pricing and Deep Learning",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Traditional machine learning methods have been widely studied in financial\ninnovation. My study focuses on the application of deep learning methods on\nasset pricing. I investigate various deep learning methods for asset pricing,\nespecially for risk premia measurement. All models take the same set of\npredictive signals (firm characteristics, systematic risks and macroeconomics).\nI demonstrate high performance of all kinds of state-of-the-art (SOTA) deep\nlearning methods, and figure out that RNNs with memory mechanism and attention\nhave the best performance in terms of predictivity. Furthermore, I demonstrate\nlarge economic gains to investors using deep learning forecasts. The results of\nmy comparative experiments highlight the importance of domain knowledge and\nfinancial theory when designing deep learning models. I also show return\nprediction tasks bring new challenges to deep learning. The time varying\ndistribution causes distribution shift problem, which is essential for\nfinancial time series prediction. I demonstrate that deep learning methods can\nimprove asset risk premium measurement. Due to the booming deep learning\nstudies, they can constantly promote the study of underlying financial\nmechanisms behind asset pricing. I also propose a promising research method\nthat learning from data and figuring out the underlying economic mechanisms\nthrough explainable artificial intelligence (AI) methods. My findings not only\njustify the value of deep learning in blooming fintech development, but also\nhighlight their prospects and advantages over traditional machine learning\nmethods.\n"
    },
    {
        "paper_id": 2209.12222,
        "authors": "T. van der Zwaard, L.A. Grzelak, C.W. Oosterlee",
        "title": "Efficient Wrong-Way Risk Modelling for Funding Valuation Adjustments",
        "comments": null,
        "journal-ref": null,
        "doi": "10.1142/S0219024924500109",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Wrong-Way Risk (WWR) is an important component in Funding Valuation\nAdjustment (FVA) modelling. Yet, the standard assumption is independence\nbetween market risks and the counterparty defaults and funding costs. This\ntypical industrial setting is our point of departure, where we aim to assess\nthe impact of WWR without running a full Monte Carlo simulation with all credit\nand funding processes. We propose to split the exposure profile into two parts:\nan independent and a WWR-driven part. For the former, exposures can be re-used\nfrom the standard xVA calculation. We express the second part of the exposure\nprofile in terms of the stochastic drivers and approximate these by a common\nGaussian stochastic factor. Within the affine setting, the proposed\napproximation is generic, is an add-on to the existing xVA calculations and\nprovides an efficient and robust way to include WWR in FVA modelling. Case\nstudies for an interest rate swap and a representative multi-currency portfolio\nof swaps illustrate that the approximation method is applicable in a practical\nsetting. We analyze the approximation error and use the approximation to\ncompute WWR sensitivities, which are needed for risk management. The approach\nis equally applicable to other metrics such as Credit Valuation Adjustment.\n"
    },
    {
        "paper_id": 2209.12349,
        "authors": "Svetlana Boyarchenko and Sergei Levendorski\\u{i}",
        "title": "Efficient evaluation of expectations of functions of a stable L\\'evy\n  process and its extremum",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Integral representations for expectations of functions of a stable L\\'evy\nprocess $X$ and its supremum $\\bar X$ are derived. As examples, cumulative\nprobability distribution functions (cpdf) of $X_T, \\barX_T$, the joint cpdf of\n$X_T$ and $\\barX_T$, and the expectation of $(\\be X_T-\\barX_T)_+$, $\\be>1$, are\nconsidered, and efficient numerical procedures for cpdfs are developed. The\nmost efficient numerical methods use the conformal acceleration technique and\nsimplified trapezoid rule.\n"
    },
    {
        "paper_id": 2209.12383,
        "authors": "Chung-Han Hsieh",
        "title": "On Robustness of Double Linear Trading with Transaction Costs",
        "comments": "Submitted to for possible publication. arXiv admin note: substantial\n  text overlap with arXiv:2202.02300",
        "journal-ref": "IEEE Control Systems Letters, 2022",
        "doi": "10.1109/LCSYS.2022.3218541",
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  A trading system is said to be {robust} if it generates a robust return\nregardless of market direction. To this end, a consistently positive expected\ntrading gain is often used as a robustness metric for a trading system. In this\npaper, we propose a new class of trading policies called the {double linear\npolicy} in an asset trading scenario when the transaction costs are involved.\nUnlike many existing papers, we first show that the desired robust positive\nexpected gain may disappear when transaction costs are involved. Then we\nquantify under what conditions the desired positivity can still be preserved.\nIn addition, we conduct heavy Monte-Carlo simulations for an underlying asset\nwhose prices are governed by a geometric Brownian motion with jumps to validate\nour theory. A more realistic backtesting example involving historical data for\ncryptocurrency Bitcoin-USD is also studied.\n"
    },
    {
        "paper_id": 2209.12542,
        "authors": "Qi Chen, Hong-tao Wang and Chao Guo",
        "title": "A Hamiltonian Approach to Floating Barrier Option Pricing",
        "comments": "26 pages, 5 figures",
        "journal-ref": null,
        "doi": "10.1007/s10955-023-03209-0",
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Hamiltonian approach in quantum mechanics provides a new thinking for barrier\noption pricing. For proportional floating barrier step options, the option\nprice changing process is similar to the one dimensional trapezoid potential\nbarrier scattering problem in quantum mechanics; for floating double-barrier\nstep options, the option price changing process is analogous to a particle\nmoving in a finite symmetric square potential well. Using Hamiltonian\nmethodology, the analytical expressions of pricing kernel and option price\ncould be derived. Numerical results of option price as a function of underlying\nprice, floating rate, interest rate and exercise price are shown, which are\nconsistent with the results given by mathematical calculations.\n"
    },
    {
        "paper_id": 2209.12636,
        "authors": "Deb Narayan Barik and Siddhartha P. Chakrabarty",
        "title": "Does limited liability reduce leveraged risk?: The case of loan\n  portfolio management",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Return-risk models are the two pillars of modern portfolio theory, which are\nwidely used to make decisions in choosing the loan portfolio of a bank. Banks\nand other financial institutions are subjected to limited liability protection.\nHowever, in most of the model formulation, limited liability is not taken into\nconsideration. Accordingly, to address this, we have, in this article, analyzed\nthe effect of including it in the model formulation. We formulate four models,\ntwo of them are maximizing the expected return with risk constraint, including\nand excluding limited-liability, and other two are minimization of risk with\nthreshold level of return with and without limited-liability. Our theoretical\nresults show that the solutions of the models with limited-liability produce\nbetter results than the others, in both minimizing risk and maximizing expected\nreturn. It has less risky investment than the other portfolio that solves the\nother model. Finally, an illustrative example is presented to support the\ntheoretical results obtained.\n"
    },
    {
        "paper_id": 2209.12639,
        "authors": "Masaaki Fujii",
        "title": "Equilibrium pricing of securities in the co-presence of cooperative and\n  non-cooperative populations",
        "comments": "revised version, forthcoming in ESAIM: Control, Optimisation and\n  Calculus of Variations",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-sa/4.0/",
        "abstract": "  In this work, we develop an equilibrium model for price formation of\nsecurities in a market composed of two populations of different types: the\nfirst one consists of cooperative agents, while the other one consists of\nnon-cooperative agents. The trading of every cooperative member is assumed to\nbe coordinated by a central planner. In the large population limit, the problem\nfor the central planner is shown to be a conditional extended mean-field\ncontrol. In addition to the convexity assumptions, if the relative size of the\ncooperative population is small enough, then we are able to show the existence\nof a unique equilibrium for both the finite-agent and the mean-field models.\nThe strong convergence to the mean-field model is also proved under the same\nconditions.\n"
    },
    {
        "paper_id": 2209.12664,
        "authors": "Jatin Nainani (1), Nirman Taterh (1), Md Ausaf Rashid (1), Ankit\n  Khivasara (1) ((1) K. J. Somaiya College of Engineering)",
        "title": "Feature-Rich Long-term Bitcoin Trading Assistant",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  For a long time predicting, studying and analyzing financial indices has been\nof major interest for the financial community. Recently, there has been a\ngrowing interest in the Deep-Learning community to make use of reinforcement\nlearning which has surpassed many of the previous benchmarks in a lot of\nfields. Our method provides a feature rich environment for the reinforcement\nlearning agent to work on. The aim is to provide long term profits to the user\nso, we took into consideration the most reliable technical indicators. We have\nalso developed a custom indicator which would provide better insights of the\nBitcoin market to the user. The Bitcoin market follows the emotions and\nsentiments of the traders, so another element of our trading environment is the\noverall daily Sentiment Score of the market on Twitter. The agent is tested for\na period of 685 days which also included the volatile period of Covid-19. It\nhas been capable of providing reliable recommendations which give an average\nprofit of about 69%. Finally, the agent is also capable of suggesting the\noptimal actions to the user through a website. Users on the website can also\naccess the visualizations of the indicators to help fortify their decisions.\n"
    },
    {
        "paper_id": 2209.12855,
        "authors": "Valeria Bignozzi, Luca Merlo, Lea Petrella",
        "title": "Inter-order relations between moments of a Student $t$ distribution,\n  with an application to $L_p$-quantiles",
        "comments": "18 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  This paper introduces inter-order formulas for partial and complete moments\nof a Student $t$ distribution with $n$ degrees of freedom. We show how the\npartial moment of order $n - j$ about any real value $m$ can be expressed in\nterms of the partial moment of order $j - 1$ for $j$ in $\\{1,\\dots, n \\}$.\nClosed form expressions for the complete moments are also established. We then\nfocus on $L_p$-quantiles, which represent a class of generalized quantiles\ndefined through an asymmetric $p$-power loss function. Based on the results\nobtained, we also show that for a Student $t$ distribution the\n$L_{n-j+1}$-quantile and the $L_j$-quantile coincide at any confidence level\n$\\tau$ in $(0, 1)$.\n"
    },
    {
        "paper_id": 2209.12863,
        "authors": "Thilo Reintjes",
        "title": "Automatic Identification and Classification of Share Buybacks and their\n  Effect on Short-, Mid- and Long-Term Returns",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
        "abstract": "  This thesis investigates share buybacks, specifically share buyback\nannouncements. It addresses how to recognize such announcements, the excess\nreturn of share buybacks, and the prediction of returns after a share buyback\nannouncement. We illustrate two NLP approaches for the automated detection of\nshare buyback announcements. Even with very small amounts of training data, we\ncan achieve an accuracy of up to 90%. This thesis utilizes these NLP methods to\ngenerate a large dataset consisting of 57,155 share buyback announcements. By\nanalyzing this dataset, this thesis aims to show that most companies, which\nhave a share buyback announced are underperforming the MSCI World. A minority\nof companies, however, significantly outperform the MSCI World. This\nsignificant overperformance leads to a net gain when looking at the averages of\nall companies. If the benchmark index is adjusted for the respective size of\nthe companies, the average overperformance disappears, and the majority\nunderperforms even greater. However, it was found that companies that announce\na share buyback with a volume of at least 1% of their market cap, deliver, on\naverage, a significant overperformance, even when using an adjusted benchmark.\nIt was also found that companies that announce share buybacks in times of\ncrisis emerge better than the overall market. Additionally, the generated\ndataset was used to train 72 machine learning models. Through this, it was able\nto find many strategies that could achieve an accuracy of up to 77% and\ngenerate great excess returns. A variety of performance indicators could be\nimproved across six different time frames and a significant overperformance was\nidentified. This was achieved by training several models for different tasks\nand time frames as well as combining these different models, generating\nsignificant improvement by fusing weak learners, in order to create one strong\nlearner.\n"
    },
    {
        "paper_id": 2209.12885,
        "authors": "P.D. Hinds and M.V. Tretyakov",
        "title": "Neural variance reduction for stochastic differential equations",
        "comments": "Updated version",
        "journal-ref": "Journal of Computational Finance, VOLUME 27, NUMBER 3 (2023)",
        "doi": "10.21314/JCF.2023.010",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Variance reduction techniques are of crucial importance for the efficiency of\nMonte Carlo simulations in finance applications. We propose the use of neural\nSDEs, with control variates parameterized by neural networks, in order to learn\napproximately optimal control variates and hence reduce variance as\ntrajectories of the SDEs are being simulated. We consider SDEs driven by\nBrownian motion and, more generally, by L\\'{e}vy processes including those with\ninfinite activity. For the latter case, we prove optimality conditions for the\nvariance reduction. Several numerical examples from option pricing are\npresented.\n"
    },
    {
        "paper_id": 2209.13054,
        "authors": "Giulia Di Nunno and Anton Yurchenko-Tytarenko",
        "title": "Sandwiched Volterra Volatility model: Markovian approximations and\n  hedging",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  We consider stochastic volatility dynamics driven by a general H\\\"older\ncontinuous Volterra-type noise and with unbounded drift. For these so-called\nSVV-models, we consider the explicit computation of quadratic hedging\nstrategies. While the theoretical hedge is well-known in terms of the\nnon-anticipating derivative for all square integrable claims, the fact that\nthese models are typically non-Markovian provides is a challenge in the direct\ncomputation of conditional expectations at the core of the explicit hedging\nstrategy. To overcome this difficulty, we propose a Markovian approximation of\nthe model which stems from an adequate approximation of the kernel in the\nVolterra noise. We study the approximation of the volatility, of the prices and\nof the optimal mean-square hedge. We provide the corresponding error estimates.\nThe work is completed with numerical simulations.\n"
    },
    {
        "paper_id": 2209.13102,
        "authors": "Farhana Rahman",
        "title": "Discount Puzzle Of Closed-End Mutual Funds: A Case Of Bangladesh",
        "comments": "International Journal of Accounting and Financial Management Research\n  (IJAFMR)",
        "journal-ref": "International Journal of Accounting and Financial Management\n  Research (IJAFMR), Volume 12, Issue 2, 2022, P(1-14)",
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  The paper intends to perform a relevant study on the closed-end fund puzzle\nin the perspective of an emerging market. Quarterly data of 36 closed-end\nmutual funds traded in Dhaka Stock Exchange are collected over the sample\nperiod of 2016 to 2019. Dependent and independent variables are mapped down by\nexploring previous researches. Weight of top 10 investments, fund size, fund\nage, fund maturity, turnover and dividend yield are taken as explanatory\nvariable to analyze the impact on CEF discount. A fixed effects panel\nregression is performed on the data set with few diagnostic tests to ensure the\nreliability of the analysis conducted. The results show that, the variable fund\nsize and fund maturity have a significant positive and turnover has a\nsignificant negative impact on CEF discount while the impact of weight of top\n10 investments, dividend yield and fund age are found insignificant.\n"
    },
    {
        "paper_id": 2209.13314,
        "authors": "Marina Marena, Andrea Romeo and Patrizia Semeraro",
        "title": "Non-maturing deposits modelling in a Ornstein-Uhlenbeck framework",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper builds a multivariate L\\'evy-driven Ornstein-Uhlenbeck process for\nthe management of non-maturing deposits, that are a major source of funding for\nbanks. The contribution of the paper is both theoretical and operational. On\nthe theoretical side, the novelty of this model is to include three independent\nsources of randomness in a L\\'evy framework: market interest rates, deposit\nrates and deposit volumes. The choice of a L\\'evy background driving process\nallows us to model rare but severe events. On the operational side, we propose\na procedure to include severe volume outflows with positive probability in\nfuture scenarios simulation, explaining its implementation with an illustrative\nexample using Italian banking sector data.\n"
    },
    {
        "paper_id": 2209.13334,
        "authors": "Aur\\'elien Alfonsi and Edoardo Lombardo",
        "title": "High order approximations of the Cox-Ingersoll-Ross process semigroup\n  using random grids",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We present new high order approximations schemes for the Cox-Ingersoll-Ross\n(CIR) process that are obtained by using a recent technique developed by\nAlfonsi and Bally (2021) for the approximation of semigroups. The idea consists\nin using a suitable combination of discretization schemes calculated on\ndifferent random grids to increase the order of convergence. This technique\ncoupled with the second order scheme proposed by Alfonsi (2010) for the CIR\nleads to weak approximations of order $2k$, for all $k\\in\\mathbb{N}^*$. Despite\nthe singularity of the square-root volatility coefficient, we show rigorously\nthis order of convergence under some restrictions on the volatility parameters.\nWe illustrate numerically the convergence of these approximations for the CIR\nprocess and for the Heston stochastic volatility model and show the\ncomputational time gain they give.\n"
    },
    {
        "paper_id": 2209.1347,
        "authors": "Mikrajuddin Abdullah",
        "title": "Introducing Cashless Transaction Index based on the Effective Medium\n  Approximation",
        "comments": "24 pages, 12 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
        "abstract": "  The effective medium approximation (EMA) method is commonly used to estimate\nthe effective conductivity development in composites containing two types of\nmaterials: conductors and insulators. The effective conductivity is a global\nparameter that measures how easily the composite conducts electric current.\nCurrently, financial transactions in society take place in cash or cashless,\nand, in the cashless transactions the money flows faster than in the cash\ntransactions. Therefore, to provide a cashless grading of countries, we\nintroduce a cashless transaction index (CTI) which is calculated using the EMA\nmethod in which individuals who make cash transactions are analogous to the\ninsulator element in the composite and individuals who make cash transactions\nare analogous to the conductor element. We define the CTI as the logarithmic of\nthe effective conductivity of a country's transactions. We also introduce the\ntime dependent equation for the cashless share. Estimates from the proposed\nmodel can explain well the data in the last few years.\n"
    },
    {
        "paper_id": 2209.13596,
        "authors": "Ibrahim Halil Efendioglu and Yakup Durmaz",
        "title": "The Impact of Perceptions of Social Media Advertisements on Advertising\n  Value, Brand Awareness and Brand Associations: Research on Generation Y\n  Instagram Users",
        "comments": null,
        "journal-ref": null,
        "doi": "10.33182/tmj.v10i2.1606",
        "license": "http://creativecommons.org/publicdomain/zero/1.0/",
        "abstract": "  The aim of this paper is to examine how consumer perceptions of social media\nadvertisements affect advertising value and brand awareness. With a rapid\nincrease in the number of social media users over the last ten years, a new\nadvertising domain has become available for companies. Brands that manage\nsocial media well in their advertising strategies can quickly influence\nconsumer decision-making and create awareness. However, in social media\nadvertising, which is different from traditional advertising, creating content\nshould be produced and this content should be perceived in a short time by\nconsumers. To achieve this, it is necessary to build rapport with consumers and\nto present correctly what they wish to see in advertisements by creating\nawareness. In view of the increasing importance of social media advertising,\nthe study examines how consumer perceptions of Instagram advertisements affect\nadvertising value and brand awareness. This study was conducted with Generation\nY consumers on the basis of their Instagram habits, a popular social media app.\nFor this purpose, surveys were held with 665 participants who use Instagram.\nThe collected data were analyzed using structural equation modeling. According\nto the analysis results, Y generations perceptions of Instagram advertisements\nhave both a positive and negative impact on advertising value and brand\nawareness and brand associations.\n"
    },
    {
        "paper_id": 2209.13623,
        "authors": "Andrew Y. Chen and Tom Zimmermann",
        "title": "Publication Bias in Asset Pricing Research",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Researchers are more likely to share notable findings. As a result, published\nfindings tend to overstate the magnitude of real-world phenomena. This bias is\na natural concern for asset pricing research, which has found hundreds of\nreturn predictors and little consensus on their origins.\n  Empirical evidence on publication bias comes from large scale meta-studies.\nMeta-studies of cross-sectional return predictability have settled on four\nstylized facts that demonstrate publication bias is not a dominant factor: (1)\nalmost all findings can be replicated, (2) predictability persists\nout-of-sample, (3) empirical $t$-statistics are much larger than 2.0, and (4)\npredictors are weakly correlated. Each of these facts has been demonstrated in\nat least three meta-studies.\n  Empirical Bayes statistics turn these facts into publication bias\ncorrections. Estimates from three meta-studies find that the average correction\n(shrinkage) accounts for only 10 to 15 percent of in-sample mean returns and\nthat the risk of inference going in the wrong direction (the false discovery\nrate) is less than 10%.\n  Meta-studies also find that $t$-statistic hurdles exceed 3.0 in multiple\ntesting algorithms and that returns are 30 to 50 percent weaker in alternative\nportfolio tests. These facts are easily misinterpreted as evidence of\npublication bias effects. We clarify these misinterpretations and others,\nincluding the conflating of ``mostly false findings'' with ``many insignificant\nfindings,'' ``data snooping'' with ``liquidity effects,'' and ``failed\nreplications'' with ``insignificant ad-hoc trading strategies.''\n  Meta-studies outside of the cross-sectional literature are rare. The four\nfacts from cross-sectional meta-studies provide a framework for future\nresearch. We illustrate with a preliminary re-examination of equity premium\npredictability.\n"
    },
    {
        "paper_id": 2209.13932,
        "authors": "R\\'emi J\\'ez\\'equel, Dmitrii M. Ostrovskii (USC), Pierre Gaillard",
        "title": "Efficient and Near-Optimal Online Portfolio Selection",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In the problem of online portfolio selection as formulated by Cover (1991),\nthe trader repeatedly distributes her capital over $ d $ assets in each of $ T\n> 1 $ rounds, with the goal of maximizing the total return. Cover proposed an\nalgorithm, termed Universal Portfolios, that performs nearly as well as the\nbest (in hindsight) static assignment of a portfolio, with an $ O(d\\log(T)) $\nregret in terms of the logarithmic return. Without imposing any restrictions on\nthe market this guarantee is known to be worst-case optimal, and no other\nalgorithm attaining it has been discovered so far. Unfortunately, Cover's\nalgorithm crucially relies on computing certain $ d $-dimensional integral\nwhich must be approximated in any implementation; this results in a prohibitive\n$ \\tilde O(d^4(T+d)^{14}) $ per-round runtime for the fastest known\nimplementation due to Kalai and Vempala (2002). We propose an algorithm for\nonline portfolio selection that admits essentially the same regret guarantee as\nUniversal Portfolios -- up to a constant factor and replacement of $ \\log(T) $\nwith $ \\log(T+d) $ -- yet has a drastically reduced runtime of $ \\tilde\nO(d^2(T+d)) $ per round. The selected portfolio minimizes the current\nlogarithmic loss regularized by the log-determinant of its Hessian --\nequivalently, the hybrid logarithmic-volumetric barrier of the polytope\nspecified by the asset return vectors. As such, our work reveals surprising\nconnections of online portfolio selection with two classical topics in\noptimization theory: cutting-plane and interior-point algorithms.\n"
    },
    {
        "paper_id": 2209.14,
        "authors": "Ulrich Matter, Roland Hodler, Johannes Ladwig",
        "title": "Personalization of Web Search During the 2020 US Elections",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-sa/4.0/",
        "abstract": "  Search engines play a central role in routing political information to\ncitizens. The algorithmic personalization of search results by large search\nengines like Google implies that different users may be offered systematically\ndifferent information. However, measuring the causal effect of user\ncharacteristics and behavior on search results in a politically relevant\ncontext is challenging. We set up a population of 150 synthetic internet users\n(\"bots\") who are randomly located across 25 US cities and are active for\nseveral months during the 2020 US Elections and their aftermath. These users\ndiffer in their browsing preferences and political ideology, and they build up\nrealistic browsing and search histories. We run daily experiments in which all\nusers enter the same election-related queries. Search results to these queries\ndiffer substantially across users. Google prioritizes previously visited\nwebsites and local news sites. Yet, it does not generally prioritize websites\nfeaturing the user's ideology.\n"
    },
    {
        "paper_id": 2209.14443,
        "authors": "Gbatsoron Anjande, Simeon T Asom, Ngutsav Ayila, Bridget Ngodoo Mile,\n  Victor Ushahemba Ijirshar",
        "title": "Government Spending and Money Supply Roles in Alleviating Poverty in\n  Africa",
        "comments": "28 pages, 1 figure, 3 Tables",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  This study examines the roles of government spending and money supply on\nalleviating poverty in Africa. The study used 48 Sub-Saharan Africa countries\nfrom 2001 to 2017. The study employed one step and two-step system GMM and\nfound that both the procedures have similar results. Different specifications\nwere employed and the model selected was robust, with valid instruments and\nabsence of autocorrelation at the second order. The study revealed that\ngovernment spending and foreign direct investment have significant negative\ninfluence on reducing poverty while money supply has positive influence on the\nlevel of poverty in the region. The implication of the finding is that monetary\npolicy tool of money supply has no strong influence in combating the menace of\npoverty. The study therefore recommends that emphasis should be placed on\nincreasing more of government spending that would impact on the quality of life\nof the people in the region through multiplier effect, improving the financial\nsystem for effective monetary policy and attracting foreign direct inflows\nthrough enabling business environment in Africa.\n"
    },
    {
        "paper_id": 2209.14505,
        "authors": "Yihsu Chen, Andrew L. Liu, Makoto Tanaka, and Ryuta Takashima",
        "title": "Optimal Retail Tariff Design with Prosumers: Pursuing Equity at the\n  Expenses of Economic Efficiencies?",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  Distributed renewable resources owned by prosumers can be an effective way of\nfortifying grid resilience and enhancing sustainability. However, prosumers\nserve their own interests and their objectives are unlikely to align with that\nof society. This paper develops a bilevel model to study the optimal design of\nretail electricity tariffs considering the balance between economic efficiency\nand energy equity. The retail tariff entails a fixed charge and a volumetric\ncharge tied to electricity usage to recover utilities' fixed costs. We analyze\nsolution properties of the bilevel problem and prove an optimal rate design,\nwhich is to use fixed charges to recover fixed costs and to balance energy\nequity among different income groups. This suggests that programs similar to\nCARE (California Alternative Rate of Energy), which offer lower retail rates to\nlow-income households, are unlikely to be efficient, even if they are\npolitically appealing.\n"
    },
    {
        "paper_id": 2209.14532,
        "authors": "Jun Lu, Joerg Osterrieder",
        "title": "Feature Selection via the Intervened Interpolative Decomposition and its\n  Application in Diversifying Quantitative Strategies",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  In this paper, we propose a probabilistic model for computing an\ninterpolative decomposition (ID) in which each column of the observed matrix\nhas its own priority or importance, so that the end result of the decomposition\nfinds a set of features that are representative of the entire set of features,\nand the selected features also have higher priority than others. This approach\nis commonly used for low-rank approximation, feature selection, and extracting\nhidden patterns in data, where the matrix factors are latent variables\nassociated with each data dimension. Gibbs sampling for Bayesian inference is\napplied to carry out the optimization. We evaluate the proposed models on\nreal-world datasets, including ten Chinese A-share stocks, and demonstrate that\nthe proposed Bayesian ID algorithm with intervention (IID) produces comparable\nreconstructive errors to existing Bayesian ID algorithms while selecting\nfeatures with higher scores or priority.\n"
    },
    {
        "paper_id": 2209.14549,
        "authors": "Devang Sinha and Siddhartha P. Chakrabarty",
        "title": "Multilevel Monte Carlo and its Applications in Financial Engineering",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  In this article, we present a review of the recent developments on the topic\nof Multilevel Monte Carlo (MLMC) algorithm, in the paradigm of applications in\nfinancial engineering. We specifically focus on the recent studies conducted in\ntwo subareas, namely, option pricing and financial risk management. For the\nformer, the discussion involves incorporation of the importance sampling\nalgorithm, in conjunction with the MLMC estimator, thereby constructing a\nhybrid algorithm in order to achieve reduction for the overall variance of the\nestimator. In case of the latter, we discuss the studies carried out in order\nto construct an efficient algorithm in order to estimate the risk measures of\nValue-at-Risk (VaR) and Conditional Var (CVaR), in an efficient manner. In this\nregard, we briefly discuss the motivation and the construction of an adaptive\nsampling algorithm with an aim to efficiently estimate the nested expectation,\nwhich, in general is computationally expensive.\n"
    },
    {
        "paper_id": 2209.14631,
        "authors": "Nassim Nicholas Taleb, Jeffrey West",
        "title": "Working With Convex Responses: Antifragility From Finance to Oncology",
        "comments": "arXiv admin note: text overlap with arXiv:1808.00065 Final accepted\n  version in Entropy",
        "journal-ref": null,
        "doi": "10.3390/e25020343",
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  We extend techniques and learnings about the stochastic properties of\nnonlinear responses from finance to medicine, particularly oncology where it\ncan inform dosing and intervention. We define antifragility. We propose uses of\nrisk analysis to medical problems, through the properties of nonlinear\nresponses (convex or concave). We 1) link the convexity/concavity of the\ndose-response function to the statistical properties of the results; 2) define\n\"antifragility\" as a mathematical property for local beneficial convex\nresponses and the generalization of \"fragility\" as its opposite, locally\nconcave in the tails of the statistical distribution; 3) propose mathematically\ntractable relations between dosage, severity of conditions, and iatrogenics. In\nshort we propose a framework to integrate the necessary consequences of\nnonlinearities in evidence-based oncology and more general clinical risk\nmanagement.\n"
    },
    {
        "paper_id": 2209.14726,
        "authors": "Martin Keller-Ressel",
        "title": "W-shaped implied volatility curves in a variance-gamma mixture model",
        "comments": "12 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In liquid option markets, W-shaped implied volatility curves have\noccasionally be observed. We show that such shapes can be reproduced in a\nmixture of two variance-gamma models. This is in contrast to lognormal models,\nwhere at least three different distributions have to be mixed in order to\nproduce a W-shape, as recently shown by Glasserman and Pirjol.\n"
    },
    {
        "paper_id": 2209.14928,
        "authors": "Evgeny Goncharov, Alexandre Rodrigues",
        "title": "Modifications to a classic BFGS library for use with SIMD-equipped\n  hardware and an AAD library",
        "comments": "9 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  We introduce certain modifications of the BFGS method for functions that are\nnot parallelizable by nature (having consecutive operations only) taking\nadvantage of SIMD. We also provide a modified LBFGS\\texttt{++} library that\ntakes advantage of these modifications, and the use of AAD, and give an\ninterface for AAD users that takes advantage of the modified library\nautomatically. We give two examples to illustrate the performance. The modified\nlibrary is up to 3.8 times faster for European Swaption curve calibration in\nORE (not parallelizable) and 1.4 times faster for calibrating the LMM model by\na set of European options.\n"
    },
    {
        "paper_id": 2209.15037,
        "authors": "Beatrice Acciaio, Julio Backhoff and Gudmund Pammer",
        "title": "Quantitative Fundamental Theorem of Asset Pricing",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper we provide a quantitative analysis to the concept of arbitrage,\nthat allows to deal with model uncertainty without imposing the no-arbitrage\ncondition. In markets that admit ``small arbitrage\", we can still make sense of\nthe problems of pricing and hedging. The pricing measures here will be such\nthat asset price processes are close to being martingales, and the hedging\nstrategies will need to cover some additional cost. We show a quantitative\nversion of the Fundamental Theorem of Asset Pricing and of the\nSuper-Replication Theorem. Finally, we study robustness of the amount of\narbitrage and existence of respective pricing measures, showing stability of\nthese concepts with respect to a strong adapted Wasserstein distance.\n"
    },
    {
        "paper_id": 2209.15293,
        "authors": "A. N. M. Sajedul Alam, Junaid Bin Kibria, Arnob Kumar Dey, Zawad Alam,\n  Shifat Zaman, Motahar Mahtab, Mohammed Julfikar Ali Mahbub, Annajiat Alim\n  Rasel",
        "title": "A Survey: Credit Sentiment Score Prediction",
        "comments": "16 pages, 3 figures, 3 tables",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Manual approvals are still used by banks and other NGOs to approve loans. It\ntakes time and is prone to mistakes because it is controlled by a bank\nemployee. Several fields of machine learning mining technologies have been\nutilized to enhance various areas of credit rating forecast. A major goal of\nthis research is to look at current sentiment analysis techniques that are\nbeing used to generate creditworthiness.\n"
    },
    {
        "paper_id": 2209.15429,
        "authors": "Pierre Carmier",
        "title": "Generalized second law of thermodynamics in the Glosten-Milgrom model",
        "comments": "13 pages, 6 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We derive an upper bound for the expected gain of informed traders in the\nGlosten-Milgrom model with finite horizon, fully analogous to a generalized\nsecond law of thermodynamics. This result extends that obtained by Touzo et al.\na couple of years ago. The proof relies on Bayesian inference (exploiting the\ninvariance of the problem under consecutive game sequences) and an interesting\nentropic inequality. We also provide numerical results both supporting the\nexistence of a characteristic timescale in the model and illustrating the\nmagnitude of gain fluctuations. Other possible extensions are discussed.\n"
    },
    {
        "paper_id": 2209.15496,
        "authors": "Maxime Biehler, Mohamed Guermazi and C\\'elim Starck",
        "title": "Using Knowledge Distillation to improve interpretable models in a retail\n  banking context",
        "comments": "25 pages, 9 figures, 11 tables",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  This article sets forth a review of knowledge distillation techniques with a\nfocus on their applicability to retail banking contexts. Predictive machine\nlearning algorithms used in banking environments, especially in risk and\ncontrol functions, are generally subject to regulatory and technical\nconstraints limiting their complexity. Knowledge distillation gives the\nopportunity to improve the performances of simple models without burdening\ntheir application, using the results of other - generally more complex and\nbetter-performing - models. Parsing recent advances in this field, we highlight\nthree main approaches: Soft Targets, Sample Selection and Data Augmentation. We\nassess the relevance of a subset of such techniques by applying them to open\nsource datasets, before putting them to the test on the use cases of BPCE, a\nmajor French institution in the retail banking sector. As such, we demonstrate\nthe potential of knowledge distillation to improve the performance of these\nmodels without altering their form and simplicity.\n"
    },
    {
        "paper_id": 2209.15569,
        "authors": "Matheus V. X. Ferreira and David C. Parkes",
        "title": "Credible Decentralized Exchange Design via Verifiable Sequencing Rules",
        "comments": "34 Pages, 55th ACM Symposium on Theory of Computing (STOC 2023)",
        "journal-ref": "STOC 2023: Proceedings of the 55th Annual ACM Symposium on Theory\n  of Computing, 2023, 723-736",
        "doi": "10.1145/3564246.3585233",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Trading on decentralized exchanges has been one of the primary use cases for\npermissionless blockchains with daily trading volume exceeding billions of\nU.S.~dollars. In the status quo, users broadcast transactions and miners are\nresponsible for composing a block of transactions and picking an execution\nordering -- the order in which transactions execute in the exchange. Due to the\nlack of a regulatory framework, it is common to observe miners exploiting their\nprivileged position by front-running transactions and obtaining risk-fee\nprofits. In this work, we propose to modify the interaction between miners and\nusers and initiate the study of {\\em verifiable sequencing rules}. As in the\nstatus quo, miners can determine the content of a block; however, they commit\nto respecting a sequencing rule that constrains the execution ordering and is\nverifiable (there is a polynomial time algorithm that can verify if the\nexecution ordering satisfies such constraints). Thus in the event a miner\ndeviates from the sequencing rule, anyone can generate a proof of\nnon-compliance. We ask if there are sequencing rules that limit price\nmanipulation from miners in a two-token liquidity pool exchange. Our first\nresult is an impossibility theorem: for any sequencing rule, there is an\ninstance of user transactions where the miner can obtain non-zero risk-free\nprofits. In light of this impossibility result, our main result is a verifiable\nsequencing rule that provides execution price guarantees for users. In\nparticular, for any user transaction A, it ensures that either (1) the\nexecution price of A is at least as good as if A was the only transaction in\nthe block, or (2) the execution price of A is worse than this ``standalone''\nprice and the miner does not gain (or lose) when including A in the block.\n"
    },
    {
        "paper_id": 2210.00067,
        "authors": "Deborah Salon, Laura Mirtich, Matthew Wigginton Bhagat-Conway, Adam\n  Costello, Ehsan Rahimi, Abolfazl (Kouros) Mohammadian, Rishabh Singh Chauhan,\n  Sybil Derrible, Denise da Silva Baker, Ram M. Pendyala",
        "title": "The COVID-19 Pandemic and the Future of Telecommuting in the United\n  States",
        "comments": "54 pages, to be published in Transportation Research Part D,\n  Transport and Environment",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  This study focuses on an important transport-related long-term effect of the\nCOVID-19 pandemic in the United States: an increase in telecommuting. Analyzing\na nationally representative panel survey of adults, we find that 40-50% of\nworkers expect to telecommute at least a few times per month post-pandemic, up\nfrom 24% pre-COVID. If given the option, 90-95% of those who first telecommuted\nduring the pandemic plan to continue the practice regularly. We also find that\nnew telecommuters are demographically similar to pre-COVID telecommuters. Both\npre- and post-COVID, higher educational attainment and income, together with\ncertain job categories, largely determine whether workers have the option to\ntelecommute. Despite growth in telecommuting, approximately half of workers\nexpect to remain unable to telecommute and between 2/3 and 3/4 of workers\nexpect their post-pandemic telecommuting patterns to be unchanged from their\npre-COVID patterns. This limits the contribution telecommuting can make to\nreducing peak hour transport demand.\n"
    },
    {
        "paper_id": 2210.00128,
        "authors": "Amirhesam Badeanlou, Andrea Araldo, Marco Diana, Vincent Gauthier",
        "title": "Equity Scores for Public Transit Lines from Open-Data and Accessibility\n  Measures",
        "comments": null,
        "journal-ref": "Research Board (TRB) 102nd Annual Meeting, 2023",
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Current transit suffers from an evident inequity: the level of service of\ntransit in suburbs is much less satisfying than in city centers. As a\nconsequence, private cars are still the dominant transportation mode for\nsuburban people, which results in congestion and pollution. To achieve\nsustainability goals and reduce car-dependency, transit should be (re)designed\naround equity. To this aim, it is necessary to (i) quantify the \"level of\nequity\" of the transit system and (ii) provide an indicator that scores the\ntransit lines that contribute the most to keep transit equitable. This\nindicator could suggest on which lines the transit operator must invest to\nincrease the service level (frequency or coverage) in order to reduce inequity\nin the system.\n  To the best of our knowledge, this paper is the first to tackle (ii). To this\naim, we propose efficient scoring methods that rely solely on open data, which\nallows us to perform the analysis on multiple cities (7 in this paper). Our\nmethod can be used to guide large-scale iterative optimization algorithms to\nimprove accessibility equity.\n"
    },
    {
        "paper_id": 2210.00138,
        "authors": "Fernanda Estevan, Lucas Finamor",
        "title": "School closures and educational path: how the Covid-19 pandemic affected\n  transitions to college",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We investigate the impact of the Covid-19 pandemic on the transition between\nhigh school and college in Brazil. Using microdata from the universe of\nstudents that applied to a selective university, we document how the Covid-19\nshock increased enrollment for students in the top 10% high-quality public and\nprivate high schools. This increase comes at the expense of graduates from\nrelatively lower-quality schools. Furthermore, this effect is entirely driven\nby applicants who were at high school during the Covid pandemic. The effect is\nlarge and completely offsets the gains in student background diversity achieved\nby a bold quota policy implemented years before Covid. These results suggest\nthat not only students from underprivileged backgrounds endured larger negative\neffects on learning during the pandemic, but they also experienced a stall in\ntheir educational paths.\n"
    },
    {
        "paper_id": 2210.00143,
        "authors": "Bridget Ngodoo Mile, Victor Ushahemba Ijirshar, Mlumun Queen Ijirshar",
        "title": "The impact of SMEs on employment creation in Makurdi metropolis of Benue\n  state",
        "comments": "4 Tables",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  SMEs remain a veritable tool that generates employment opportunities. This\nstudy examined the impact of SMEs on employment creation in the Makurdi\nmetropolis of Benue state. A sample size of 340 entrepreneurs was chosen from\nthe population of entrepreneurs (SMEs) in the Makurdi metropolis. The study\nused logistic regression to analyse the impact of SME activities on employment\ncreation or generation in the state and found that SMEs contribute\nsignificantly to employment creation in the state but are often faced with the\nchallenges of lack of capital, absence of business planning, lack of confidence\nin the face of competition, unfavorable environment for the development of\nSMEs, high government taxes and inadequate technical knowledge. The study\ntherefore recommended that the government should implement capital or credit\nenhancing programmes and an enabling environment for smooth running of the\nSMEs. Tax incentives should also be granted to infant enterprises, and tax\nadministration should be monitored to avoid excessive tax rates imposed by tax\ncollectors.\n"
    },
    {
        "paper_id": 2210.00488,
        "authors": "Avichai Snir, Haipeng (Allan) Chen, and Daniel Levy",
        "title": "Zero-Ending Prices, Cognitive Convenience, and Price Rigidity",
        "comments": "Journal of Economic Behavior and Organization (Forthcoming), 89\n  pages, 50 pages paper, 39 pages appendix",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  We assess the role of cognitive convenience in the popularity and rigidity of\n0 ending prices in convenience settings. Studies show that 0 ending prices are\ncommon at convenience stores because of the transaction convenience that 0\nending prices offer. Using a large store level retail CPI data, we find that 0\nending prices are popular and rigid at convenience stores even when they offer\nlittle transaction convenience. We corroborate these findings with two large\nretail scanner price datasets from Dominicks and Nielsen. In the Dominicks\ndata, we find that there are more 0 endings in the prices of the items in the\nfront end candies category than in any other category, even though these prices\nhave no effect on the convenience of the consumers check out transaction. In\naddition, in both Dominicks and Nielsens datasets, we find that 0 ending prices\nhave a positive effect on demand. Ruling out consumer antagonism and retailers\nuse of heuristics in pricing, we conclude that 0 ending prices are popular and\nrigid, and that they increase demand at convenience settings, not only for\ntheir transaction convenience, but also for the cognitive convenience they\noffer.\n"
    },
    {
        "paper_id": 2210.00731,
        "authors": "Sudeep R. Bapat, Saumya Kothari, and Rushil Bansal",
        "title": "Sentiment Analysis of ESG disclosures on Stock Market",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-sa/4.0/",
        "abstract": "  In this paper, we look at the impact of Environment, Social and Governance\nrelated news articles and social media data on the stock market performance. We\npick four stocks of companies which are widely known in their domain to\nunderstand the complete effect of ESG as the newly opted investment style\nremains restricted to only the stocks with widespread information. We summarise\nlive data of both twitter tweets and newspaper articles and create a sentiment\nindex using a dictionary technique based on online information for the month of\nJuly, 2022. We look at the stock price data for all the four companies and\ncalculate the percentage change in each of them. We also compare the overall\nsentiment of the company to its percentage change over a specific historical\nperiod.\n"
    },
    {
        "paper_id": 2210.00779,
        "authors": "Mouna Ben Derouich and Ahmed Kebaier",
        "title": "The interpolated drift implicit Euler scheme Multilevel Monte Carlo\n  method for pricing Barrier options and applications to the CIR and CEV models",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Recently, Giles et al. [14] proved that the efficiency of the Multilevel\nMonte Carlo (MLMC) method for evaluating Down-and-Out barrier options for a\ndiffusion process $(X_t)_{t\\in[0,T]}$ with globally Lipschitz coefficients, can\nbe improved by combining a Brownian bridge technique and a conditional Monte\nCarlo method provided that the running minimum $\\inf_{t\\in[0,T]}X_t$ has a\nbounded density in the vicinity of the barrier.\n  In the present work, thanks to the Lamperti transformation technique and\nusing a Brownian interpolation of the drift implicit Euler scheme of Alfonsi\n[2], we show that the efficiency of the MLMC can be also improved for the\nevaluation of barrier options for models with non-Lipschitz diffusion\ncoefficients under certain moment constraints. We study two example models: the\nCox-Ingersoll-Ross (CIR) and the Constant of Elasticity of Variance (CEV)\nprocesses for which we show that the conditions of our theoretical framework\nare satisfied under certain restrictions on the models parameters. In\nparticular, we develop semi-explicit formulas for the densities of the running\nminimum and running maximum of both CIR and CEV processes which are of\nindependent interest. Finally, numerical tests are processed to illustrate our\nresults.\n"
    },
    {
        "paper_id": 2210.00807,
        "authors": "\\'Alvaro Rubio-Garc\\'ia and Juan Jos\\'e Garc\\'ia-Ripoll and Diego\n  Porras",
        "title": "Portfolio optimization with discrete simulated annealing",
        "comments": "9 pages, 3 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Portfolio optimization is an important process in finance that consists in\nfinding the optimal asset allocation that maximizes expected returns while\nminimizing risk. When assets are allocated in discrete units, this is a\ncombinatorial optimization problem that can be addressed by quantum and\nquantum-inspired algorithms. In this work we present an integer simulated\nannealing method to find optimal portfolios in the presence of discretized\nconvex and non-convex cost functions. Our algorithm can deal with large size\nportfolios with hundreds of assets. We introduce a performance metric, the time\nto target, based on a lower bound to the cost function obtained with the\ncontinuous relaxation of the combinatorial optimization problem. This metric\nallows us to quantify the time required to achieve a solution with a given\nquality. We carry out numerical experiments and we benchmark the algorithm in\ntwo situations: (i) Monte Carlo instances are started at random, and (ii) the\nalgorithm is warm-started with an initial instance close to the continuous\nrelaxation of the problem. We find that in the case of warm-starting with\nconvex cost functions, the time to target does not grow with the size of the\noptimization problem, so discretized versions of convex portfolio optimization\nproblems are not hard to solve using classical resources. We have applied our\nmethod to the problem of re-balancing in the presence of non-convex transaction\ncosts, and we have found that our algorithm can efficiently minimize those\nterms.\n"
    },
    {
        "paper_id": 2210.0087,
        "authors": "Marshall R. McCraw",
        "title": "Multiclass Sentiment Prediction for Stock Trading",
        "comments": "5 pages, 11 figures, written for course credit in the spring semester\n  of 2020",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Python was used to download and format NewsAPI article data relating to 400\npublicly traded, low cap. Biotech companies. Crowd-sourcing was used to label a\nsubset of this data to then train and evaluate a variety of models to classify\nthe public sentiment of each company. The best performing models were then used\nto show that trading entirely off public sentiment could provide market beating\nreturns.\n"
    },
    {
        "paper_id": 2210.00876,
        "authors": "Jianlong Zhu, Dan Xian, Fengxiao, Yichen Nie",
        "title": "Embedding-based neural network for investment return prediction",
        "comments": "Accepted at 2022 2nd IEEE International Conference on Computer\n  Science, Electronic Information Engineering and Intelligent Control\n  Technology",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  In addition to being familiar with policies, high investment returns also\nrequire extensive knowledge of relevant industry knowledge and news. In\naddition, it is necessary to leverage relevant theories for investment to make\ndecisions, thereby amplifying investment returns. A effective investment return\nestimate can feedback the future rate of return of investment behavior. In\nrecent years, deep learning are developing rapidly, and investment return\nprediction based on deep learning has become an emerging research topic. This\npaper proposes an embedding-based dual branch approach to predict an\ninvestment's return. This approach leverages embedding to encode the investment\nid into a low-dimensional dense vector, thereby mapping high-dimensional data\nto a low-dimensional manifold, so that highdimensional features can be\nrepresented competitively. In addition, the dual branch model realizes the\ndecoupling of features by separately encoding different information in the two\nbranches. In addition, the swish activation function further improves the model\nperformance. Our approach are validated on the Ubiquant Market Prediction\ndataset. The results demonstrate the superiority of our approach compared to\nXgboost, Lightgbm and Catboost.\n"
    },
    {
        "paper_id": 2210.00883,
        "authors": "Federico D'Amario, Milos Ciganovic",
        "title": "Forecasting Cryptocurrencies Log-Returns: a LASSO-VAR and Sentiment\n  Approach",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Cryptocurrencies have become a trendy topic recently, primarily due to their\ndisruptive potential and reports of unprecedented returns. In addition,\nacademics increasingly acknowledge the predictive power of Social Media in many\nfields and, more specifically, for financial markets and economics. In this\npaper, we leverage the predictive power of Twitter and Reddit sentiment\ntogether with Google Trends indexes and volume to forecast the log returns of\nten cryptocurrencies. Specifically, we consider $Bitcoin$, $Ethereum$,\n$Tether$, $Binance Coin$, $Litecoin$, $Enjin Coin$, $Horizen$, $Namecoin$,\n$Peercoin$, and $Feathercoin$. We evaluate the performance of LASSO-VAR using\ndaily data from January 2018 to January 2022. In a 30 days recursive forecast,\nwe can retrieve the correct direction of the actual series more than 50% of the\ntime. We compare this result with the main benchmarks, and we see a 10%\nimprovement in Mean Directional Accuracy (MDA). The use of sentiment and\nattention variables as predictors increase significantly the forecast accuracy\nin terms of MDA but not in terms of Root Mean Squared Errors. We perform a\nGranger causality test using a post-double LASSO selection for high-dimensional\nVARs. Results show no \"causality\" from Social Media sentiment to\ncryptocurrencies returns\n"
    },
    {
        "paper_id": 2210.0095,
        "authors": "Ruoxin Xiao",
        "title": "Optimal consumption-investment choices under wealth-driven risk aversion",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  CRRA utility where the risk aversion coefficient is a constant is commonly\nseen in various economics models. But wealth-driven risk aversion rarely shows\nup in investor's investment problems. This paper mainly focus on numerical\nsolutions to the optimal consumption-investment choices under wealth-driven\naversion done by neural network. A jump-diffusion model is used to simulate the\nartificial data that is needed for the neural network training. The WDRA Model\nis set up for describing the investment problem and there are two parameters\nthat require to be optimized, which are the investment rate of the wealth on\nthe risky assets and the consumption during the investment time horizon. Under\nthis model, neural network LSTM with one objective function is implemented and\nshows promising results.\n"
    },
    {
        "paper_id": 2210.00984,
        "authors": "Jaydip Sen and Abhishek Dutta",
        "title": "A Comparative Study of Hierarchical Risk Parity Portfolio and Eigen\n  Portfolio on the NIFTY 50 Stocks",
        "comments": "This is the accepted version of our paper at the 2nd International\n  Conference on Computational Intelligence and Data Analytics, January 8 - 9,\n  2021, Hyderabad. The paper is 15 pages long and it contains 21 figures and 7\n  tables. arXiv admin note: substantial text overlap with arXiv:2202.02728",
        "journal-ref": null,
        "doi": "10.1007/978-981-19-3391-2_34",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Portfolio optimization has been an area of research that has attracted a lot\nof attention from researchers and financial analysts. Designing an optimum\nportfolio is a complex task since it not only involves accurate forecasting of\nfuture stock returns and risks but also needs to optimize them. This paper\npresents a systematic approach to portfolio optimization using two approaches,\nthe hierarchical risk parity algorithm and the Eigen portfolio on seven sectors\nof the Indian stock market. The portfolios are built following the two\napproaches to historical stock prices from Jan 1, 2016, to Dec 31, 2020. The\nportfolio performances are evaluated on the test data from Jan 1, 2021, to Nov\n1, 2021. The backtesting results of the portfolios indicate that the\nperformance of the HRP portfolio is superior to that of its Eigen counterpart\non both training and test data for the majority of the sectors studied.\n"
    },
    {
        "paper_id": 2210.00997,
        "authors": "Chung-En Tsai and Hao-Chung Cheng and Yen-Huan Li",
        "title": "Online Self-Concordant and Relatively Smooth Minimization, With\n  Applications to Online Portfolio Selection and Learning Quantum States",
        "comments": "34th Int. Conf. Algorithmic Learning Theory (ALT 2023). A typo in the\n  last equation in the proof of Lemma 10 is corrected",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Consider an online convex optimization problem where the loss functions are\nself-concordant barriers, smooth relative to a convex function $h$, and\npossibly non-Lipschitz. We analyze the regret of online mirror descent with\n$h$. Then, based on the result, we prove the following in a unified manner.\nDenote by $T$ the time horizon and $d$ the parameter dimension. 1. For online\nportfolio selection, the regret of $\\widetilde{\\text{EG}}$, a variant of\nexponentiated gradient due to Helmbold et al., is $\\tilde{O} ( T^{2/3} d^{1/3}\n)$ when $T > 4 d / \\log d$. This improves on the original $\\tilde{O} ( T^{3/4}\nd^{1/2} )$ regret bound for $\\widetilde{\\text{EG}}$. 2. For online portfolio\nselection, the regret of online mirror descent with the logarithmic barrier is\n$\\tilde{O}(\\sqrt{T d})$. The regret bound is the same as that of Soft-Bayes due\nto Orseau et al. up to logarithmic terms. 3. For online learning quantum states\nwith the logarithmic loss, the regret of online mirror descent with the\nlog-determinant function is also $\\tilde{O} ( \\sqrt{T d} )$. Its per-iteration\ntime is shorter than all existing algorithms we know.\n"
    },
    {
        "paper_id": 2210.01016,
        "authors": "Weidong Tian and Zimu Zhu",
        "title": "Smoothness of the Value Function for Optimal Consumption Model with\n  Consumption-Wealth Utility and Borrowing Constraint",
        "comments": "35 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper studies an optimal consumption-investment problem for an investor\nwhose instantaneous utility depends on both consumption and wealth, and the\ninvestor faces a general borrowing constraint that the investment amount in the\nrisky asset does not exceed an exogenous function of the wealth. We show that\nthe value function is second-order smooth and present the optimal\nconsumption-investment policy in a feedback form. Moreover, when the risky\ninvestment amount is bounded above by a fixed constant, we show that under\ncertain conditions, the constraint is binding if and only if an endogenous\nthreshold bounds the portfolio wealth, and we determine the endogenous wealth\nthreshold with the smooth fit condition. Our results encompass several\nwell-developed portfolio choice models and imply new applications.\n"
    },
    {
        "paper_id": 2210.01109,
        "authors": "Valeria Stefanelli, Francesco Manta, Pierluigi Toma",
        "title": "Digital financial services and open banking innovation: are banks\n  becoming invisible?",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Digitalization in many economic sectors drove the financial system to adapt\nto new paradigms of technological transformation. Moreover, the extant\nregulatory framework forced the financial system to reconsider its business\nmodels and its relationship with the market. Such reasons generated also in the\nbanking sector a new model of competition within the ecosystem never seen\nbefore in this sector. The new ecosystem of banks and financial institutions\nlacks a common framework that not only synthesizes the development lines of\nopen innovation in the banking sector, but also regarding the planification of\nstrategic choices and organisation within the new ecosystems. The present study\naims to inquire the strategic positioning of European banks toward their\ndigital transformation strategies, by analysing the decision-making processes\nthat occurred between 2015 and 2019. A qualitative analysis on partnerships and\nthe adoption of Application Programming Interfaces (APIs) development in\nsupport of new service models was carried out. Results have relevant policy\nimplications for regulators, linked to the business evolution and the risks of\noutsourcing, and managerial implications for the followers, specifically on the\nplan of service integration to diversify and boost their activities in the\nsegment of customer relationship management and care, providing a better user\nexperience.\n"
    },
    {
        "paper_id": 2210.01153,
        "authors": "Hongyan Chen, Pushpam Kumar and Tom Barker",
        "title": "Wetland Quality as a Determinant of Economic Value of Ecosystem\n  Services: an Exploration",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Wetland quality is a critical factor in determining values of wetland goods\nand services. However, in many studies on wetland valuation, wetland quality\nhas been ignored. While those studies might give useful information to the\nlocal people for decision-making, their lack of wetland quality information may\nlead to the difficulty in integrating wetland quality into a cross-studies\nresearch like meta-analysis. In a meta-analysis, a statistical regression\nfunction needs withdrawing from those individual studies for analysis and\nprediction. This research introduces the wetland quality factor, a critical but\nfrequently missed factor, into a meta-analysis and simultaneously considers\nother influential factors, such as study method, socio-economic state and other\nwetland site characteristics, as well. Thus, a more accurate and valid\nmeta-regression function is expected. Due to no obvious quality information in\nprimary studies, we extract two kinds of wetland states from the study context\nas relative but globally consistent quality measurement to use in the analysis,\nas the first step to explore the effect of wetland quality on values of\necosystem services.\n"
    },
    {
        "paper_id": 2210.01214,
        "authors": "Carsten Chong, Marc Hoffmann, Yanghui Liu, Mathieu Rosenbaum,\n  Gr\\'egoire Szymanski",
        "title": "Statistical inference for rough volatility: Minimax Theory",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Rough volatility models have gained considerable interest in the quantitative\nfinance community in recent years. In this paradigm, the volatility of the\nasset price is driven by a fractional Brownian motion with a small value for\nthe Hurst parameter $H$. In this work, we provide a rigorous statistical\nanalysis of these models. To do so, we establish minimax lower bounds for\nparameter estimation and design procedures based on wavelets attaining them. We\nnotably obtain an optimal speed of convergence of $n^{-1/(4H+2)}$ for\nestimating $H$ based on n sampled data, extending results known only for the\neasier case $H>1/2$ so far. We therefore establish that the parameters of rough\nvolatility models can be inferred with optimal accuracy in all regimes.\n"
    },
    {
        "paper_id": 2210.01216,
        "authors": "Carsten Chong and Marc Hoffmann and Yanghui Liu and Mathieu Rosenbaum\n  and Gr\\'egoire Szymanski",
        "title": "Statistical inference for rough volatility: Central limit theorems",
        "comments": null,
        "journal-ref": "The Annals of Applied Probability, Vol. 34, No. 3, 2600-2649, 2024",
        "doi": "10.1214/23-AAP2002",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In recent years, there has been a substantive interest in rough volatility\nmodels. In this class of models, the local behavior of stochastic volatility is\nmuch more irregular than semimartingales and resembles that of a fractional\nBrownian motion with Hurst parameter $H < 0.5$. In this paper, we derive a\nconsistent and asymptotically mixed normal estimator of $H$ based on\nhigh-frequency price observations. In contrast to previous works, we work in a\nsemiparametric setting and do not assume any a priori relationship between\nvolatility estimators and true volatility. Furthermore, our estimator attains a\nrate of convergence that is known to be optimal in a minimax sense in\nparametric rough volatility models.\n"
    },
    {
        "paper_id": 2210.01227,
        "authors": "Maxim Bichuch, Zachary Feinstein",
        "title": "Axioms for Automated Market Makers: A Mathematical Framework in FinTech\n  and Decentralized Finance",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Within this work we consider an axiomatic framework for Automated Market\nMakers (AMMs). By imposing reasonable axioms on the underlying utility\nfunction, we are able to characterize the properties of the swap size of the\nassets and of the resulting pricing oracle. In providing these general AMM\naxioms, we define a novel measure of price impacts that can be used to quantify\nthose costs between different constructions. We have analyzed many existing\nAMMs and shown that the vast majority of them satisfy our axioms. We have also\nconsidered the question of fees and divergence loss. In doing so, we have\nproposed a new fee structure so as to make the AMM indifferent to transaction\nsplitting. Finally, we have proposed a novel AMM that has nice analytical\nproperties and provides a large range over which there is no divergence loss.\n"
    },
    {
        "paper_id": 2210.01392,
        "authors": "Tomoya Mori, Jonathan Newton, and Shosei Sakaguchi",
        "title": "Collaborative knowledge exchange promotes innovation",
        "comments": "3 pages, 3 figures, and supporting information",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Considering collaborative patent development, we provide micro-level evidence\nfor innovation through exchanges of differentiated knowledge. Knowledge\nembodied in a patent is proxied by word pairs appearing in its abstract, while\nnovelty is measured by the frequency with which these word pairs have appeared\nin past patents. Inventors are assumed to possess the knowledge associated with\npatents in which they have previously participated. We find that collaboration\nby inventors with more mutually differentiated knowledge sets is likely to\nresult in patents with higher novelty.\n"
    },
    {
        "paper_id": 2210.01535,
        "authors": "Fabian Stephany, Ole Teutloff",
        "title": "What is the Price of a Skill? The Value of Complementarity",
        "comments": "58 pages, 12 figures, 3 tables",
        "journal-ref": null,
        "doi": "10.1016/j.respol.2023.104898",
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  The global workforce is urged to constantly reskill, as technological change\nfavours particular new skills while making others redundant. But which skills\nare a good investment for workers and firms? As skills are seldomly applied in\nisolation, we propose that complementarity strongly determines a skill's\neconomic value. For 962 skills, we demonstrate that their value is strongly\ndetermined by complementarity - that is, how many different skills, ideally of\nhigh value, a competency can be combined with. We show that the value of a\nskill is relative, as it depends on the skill background of the worker. For\nmost skills, their value is highest when used in combination with skills of a\ndifferent type. We put our model to the test with a set of skills related to\nArtificial Intelligence (AI). We find that AI skills are particularly valuable\n- increasing worker wages by 21% on average - because of their strong\ncomplementarities and their rising demand in recent years. The model and\nmetrics of our work can inform the policy and practice of digital re-skilling\nto reduce labour market mismatches. In cooperation with data and education\nproviders, researchers and policy makers should consider using this blueprint\nto provide learners with personalised skill recommendations that complement\ntheir existing capacities and fit their occupational background.\n"
    },
    {
        "paper_id": 2210.01573,
        "authors": "Bent Flyvbjerg, Alexander Budzier, Jong Seok Lee, Mark Keil, Daniel\n  Lunn, Dirk W. Bester",
        "title": "The Empirical Reality of IT Project Cost Overruns: Discovering A\n  Power-Law Distribution",
        "comments": null,
        "journal-ref": "Journal of Management Information Systems, vol. 39, no. 3, fall\n  issue, 2022",
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  If managers assume a normal or near-normal distribution of Information\nTechnology (IT) project cost overruns, as is common, and cost overruns can be\nshown to follow a power-law distribution, managers may be unwittingly exposing\ntheir organizations to extreme risk by severely underestimating the probability\nof large cost overruns. In this research, we collect and analyze a large sample\ncomprised of 5,392 IT projects to empirically examine the probability\ndistribution of IT project cost overruns. Further, we propose and examine a\nmechanism that can explain such a distribution. Our results reveal that IT\nprojects are far riskier in terms of cost than normally assumed by decision\nmakers and scholars. Specifically, we found that IT project cost overruns\nfollow a power-law distribution in which there are a large number of projects\nwith relatively small overruns and a fat tail that includes a smaller number of\nprojects with extreme overruns. A possible generative mechanism for the\nidentified power-law distribution is found in interdependencies among\ntechnological components in IT systems. We propose and demonstrate, through\ncomputer simulation, that a problem in a single technological component can\nlead to chain reactions in which other interdependent components are affected,\ncausing substantial overruns. What the power law tells us is that extreme IT\nproject cost overruns will occur and that the prevalence of these will be\ngrossly underestimated if managers assume that overruns follow a normal or\nnear-normal distribution. This underscores the importance of realistically\nassessing and mitigating the cost risk of new IT projects up front.\n"
    },
    {
        "paper_id": 2210.0161,
        "authors": "H. Dharma Kwon, Jan Palczewski",
        "title": "Exit game with private information",
        "comments": "42 pages; significantly revised presentation; strengthened uniqueness\n  result",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The timing of strategic exit is one of the most important but difficult\nbusiness decisions, especially under competition and uncertainty. Motivated by\nthis problem, we examine a stochastic game of exit in which players are\nuncertain about their competitor's exit value. We construct an equilibrium for\na large class of payoff flows driven by a general one-dimensional diffusion. In\nthe equilibrium, the players employ sophisticated exit strategies involving\nboth the state variable and the posterior belief process. These strategies are\nspecified explicitly in terms of the problem data and a solution to an\nauxiliary optimal stopping problem. The equilibrium we obtain is further shown\nto be unique within a wide subclass of symmetric Bayesian equilibria.\n"
    },
    {
        "paper_id": 2210.01726,
        "authors": "Francesca Biagini, Lukas Gonon, Andrea Mazzon and Thilo Meyer-Brandis",
        "title": "Detecting asset price bubbles using deep learning",
        "comments": "31 pages, 3 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper we employ deep learning techniques to detect financial asset\nbubbles by using observed call option prices. The proposed algorithm is widely\napplicable and model-independent. We test the accuracy of our methodology in\nnumerical experiments within a wide range of models and apply it to market data\nof tech stocks in order to assess if asset price bubbles are present. Under a\ngiven condition on the pricing of call options under asset price bubbles, we\nare able to provide a theoretical foundation of our approach for positive and\ncontinuous stochastic asset price processes. When such a condition is not\nsatisfied, we focus on local volatility models. To this purpose, we give a new\nnecessary and sufficient condition for a process with time-dependent local\nvolatility function to be a strict local martingale.\n"
    },
    {
        "paper_id": 2210.01774,
        "authors": "Hui Niu, Siyuan Li, Jian Li",
        "title": "MetaTrader: An Reinforcement Learning Approach Integrating Diverse\n  Policies for Portfolio Optimization",
        "comments": null,
        "journal-ref": null,
        "doi": "10.1145/3511808.3557363",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Portfolio management is a fundamental problem in finance. It involves\nperiodic reallocations of assets to maximize the expected returns within an\nappropriate level of risk exposure. Deep reinforcement learning (RL) has been\nconsidered a promising approach to solving this problem owing to its strong\ncapability in sequential decision making. However, due to the non-stationary\nnature of financial markets, applying RL techniques to portfolio optimization\nremains a challenging problem. Extracting trading knowledge from various expert\nstrategies could be helpful for agents to accommodate the changing markets. In\nthis paper, we propose MetaTrader, a novel two-stage RL-based approach for\nportfolio management, which learns to integrate diverse trading policies to\nadapt to various market conditions. In the first stage, MetaTrader incorporates\nan imitation learning objective into the reinforcement learning framework.\nThrough imitating different expert demonstrations, MetaTrader acquires a set of\ntrading policies with great diversity. In the second stage, MetaTrader learns a\nmeta-policy to recognize the market conditions and decide on the most proper\nlearned policy to follow. We evaluate the proposed approach on three real-world\nindex datasets and compare it to state-of-the-art baselines. The empirical\nresults demonstrate that MetaTrader significantly outperforms those baselines\nin balancing profits and risks. Furthermore, thorough ablation studies validate\nthe effectiveness of the components in the proposed approach.\n"
    },
    {
        "paper_id": 2210.01844,
        "authors": "Tiziano De Angelis, Jhanvi Garg, Quan Zhou",
        "title": "A quickest detection problem with false negatives",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We formulate and solve a quickest detection problem with false negatives. A\nstandard Brownian motion acquires a drift at an independent exponential random\ntime which is not directly observable. Based on the observation in continuous\ntime of the sample path of the process, an optimiser must detect the drift as\nquickly as possible after it has appeared. The optimiser can inspect the system\nmultiple times upon payment of a fixed cost per inspection. If a test is\nperformed on the system before the drift has appeared then, naturally, the test\nwill return a negative outcome. However, if a test is performed after the drift\nhas appeared, then the test may fail to detect it and return a false negative\nwith probability $\\epsilon\\in(0,1)$. The optimisation ends when the drift is\neventually detected. The problem is formulated mathematically as an optimal\nmultiple stopping problem and it is shown to be equivalent to a recursive\noptimal stopping problem. Exploiting such connection and free boundary methods\nwe find explicit formulae for the expected cost and the optimal strategy. We\nalso show that when $\\epsilon = 0$ our expected cost coincides with the one in\nShiryaev's classical optimal detection problem.\n"
    },
    {
        "paper_id": 2210.01846,
        "authors": "Moritz Laber, Peter Klimek, Martin Bruckner, Liuhuaying Yang, and\n  Stefan Thurner",
        "title": "Shock propagation from the Russia-Ukraine conflict on international\n  multilayer food production network determines global food availability",
        "comments": "36 pages, 8 figures. Nat Food (2023)",
        "journal-ref": null,
        "doi": "10.1038/s43016-023-00771-4",
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Dependencies in the global food production network can lead to shortages in\nnumerous regions, as demonstrated by the impacts of the Russia-Ukraine conflict\non global food supplies. Here, we reveal the losses of $125$ food products\nafter a localized shock to agricultural production in $192$ countries and\nterritories using a multilayer network model of trade (direct) and conversion\nof food products (indirect), thereby quantifying $10^8$ shock transmissions. We\nfind that a complete agricultural production loss in Ukraine has heterogeneous\nimpacts on other countries, causing relative losses of up to $89\\%$ in\nsunflower oil and $85\\%$ in maize via direct effects, and up to $25\\%$ in\npoultry meat via indirect impacts. Whilst previous studies often treated\nproducts in isolation and did not account for product conversion during\nproduction, our model studies the global propagation of local supply shocks\nalong both production and trade relations, allowing comparison of different\nresponse strategies.\n"
    },
    {
        "paper_id": 2210.01901,
        "authors": "Rama Cont, Alessandro Micheli and Eyal Neuman",
        "title": "Fast and Slow Optimal Trading with Exogenous Information",
        "comments": "66 pages, 6 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  We consider a stochastic game between a slow institutional investor and a\nhigh-frequency trader who are trading a risky asset and their aggregated\norder-flow impacts the asset price. We model this system by means of two\ncoupled stochastic control problems, in which the high-frequency trader\nexploits the available information on a price predicting signal more\nfrequently, but is also subject to periodic \"end of day\" inventory constraints.\nWe first derive the optimal strategy of the high-frequency trader given any\nadmissible strategy of the institutional investor. Then, we solve the problem\nof the institutional investor given the optimal signal-adaptive strategy of the\nhigh-frequency trader, in terms of the resolvent of a Fredholm integral\nequation, thus establishing the unique multi-period Stackelberg equilibrium of\nthe game. Our results provide an explicit solution to the game, which shows\nthat the high-frequency trader can adopt either predatory or cooperative\nstrategies in each period, depending on the tradeoff between the order-flow and\nthe trading signal. We also show that the institutional investor's strategy is\nconsiderably more profitable when the order-flow of the high-frequency trader\nis taken into account in her trading strategy.\n"
    },
    {
        "paper_id": 2210.01984,
        "authors": "Matthew Olckers, Toby Walsh",
        "title": "Manipulation and Peer Mechanisms: A Survey",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  In peer mechanisms, the competitors for a prize also determine who wins. Each\ncompetitor may be asked to rank, grade, or nominate peers for the prize. Since\nthe prize can be valuable, such as financial aid, course grades, or an award at\na conference, competitors may be tempted to manipulate the mechanism. We survey\napproaches to prevent or discourage the manipulation of peer mechanisms. We\nconclude our survey by identifying several important research challenges.\n"
    },
    {
        "paper_id": 2210.02126,
        "authors": "Ananda Chatterjee, Hrisav Bhowmick, and Jaydip Sen",
        "title": "Stock Volatility Prediction using Time Series and Deep Learning Approach",
        "comments": "This is the accepted version of the paper in the 2022 IEEE 2nd Mysore\n  Sub Section International Conference, MysuruCon22. The conference will be\n  organized in Mysuore, during October 16-17, 2022. The paper is 6 pages long,\n  and it contains 10 figures and 8 tables",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Volatility clustering is a crucial property that has a substantial impact on\nstock market patterns. Nonetheless, developing robust models for accurately\npredicting future stock price volatility is a difficult research topic. For\npredicting the volatility of three equities listed on India's national stock\nmarket (NSE), we propose multiple volatility models depending on the\ngeneralized autoregressive conditional heteroscedasticity (GARCH),\nGlosten-Jagannathan-GARCH (GJR-GARCH), Exponential general autoregressive\nconditional heteroskedastic (EGARCH), and LSTM framework. Sector-wise stocks\nhave been chosen in our study. The sectors which have been considered are\nbanking, information technology (IT), and pharma. yahoo finance has been used\nto obtain stock price data from Jan 2017 to Dec 2021. Among the pulled-out\nrecords, the data from Jan 2017 to Dec 2020 have been taken for training, and\ndata from 2021 have been chosen for testing our models. The performance of\npredicting the volatility of stocks of three sectors has been evaluated by\nimplementing three different types of GARCH models as well as by the LSTM model\nare compared. It has been observed the LSTM performed better in predicting\nvolatility in pharma over banking and IT sectors. In tandem, it was also\nobserved that E-GARCH performed better in the case of the banking sector and\nfor IT and pharma, GJR-GARCH performed better.\n"
    },
    {
        "paper_id": 2210.02175,
        "authors": "Joel P. Villarino, \\'Alvaro Leitao, Jos\\'e A. Garc\\'ia-Rodr\\'iguez",
        "title": "Boundary-safe PINNs extension: Application to non-linear parabolic PDEs\n  in counterparty credit risk",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The goal of this work is to develop deep learning numerical methods for\nsolving option XVA pricing problems given by non-linear PDE models. A novel\nstrategy for the treatment of the boundary conditions is proposed, which allows\nto get rid of the heuristic choice of the weights for the different addends\nthat appear in the loss function related to the training process. It is based\non defining the losses associated to the boundaries by means of the PDEs that\narise from substituting the related conditions into the model equation itself.\nFurther, automatic differentiation is employed to obtain accurate approximation\nof the partial derivatives.\n"
    },
    {
        "paper_id": 2210.02541,
        "authors": "Jherek Healy",
        "title": "Inserting or Stretching Points in Finite Difference Discretizations",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Partial differential equations sometimes have critical points where the\nsolution or some of its derivatives are discontinuous. The simplest example is\na discontinuity in the initial condition. It is well known that those decrease\nthe accuracy of finite difference methods. A common remedy is to stretch the\ngrid, such that many more grid points are present near the critical points, and\nfewer where the solution is deemed smooth. An alternative solution is to insert\npoints such that the discontinuities fall in the middle of two grid points.\nThis paper compares the accuracy of both approaches in the context of the\npricing of financial derivative contracts in the Black-Scholes model and\nproposes a new fast and simple stretching function.\n"
    },
    {
        "paper_id": 2210.02633,
        "authors": "Noe Rodriguez-Rodriguez and Octavio Miramontes",
        "title": "Shannon entropy: an econophysical approach to cryptocurrency portfolios",
        "comments": "14 pages, 5 figures, submitted to Entropy",
        "journal-ref": null,
        "doi": "10.3390/e24111583",
        "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
        "abstract": "  Cryptocurrency markets have attracted many interest for global investors\nbecause of their novelty, wide online availability, increasing capitalization\nand potential profits. In the econophysics tradition we show that many of the\nmost available cryptocurrencies have return statistics that do not follow\nGaussian distributions but heavy--tailed distributions instead. Entropy\nmeasures are also applied showing that portfolio diversification is a\nreasonable practice for decreasing return uncertainty.\n"
    },
    {
        "paper_id": 2210.02736,
        "authors": "Riccardo Gianluigi Serio, Maria Michela Dickson, Diego Giuliani,\n  Giuseppe Espa",
        "title": "Toward environmental sustainability: an empirical study on airports\n  efficiency",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The transition to more environmentally sustainable production processes and\nmanagerial practices is an increasingly important topic. Many industries need\nto undergo radical change to meet environmental sustainability requirements;\nthe tourism industry is no exception. In this respect, a particular aspect that\nneeds further attention is the relationship between airport performances and\ninvestments in environmental sustainability policies. This work represents a\nfirst attempt to provide empirical evidences about this relationship. Through\nthe application of a non-parametrical method, we first assess the efficiency of\nthe Italian airports industry. Secondly, we investigated the relationship\nbetween airports performance and management commitment toward the ecological\ntransition using a Tobit regression model. The results show that airports\nadherence to formal multi-year ecological transition programs has a positive\nand consistent impact on their performance.\n"
    },
    {
        "paper_id": 2210.02957,
        "authors": "W. Benedikt Schmal",
        "title": "From Rules to Regs: A Structural Topic Model of Collusion Research",
        "comments": null,
        "journal-ref": "Journal of Economic Surveys 2023, online first",
        "doi": "10.1111/joes.12600",
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  Collusive practices of firms continue to be a major threat to competition and\nconsumer welfare. Academic research on this topic aims at understanding the\neconomic drivers and behavioral patterns of cartels, among others, to guide\ncompetition authorities on how to tackle them. Utilizing topical machine\nlearning techniques in the domain of natural language processing enables me to\nanalyze the publications on this issue over more than 20 years in a novel way.\nComing from a stylized oligopoly-game theory focus, researchers recently turned\ntoward empirical case studies of bygone cartels. Uni- and multivariate time\nseries analyses reveal that the latter did not supersede the former but filled\na gap the decline in rule-based reasoning has left. Together with a tendency\ntowards monocultures in topics covered and an endogenous constriction of the\ntopic variety, the course of cartel research has changed notably: The variety\nof subjects included has grown, but the pluralism in economic questions\naddressed is in descent. It remains to be seen whether this will benefit or\nharm the cartel detection capabilities of authorities in the future.\n"
    },
    {
        "paper_id": 2210.0321,
        "authors": "Shouvanik Chakrabarti, Pierre Minssen, Romina Yalovetzky, Marco\n  Pistoia",
        "title": "Universal Quantum Speedup for Branch-and-Bound, Branch-and-Cut, and\n  Tree-Search Algorithms",
        "comments": "25 pages, 5 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Mixed Integer Programs (MIPs) model many optimization problems of interest in\nComputer Science, Operations Research, and Financial Engineering. Solving MIPs\nis NP-Hard in general, but several solvers have found success in obtaining\nnear-optimal solutions for problems of intermediate size. Branch-and-Cut\nalgorithms, which combine Branch-and-Bound logic with cutting-plane routines,\nare at the core of modern MIP solvers. Montanaro proposed a quantum algorithm\nwith a near-quadratic speedup compared to classical Branch-and-Bound algorithms\nin the worst case, when every optimal solution is desired. In practice,\nhowever, a near-optimal solution is satisfactory, and by leveraging tree-search\nheuristics to search only a portion of the solution tree, classical algorithms\ncan perform much better than the worst-case guarantee. In this paper, we\npropose a quantum algorithm, Incremental-Quantum-Branch-and-Bound, with\nuniversal near-quadratic speedup over classical Branch-and-Bound algorithms for\nevery input, i.e., if classical Branch-and-Bound has complexity $Q$ on an\ninstance that leads to solution depth $d$, Incremental-Quantum-Branch-and-Bound\noffers the same guarantees with a complexity of $\\tilde{O}(\\sqrt{Q}d)$. Our\nresults are valid for a wide variety of search heuristics, including\ndepth-based, cost-based, and $A^{\\ast}$ heuristics. Universal speedups are also\nobtained for Branch-and-Cut as well as heuristic tree search. Our algorithms\nare directly comparable to commercial MIP solvers, and guarantee near quadratic\nspeedup whenever $Q \\gg d$. We use numerical simulation to verify that $Q \\gg\nd$ for typical instances of the Sherrington-Kirkpatrick model, Maximum\nIndependent Set, and Portfolio Optimization; as well as to extrapolate the\ndependence of $Q$ on input size parameters. This allows us to project the\ntypical performance of our quantum algorithms for these important problems.\n"
    },
    {
        "paper_id": 2210.03469,
        "authors": "Naseh Majidi, Mahdi Shamsi, Farokh Marvasti",
        "title": "Algorithmic Trading Using Continuous Action Space Deep Reinforcement\n  Learning",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Price movement prediction has always been one of the traders' concerns in\nfinancial market trading. In order to increase their profit, they can analyze\nthe historical data and predict the price movement. The large size of the data\nand complex relations between them lead us to use algorithmic trading and\nartificial intelligence. This paper aims to offer an approach using\nTwin-Delayed DDPG (TD3) and the daily close price in order to achieve a trading\nstrategy in the stock and cryptocurrency markets. Unlike previous studies using\na discrete action space reinforcement learning algorithm, the TD3 is\ncontinuous, offering both position and the number of trading shares. Both the\nstock (Amazon) and cryptocurrency (Bitcoin) markets are addressed in this\nresearch to evaluate the performance of the proposed algorithm. The achieved\nstrategy using the TD3 is compared with some algorithms using technical\nanalysis, reinforcement learning, stochastic, and deterministic strategies\nthrough two standard metrics, Return and Sharpe ratio. The results indicate\nthat employing both position and the number of trading shares can improve the\nperformance of a trading system based on the mentioned metrics.\n"
    },
    {
        "paper_id": 2210.03494,
        "authors": "Benjamin Avanzi, Debbie Kusch Falden and Mogens Steffensen",
        "title": "Stable Dividends under Linear-Quadratic Optimization",
        "comments": null,
        "journal-ref": null,
        "doi": "10.1080/14697688.2023.2227661",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The optimization criterion for dividends from a risky business is most often\nformalized in terms of the expected present value of future dividends. That\ncriterion disregards a potential, explicit demand for stability of dividends.\nIn particular, within actuarial risk theory, maximization of future dividends\nhave been intensively studied as the so-called de Finetti problem. However,\nthere the optimal strategies typically become so-called barrier strategies.\nThese are far from stable and suboptimal affine dividend strategies have\ntherefore received attention recently. In contrast, in the class of\nlinear-quadratic problems a demand for stability if explicitly stressed. These\nhave most often been studied in diffusion models different from the actuarial\nrisk models. We bridge the gap between these patterns of thinking by deriving\noptimal affine dividend strategies under a linear-quadratic criterion for a\ngeneral L\\'evy process. We characterize the value function by the\nHamilton-Jacobi-Bellman equation, solve it, and compare the objective and the\noptimal controls to the classical objective of maximizing expected present\nvalue of future dividends. Thereby we provide a framework within which\nstability of dividends from a risky business, as e.g. in classical risk theory,\nis explicitly demanded and explicitly obtained.\n"
    },
    {
        "paper_id": 2210.03514,
        "authors": "Philipp Andreas Gunkel (1), Claire-Marie Bergaentzl\\'e (1), Dogan\n  Keles (1), Fabian Scheller (1,2) and Henrik Klinge Jacobsen (1) ((1) Energy\n  Economics and System Analysis, DTU Management, Technical University of\n  Denmark, 2800 Kongens Lyngby, Denmark, (2) Faculty of Business and\n  Engineering, University of Applied Sciences Wurzburg-Schweinfurt,\n  Ignaz-Schon-Street 11, 97421 Schweinfurt, Germany)",
        "title": "Grid tariff designs coping with the challenges of electrification and\n  their socio-economic impacts",
        "comments": "30 pages, 18 figures, journal article",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  This paper investigates volumetric grid tariff designs under consideration of\ndifferent pricing mechanisms and resulting cost allocation across\nsocio-techno-economic consumer categories. In a case study of 1.56 million\nDanish households divided into 90 socio-techno-economic categories, we compare\nthree alternative grid tariffs and investigate their impact on annual\nelectricity bills. The results of our design consisting of a time-dependent\nthreshold penalizing individual peak consumption and a system peak tariff show\n(a) a range of different allocations that distribute the burden of additional\ngrid costs across both technologies and (b) strong positive outcomes, including\nreduced expenses for lower-income groups and smaller households.\n"
    },
    {
        "paper_id": 2210.03572,
        "authors": "Quentin Gallea, Massimo Morelli, Dominic Rohner",
        "title": "Power in the Pipeline",
        "comments": "38 pages, 13 figures, submitted, presented at CEPR RPN workshop and\n  the Kiel CEPR geopolitics and economics workshop",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  This paper provides the first comprehensive empirical analysis of the role of\nnatural gas for the domestic and international distribution of power. The\ncrucial role of pipelines for the trade of natural gas determines a set of\nnetwork effects that are absent for other natural resources such as oil and\nminerals. Gas rents are not limited to producers but also accrue to key players\noccupying central nodes in the gas network. Drawing on our new gas pipeline\ndata, this paper shows that gas betweenness-centrality of a country increases\nsubstantially the ruler's grip on power as measured by leader turnover. A main\nmechanism at work is the reluctance of connected gas trade partners to impose\nsanctions, meaning that bad behavior of gas-central leaders is tolerated for\nlonger before being sanctioned. Overall, this reinforces the notion that fossil\nfuels are not just poison for the environment but also for political pluralism\nand healthy regime turnover.\n"
    },
    {
        "paper_id": 2210.03639,
        "authors": "Ankur Betageri",
        "title": "Quality of Life and the Experience of Context",
        "comments": "11 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  I propose that quality of life can be compared despite the difference in\nvalues across cultures when it is experienced at the sensory and perceptual\nlevel. I argue that an approach to assessing quality of life which focuses on\nan individual's ability to organize his or her context by perceiving positive\nconstellations of factors in the environment and his or her ability to achieve\nvaluable acts and realize valuable states of being is more meaningful than the\napproaches of metrics which focus directly, and often solely, on the means of\nliving and the means of freedom. Because the felt experience of quality of life\nis derived from a constellation of factors which make up the indivisible\nstructure of a milieu, the experience of quality of life cannot be regarded as\na subjective experience. Through the example of how different frequencies, and\nmixtures of frequencies, of light are perceived as colour by the eye, I\ndemonstrate that the human cognitive apparatus, because of its relation to the\nobject that is measured, apprehends different scales of quantity as degrees of\nquality. I show that lived experience is the result of a selective\nrelationality with one's environment and that the experience of quality has\nsomething to do with the perception of entities in their interrelated and\nnetworked nature as wholes.\n"
    },
    {
        "paper_id": 2210.03917,
        "authors": "Yan Dolinsky",
        "title": "Duality Theory for Exponential Utility--Based Hedging in the\n  Almgren--Chriss Model",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  In this paper, we obtain a duality result for the exponential utility\nmaximization problem where trading is subject to quadratic transaction costs\nand the investor is required to liquidate her position at the maturity date. As\nan application of the duality, we treat utility-based hedging in the Bachelier\nmodel. For European contingent claims with a quadratic payoff, we compute\nexplicitly the optimal trading strategy.\n"
    },
    {
        "paper_id": 2210.03943,
        "authors": "Jaydip Sen and Abhishek Dutta",
        "title": "Design and Analysis of Optimized Portfolios for Selected Sectors of the\n  Indian Stock Market",
        "comments": "This is the accepted version of the paper at the 2022 International\n  Conference on Decision Aid Sciences and Applications (DASA'22) which was\n  organized during March 23-25, 2022, Chiangrai, Thailand. The paper is 7 pages\n  long, and it contains 13 tables and 18 figures",
        "journal-ref": null,
        "doi": "10.1109/DASA54658.2022.9765289",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Portfolio optimization is a challenging problem that has attracted\nconsiderable attention and effort from researchers. The optimization of stock\nportfolios is a particularly hard problem since the stock prices are volatile\nand estimation of their future volatilities and values, in most cases, is very\ndifficult, if not impossible. This work uses three ratios, the Sharpe ratio,\nthe Sortino ratio, and the Calmar ratio, for designing the mean-variance\noptimized portfolios for six important sectors listed in the National Stock\nExchange (NSE) of India. Three portfolios are designed for each sector\nmaximizing the ratios based on the historical prices of the ten most important\nstocks of each sector from Jan 1, 2017, to Dec 31, 2020. The evaluation of the\nportfolios is done based on their cumulative returns over the test period from\nJan 1, 2021, to Dec 31, 2021. The ratio that yields the maximum cumulative\nreturns for both the training and the test periods for the majority of the\nsectors is identified. The sectors that exhibit the maximum cumulative returns\nfor the same ratio are also identified. The results provide useful insights for\ninvestors in the stock market in making their investment decisions based on the\ncurrent return and risks associated with the six sectors and their stocks.\n"
    },
    {
        "paper_id": 2210.03988,
        "authors": "Alejandro Rodriguez Dominguez, David Stynes",
        "title": "A Clustering Algorithm for Correlation Quickest Hub Discovery Mixing\n  Time Evolution and Random Matrix Theory",
        "comments": null,
        "journal-ref": "2022 IEEE 34th International Conference on Tools with Artificial\n  Intelligence (ICTAI), Macao, China, 2022",
        "doi": "10.1109/ICTAI56018.2022.00154",
        "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
        "abstract": "  We present a geometric version of Quickest Change Detection (QCD) and\nQuickest Hub Discovery (QHD) tests in correlation structures that allows us to\ninclude and combine new information with distance metrics. The topic falls\nwithin the scope of sequential, nonparametric, high-dimensional QCD and QHD,\nfrom which state-of-the-art settings developed global and local summary\nstatistics from asymptotic Random Matrix Theory (RMT) to detect changes in\nrandom matrix law. These settings work only for uncorrelated pre-change\nvariables. With our geometric version of the tests via clustering, we can test\nthe hypothesis that we can improve state-of-the-art settings for QHD, by\ncombining QCD and QHD simultaneously, as well as including information about\npre-change time-evolution in correlations. We can work with correlated\npre-change variables and test if the time-evolution of correlation improves\nperformance. We prove test consistency and design test hypothesis based on\nclustering performance. We apply this solution to financial time series\ncorrelations. Future developments on this topic are highly relevant in finance\nfor Risk Management, Portfolio Management, and Market Shocks Forecasting which\ncan save billions of dollars for the global economy. We introduce the\nDiversification Measure Distribution (DMD) for modeling the time-evolution of\ncorrelations as a function of individual variables which consists of a\nDirichlet-Multinomial distribution from a distance matrix of rolling\ncorrelations with a threshold. Finally, we are able to verify all these\nhypotheses.\n"
    },
    {
        "paper_id": 2210.04102,
        "authors": "Josef Taalbi",
        "title": "Innovation with and without patents",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  A long-standing discussion is to what extent patents can be used to monitor\ntrends in innovation activity. This study quantifies the amount and quality of\ninformation about actual innovation contained in the patent system, based on\n4,460 Swedish innovations (1970-2015) that have been matched to international\npatents. The results show that most innovations were not patented and that\namong those that were, 43.9% of all innovations, only a fraction can be\nidentified with patent quality data. The best-performing models identify 17% of\nall information about innovations, equivalent to an information loss of at\nleast 83%. Econometric tests also show that the fraction of innovations\nresponding to strengthened patent laws during the period were on average 8%\npercent. The overlap between the patent and innovation systems is hence more\nmodest than often assumed. This accentuates the need to, alongside patents,\ndevelop versatile approaches in order to induce and monitor various aspects of\ninnovation.\n"
    },
    {
        "paper_id": 2210.04167,
        "authors": "Rene Carmona and Claire Zeng",
        "title": "Optimal Execution with Identity Optionality",
        "comments": "22 pages, 10 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  This paper investigates the impact of anonymous trading on the agents'\nstrategy in an optimal execution framework. It mainly explores the specificity\nof order attribution on the Toronto Stock Exchange, where brokers can choose to\neither trade with their own identity or under a generic anonymous code that is\ncommon to all the brokers. We formulate a stochastic differential game for the\noptimal execution problem of a population of $N$ brokers and incorporate\npermanent and temporary price impacts for both the identity-revealed and\nanonymous trading processes. We then formulate the limiting mean-field game of\ncontrols with common noise and obtain a solution in closed-form via the\nprobabilistic approach for the Almgren-Chris price impact framework. Finally,\nwe perform a sensitivity analysis to explore the impact of the model parameters\non the optimal strategy.\n"
    },
    {
        "paper_id": 2210.04194,
        "authors": "Jiahua Xu and Yebo Feng",
        "title": "Reap the Harvest on Blockchain: A Survey of Yield Farming Protocols",
        "comments": null,
        "journal-ref": "IEEE Transactions on Network and Service Management, 2022",
        "doi": "10.1109/TNSM.2022.3222815",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Yield farming represents an immensely popular asset management activity in\ndecentralized finance (DeFi). It involves supplying, borrowing, or staking\ncrypto assets to earn an income in forms of transaction fees, interest, or\nparticipation rewards at different DeFi marketplaces. In this systematic\nsurvey, we present yield farming protocols as an aggregation-layer constituent\nof the wider DeFi ecosystem that interact with primitive-layer protocols such\nas decentralized exchanges (DEXs) and protocols for loanable funds (PLFs). We\nexamine the yield farming mechanism by first studying the operations encoded in\nthe yield farming smart contracts, and then performing stylized, parameterized\nsimulations on various yield farming strategies. We conduct a thorough\nliterature review on related work, and establish a framework for yield farming\nprotocols that takes into account pool structure, accepted token types, and\nimplemented strategies. Using our framework, we characterize major yield\naggregators in the market including Yearn Finance, Beefy, and Badger DAO.\nMoreover, we discuss anecdotal attacks against yield aggregators and generalize\na number of risks associated with yield farming.\n"
    },
    {
        "paper_id": 2210.04223,
        "authors": "Vladislav Gennadievich Malyshkin, Mikhail Gennadievich Belov",
        "title": "Market Directional Information Derived From (Time, Execution Price,\n  Shares Traded) Sequence of Transactions. On The Impact From The Future",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  An attempt to obtain market directional information from non-stationary\nsolution of the dynamic equation: \"future price tends to the value maximizing\nthe number of shares traded per unit time\" is presented. A remarkable feature\nof the approach is an automatic time scale selection. It is determined from the\nstate of maximal execution flow calculated on past transactions. Both lagging\nand advancing prices are calculated.\n"
    },
    {
        "paper_id": 2210.04339,
        "authors": "Giovanni Colavizza",
        "title": "Seller-buyer networks in NFT art are driven by preferential ties",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Non-Fungible Tokens (NFTs) have recently surged to mainstream attention by\nallowing the exchange of digital assets via blockchains. NFTs have also been\nadopted by artists to sell digital art. One of the promises of NFTs is\nbroadening participation to the arts market, a traditionally closed and opaque\nsystem, to sustain a wider and more diverse set of artists and collectors. A\nkey sign of this effect would be the disappearance or at least reduction in\nimportance of seller-buyer preferential ties, whereby the success of an artist\nis strongly dependent on the patronage of a single collector. We investigate\nNFT art seller-buyer networks considering several galleries and a large set of\nnearly 40,000 sales for over 230M USD in total volume. We find that NFT art is\na highly concentrated market driven by few successful sellers and even fewer\nsystematic buyers. High concentration is present in both the number of sales\nand, even more strongly, in their priced volume. Furthermore, we show that,\nwhile a broader-participation market was present in the early phase of NFT art\nadoption, preferential ties have dominated during market growth, peak and\nrecent decline. We consistently find that the top buyer accounts on average for\nover 80% of buys for a given seller. Similar trends apply to buyers and their\ntop seller. We conclude that NFT art constitutes, at the present, a highly\nconcentrated market driven by preferential seller-buyer ties.\n"
    },
    {
        "paper_id": 2210.04648,
        "authors": "Louisa Chen, Estelle Xue Liu and Zijun Liu",
        "title": "FX Resilience around the World: Fighting Volatile Cross-Border Capital\n  Flows",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  We show that capital flow (CF) volatility exerts an adverse effect on\nexchange rate (FX) volatility, regardless of whether capital controls have been\nput in place. However, this effect can be significantly moderated by certain\nmacroeconomic fundamentals that reflect trade openness, foreign assets\nholdings, monetary policy easing, fiscal sustainability, and financial\ndevelopment. Passing the threshold levels of these macroeconomic fundamentals,\nthe adverse effect of CF volatility may be negligible. We further construct an\nintuitive FX resilience measure, which provides an assessment of the strength\nof a country's exchange rates.\n"
    },
    {
        "paper_id": 2210.04797,
        "authors": "Fernando Moreno-Pino, Stefan Zohren",
        "title": "DeepVol: Volatility Forecasting from High-Frequency Data with Dilated\n  Causal Convolutions",
        "comments": "Updated version",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Volatility forecasts play a central role among equity risk measures. Besides\ntraditional statistical models, modern forecasting techniques based on machine\nlearning can be employed when treating volatility as a univariate, daily\ntime-series. Moreover, econometric studies have shown that increasing the\nnumber of daily observations with high-frequency intraday data helps to improve\nvolatility predictions. In this work, we propose DeepVol, a model based on\nDilated Causal Convolutions that uses high-frequency data to forecast day-ahead\nvolatility. Our empirical findings demonstrate that dilated convolutional\nfilters are highly effective at extracting relevant information from intraday\nfinancial time-series, proving that this architecture can effectively leverage\npredictive information present in high-frequency data that would otherwise be\nlost if realised measures were precomputed. Simultaneously, dilated\nconvolutional filters trained with intraday high-frequency data help us avoid\nthe limitations of models that use daily data, such as model misspecification\nor manually designed handcrafted features, whose devise involves optimising the\ntrade-off between accuracy and computational efficiency and makes models prone\nto lack of adaptation into changing circumstances. In our analysis, we use two\nyears of intraday data from NASDAQ-100 to evaluate the performance of DeepVol.\nOur empirical results suggest that the proposed deep learning-based approach\neffectively learns global features from high-frequency data, resulting in more\naccurate predictions compared to traditional methodologies and producing more\naccurate risk measures.\n"
    },
    {
        "paper_id": 2210.05136,
        "authors": "Aadi Gupta and Priya Gulati and Siddhartha P. Chakrabarty",
        "title": "Classification based credit risk analysis: The case of Lending Club",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  In this paper, we performs a credit risk analysis, on the data of past loan\napplicants of a company named Lending Club. The calculation required the use of\nexploratory data analysis and machine learning classification algorithms,\nnamely, Logistic Regression and Random Forest Algorithm. We further used the\ncalculated probability of default to design a credit derivative based on the\nidea of a Credit Default Swap, to hedge against an event of default. The\nresults on the test set are presented using various performance measures.\n"
    },
    {
        "paper_id": 2210.05447,
        "authors": "Mohit Garg and Suneel Sarswat",
        "title": "The Design and Regulation of Exchanges: A Formal Approach",
        "comments": "21 pages, FSTTCS 2022 (to appear)",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We use formal methods to specify, design, and monitor continuous double\nauctions, which are widely used to match buyers and sellers at exchanges of\nforeign currencies, stocks, and commodities. We identify three natural\nproperties of such auctions and formally prove that these properties completely\ndetermine the input-output relationship. We then formally verify that a natural\nalgorithm satisfies these properties. All definitions, theorems, and proofs are\nformalized in an interactive theorem prover. We extract a verified program of\nour algorithm to build an automated checker that is guaranteed to detect errors\nin the trade logs of exchanges if they generate transactions that violate any\nof the natural properties.\n"
    },
    {
        "paper_id": 2210.06139,
        "authors": "David Cerezo S\\'anchez",
        "title": "Zero-Knowledge Optimal Monetary Policy under Stochastic Dominance",
        "comments": "Implementation available at:\n  https://github.com/Calctopia-OpenSource/cothority/tree/zkmonpolicy",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Optimal simple rules for the monetary policy of the first stochastically\ndominant crypto-currency are derived in a Dynamic Stochastic General\nEquilibrium (DSGE) model, in order to provide optimal responses to changes in\ninflation, output, and other sources of uncertainty. The optimal monetary\npolicy stochastically dominates all the previous crypto-currencies, thus the\nefficient portfolio is to go long on the stochastically dominant\ncrypto-currency: a strategy-proof arbitrage featuring a higher Omega ratio with\nhigher expected returns, inducing an investment-efficient Nash equilibrium over\nthe crypto-market. Zero-knowledge proofs of the monetary policy are committed\non the blockchain: an implementation is provided.\n"
    },
    {
        "paper_id": 2210.06148,
        "authors": "Weihuan Huang, Nifei Lin, and L. Jeff Hong",
        "title": "Monte-Carlo Estimation of CoVaR",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  ${\\rm CoVaR}$ is one of the most important measures of financial systemic\nrisks. It is defined as the risk of a financial portfolio conditional on\nanother financial portfolio being at risk. In this paper we first develop a\nMonte-Carlo simulation-based batching estimator of CoVaR and study its\nconsistency and asymptotic normality. We show that the optimal rate of\nconvergence of the batching estimator is $n^{-1/3}$, where $n$ is the sample\nsize. We then develop an importance-sampling inspired estimator under the\ndelta-gamma approximations to the portfolio losses, and we show that the rate\nof convergence of the estimator is $n^{-1/2}$. Numerical experiments support\nour theoretical findings and show that both estimators work well.\n"
    },
    {
        "paper_id": 2210.06172,
        "authors": "Michael Adam",
        "title": "Potential Applications of Quantum Computing for the Insurance Industry",
        "comments": "43 pages, 15 figures, 2 tables",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  This paper is the documentation of a pre-study performed by AXA Konzern AG in\ncollaboration with Fraunhofer ITWM to assess the relevance of quantum computing\nfor the insurance industry. Beside a general overview of the status quo of\nquantum computing technologies, we investigate its applicability for the\nvaluation of insurance contracts as a concrete use case. This valuation is a\ncomputationally intensive problem because the lack of closed pricing formulas\nrequires the use of Monte Carlo methods. Therefore current technical\ncapabilities force insurers to apply approximation methods for many subsequent\ntasks like economic capital calculation or optimization of strategic asset\nallocations. The business-criticality of these tasks combined with the\nexistence of a quantum algorithm called Amplitude Estimation which promises a\nquadratic speed-up of Monte Carlo simulation makes this use case obvious. We\nprovide a detailed explanation of Amplitude Estimation and present two quantum\ncircuits which describe insurance-related payoff features in a quantum circuit\nmodel. An exemplary circuit that encodes dynamic lapse is evaluated both on a\nsimulator and on real quantum hardware.\n"
    },
    {
        "paper_id": 2210.06255,
        "authors": "S. Kirusheva, H. Huang, T.S. Salisbury",
        "title": "Retirement spending problem under Habit Formation Model",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  In this paper we consider the problem of optimizing lifetime consumption\nunder a habit formation model. Our work differs from previous results, because\nwe incorporate mortality and pension income. Lifetime utility of consumption\nmakes the problem time inhomogeneous, because of the effect of ageing.\nConsidering habit formation means increasing the dimension of the stochastic\ncontrol problem, because one must track smoothed-consumption using an\nadditional variable, habit $\\bar c$. Including exogenous pension income $\\pi$\nmeans that we cannot rely on a kind of scaling transformation to reduce the\ndimension of the problem as in earlier work, therefore we solve it numerically,\nusing a finite difference scheme. We also explore how consumption changes over\ntime based on habit if the retiree follows the optimal strategy. Finally, we\nanswer the question of whether it is reasonable to annuitize wealth at the time\nof retirement or not by varying parameters, such as asset allocation $\\theta$\nand the smoothing factor $\\eta$.\n"
    },
    {
        "paper_id": 2210.06611,
        "authors": "A. Arda Gitmez, Rom\\'an Andr\\'es Z\\'arate",
        "title": "Proximity, Similarity, and Friendship Formation: Theory and Evidence",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Can proximity make friendships more diverse? To address this question, we\npropose a learning-driven friendship formation model to study how proximity and\nsimilarity influence the likelihood of forming social connections. The model\npredicts that proximity affects more friendships between dissimilar than\nsimilar individuals, in opposition to a preference-driven version of the model.\nWe use an experiment at selective boarding schools in Peru that generates\nrandom variation in the physical proximity between students to test these\npredictions. The empirical evidence is consistent with the learning model:\nwhile social networks exhibit homophily by academic achievement and poverty,\nproximity generates more diverse social connections.\n"
    },
    {
        "paper_id": 2210.07129,
        "authors": "E. Ruben van Beesten, Daan Hulshof",
        "title": "Economic incentives for capacity reductions on interconnectors in the\n  day-ahead market",
        "comments": "20 pages, 16 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We consider a zonal international power market and investigate potential\neconomic incentives for short-term reductions of transmission capacities on\nexisting interconnectors by the responsible transmission system operators\n(TSOs). We show that if a TSO aims to maximize domestic total welfare, it often\nhas an incentive to reduce the capacity on the interconnectors to neighboring\ncountries.\n  In contrast with the (limited) literature on this subject, which focuses on\nincentives through the avoidance of future balancing costs, we show that\nincentives can exist even if one ignores balancing and focuses solely on\nwelfare gains in the day-ahead market itself. Our analysis consists of two\nparts. In the first part, we develop an analytical framework that explains why\nthese incentives exist. In particular, we distinguish two mechanisms: one based\non price differences with neighboring countries and one based on the domestic\nelectricity price. In the second part, we perform numerical experiments using a\nmodel of the Northern-European power system, focusing on the Danish TSO. In 97%\nof the historical hours tested, we indeed observe economic incentives for\ncapacity reductions, leading to significant welfare gains for Denmark and\nwelfare losses for the system as a whole. We show that the potential for\nwelfare gains greatly depends on the ability of the TSO to adapt interconnector\ncapacities to short-term market conditions. Finally, we explore the extent to\nwhich the recently introduced European \"70%-rule\" can mitigate the incentives\nfor capacity reductions and their welfare effects.\n"
    },
    {
        "paper_id": 2210.07184,
        "authors": "Nelson Vadori, Leo Ardon, Sumitra Ganesh, Thomas Spooner, Selim\n  Amrouni, Jared Vann, Mengda Xu, Zeyu Zheng, Tucker Balch, Manuela Veloso",
        "title": "Towards Multi-Agent Reinforcement Learning driven Over-The-Counter\n  Market Simulations",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We study a game between liquidity provider and liquidity taker agents\ninteracting in an over-the-counter market, for which the typical example is\nforeign exchange. We show how a suitable design of parameterized families of\nreward functions coupled with shared policy learning constitutes an efficient\nsolution to this problem. By playing against each other, our\ndeep-reinforcement-learning-driven agents learn emergent behaviors relative to\na wide spectrum of objectives encompassing profit-and-loss, optimal execution\nand market share. In particular, we find that liquidity providers naturally\nlearn to balance hedging and skewing, where skewing refers to setting their buy\nand sell prices asymmetrically as a function of their inventory. We further\nintroduce a novel RL-based calibration algorithm which we found performed well\nat imposing constraints on the game equilibrium. On the theoretical side, we\nare able to show convergence rates for our multi-agent policy gradient\nalgorithm under a transitivity assumption, closely related to generalized\nordinal potential games.\n"
    },
    {
        "paper_id": 2210.07322,
        "authors": "Anuradha M. Annaswamy, Vineet Jagadeesan Nair",
        "title": "Human Behavioral Models Using Utility Theory and Prospect Theory",
        "comments": "26 pages, submitted chapter to upcoming Wiley book on Cyber-Physical\n  Human Systems (CPHS). arXiv admin note: text overlap with arXiv:1904.04824",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Several examples of Cyber-physical human systems (CPHS) include real-time\ndecisions from humans as a necessary building block for the successful\nperformance of the overall system. Many of these decision-making problems\nnecessitate an appropriate model of human behavior. Tools from Utility Theory\nhave been used successfully in several problems in transportation for resource\nallocation and balance of supply and demand \\citep{ben1985discrete}. More\nrecently, Prospect Theory has been demonstrated as a useful tool in behavioral\neconomics and cognitive psychology for deriving human behavioral models that\ncharacterize their subjective decision-making in the presence of stochastic\nuncertainties and risks, as an alternative to conventional Utility Theory\n\\citep{kahneman_prospect_2012}. These models will be described in this article.\nTheoretical implications of Prospect Theory are also discussed. Examples will\nbe drawn from transportation use cases such as shared mobility to illustrate\nthese models as well as the distinctions between Utility Theory and Prospect\nTheory.\n"
    },
    {
        "paper_id": 2210.07393,
        "authors": "Jason B. Cho, Sven Serneels, David S. Matteson",
        "title": "Non-fungible token transactions: data and challenges",
        "comments": null,
        "journal-ref": "Data Science in Science 2:1 (2023)",
        "doi": "10.1080/26941899.2022.2151950",
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Non-fungible tokens (NFT) have recently emerged as a novel blockchain hosted\nfinancial asset class that has attracted major transaction volumes. Investment\ndecisions rely on data and adequate preprocessing and application of analytics\nto them. Both owing to the non-fungible nature of the tokens and to a\nblockchain being the primary data source, NFT transaction data pose several\nchallenges not commonly encountered in traditional financial data. Using data\nthat consist of the transaction history of eight highly valued NFT collections,\na selection of such challenges is illustrated. These are: price differentiation\nby token traits, the possible existence of lateral swaps and wash trades in the\ntransaction history and finally, severe volatility. While this paper merely\nscratches the surface of how data analytics can be applied in this context, the\ndata and challenges laid out here may present opportunities for future research\non the topic.\n"
    },
    {
        "paper_id": 2210.07824,
        "authors": "Anita Mezzetti, Lo\\\"ic Mar\\'echal, Dimitri Percia David, William\n  Lacube, S\\'ebastien Gillard, Michael Tsesmelis, Thomas Maillart, Alain\n  Mermoud",
        "title": "TechRank",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  We introduce TechRank, a recursive algorithm based on a bi-partite graph with\nweighted nodes. We develop TechRank to link companies and technologies based on\nthe method of reflection. We allow the algorithm to incorporate exogenous\nvariables that reflect an investor's preferences. We calibrate the algorithm in\nthe cybersecurity sector. First, our results help estimate each entity's\ninfluence and explain companies' and technologies' ranking. Second, they\nprovide investors with a quantitative optimal ranking of technologies and thus,\nhelp them design their optimal portfolio. We propose this method as an\nalternative to traditional portfolio management and, in the case of private\nequity investments, as a new way to price assets for which cash flows are not\nobservable.\n"
    },
    {
        "paper_id": 2210.07955,
        "authors": "Farhana Rahman",
        "title": "Impact of WACC on Firm Profitability: Evidence from Food and Allied\n  Industry of Bangladesh",
        "comments": null,
        "journal-ref": "European Journal of Business and Management Research, Volume 7,\n  Issue 6, November 2022",
        "doi": "10.24018/ejbmr.2022.7.6.1707",
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  The research paper aims to analyze the underlying relationship in between the\nprofitability and cost of funds of a firm. A total of twelve companies were\nselected as a sample for this study which are listed in Dhaka Stock Exchange\nunder Food and Allied Industry. A panel data set of 15 years from 2005 to 2019\nwas used to conduct the necessary analysis. In this paper, Return on Asset\n(ROA) is used as the accounting criteria of profitability. WACC is the\nindependent variable while Firm Size, Firm Age and Firm Leverage are used as\ncontrol variable for the study. Fixed Effects Panel Regression Model is used to\nanalyze the dataset. The result of the analysis shows that WACC is negatively\nrelated with the profitability measure and this relationship is significant.\nThe study has potential to be replicated by other industries like textile,\ncement, pharmaceutical & chemical, fuel & power, tannery etc.\n"
    },
    {
        "paper_id": 2210.08197,
        "authors": "Kiana Asgari, Aida Afshar Mohammadian, Mojtaba Tefagh",
        "title": "DyFEn: Agent-Based Fee Setting in Payment Channel Networks",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  In recent years, with the development of easy to use learning environments,\nimplementing and reproducible benchmarking of reinforcement learning algorithms\nhas been largely accelerated by utilizing these frameworks. In this article, we\nintroduce the Dynamic Fee learning Environment (DyFEn), an open-source\nreal-world financial network model. It can provide a testbed for evaluating\ndifferent reinforcement learning techniques. To illustrate the promise of\nDyFEn, we present a challenging problem which is a simultaneous multi-channel\ndynamic fee setting for off-chain payment channels. This problem is well-known\nin the Bitcoin Lightning Network and has no effective solutions. Specifically,\nwe report the empirical results of several commonly used deep reinforcement\nlearning methods on this dynamic fee setting task as a baseline for further\nexperiments. To the best of our knowledge, this work proposes the first virtual\nlearning environment based on a simulation of blockchain and distributed ledger\ntechnologies, unlike many others which are based on physics simulations or game\nplatforms.\n"
    },
    {
        "paper_id": 2210.08422,
        "authors": "Kexin Chen and Hoi Ying Wong",
        "title": "Duality in optimal consumption--investment problems with alternative\n  data",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This study investigates an optimal consumption--investment problem in which\nthe unobserved stock trend is modulated by a hidden Markov chain that\nrepresents different economic regimes. In the classical approach, the hidden\nstate is estimated from historical asset prices, but recent advancements in\ntechnology enable investors to consider alternative data in their\ndecision-making. These include social media commentary, expert opinions,\nCOVID-19 pandemic data, and GPS data, which originate outside of the standard\nsources of market data but are considered useful for predicting stock trends.\nWe develop a novel duality theory for this problem and consider a\njump-diffusion process for the alternative data series. This theory helps\ninvestors in identifying ``useful'' alternative data for dynamic\ndecision-making by offering conditions to the filter equation that permit the\nuse of a control approach based on the dynamic programming principle. We\ndemonstrate an application for proving a unique smooth solution for a constant\nrelative risk-averse agent once the distributions of the signals generated from\nalternative data satisfy a bounded likelihood ratio condition. In doing so, we\nobtain an explicit consumption--investment strategy that takes advantage of\ndifferent types of alternative data that have not been addressed in the\nliterature.\n"
    },
    {
        "paper_id": 2210.08569,
        "authors": "Penghang Liu, Kshama Dwarakanath, Svitlana S Vyetrenko, Tucker Balch",
        "title": "Limited or Biased: Modeling Sub-Rational Human Investors in Financial\n  Markets",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Human decision-making in real-life deviates significantly from the optimal\ndecisions made by fully rational agents, primarily due to computational\nlimitations or psychological biases. While existing studies in behavioral\nfinance have discovered various aspects of human sub-rationality, there lacks a\ncomprehensive framework to transfer these findings into an adaptive human model\napplicable across diverse financial market scenarios. In this study, we\nintroduce a flexible model that incorporates five different aspects of human\nsub-rationality using reinforcement learning. Our model is trained using a\nhigh-fidelity multi-agent market simulator, which overcomes limitations\nassociated with the scarcity of labeled data of individual investors. We\nevaluate the behavior of sub-rational human investors using hand-crafted market\nscenarios and SHAP value analysis, showing that our model accurately reproduces\nthe observations in the previous studies and reveals insights of the driving\nfactors of human behavior. Finally, we explore the impact of sub-rationality on\nthe investor's Profit and Loss (PnL) and market quality. Our experiments reveal\nthat bounded-rational and prospect-biased human behaviors improve liquidity but\ndiminish price efficiency, whereas human behavior influenced by myopia,\noptimism, and pessimism reduces market liquidity.\n"
    },
    {
        "paper_id": 2210.08785,
        "authors": "Wahab Ibrahim and Ola Hall",
        "title": "Welfare estimations from imagery. A test of domain experts ability to\n  rate poverty from visual inspection of satellite imagery",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
        "abstract": "  The present study uses domain experts to estimate welfare levels and\nindicators from high-resolution satellite imagery. We use the wealth quintiles\nfrom the 2015 Tanzania DHS dataset as ground truth data. We analyse the\nperformance of the visual estimation of relative wealth at the cluster level\nand compare these with wealth rankings from the DHS survey of 2015 for that\ncountry using correlations, ordinal regressions and multinomial logistic\nregressions. Of the 608 clusters, 115 received the same ratings from human\nexperts and the independent DHS rankings. For 59 percent of the clusters,\nexperts ratings were slightly lower. On the one hand, significant positive\npredictors of wealth are the presence of modern roofs and wider roads. For\ninstance, the log odds of receiving a rating in a higher quintile on the wealth\nrankings is 0.917 points higher on average for clusters with buildings with\nslate or tile roofing compared to those without. On the other hand, significant\nnegative predictors included poor road coverage, low to medium greenery\ncoverage, and low to medium building density. Other key predictors from the\nmultinomial regression model include settlement structure and farm sizes. These\nfindings are significant to the extent that these correlates of wealth and\npoverty are visually readable from satellite imagery and can be used to train\nmachine learning models in poverty predictions. Using these features for\ntraining will contribute to more transparent ML models and, consequently,\nexplainable AI.\n"
    },
    {
        "paper_id": 2210.08982,
        "authors": "Alvarez-Telena Sergio and Diez-Fernandez Marta",
        "title": "Title Redacted",
        "comments": "This article has been removed by arXiv administrators because the\n  submitter did not have the authority to grant the license at the time of\n  submission",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Abstract redacted by arXiv administrators.\n"
    },
    {
        "paper_id": 2210.09066,
        "authors": "Felix K\\\"ubler",
        "title": "Climate uncertainty, financial frictions and constrained efficient\n  carbon taxation",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  In this paper, I consider a simple heterogeneous agents model of a production\neconomy with uncertain climate change and examine constrained efficient carbon\ntaxation. If there are frictionless, complete financial markets, the simple\nmodel predicts a unique Pareto-optimal level of carbon taxes and abatement. In\nthe presence of financial frictions, however, the optimal level of abatement\ncannot be defined without taking a stand on how abatement costs are distributed\namong individuals. I propose a simple linear cost-sharing scheme that has\nseveral desirable normative properties. I use calibrated examples of economies\nwith incomplete financial markets and/or limited market participation to\ndemonstrate that different schemes to share abatement costs can have large\neffects on optimal abatement levels and that the presence of financial\nfrictions can increase optimal abatement by a factor of three relative to the\ncase of frictionless financial market.\n"
    },
    {
        "paper_id": 2210.09094,
        "authors": "Frauke von Bieberstein, Anna-Corinna Kulle, Stefanie Schumacher",
        "title": "Large gender and age differences in hand disinfection behavior during\n  the COVID-19 pandemic: Field data from Swiss retail stores",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
        "abstract": "  Hand hygiene is one of the key low-cost measures proposed by the World Health\nOrganization (WHO) to contain the spread of COVID-19. In a field study\nconducted during the pandemic in June and July 2020 in Switzerland, we captured\nthe hand disinfection behavior of customers in five stores of a large retail\nchain (n = 8,245). The study reveals considerable differences with respect to\ngender and age: Women were 8.3 percentage points more likely to disinfect their\nhands compared to men. With respect to age, we identified a steep increase\nacross age groups, with people age 60 years and older disinfecting their hands\nsignificantly more often than younger adults (>+16.7 percentage points) and\nyouth (>+ 31.7 percentage points). A validation study conducted in December\n2020 (n = 1,918) confirmed the gender and age differences at a later point in\nthe pandemic. In sum, the differences between gender and age groups are\nsubstantial and should be considered in the design of protective measures to\nensure clean and safe hands.\n"
    },
    {
        "paper_id": 2210.09097,
        "authors": "Norbert Ankri and Pa\\\"ikan Marcaggi",
        "title": "From Marx's fundamental equalities to the solving of the transformation\n  problem -- Coherence of the model",
        "comments": "89 pages, 12 figures, 23 tables",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/publicdomain/zero/1.0/",
        "abstract": "  Recently, V. Laure van Bambeke used an original approach to solve the famous\nproblem of transformation of values into production prices by considering that\ncapital reallocation to each department (branch) was part of the problem. Here,\nwe confirm the validity of this consideration in relation with the satisfaction\nof demand (social need which is able to pay for the given product). In contrast\nto V. Laure van Bambeke's method of solving an overdetermined system of\nequations (implying that compliance with Marx's fundamental equalities could\nonly be approached), we show that the transformation problem is solvable from a\ndetermined (two-branch models) or an underdetermined system of equations\nenabling to obtain exact solutions through an algorithm we provide, with no\napproximation needed. For systems with three branches or more, the solution of\nthe transformation problem belongs to an infinite ensemble, accounting for the\nobserved high competition-driven market fluidity. Furthermore, we show that the\ntransformation problem is solvable in the absence of fixed capital, supporting\nthat dealing with the latter is not essential and cannot be seen as a potential\nflaw of the approach. Our algorithm enables simulations illustrating how the\ntransient rise in the rate of profit predicted by the Okishio theorem is\nconsistent with the tendency of the rate of profit to fall (TRPF) subsequent to\ncapital reallocation, and how the TRPF is governed by the increase of organic\ncomposition, in value. We establish that the long-standing transformation\nproblem is not such a problem since it is easily solved through our algorithm,\nwhatever the number of branches considered. This emphasizes the high coherence\nof Marx's conception, and its impressive relevance regarding issues such as the\nTRPF, which have remained intensely debated.\n"
    },
    {
        "paper_id": 2210.09133,
        "authors": "Max Nendel",
        "title": "Lower semicontinuity of monotone functionals in the mixed topology on\n  $C_b$",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The main result of this paper characterizes the continuity from below of\nmonotone functionals on the space $C_b$ of bounded continuous functions on an\narbitrary Polish space as lower semicontinuity in the mixed topology. In this\nparticular situation, the mixed topology coincides with the Mackey topology for\nthe dual pair $(C_b,{\\rm ca})$, where ${\\rm ca}$ denotes the space of all\ncountably additive signed Borel measures of finite variation. Hence, lower\nsemicontinuity in the mixed topology of convex monotone maps $C_b\\to \\mathbb R$\nis equivalent to a dual representation in terms of countably additive measures.\nSuch representations are of fundamental importance in finance, e.g., in the\ncontext of risk measures and super hedging problems. Based on the main result,\nregularity properties of capacities and dual representations of Choquet\nintegrals in terms of countably additive measures for $2$-alternating\ncapacities are studied. In a second step, the paper provides a characterization\nof equicontinuity in the mixed topology for families of convex monotone maps.\nAs a consequence, for every convex monotone map on $C_b$ taking values in a\nlocally convex vector lattice, continuity in the mixed topology is equivalent\nto continuity on norm bounded sets.\n"
    },
    {
        "paper_id": 2210.09145,
        "authors": "Niklas V. Lehmann",
        "title": "Exploring the stability of solar geoengineering agreements",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  A simple model is introduced to study the cooperative behavior of nations\nregarding solar geoengineering. The results of this model are explored through\nnumerical methods. A general finding is that cooperation and coordination\nbetween nations on solar geoengineering is very much incentivized. Furthermore,\nthe stability of solar geoengineering agreements between nations crucially\ndepends on the perceived riskiness of solar geoengineering. If solar\ngeoengineering is perceived as riskier, the stability of the most stable solar\ngeoengineering agreements is reduced. However, the stability of agreements is\ncompletely independent of countries preferences.\n"
    },
    {
        "paper_id": 2210.09302,
        "authors": "Maxime Markov, Vladimir Markov",
        "title": "The impact of big winners on passive and active equity investment\n  strategies",
        "comments": "15 pages, 3 figures, 4 tables",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  We investigate the impact of big winner stocks on the performance of active\nand passive investment strategies using a combination of numerical and\nanalytical techniques. Our analysis is based on historical stock price data\nfrom 2006 to 2021 for a large variety of global indexes. We show that the\nlog-normal distribution provides a reasonable fit for total returns for the\nmajority of world stock indexes but highlight the limitations of this model.\nUsing an analytical expression for a finite sum of log-normal random variables,\nwe show that the typical return of a concentrated portfolio is less than that\nof an equally weighted index. This finding indicates that active managers face\na significant risk of underperforming due to the potential for missing out on\nthe substantial returns generated by big winner stocks. Our results suggest\nthat passive investing strategies, that do not involve the selection of\nindividual stocks, are likely to be more effective in achieving long-term\nfinancial goals.\n"
    },
    {
        "paper_id": 2210.09331,
        "authors": "Christa Cuchiero, Luca Di Persio, Francesco Guida, Sara Svaluto-Ferro",
        "title": "Measure-valued processes for energy markets",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  We introduce a framework that allows to employ (non-negative) measure-valued\nprocesses for energy market modeling, in particular for electricity and gas\nfutures. Interpreting the process' spatial structure as time to maturity, we\nshow how the Heath-Jarrow-Morton approach can be translated to this framework,\nthus guaranteeing arbitrage free modeling in infinite dimensions. We derive an\nanalog to the HJM-drift condition and then treat in a Markovian setting\nexistence of non-negative measure-valued diffusions that satisfy this\ncondition. To analyze mathematically convenient classes we build on Cuchiero et\nal. (2021) and consider measure-valued polynomial and affine diffusions, where\nwe can precisely specify the diffusion part in terms of continuous functions\nsatisfying certain admissibility conditions. For calibration purposes these\nfunctions can then be parameterized by neural networks yielding measure-valued\nanalogs of neural SPDEs. By combining Fourier approaches or the moment formula\nwith stochastic gradient descent methods, this then allows for tractable\ncalibration procedures which we also test by way of example on market data. We\nalso sketch how measure-valued processes can be applied in the context of\nrenewable energy production modeling.\n"
    },
    {
        "paper_id": 2210.09619,
        "authors": "Suchetana Sadhukhan and Poulomi Sadhukhan",
        "title": "Sector-wise analysis of Indian stock market: Long and short-term risk\n  and stability analysis",
        "comments": "15 pages, 3 figures, 2 tables",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper, for the first time, focuses on the sector-wise analysis of a\nstock market through multifractal analysis. We have considered Bombay Stock\nExchange, India, and identified two time scales, short ($<200$ days) and long\ntime-scale ($>200$ days) for investment. We infer that long-term investment\nwill be more profitable. For long time scale, sectors can be separated into two\ncategories based on the Hurst exponent values; one corresponds to stable\nsectors with small fluctuations, and the other with dominance of large\nfluctuations leading to possible downturns in those sectors.\n"
    },
    {
        "paper_id": 2210.0969,
        "authors": "Claire-Marie Bergaentzl\\'e (1), Philipp Andreas Gunkel (1), Mohammad\n  Ansarin (2), Yashar Ghiassi-Farrokhfal (2), Henrik Klinge Jacobsen (1) ((1)\n  DTU Management, Technical University of Denmark, 2800 Kgs Lyngby, Denmark,\n  (2) Rotterdam School of Management, Erasmus University, 3062PA Rotterdam,\n  Netherlands)",
        "title": "Electricity grid tariffs for electrification in households: Bridging the\n  gap between cross-subsidies and fairness",
        "comments": "40 pages, 8 figures, journal article",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  Developing new electricity grid tariffs in the context of household\nelectrification raises old questions about who pays for what and to what\nextent. When electric vehicles (EVs) and heat pumps (HPs) are owned primarily\nby households with higher financial status than others, new tariff designs may\nclash with the economic argument for efficiency and the political arguments for\nfairness. This article combines tariff design and redistributive mechanisms to\nstrike a balance between time-differentiated signals, revenue stability for the\nutility, limited grid costs for vulnerable households, and promoting\nelectrification. We simulate the impacts of this combination on 1.4 million\nDanish households (about 50% of the country's population) and quantify the\ncross-subsidization effects between groups. With its unique level of detail,\nthis study stresses the spillover effects of tariffs. We show that a\nsubscription-heavy tariff associated with a ToU rate and a low redistribution\nfactor tackles all the above goals.\n"
    },
    {
        "paper_id": 2210.09897,
        "authors": "Andrea Coletta, Aymeric Moulin, Svitlana Vyetrenko, Tucker Balch",
        "title": "Learning to simulate realistic limit order book markets from data as a\n  World Agent",
        "comments": null,
        "journal-ref": null,
        "doi": "10.1145/3533271.3561753",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Multi-agent market simulators usually require careful calibration to emulate\nreal markets, which includes the number and the type of agents. Poorly\ncalibrated simulators can lead to misleading conclusions, potentially causing\nsevere loss when employed by investment banks, hedge funds, and traders to\nstudy and evaluate trading strategies. In this paper, we propose a world model\nsimulator that accurately emulates a limit order book market -- it requires no\nagent calibration but rather learns the simulated market behavior directly from\nhistorical data. Traditional approaches fail short to learn and calibrate\ntrader population, as historical labeled data with details on each individual\ntrader strategy is not publicly available. Our approach proposes to learn a\nunique \"world\" agent from historical data. It is intended to emulate the\noverall trader population, without the need of making assumptions about\nindividual market agent strategies. We implement our world agent simulator\nmodels as a Conditional Generative Adversarial Network (CGAN), as well as a\nmixture of parametric distributions, and we compare our models against previous\nwork. Qualitatively and quantitatively, we show that the proposed approaches\nconsistently outperform previous work, providing more realism and\nresponsiveness.\n"
    },
    {
        "paper_id": 2210.09927,
        "authors": "G.A. Nigmatulin, O.B. Chaganova",
        "title": "Research of an optimization model for servicing a network of ATMs and\n  information payment terminals",
        "comments": "Convergent Cognitive Information Technologies. Convergent 2020.\n  Communications in Computer and Information Science, in press, Springer, Cham.\n  http://it-edu.oit.cmc.msu.ru/index.php/convergent/convergent2020 (12 pages,\n  12 figures)",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
        "abstract": "  The steadily high demand for cash contributes to the expansion of the network\nof Bank payment terminals. To optimize the amount of cash in payment terminals,\nit is necessary to minimize the cost of servicing them and ensure that there\nare no excess funds in the network. The purpose of this work is to create a\ncash management system in the network of payment terminals. The article\ndiscusses the solution to the problem of determining the optimal amount of\nfunds to be loaded into the terminals, and the effective frequency of\ncollection, which allows to get additional income by investing the released\nfunds. The paper presents the results of predicting daily cash withdrawals at\nATMs using a triple exponential smoothing model, a recurrent neural network\nwith long short-term memory, and a model of singular spectrum analysis. These\nforecasting models allowed us to obtain a sufficient level of correct forecasts\nwith good accuracy and completeness. The results of forecasting cash\nwithdrawals were used to build a discrete optimal control model, which was used\nto develop an optimal schedule for adding funds to the payment terminal. It is\nproved that the efficiency and reliability of the proposed model is higher than\nthat of the classical Baumol-Tobin inventory management model: when tested on\nthe time series of three ATMs, the discrete optimal control model did not allow\nexhaustion of funds and allowed to earn on average 30% more than the classical\nmodel.\n"
    },
    {
        "paper_id": 2210.10087,
        "authors": "Agni Rajinikanth",
        "title": "Cryptocurrency, Sanctions and Agricultural Prices: An empirical study on\n  the negative implications of sanctions and how decentralized technologies\n  affect the agriculture futures market in developing countries",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The 2022 Russia Ukraine War has led to many sanctions being placed on Russia\nand Ukraine. The paper will discuss the impact the 2022 Russian Sanctions have\non agricultural food prices and hunger. The paper also uses Instrumental\nVariable Analysis to find how Cryptocurrency and Bitcoin can be used to hedge\nagainst the impact of sanctions. The 6 different countries analyzed in this\nstudy including Bangladesh, El Salvador, Iran, Nigeria, Philippines, and South\nAfrica, all of which are heavy importers of wheat and corn. The paper shows\nthat although Bitcoin may be volatile compared to other local currencies, it\nmight be a good investment to safeguard assets since it is not correlated with\ncommodity prices.Furthermore, the study demonstrates that transaction volume\nhas a strong relationship with prices.\n"
    },
    {
        "paper_id": 2210.10146,
        "authors": "Varun Mittal and Laura P. Schaposnik",
        "title": "Housing Forecasts via Stock Market Indicators",
        "comments": "9 pages, 13 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Through the reinterpretation of housing data as candlesticks, we extend\nNature Scientific Reports' article by Liang and Unwin [LU22] on stock market\nindicators for COVID-19 data, and utilize some of the most prominent technical\nindicators from the stock market to estimate future changes in the housing\nmarket, comparing the findings to those one would obtain from studying real\nestate ETF's. By providing an analysis of MACD, RSI, and Candlestick indicators\n(Bullish Engulfing, Bearish Engulfing, Hanging Man, and Hammer), we exhibit\ntheir statistical significance in making predictions for USA data sets (using\nZillow Housing data) and also consider their applications within three\ndifferent scenarios: a stable housing market, a volatile housing market, and a\nsaturated market. In particular, we show that bearish indicators have a much\nhigher statistical significance then bullish indicators, and we further\nillustrate how in less stable or more populated countries, bearish trends are\nonly slightly more statistically present compared to bullish trends.\n"
    },
    {
        "paper_id": 2210.10151,
        "authors": "Koki Inoue, Shuichiro Ogake, Hayato Kawamura, Naoki Igo",
        "title": "Dialogue system with humanoid robot",
        "comments": "This paper is part of the proceedings of the Dialogue Robot\n  Competition2022",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Today, as seen in smart speakers, spoken dialogue technology is rapidly\nadvancing to enable human-like interaction. However, current dialogue systems\ncannot pay attention not only to the content of speech, but also to the way of\nspeaking and eye contact and facial expressions, while watching the facial\nexpressions of the person with whom one is speaking. Therefore, this study\nparticipated in a Japanese competition called the \"Dialogue Robot Competition\"\nand attempted to develop a dialogue system that includes control of not only\nthe content of speech but also the robot's facial expressions and gaze in order\nto realize a humanoid robot that can naturally interact with humans.\n"
    },
    {
        "paper_id": 2210.10166,
        "authors": "Tayfun S\\\"onmez, Utku \\\"Unver",
        "title": "Informed Neutrality in Minimalist Market Design: A Case Study on a\n  Constitutional Crisis in India",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  In a 3-2 split verdict, the Supreme Court approved the exclusion of India's\nsocially and economically backward classes from its affirmative action measures\nto address economic deprivation. Dissenting justices, including the Chief\nJustice of India, protested the Majority Opinion for sanctioning \"an avowedly\nexclusionary and discriminatory principle.\" To justify their controversial\ndecision, the majority justices relied on technical arguments that are\ncategorically false. The confusion of the justices is due to a subtle technical\naspect of the affirmative action system in India: the significance of overlaps\nbetween members of protected groups. Conventionally, protected classes were\ndetermined by the caste system, which meant they did not overlap. The addition\nof a new protected class defined by economic criteria alters this structure,\nunless it is artificially enforced. The majority justices failed to appreciate\nthe significance of this critical change and inaccurately argued that the\ncontroversial exclusion is a technical necessity to provide benefits to\npreviously unprotected members of a new class. We show that this case could\nhave been resolved with three competing policies that each avoid the\ncontroversial exclusion. One of these policies aligns with the core arguments\nin the Majority Opinion, whereas a second aligns with those in the Dissenting\nOpinion.\n"
    },
    {
        "paper_id": 2210.10169,
        "authors": "Eugene Larsen-Hallock, Adam Rej, and David Thesmar",
        "title": "Expectations Formation with Fat-tailed Processes: Evidence from Sales\n  Forecasts",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  We empirically analyze a large sample of firm sales growth expectations. We\nfind that the relationship between forecast errors and lagged revision is\nnon-linear. Forecasters underreact to typical (positive or negative) news about\nfuture sales, but overreact to very significant news. To account for this\nnon-linearity, we propose a simple framework, where (1) sales growth dynamics\nhave a fat-tailed high frequency component and (2) forecasters use a simple\nlinear rule. This framework qualitatively fits several additional features of\ndata on sales growth dynamics, forecast errors, and stock returns.\n"
    },
    {
        "paper_id": 2210.10425,
        "authors": "Katia Colaneri, Alessandra Cretarola and Benedetta Salterini",
        "title": "Optimal investment and reinsurance under exponential forward preferences",
        "comments": "38 pages, 9 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We study the optimal investment and proportional reinsurance problem of an\ninsurance company, whose investment preferences are described via a forward\ndynamic utility of exponential type in a stochastic factor model allowing for a\npossible dependence between the financial and insurance markets. Specifically,\nwe assume that the asset price process dynamics and the claim arrival intensity\nare both affected by a common stochastic process and we account for a possible\nenvironmental contagion effect through the non-zero correlation parameter\nbetween the underlying Brownian motions driving the asset price process and the\nstochastic factor dynamics. By stochastic control techniques, we construct a\nforward dynamic exponential utility, and we characterize the optimal investment\nand reinsurance strategy. Moreover, we investigate in detail the\nzero-volatility case and provide a comparison analysis with classical results\nin an analogous setting under backward utility preferences. We also discuss an\nextension of the conditional certainty equivalent. Finally, we perform a\nnumerical analysis to highlight some features of the optimal strategy.\n"
    },
    {
        "paper_id": 2210.10443,
        "authors": "Lukas Gonon",
        "title": "Deep neural network expressivity for optimal stopping problems",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This article studies deep neural network expression rates for optimal\nstopping problems of discrete-time Markov processes on high-dimensional state\nspaces. A general framework is established in which the value function and\ncontinuation value of an optimal stopping problem can be approximated with\nerror at most $\\varepsilon$ by a deep ReLU neural network of size at most\n$\\kappa d^{\\mathfrak{q}} \\varepsilon^{-\\mathfrak{r}}$. The constants\n$\\kappa,\\mathfrak{q},\\mathfrak{r} \\geq 0$ do not depend on the dimension $d$ of\nthe state space or the approximation accuracy $\\varepsilon$. This proves that\ndeep neural networks do not suffer from the curse of dimensionality when\nemployed to solve optimal stopping problems. The framework covers, for example,\nexponential L\\'evy models, discrete diffusion processes and their running\nminima and maxima. These results mathematically justify the use of deep neural\nnetworks for numerically solving optimal stopping problems and pricing American\noptions in high dimensions.\n"
    },
    {
        "paper_id": 2210.10538,
        "authors": "Allister Loder, Fabienne Cantner, Andrea Cadavid, Markus B. Siewert,\n  Stefan Wurster, Sebastian Goerg, Klaus Bogenberger",
        "title": "A nation-wide experiment: fuel tax cuts and almost free public transport\n  for three months in Germany -- Report 4 Third wave results",
        "comments": "arXiv admin note: text overlap with arXiv:2208.14902",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
        "abstract": "  In spring 2022, the German federal government agreed on a set of measures\nthat aimed at reducing households' financial burden resulting from a recent\nprice increase, especially in energy and mobility. These measures included\namong others, a nation-wide public transport ticket for 9EUR per month and a\nfuel tax cut that reduced fuel prices by more than 15%. In transportation\npolicy and travel behavior research this is an almost unprecedented behavioral\nexperiment. It allows to study not only behavioral responses in mode choice and\ninduced demand but also to assess the effectiveness of transport policy\ninstruments. We observe this natural experiment with a three-wave survey and an\napp-based travel diary on a sample 2'263 individuals; for the Munich Study, 919\nparticipants in the survey-and-app group and 425 in the survey-only group have\nbeen successfully recruited, while 919 participants have been recruited through\na professional panel provider to obtain a representative nation-wide reference\ngroup for the three-wave survey. In this fourth report we present the results\nof the third wave. At the end of the study, all three surveys have been\ncompleted by 1'484 participants and 642 participants completed all three\nsurveys and used the travel diary throughout the entire study. Based on our\nresults we conclude that when offering a 49EUR-Ticket as a successor to the\n9~EUR-Ticket and a local travel pass for 30EUR/month more than 60% of all\n9~EUR-Ticket owners would buy one of the two new travel passes. In other words,\na substantial increase in travel pass ownership in Germany can be expected,\nwith our modest estimate being around 20%. With the announcement of the\nintroduction of a successor ticket in 2023 as well as with the prevailing high\ninflation, this study will continue into the year 2023 to monitor the impact on\nmobility and daily activities.\n"
    },
    {
        "paper_id": 2210.10585,
        "authors": "Dario G. Soatto",
        "title": "The Minimum Wage as an Anchor: Effects on Determinations of Fairness by\n  Humans and AI",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  I study the role of minimum wage as an anchor for judgements of the fairness\nof wages by both human subjects and artificial intelligence (AI). Through\nsurveys of human subjects enrolled in the crowdsourcing platform Prolific.co\nand queries submitted to the OpenAI's language model GPT-3, I test whether the\nnumerical response for what wage is deemed fair for a particular job\ndescription changes when respondents and GPT-3 are prompted with additional\ninformation that includes a numerical minimum wage, whether realistic or\nunrealistic, relative to a control where no minimum wage is stated. I find that\nthe minimum wage influences the distribution of responses for the wage\nconsidered fair by shifting the mean response toward the minimum wage, thus\nestablishing the minimum wage's role as an anchor for judgements of fairness.\nHowever, for unrealistically high minimum wages, namely $50 and $100, the\ndistribution of responses splits into two distinct modes, one that\napproximately follows the anchor and one that remains close to the control,\nalbeit with an overall upward shift towards the anchor. The anchor exerts a\nsimilar effect on the AI bot; however, the wage that the AI bot perceives as\nfair exhibits a systematic downward shift compared to human subjects'\nresponses. For unrealistic values of the anchor, the responses of the bot also\nsplit into two modes but with a smaller proportion of the responses adhering to\nthe anchor compared to human subjects. As with human subjects, the remaining\nresponses are close to the control group for the AI bot but also exhibit a\nsystematic shift towards the anchor. During experimentation, I noted some\nvariability in the bot responses depending on small perturbations of the\nprompt, so I also test variability in the bot's responses with respect to more\nmeaningful differences in gender and race cues in the prompt, finding anomalies\nin the distribution of responses.\n"
    },
    {
        "paper_id": 2210.10832,
        "authors": "Jingjing Li, Nicole Montgomery, and Reza Mousavi",
        "title": "How a Brand's Social Activism Impacts Consumers' Brand Evaluations: The\n  Role of Brand Relationship Norms",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  With the proliferation of social activism online, brands face heightened\npressure from consumers to publicly address these issues. Yet, the optimal\nbrand response strategy (i.e., whether and how to respond) in these contexts\nremains unclear. This research investigates consumers' reactions to brand\nresponse strategies (e.g., engage vs. not) during social activism and offers\npotentially effective responses that brands can employ to engage in these\nissues. By analyzing real-world data collected from Twitter and conducting four\nrandomized experiments, this research discovers that brand relationship type\n(exchange, communal) affects consumers' brand evaluations in the wake of social\nactivism. Communal (vs. exchange) brands are evaluated less favorably when they\ndo not respond or utilize a low-empathy response. This difference is attenuated\nwhen brands employ a high-empathy response. These findings are attributable to\nconsumers' perceptions of whether the brand's response strategy complies with\nrelationship norms during social activism. The effects persist across activism\nevents that vary in their political polarization. This research contributes to\nthe literatures on brand engagement in social activism, brand relationships,\nand crisis communication. The findings also offer guidance to practitioners on\ncrafting response strategies during social activism and aid activists in\nsecuring brand support for societal benefits.\n"
    },
    {
        "paper_id": 2210.10858,
        "authors": "Han Shu and Jacob Mays",
        "title": "Beyond capacity: contractual form in electricity reliability obligations",
        "comments": null,
        "journal-ref": null,
        "doi": "10.1016/j.eneco.2023.106943",
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Liberalized electricity markets often include resource adequacy mechanisms\nthat require consumers to contract with generation resources well in advance of\nreal-time operations. While administratively defined mechanisms have most\ncommonly taken the form of a capacity obligation, efficient markets would\nfeature a broad array of arrangements adapted to the risk profiles and\nappetites of market participants. This article considers how the financial\nhedge embedded in alternative resource adequacy contract designs can induce\ndifferent responses from risk-averse investors, with consequences for the\nresource mix and market structure. We construct a stochastic equilibrium model\ndescribing a competitive market with incomplete risk trading and compute\ninvestment equilibria under different contracting regimes. Two policy\nrecommendations result. First, to avoid creating inefficiency by crowding out\nother forms of risk sharing, system operators should allow resources contracted\nthrough other means to opt out of mandatory capacity mechanisms, with their\ncontribution to those requirements subtracted from administratively defined\ndemand curves. Second, if they wish to promote a single contractual form,\nregulators should consider replacing existing option-like capacity mechanisms\nwith a shaped forward contract for energy. Beyond these recommendations, we\ndiscuss the tension that liberalized systems face in seeking to promote both\nreliability and competitive outcomes.\n"
    },
    {
        "paper_id": 2210.10971,
        "authors": "Di Zhang, Youzhou Zhou",
        "title": "Optimal Settings for Cryptocurrency Trading Pairs",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The goal of cryptocurrencies is decentralization. In principle, all\ncurrencies have equal status. Unlike traditional stock markets, there is no\ndefault currency of denomination (fiat), thus the trading pairs can be set\nfreely. However, it is impractical to set up a trading market between every two\ncurrencies. In order to control management costs and ensure sufficient\nliquidity, we must give priority to covering those large-volume trading pairs\nand ensure that all coins are reachable. We note that this is an optimization\nproblem. Its particularity lies in: 1) the trading volume between most (>99.5%)\npossible trading pairs cannot be directly observed. 2) It satisfies the\nconnectivity constraint, that is, all currencies are guaranteed to be tradable.\n  To solve this problem, we use a two-stage process: 1) Fill in missing values\nbased on a regularized, truncated eigenvalue decomposition, where the\nregularization term is used to control what extent missing values should be\nlimited to zero. 2) Search for the optimal trading pairs, based on a branch and\nbound process, with heuristic search and pruning strategies.\n  The experimental results show that: 1) If the number of denominated coins is\nnot limited, we will get a more decentralized trading pair settings, which\nadvocates the establishment of trading pairs directly between large currency\npairs. 2) There is a certain room for optimization in all exchanges. The\nsetting of inappropriate trading pairs is mainly caused by subjectively setting\nsmall coins to quote, or failing to track emerging big coins in time. 3) Too\nfew trading pairs will lead to low coverage; too many trading pairs will need\nto be adjusted with markets frequently. Exchanges should consider striking an\nappropriate balance between them.\n"
    },
    {
        "paper_id": 2210.11138,
        "authors": "Salvador Linares-Mustar\\'os (1), Maria \\`Angels Farreras-Noguer (1),\n  N\\'uria Arimany-Serrat (2), Germ\\`a Coenders (1) ((1) University of Girona,\n  (2) University of Vic-Central University of Catalonia)",
        "title": "New financial ratios based on the compositional data methodology",
        "comments": null,
        "journal-ref": "Axioms, 11, 12 (2022), 694",
        "doi": "10.3390/axioms11120694",
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  Due to their type of mathematical construction, the use of standard financial\nratios in studies analysing the financial health of a group of firms leads to a\nseries of statistical problems that can invalidate the results obtained. These\nproblems are originated by the asymmetry of financial ratios. The present\narticle justifies the use of a new methodology using compositional data (CoDa)\nto analyse the financial statements of a sector, improving analyses using\nconventional ratios since the new methodology enables statistical techniques to\nbe applied without encountering any serious drawbacks such as skewness and\noutliers, and without the results depending on the arbitrary choice as to which\nof the accounting figures is the numerator of the ratio and which is the\ndenominator. An example with data of the wine sector is provided. The results\nshow that when using CoDa, outliers and skewness are much reduced and results\nare invariant to numerator and denominator permutation.\n"
    },
    {
        "paper_id": 2210.11203,
        "authors": "Xiaotong Sun, Xi Chen, Charalampos Stasinakis, Georgios Sermpinis",
        "title": "Voter Coalitions and democracy in Decentralized Finance: Evidence from\n  MakerDAO",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Decentralized Autonomous Organization (DAO) provides a decentralized\ngovernance solution through blockchain, where decision-making process relies on\non-chain voting and follows majority rule. This paper focuses on MakerDAO, and\nwe find three voter coalitions after applying clustering algorithm to voting\nhistory. The emergence of a dominant voter coalition is a signal of governance\ncentralization in DAO, and voter coalitions have complicated influence on Maker\nprotocol, which is governed by MakerDAO. This paper presents empirical evidence\nof multicoalition democracy in DAO and further contributes to the contemporary\ndebate on whether decentralized governance is possible.\n"
    },
    {
        "paper_id": 2210.11315,
        "authors": "Miles B. Gietzmann and Adam J. Ostaszewski",
        "title": "The Kind of Silence: Managing a Reputation for Voluntary Disclosure in\n  Financial Markets",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  In a continuous-time setting we investigate how the management of a firm\ncontrols a dynamic choice between two generic voluntary disclosure decision\nrules: one with full and transparent disclosure termed $\\it{candid}$, the\nother, termed $\\it{sparing}$, under which values only above a dynamic threshold\nare disclosed. We show how management are rewarded with a reputational premium\nfor candour. The candid strategy is costly because the sparing alternative\nshields the firm from potential downgrades following low value disclosures. We\nshow how parameters of the model such as news intensity, pay-for-performance\nand time-to-mandatory-disclosure determine the optimal choice of candid versus\nsparing strategy and the optimal time for management to switch between the two.\nThe private news updates received by management are modelled with a Poisson\nprocess, occurring between the fixed mandatory disclosure dates, such as fiscal\nyears or quarters, with the news received generated by a background\nBlack-Scholes model of economic activity and of its partial observation. The\nmodel presented develops a number of insights, based on a very simple ordinary\ndifferential equation (ODE) characterizing equilibrium in a\npiecewise-deterministic model, derivable from the background Black-Scholes\nmodel. It is shown that in equilibrium when news intensity is low a firm may\nemploy a candid disclosure strategy throughout, but will otherwise switch\n(alternate) between periods of being candid and periods of being sparing with\nthe truth (or the other way about); that is, we are able to characterize when\nin equilibrium a candid firm will switch to adopting a sparing strategy. The\nmodel illustrates how parameters such as time to mandatory disclosure, news\nintensity and pay-for-performance may drive such switching behaviour.\n$\\it{With\\,constant\\,pay\\,for\\,performance\\,parameters,\\,at\\,most\\,one\\,\nswitching\\,occurs.}$\n"
    },
    {
        "paper_id": 2210.11403,
        "authors": "Abhinav Shanbhag",
        "title": "Exploring Causes, Effects, and Solutions to Financial Illiteracy and\n  Exclusion among Minority Demographic Groups",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  Americans across demographic groups tend to have low financial literacy, with\nlow-income people and minorities at highest risk. This opens the door to the\nexploitation of unbanked low-income families through high-interest alternative\nfinancial services. This paper studies the causes and effects of financial\nilliteracy and exclusion in the most at-risk demographic groups, and solutions\nproven to bring them into the financial mainstream. This paper finds that\nimmigrants, ethnic minorities, and low-income families are most likely to be\nunbanked. Furthermore, the causes for being unbanked include the high fees of\nbank accounts, the inability of Americans to maintain bank accounts due to low\nfinancial assets or time, banking needs being met by alternative financial\nservices, and being provided minimal help while transitioning from welfare to\nthe workforce. The most effective solutions to financial illiteracy and\nexclusion include partnerships between nonprofits, banks, and businesses that\nuse existing alternative financial service platforms to transition the unbanked\ninto using products that meet their needs, educating the unbanked in the use of\nmobile banking, and providing profitable consumer credit products targeting\nunbanked families with features that support their needs in addition to\ntargeted and properly implemented financial literacy programs.\n"
    },
    {
        "paper_id": 2210.11525,
        "authors": "Francis X. Diebold",
        "title": "On the Financing of Climate Change Adaptation in Developing Countries",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  I offer reflections on adaptation to climate change, with emphasis on\ndeveloping areas.\n"
    },
    {
        "paper_id": 2210.11532,
        "authors": "Ivan Letteri, Giuseppe Della Penna, Giovanni De Gasperis, Abeer Dyoub",
        "title": "DNN-ForwardTesting: A New Trading Strategy Validation using Statistical\n  Timeseries Analysis and Deep Neural Networks",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  In general, traders test their trading strategies by applying them on the\nhistorical market data (backtesting), and then apply to the future trades the\nstrategy that achieved the maximum profit on such past data.\n  In this paper, we propose a new trading strategy, called DNN-forwardtesting,\nthat determines the strategy to apply by testing it on the possible future\npredicted by a deep neural network that has been designed to perform stock\nprice forecasts and trained with the market historical data.\n  In order to generate such an historical dataset, we first perform an\nexploratory data analysis on a set of ten securities and, in particular,\nanalize their volatility through a novel k-means-based procedure. Then, we\nrestrict the dataset to a small number of assets with the same volatility\ncoefficient and use such data to train a deep feed-forward neural network that\nforecasts the prices for the next 30 days of open stocks market. Finally, our\ntrading system calculates the most effective technical indicator by applying it\nto the DNNs predictions and uses such indicator to guide its trades.\n  The results confirm that neural networks outperform classical statistical\ntechniques when performing such forecasts, and their predictions allow to\nselect a trading strategy that, when applied to the real future, increases\nExpectancy, Sharpe, Sortino, and Calmar ratios with respect to the strategy\nselected through traditional backtesting.\n"
    },
    {
        "paper_id": 2210.11792,
        "authors": "Aleksejus Kononovicius, Bronislovas Kaulakys",
        "title": "$1/f$ noise from the sequence of nonoverlapping rectangular pulses",
        "comments": "14 pages, 5 figures",
        "journal-ref": "Phys. Rev. E 107: 034117 (2023)",
        "doi": "10.1103/PhysRevE.107.034117",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We analyze the power spectral density of a signal composed of nonoverlapping\nrectangular pulses. First, we derive a general formula for the power spectral\ndensity of a signal constructed from the sequence of nonoverlapping pulses.\nThen we perform a detailed analysis of the rectangular pulse case. We show that\npure $1/f$ noise can be observed until extremely low frequencies when the\ncharacteristic pulse (or gap) duration is long in comparison to the\ncharacteristic gap (or pulse) duration, and gap (or pulse) durations are\npower-law distributed. The obtained results hold for the ergodic and weakly\nnonergodic processes.\n"
    },
    {
        "paper_id": 2210.12307,
        "authors": "Anne de Bortoli, Yacine Baouch, Mustapha Masdan",
        "title": "BIM can help decarbonize the construction sector: life cycle evidence\n  from Pavement Management Systems",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
        "abstract": "  Transforming the construction sector is key to reaching net-zero, and many\nstakeholders expect its decarbonization through digitalization. But no\nquantified evidence has been brought to date. We propose the first\nenvironmental quantification of the impact of Building Information Modeling\n(BIM) in the construction sector. Specifically, the direct and indirect\ngreenhouse gas (GHG) emissions generated by a monofunctional BIM to plan road\nmaintenance, a Pavement Management System (PMS), are evaluated using field data\nfrom France. The related carbon footprints are calculated following a life\ncycle approach, using different sources of data, including ecoinvent v3.6, and\nthe IPCC 2013 GWP 100a characterization factors. Three design-build-maintain\npavement alternatives are compared: scenario 1 relates to a massive design and\nsurface maintenance, scenario 2 to a progressive design and pre-planned\nstructural maintenance, and scenario 3 to a progressive design and tailored\nstructural maintenance supported by the PMS. First, results show negligible\ndirect emissions due to the PMS existence: 0.02% of the life cycle emissions of\nscenario 3. Second, complementary sensitivity analyses show that using a PMS is\nclimate-positive over the life cycle when pavement subgrade bearing capacity\nimproves over time, and climate-neutral otherwise. The GHG emissions savings\nusing BIM can reach up to 30% of the life cycle emissions compared to other\nscenarios, and 65% when restraining the scope to maintenance and rehabilitation\nand excluding original pavement construction. Third, the neutral effect of BIM\nin case of a deterioration of the bearing capacity of the subgrade may be\nexplained by design practices and safety margins, that could be enhanced using\nBIM. Fourth, the decarbonization potential of a multifunctional BIM is\ndiscussed, and research perspectives are presented.\n"
    },
    {
        "paper_id": 2210.12393,
        "authors": "Alessandro Bondi, Sergio Pulido, Simone Scotti",
        "title": "The rough Hawkes Heston stochastic volatility model",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  We study an extension of the Heston stochastic volatility model that\nincorporates rough volatility and jump clustering phenomena. In our model,\nnamed the rough Hawkes Heston stochastic volatility model, the spot variance is\na rough Hawkes-type process proportional to the intensity process of the jump\ncomponent appearing in the dynamics of the spot variance itself and the log\nreturns. The model belongs to the class of affine Volterra models. In\nparticular, the Fourier-Laplace transform of the log returns and the square of\nthe volatility index can be computed explicitly in terms of solutions of\ndeterministic Riccati-Volterra equations, which can be efficiently approximated\nusing a multi-factor approximation technique. We calibrate a parsimonious\nspecification of our model characterized by a power kernel and an exponential\nlaw for the jumps. We show that our parsimonious setup is able to\nsimultaneously capture, with a high precision, the behavior of the implied\nvolatility smile for both S&P 500 and VIX options. In particular, we observe\nthat in our setting the usual shift in the implied volatility of VIX options is\nexplained by a very low value of the power in the kernel. Our findings\ndemonstrate the relevance, under an affine framework, of rough volatility and\nself-exciting jumps in order to capture the joint evolution of the S&P 500 and\nVIX.\n"
    },
    {
        "paper_id": 2210.12462,
        "authors": "Zikai Wei, Bo Dai, Dahua Lin",
        "title": "Factor Investing with a Deep Multi-Factor Model",
        "comments": "8 pages, 4 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Modeling and characterizing multiple factors is perhaps the most important\nstep in achieving excess returns over market benchmarks. Both academia and\nindustry are striving to find new factors that have good explanatory power for\nfuture stock returns and good stability of their predictive power. In practice,\nfactor investing is still largely based on linear multi-factor models, although\nmany deep learning methods show promising results compared to traditional\nmethods in stock trend prediction and portfolio risk management. However, the\nexisting non-linear methods have two drawbacks: 1) there is a lack of\ninterpretation of the newly discovered factors, 2) the financial insights\nbehind the mining process are unclear, making practitioners reluctant to apply\nthe existing methods to factor investing. To address these two shortcomings, we\ndevelop a novel deep multi-factor model that adopts industry neutralization and\nmarket neutralization modules with clear financial insights, which help us\neasily build a dynamic and multi-relational stock graph in a hierarchical\nstructure to learn the graph representation of stock relationships at different\nlevels, e.g., industry level and universal level. Subsequently, graph attention\nmodules are adopted to estimate a series of deep factors that maximize the\ncumulative factor returns. And a factor-attention module is developed to\napproximately compose the estimated deep factors from the input factors, as a\nway to interpret the deep factors explicitly. Extensive experiments on\nreal-world stock market data demonstrate the effectiveness of our deep\nmulti-factor model in the task of factor investing.\n"
    },
    {
        "paper_id": 2210.12881,
        "authors": "Oguzhan Akcin, Robert P. Streit, Benjamin Oommen, Sriram Vishwanath,\n  Sandeep Chinchali",
        "title": "A Control Theoretic Approach to Infrastructure-Centric Blockchain\n  Tokenomics",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  There are a multitude of Blockchain-based physical infrastructure systems,\noperating on a crypto-currency enabled token economy, where infrastructure\nsuppliers are rewarded with tokens for enabling, validating, managing and/or\nsecuring the system. However, today's token economies are largely designed\nwithout infrastructure systems in mind, and often operate with a fixed token\nsupply (e.g., Bitcoin). This paper argues that token economies for\ninfrastructure networks should be structured differently - they should\ncontinually incentivize new suppliers to join the network to provide services\nand support to the ecosystem. As such, the associated token rewards should\ngracefully scale with the size of the decentralized system, but should be\ncarefully balanced with consumer demand to manage inflation and be designed to\nultimately reach an equilibrium. To achieve such an equilibrium, the\ndecentralized token economy should be adaptable and controllable so that it\nmaximizes the total utility of all users, such as achieving stable (overall\nnon-inflationary) token economies.\n  Our main contribution is to model infrastructure token economies as dynamical\nsystems - the circulating token supply, price, and consumer demand change as a\nfunction of the payment to nodes and costs to consumers for infrastructure\nservices. Crucially, this dynamical systems view enables us to leverage tools\nfrom mathematical control theory to optimize the overall decentralized\nnetwork's performance. Moreover, our model extends easily to a Stackelberg game\nbetween the controller and the nodes, which we use for robust, strategic\npricing. In short, we develop predictive, optimization-based controllers that\noutperform traditional algorithmic stablecoin heuristics by up to $2.4 \\times$\nin simulations based on real demand data from existing decentralized wireless\nnetworks.\n"
    },
    {
        "paper_id": 2210.133,
        "authors": "Luca Galimberti, Anastasis Kratsios, Giulia Livieri",
        "title": "Designing Universal Causal Deep Learning Models: The Case of\n  Infinite-Dimensional Dynamical Systems from Stochastic Analysis",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Causal operators (CO), such as various solution operators to stochastic\ndifferential equations, play a central role in contemporary stochastic\nanalysis; however, there is still no canonical framework for designing Deep\nLearning (DL) models capable of approximating COs. This paper proposes a\n\"geometry-aware'\" solution to this open problem by introducing a DL\nmodel-design framework that takes suitable infinite-dimensional linear metric\nspaces as inputs and returns a universal sequential DL model adapted to these\nlinear geometries. We call these models Causal Neural Operators (CNOs). Our\nmain result states that the models produced by our framework can uniformly\napproximate on compact sets and across arbitrarily finite-time horizons\nH\\\"older or smooth trace class operators, which causally map sequences between\ngiven linear metric spaces. Our analysis uncovers new quantitative\nrelationships on the latent state-space dimension of CNOs which even have new\nimplications for (classical) finite-dimensional Recurrent Neural Networks\n(RNNs). We find that a linear increase of the CNO's (or RNN's) latent parameter\nspace's dimension and of its width, and a logarithmic increase of its depth\nimply an exponential increase in the number of time steps for which its\napproximation remains valid. A direct consequence of our analysis shows that\nRNNs can approximate causal functions using exponentially fewer parameters than\nReLU networks.\n"
    },
    {
        "paper_id": 2210.13655,
        "authors": "Elie Kapengut and Bruce Mizrach",
        "title": "An Event Study of the Ethereum Transition to Proof-of-Stake",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  On September 15, 2022, the Ethereum network adopted a proof-of-stake (PoS)\nconsensus mechanism. We study the impact on the network and competing platforms\nin a two month event window around the Beacon chain merge. We find that the\ntransition to PoS has reduced energy consumption by 99.98%. Miners have not\ntransformed into validators, and total block reward income (in USD) has fallen\nby 97%, though transaction fees (in ETH) for Ether have increased nearly 10%.\nThe Herfindahl index for the top 10 is 1,009; the network is 19% less\nconcentrated after the merge. Ethereum supply growth has been deflationary\nsince the merge. The time between consecutive blocks is now steady at 12\nseconds and transactions per day are up 7.0%. On Polygon, Matic fees rose but\ntoken fees fell. Polygon also slows, processing 3.3% fewer transactions per\nday. Solana's fees fall by $0.0003, and transactions per day are down 48%.\nStablecoin transfer volumes fall on Ethereum and Polygon, but rise on Solana.\n"
    },
    {
        "paper_id": 2210.13667,
        "authors": "Clary Rodriguez-Cruz, Mehdi Molaei, Amruthesh Thirumalaiswamy, Klebert\n  Feitosa, Vinothan N. Manoharan, Shankar Sivarajan, Daniel H. Reich, Robert A.\n  Riggleman and John C. Crocker",
        "title": "Experimental observations of fractal landscape dynamics in a dense\n  emulsion",
        "comments": "10 pages, 5 figures with Appendices",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  Many soft and biological materials display so-called 'soft glassy' dynamics;\ntheir constituents undergo anomalous random motions and complex cooperative\nrearrangements. A recent simulation model of one soft glassy material, a\ncoarsening foam, suggested that the random motions of its bubbles are due to\nthe system configuration moving over a fractal energy landscape in\nhigh-dimensional space. Here we show that the salient geometrical features of\nsuch high-dimensional fractal landscapes can be explored and reliably\nquantified, using empirical trajectory data from many degrees of freedom, in a\nmodel-free manner. For a mayonnaise-like dense emulsion, analysis of the\nobserved trajectories of oil droplets quantitatively reproduces the\nhigh-dimensional fractal geometry of the configuration path and its associated\nenergy minima generated using a computational model. That geometry in turn\ndrives the droplets' complex random motion observed in real space. Our results\nindicate that experimental studies can elucidate whether the similar dynamics\nin different soft and biological materials may also be due to fractal landscape\ndynamics.\n"
    },
    {
        "paper_id": 2210.13671,
        "authors": "Yoshihiro Shirai",
        "title": "Extreme Measures in Continuous Time Conic Finace",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
        "abstract": "  Dynamic spectral risk measures define a claim's valuation bounds as supremum\nand infimum of expectations of the claim's payoff over a dominated set of\nmeasures. The measures at which such extrema are attained are called extreme\nmeasures. We determine explicit expressions for their Radon-Nykodim derivatives\nwith respect to the common dominating measure. Based on the formulas found, we\nestimate the extreme measures in two cases. First, the dominating measure is\ncalibrated to mid prices of options and valuation bounds are given by options\nbid and ask prices. Second, the dominating measure is estimated from historical\nmid equity prices and valuation bounds are given by historical 5-day high and\nlow prices. In both experiments, we find that the market determines upper\nbounds by testing scenarios in which losses are significantly lower than\nexpected under the dominating measure, while lower bounds by ones in which\ngains are only slightly lower than in the base case.\n"
    },
    {
        "paper_id": 2210.13684,
        "authors": "Gholamreza Hajargasht",
        "title": "Reliability of Ideal Indexes",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  The Fisher and GEKS are celebrated as ideal bilateral and multilateral\nindexes due to their superior axiomatic and econ-theoretic properties. The\nFisher index is the main index used for constructing CPI by statistical\nagencies and the GEKS is the index used for compiling PPPs in World Bank's\nInternational Comparison Program (ICP). Despite such a high status and the\nimportance attached to these indexes, the stochastic approach to these indexes\nis not well-developed and no measures of reliability exist for these important\neconomic statistics. The main objective of this paper is to fill this gap. We\nshow how appropriate reliability measures for the Fisher and GEKS indexes, and\nother ideal price index numbers can be derived and how they should be\ninterpreted and used in practice. In an application to 2017 ICP data on a\nsample of 173 countries, we estimate the Fisher and GEKS indexes along with\ntheir reliability measures, make comparisons with several other notable indexes\nand discuss the implications.\n"
    },
    {
        "paper_id": 2210.13804,
        "authors": "Francesca Biagini, Andrea Mazzon, Thilo Meyer-Brandis, Katharina\n  Oberpriller",
        "title": "Liquidity based modeling of asset price bubbles via random matching",
        "comments": "37 pages, 3 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper we study the evolution of asset price bubbles driven by\ncontagion effects spreading among investors via a random matching mechanism in\na discrete-time version of the liquidity based model of [25]. To this scope, we\nextend the Markov conditionally independent dynamic directed random matching of\n[13] to a stochastic setting to include stochastic exogenous factors in the\nmodel. We derive conditions guaranteeing that the financial market model is\narbitrage-free and present some numerical simulation illustrating our approach.\n"
    },
    {
        "paper_id": 2210.13824,
        "authors": "Ekaterina Morozova and Vladimir Panov",
        "title": "Modelling the Bitcoin prices and the media attention to Bitcoin via the\n  jump-type processes",
        "comments": "20 pages; 8 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  In this paper, we present a new bivariate model for the joint description of\nthe Bitcoin prices and the media attention to Bitcoin. Our model is based on\nthe class of the L\\'evy processes and is able to realistically reproduce the\njump-type dynamics of the considered time series. We focus on the low-frequency\nsetup, which is for the L\\'evy - based models essentially more difficult than\nthe high-frequency case. We design a semiparametric estimation procedure for\nthe statistical inference on the parameters and the L\\'evy measures of the\nconsidered processes. We show that the dynamics of the market attention can be\neffectively modelled by the L\\'evy processes with finite L\\'evy measures, and\npropose a data-driven procedure for the description of the Bitcoin prices.\n"
    },
    {
        "paper_id": 2210.13825,
        "authors": "Sarah Kaakai (LMM), Anis Matoussi (LMM), Achraf Tamtalini (LMM)",
        "title": "Multivariate Optimized Certainty Equivalent Risk Measures and their\n  Numerical Computation",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We present a framework for constructing multivariate risk measures that is\ninspired from univariate Optimized Certainty Equivalent (OCE) risk measures. We\nshow that this new class of risk measures verifies the desirable properties\nsuch as convexity, monotonocity and cash invariance. We also address numerical\naspects of their computations using stochastic algorithms instead of using\nMonte Carlo or Fourier methods that do not provide any error of the estimation.\n"
    },
    {
        "paper_id": 2210.13833,
        "authors": "Guohui Guan, Zongxia Liang and Yilun Song",
        "title": "The continuous-time pre-commitment KMM problem in incomplete markets",
        "comments": "53 pages, 7 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper studies the continuous-time pre-commitment KMM problem proposed by\nKlibanoff, Marinacci and Mukerji (2005) in incomplete financial markets, which\nconcerns with the portfolio selection under smooth ambiguity. The decision\nmaker (DM) is uncertain about the dominated priors of the financial market,\nwhich are characterized by a second-order distribution (SOD). The KMM model\nseparates risk attitudes and ambiguity attitudes apart and the aim of the DM is\nto maximize the two-fold utility of terminal wealth, which does not belong to\nthe classical subjective utility maximization problem. By constructing the\nefficient frontier, the original KMM problem is first simplified as an one-fold\nexpected utility problem on the second-order space. In order to solve the\nequivalent simplified problem, this paper imposes an assumption and introduces\na new distorted Legendre transformation to establish the bipolar relation and\nthe distorted duality theorem. Then, under a further assumption that the\nasymptotic elasticity of the ambiguous attitude is less than 1, the uniqueness\nand existence of the solution to the KMM problem are shown and we obtain the\nsemi-explicit forms of the optimal terminal wealth and the optimal strategy.\nExplicit forms of optimal strategies are presented for CRRA, CARA and HARA\nutilities in the case of Gaussian SOD in a Black-Scholes financial market,\nwhich show that DM with higher ambiguity aversion tends to be more concerned\nabout extreme market conditions with larger bias. In the end of this work,\nnumerical comparisons with the DMs ignoring ambiguity are revealed to\nillustrate the effects of ambiguity on the optimal strategies and value\nfunctions.\n"
    },
    {
        "paper_id": 2210.14195,
        "authors": "Lele Cao, Vilhelm von Ehrenheim, Sebastian Krakowski, Xiaoxue Li,\n  Alexandra Lutz",
        "title": "Using Deep Learning to Find the Next Unicorn: A Practical Synthesis",
        "comments": "A condensed version is published by IJCAI 2024 Workshop on FinNLP and\n  Muffin (48 pages, 18 figures). ACL Link:\n  https://aclanthology.org/2023.finnlp-1.6",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Startups often represent newly established business models associated with\ndisruptive innovation and high scalability. They are commonly regarded as\npowerful engines for economic and social development. Meanwhile, startups are\nheavily constrained by many factors such as limited financial funding and human\nresources. Therefore, the chance for a startup to eventually succeed is as rare\nas \"spotting a unicorn in the wild\". Venture Capital (VC) strives to identify\nand invest in unicorn startups during their early stages, hoping to gain a high\nreturn. To avoid entirely relying on human domain expertise and intuition,\ninvestors usually employ data-driven approaches to forecast the success\nprobability of startups. Over the past two decades, the industry has gone\nthrough a paradigm shift moving from conventional statistical approaches\ntowards becoming machine-learning (ML) based. Notably, the rapid growth of data\nvolume and variety is quickly ushering in deep learning (DL), a subset of ML,\nas a potentially superior approach in terms of capacity and expressivity. In\nthis work, we carry out a literature review and synthesis on DL-based\napproaches, covering the entire DL life cycle. The objective is a) to obtain a\nthorough and in-depth understanding of the methodologies for startup evaluation\nusing DL, and b) to distil valuable and actionable learning for practitioners.\nTo the best of our knowledge, our work is the first of this kind.\n"
    },
    {
        "paper_id": 2210.14266,
        "authors": "Jason R. Bailey, Davide Lauria, W. Brent Lindquist, Stefan Mittnik,\n  Svetlozar T. Rachev",
        "title": "Hedonic Models of Real Estate Prices: GAM and Environmental Factors",
        "comments": "12 pages, 3 figures, 4 tables",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  We consider the use of P-spline generalized additive hedonic models for real\nestate prices in large U.S. cities, contrasting their predictive efficiency\nagainst linear and polynomial based generalized linear models. Using intrinsic\nand extrinsic factors available from Redfin, we show that GAM models are\ncapable of describing 84% to 92% of the variance in the expected ln(sales\nprice), based upon 2021 data. As climate change is becoming increasingly\nimportant, we utilized the GAM model to examine the significance of\nenvironmental factors in two urban centers on the northwest coast. The results\nindicate city dependent differences in the significance of environmental\nfactors. We find that inclusion of the environmental factors increases the\nadjusted R-squared of the GAM model by less than one percent.\n"
    },
    {
        "paper_id": 2210.14304,
        "authors": "Xianzhi Li, Will Aitken, Xiaodan Zhu, Stephen W. Thomas",
        "title": "Learning Better Intent Representations for Financial Open Intent\n  Classification",
        "comments": "Accepted to FinNLP-2022, in conjunction with EMNLP-2022",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  With the recent surge of NLP technologies in the financial domain, banks and\nother financial entities have adopted virtual agents (VA) to assist customers.\nA challenging problem for VAs in this domain is determining a user's reason or\nintent for contacting the VA, especially when the intent was unseen or open\nduring the VA's training. One method for handling open intents is adaptive\ndecision boundary (ADB) post-processing, which learns tight decision boundaries\nfrom intent representations to separate known and open intents. We propose\nincorporating two methods for supervised pre-training of intent\nrepresentations: prefix-tuning and fine-tuning just the last layer of a large\nlanguage model (LLM). With this proposal, our accuracy is 1.63% - 2.07% higher\nthan the prior state-of-the-art ADB method for open intent classification on\nthe banking77 benchmark amongst others. Notably, we only supplement the\noriginal ADB model with 0.1% additional trainable parameters. Ablation studies\nalso determine that our method yields better results than full fine-tuning the\nentire model. We hypothesize that our findings could stimulate a new optimal\nmethod of downstream tuning that combines parameter efficient tuning modules\nwith fine-tuning a subset of the base model's layers.\n"
    },
    {
        "paper_id": 2210.1434,
        "authors": "Max Nendel and Alessandro Sgarabottolo",
        "title": "A parametric approach to the estimation of convex risk functionals based\n  on Wasserstein distance",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper, we explore a static setting for the assessment of risk in the\ncontext of mathematical finance and actuarial science that takes into account\nmodel uncertainty in the distribution of a possibly infinite-dimensional risk\nfactor. We allow for perturbations around a baseline model, measured via\nWasserstein distance, and we investigate to which extent this form of\nprobabilistic imprecision can be parametrized. The aim is to come up with a\nconvex risk functional that incorporates a sefety margin with respect to\nnonparametric uncertainty and still can be approximated through parametrized\nmodels. The particular form of the parametrization allows us to develop a\nnumerical method, based on neural networks, which gives both the value of the\nrisk functional and the optimal perturbation of the reference measure.\nMoreover, we study the problem under additional constraints on the\nperturbations, namely, a mean and a martingale constraint. We show that, in\nboth cases, under suitable conditions on the loss function, it is still\npossible to estimate the risk functional by passing to a parametric family of\nperturbed models, which again allows for a numerical approximation via neural\nnetworks.\n"
    },
    {
        "paper_id": 2210.14518,
        "authors": "Max Berre",
        "title": "Which Factors Matter Most? Can Startup Valuation be Micro-Targeted?",
        "comments": null,
        "journal-ref": "International Congress on Small Business (ICSB) World Congress,\n  Jul 2022, Washington DC, United States",
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  While startup valuations are influenced by revenues, risks, age, and\nmacroeconomic conditions, specific causality is traditionally a black box.\nBecause valuations are not disclosed, roles played by other factors (industry,\ngeography, and intellectual property) can often only be guessed at. VC\nvaluation research indicates the importance of establishing a factor-hierarchy\nto better understand startup valuations and their dynamics, suggesting the\nwisdom of hiring data-scientists for this purpose. Bespoke understanding can be\nestablished via construction of hierarchical prediction models based on\ndecision trees and random forests. These have the advantage of understanding\nwhich factors matter most. In combination with OLS, the also tell us the\ncircumstances of when specific causalities apply. This study explores the\ndeterministic role of categorical variables on the valuation of start-ups (i.e.\nthe joint-combination geographic, urban, and sectoral denomination-variables),\nin order to be able to build a generalized valuation scorecard approach. Using\na dataset of 1,091 venture-capital investments, containing 1,044 unique EU and\nEEA, this study examines microeconomic, sectoral, and local-level impacts on\nstartup valuation. In principle, the study relies on Fixedeffects and\nJoint-fixed-effects regressions as well as the analysis and exploration of\ndivergent micropopulations and fault-lines by means of non-parametric\napproaches combining econometric and machinelearning techniques.\n"
    },
    {
        "paper_id": 2210.14605,
        "authors": "Mostafa Shabani, Martin Magris, George Tzagkarakis, Juho Kanniainen,\n  Alexandros Iosifidis",
        "title": "Predicting the State of Synchronization of Financial Time Series using\n  Cross Recurrence Plots",
        "comments": "Paper submitted to and under consideration at Pattern and Recognition\n  Letters",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Cross-correlation analysis is a powerful tool for understanding the mutual\ndynamics of time series. This study introduces a new method for predicting the\nfuture state of synchronization of the dynamics of two financial time series.\nTo this end, we use the cross-recurrence plot analysis as a nonlinear method\nfor quantifying the multidimensional coupling in the time domain of two time\nseries and for determining their state of synchronization. We adopt a deep\nlearning framework for methodologically addressing the prediction of the\nsynchronization state based on features extracted from dynamically sub-sampled\ncross-recurrence plots. We provide extensive experiments on several stocks,\nmajor constituents of the S\\&P100 index, to empirically validate our approach.\nWe find that the task of predicting the state of synchronization of two time\nseries is in general rather difficult, but for certain pairs of stocks\nattainable with very satisfactory performance.\n"
    },
    {
        "paper_id": 2210.14631,
        "authors": "Yizhao Jiang",
        "title": "The Influence of Payment Method: Do Consumers Pay More with Mobile\n  Payment?",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
        "abstract": "  The introduction of new payment methods has resulted in one of the most\nsignificant changes in the way we consume goods and services. In this paper, I\npresent results of a field and a laboratory experiment designed to determine\nthe effect of payment method (cash vs. mobile payment) on spending, and a\nmeta-analysis of previous literature about payment method effect. In the field\nexperiment, I collected cashier receipts from Chinese supermarkets. Compared to\ncash payment, mobile payments significantly increased the amount purchased and\nthe average amount spent on each item. This effect was found to be particularly\nlarge for high price elasticity goods. In the laboratory experiment,\nparticipants were randomly assigned to one of four groups that varied with\nrespect to the kind of payment and the kind of incentives, eliminating the\npotential endogeneity problem from the field experiment. I found that compared\nto cash, mobile payments lead to a significantly higher willingness to pay\n(WTP) for consumption. In contrast to others, I found that pain of paying does\nnot moderate the payment method effect; however, other psychological factors\nwere found to work as potential mechanisms for affecting WTP.\n"
    },
    {
        "paper_id": 2210.14655,
        "authors": "Joanna Nie\\.zurawska, Rados{\\l}aw A. Kycia, Iveta Ludviga, Agnieszka\n  Niemczynowicz",
        "title": "Model of work motivation based on happiness: pandemic related study",
        "comments": "13 pages, 6 figures, 2 tables",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This study aims to enrich the current literature by providing a new approach\nto motivating Generation Z employees in Poland. Employees need to be motivated\nin order to be efficient at doing a particular task at the workplace. As young\npeople born between 1995 and 2004 called Generation Z, enter the labour market\nit, is essential to consider how employees' motivation might be affected.\nTraditionally identified motivators are known, but many reports indicate that\nthe motivation continues decreasing. This situation causes some perturbations\nin business and fluctuations of staff. In order to prevent this situation, the\nemployers are looking for new solutions to motivate the employees. A\nquantitative approach was used to collect new evidence from 200 Polish\nrespondents completing an online survey. The research were conducted before and\nduring pandemic time. We report and analyse the survey results conducted in\nPoland among representatives of Generation Z, who were employed for at least 6\nmonths. We developed and validated a new approach to motivation using\nmethodologies called Factor Analysis. Based on empirical verification, we found\na new tool that connects employee motivation and selected areas of the Hygge\nconcept called Hygge star model, which has the same semantics before and during\nCovid-19 pandemic.\n"
    },
    {
        "paper_id": 2210.14908,
        "authors": "Zunian Luo",
        "title": "Powering Up a Slow Charging Market: How Do Government Subsidies Affect\n  Charging Station Supply?",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Electric vehicle adoption is considered to be a promising pathway for\naddressing climate change. However, the market for charging stations suffers\nfrom a market failure: a lack of EV sales disincentives charging station\nproduction, which in turn inhibits mass EV adoption. Charging station subsidies\nare often discussed as policy levers that can stimulate charging station supply\nand correct this market failure. Nonetheless, there is limited research\nexamining the extent such subsidies are successful in promoting charging\nstation supply. Using annual data on electric vehicle sales, charging station\ncounts, and subsidy amounts from 57 California counties and a staggered\ndifference-in-differences methodology, I find that charging station subsidies\nare highly effective: counties that adopt subsidies experience a 36% increase\nin charging station supply 2 years following subsidy adoption. This finding\nsuggests that governmental intervention can help correct the market failure in\nthe charging station market.\n"
    },
    {
        "paper_id": 2210.14942,
        "authors": "Lanqing Du, Michelle Kim, Jinwook Lee",
        "title": "The Art NFTs and Their Marketplaces",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Non-Fungible Tokens (NFTs) are crypto assets with a unique digital identifier\nfor ownership, powered by blockchain technology. Technically speaking, anything\ndigital could be minted and sold as an NFT, which provides proof of ownership\nand authenticity of a digital file. For this reason, it helps us distinguish\nbetween the originals and their copies, making it possible to trade them. This\npaper focuses on art NFTs that change how artists can sell their products. It\nalso changes how the art trade market works since NFT technology cuts out the\nmiddleman. Recently, the utility of NFTs has become an essential issue in the\nNFT ecosystem, which refers to the owners' usefulness, profitability, and\nbenefits. Using recent major art NFT marketplace datasets, we summarize and\ninterpret the current market trends and patterns in a way that brings insight\ninto the future art market. Numerical examples are presented.\n"
    },
    {
        "paper_id": 2210.15329,
        "authors": "Ricardo Crisostomo",
        "title": "Measuring Transition Risk in Investment Funds",
        "comments": "25 pages, 6 figures, 11 tables",
        "journal-ref": "CNMV Working Paper (2022), Forthcoming",
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  We develop a comprehensive framework to measure the impact of the climate\ntransition on investment portfolios. Our analysis is enriched by including\ngeographical, sectoral, company and ISIN-level data to assess transition risk.\nWe find that investment funds suffer a moderate 5.7% loss upon materialization\nof a high transition risk scenario. However, the risk distribution is\nsignificantly left-skewed, with the worst 1% funds experiencing an average loss\nof 21.3%. In terms of asset classes, equities are the worst performers\n(-12.7%), followed by corporate bonds (-5.6%) and government bonds (-4.8%). We\ndiscriminate among financial instruments by considering the carbon footprint of\nspecific counterparties and the credit rating, duration, convexity and\nvolatility of individual exposures. We find that sustainable funds are less\nexposed to transition risk and perform better than the overall fund sector in\nthe low-carbon transition, validating their choice as green investments.\n"
    },
    {
        "paper_id": 2210.15343,
        "authors": "David R. Ba\\~nos, Salvador Ortiz-Latorre and Oriol Zamora Font",
        "title": "Change of measure in a Heston-Hawkes stochastic volatility model",
        "comments": "23 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  We consider the stochastic volatility model obtained by adding a compound\nHawkes process to the volatility of the well-known Heston model. A Hawkes\nprocess is a self-exciting counting process with many applications in\nmathematical finance, insurance, epidemiology, seismology and other fields. We\nprove a general result on the existence of a family of equivalent (local)\nmartingale measures. We apply this result to a particular example where the\nsizes of the jumps are exponentially distributed.\n"
    },
    {
        "paper_id": 2210.15448,
        "authors": "Amit Milstein, Haoran Deng, Guy Revach, Hai Morgenstern and Nir\n  Shlezinger",
        "title": "Neural Augmented Kalman Filtering with Bollinger Bands for Pairs Trading",
        "comments": "Submitted to Transactions on Signal Processing",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Pairs trading is a family of trading techniques that determine their policies\nbased on monitoring the relationships between pairs of assets. A common pairs\ntrading approach relies on describing the pair-wise relationship as a linear\nSpace State (SS) model with Gaussian noise. This representation facilitates\nextracting financial indicators with low complexity and latency using a Kalman\nFilter (KF), that are then processed using classic policies such as Bollinger\nBands (BB). However, such SS models are inherently approximated and mismatched,\noften degrading the revenue. In this work, we propose KalmenNet-aided Bollinger\nbands Pairs Trading (KBPT), a deep learning aided policy that augments the\noperation of KF-aided BB trading. KBPT is designed by formulating an extended\nSS model for pairs trading that approximates their relationship as holding\npartial co-integration. This SS model is utilized by a trading policy that\naugments KF-BB trading with a dedicated neural network based on the KalmanNet\narchitecture. The resulting KBPT is trained in a two-stage manner which first\ntunes the tracking algorithm in an unsupervised manner independently of the\ntrading task, followed by its adaptation to track the financial indicators to\nmaximize revenue while approximating BB with a differentiable mapping. KBPT\nthus leverages data to overcome the approximated nature of the SS model,\nconverting the KF-BB policy into a trainable model. We empirically demonstrate\nthat our proposed KBPT systematically yields improved revenue compared with\nmodel-based and data-driven benchmarks over various different assets.\n"
    },
    {
        "paper_id": 2210.15453,
        "authors": "Yuecai Han and Xudong Zheng",
        "title": "Approximate Pricing of Derivatives Under Fractional Stochastic\n  Volatility Model",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We investigate the problem of pricing derivatives under a fractional\nstochastic volatility model. We obtain an approximate expression of the\nderivative price where the stochastic volatility can be composed of\ndeterministic functions of time and fractional Ornstein-Uhlenbeck process.\nNumerical simulations are given to illustrate the feasibility and operability\nof the approximation, and also demonstrate the effect of long-range on\nderivative prices.\n"
    },
    {
        "paper_id": 2210.15493,
        "authors": "Wesley Joon-Wie Tann, Akhil Vuputuri, Ee-Chien Chang",
        "title": "Projecting Non-Fungible Token (NFT) Collections: A Contextual Generative\n  Approach",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  Non-fungible tokens (NFTs) are digital assets stored on a blockchain\nrepresenting real-world objects such as art or collectibles. An NFT collection\ncomprises numerous tokens; each token can be transacted multiple times. It is a\nmultibillion-dollar market where the number of collections has more than\ndoubled in 2022. In this paper, we want to obtain a generative model that,\ngiven the early transactions history (first quarter Q1) of a newly minted\ncollection, generates subsequent transactions (quarters Q2, Q3, Q4), where the\ngenerative model is trained using the transaction history of a few mature\ncollections. The goal is to use the generated transactions to project the\npotential market value of this newly minted collection over the next few\nquarters. A technical challenge exists in that different collections have\ndiverse characteristics, and the generative model should generate based on the\nappropriate \"contexts\" of the collection. Our method takes a two-step approach.\nFirst, it employs unsupervised learning on the early transactions to extract\ncharacteristics (which we call contexts) of NFT collections. Next, it generates\nfuture transactions of each token based on these contexts and the early\ntransactions, projecting the target collection's potential market value.\nComprehensive experiments demonstrate our contextual generative approach's NFT\nprojection capabilities.\n"
    },
    {
        "paper_id": 2210.15499,
        "authors": "Ali Hirsa and Massoud Heidari",
        "title": "Post trade allocation: how much are bunched orders costing your\n  performance?",
        "comments": "16 pages, 2 figures, 12 tables",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Individual trade orders are often bunched into a block order for processing\nefficiency, where in post execution, they are allocated into individual\naccounts. Since Regulators have not mandated any specific post trade allocation\npractice or methodology, entities try to rigorously follow internal policies\nand procedures to meet the minimum Regulatory ask of being procedurally fair\nand equitable. However, as many have found over the years, there is no simple\nsolution for post trade allocation between accounts that results in a uniform\ndistribution of returns. Furthermore, in many instances, the divergences\nbetween returns do not dissipate with more transactions, and tend to increase\nin some cases. This paper is the first systematic treatment of trade allocation\nrisk. We shed light on the reasons for return divergence among accounts, and we\npresent a solution that supports uniform allocation of return irrespective of\nnumber of accounts and trade sizes.\n"
    },
    {
        "paper_id": 2210.15613,
        "authors": "Ale\\v{s} \\v{C}ern\\'y and Christoph Czichowsky",
        "title": "The law of one price in quadratic hedging and mean-variance portfolio\n  selection",
        "comments": "37 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The \\emph{law of one price (LOP)} broadly asserts that identical financial\nflows should command the same price. We show that, when properly formulated,\nLOP is the minimal condition for a well-defined mean--variance portfolio\nallocation framework without degeneracy. Crucially, the paper identifies a new\nmechanism through which LOP can fail in a continuous-time $L^2(P)$ setting\nwithout frictions, namely `trading from just before a predictable stopping\ntime', which surprisingly identifies LOP violations even for continuous price\nprocesses. Closing this loophole allows to give a version of the ``Fundamental\nTheorem of Asset Pricing'' appropriate in the quadratic context, establishing\nthe equivalence of the economic concept of LOP with the probabilistic property\nof the existence of a local $\\scr{E}$-martingale state price density. The\nlatter provides unique prices for all square-integrable contingent claims in an\nextended market and subsequently plays an important role in mean-variance\nportfolio selection and quadratic hedging.\n"
    },
    {
        "paper_id": 2210.15776,
        "authors": "Felipe Lobel",
        "title": "The Unequal Incidence of Payroll Taxes with Imperfect Competition:\n  Theory and Evidence",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
        "abstract": "  This paper provides a comprehensive examination of a Brazilian corporate tax\nreform targeted at the sector and product level. Difference-in-differences\nestimates instrumented by sector eligibility show that a 20 percentage point\ncut on payroll tax rates caused a 9% employment increase at the firm level,\nmostly driven by small firms. This expansion is not driven by formalization of\nexisting workers, and it is explained by reduction on separations rather than\nadditional hires. In terms of earnings, there is a significant 4% earnings\nincrease in the long run, which is concentrated at leadership positions. The\nunequal pass-through worsen within-firm wage inequality. I exploit the\nexogenous variation on labor cost to document substantial labor market power in\nBrazil, where wages are marked down by 36%. Consistent with the empirical\nfindings, I develop a model of factor demand with imperfect competition in the\ngoods and labor market to shed light on the mechanism through which imperfect\ncompetition drives corporate tax incidence. The model is identified by the\nreduced form elasticities, and allows me to structurally estimate the\ncapital-labor elasticity of substitution, which differs from the benchmark case\nof perfect competition.\n"
    },
    {
        "paper_id": 2210.15785,
        "authors": "Kevin Hu (1), Retsef Levi (1), Raphael Yahalom (1), El Ghali Zerhouni\n  (1) ((1) Massachusetts Institute of Technology)",
        "title": "Supply Chain Characteristics as Predictors of Cyber Risk: A\n  Machine-Learning Assessment",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  This paper provides the first large-scale data-driven analysis to evaluate\nthe predictive power of different attributes for assessing risk of cyberattack\ndata breaches. Furthermore, motivated by rapid increase in third party enabled\ncyberattacks, the paper provides the first quantitative empirical evidence that\ndigital supply-chain attributes are significant predictors of enterprise cyber\nrisk. The paper leverages outside-in cyber risk scores that aim to capture the\nquality of the enterprise internal cybersecurity management, but augment these\nwith supply chain features that are inspired by observed third party\ncyberattack scenarios, as well as concepts from network science research. The\nmain quantitative result of the paper is to show that supply chain network\nfeatures add significant detection power to predicting enterprise cyber risk,\nrelative to merely using enterprise-only attributes. Particularly, compared to\na base model that relies only on internal enterprise features, the supply chain\nnetwork features improve the out-of-sample AUC by 2.3\\%. Given that each cyber\ndata breach is a low probability high impact risk event, these improvements in\nthe prediction power have significant value. Additionally, the model highlights\nseveral cybersecurity risk drivers related to third party cyberattack and\nbreach mechanisms and provides important insights as to what interventions\nmight be effective to mitigate these risks.\n"
    },
    {
        "paper_id": 2210.15914,
        "authors": "Philipp Koch, Viktor Stojkoski, C\\'esar A. Hidalgo",
        "title": "The Role of Immigrants, Emigrants, and Locals in the Historical\n  Formation of European Knowledge Agglomerations",
        "comments": null,
        "journal-ref": null,
        "doi": "10.1080/00343404.2023.2275571",
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Did migrants make Paris a Mecca for the arts and Vienna a beacon of classical\nmusic? Or was their rise a pure consequence of local actors? Here, we use data\non more than 22,000 historical individuals born between the years 1000 and 2000\nto estimate the contribution of famous immigrants, emigrants, and locals to the\nknowledge specializations of European regions. We find that the probability\nthat a region develops or keeps specialization in an activity (based on the\nbirth of famous physicists, painters, etc.) grows with both, the presence of\nimmigrants with knowledge on that activity and immigrants with knowledge in\nrelated activities. In contrast, we do not find robust evidence that the\npresence of locals with related knowledge explains entries and/or exits. We\naddress some endogeneity concerns using fixed-effects models considering any\nlocation-period-activity specific factors (e.g. the presence of a new\nuniversity attracting scientists).\n"
    },
    {
        "paper_id": 2210.15925,
        "authors": "Qiang Gao, Xinzhu Zhou, Kunpeng Zhang, Li Huang, Siyuan Liu, Fan Zhou",
        "title": "Incorporating Interactive Facts for Stock Selection via Neural Recursive\n  ODEs",
        "comments": "14 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Stock selection attempts to rank a list of stocks for optimizing investment\ndecision making, aiming at minimizing investment risks while maximizing profit\nreturns. Recently, researchers have developed various (recurrent) neural\nnetwork-based methods to tackle this problem. Without exceptions, they\nprimarily leverage historical market volatility to enhance the selection\nperformance. However, these approaches greatly rely on discrete sampled market\nobservations, which either fail to consider the uncertainty of stock\nfluctuations or predict continuous stock dynamics in the future. Besides, some\nstudies have considered the explicit stock interdependence derived from\nmultiple domains (e.g., industry and shareholder). Nevertheless, the implicit\ncross-dependencies among different domains are under-explored. To address such\nlimitations, we present a novel stock selection solution -- StockODE, a latent\nvariable model with Gaussian prior. Specifically, we devise a Movement Trend\nCorrelation module to expose the time-varying relationships regarding stock\nmovements. We design Neural Recursive Ordinary Differential Equation Networks\n(NRODEs) to capture the temporal evolution of stock volatility in a continuous\ndynamic manner. Moreover, we build a hierarchical hypergraph to incorporate the\ndomain-aware dependencies among the stocks. Experiments conducted on two\nreal-world stock market datasets demonstrate that StockODE significantly\noutperforms several baselines, such as up to 18.57% average improvement\nregarding Sharpe Ratio.\n"
    },
    {
        "paper_id": 2210.15934,
        "authors": "Ioana Boier",
        "title": "Multiresolution Signal Processing of Financial Market Objects",
        "comments": "7 pages, 12 figures. Copy at https://ioanaboier.com may be updated\n  more frequently than arXiv copy",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Multiresolution analysis has applications across many disciplines in the\nstudy of complex systems and their dynamics. Financial markets are among the\nmost complex entities in our environment, yet mainstream quantitative models\noperate at predetermined scale, rely on linear correlation measures, and\nstruggle to recognize non-linear or causal structures. In this paper, we\ncombine neural networks known to capture non-linear associations with a\nmultiscale decomposition to facilitate a better understanding of financial\nmarket data substructures. Quantization keeps our decompositions calibrated to\nmarket at every scale. We illustrate our approach in the context of seven use\ncases.\n"
    },
    {
        "paper_id": 2210.15946,
        "authors": "Ada Gonzalez-Torres",
        "title": "Local Media and the Shaping of Social Norms: Evidence from the Ebola\n  outbreak",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Media around the world is disseminated at the national level as well as at\nthe local level. While the capacity of media to shape preferences and behavior\nhas been widely recognized, less is known about the differential impacts of\nlocal media. Local media may have particularly important effects on social\nnorms due to the provision of locally relevant information that becomes common\nknowledge in a community. I examine this possibility in a high-stakes context:\nthe Ebola epidemic in Guinea. I exploit quasi-random variation in access to\ndistinct media outlets and the timing of a public-health campaign on community\nradio. I find that 13% of Ebola cases would have been prevented if places with\naccess to neighboring community radio stations had instead their own. This is\ndriven by radio stations' locality, not ethno-linguistic boundaries, and by\ncoordination in social behaviors sanctioned locally.\n"
    },
    {
        "paper_id": 2210.15969,
        "authors": "Geon Lee, Tae-Kyoung Kim, Hyun-Gyoon Kim, Jeonggyu Huh",
        "title": "Newton Raphson Emulation Network for Highly Efficient Computation of\n  Numerous Implied Volatilities",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In finance, implied volatility is an important indicator that reflects the\nmarket situation immediately. Many practitioners estimate volatility using\niteration methods, such as the Newton--Raphson (NR) method. However, if\nnumerous implied volatilities must be computed frequently, the iteration\nmethods easily reach the processing speed limit. Therefore, we emulate the NR\nmethod as a network using PyTorch, a well-known deep learning package, and\noptimize the network further using TensorRT, a package for optimizing deep\nlearning models. Comparing the optimized emulation method with the NR function\nin SciPy, a popular implementation of the NR method, we demonstrate that the\nemulation network is up to 1,000 times faster than the benchmark function.\n"
    },
    {
        "paper_id": 2210.16042,
        "authors": "Alain-Philippe Fortin, Patrick Gagliardini, Olivier Scaillet",
        "title": "Eigenvalue tests for the number of latent factors in short panels",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  This paper studies new tests for the number of latent factors in a large\ncross-sectional factor model with small time dimension. These tests are based\non the eigenvalues of variance-covariance matrices of (possibly weighted) asset\nreturns, and rely on either the assumption of spherical errors, or instrumental\nvariables for factor betas. We establish the asymptotic distributional results\nusing expansion theorems based on perturbation theory for symmetric matrices.\nOur framework accommodates semi-strong factors in the systematic components. We\npropose a novel statistical test for weak factors against strong or semi-strong\nfactors. We provide an empirical application to US equity data. Evidence for a\ndifferent number of latent factors according to market downturns and market\nupturns, is statistically ambiguous in the considered subperiods. In\nparticular, our results contradicts the common wisdom of a single factor model\nin bear markets.\n"
    },
    {
        "paper_id": 2210.16113,
        "authors": "Kazuo Sano",
        "title": "Intelligence and Global Bias in the Stock Market",
        "comments": "8 pages, 4 figures",
        "journal-ref": null,
        "doi": "10.3934/DSFE.2023011",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Trade is one of the essential feature of human intelligence. The securities\nmarket is the ultimate expression of it. The fundamental indicators of stocks\ninclude information as well as the effects of noise and bias on the stock\nprices; however, identifying the effects of noise and bias is generally\ndifficult. In this article, I present the true fundamentals hypothesis based on\nrational expectations and detect the global bias components from the actual\nfundamental indicators by using a log-normal distribution model based on the\ntrue fundamentals hypothesis. The analysis results show that biases generally\nexhibit the same characteristics, strongly supporting the true fundamentals\nhypothesis. Notably, the positive price-to-cash flows from the investing\nactivities ratio is a proxy for the true fundamentals. Where do these biases\ncome from? The answer is extremely simple: ``Cash is a fact, profit is an\nopinion.'' Namely, opinions of management and accounting are added to true\nfundamentals. As a result, Kesten process is realized and the Pareto\ndistribution is to be obtained. This means that the market knows it and\nrepresents as a stable global bias in the stock market.\n"
    },
    {
        "paper_id": 2210.16155,
        "authors": "Scott W Hegerty",
        "title": "\"Rust Belt\" Across America: An Application of a Nationwide,\n  Block-Group-Level Deprivation Index",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In the United States, large post-industrial cites such as Detroit are\nwell-known for high levels of socioeconomic deprivation. But while Detroit is\nan exceptional case, similar levels of deprivation can still be found in other\nlarge cities, as well as in smaller towns and rural areas. This study\ncalculates a standardized measure for all block groups in the lower 48 states\nand DC, before isolating \"high-deprivation\" areas that exceed Detroit's median\nvalue. These block groups are investigated and mapped for the 83 cities with\npopulations above 250,000, as well as at the state level and for places of all\nsizes. Detroit is shown to indeed be unique not only for its levels of\ndeprivation (which are higher than 95 percent of the country), but also for the\ndispersion of highly-deprived block groups throughout the city. Smaller, more\nconcentrated pockets of high deprivation can be found in nearly every large\ncity, and some cities below 20,000 residents have an even larger share of\nhigh-deprivation areas. Cities' percentages of high-deprivation areas are\npositively related to overall poverty, population density, and the percentage\nof White residents, and negatively related to the share of Black residents.\n"
    },
    {
        "paper_id": 2210.16408,
        "authors": "Rafael P. Greminger",
        "title": "Heterogeneous Position Effects and the Power of Rankings",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Many online retailers and search intermediaries present products on ranked\nproduct lists. Because changing the ordering of products influences their\nrevenues and consumer welfare, these platforms can design rankings to maximize\neither of these two metrics. In this paper, I study how rankings serving these\ntwo objectives differ and what determines this difference. First, I highlight\nthat when position effects are heterogeneous, rankings can increase both\nrevenues and consumer welfare by increasing the overall conversion rate.\nSecond, I provide empirical evidence for this heterogeneity, highlighting that\ncheaper alternatives have stronger position effects. Finally, I quantify\nrevenue and consumer welfare effects across the different rankings. To this\nend, I develop an estimation procedure for the search and discovery model of\nGreminger (2022) that yields a smooth likelihood function by construction. The\nresults from the model show that rankings designed to maximize revenues can\nalso increase consumer welfare. Moreover, these revenue-based rankings reduce\nconsumer welfare only to a limited extent relative to a\nconsumer-welfare-maximizing ranking.\n"
    },
    {
        "paper_id": 2210.16548,
        "authors": "J. Hok and S. Kucherenko",
        "title": "The importance of being scrambled: supercharged Quasi Monte Carlo",
        "comments": "arXiv admin note: text overlap with arXiv:2106.08421",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In many financial applications Quasi Monte Carlo (QMC) based on Sobol\nlow-discrepancy sequences (LDS) outperforms Monte Carlo showing faster and more\nstable convergence. However, unlike MC QMC lacks a practical error estimate.\nRandomized QMC (RQMC) method combines the best of two methods. Application of\nscrambled LDS allow to compute confidence intervals around the estimated value,\nproviding a practical error bound. Randomization of Sobol' LDS by two methods:\nOwen's scrambling and digital shift are compared considering computation of\nAsian options and Greeks using hyperbolic local volatility model. RQMC\ndemonstrated the superior performance over standard QMC showing increased\nconvergence rates and providing practical error bounds.\n"
    },
    {
        "paper_id": 2210.16679,
        "authors": "Elena Farahbakhsh Touli, Hoang Nguyen and Olha Bodnar",
        "title": "Monitoring the Dynamic Networks of Stock Returns",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper, we study the connection between the companies in the Swedish\ncapital market. We consider 28 companies included in the determination of the\nmarket index OMX30. The network structure of the market is constructed using\ndifferent methods to determine the distance between the companies. We use\nhierarchical clustering methods to find the relation among the companies in\neach window. Next, we obtain one-dimensional time series of the distances\nbetween the clustering trees that reflect the changes in the relationship\nbetween the companies in the market over time. The method of statistical\nprocess control, namely the Shewhart control chart, is applied to those time\nseries to detect abnormal changes in the financial market.\n"
    },
    {
        "paper_id": 2210.16846,
        "authors": "Teng Andrea Xu, Jiahua Xu, Kristof Lommers",
        "title": "DeFi vs TradFi: Valuation Using Multiples and Discounted Cash Flow",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  As of August 2022, blockchain-based assets boast a combined market\ncapitalisation exceeding one trillion USD, among which the most prominent are\nthe decentralised autonomous organisation (DAO) tokens associated with\ndecentralised finance (DeFi) protocols. In this work, we seek to value DeFi\ntokens using the canonical multiples and Discount Cash Flow (DCF) approaches.\nWe examine a subset of DeFi services including decentralised exchanges (DEXs),\nprotocol for loanable funds (PLFs), and yield aggregators. We apply the same\nanalysis to some publicly traded firms and compare them with DeFi tokens of the\nanalogous category. Interestingly, despite the crypto bear market lasting for\nmore than one year as of August 2022, both approaches evidence overvaluation in\nDeFi.\n"
    },
    {
        "paper_id": 2210.16863,
        "authors": "Chengxiang Jin, Jiajun Zhou, Jie Jin, Jiajing Wu, Qi Xuan",
        "title": "Time-aware Metapath Feature Augmentation for Ponzi Detection in Ethereum",
        "comments": "Accepted by IEEE Transactions on Network Science and Engineering",
        "journal-ref": null,
        "doi": "10.1109/TNSE.2024.3384499",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  With the development of Web 3.0 which emphasizes decentralization, blockchain\ntechnology ushers in its revolution and also brings numerous challenges,\nparticularly in the field of cryptocurrency. Recently, a large number of\ncriminal behaviors continuously emerge on blockchain, such as Ponzi schemes and\nphishing scams, which severely endanger decentralized finance. Existing\ngraph-based abnormal behavior detection methods on blockchain usually focus on\nconstructing homogeneous transaction graphs without distinguishing the\nheterogeneity of nodes and edges, resulting in partial loss of transaction\npattern information. Although existing heterogeneous modeling methods can\ndepict richer information through metapaths, the extracted metapaths generally\nneglect temporal dependencies between entities and do not reflect real\nbehavior. In this paper, we introduce Time-aware Metapath Feature Augmentation\n(TMFAug) as a plug-and-play module to capture the real metapath-based\ntransaction patterns during Ponzi scheme detection on Ethereum. The proposed\nmodule can be adaptively combined with existing graph-based Ponzi detection\nmethods. Extensive experimental results show that our TMFAug can help existing\nPonzi detection methods achieve significant performance improvements on the\nEthereum dataset, indicating the effectiveness of heterogeneous temporal\ninformation for Ponzi scheme detection.\n"
    },
    {
        "paper_id": 2210.1688,
        "authors": "Yunran Wei, Ricardas Zitikis",
        "title": "Assessing the difference between integrated quantiles and integrated\n  cumulative distribution functions",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper offers a mathematical invention that shows how to convert\nintegrated quantiles, which often appear in risk measures, into integrated\ncumulative distribution functions, which are technically more tractable from\nvarious perspectives. The invention helps to avoid a number of technical\nassumptions that have been traditionally imposed when working with quantities\ncontaining quantiles. In particular it helps to completely avoid the\nrequirement of the existence of a probability density function. The developed\nresults explain and illustrate the invention, whose byproducts include the\nassessment of model uncertainty and misspecification, and the derivation of\nstatistical inference results.\n"
    },
    {
        "paper_id": 2210.16899,
        "authors": "Jason Chen, Kathy Fogel, and Kose John",
        "title": "Understanding the Maker Protocol",
        "comments": "7 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  This paper discusses a decentralized finance (DeFi) application called\nMakerDAO. The Maker Protocol, built on the Ethereum blockchain, enables users\nto create and hold currency. Current elements of the Maker Protocol are the Dai\nstable coin, Maker Vaults, and Voting. MakerDAO governs the Maker Protocol by\ndeciding on key parameters (e.g., stability fees, collateral types and rates,\netc.) through the voting power of Maker (MKR) holders. The Maker Protocol is\none of the largest decentralized applications (DApps) on the Ethereum\nblockchain and is the first decentralized finance (DeFi) application to earn\nsignificant adoption. The objective of this paper is to analyze and discuss the\nsignificance, uses, and functions of this DeFi application.\n"
    },
    {
        "paper_id": 2210.16905,
        "authors": "Sherry Hu, Kose John, and Balbinder Singh Gill",
        "title": "Stock price reaction to power outages following extreme weather events:\n  Evidence from Texas power outage",
        "comments": "34 pages, 4 figures, and 2 tables",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this study, we evaluate the effects of natural disasters on the stock\n(market) values of firms located in the affected counties. We are able to\nmeasure the change in stock prices of the firms affected by the 2021 Texas\nwinter storm. To measure the abnormal return due to the storm, we use four\ndifferent benchmark models: (1) the market-adjusted model, (2) the market\nmodel, (3) the Fama-French three-factor model, and (4) the Fama French plus\nmomentum model. These statistical models in finance characterize the normal\nrisk-return trade-off.\n"
    },
    {
        "paper_id": 2210.1703,
        "authors": "Yugo Fujimoto, Kei Nakagawa, Kentaro Imajo, Kentaro Minami",
        "title": "Uncertainty Aware Trader-Company Method: Interpretable Stock Price\n  Prediction Capturing Uncertainty",
        "comments": "IEEE BIGDATA 2022 Accepted",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Machine learning is an increasingly popular tool with some success in\npredicting stock prices. One promising method is the Trader-Company~(TC)\nmethod, which takes into account the dynamism of the stock market and has both\nhigh predictive power and interpretability. Machine learning-based stock\nprediction methods including the TC method have been concentrating on point\nprediction. However, point prediction in the absence of uncertainty estimates\nlacks credibility quantification and raises concerns about safety. The\nchallenge in this paper is to make an investment strategy that combines high\npredictive power and the ability to quantify uncertainty. We propose a novel\napproach called Uncertainty Aware Trader-Company Method~(UTC) method. The core\nidea of this approach is to combine the strengths of both frameworks by merging\nthe TC method with the probabilistic modeling, which provides probabilistic\npredictions and uncertainty estimations. We expect this to retain the\npredictive power and interpretability of the TC method while capturing the\nuncertainty. We theoretically prove that the proposed method estimates the\nposterior variance and does not introduce additional biases from the original\nTC method. We conduct a comprehensive evaluation of our approach based on the\nsynthetic and real market datasets. We confirm with synthetic data that the UTC\nmethod can detect situations where the uncertainty increases and the prediction\nis difficult. We also confirmed that the UTC method can detect abrupt changes\nin data generating distributions. We demonstrate with real market data that the\nUTC method can achieve higher returns and lower risks than baselines.\n"
    },
    {
        "paper_id": 2210.17208,
        "authors": "Ryan Donnelly, Zi Li",
        "title": "Dynamic Inventory Management with Mean-Field Competition",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Agents attempt to maximize expected profits earned by selling multiple units\nof a perishable product where their revenue streams are affected by the prices\nthey quote as well as the distribution of other prices quoted in the market by\nother agents. We propose a model which captures this competitive effect and\ndirectly analyze the model in the mean-field limit as the number of agents is\nvery large. We classify mean-field Nash equilibrium in terms of the solution to\na Hamilton-Jacobi-Bellman equation and a consistency condition and use this to\nmotivate an iterative numerical algorithm to compute equilibrium. Properties of\nthe equilibrium pricing strategies and overall market dynamics are then\ninvestigated, in particular how they depend on the strength of the competitive\ninteraction and the ability to oversell the product.\n"
    },
    {
        "paper_id": 2210.17384,
        "authors": "Reda Chhaibi, Ibrahim Ekren, Eunjung Noh, Lu Vy",
        "title": "A unified approach to informed trading via Monge-Kantorovich duality",
        "comments": "v1",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We solve a generalized Kyle model type problem using Monge-Kantorovich\nduality and backward stochastic partial differential equations.\n  First, we show that the the generalized Kyle model with dynamic information\ncan be recast into a terminal optimization problem with distributional\nconstraints. Therefore, the theory of optimal transport between spaces of\nunequal dimension comes as a natural tool.\n  Second, the pricing rule of the market maker and an optimality criterion for\nthe problem of the informed trader are established using the Kantorovich\npotentials and transport maps.\n  Finally, we completely characterize the optimal strategies by analyzing the\nfiltering problem from the market maker's point of view. In this context, the\nKushner-Zakai filtering SPDE yields to an interesting backward stochastic\npartial differential equation whose measure-valued terminal condition comes\nfrom the optimal coupling of measures.\n"
    },
    {
        "paper_id": 2211.00131,
        "authors": "Kazuo Sano",
        "title": "New Concept for the Value Function of Prospect Theory",
        "comments": "15 pages, 3 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In the prospect theory, value function is typically concave for gains,\ncommonly convex for losses, with losses usually having a steeper slope than\ngains. The neural system largely differs from the loss and gains sides. Five\nnew studies on neurons related to this issue have examined neuronal responses\nto losses, gains, and reference points. This study investigates a new concept\nof the value function. A value function with a neuronal cusp may show\nvariations and behavior cusps with catastrophe where a trader closes one's\nposition.\n"
    },
    {
        "paper_id": 2211.00291,
        "authors": "Zae Young Kim and Jeong-Hyuck Park",
        "title": "Distinguishable Cash, Bosonic Bitcoin, and Fermionic Non-fungible Token",
        "comments": "8 pages including 3 figures; v2) Eqns (46) and (47) added, minor\n  changes, version to appear in Frontiers in Physics",
        "journal-ref": "Front. Phys. 11:1113714 (2023)",
        "doi": "10.3389/fphy.2023.1113714",
        "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
        "abstract": "  Modern technology has brought novel types of wealth. In contrast to hard\ncashes, digital currencies do not have a physical form. They exist in\nelectronic forms only. Yet, it has not been clear what impacts their ongoing\ngrowth will make, if any, on wealth distribution. Here we propose to identify\nall forms of contemporary wealth into two classes: 'distinguishable' or\n'identical'. Traditional tangible moneys are all distinguishable. Financial\nassets and cryptocurrencies, such as bank deposits and Bitcoin, are boson-like,\nwhile non-fungible tokens are fermion-like. We derive their ownership-based\ndistributions in a unified manner. Each class follows essentially the Poisson\nor the geometric distribution. We contrast their distinct features such as Gini\ncoefficients. Further, aggregating different kinds of wealth corresponds to a\nweighted convolution where the number of banks matters and Bitcoin follows\nBose-Einstein distribution. Our proposal opens a new avenue to understand the\ndeepened inequality in modern economy, which is based on the statistical\nphysics property of wealth rather than the individual ability of owners. We\ncall for verifications with real data.\n"
    },
    {
        "paper_id": 2211.00326,
        "authors": "Kevin Kamm and Michelle Muniz",
        "title": "Rating Triggers for Collateral-Inclusive XVA via Machine Learning and\n  SDEs on Lie Groups",
        "comments": "arXiv admin note: text overlap with arXiv:2207.03883",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  In this paper, we model the rating process of an entity by using a\ngeometrical approach. We model rating transitions as an SDE on a Lie group.\nSpecifically, we focus on calibrating the model to both historical data (rating\ntransition matrices) and market data (CDS quotes) and compare the most popular\nchoices of changes of measure to switch from the historical probability to the\nrisk-neutral one. For this, we show how the classical Girsanov theorem can be\napplied in the Lie group setting. Moreover, we overcome some of the\nimperfections of rating matrices published by rating agencies, which are\ncomputed with the cohort method, by using a novel Deep Learning approach. This\nleads to an improvement of the entire scheme and makes the model more robust\nfor applications. We apply our model to compute bilateral credit and debit\nvaluation adjustments of a netting set under a CSA with thresholds depending on\nratings of the two parties.\n"
    },
    {
        "paper_id": 2211.0042,
        "authors": "Eranda \\c{C}ela, Stephan Hafner, Roland Mestel and Ulrich Pferschy",
        "title": "Integrating multiple sources of ordinal information in portfolio\n  optimization",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  Active portfolio management tries to incorporate any source of meaningful\ninformation into the asset selection process. In this contribution we consider\nqualitative views specified as total orders of the expected asset returns and\ndiscuss two different approaches for incorporating this input in a\nmean-variance portfolio optimization model. In the robust optimization approach\nwe first compute a posterior expectation of asset returns for every given total\norder by an extension of the Black-Litterman (BL) framework. Then these\nexpected asset returns are considered as possible input scenarios for robust\noptimization variants of the mean-variance portfolio model (max-min robustness,\nmin regret robustness and soft robustness). In the order aggregation approach\nrules from social choice theory (Borda, Footrule, Copeland, Best-of-k and MC4)\nare used to aggregate the total order in a single ``consensus total order''.\nThen expected asset returns are computed for this ``consensus total order'' by\nthe extended BL framework mentioned above. Finally, these expectations are used\nas an input of the classical mean-variance optimization. Using data from\nEUROSTOXX 50 and S&P 100 we empirically compare the success of the two\napproaches in the context of portfolio performance analysis and observe that in\ngeneral aggregating orders by social choice methods outperforms robust\noptimization based methods for both data sets.\n"
    },
    {
        "paper_id": 2211.00447,
        "authors": "Eduardo Abi Jaber and Eyal Neuman",
        "title": "Optimal Liquidation with Signals: the General Propagator Case",
        "comments": "48 pages, 5 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We consider a class of optimal liquidation problems where the agent's\ntransactions create transient price impact driven by a Volterra-type propagator\nalong with temporary price impact. We formulate these problems as minimization\nof a revenue-risk functionals, where the agent also exploits available\ninformation on a progressively measurable price predicting signal. By using an\ninfinite dimensional stochastic control approach, we characterize the value\nfunction in terms of a solution to a free-boundary $L^2$-valued backward\nstochastic differential equation and an operator-valued Riccati equation. We\nthen derive analytic solutions to these equations which yields an explicit\nexpression for the optimal trading strategy. We show that our formulas can be\nimplemented in a straightforward and efficient way for a large class of price\nimpact kernels with possible singularities such as the power-law kernel.\n"
    },
    {
        "paper_id": 2211.00496,
        "authors": "Bingyan Han",
        "title": "Can maker-taker fees prevent algorithmic cooperation in market making?",
        "comments": null,
        "journal-ref": "3rd ACM International Conference on AI in Finance (ICAIF'22),\n  November 2--4, 2022, New York, NY, USA",
        "doi": "10.1145/3533271.3561685",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In a semi-realistic market simulator, independent reinforcement learning\nalgorithms may facilitate market makers to maintain wide spreads even without\ncommunication. This unexpected outcome challenges the current antitrust law\nframework. We study the effectiveness of maker-taker fee models in preventing\ncooperation via algorithms. After modeling market making as a repeated\ngeneral-sum game, we experimentally show that the relation between net\ntransaction costs and maker rebates is not necessarily monotone. Besides an\nupper bound on taker fees, we may also need a lower bound on maker rebates to\ndestabilize the cooperation. We also consider the taker-maker model and the\neffects of mid-price volatility, inventory risk, and the number of agents.\n"
    },
    {
        "paper_id": 2211.00528,
        "authors": "Suchetana Sadhukhan, Shiv Manjaree Gopaliya, Pushpdant Jain",
        "title": "A novel approach to quantify volatility prediction",
        "comments": "15 pages, 5 figures, 2 tables",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Volatility prediction in the financial market helps to understand the profit\nand involved risks in investment. However, due to irregularities, high\nfluctuations, and noise in the time series, predicting volatility poses a\nchallenging task. In the recent Covid-19 pandemic situation, volatility\nprediction using complex intelligence techniques has attracted enormous\nattention from researchers worldwide. In this paper, a novel and simple\napproach based on the robust least squares method in two approaches a) with\nleast absolute residuals (LAR) and b) without LAR, have been applied to the\nChicago Board Options Exchange (CBOE) Volatility Index (VIX) for a period of\nten years. For a deeper analysis, the volatility time series has been\ndecomposed into long-term trends, and seasonal, and random fluctuations. The\ndata sets have been divided into parts viz. training data set and testing data\nset. The validation results have been achieved using root mean square error\n(RMSE) values. It has been found that robust least squares method with LAR\napproach gives better results for volatility (RMSE = 0.01366) and its\ncomponents viz. long term trend (RMSE = 0.10087), seasonal (RMSE = 0.010343)\nand remainder fluctuations (RMSE = 0.014783), respectively. For the first time,\ngeneralized prediction equations for volatility and its three components have\nbeen presented. Young researchers working in this domain can directly use the\npresented prediction equations to understand their data sets.\n"
    },
    {
        "paper_id": 2211.00532,
        "authors": "Christoph Czichowsky, Raphael Huwyler",
        "title": "Robust utility maximisation under proportional transaction costs for\n  c\\`adl\\`ag price processes",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  We consider robust utility maximisation in continuous-time financial markets\nwith proportional transaction costs under model uncertainty. For this purpose,\nwe work in the framework of Chau and R\\'asonyi (2019), where robustness is\nachieved by maximising the worst-case expected utility over a possibly\nuncountable class of models that are all given on the same underlying filtered\nprobability space with incomplete filtration. In this setting, we give\nsufficient conditions for the existence of an optimal trading strategy\nextending the result for utility functions on the positive half-line of Chau\nand R\\'asonyi (2019) from continuous to general strictly positive c\\`adl\\`ag\nprice processes. This result allows us to provide a positive answer to an open\nquestion pointed out in Chau and R\\'asonyi (2019), and shows that the embedding\ninto a countable product space is not essential.\n"
    },
    {
        "paper_id": 2211.00728,
        "authors": "Jaros{\\l}aw Kwapie\\'n, Pawel Blasiak, Stanis{\\l}aw Dro\\.zd\\.z,\n  Pawe{\\l} O\\'swi\\k{e}cimka",
        "title": "Genuine multifractality in time series is due to temporal correlations",
        "comments": null,
        "journal-ref": "Physical Review E 107, 034139 (2023)",
        "doi": "10.1103/PhysRevE.107.034139",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Based on the mathematical arguments formulated within the Multifractal\nDetrended Fluctuation Analysis (MFDFA) approach it is shown that in the\nuncorrelated time series from the Gaussian basin of attraction the effects\nresembling multifractality asymptotically disappear for positive moments when\nthe length of time series increases. A hint is given that this applies to the\nnegative moments as well and extends to the L\\'evy stable regime of\nfluctuations. The related effects are also illustrated and confirmed by\nnumerical simulations. This documents that the genuine multifractality in time\nseries may only result from the long-range temporal correlations and the fatter\ndistribution tails of fluctuations may broaden the width of singularity\nspectrum only when such correlations are present. The frequently asked question\nof what makes multifractality in time series - temporal correlations or broad\ndistribution tails - is thus ill posed. In the absence of correlations only the\nbifractal or monofractal cases are possible. The former corresponds to the\nL\\'evy stable regime of fluctuations while the latter to the ones belonging to\nthe Gaussian basin of attraction in the sense of the Central Limit Theorem.\n"
    },
    {
        "paper_id": 2211.00871,
        "authors": "Reza Bradrania and Davood Pirayesh Neghab",
        "title": "State-dependent Asset Allocation Using Neural Networks",
        "comments": null,
        "journal-ref": "European Journal of Finance. 28 (2021) 1130-1156",
        "doi": "10.1080/1351847X.2021.1960404",
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  Changes in market conditions present challenges for investors as they cause\nperformance to deviate from the ranges predicted by long-term averages of means\nand covariances. The aim of conditional asset allocation strategies is to\novercome this issue by adjusting portfolio allocations to hedge changes in the\ninvestment opportunity set. This paper proposes a new approach to conditional\nasset allocation that is based on machine learning; it analyzes historical\nmarket states and asset returns and identifies the optimal portfolio choice in\na new period when new observations become available. In this approach, we\ndirectly relate state variables to portfolio weights, rather than firstly\nmodeling the return distribution and subsequently estimating the portfolio\nchoice. The method captures nonlinearity among the state (predicting) variables\nand portfolio weights without assuming any particular distribution of returns\nand other data, without fitting a model with a fixed number of predicting\nvariables to data and without estimating any parameters. The empirical results\nfor a portfolio of stock and bond indices show the proposed approach generates\na more efficient outcome compared to traditional methods and is robust in using\ndifferent objective functions across different sample periods.\n"
    },
    {
        "paper_id": 2211.00921,
        "authors": "Wei Li, Wolfgang Karl H\\\"ardle, Stefan Lessmann",
        "title": "A Data-driven Case-based Reasoning in Bankruptcy Prediction",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
        "abstract": "  There has been intensive research regarding machine learning models for\npredicting bankruptcy in recent years. However, the lack of interpretability\nlimits their growth and practical implementation. This study proposes a\ndata-driven explainable case-based reasoning (CBR) system for bankruptcy\nprediction. Empirical results from a comparative study show that the proposed\napproach performs superior to existing, alternative CBR systems and is\ncompetitive with state-of-the-art machine learning models. We also demonstrate\nthat the asymmetrical feature similarity comparison mechanism in the proposed\nCBR system can effectively capture the asymmetrically distributed nature of\nfinancial attributes, such as a few companies controlling more cash than the\nmajority, hence improving both the accuracy and explainability of predictions.\nIn addition, we delicately examine the explainability of the CBR system in the\ndecision-making process of bankruptcy prediction. While much research suggests\na trade-off between improving prediction accuracy and explainability, our\nfindings show a prospective research avenue in which an explainable model that\nthoroughly incorporates data attributes by design can reconcile the dilemma.\n"
    },
    {
        "paper_id": 2211.00948,
        "authors": "Ruochen Xiao and Qiaochu Feng and Ruxin Deng",
        "title": "Inflexible Multi-Asset Hedging of incomplete market",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Models trained under assumptions in the complete market usually don't take\neffect in the incomplete market. This paper solves the hedging problem in\nincomplete market with three sources of incompleteness: risk factor,\nilliquidity, and discrete transaction dates. A new jump-diffusion model is\nproposed to describe stochastic asset prices. Three neutral networks, including\nRNN, LSTM, Mogrifier-LSTM are used to attain hedging strategies with MSE Loss\nand Huber Loss implemented and compared.As a result, Mogrifier-LSTM is the\nfastest model with the best results under MSE and Huber Loss.\n"
    },
    {
        "paper_id": 2211.01116,
        "authors": "Alex Hoagland, David M. Anderson, Ed Zhu",
        "title": "Medical Bill Shock and Imperfect Moral Hazard",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Consumers are sensitive to medical prices when consuming care, but delays in\nprice information may distort moral hazard. We study how medical bills affect\nhousehold spillover spending following utilization, leveraging variation in\ninsurer claim processing times. Households increase spending by 22\\% after a\nscheduled service, but then reduce spending by 11\\% after the bill arrives.\nObserved bill effects are consistent with resolving price uncertainty; bill\neffects are strongest when pricing information is particularly salient. A model\nof demand for healthcare with delayed pricing information suggests households\nmisperceive pricing signals prior to bills, and that correcting these\nperceptions reduce average (median) spending by 16\\% (7\\%) annually.\n"
    },
    {
        "paper_id": 2211.0124,
        "authors": "George Samartzis, Nikitas Pittis",
        "title": "On The Equivalence Of The Mean Variance Criterion And Stochastic\n  Dominance Criteria",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
        "abstract": "  We study the necessary and sufficient conditions under which the\nMean-Variance Criterion (MVC) is equivalent to the Maximum Expected Utility\nCriterion (MEUC), for two lotteries. Based on Chamberlain (1983), we conclude\nthat the MVC is equivalent to the Second-order Stochastic Dominance Rule (SSDR)\nunder any symmetric Elliptical distribution. We then discuss the work of\nSchuhmacher et al. (2021). Although their theoretical findings deduce that the\nMean-Variance Analysis remains valid under Skew-Elliptical distributions, we\nargue that this does not entail that the MVC coincides with the SSDR. In fact,\ngenerating multiple MV-pairs that follow a Skew-Normal distribution it becomes\nevident that the MVC fails to coincide with the SSDR for some types of\nrisk-averse investors. In the second part of this work, we examine the premise\nof Levy and Markowitz (1979) that \"the MVC deduces the maximization of the\nexpected utility of an investor, under any approximately quadratic utility\nfunction, without making any further assumption on the distribution of the\nlotteries\". Using Monte Carlo Simulations, we find out that the set of\napproximately quadratic utility functions is too narrow. Specifically, our\nsimulations indicate that $\\log{(a+Z)}$ and $(1+Z)^a$ are almost quadratic,\nwhile $-e^{-a(1+Z)}$ and $-(1+Z)^{-a}$ fail to approximate a quadratic utility\nfunction under either an Extreme Value or a Stable Pareto distribution.\n"
    },
    {
        "paper_id": 2211.01287,
        "authors": "Anubhav Sarkar, Swagata Chakraborty, Sohom Ghosh, Sudip Kumar Naskar",
        "title": "Evaluating Impact of Social Media Posts by Executives on Stock Prices",
        "comments": "Accepted at the 14th meeting of Forum for Information Retrieval\n  Evaluation (FIRE-2022)",
        "journal-ref": null,
        "doi": "10.1145/3574318.3574339",
        "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
        "abstract": "  Predicting stock market movements has always been of great interest to\ninvestors and an active area of research. Research has proven that popularity\nof products is highly influenced by what people talk about. Social media like\nTwitter, Reddit have become hotspots of such influences. This paper\ninvestigates the impact of social media posts on close price prediction of\nstocks using Twitter and Reddit posts. Our objective is to integrate sentiment\nof social media data with historical stock data and study its effect on closing\nprices using time series models. We carried out rigorous experiments and deep\nanalysis using multiple deep learning based models on different datasets to\nstudy the influence of posts by executives and general people on the close\nprice. Experimental results on multiple stocks (Apple and Tesla) and\ndecentralised currencies (Bitcoin and Ethereum) consistently show improvements\nin prediction on including social media data and greater improvements on\nincluding executive posts.\n"
    },
    {
        "paper_id": 2211.01346,
        "authors": "Tristan Lim",
        "title": "Predictive Crypto-Asset Automated Market Making Architecture for\n  Decentralized Finance using Deep Reinforcement Learning",
        "comments": "20 pages, 6 figures, 1 algorithm",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
        "abstract": "  The study proposes a quote-driven predictive automated market maker (AMM)\nplatform with on-chain custody and settlement functions, alongside off-chain\npredictive reinforcement learning capabilities to improve liquidity provision\nof real-world AMMs. The proposed AMM architecture is an augmentation to the\nUniswap V3, a cryptocurrency AMM protocol, by utilizing a novel market\nequilibrium pricing for reduced divergence and slippage loss. Further, the\nproposed architecture involves a predictive AMM capability, utilizing a deep\nhybrid Long Short-Term Memory (LSTM) and Q-learning reinforcement learning\nframework that looks to improve market efficiency through better forecasts of\nliquidity concentration ranges, so liquidity starts moving to expected\nconcentration ranges, prior to asset price movement, so that liquidity\nutilization is improved. The augmented protocol framework is expected have\npractical real-world implications, by (i) reducing divergence loss for\nliquidity providers, (ii) reducing slippage for crypto-asset traders, while\n(iii) improving capital efficiency for liquidity provision for the AMM\nprotocol. To our best knowledge, there are no known protocol or literature that\nare proposing similar deep learning-augmented AMM that achieves similar capital\nefficiency and loss minimization objectives for practical real-world\napplications.\n"
    },
    {
        "paper_id": 2211.01406,
        "authors": "Anders Christensen, Joel Ferguson, Sim\\'on Ram\\'irez Amaya",
        "title": "Incorporating High-Frequency Weather Data into Consumption Expenditure\n  Predictions",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Recent efforts have been very successful in accurately mapping welfare in\ndatasparse regions of the world using satellite imagery and other\nnon-traditional data sources. However, the literature to date has focused on\npredicting a particular class of welfare measures, asset indices, which are\nrelatively insensitive to short term fluctuations in well-being. We suggest\nthat predicting more volatile welfare measures, such as consumption\nexpenditure, substantially benefits from the incorporation of data sources with\nhigh temporal resolution. By incorporating daily weather data into training and\nprediction, we improve consumption prediction accuracy significantly compared\nto models that only utilize satellite imagery.\n"
    },
    {
        "paper_id": 2211.01762,
        "authors": "Ruibo Chen, Wei Li, Zhiyuan Zhang, Ruihan Bao, Keiko Harimoto, Xu Sun",
        "title": "Stock Trading Volume Prediction with Dual-Process Meta-Learning",
        "comments": "16 pages, 3 figures, 5 tables. Published in ECML-PKDD 2022",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Volume prediction is one of the fundamental objectives in the Fintech area,\nwhich is helpful for many downstream tasks, e.g., algorithmic trading. Previous\nmethods mostly learn a universal model for different stocks. However, this kind\nof practice omits the specific characteristics of individual stocks by applying\nthe same set of parameters for different stocks. On the other hand, learning\ndifferent models for each stock would face data sparsity or cold start problems\nfor many stocks with small capitalization. To take advantage of the data scale\nand the various characteristics of individual stocks, we propose a dual-process\nmeta-learning method that treats the prediction of each stock as one task under\nthe meta-learning framework. Our method can model the common pattern behind\ndifferent stocks with a meta-learner, while modeling the specific pattern for\neach stock across time spans with stock-dependent parameters. Furthermore, we\npropose to mine the pattern of each stock in the form of a latent variable\nwhich is then used for learning the parameters for the prediction module. This\nmakes the prediction procedure aware of the data pattern. Extensive experiments\non volume predictions show that our method can improve the performance of\nvarious baseline models. Further analyses testify the effectiveness of our\nproposed meta-learning framework.\n"
    },
    {
        "paper_id": 2211.01944,
        "authors": "Piyu Ke, Zhu Deng, Biqing Zhu, Bo Zheng, Yilong Wang, Olivier Boucher,\n  Simon Ben Arous, Chuanlong Zhou, Xinyu Dou, Taochun Sun, Zhao Li, Feifan Yan,\n  Duo Cui, Yifan Hu, Da Huo, Jean Pierre, Richard Engelen, Steven J. Davis,\n  Philippe Ciais, Zhu Liu",
        "title": "Carbon Monitor Europe, near-real-time daily CO$_2$ emissions for 27 EU\n  countries and the United Kingdom",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  With the urgent need to implement the EU countries pledges and to monitor the\neffectiveness of Green Deal plan, Monitoring Reporting and Verification tools\nare needed to track how emissions are changing for all the sectors. Current\nofficial inventories only provide annual estimates of national CO$_2$ emissions\nwith a lag of 1+ year which do not capture the variations of emissions due to\nrecent shocks including COVID lockdowns and economic rebounds, war in Ukraine.\nHere we present a near-real-time country-level dataset of daily fossil fuel and\ncement emissions from January 2019 through December 2021 for 27 EU countries\nand UK, which called Carbon Monitor Europe. The data are calculated separately\nfor six sectors: power, industry, ground transportation, domestic aviation,\ninternational aviation and residential. Daily CO$_2$ emissions are estimated\nfrom a large set of activity data compiled from different sources. The goal of\nthis dataset is to improve the timeliness and temporal resolution of emissions\nfor European countries, to inform the public and decision makers about current\nemissions changes in Europe.\n"
    },
    {
        "paper_id": 2211.01951,
        "authors": "Akshai Gaddam, Sravan Malla, Sandhya Dasari, Narayana Darapaneni,\n  Mukesh Kumar Shukla",
        "title": "Creating an Optimal Portfolio of Crops Using Price Forecasting to\n  Increase ROI for Indian Farmers",
        "comments": "14 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  The Indian agricultural sector being in a constant phase of upgradation, has\nbeen on the road to modernization for the last couple of years. The fundamental\nsource of livelihood for over 70 percent of the population living in rural\nparts of the country is still agriculture. The average Indian farmer, although\nhas access to raw and trend data pertaining to crop prices, yield and demand\nfrom Indian government and private websites, still struggles to make the right\nchoices. They are constantly faced with the dilemma of choosing the right crop\nto address market demand and fetch them a decent profit. Since the process of\nshortlisting crops amongst the many suitable ones isn't completely scientific\nand usually dictated by area tradition, this project has aimed to address that\nissue by forecasting the price of those crops and uses that to create an\noptimal portfolio that the farmers can obtain to arrive at a data-driven\ndecision for crop selection with optimal estimated ROI.\n"
    },
    {
        "paper_id": 2211.02196,
        "authors": "Christoph Graf and Federico Quaglia and Frank A. Wolak",
        "title": "(Machine) Learning from the COVID-19 Lockdown about Electricity Market\n  Performance with a Large Share of Renewables",
        "comments": null,
        "journal-ref": "Journal of Environmental Economics and Management Volume 105,\n  2021, 102398",
        "doi": "10.1016/j.jeem.2020.102398",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The negative demand shock due to the COVID-19 lockdown has reduced net demand\nfor electricity -- system demand less amount of energy produced by intermittent\nrenewables, hydroelectric units, and net imports -- that must be served by\ncontrollable generation units. Under normal demand conditions, introducing\nadditional renewable generation capacity reduces net demand. Consequently, the\nlockdown can provide insights about electricity market performance with a large\nshare of renewables. We find that although the lockdown reduced average\nday-ahead prices in Italy by 45%, re-dispatch costs increased by 73%, both\nrelative to the average of the same magnitude for the same period in previous\nyears. We estimate a deep-learning model using data from 2017--2019 and find\nthat predicted re-dispatch costs during the lockdown period are only 26% higher\nthan the same period in previous years. We argue that the difference between\nactual and predicted lockdown period re-dispatch costs is the result of\nincreased opportunities for suppliers with controllable units to exercise\nmarket power in the re-dispatch market in these persistently low net demand\nconditions. Our results imply that without grid investments and other\ntechnologies to manage low net demand conditions, an increased share of\nintermittent renewables is likely to increase costs of maintaining a reliable\ngrid.\n"
    },
    {
        "paper_id": 2211.02441,
        "authors": "Richard H. Day, Oleg V. Pavlov",
        "title": "Computing Economic Chaos",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Existence theory in economics is usually in real domains such as the findings\nof chaotic trajectories in models of economic growth, tatonnement, or\noverlapping generations models. Computational examples, however, sometimes\nconverge rapidly to cyclic orbits when in theory they should be nonperiodic\nalmost surely. We explain this anomaly as the result of digital approximation\nand conclude that both theoretical and numerical behavior can still illuminate\nessential features of the real data.\n"
    },
    {
        "paper_id": 2211.02528,
        "authors": "Aleksandar Mijatovi\\'c and Romain Palfray",
        "title": "A weak MLMC scheme for L\\'evy-copula-driven SDEs with applications to\n  the pricing of credit, equity and interest rate derivatives",
        "comments": "35 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper develops a novel weak multilevel Monte-Carlo (MLMC) approximation\nscheme for L\\'evy-driven Stochastic Differential Equations (SDEs). The scheme\nis based on the state space discretization (via a continuous-time Markov chain\napproximation) of the pure-jump component of the driving L\\'evy process and is\nparticularly suited if the multidimensional driver is given by a L\\'evy copula.\nThe multilevel version of the algorithm requires a new coupling of the\napproximate L\\'evy drivers in the consecutive levels of the scheme, which is\ndefined via a coupling of the corresponding Poisson point processes. The\nmultilevel scheme is weak in the sense that the bound on the level variances is\nbased on the coupling alone without requiring strong convergence. Moreover, the\ncoupling is natural for the proposed discretization of jumps and is easy to\nsimulate. The approximation scheme and its multilevel analogous are applied to\nexamples taken from mathematical finance, including the pricing of credit,\nequity and interest rate derivatives.\n"
    },
    {
        "paper_id": 2211.02742,
        "authors": "David Albrecht and Thomas Meissner",
        "title": "The debt aversion survey module: An experimentally validated tool to\n  measure individual debt aversion",
        "comments": "13 pages, 1 table, 2 figures. arXiv admin note: text overlap with\n  arXiv:2207.07538",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  We develop an experimentally validated, short and easy-to-use survey module\nfor measuring individual debt aversion. To this end, we first estimate debt\naversion on an individual level, using choice data from Meissner and Albrecht\n(2022). This data also contains responses to a large set of debt aversion\nsurvey items, consisting of existing items from the literature and novel items\ndeveloped for this study. Out of these, we identify a survey module comprising\ntwo qualitative survey items to best predict debt aversion in the incentivized\nexperiment.\n"
    },
    {
        "paper_id": 2211.0299,
        "authors": "Steven Campbell, Ting-Kam Leonard Wong",
        "title": "Efficient Convex PCA with applications to Wasserstein geodesic PCA and\n  ranked data",
        "comments": "40 pages, 9 figures, sample code available at:\n  https://github.com/stevenacampbell/ConvexPCA",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Convex PCA, which was introduced by Bigot et al., is a dimension reduction\nmethodology for data with values in a convex subset of a Hilbert space. This\nsetting arises naturally in many applications, including distributional data in\nthe Wasserstein space of an interval, and ranked compositional data under the\nAitchison geometry. Our contribution in this paper is threefold. First, we\npresent several new theoretical results including consistency as well as\ncontinuity and differentiability of the objective function in the finite\ndimensional case. Second, we develop a numerical implementation of finite\ndimensional convex PCA when the convex set is polyhedral, and show that this\nprovides a natural approximation of Wasserstein geodesic PCA. Third, we\nillustrate our results with two financial applications, namely distributions of\nstock returns ranked by size and the capital distribution curve, both of which\nare of independent interest in stochastic portfolio theory.\n"
    },
    {
        "paper_id": 2211.03002,
        "authors": "Abhijit Chakraborty, Tetsuo Hatsuda and Yuichi Ikeda",
        "title": "Projecting XRP price burst by correlation tensor spectra of transaction\n  networks",
        "comments": "12 pages 8 figures, Supplementary Information: 4 pages, 6 figures",
        "journal-ref": "Scientific Reports 13, 4718 (2023)",
        "doi": "10.1038/s41598-023-31881-5",
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Cryptoassets are becoming essential in the digital economy era. XRP is one of\nthe large market cap cryptoassets. Here, we develop a novel method of\ncorrelation tensor spectra for the dynamical XRP networks, which can provide an\nearly indication for XRP price. A weighed directed weekly transaction network\namong XRP wallets is constructed by aggregating all transactions for a week. A\nvector for each node is then obtained by embedding the weekly network in\ncontinuous vector space. From a set of weekly snapshots of node vectors, we\nconstruct a correlation tensor. A double singular value decomposition of the\ncorrelation tensors gives its singular values. The significance of the singular\nvalues is shown by comparing with its randomize counterpart. The evolution of\nsingular values shows a distinctive behavior. The largest singular value shows\na significant negative correlation with XRP/USD price. We observe the minimum\nof the largest singular values at the XRP/USD price peak during the first week\nof January 2018. The minimum of the largest singular value during January 2018\nis explained by decomposing the correlation tensor in the signal and noise\ncomponents and also by evolution of community structure.\n"
    },
    {
        "paper_id": 2211.03107,
        "authors": "Xiao-Yang Liu, Ziyi Xia, Jingyang Rui, Jiechao Gao, Hongyang Yang,\n  Ming Zhu, Christina Dan Wang, Zhaoran Wang, Jian Guo",
        "title": "FinRL-Meta: Market Environments and Benchmarks for Data-Driven Financial\n  Reinforcement Learning",
        "comments": "NeurIPS 2022 Datasets and Benchmarks. 36th Conference on Neural\n  Information Processing Systems Datasets and Benchmarks Track",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Finance is a particularly difficult playground for deep reinforcement\nlearning. However, establishing high-quality market environments and benchmarks\nfor financial reinforcement learning is challenging due to three major factors,\nnamely, low signal-to-noise ratio of financial data, survivorship bias of\nhistorical data, and model overfitting in the backtesting stage. In this paper,\nwe present an openly accessible FinRL-Meta library that has been actively\nmaintained by the AI4Finance community. First, following a DataOps paradigm, we\nwill provide hundreds of market environments through an automatic pipeline that\ncollects dynamic datasets from real-world markets and processes them into\ngym-style market environments. Second, we reproduce popular papers as stepping\nstones for users to design new trading strategies. We also deploy the library\non cloud platforms so that users can visualize their own results and assess the\nrelative performance via community-wise competitions. Third, FinRL-Meta\nprovides tens of Jupyter/Python demos organized into a curriculum and a\ndocumentation website to serve the rapidly growing community. FinRL-Meta is\navailable at: https://github.com/AI4Finance-Foundation/FinRL-Meta\n"
    },
    {
        "paper_id": 2211.03125,
        "authors": "Arnab K. Ray",
        "title": "Logistic forecasting of GDP competitiveness",
        "comments": "5 pages, 4 figures, ReVTeX double column format",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The GDP growth of national economies is modelled by the logistic function.\nApplying it on the GDP data of the World Bank till the year 2020, we forecast\nthe outcome of the competitive GDP growth of Japan, Germany, UK and India, all\nof whose current GDPs are very close to one another. Fulfilling one of the\npredictions, in 2022 the GDP of India has indeed overtaken the GDP of UK. Our\noverall forecast is that by 2047, the GDP of India will be greater than that of\nthe other three countries. We argue that when trade saturates, large and\npopulous countries (like India) have the benefit of high domestic consumption\nto propel their GDP growth.\n"
    },
    {
        "paper_id": 2211.03221,
        "authors": "Emma Kroell, Silvana M. Pesenti, Sebastian Jaimungal",
        "title": "Stressing Dynamic Loss Models",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Stress testing, and in particular, reverse stress testing, is a prominent\nexercise in risk management practice. Reverse stress testing, in contrast to\n(forward) stress testing, aims to find an alternative but plausible model such\nthat under that alternative model, specific adverse stresses (i.e. constraints)\nare satisfied. Here, we propose a reverse stress testing framework for dynamic\nmodels. Specifically, we consider a compound Poisson process over a finite time\nhorizon and stresses composed of expected values of functions applied to the\nprocess at the terminal time. We then define the stressed model as the\nprobability measure under which the process satisfies the constraints and which\nminimizes the Kullback-Leibler divergence to the reference compound Poisson\nmodel.\n  We solve this optimization problem, prove existence and uniqueness of the\nstressed probability measure, and provide a characterization of the\nRadon-Nikodym derivative from the reference model to the stressed model. We\nfind that under the stressed measure, the intensity and the severity\ndistribution of the process depend on time and state. We illustrate the dynamic\nstress testing by considering stresses on VaR and both VaR and CVaR jointly and\nprovide illustrations of how the stochastic process is altered under these\nstresses. We generalize the framework to multivariate compound Poisson\nprocesses and stresses at times other than the terminal time. We illustrate the\napplicability of our framework by considering ``what if'' scenarios, where we\nanswer the question: What is the severity of a stress on a portfolio component\nat an earlier time such that the aggregate portfolio exceeds a risk threshold\nat the terminal time? Furthermore, for general constraints, we propose an\nalgorithm to simulate sample paths under the stressed measure, thus allowing to\ncompare the effects of stresses on the dynamics of the process.\n"
    },
    {
        "paper_id": 2211.03287,
        "authors": "Reza Bradrania, Robert Elliott and Winston Wu",
        "title": "Institutional ownership and liquidity commonality: evidence from\n  Australia",
        "comments": null,
        "journal-ref": "Acc.Fin. 62(2022)1231-1272",
        "doi": "10.1111/acfi.12822",
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  We study the liquidity commonality impact of local and foreign institutional\ninvestment in the Australian equity market in the cross-section and over time.\nWe find that commonality in liquidity is higher for large stocks compared to\nsmall stocks in the cross-section of stocks, and the spread between the two has\nincreased over the past two decades. We show that this divergence can be\nexplained by foreign institutional ownership. This finding suggests that\nforeign institutional investment contributes to an increase in the exposure of\nlarge stocks to unexpected liquidity events in the local market. We find a\npositive association between foreign institutional ownership and commonality in\nliquidity across all stocks, particularly in large and mid-cap stocks.\nCorrelated trading by foreign institutions explains this association. However,\nlocal institutional ownership is positively related to the commonality in\nliquidity for large-cap stocks only.\n"
    },
    {
        "paper_id": 2211.03411,
        "authors": "Janifar Alam, Quazi Nur Alam, Abu Kalam",
        "title": "Prospects and Challenges for Sustainable Tourism: Evidence from South\n  Asian Countries",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Tourism is one of the world's fastest expanding businesses, as well as a\nsignificant source of foreign exchange profits and jobs. The research is based\non secondary sources. The facts and information were primarily gathered and\nanalyzed from various published papers and articles. The study goals are to\nillustrate the current scenario of tourism industry in south Asia, classifies\nthe restraints and recommends helpful key developments to achieve sustainable\ntourism consequently. The study revealed that major challenges of sustainable\ntourism in south Asian region are lack of infrastructure facilities, modern and\nsufficient recreation facilities, security and safety, proper training and HR,\nproper planning from government, marketing and information, product\ndevelopment, tourism awareness, security and safety, and political instability\netc. The study also provides some suggestive measures that for the long-term\ngrowth of regional tourism, the government should establish and implement\npolicies involving public and private investment and collaboration.\n"
    },
    {
        "paper_id": 2211.03591,
        "authors": "Nikolay Khabarov, Alexey Smirnov and Michael Obersteiner",
        "title": "Shadow prices and optimal cost in economic applications",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-sa/4.0/",
        "abstract": "  Shadow prices are well understood and are widely used in economic\napplications. However, there are limits to where shadow prices can be applied\nassuming their natural interpretation and the fact that they reflect the first\norder optimality conditions (FOC). In this paper, we present a simple ad-hoc\nexample demonstrating that marginal cost associated with exercising an optimal\ncontrol may exceed the respective cost estimated from a ratio of shadow prices.\nMoreover, such cost estimation through shadow prices is arbitrary and depends\non a particular (mathematically equivalent) formulation of the optimization\nproblem. These facts render a ratio of shadow prices irrelevant to estimation\nof optimal marginal cost. The provided illustrative optimization problem links\nto a similar approach of calculating social cost of carbon (SCC) in the widely\nused dynamic integrated model of climate and the economy (DICE).\n"
    },
    {
        "paper_id": 2211.03604,
        "authors": "George Samartzis, Nikitas Pittis",
        "title": "Dynamic Estimates Of The Arrow-Pratt Absolute And Relative Risk Aversion\n  Coefficients",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  We derive a closed-form expression capturing the degree of Relative Risk\nAversion (RRA) of investors for non-\"fair\" lotteries. We argue that our formula\nis superior to earlier methods that have been proposed, as it is a function of\nonly three variables. Namely, the Treasury yields, the returns and the market\ncapitalization of a specific market index. Our formula, is tested on CAC 40,\nEURO, S&P 500 and STOXX 600, with respect to the market capitalization of each\nindex, for different time periods. We deduce that the investors in these\nmarkets exhibit Decreasing Absolute Risk Aversion (DARA) through all the\ndifferent time periods that we consider, while the degree of RRA has altered\nbetween being constant, decreasing or increasing. Furthermore, we propose a\nsimple and intuitive way to measure the degree to which a wrong assumption with\nrespect to the utility function of an investor will affect the structure of his\nportfolio. Our method is built on a two asset portfolio framework. Namely, a\nportfolio consisting of one risky and one risk-free asset. Applying our method,\nthe empirical findings indicate that the weight invested in the risky asset\nvaries substantially even among utility functions with similar characteristics.\n"
    },
    {
        "paper_id": 2211.03638,
        "authors": "Leonardo Perotti, Lech A. Grzelak",
        "title": "On Pricing of Discrete Asian and Lookback Options under the Heston Model",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  We propose a new, data-driven approach for efficient pricing of - fixed- and\nfloat-strike - discrete arithmetic Asian and Lookback options when the\nunderlying process is driven by the Heston model dynamics. The method proposed\nin this article constitutes an extension of our previous work, where the\nproblem of sampling from time-integrated stochastic bridges was addressed. The\nmodel relies on the Seven-League scheme, where artificial neural networks are\nemployed to \"learn\" the distribution of the random variable of interest\nutilizing stochastic collocation points. The method results in a robust\nprocedure for Monte Carlo pricing. Furthermore, semi-analytic formulae for\noption pricing are provided in a simplified, yet general, framework. The model\nguarantees high accuracy and a reduction of the computational time up to\nthousands of times compared to classical Monte Carlo pricing schemes.\n"
    },
    {
        "paper_id": 2211.03912,
        "authors": "Francisco Cabezon",
        "title": "The Optimal Size and Progressivity of Old-Age Social Security",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Almost every public pension system shares two attributes: earning deductions\nto finance benefits, and benefits that depend on earnings. This paper analyzes\ntheoretically and empirically the trade-off between social insurance and\nincentive provision faced by reforms to these two attributes. First, I combine\nthe social insurance and the optimal linear-income literature to build a model\nwith a flexible pension contribution rate and benefits' progressivity that\nincorporates inter-temporal and inter-worker types of redistribution and\nincentive distortion. The model is general, allowing workers to be\nheterogeneous on productivity and retirement preparedness, and they exhibit\npresent-focused bias. I then estimate the model by leveraging three\nquasi-experimental variations on the design of the Chilean pension system and\nadministrative data merged with a panel survey. I find that taxable earnings\nrespond to changes in the benefit-earnings link, future pension payments, and\nnet-of-tax rate, which increases the costs of reforms. I also find that\nlifetime payroll earnings have a strong positive relationship with productivity\nand retirement preparedness, and that pension transfers are effective in\nincreasing retirement consumption. Therefore, there is a large inter-worker\nredistribution value through the pension system. Overall, there are significant\nsocial gains from marginal reforms: a 1% increase in the contribution rate and\nin the benefit progressivity generates social gains of 0.08% and 0.29% of the\nGDP, respectively. The optimal design has a pension contribution rate of 17%\nand focuses 42% of pension public spending on workers below the median of\nlifetime earnings.\n"
    },
    {
        "paper_id": 2211.03945,
        "authors": "Raul Enrique Rodriguez Luna and Jose Luis Rosenstiehl Martinez",
        "title": "Digital Transformation of Nature Tourism",
        "comments": "7 pages",
        "journal-ref": null,
        "doi": "10.22267/rtend.222301.185",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The purpose of this article is to explore the digital behavior of nature\ntourism SMEs in the department of Magdalena-Colombia, hereinafter referred to\nas the region. In this sense, the concept of endogenization as an evolutionary\nmechanism refers to the application of the discrete choice model as an engine\nof analysis for the variables to be studied within the model. The type of study\nwas quantitative, of correlational level, the sample was 386 agents of the\ntourism chain; a survey-type instrument with five factors and a Likert-type\nscale was used for data collection. For the extraction of the factors, a\nconfirmatory factor analysis was used, using structural equations, followed by\na discrete choice model and then the analysis of the results. Among the main\nfindings are that the SMEs in the tourism chain that tried to incorporate Big\nData activities in the decision-making processes, have greater chances of\nsuccess in the digital transformation, in addition, statistical evidence was\nfound that the training of staff in Data Science, contributes significantly to\nthe marketing and commercialization processes within the SME in this region.\n"
    },
    {
        "paper_id": 2211.04095,
        "authors": "Abel Azze, Bernardo D'Auria, Eduardo Garc\\'ia-Portugu\\'es",
        "title": "Optimal exercise of American options under time-dependent\n  Ornstein-Uhlenbeck processes",
        "comments": "25 pages, 3 figures",
        "journal-ref": "Stochastics, 96(1):921-946, 2024",
        "doi": "10.1080/17442508.2024.2325402",
        "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
        "abstract": "  We study the barrier that gives the optimal time to exercise an American\noption written on a time-dependent Ornstein--Uhlenbeck process, a diffusion\noften adopted by practitioners to model commodity prices and interest rates. By\nframing the optimal exercise of the American option as a problem of optimal\nstopping and relying on probabilistic arguments, we provide a non-linear\nVolterra-type integral equation characterizing the exercise boundary, develop a\nnovel comparison argument to derive upper and lower bounds for such a boundary,\nand prove its Lipschitz continuity in any closed interval that excludes the\nexpiration date and, thus, its differentiability almost everywhere. We\nimplement a Picard iteration algorithm to solve the Volterra integral equation\nand show illustrative examples that shed light on the boundary's dependence on\nthe process's drift and volatility.\n"
    },
    {
        "paper_id": 2211.04184,
        "authors": "Francis X. Diebold and Kamil Yilmaz",
        "title": "On the Past, Present, and Future of the Diebold-Yilmaz Approach to\n  Dynamic Network Connectedness",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  We offer retrospective and prospective assessments of the Diebold-Yilmaz\nconnectedness research program, combined with personal recollections of its\ndevelopment. Its centerpiece in many respects is Diebold and Yilmaz (2014),\naround which our discussion is organized.\n"
    },
    {
        "paper_id": 2211.04349,
        "authors": "Alessandro Gnoatto, Marco Patacca and Athena Picarelli",
        "title": "A deep solver for BSDEs with jumps",
        "comments": "31 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  The aim of this work is to propose an extension of the Deep BSDE solver by\nHan, E, Jentzen (2017) to the case of FBSDEs with jumps. As in the\naforementioned solver, starting from a discretized version of the BSDE and\nparametrizing the (high dimensional) control processes by means of a family of\nANNs, the BSDE is viewed as model-based reinforcement learning problem and the\nANN parameters are fitted so as to minimize a prescribed loss function. We take\ninto account both finite and infinite jump activity by introducing, in the\nlatter case, an approximation with finitely many jumps of the forward process.\n"
    },
    {
        "paper_id": 2211.04388,
        "authors": "Alessandro Ferrari, S\\'ebastien Laffitte, Mathieu Parenti, Farid\n  Toubal",
        "title": "Profit Shifting Frictions and the Geography of Multinational Activity",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We develop a quantitative general equilibrium model of multinational activity\nembedding corporate taxation and profit shifting. In addition to trade and\ninvestment frictions, our model shows that profit-shifting frictions shape the\ngeography of multinational production. Key to our model is the distinction\nbetween the corporate tax elasticity of real activity and profit shifting. The\nquantification of our model requires estimates of shifted profits flows. We\nprovide a new, model-consistent methodology to calibrate bilateral\nprofit-shifting frictions based on accounting identities. We simulate various\ntax reforms aimed at curbing tax-dodging practices of multinationals and their\nimpact on a range of outcomes, including tax revenues and production. Our\nresults show that the effects of the international relocation of firms across\ncountries are of comparable magnitude as the direct gains in taxable income.\n"
    },
    {
        "paper_id": 2211.04436,
        "authors": "Pierre-Lo\\\"ic M\\'eliot, Ashkan Nikeghbali and Gabriele Visentin",
        "title": "Mod-Poisson approximation schemes: Applications to credit risk",
        "comments": "42 pages, 7 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We introduce a new numerical approximation method for functionals of factor\ncredit portfolio models based on the theory of mod-$\\phi$ convergence and\nmod-$\\phi$ approximation schemes. The method can be understood as providing\ncorrection terms to the classic Poisson approximation, where higher order\ncorrections lead to asymptotically better approximations as the number of\nobligors increases. We test the model empirically on two tasks: the estimation\nof risk measures ($\\mathrm{VaR}$ and $\\mathrm{ES}$) and the computation of CDO\ntranche prices. We compare it to other commonly used methods -- such as the\nrecursive method, the large deviations approximation, the Chen--Stein method\nand the Monte Carlo simulation technique (with and without importance sampling)\n-- and we show that it leads to more accurate estimates while requiring less\ncomputational time.\n"
    },
    {
        "paper_id": 2211.04592,
        "authors": "Giulio Principi and Fabio Maccheroni",
        "title": "Conditional divergence risk measures",
        "comments": "33 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Our paper contributes to the theory of conditional risk measures and\nconditional certainty equivalents. We adopt a random modular approach which\nproved to be effective in the study of modular convex analysis and conditional\nrisk measures. In particular, we study the conditional counterpart of optimized\ncertainty equivalents. In the process, we provide representation results for\nniveloids in the conditional $L^{\\infty}$-space. By employing such\nrepresentation results we retrieve a conditional version of the variational\nformula for optimized certainty equivalents. In conclusion, we apply this\nformula to provide a variational representation of the conditional entropic\nrisk measure.\n"
    },
    {
        "paper_id": 2211.04695,
        "authors": "M. Reza Bradrania, Maurice Peat, Stephen Satchell",
        "title": "Liquidity Costs, Idiosyncratic Volatility and Expected Stock Returns",
        "comments": null,
        "journal-ref": "International review of financial analysis 42 (2015)394-406",
        "doi": "10.1016/j.irfa.2015.09.005",
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  This paper considers liquidity as an explanation for the positive association\nbetween expected idiosyncratic volatility (IV) and expected stock returns.\nLiquidity costs may affect the stock returns, through bid-ask bounce and other\nmicrostructure-induced noise, which will affect the estimation of IV. We use a\nnovel method (developed by Weaver, 1991) to eliminate microstructure influences\nfrom stock closing price-based returns and then estimate IV. We show that there\nis a premium for IV in value-weighted portfolios, but this premium is less\nstrong after correcting returns for microstructure bias. We further show that\nthis premium is driven by liquidity in the prior month after correcting returns\nfor microstructure noise. The pricing results from equally-weighted portfolios\nindicate that IV does not predict returns either before or after controlling\nfor liquidity costs. These findings are robust after controlling for common\nrisk factors as well as analysing double-sorted portfolios based on IV and\nliquidity.\n"
    },
    {
        "paper_id": 2211.04762,
        "authors": "Kerstin Awiszus, Yannick Bell, Jan L\\\"uttringhaus, Gregor Svindland,\n  Alexander Vo{\\ss}, Stefan Weber",
        "title": "Building Resilience in Cybersecurity -- An Artificial Lab Approach",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Based on classical contagion models we introduce an artificial cyber lab: the\ndigital twin of a complex cyber system in which possible cyber resilience\nmeasures may be implemented and tested. Using the lab, in numerical case\nstudies, we identify two classes of measures to control systemic cyber risks:\nsecurity- and topology-based interventions. We discuss the implications of our\nfindings on selected real-world cybersecurity measures currently applied in the\ninsurance and regulation practice or under discussion for future cyber risk\ncontrol. To this end, we provide a brief overview of the current cybersecurity\nregulation and emphasize the role of insurance companies as private regulators.\nMoreover, from an insurance point of view, we provide first attempts to design\nsystemic cyber risk obligations and to measure the systemic risk contribution\nof individual policyholders.\n"
    },
    {
        "paper_id": 2211.04763,
        "authors": "Masahiro Kubo and Shunsuke Tsuda",
        "title": "The Golden City on the Edge: Economic Geography and Jihad over Centuries",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper uncovers the evolution of cities and Islamist insurgencies, so\ncalled jihad, in the process of the reversal of fortune over the centuries. In\nWest Africa, water access in ancient periods predicts the locations of the core\ncities of inland trade routes -- the trans-Saharan caravan routes -- founded up\nto the 1800s, when historical Islamic states played significant economic roles\nbefore European colonization. In contrast, ancient water access does not have a\npersistent influence on contemporary city formation and economic activities.\nAfter European colonization and the invention of modern trading technologies,\nalong with the constant shrinking of water sources, landlocked pre-colonial\ncore cities contracted or became extinct. Employing an instrumental variable\nstrategy, we show that these deserted locations have today been replaced by\nbattlefields for jihadist organizations. We argue that the power relations\nbetween Islamic states and the European military during the 19th century\ncolonial era shaped the persistence of jihadist ideology as a legacy of\ncolonization. Investigations into religious ideology related to jihadism, using\nindividual-level surveys from Muslims, support this mechanism. Moreover, the\nconcentration of jihadist violence in \"past-core-and-present-periphery\" areas\nin West Africa is consistent with a global-scale phenomenon. Finally,\nspillovers of violent events beyond these stylized locations are partly\nexplained by organizational heterogeneity among competing factions (Al Qaeda\nand the Islamic State) over time.\n"
    },
    {
        "paper_id": 2211.04954,
        "authors": "Ayaz Zeynalov and Kristina Tiron",
        "title": "Macroeconomic performance of oil price shocks in Russia",
        "comments": "10 pages, 13 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Oil price fluctuations severely impact the economies of both oil-exporting\nand importing countries. High oil prices can benefit oil exporters by\nincreasing foreign currency inflow; however, an economy can suffer from a\nweakening of the manufacturing sectors and experience a significant downtrend\nin the country's price competitiveness as the domestic currency appreciates. We\ninvestigate the oil price fluctuations from Q1, 2004 to Q3, 2021 and their\nimpact on the Russian macroeconomic indicators, particularly industrial\nproduction, exchange rate, inflation and interest rates. We assess whether and\nhow much the Russian macroeconomic variables have been responsive to the oil\nprice fluctuations in recent years. The outcomes from VAR model confirm that\nthe monetary channel is more responsive to oil price shocks than the fiscal\none. Regarding fiscal channel of the oil price impact, industrial production is\nstrongly pro-cyclical to oil price shocks. As for the monetary channel, higher\noil price volatility is pressuring the Russian ruble, inflation and interest\nrates are substantially counter-cyclical to oil price shocks.\n"
    },
    {
        "paper_id": 2211.05014,
        "authors": "Lech A. Grzelak",
        "title": "Randomization of Short-Rate Models, Analytic Pricing and Flexibility in\n  Controlling Implied Volatilities",
        "comments": "22 Pages, 9 Figures, 2 Tables",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  We focus on extending existing short-rate models, enabling control of the\ngenerated implied volatility while preserving analyticity. We achieve this goal\nby applying the Randomized Affine Diffusion (RAnD) method to the class of\nshort-rate processes under the Heath-Jarrow-Morton framework. Under\narbitrage-free conditions, the model parameters can be exogenously stochastic,\nthus facilitating additional degrees of freedom that enhance the calibration\nprocedure. We show that with the randomized short-rate models, the shapes of\nimplied volatility can be controlled and significantly improve the quality of\nthe model calibration, even for standard 1D variants. In particular, we\nillustrate that randomization applied to the Hull-White model leads to dynamics\nof the local volatility type, with the prices for standard volatility-sensitive\nderivatives explicitly available. The randomized Hull-White (rHW) model offers\nan almost perfect calibration fit to the swaption implied volatilities.\n"
    },
    {
        "paper_id": 2211.05291,
        "authors": "Ying Hu, Xiaomin Shi, Zuo Quan Xu",
        "title": "Optimal consumption-investment with coupled constraints on consumption\n  and investment strategies in a regime switching market with random\n  coefficients",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  This paper studies finite-time optimal consumption-investment problems with\npower, logarithmic and exponential utilities, in a regime switching market with\nrandom coefficients, subject to coupled constraints on the consumption and\ninvestment strategies. We provide explicit optimal consumption-investment\nstrategies and optimal values for the problems in terms of the solutions to\nsome diagonally quadratic backward stochastic differential equation (BSDE)\nsystems and linear BSDE systems with unbound coefficients. Some of these BSDEs\nare new in the literature and solving them is one of the main theoretical\ncontributions of this paper. We accomplish the latter by applying the\ntruncation, approximation technique to get some a priori uniformly lower and\nupper bounds for their solutions.\n"
    },
    {
        "paper_id": 2211.05316,
        "authors": "Mikhail Zhitlukhin",
        "title": "Optimal growth strategies for a representative agent in a\n  continuous-time asset market",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We propose a multi-agent model of an asset market and study conditions that\nguarantee that the strategy of an individual agent cannot outperform the\nmarket. The model assumes a mean-field approximation of the market by\nconsidering an infinite number of infinitesimal agents who use the same\nstrategy and another infinitesimal agent with a different strategy who tries to\noutperform the market. We show that the optimal strategy for the market agents\nis to split their investment budgets among the assets proportionally to their\ndiscounted expected relative dividend intensities.\n"
    },
    {
        "paper_id": 2211.05367,
        "authors": "Wahid Faidi",
        "title": "Optimal investment and consumption under logarithmic utility and\n  uncertainty model",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We study a robust utility maximization problem in the case of an incomplete\nmarket and logarithmic utility with general stochastic constraints, not\nnecessarily convex. Our problem is equivalent to maximizing of nonlinear\nexpected logarithmic utility. We characterize the optimal solution using\nquadratic BSDE.\n"
    },
    {
        "paper_id": 2211.05402,
        "authors": "Jing Peng, Pengyu Wei, Zuo Quan Xu",
        "title": "Relative growth rate optimization under behavioral criterion",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper studies a continuous-time optimal portfolio selection problem in\nthe complete market for a behavioral investor whose preference is of the\nprospect type with probability distortion. The investor concerns about the\nterminal relative growth rate (log-return) instead of absolute capital value.\nThis model can be regarded as an extension of the classical growth optimal\nproblem to the behavioral framework. It leads to a new type of M-shaped utility\nmaximization problem under nonlinear Choquet expectation. Due to the presence\nof probability distortion, the classical stochastic control methods are not\napplicable. By the martingale method, concavification and quantile optimization\ntechniques, we derive the closed-form optimal growth rate. We find that the\nbenchmark growth rate has a significant impact on investment behaviors.\nCompared to Zhang et al where the same preference measure is applied to the\nterminal relative wealth, we find a new phenomenon when the investor's risk\ntolerance level is high and the market states are bad. In addition, our optimal\nwealth in every scenario is less sensitive to the pricing kernel and thus more\nstable than theirs.\n"
    },
    {
        "paper_id": 2211.05415,
        "authors": "Andrey Shternshis, Piero Mazzarisi",
        "title": "Variance of entropy for testing time-varying regimes with an application\n  to meme stocks",
        "comments": "30 pages, 10 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Shannon entropy is the most common metric to measure the degree of randomness\nof time series in many fields, ranging from physics and finance to medicine and\nbiology. Real-world systems may be in general non stationary, with an entropy\nvalue that is not constant in time. The goal of this paper is to propose a\nhypothesis testing procedure to test the null hypothesis of constant Shannon\nentropy for time series, against the alternative of a significant variation of\nthe entropy between two subsequent periods. To this end, we find an unbiased\napproximation of the variance of the Shannon entropy's estimator, up to the\norder O(n^(-4)) with n the sample size. In order to characterize the variance\nof the estimator, we first obtain the explicit formulas of the central moments\nfor both the binomial and the multinomial distributions, which describe the\ndistribution of the Shannon entropy. Second, we find the optimal length of the\nrolling window used for estimating the time-varying Shannon entropy by\noptimizing a novel self-consistent criterion based on the counting of\nsignificant variations of entropy within a time window. We corroborate our\nfindings by using the novel methodology to test for time-varying regimes of\nentropy for stock price dynamics, in particular considering the case of meme\nstocks in 2020 and 2021. We empirically show the existence of periods of market\ninefficiency for meme stocks. In particular, sharp increases of prices and\ntrading volumes correspond to statistically significant drops of Shannon\nentropy.\n"
    },
    {
        "paper_id": 2211.05581,
        "authors": "Yao Lei Xu, Kriton Konstantinidis, Danilo P. Mandic",
        "title": "Graph-Regularized Tensor Regression: A Domain-Aware Framework for\n  Interpretable Multi-Way Financial Modelling",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Analytics of financial data is inherently a Big Data paradigm, as such data\nare collected over many assets, asset classes, countries, and time periods.\nThis represents a challenge for modern machine learning models, as the number\nof model parameters needed to process such data grows exponentially with the\ndata dimensions; an effect known as the Curse-of-Dimensionality. Recently,\nTensor Decomposition (TD) techniques have shown promising results in reducing\nthe computational costs associated with large-dimensional financial models\nwhile achieving comparable performance. However, tensor models are often unable\nto incorporate the underlying economic domain knowledge. To this end, we\ndevelop a novel Graph-Regularized Tensor Regression (GRTR) framework, whereby\nknowledge about cross-asset relations is incorporated into the model in the\nform of a graph Laplacian matrix. This is then used as a regularization tool to\npromote an economically meaningful structure within the model parameters. By\nvirtue of tensor algebra, the proposed framework is shown to be fully\ninterpretable, both coefficient-wise and dimension-wise. The GRTR model is\nvalidated in a multi-way financial forecasting setting and compared against\ncompeting models, and is shown to achieve improved performance at reduced\ncomputational costs. Detailed visualizations are provided to help the reader\ngain an intuitive understanding of the employed tensor operations.\n"
    },
    {
        "paper_id": 2211.05628,
        "authors": "Maksym Obrizan",
        "title": "Poverty, Unemployment and Displacement in Ukraine: three months into the\n  war",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper identifies the causal effects of full-scale kremlin aggression on\nsocio-economic outcomes in Ukraine three months into the full-scale war. First,\nforced migration after February 24th, 2022 is associated with an elevated risk\nof becoming unemployed by 7.5% points. Second, difference-in-difference\nregressions show that in regions with fighting on the ground females without a\nhigher education face a 9.6-9.9% points higher risk of not having enough money\nfor food. Finally, in the regions subject to ground attack females with and\nwithout a higher education, as well as males without a higher education are\nmore likely to become unemployed by 6.1-6.9%, 4.2-4.7% and 6.5-6.6% points\ncorrespondingly. This persistent gender gap in poverty and unemployment, when\neven higher education is not protective for females, calls for policy action.\nWhile more accurate results may obtain with more comprehensive surveys, this\npaper provides a remarkably robust initial estimate of the war's effects on\npoverty, unemployment and internal migration.\n"
    },
    {
        "paper_id": 2211.05666,
        "authors": "Khrystyna Huk and Ayaz Zeynalov",
        "title": "Regional Disparities and Economic Growth in Ukraine",
        "comments": "8 pages, 6 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This research is devoted to assessing regional economic disparities in\nUkraine, where regional economic inequality is a crucial issue the country\nfaces in its medium and long-term development, recently, even in the short\nterm. We analyze the determinants of regional economic growth, mainly\nindustrial and agricultural productions, population, human capital, fertility,\nmigration, and regional government expenditures. Using panel data estimations\nfrom 2004 to 2020 for 27 regions of Ukraine, our results show that the gaps\nbetween regions in Ukraine have widened last two decades. Natural resource\ndistribution, agricultural and industrial productions, government spending, and\nmigration can explain the disparities. We show that regional government\nspending is highly concentrated in Kyiv, and the potential of the other\nregions, especially the Western ones, needs to be used sufficiently. Moreover,\ndespite its historical and economic opportunity, the East region performed\nlittle development during the last two decades. The inefficient and\ninconsistent regional policies played a crucial role in these disparities.\n"
    },
    {
        "paper_id": 2211.05835,
        "authors": "Abel Azze, Bernardo D'Auria, Eduardo Garc\\'ia-Portugu\\'es",
        "title": "Optimal stopping of Gauss-Markov bridges",
        "comments": "32 pages, 2 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
        "abstract": "  We solve the non-discounted, finite-horizon optimal stopping problem of a\nGauss-Markov bridge by using a time-space transformation approach. The\nassociated optimal stopping boundary is proved to be Lipschitz continuous on\nany closed interval that excludes the horizon, and it is characterized by the\nunique solution of an integral equation. A Picard iteration algorithm is\ndiscussed and implemented to exemplify the numerical computation and geometry\nof the optimal stopping boundary for some illustrative cases.\n"
    },
    {
        "paper_id": 2211.06042,
        "authors": "David Criens and Mikhail Urusov",
        "title": "Separating Times for One-Dimensional Diffusions",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The separating time for two probability measures on a filtered space is an\nextended stopping time which captures the phase transition between equivalence\nand singularity. More specifically, two probability measures are equivalent\nbefore their separating time and singular afterwards. In this paper, we\ninvestigate the separating time for two laws of general one-dimensional regular\ncontinuous strong Markov processes, so-called general diffusions, which are\nparameterized via scale functions and speed measures. Our main result is a\nrepresentation of the corresponding separating time as (loosely speaking) a\nhitting time of a deterministic set which is characterized via speed and scale.\nAs hitting times are fairly easy to understand, our result gives access to\nexplicit and easy-to-check sufficient and necessary conditions for two laws of\ngeneral diffusions to be (locally) absolutely continuous and/or singular. Most\nof the related literature treats the case of stochastic differential equations.\nIn our setting we encounter several novel features, which are due to general\nspeed and scale on the one hand, and to the fact that we do not exclude\n(instantaneous or sticky) reflection on the other hand. These new features are\ndiscussed in a variety of examples. As an application of our main theorem, we\ninvestigate the no arbitrage concept no free lunch with vanishing risk (NFLVR)\nfor a single asset financial market whose (discounted) asset is modeled as a\ngeneral diffusion which is bounded from below (e.g., non-negative). More\nprecisely, we derive deterministic criteria for NFLVR and we identify the\n(unique) equivalent local martingale measure as the law of a certain general\ndiffusion on natural scale.\n"
    },
    {
        "paper_id": 2211.06046,
        "authors": "Ziyi Xu and Xue Cheng",
        "title": "Are Large Traders Harmed by Front-running HFTs?",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper studies the influences of a high-frequency trader (HFT) on a large\ntrader whose future trading is predicted by the former. We conclude that HFT\nalways front-runs and the large trader is benefited when: (1) there is\nsufficient high-speed noise trading; (2) HFT's prediction is vague enough.\nBesides, we find surprisingly that (1) making HFT's prediction less accurate\nmight decrease large trader's profit; (2) when there is little high-speed noise\ntrading, although HFT nearly does nothing, the large trader is still hurt.\n"
    },
    {
        "paper_id": 2211.06052,
        "authors": "Marius \\\"Otting, Christian Deutscher, Carl Singleton, Luca De Angelis",
        "title": "Gambling on Momentum",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Sports betting markets are proven real-world laboratories to test theories of\nasset pricing anomalies and risky behaviour. Using a high-frequency dataset\nprovided directly by a major bookmaker, containing the odds and amounts staked\nthroughout German Bundesliga football matches, we test for evidence of momentum\nin the betting and pricing behaviour after equalising goals. We find that\nbettors see value in teams that have the apparent momentum, staking about 40%\nmore on them than teams that just conceded an equaliser. Still, there is no\nevidence that such perceived momentum matters on average for match outcomes or\nis associated with the bookmaker offering favourable odds. We also confirm that\nbetting on the apparent momentum would lead to substantial losses for bettors.\n"
    },
    {
        "paper_id": 2211.06209,
        "authors": "Robert C. Schmidt, Moritz Drupp, Frikk Nesje and Hendrik Hoegen",
        "title": "Testing the free-rider hypothesis in climate policy",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Free-riding is widely perceived as a key obstacle for effective climate\npolicy. In the game-theoretic literature on non-cooperative climate policy and\non climate cooperation, the free-rider hypothesis is ubiquitous. Yet, the\nfree-rider hypothesis has not been tested empirically in the climate policy\ncontext. With the help of a theoretical model, we demonstrate that if\nfree-riding were the main driver of lax climate policies around the globe, then\nthere should be a pronounced country-size effect: Countries with a larger share\nof the world's population should, all else equal, internalize more climate\ndamages and thus set higher carbon prices. We use this theoretical prediction\nfor testing the free-rider hypothesis empirically. Drawing on data on\nemission-weighted carbon prices from 2020, while controlling for a host of\nother potential explanatory variables of carbon pricing, we find that the\nfree-rider hypothesis cannot be supported empirically, based on the criterion\nthat we propose. Hence, other issues may be more important for explaining\nclimate policy stringency or the lack thereof in many countries.\n"
    },
    {
        "paper_id": 2211.06378,
        "authors": "Rian Dolphin, Barry Smyth, Ruihai Dong",
        "title": "A Multimodal Embedding-Based Approach to Industry Classification in\n  Financial Markets",
        "comments": "8 pages. Accepted at AICS 2022 under title \"A Machine Learning\n  Approach to Industry Classification in Financial Markets\". Preliminary\n  version under this title was discussed at ICAIF '22 Workshop on NLP and\n  Network Analysis in Financial Applications. arXiv admin note: text overlap\n  with arXiv:2202.08968",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Industry classification schemes provide a taxonomy for segmenting companies\nbased on their business activities. They are relied upon in industry and\nacademia as an integral component of many types of financial and economic\nanalysis. However, even modern classification schemes have failed to embrace\nthe era of big data and remain a largely subjective undertaking prone to\ninconsistency and misclassification. To address this, we propose a multimodal\nneural model for training company embeddings, which harnesses the dynamics of\nboth historical pricing data and financial news to learn objective company\nrepresentations that capture nuanced relationships. We explain our approach in\ndetail and highlight the utility of the embeddings through several case studies\nand application to the downstream task of industry classification.\n"
    },
    {
        "paper_id": 2211.06568,
        "authors": "Sebastian Calcetero-Vanegas, Andrei L. Badescu, X. Sheldon Lin",
        "title": "Effective experience rating for large insurance portfolios via surrogate\n  modeling",
        "comments": null,
        "journal-ref": "Insurance: Mathematics and Economics, Volume 118, September 2024,\n  Pages 25-43",
        "doi": "10.1016/j.insmatheco.2024.05.004",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Experience rating in insurance uses a Bayesian credibility model to upgrade\nthe current premiums of a contract by taking into account policyholders'\nattributes and their claim history. Most data-driven models used for this task\nare mathematically intractable, and premiums must be obtained through numerical\nmethods such as simulation via MCMC. However, these methods can be\ncomputationally expensive and even prohibitive for large portfolios when\napplied at the policyholder level. Additionally, these computations become\n``black-box\" procedures as there is no analytical expression showing how the\nclaim history of policyholders is used to upgrade their premiums. To address\nthese challenges, this paper proposes a surrogate modeling approach to\ninexpensively derive an analytical expression for computing the Bayesian\npremiums for any given model, approximately. As a part of the methodology, the\npaper introduces a \\emph{likelihood-based summary statistic} of the\npolicyholder's claim history that serves as the main input of the surrogate\nmodel and that is sufficient for certain families of distribution, including\nthe exponential dispersion family. As a result, the computational burden of\nexperience rating for large portfolios is reduced through the direct evaluation\nof such analytical expression, which can provide a transparent and\ninterpretable way of computing Bayesian premiums.\n"
    },
    {
        "paper_id": 2211.07035,
        "authors": "Misha Perepelitsa",
        "title": "Elementary Bitcoin economics: from production and transaction demand to\n  values",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  In this paper we give an elementary analysis of economics of Bitcoin that\ncombines the transaction demand by the consumers and the supply of hashrate by\nminers. We argue that the decreasing block reward will have no significant\neffect on the exchange rate (price) of Bitcoin and thus the network will be\ntransitioning to a regime where transaction fees will play a bigger part of\nminers' revenue. We consider a simple model where consumers demand bitcoins for\ntransactions, but not for hoarding bitcoins, and we analyze market equilibrium\nwhere the demand is matched with the hashrate supplied by miners. Our main\nconclusion is that the exchange rate of Bitcoin cannot be determined from the\nmarket equilibrium and so our arguments support the hypothesis that Bitcoin\nprice has no economic fundamentals and is free to fluctuate according to the\npresent demand for hoarding and speculation. We point out that increasing fees\nbear the risk of Bitcoin being outcompeted by its main rival Ethereum, and that\ndecreasing revenues to miners depreciate the perception of Bitcoin as a medium\nfor store value (hoarding demand) which will have effect its exchange rate.\n"
    },
    {
        "paper_id": 2211.0708,
        "authors": "Jaydip Sen",
        "title": "Designing Efficient Pair-Trading Strategies Using Cointegration for the\n  Indian Stock Market",
        "comments": "The is the accepted version of the paper that was presented at the\n  Second IEEE International Conference ASIANCON'22. The conference was\n  organized in Pune, India, in August 2022. The paper is 8 pages long and it\n  contains 5 tables and 33 figures. This is not the published version. The\n  published version is copyright-protected by IEEE and has access-controlled",
        "journal-ref": null,
        "doi": "10.1063/1.1395242",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  A pair-trading strategy is an approach that utilizes the fluctuations between\nprices of a pair of stocks in a short-term time frame, while in the long-term\nthe pair may exhibit a strong association and co-movement pattern. When the\nprices of the stocks exhibit significant divergence, the shares of the stock\nthat gains in price are sold (a short strategy) while the shares of the other\nstock whose price falls are bought (a long strategy). This paper presents a\ncointegration-based approach that identifies stocks listed in the five sectors\nof the National Stock Exchange (NSE) of India for designing efficient\npair-trading portfolios. Based on the stock prices from Jan 1, 2018, to Dec 31,\n2020, the cointegrated stocks are identified and the pairs are formed. The\npair-trading portfolios are evaluated on their annual returns for the year\n2021. The results show that the pairs of stocks from the auto and the realty\nsectors, in general, yielded the highest returns among the five sectors studied\nin the work. However, two among the five pairs from the information technology\n(IT) sector are found to have yielded negative returns.\n"
    },
    {
        "paper_id": 2211.0718,
        "authors": "C\\'elestin Coquid\\'e, Jos\\'e Lages and Dima L. Shepelyansky",
        "title": "Dollar-Yuan Battle in the World Trade Network",
        "comments": "The preprint contains 19 pages including 13 pages of supplementary\n  materials, 12 figures (including 7 supplementary figs) and 6 tables",
        "journal-ref": "Entropy 2023, 25(2), 373",
        "doi": "10.3390/e25020373",
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  From the Bretton Woods agreement in 1944 till the present day, the US dollar\nhas been the dominant currency in the world trade. However, the rise of the\nChinese economy led recently to the emergence of trade transactions in Chinese\nyuan. Here, we analyze mathematically how the structure of the international\ntrade flows would favor a country to trade whether in US dollar or in Chinese\nyuan. The computation of the trade currency preference is based on the world\ntrade network built from the 2010-2020 UN Comtrade data. The preference of a\ncountry to trade in US dollar or Chinese yuan is determined by two\nmultiplicative factors: the relative weight of trade volume exchanged by the\ncountry with its direct trade partners, and the relative weight of its trade\npartners in the global international trade. The performed analysis, based on\nIsing spin interactions on the world trade network, shows that, from 2010 to\npresent, a transition took place, and the majority of the world countries would\nhave now a preference to trade in Chinese yuan if one only consider the world\ntrade network structure.\n"
    },
    {
        "paper_id": 2211.07212,
        "authors": "Adil Rengim Cetingoz, Jean-David Fermanian, Olivier Gu\\'eant",
        "title": "Risk Budgeting Portfolios: Existence and Computation",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Modern portfolio theory has provided for decades the main framework for\noptimizing portfolios. Because of its sensitivity to small changes in input\nparameters, especially expected returns, the mean-variance framework proposed\nby Markowitz (1952) has however been challenged by new construction methods\nthat are purely based on risk. Among risk-based methods, the most popular ones\nare Minimum Variance, Maximum Diversification, and Risk Budgeting (especially\nEqual Risk Contribution) portfolios. Despite some drawbacks, Risk Budgeting is\nparticularly attracting because of its versatility: based on Euler's\nhomogeneous function theorem, it can indeed be used with a wide range of risk\nmeasures. This paper presents mathematical results regarding the existence and\nthe uniqueness of Risk Budgeting portfolios for a very wide spectrum of risk\nmeasures and shows that, for many of them, computing the weights of Risk\nBudgeting portfolios only requires a standard stochastic algorithm.\n"
    },
    {
        "paper_id": 2211.07392,
        "authors": "Shayan Halder",
        "title": "FinBERT-LSTM: Deep Learning based stock price prediction using News\n  Sentiment Analysis",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Economy is severely dependent on the stock market. An uptrend usually\ncorresponds to prosperity while a downtrend correlates to recession. Predicting\nthe stock market has thus been a centre of research and experiment for a long\ntime. Being able to predict short term movements in the market enables\ninvestors to reap greater returns on their investments. Stock prices are\nextremely volatile and sensitive to financial market. In this paper we use Deep\nLearning networks to predict stock prices, assimilating financial, business and\ntechnology news articles which present information about the market. First, we\ncreate a simple Multilayer Perceptron (MLP) network and then expand into more\ncomplex Recurrent Neural Network (RNN) like Long Short Term Memory (LSTM), and\nfinally propose FinBERT-LSTM model, which integrates news article sentiments to\npredict stock price with greater accuracy by analysing short-term market\ninformation. We then train the model on NASDAQ-100 index stock data and New\nYork Times news articles to evaluate the performance of MLP, LSTM, FinBERT-LSTM\nmodels using mean absolute error (MAE), mean absolute percentage error (MAPE)\nand accuracy metrics.\n"
    },
    {
        "paper_id": 2211.074,
        "authors": "Thanh Trung Huynh and Minh Hieu Nguyen and Thanh Tam Nguyen and Phi Le\n  Nguyen and Matthias Weidlich and Quoc Viet Hung Nguyen and Karl Aberer",
        "title": "Efficient Integration of Multi-Order Dynamics and Internal Dynamics in\n  Stock Movement Prediction",
        "comments": "Technical report for accepted paper at WSDM 2023",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  Advances in deep neural network (DNN) architectures have enabled new\nprediction techniques for stock market data. Unlike other multivariate\ntime-series data, stock markets show two unique characteristics: (i)\n\\emph{multi-order dynamics}, as stock prices are affected by strong\nnon-pairwise correlations (e.g., within the same industry); and (ii)\n\\emph{internal dynamics}, as each individual stock shows some particular\nbehaviour. Recent DNN-based methods capture multi-order dynamics using\nhypergraphs, but rely on the Fourier basis in the convolution, which is both\ninefficient and ineffective. In addition, they largely ignore internal dynamics\nby adopting the same model for each stock, which implies a severe information\nloss.\n  In this paper, we propose a framework for stock movement prediction to\novercome the above issues. Specifically, the framework includes temporal\ngenerative filters that implement a memory-based mechanism onto an LSTM network\nin an attempt to learn individual patterns per stock. Moreover, we employ\nhypergraph attentions to capture the non-pairwise correlations. Here, using the\nwavelet basis instead of the Fourier basis, enables us to simplify the message\npassing and focus on the localized convolution. Experiments with US market data\nover six years show that our framework outperforms state-of-the-art methods in\nterms of profit and stability. Our source code and data are available at\n\\url{https://github.com/thanhtrunghuynh93/estimate}.\n"
    },
    {
        "paper_id": 2211.07416,
        "authors": "Simon Weber",
        "title": "Collective models and the marriage market",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  In this paper, I develop an integrated approach to collective models and\nmatching models of the marriage market. In the collective framework, both\nhousehold formation and the intra-household allocation of bargaining power are\ntaken as given. This is no longer the case in the present contribution, where\nboth are endogenous to the determination of equilibrium on the marriage market.\nI characterize a class of \"proper\" collective models which can be embedded into\na general matching framework with imperfectly transferable utility. In such\nmodels, the bargaining sets are parametrized by an analytical device called\ndistance function, which plays a key role both for writing down the usual\nstability conditions and for estimation. In general, however, distance\nfunctions are not known in closed-form. I provide an efficient method for\ncomputing distance functions, that works even with the most complex collective\nmodels. Finally, I provide a fully-fledged application using PSID data. I\nidentify the sharing rule and its distribution and study the evolution of the\nsharing rule and housework time sharing in the United States since 1969. In a\ncounterfactual experiment, I simulate the impact of closing the gender wage\ngap.\n"
    },
    {
        "paper_id": 2211.07471,
        "authors": "Mauricio Elizalde, Carlos Escudero, Tomoyuki Ichiba",
        "title": "Optimal investment with insider information using Skorokhod &\n  Russo-Vallois integration",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We study the maximization of the logarithmic utility of an insider with\ndifferent anticipating techniques. Our aim is to compare the usage of the\nforward and Skorokhod integrals in this context with multiple assets. We show\ntheoretically and with simulations that the Skorokhod insider always overcomes\nthe forward insider, just the opposite of what happens in the case of\nrisk-neutral traders. Moreover, an ordinary trader might overcome both insiders\nif there is a large enough negative fluctuation in the driving stochastic\nprocess that leads to a negative enough final value. Our results point to the\nfact that the interplay between anticipating stochastic calculus and nonlinear\nutilities might yield non-intuitive results from the financial viewpoint.\n"
    },
    {
        "paper_id": 2211.07564,
        "authors": "Axel A. Araneda",
        "title": "Credit Default Swaps and the mixed-fractional CEV model",
        "comments": "5 pages, 1 figure, 1 table",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper explores the capabilities of the Constant Elasticity of Variance\nmodel driven by a mixed-fractional Brownian motion (mfCEV) [Axel A. Araneda.\nThe fractional and mixed-fractional CEV model. Journal of Computational and\nApplied Mathematics, 363:106-123, 2020] to address default-related financial\nproblems, particularly the pricing of Credit Default Swaps. The increase in\nboth, the probability of default and the CDS spreads under mixed-fractional\ndiffusion compared to the standard Brownian case, improves the lower empirical\nperformance of the standard Constant Elasticity of Variance model (CEV),\nyielding a more realistic model for credit events.\n"
    },
    {
        "paper_id": 2211.07622,
        "authors": "Ryan Donnelly, Sebastian Jaimungal",
        "title": "Exploratory Control with Tsallis Entropy for Latent Factor Models",
        "comments": null,
        "journal-ref": "SIAM J. Financial Mathematisc, Forthcoming, 2023",
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  We study optimal control in models with latent factors where the agent\ncontrols the distribution over actions, rather than actions themselves, in both\ndiscrete and continuous time. To encourage exploration of the state space, we\nreward exploration with Tsallis Entropy and derive the optimal distribution\nover states - which we prove is $q$-Gaussian distributed with location\ncharacterized through the solution of an FBS$\\Delta$E and FBSDE in discrete and\ncontinuous time, respectively. We discuss the relation between the solutions of\nthe optimal exploration problems and the standard dynamic optimal control\nsolution. Finally, we develop the optimal policy in a model-agnostic setting\nalong the lines of soft $Q$-learning. The approach may be applied in, e.g.,\ndeveloping more robust statistical arbitrage trading strategies.\n"
    },
    {
        "paper_id": 2211.07765,
        "authors": "Svetlana Boyarchenko and Sergei Levendorski\\u{i}",
        "title": "Efficient evaluation of double-barrier options and joint cpdf of a\n  L\\'evy process and its two extrema",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In the paper, we develop a very fast and accurate method for pricing double\nbarrier options with continuous monitoring in wide classes of L\\'evy models;\nthe calculations are in the dual space, and the Wiener-Hopf factorization is\nused. For wide regions in the parameter space, the precision of the order of\n$10^{-15}$ is achievable in seconds, and of the order of $10^{-9}-10^{-8}$ - in\nfractions of a second. The Wiener-Hopf factors and repeated integrals in the\npricing formulas are calculated using sinh-deformations of the lines of\nintegration, the corresponding changes of variables and the simplified\ntrapezoid rule. If the Bromwich integral is calculated using the Gaver-Wynn Rho\nacceleration instead of the sinh-acceleration, the CPU time is typically\nsmaller but the precision is of the order of $10^{-9}-10^{-6}$, at best.\nExplicit pricing algorithms and numerical examples are for no-touch options,\ndigitals (equivalently, for the joint distribution function of a L\\'evy process\nand its supremum and infimum processes), and call options. Several graphs are\nproduced to explain fundamental difficulties for accurate pricing of barrier\noptions using time discretization and interpolation-based calculations in the\nstate space.\n"
    },
    {
        "paper_id": 2211.07956,
        "authors": "Youru Li, Zhenfeng Zhu, Xiaobo Guo, Shaoshuai Li, Yuchen Yang and Yao\n  Zhao",
        "title": "HGV4Risk: Hierarchical Global View-guided Sequence Representation\n  Learning for Risk Prediction",
        "comments": "12 pages, 10 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  Risk prediction, as a typical time series modeling problem, is usually\nachieved by learning trends in markers or historical behavior from sequence\ndata, and has been widely applied in healthcare and finance. In recent years,\ndeep learning models, especially Long Short-Term Memory neural networks\n(LSTMs), have led to superior performances in such sequence representation\nlearning tasks. Despite that some attention or self-attention based models with\ntime-aware or feature-aware enhanced strategies have achieved better\nperformance compared with other temporal modeling methods, such improvement is\nlimited due to a lack of guidance from global view. To address this issue, we\npropose a novel end-to-end Hierarchical Global View-guided (HGV) sequence\nrepresentation learning framework. Specifically, the Global Graph Embedding\n(GGE) module is proposed to learn sequential clip-aware representations from\ntemporal correlation graph at instance level. Furthermore, following the way of\nkey-query attention, the harmonic $\\beta$-attention ($\\beta$-Attn) is also\ndeveloped for making a global trade-off between time-aware decay and\nobservation significance at channel level adaptively. Moreover, the\nhierarchical representations at both instance level and channel level can be\ncoordinated by the heterogeneous information aggregation under the guidance of\nglobal view. Experimental results on a benchmark dataset for healthcare risk\nprediction, and a real-world industrial scenario for Small and Mid-size\nEnterprises (SMEs) credit overdue risk prediction in MYBank, Ant Group, have\nillustrated that the proposed model can achieve competitive prediction\nperformance compared with other known baselines.\n"
    },
    {
        "paper_id": 2211.08078,
        "authors": "Valeria Terrones and Richard S.J. Tol",
        "title": "Relevance of financial development and fiscal stability in dealing with\n  disasters in Emerging Economies",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Previous studies show that natural disasters decelerate economic growth, and\nmore so in countries with lower financial development. We confirm these results\nwith more recent data. We are the first to show that fiscal stability reduces\nthe negative economic impact of natural disasters in poorer countries, and that\ncatastrophe bonds have the same effect in richer countries.\n"
    },
    {
        "paper_id": 2211.08281,
        "authors": "Dorien Herremans, Kah Wee Low",
        "title": "Forecasting Bitcoin volatility spikes from whale transactions and\n  CryptoQuant data using Synthesizer Transformer models",
        "comments": "Co-first authors",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  The cryptocurrency market is highly volatile compared to traditional\nfinancial markets. Hence, forecasting its volatility is crucial for risk\nmanagement. In this paper, we investigate CryptoQuant data (e.g. on-chain\nanalytics, exchange and miner data) and whale-alert tweets, and explore their\nrelationship to Bitcoin's next-day volatility, with a focus on extreme\nvolatility spikes. We propose a deep learning Synthesizer Transformer model for\nforecasting volatility. Our results show that the model outperforms existing\nstate-of-the-art models when forecasting extreme volatility spikes for Bitcoin\nusing CryptoQuant data as well as whale-alert tweets. We analysed our model\nwith the Captum XAI library to investigate which features are most important.\nWe also backtested our prediction results with different baseline trading\nstrategies and the results show that we are able to minimize drawdown while\nkeeping steady profits. Our findings underscore that the proposed method is a\nuseful tool for forecasting extreme volatility movements in the Bitcoin market.\n"
    },
    {
        "paper_id": 2211.08405,
        "authors": "Rogelio A. Mancisidor and Kjersti Aas",
        "title": "Multimodal Generative Models for Bankruptcy Prediction Using Textual\n  Data",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Textual data from financial filings, e.g., the Management's Discussion &\nAnalysis (MDA) section in Form 10-K, has been used to improve the prediction\naccuracy of bankruptcy models. In practice, however, we cannot obtain the MDA\nsection for all public companies, which limits the use of MDA data in\ntraditional bankruptcy models, as they need complete data to make predictions.\nThe two main reasons for the lack of MDA are: (i) not all companies are obliged\nto submit the MDA and (ii) technical problems arise when crawling and scrapping\nthe MDA section. To solve this limitation, this research introduces the\nConditional Multimodal Discriminative (CMMD) model that learns multimodal\nrepresentations that embed information from accounting, market, and textual\ndata modalities. The CMMD model needs a sample with all data modalities for\nmodel training. At test time, the CMMD model only needs access to accounting\nand market modalities to generate multimodal representations, which are further\nused to make bankruptcy predictions and to generate words from the missing MDA\nmodality. With this novel methodology, it is realistic to use textual data in\nbankruptcy prediction models, since accounting and market data are available\nfor all companies, unlike textual data. The empirical results of this research\nshow that if financial regulators, or investors, were to use traditional models\nusing MDA data, they would only be able to make predictions for 60% of the\ncompanies. Furthermore, the classification performance of our proposed\nmethodology is superior to that of a large number of traditional classifier\nmodels, taking into account all the companies in our sample.\n"
    },
    {
        "paper_id": 2211.0887,
        "authors": "Amit Chaudhary, Daniele Pinna",
        "title": "A multi-asset, agent-based approach applied to DeFi lending protocol\n  modelling",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  We assess the market risk of the DeFi lending protocols using a multi-asset\nagent-based model to simulate ensembles of users subject to price-driven\nliquidation risk. Our multi-asset methodology shows that the protocol's\nsystemic risk is small under stress and that enough collateral is always\npresent to underwrite active loans. Our simulations use a wide variety of\nhistorical data to model market volatility and run the agent-based simulation\nto show that even if all the assets like ETH, BTC and MATIC increase their\nhourly volatility by more than ten times, the protocol carries less than 0.1\\%\ndefault risk given suggested protocol parameter values for liquidation\nloan-to-value ratio and liquidation incentives.\n"
    },
    {
        "paper_id": 2211.08871,
        "authors": "Jun Zhou, Zhichao Yin, Pengpeng Yue",
        "title": "The impact of access to credit on energy efficiency",
        "comments": null,
        "journal-ref": "Finance Research Letters, 103472 (2022)",
        "doi": "10.1016/j.frl.2022.103472",
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  This paper proposes a brand-new measure of energy efficiency at household\nlevel and explores how it is affected by access to credit. We calculate the\nenergy and carbon intensity of the related sectors, which experience a\nsubstantial decline from 2005 to 2019. Although there is still high inequality\nin energy use and carbon emissions among Chinese households, the energy\nefficiency appears to be improved in long run. Our research further maps the\nrelationship between financial market and energy. The results suggest that\nbroadened access to credit encourages households to improve energy efficiency,\nwith higher energy use and carbon emission.\n"
    },
    {
        "paper_id": 2211.08919,
        "authors": "Jiahao Cui, Qiushi Li, Yuezhi Pen",
        "title": "Efficient implementation of portfolio strategies involving\n  cryptocurrencies and VIX INDEX and Gold",
        "comments": "All authors share co-first authorship",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This research mainly explores the characteristics of different strategies and\nwhether VIX INDEX positively influences the investment portfolio in any period.\nOur portfolio has six significant cryptocurrencies, VIX INDEX and gold. We\nperform parameter estimation on all raw data and bring the two types into\ndifferent investment strategies, complete them effectively according to other\ncharacteristics, and compare the results. At the same time, we make two\ndifferent portfolios, one contains VIX INDEX, and one does not have VIX INDEX.\nWe use different portfolios in different portfolio strategies and find that VIX\nINDEX can positively impact the investment portfolio of cryptocurrencies, no\nmatter in the standard market or the downward market. The research shows that\ngold has the same attributes as VIX INDEX and should have a specific positive\neffect, but no comparative experiment has been done.\n"
    },
    {
        "paper_id": 2211.09176,
        "authors": "Jackson P. Lautier and Vladimir Pozdnyakov and Jun Yan",
        "title": "On the Convergence of Credit Risk in Current Consumer Automobile Loans",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Loan seasoning and inefficient consumer interest rate refinance behavior are\nwell-known for mortgages. Consumer automobile loans, which are collateralized\nloans on a rapidly depreciating asset, have attracted less attention, however.\nWe derive a novel large-sample statistical hypothesis test suitable for loans\nsampled from asset-backed securities to populate a transition matrix between\nrisk bands. We find all current risk bands eventually converge to a super-prime\ncredit, despite remaining underwater. Economically, our results imply borrowers\nforwent \\$1,153-\\$2,327 in potential credit-based savings through delayed\nprepayment. We present an expected present value analysis to derive lender\nrisk-adjusted profitability. Our results appear robust to COVID-19.\n"
    },
    {
        "paper_id": 2211.09205,
        "authors": "Alexandra Lukyanova and Ayaz Zeynalov",
        "title": "Russian Agricultural Industry under Sanction Wars",
        "comments": "8 pages, 7 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The motivation for focusing on economic sanctions is the mixed evidence of\ntheir effectiveness. We assess the role of sanctions on the Russian\ninternational trade flow of agricultural products after 2014. We use a\ndifferences-in-differences model of trade flows data for imported and exported\nagricultural products from 2010 to 2020 in Russia. The main expectation was\nthat the Russian economy would take a hit since it had lost its importers. We\nassess the economic impact of the Russian food embargo on agricultural\ncommodities, questioning whether it has achieved its objective and resulted in\na window of opportunity for the development of the domestic agricultural\nsector. Our results confirm that the sanctions have significantly impacted\nfoodstuff imports; they have almost halved in the first two years since the\nsanctions were imposed. However, Russia has embarked on a path to reduce\ndependence on food imports and managed self-sufficient agricultural production.\n"
    },
    {
        "paper_id": 2211.09591,
        "authors": "Zhiheng Yi, Xiaoli Chen",
        "title": "Personal Privacy Protection Problems in the Digital Age",
        "comments": "9 pages, 3 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  With the development of Internet technology, the issue of privacy leakage has\nattracted more and more attention from the public. In our daily life, mobile\nphone applications and identity documents that we use may bring the risk of\nprivacy leakage, which had increasingly aroused public concern. The path of\nprivacy protection in the digital age remains to be explored. To explore the\nsource of this risk and how it can be reduced, we conducted this study by using\npersonal experience, collecting data and applying the theory.\n"
    },
    {
        "paper_id": 2211.09968,
        "authors": "Susan Athey and Emil Palikot",
        "title": "Effective and scalable programs to facilitate labor market transitions\n  for women in technology",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We describe the design, implementation, and evaluation of a low-cost\n(approximately $15 per person) and scalable program, called Challenges, aimed\nat aiding women in Poland transition to technology-sector jobs. This program\nhelps participants develop portfolios demonstrating job-relevant competencies.\nWe conduct two independent evaluations, one of the Challenges program and the\nother of a traditional mentoring program -- Mentoring -- where experienced tech\nprofessionals work individually with mentees to support them in their job\nsearch. Exploiting the fact that both programs were oversubscribed, we\nrandomized admissions and measured their impact on the probability of finding a\njob in the technology sector. We estimate that Mentoring increases the\nprobability of finding a technology job within four months from 29% to 42% and\nChallenges from 20% to 29%, and the treatment effects do not attenuate over 12\nmonths. Since both programs are capacity constrained in practice (only 28% of\napplicants can be accommodated), we evaluate the effectiveness of several\nalternative prioritization rules based on applicant characteristics. We find\nthat a policy that selects applicants based on their predicted treatment\neffects increases the average treatment effect across the two programs to 22\npercentage points. We further analyze how alternative prioritization rules\ncompare to the selection that mentors used. We find that mentors selected\napplicants who were more likely to get a tech job even without participating in\nthe program, and the treatment effect for applicants with similar\ncharacteristics to those selected by mentors is about half of the effect\nattainable when participants are prioritized optimally.\n"
    },
    {
        "paper_id": 2211.10232,
        "authors": "Fabien Le Floc'h",
        "title": "On the Bachelier implied volatility at extreme strikes",
        "comments": null,
        "journal-ref": null,
        "doi": "10.1002/wilm.11076",
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  What kind of implied volatility extrapolation is appropriate? Roger Lee\nproved that the Black-Scholes implied variance can not grow faster than\nlinearly in log-moneyness. This paper investigates what happens in the\nBachelier (or Normal) implied volatility world, making sure to cover the\nvarious aspects of vanilla option arbitrages.\n"
    },
    {
        "paper_id": 2211.10328,
        "authors": "Lennart Adenaw, David Ziegler, Nico Nachtigall, Felix Gotzler,\n  Allister Loder, Markus B. Siewert, Markus Lienkamp, Klaus Bogenberger",
        "title": "A nation-wide experiment: fuel tax cuts and almost free public transport\n  for three months in Germany -- Report 5 Insights into four months of mobility\n  tracking",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
        "abstract": "  In spring 2022, the German federal government agreed on a set of measures\nthat aim at reducing households' financial burden resulting from a recent price\nincrease, especially in energy and mobility. These measures include among\nothers, a nation-wide public transport ticket for 9 EUR per month and a fuel\ntax cut that reduces fuel prices by more than 15%. In transportation research\nthis is an almost unprecedented behavioral experiment. It allows to study not\nonly behavioral responses in mode choice and induced demand but also to assess\nthe effectiveness of transport policy instruments. We observe this natural\nexperiment with a three-wave survey and an app-based travel diary on a sample\nof hundreds of participants as well as an analysis of traffic counts. In this\nfifth report, we present first analyses of the recorded tracking data. 910\nparticipants completed the tracking until September, 30th. First, an overview\nover the socio-demographic characteristics of the participants within our\ntracking sample is given. We observe an adequate representation of female and\nmale participants, a slight over-representation of young participants, and an\nincome distribution similar to the one known from the \"Mobilit\\\"at in\nDeutschland\" survey. Most participants of the tracking study live in Munich,\nGermany. General transportation statistics are derived from the data for all\nphases of the natural experiment - prior, during, and after the 9 EUR-Ticket -\nto assess potential changes in the participants' travel behavior on an\naggregated level. A significant impact of the 9 EUR-Ticket on modal shares can\nbe seen. An analysis of the participants' mobility behavior considering trip\npurposes, age, and income sheds light on how the 9 EUR-Ticket impacts different\nsocial groups and activities. We find that age, income, and trip purpose\nsignificantly influence the impact of the 9 EUR-Ticket on the observed modal\nsplit.\n"
    },
    {
        "paper_id": 2211.10509,
        "authors": "Peter A. Forsyth and Kenneth R. Vetzal and G. Westmacott",
        "title": "Optimal performance of a tontine overlay subject to withdrawal\n  constraints",
        "comments": "arXiv admin note: text overlap with arXiv:2008.06598",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  We consider the holder of an individual tontine retirement account, with\nmaximum and minimum withdrawal amounts (per year) specified. The tontine\naccount holder initiates the account at age 65, and earns mortality credits\nwhile alive, but forfeits all wealth in the account upon death. The holder\ndesires to maximize total withdrawals, and minimize the expected shortfall,\nassuming the holder survives to age 95. The investor controls the amount\nwithdrawn each year and the fraction of the investments in stocks and bonds.\nThe optimal controls are determined based on a parametric model fitted to\nalmost a century of market data. The optimal control algorithm is based on\ndynamic programming and solution of a partial integro differential equation\n(PIDE) using Fourier methods. The optimal strategy (based on the parametric\nmodel) is tested out of sample using stationary block bootstrap resampling of\nthe historical data. In terms of an expected total withdrawal, expected\nshortfall (EW-ES) efficient frontier, the tontine overlay greatly outperforms\nan optimal strategy (without the tontine overlay), which in turn outperforms a\nconstant weight strategy with withdrawals based on the ubiquitous four per cent\nrule.\n"
    },
    {
        "paper_id": 2211.10864,
        "authors": "Santiago Camara, Maximo Sangiacomo",
        "title": "Borrowing Constraints in Emerging Markets",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Borrowing constraints are a key component of modern international\nmacroeconomic models. The analysis of Emerging Markets (EM) economies generally\nassumes collateral borrowing constraints, i.e., firms access to debt is\nconstrained by the value of their collateralized assets. Using credit registry\ndata from Argentina for the period 1998-2020 we show that less than 15% of\nfirms debt is based on the value of collateralized assets, with the remaining\n85% based on firms cash flows. Exploiting central bank regulations over banks\ncapital requirements and credit policies we argue that the most prevalent\nborrowing constraints is defined in terms of the ratio of their interest\npayments to a measure of their present and past cash flows, akin to the\ninterest coverage borrowing constraint studied by the corporate finance\nliterature. Lastly, we argue that EMs exhibit a greater share of interest\nsensitive borrowing constraints than the US and other Advanced Economies. From\na structural point of view, we show that in an otherwise standard small open\neconomy DSGE model, an interest coverage borrowing constraints leads to\nsignificantly stronger amplification of foreign interest rate shocks compared\nto the standard collateral constraint. This greater amplification provides a\nsolution to the Spillover Puzzle of US monetary policy rates by which EMs\nexperience greater negative effects than Advanced Economies after a US interest\nrate hike. In terms of policy implications, this greater amplification leads to\nmanaged exchange rate policy being more costly in the presence of an interest\ncoverage constraint, given their greater interest rate sensitivity, compared to\nthe standard collateral borrowing constraint.\n"
    },
    {
        "paper_id": 2211.10953,
        "authors": "Kiseop Lee, Seongje Lim, Hyungbin Park",
        "title": "Option pricing under path-dependent stock models",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper studies how to price and hedge options under stock models given as\na path-dependent SDE solution. When the path-dependent SDE coefficients have\nFr\\'{e}chet derivatives, an option price is differentiable with respect to time\nand the path, and is given as a solution to the path-dependent PDE. This can be\nregarded as a path-dependent version of the Feynman-Kac formula. As a\nbyproduct, we obtain the differentiability of path-dependent SDE solutions and\nthe SDE representation of their derivatives. In addition, we provide formulas\nfor Greeks with path-dependent coefficient perturbations. A stock model having\ncoefficients with time integration forms of paths is covered as an example.\n"
    },
    {
        "paper_id": 2211.11043,
        "authors": "Dylan Radovic, Lucas Kruitwagen, Christian Schroeder de Witt, Ben\n  Caldecott, Shane Tomlinson, Mark Workman",
        "title": "Revealing Robust Oil and Gas Company Macro-Strategies using Deep\n  Multi-Agent Reinforcement Learning",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The energy transition potentially poses an existential risk for major\ninternational oil companies (IOCs) if they fail to adapt to low-carbon business\nmodels. Projections of energy futures, however, are met with diverging\nassumptions on its scale and pace, causing disagreement among IOC\ndecision-makers and their stakeholders over what the business model of an\nincumbent fossil fuel company should be. In this work, we used deep multi-agent\nreinforcement learning to solve an energy systems wargame wherein players\nsimulate IOC decision-making, including hydrocarbon and low-carbon investments\ndecisions, dividend policies, and capital structure measures, through an\nuncertain energy transition to explore critical and non-linear governance\nquestions, from leveraged transitions to reserve replacements. Adversarial play\nfacilitated by state-of-the-art algorithms revealed decision-making strategies\nrobust to energy transition uncertainty and against multiple IOCs. In all\ngames, robust strategies emerged in the form of low-carbon business models as a\nresult of early transition-oriented movement. IOCs adopting such strategies\noutperformed business-as-usual and delayed transition strategies regardless of\nhydrocarbon demand projections. In addition to maximizing value, these\nstrategies benefit greater society by contributing substantial amounts of\ncapital necessary to accelerate the global low-carbon energy transition. Our\nfindings point towards the need for lenders and investors to effectively\nmobilize transition-oriented finance and engage with IOCs to ensure responsible\nreallocation of capital towards low-carbon business models that would enable\nthe emergence of fossil fuel incumbents as future low-carbon leaders.\n"
    },
    {
        "paper_id": 2211.11322,
        "authors": "Ilaria Perissi and Aled Jones",
        "title": "Influence of Economic Decoupling in assessing carbon budget quotas for\n  the European Union",
        "comments": "41 pages, 17 figure, 21 Tables. This research was funded by PLEDGES\n  project- European Union s Horizon 2020 MSCA IF GA ID: 101023109",
        "journal-ref": "Carbon Management 14 (2023) 1-16",
        "doi": "10.1080/17583004.2023.2217423",
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  In the present study, for the first time, an effort sharing approach based on\nInertia and Capability principles is proposed to assess European Union (EU27)\ncarbon budget distribution among the Member States. This is done within the\ncontext of achieving the Green Deal objective and EU27 carbon neutrality by\n2050. An in-depth analysis is carried out about the role of Economic Decoupling\nembedded in the Capability principle to evaluate the correlation between the\nexpected increase of economic production and the level of carbon intensity in\nthe Member States. As decarbonization is a dynamic process, the study proposes\na simple mathematical model as a policy tool to assess and redistribute Member\nStates carbon budgets as frequently as necessary to encourage progress or\novercome the difficulties each Member State may face during the decarbonization\npathways.\n"
    },
    {
        "paper_id": 2211.11513,
        "authors": "Defu Cao, Yousef El-Laham, Loc Trinh, Svitlana Vyetrenko, Yan Liu",
        "title": "DSLOB: A Synthetic Limit Order Book Dataset for Benchmarking Forecasting\n  Algorithms under Distributional Shift",
        "comments": "11 pages, 5 figures, already accepted by NeurIPS 2022 Distribution\n  Shifts Workshop",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In electronic trading markets, limit order books (LOBs) provide information\nabout pending buy/sell orders at various price levels for a given security.\nRecently, there has been a growing interest in using LOB data for resolving\ndownstream machine learning tasks (e.g., forecasting). However, dealing with\nout-of-distribution (OOD) LOB data is challenging since distributional shifts\nare unlabeled in current publicly available LOB datasets. Therefore, it is\ncritical to build a synthetic LOB dataset with labeled OOD samples serving as a\ntestbed for developing models that generalize well to unseen scenarios. In this\nwork, we utilize a multi-agent market simulator to build a synthetic LOB\ndataset, named DSLOB, with and without market stress scenarios, which allows\nfor the design of controlled distributional shift benchmarking. Using the\nproposed synthetic dataset, we provide a holistic analysis on the forecasting\nperformance of three different state-of-the-art forecasting methods. Our\nresults reflect the need for increased researcher efforts to develop algorithms\nwith robustness to distributional shifts in high-frequency time series data.\n"
    },
    {
        "paper_id": 2211.11691,
        "authors": "Erhan Bayraktar, Qi Feng, and Zhaoyu Zhang",
        "title": "Deep Signature Algorithm for Multi-dimensional Path-Dependent Options",
        "comments": "21 pages, 1 figure",
        "journal-ref": "SIAM Journal on Financial Mathematics. 2024",
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this work, we study the deep signature algorithms for path-dependent\noptions. We extend the backward scheme in [Hur\\'e-Pham-Warin. Mathematics of\nComputation 89, no. 324 (2020)] for state-dependent FBSDEs with reflections to\npath-dependent FBSDEs with reflections, by adding the signature layer to the\nbackward scheme. Our algorithm applies to both European and American type\noption pricing problems while the payoff function depends on the whole paths of\nthe underlying forward stock process. We prove the convergence analysis of our\nnumerical algorithm with explicit dependence on the truncation order of the\nsignature and the neural network approximation errors. Numerical examples for\nthe algorithm are provided including: Amerasian option under the Black-Scholes\nmodel, American option with a path-dependent geometric mean payoff function,\nand the Shiryaev's optimal stopping problem.\n"
    },
    {
        "paper_id": 2211.11803,
        "authors": "Chinonso Nwankwo, Nneka Umeorah, Tony Ware, Weizhong Dai",
        "title": "Deep learning and American options via free boundary framework",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We propose a deep learning method for solving the American options model with\na free boundary feature. To extract the free boundary known as the early\nexercise boundary from our proposed method, we introduce the Landau\ntransformation. For efficient implementation of our proposed method, we further\nconstruct a dual solution framework consisting of a novel auxiliary function\nand free boundary equations. The auxiliary function is formulated to include\nthe feed forward deep neural network (DNN) output and further mimic the far\nboundary behaviour, smooth pasting condition, and remaining boundary conditions\ndue to the second-order space derivative and first-order time derivative.\nBecause the early exercise boundary and its derivative are not a priori known,\nthe boundary values mimicked by the auxiliary function are in approximate form.\nConcurrently, we then establish equations that approximate the early exercise\nboundary and its derivative directly from the DNN output based on some linear\nrelationships at the left boundary. Furthermore, the option Greeks are obtained\nfrom the derivatives of this auxiliary function. We test our implementation\nwith several examples and compare them with the existing numerical methods. All\nindicators show that our proposed deep learning method presents an efficient\nand alternative way of pricing options with early exercise features.\n"
    },
    {
        "paper_id": 2211.11907,
        "authors": "Xiyue Han and Alexander Schied",
        "title": "Robust Faber--Schauder approximation based on discrete observations of\n  an antiderivative",
        "comments": "29 pages, 2 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  We study the problem of reconstructing the Faber--Schauder coefficients of a\ncontinuous function $f$ from discrete observations of its antiderivative $F$.\nOur approach starts with formulating this problem through piecewise quadratic\nspline interpolation. We then provide a closed-form solution and an in-depth\nerror analysis. These results lead to some surprising observations, which also\nthrow new light on the classical topic of quadratic spline interpolation\nitself: They show that the well-known instabilities of this method can be\nlocated exclusively within the final generation of estimated Faber--Schauder\ncoefficients, which suffer from non-locality and strong dependence on the\ninitial value and the given data. By contrast, all other Faber--Schauder\ncoefficients depend only locally on the data, are independent of the initial\nvalue, and admit uniform error bounds. We thus conclude that a robust and\nwell-behaved estimator for our problem can be obtained by simply dropping the\nfinal-generation coefficients from the estimated Faber--Schauder coefficients.\n"
    },
    {
        "paper_id": 2211.11968,
        "authors": "Zhu Xiaoxu and Fan kecai and He hai and Zhang Ziyu",
        "title": "Birth Order and Son Preference to Determine the Children of Shandong\n  Province So Tall",
        "comments": "arXiv admin note: This submission has been withdrawn by arXiv\n  administrators due to inappropriate text overlap with external sources",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  More children in Shandong Province are stunted than any other province in\nChina. Data on more than 122,000 children show a dramatic increase in height\nadvantage with birth order in Shandong relative to the average of other\nprovinces. We suggest that the steep birth order gradient in Shandong is due to\na preference for the eldest child, which influences parental fertility\ndecisions and resource allocation to children. We show that within Shandong\nprovince, the gradient is steeper for regions and cultures with a high\npreference for the eldest child. As predicted, this gradient also varies with\nthe sex of the sibling. By back-calculating, the steeper birth order gradient\nin Shandong Province explains more than half of the average height gap between\nShandong Province and the rest of China.\n"
    },
    {
        "paper_id": 2211.12061,
        "authors": "Wudu Muluneh (1) and Tadesse Amsalu (1), ((1) Institute of Land\n  Administration, Bahir Dar University, Bahir Dar, Ethiopia)",
        "title": "Financing Urban Infrastructure through Land Leasing: Evidence from Bahir\n  Dar City, Ethiopia",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-sa/4.0/",
        "abstract": "  The provision of essential urban infrastructure and services for the\nexpanding population is a persistent financial challenge for many of the\nrapidly expanding cities in developing nations like Ethiopia. The land lease\nsystem has received little academic attention as a means of financing urban\ninfrastructure in developing countries. Therefore, the main objective of this\nstudy is to assess the contribution of land leasing in financing urban\ninfrastructure and services using evidence from Bahir Dar city, Ethiopia.\nPrimary and secondary data-gathering techniques have been used. Descriptive\nstatistics and qualitative analysis have been adopted. The results show land\nlease revenue is a dominant source of extra-budgetary revenue for Bahir Dar\ncity. As evidenced by Bahir Dar city, a significant portion of urban\ninfrastructure expenditure is financed by revenues from land leasing. However,\ndespite the critical importance of land lease revenue to investments in urban\ninfrastructure, there is inefficiency in the collection of potential lease\nrevenue due to weak information exchange, inadequate land provision for various\nuses, lack of transparency in tender committees, and the existence of poor\ndocumentation. Our findings suggest that Bahir Dar City needs to manage lease\nrevenue more effectively to increase investment in urban infrastructure while\ngiving due consideration to availing more land for leasing.\n  Keywords: urban, land, revenue, inefficiency, lease, financing, Bahir Dar\nCity\n"
    },
    {
        "paper_id": 2211.12168,
        "authors": "Yuchen Li, Zongxia Liang, Shunzhi Pang",
        "title": "Continuous-Time Monotone Mean-Variance Portfolio Selection in\n  Jump-Diffusion Model",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We study continuous-time portfolio selection under monotone mean-variance\n(MMV) preferences in a jump-diffusion model, presenting an explicit solution\ndifferent from that under classical mean-variance (MV) preferences in dynamic\nsettings for the first time. We prove that the potential measures calculating\nMMV preferences can be restricted to non-negative Dol\\'eans-Dade exponentials.\nWe find that MMV can resolve the non-monotonicity and free cash flow stream\nproblems of MV when the jump size can be larger than the inverse of the market\nprice of risk. Such result is completely comparable to the earliest result by\nDybvig and Ingersoll. Economically, we show that the essence of MMV lies in the\npricing operator always remaining non-negative, with a value of zero assigned\nwhen the jump exceeds a certain threshold, avoiding the issue of\nnon-monotonicity. As a result, MMV investors behave markedly different from MV\ninvestors. Furthermore, we validate the two-fund separation and establish the\nmonotone capital asset pricing model (monotone CAPM) for MMV investors. We also\nstudy MMV in a constrained trading model and provide three specific numerical\nexamples to show MMV's efficiency. Our finding can serve as a crucial\ntheoretical foundation for future empirical tests of MMV and monotone CAPM's\neffectiveness.\n"
    },
    {
        "paper_id": 2211.12356,
        "authors": "Vishwas Kukreti",
        "title": "Early Warning Signals for Cryptocurrency Market States",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Being archetypal complex systems, financial markets exhibit rich set of\ndynamics in their interactions. In this paper, we focus on the recently evolved\ncryptocurrency market as an example of a complex system and analyse the\nevolution of cross correlation structure of cryptocurrencies in the 5 year\nperiod from 2017 to 2022. We observe characteristic correlation structures in\nthe observation time window duration and use these specific structures to\ncluster the cryptocurrency market in 4 market states.\n"
    },
    {
        "paper_id": 2211.12376,
        "authors": "Vladim\\'ir Hol\\'y",
        "title": "An Intraday GARCH Model for Discrete Price Changes and Irregularly\n  Spaced Observations",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We develop a novel observation-driven model for high-frequency prices. We\naccount for irregularly spaced observations, simultaneous transactions,\ndiscreteness of prices, and market microstructure noise. The relation between\ntrade durations and price volatility, as well as intraday patterns of trade\ndurations and price volatility, is captured using smoothing splines. The\ndynamic model is based on the zero-inflated Skellam distribution with\ntime-varying volatility in a score-driven framework. Market microstructure\nnoise is filtered by including a moving average component. The model is\nestimated by the maximum likelihood method. In an empirical study of the IBM\nstock, we demonstrate that the model provides a good fit to the data. Besides\nmodeling intraday volatility, it can also be used to measure daily realized\nvolatility.\n"
    },
    {
        "paper_id": 2211.12404,
        "authors": "Daniel E. Rigobon and Ronnie Sircar",
        "title": "Formation of Optimal Interbank Lending Networks under Liquidity Shocks",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  We formulate a model of the banking system in which banks control both their\nsupply of liquidity, through cash holdings, and their exposures to risky\ninterbank loans. The value of interbank loans jumps when banks suffer liquidity\nshortages, which can be caused by the arrival of large enough liquidity shocks.\nIn two distinct settings, we compute the unique optimal allocations of capital.\nIn the first, banks seek only to maximize their own utility -- in a\ndecentralized manner. Second, a central planner aims to maximize the sum of all\nbanks' utilities. Both of the resulting financial networks exhibit a\n`core-periphery' structure. However, the optimal allocations differ --\ndecentralized banks are more susceptible to liquidity shortages, while the\nplanner ensures that banks with more debt hold greater liquidity. We\ncharacterize the behavior of the planner's optimal allocation as the size of\nthe system grows. Surprisingly, the `price of anarchy' is of constant order.\nFinally, we derive capitalization requirements that cause the decentralized\nsystem to achieve the planner's level of risk. In doing so, we find that\nsystemically important banks must face the greatest losses when they suffer\nliquidity crises -- ensuring that they are incentivized to avoid such crises.\n"
    },
    {
        "paper_id": 2211.12475,
        "authors": "Xinyu Li",
        "title": "The impact of moving expenses on social segregation: a simulation with\n  RL and ABM",
        "comments": "7 pages with 1 table and 1 figure",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Over the past decades, breakthroughs such as Reinforcement Learning (RL) and\nAgent-based modeling (ABM) have made simulations of economic models feasible.\nRecently, there has been increasing interest in applying ABM to study the\nimpact of residential preferences on neighborhood segregation in the Schelling\nSegregation Model. In this paper, RL is combined with ABM to simulate a\nmodified Schelling Segregation model, which incorporates moving expenses as an\ninput parameter. In particular, deep Q network (DQN) is adopted as RL agents'\nlearning algorithm to simulate the behaviors of households and their\npreferences. This paper studies the impact of moving expenses on the overall\nsegregation pattern and its role in social integration. A more comprehensive\nsimulation of the segregation model is built for policymakers to forecast the\npotential consequences of their policies.\n"
    },
    {
        "paper_id": 2211.12619,
        "authors": "Ebba Mark, Ryan Rafaty, Moritz Schwarz",
        "title": "Spatial-temporal dynamics of employment shocks in declining coal mining\n  regions and potentialities of the 'just transition'",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  The United States, much like other countries around the world, faces\nsignificant obstacles to achieving a rapid decarbonization of its economy.\nCrucially, decarbonization disproportionately affects the communities that have\nbeen historically, politically, and socially embedded in the nation's fossil\nfuel production. However, this effect has rarely been quantified in the\nliterature. Using econometric estimation methods that control for unobserved\nheterogeneity via two-way fixed effects, spatial effects, heterogeneous time\ntrends, and grouped fixed effects, we demonstrate that mine closures induce a\nsignificant and consistent contemporaneous rise in the unemployment rate across\nUS counties. A single mine closure can raise a county's unemployment rate by\n0.056 percentage points in a given year; this effect is amplified by a factor\nof four when spatial econometric dynamics are considered. Although this\nresponse in the unemployment rate fades within 2-3 years, it has far-reaching\neffects in its immediate vicinity. Furthermore, we use cluster analysis to\nbuild a novel typology of coal counties based on qualities that are thought to\nfacilitate a successful recovery in the face of local industrial decline. The\ncombined findings of the econometric analysis and typology point to the\nimportance of investing in alternative sectors in places with promising levels\nof economic diversity, retraining job seekers in places with lower levels of\neducational attainment, providing relocation (or telecommuting) support in\nrural areas, and subsidizing childcare and after school programs in places with\nlow female labor force participation due to the gendered division of domestic\nwork.\n"
    },
    {
        "paper_id": 2211.12652,
        "authors": "J. Mart\\'in Ovejero",
        "title": "Vanna-Volga pricing for single and double barrier FX options",
        "comments": "19 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
        "abstract": "  In this paper, we provide a unified treatment of the Vanna-Volga pricing\ntechnique. We derive the value of single and double barriers FX options, as\nwell as closed formulas for the Delta, Vega, Vanna and Volga of those\ncontracts.\n"
    },
    {
        "paper_id": 2211.12839,
        "authors": "Wei-Chang Yeh, Yu-Hsin Hsieh, Chia-Ling Huang",
        "title": "Newly Developed Flexible Grid Trading Model Combined ANN and SSO\n  algorithm",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/publicdomain/zero/1.0/",
        "abstract": "  In modern society, the trading methods and strategies used in financial\nmarket have gradually changed from traditional on-site trading to electronic\nremote trading, and even online automatic trading performed by a pre-programmed\ncomputer programs because the continuous development of network and computer\ncomputing technology. The quantitative trading, which the main purpose is to\nautomatically formulate people's investment decisions into a fixed and\nquantifiable operation logic that eliminates all emotional interference and the\ninfluence of subjective thoughts and applies this logic to financial market\nactivities in order to obtain excess profits above average returns, has led a\nlot of attentions in financial market. The development of self-adjustment\nprogramming algorithms for automatically trading in financial market has\ntransformed a top priority for academic research and financial practice. Thus,\na new flexible grid trading model combined with the Simplified Swarm\nOptimization (SSO) algorithm for optimizing parameters for various market\nsituations as input values and the fully connected neural network (FNN) and\nLong Short-Term Memory (LSTM) model for training a quantitative trading model\nto automatically calculate and adjust the optimal trading parameters for\ntrading after inputting the existing market situation is developed and studied\nin this work. The proposed model provides a self-adjust model to reduce\ninvestors' effort in the trading market, obtains outperformed investment return\nrate and model robustness, and can properly control the balance between risk\nand return.\n"
    },
    {
        "paper_id": 2211.12892,
        "authors": "Zheng Gong, Wojciech Frys, Renzo Tiranti, Carmine Ventre, John O'Hara,\n  Yingbo Bai",
        "title": "A new encoding of implied volatility surfaces for their synthetic\n  generation",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  In financial terms, an implied volatility surface can be described by its\nterm structure, its skewness and its overall volatility level. We use a PCA\nvariational auto-encoder model to perfectly represent these descriptors into a\nlatent space of three dimensions. Our new encoding brings significant benefits\nfor synthetic surface generation, in that (i) scenario generation is more\ninterpretable; (ii) volatility extrapolation achieve better accuracy; and,\n(iii) we propose a solution to infer implied volatility surfaces of a stock\nfrom an index to which it belongs directly by modelling their relationship on\nthe latent space of the encoding. All these applications, and the latter in\nparticular, have the potential to improve risk management of financial\nderivatives whenever data is scarce.\n"
    },
    {
        "paper_id": 2211.12998,
        "authors": "Iraj Daizadeh",
        "title": "The Impact of US Medical Product Regulatory Complexity on Innovation:\n  Preliminary Evidence of Interdependence, Early Acceleration, and Subsequent\n  Inversion",
        "comments": "53 pages (from Title Page to References), 6 Tables, 9 Figures. Pharm\n  Res (2023)",
        "journal-ref": null,
        "doi": "10.1007/s11095-023-03512-1",
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Is the complexity of medical product (medicines and medical devices)\nregulation impacting innovation in the US? If so, how? Here, this question is\ninvestigated as follows: Various novel proxy metrics of regulation (FDA-issued\nguidelines) and innovation (corresponding FDA-registrations) from 1976-2020 are\nused to determine interdependence, a concept relying on strong correlation and\nreciprocal causality (estimated via variable lag transfer entropy and wavelet\ncoherence). Based on this interdependence, a mapping of regulation onto\ninnovation is conducted and finds that regulation seems to accelerate then\nsupports innovation until on or around 2015; at which time, an inverted U-curve\nemerged. If empirically evidentiary, an important innovation-regulation nexus\nin the US has been reached; and, as such, stakeholders should (re)consider the\ncomplexity of the regulatory landscape to enhance US medical product\ninnovation. Study limitations, extensions, and further thoughts complete this\ninvestigation.\n"
    },
    {
        "paper_id": 2211.13002,
        "authors": "Simon Hirsch, Florian Ziel",
        "title": "Simulation-based Forecasting for Intraday Power Markets: Modelling\n  Fundamental Drivers for Location, Shape and Scale of the Price Distribution",
        "comments": null,
        "journal-ref": "The Energy Journal (45) 3, 2024",
        "doi": "10.5547/01956574.45.3.shir",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  During the last years, European intraday power markets have gained importance\nfor balancing forecast errors due to the rising volumes of intermittent\nrenewable generation. However, compared to day-ahead markets, the drivers for\nthe intraday price process are still sparsely researched. In this paper, we\npropose a modelling strategy for the location, shape and scale parameters of\nthe return distribution in intraday markets, based on fundamental variables. We\nconsider wind and solar forecasts and their intraday updates, outages, price\ninformation and a novel measure for the shape of the merit-order, derived from\nspot auction curves as explanatory variables. We validate our modelling by\nsimulating price paths and compare the probabilistic forecasting performance of\nour model to benchmark models in a forecasting study for the German market. The\napproach yields significant improvements in the forecasting performance,\nespecially in the tails of the distribution. At the same time, we are able to\nderive the contribution of the driving variables. We find that, apart from the\nfirst lag of the price changes, none of our fundamental variables have\nexplanatory power for the expected value of the intraday returns. This implies\nweak-form market efficiency as renewable forecast changes and outage\ninformation seems to be priced in by the market. We find that the volatility is\ndriven by the merit-order regime, the time to delivery and the closure of\ncross-border order books. The tail of the distribution is mainly influenced by\npast price differences and trading activity. Our approach is directly\ntransferable to other continuous intraday markets in Europe.\n"
    },
    {
        "paper_id": 2211.131,
        "authors": "Tomohiro Hirano, Ryo Jinnai, Alexis Akira Toda",
        "title": "Leverage, Endogenous Unbalanced Growth, and Asset Price Bubbles",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We present a general equilibrium macro-finance model with a positive feedback\nloop between capital investment and land price. As leverage is relaxed beyond a\ncritical value, through the financial accelerator, a phase transition occurs\nfrom balanced growth where land prices reflect fundamentals (present value of\nrents) to unbalanced growth where land prices grow faster than rents,\ngenerating land price bubbles. Unbalanced growth dynamics and bubbles are\nassociated with financial loosening and technological progress. In an\nanalytically tractable two-sector large open economy model with unique\nequilibria, financial loosening simultaneously leads to low interest rates,\nasset overvaluation, and top-end wealth concentration.\n"
    },
    {
        "paper_id": 2211.13117,
        "authors": "Mayank Kejriwal and Yuesheng Luo",
        "title": "On the Empirical Association between Trade Network Complexity and Global\n  Gross Domestic Product",
        "comments": "Peer-reviewed and presented at The 11th International Conference on\n  Complex Networks and their Applications (2022)",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  In recent decades, trade between nations has constituted an important\ncomponent of global Gross Domestic Product (GDP), with official estimates\nshowing that it likely accounted for a quarter of total global production.\nWhile evidence of association already exists in macro-economic data between\ntrade volume and GDP growth, there is considerably less work on whether, at the\nlevel of individual granular sectors (such as vehicles or minerals),\nassociations exist between the complexity of trading networks and global GDP.\nIn this paper, we explore this question by using publicly available data from\nthe Atlas of Economic Complexity project to rigorously construct global trade\nnetworks between nations across multiple sectors, and studying the correlation\nbetween network-theoretic measures computed on these networks (such as average\nclustering coefficient and density) and global GDP. We find that there is\nindeed significant association between trade networks' complexity and global\nGDP across almost every sector, and that network metrics also correlate with\nbusiness cycle phenomena such as the Great Recession of 2007-2008. Our results\nshow that trade volume alone cannot explain global GDP growth, and that network\nscience may prove to be a valuable empirical avenue for studying complexity in\nmacro-economic phenomena such as trade.\n"
    },
    {
        "paper_id": 2211.13123,
        "authors": "Song Li, Jiandong Zhou, Chong MO, Jin LI, Geoffrey K. F. Tso, Yuxing\n  Tian",
        "title": "Motif-aware temporal GCN for fraud detection in signed cryptocurrency\n  trust networks",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Graph convolutional networks (GCNs) is a class of artificial neural networks\nfor processing data that can be represented as graphs. Since financial\ntransactions can naturally be constructed as graphs, GCNs are widely applied in\nthe financial industry, especially for financial fraud detection. In this\npaper, we focus on fraud detection on cryptocurrency truct networks. In the\nliterature, most works focus on static networks. Whereas in this study, we\nconsider the evolving nature of cryptocurrency networks, and use local\nstructural as well as the balance theory to guide the training process. More\nspecifically, we compute motif matrices to capture the local topological\ninformation, then use them in the GCN aggregation process. The generated\nembedding at each snapshot is a weighted average of embeddings within a time\nwindow, where the weights are learnable parameters. Since the trust networks is\nsigned on each edge, balance theory is used to guide the training process.\nExperimental results on bitcoin-alpha and bitcoin-otc datasets show that the\nproposed model outperforms those in the literature.\n"
    },
    {
        "paper_id": 2211.13132,
        "authors": "Clint Harris",
        "title": "Interpreting Instrumental Variable Estimands with Unobserved Treatment\n  Heterogeneity: The Effects of College Education",
        "comments": "39 pages, 2 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Many treatment variables used in empirical applications nest multiple\nunobserved versions of a treatment. I show that instrumental variable (IV)\nestimands for the effect of a composite treatment are IV-specific weighted\naverages of effects of unobserved component treatments. Differences between IVs\nin unobserved component compliance produce differences in IV estimands even\nwithout treatment effect heterogeneity. I describe a monotonicity condition\nunder which IV estimands are positively-weighted averages of unobserved\ncomponent treatment effects. Next, I develop a method that allows instruments\nthat violate this condition to contribute to estimation of treatment effects by\nallowing them to place nonconvex, outcome-invariant weights on unobserved\ncomponent treatments across multiple outcomes. Finally, I apply the method to\nestimate returns to college, finding wage returns that range from 7\\% to 30\\%\nover the life cycle. My findings emphasize the importance of leveraging\ninstrumental variables that do not shift individuals between versions of\ntreatment, as well as the importance of policies that encourage students to\nattend \"high-return college\" in addition to those that encourage \"high-return\nstudents\" to attend college.\n"
    },
    {
        "paper_id": 2211.13274,
        "authors": "Amin Izadyar, Shiva Zamani",
        "title": "Investor base and idiosyncratic volatility of cryptocurrencies",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  This paper investigates how changes in investor base is related to\nidiosyncratic volatility in cryptocurrency markets. For each cryptocurrency, we\nset change in its subreddit followers as a proxy for the change in its investor\nbase, and find out that the latter can significantly increase cryptocurrencies\nidiosyncratic volatility. This finding is not subsumed by effects of size,\nmomentum, liquidity and volume and is robust to various measures of\nidiosyncratic volatility.\n"
    },
    {
        "paper_id": 2211.13278,
        "authors": "Shteryo Nozharov and Petya Koralova-Nozharova",
        "title": "The Correlation: minimum wage - unemployment in the conditions of\n  transition to digital economy",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  The research is done in the context of the upcoming introduction of new\nEuropean legislation for the first time for regulation of minimum wage at\nEuropean level. Its purpose is to identify the direction and strength of the\ncorrelation amongst changes of minimum wage and unemployment rate in the\ncontext of conflicting findings of the scientific literature. There will be\nused statistical instruments for the purposes of the analysis as it\nincorporates data for Bulgaria for the period 1991-2021. The significance of\nthe research is related to the transition to digital economy and the necessity\nfor complex transformation of the minimum wage functions in the context of the\nnew socio-economic reality.\n"
    },
    {
        "paper_id": 2211.13777,
        "authors": "Lorenzo Lucchese, Mikko Pakkanen, Almut Veraart",
        "title": "The Short-Term Predictability of Returns in Order Book Markets: a Deep\n  Learning Perspective",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  In this paper, we conduct a systematic large-scale analysis of order\nbook-driven predictability in high-frequency returns by leveraging deep\nlearning techniques. First, we introduce a new and robust representation of the\norder book, the volume representation. Next, we carry out an extensive\nempirical experiment to address various questions regarding predictability. We\ninvestigate if and how far ahead there is predictability, the importance of a\nrobust data representation, the advantages of multi-horizon modeling, and the\npresence of universal trading patterns. We use model confidence sets, which\nprovide a formalized statistical inference framework particularly well suited\nto answer these questions. Our findings show that at high frequencies\npredictability in mid-price returns is not just present, but ubiquitous. The\nperformance of the deep learning models is strongly dependent on the choice of\norder book representation, and in this respect, the volume representation\nappears to have multiple practical advantages.\n"
    },
    {
        "paper_id": 2211.13915,
        "authors": "Aryan Bhambu, Arabin Kumar Dey",
        "title": "Confidence Interval Construction for Multivariate time series using Long\n  Short Term Memory Network",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-sa/4.0/",
        "abstract": "  In this paper we propose a novel procedure to construct a confidence interval\nfor multivariate time series predictions using long short term memory network.\nThe construction uses a few novel block bootstrap techniques. We also propose\nan innovative block length selection procedure for each of these schemes. Two\nnovel benchmarks help us to compare the construction of this confidence\nintervals by different bootstrap techniques. We illustrate the whole\nconstruction through S\\&P $500$ and Dow Jones Index datasets.\n"
    },
    {
        "paper_id": 2211.14075,
        "authors": "Linda Boudjemila, Alexander Bobyl, Vadim Davydov, Vladislav Malyshkin",
        "title": "On a Moving Average with Internal Degrees of Freedom",
        "comments": "arXiv admin note: substantial text overlap with arXiv:2210.04223",
        "journal-ref": "2022 International Conference on Electrical Engineering and\n  Photonics (EExPolytech)",
        "doi": "10.1109/EExPolytech56308.2022.9950893",
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  A new type of moving average is developed. Whereas a regular moving average\n(e.g. of price) has a built-in internal time scale (time-window, exponential\nweight, etc.), the moving average developed in this paper has the weight as the\nproduct of a polynomial by window factor. The polynomial is the square of a\nwavefunction obtained from an eigenproblem corresponding to other observable\n(e.g. execution flow I=dV/dt , the number of shares traded per unit time). This\nallows to obtain an immediate \"switch\" without lagging typical for regular\nmoving average.\n"
    },
    {
        "paper_id": 2211.14219,
        "authors": "Guy Aridor, Duarte Goncalves, Daniel Kluver, Ruoyan Kong, Joseph\n  Konstan",
        "title": "The Economics of Recommender Systems: Evidence from a Field Experiment\n  on MovieLens",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We conduct a field experiment on a movie-recommendation platform to identify\nif and how recommendations affect consumption. We use within-consumer\nrandomization at the good level and elicit beliefs about unconsumed goods to\ndisentangle exposure from informational effects. We find recommendations\nincrease consumption beyond its role in exposing goods to consumers. We provide\nsupport for an informational mechanism: recommendations affect consumers'\nbeliefs, which in turn explain consumption. Recommendations reduce uncertainty\nabout goods consumers are most uncertain about and induce information\nacquisition. Our results highlight the importance of recommender systems'\ninformational role when considering policies targeting these systems in online\nmarketplaces.\n"
    },
    {
        "paper_id": 2211.14431,
        "authors": "Dongli Wu, Bufan Zhang, Xiao Lin",
        "title": "Efficient and Accurate Calibration to FX Market Skew with Fully\n  Parameterized Local Volatility Model",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  When trading American and Asian options in the FX derivatives market, banks\nmust calculate prices using a complex mathematical model. It is often observed\nthat different models produce varying prices for the same exotic option, which\nviolates the non-arbitrage requirement of derivative risk management. To\naddress this issue, we have studied a fully parameterized local volatility\nmodel for pricing American/Asian options. This model, when implemented using a\ngrid or Monte-Carlo numerical method, can be efficiently and accurately\ncalibrated to FX market skew volatilities. As a result, the model can provide\nreliable prices for exotic options during daily trading activities.\n"
    },
    {
        "paper_id": 2211.14634,
        "authors": "Jonathan Libgober and Ruozi Song",
        "title": "Familiarity Facilitates Adoption: Evidence from Electric Vehicles",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper shows that a non-price intervention which increased the prevalence\nof a new technology facilitated its further adoption. The BlueLA program put\nElectric Vehicles (EVs) for public use in many heavily trafficked areas,\nprimarily (but not exclusively) aimed at low-to-middle income households. We\nshow, using data on subsidies for these households and a difference-in\ndifferences strategy, that BlueLA is associated with a 33\\% increase of new EV\nadoptions, justifying a substantial portion of public investment. While the\nprogram provides a substitute to car ownership, our findings are consistent\nwith the hypothesis that increasing familiarity with EVs could facilitate\nadoption.\n"
    },
    {
        "paper_id": 2211.14779,
        "authors": "Zhengjie Huang, Zhenguang Liu, Jianhai Chen, Qinming He, Shuang Wu,\n  Lei Zhu, Meng Wang",
        "title": "Who is Gambling? Finding Cryptocurrency Gamblers Using Multi-modal\n  Retrieval Methods",
        "comments": null,
        "journal-ref": "International Journal of Multimedia Information Retrieval (2022):\n  1-13",
        "doi": "10.1007/s13735-022-00264-3",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  With the popularity of cryptocurrencies and the remarkable development of\nblockchain technology, decentralized applications emerged as a revolutionary\nforce for the Internet. Meanwhile, decentralized applications have also\nattracted intense attention from the online gambling community, with more and\nmore decentralized gambling platforms created through the help of smart\ncontracts. Compared with conventional gambling platforms, decentralized\ngambling have transparent rules and a low participation threshold, attracting a\nsubstantial number of gamblers. In order to discover gambling behaviors and\nidentify the contracts and addresses involved in gambling, we propose a tool\ntermed ETHGamDet. The tool is able to automatically detect the smart contracts\nand addresses involved in gambling by scrutinizing the smart contract code and\naddress transaction records. Interestingly, we present a novel LightGBM model\nwith memory components, which possesses the ability to learn from its own\nmisclassifications. As a side contribution, we construct and release a\nlarge-scale gambling dataset at\nhttps://github.com/AwesomeHuang/Bitcoin-Gambling-Dataset to facilitate future\nresearch in this field. Empirically, ETHGamDet achieves a F1-score of 0.72 and\n0.89 in address classification and contract classification respectively, and\noffers novel and interesting insights.\n"
    },
    {
        "paper_id": 2211.14814,
        "authors": "Jaros{\\l}aw Gruszka, Janusz Szwabi\\'nski",
        "title": "Parameter Estimation of the Heston Volatility Model with Jumps in the\n  Asset Prices",
        "comments": "41 pages, 6 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Parametric estimation of stochastic differential equations (SDEs) has been a\nsubject of intense studies already for several decades. The Heston model for\ninstance is driven by two coupled SDEs and is often used in financial\nmathematics for the dynamics of the asset prices and their volatility.\nCalibrating it to real data would be very useful in many practical scenarios.\nIt is very challenging however, since the volatility is not directly\nobservable. In this paper, a complete estimation procedure of the Heston model\nwithout and with jumps in the asset prices is presented. Bayesian regression\ncombined with the particle filtering method is used as the estimation\nframework. Within the framework, we propose a novel approach to handle jumps in\norder to neutralise their negative impact on the estimates of the key\nparameters of the model. An improvement of the sampling in the particle\nfiltering method is discussed as well. Our analysis is supported by numerical\nsimulations of the Heston model to investigate the performance of the\nestimators. And a practical follow-along recipe is given to allow for finding\nadequate estimates from any given data.\n"
    },
    {
        "paper_id": 2211.14977,
        "authors": "Dev Churiwala, Bhaskar Krishnamachari",
        "title": "QLAMMP: A Q-Learning Agent for Optimizing Fees on Automated Market\n  Making Protocols",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Automated Market Makers (AMMs) have cemented themselves as an integral part\nof the decentralized finance (DeFi) space. AMMs are a type of exchange that\nallows users to trade assets without the need for a centralized exchange. They\nform the foundation for numerous decentralized exchanges (DEXs), which help\nfacilitate the quick and efficient exchange of on-chain tokens. All present-day\npopular DEXs are static protocols, with fixed parameters controlling the fee\nand the curvature - they suffer from invariance and cannot adapt to quickly\nchanging market conditions. This characteristic may cause traders to stay away\nduring high slippage conditions brought about by intractable market movements.\nWe propose an RL framework to optimize the fees collected on an AMM protocol.\nIn particular, we develop a Q-Learning Agent for Market Making Protocols\n(QLAMMP) that learns the optimal fee rates and leverage coefficients for a\ngiven AMM protocol and maximizes the expected fee collected under a range of\ndifferent market conditions. We show that QLAMMP is consistently able to\noutperform its static counterparts under all the simulated test conditions.\n"
    },
    {
        "paper_id": 2211.14978,
        "authors": "Juan E. Jacobo",
        "title": "Back to the Surplus: An Unorthodox Neoclassical Model of Growth,\n  Distribution and Unemployment with Technical Change",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  The article examines how institutions, automation, unemployment and income\ndistribution interact in the context of a neoclassical growth model where\nprofits are interpreted as a surplus over costs of production. Adjusting the\nmodel to the experience of the US economy, I show that joint variations in\nlabor institutions and technology are required to provide reasonable\nexplanations for the behavior of income shares, capital returns, unemployment,\nand the big ratios in macroeconomics. The model offers new perspectives on\nrecent trends by showing that they can be analyzed by the interrelation between\nthe profit-making capacity of capitalist economies and the political\nenvironment determining labor institutions.\n"
    },
    {
        "paper_id": 2211.14997,
        "authors": "Yu Zhao, Huaming Du, Qing Li, Fuzhen Zhuang, Ji Liu, Gang Kou",
        "title": "A Comprehensive Survey on Enterprise Financial Risk Analysis from Big\n  Data Perspective",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Enterprise financial risk analysis aims at predicting the future financial\nrisk of enterprises. Due to its wide and significant application, enterprise\nfinancial risk analysis has always been the core research topic in the fields\nof Finance and Management. Based on advanced computer science and artificial\nintelligence technologies, enterprise risk analysis research is experiencing\nrapid developments and making significant progress. Therefore, it is both\nnecessary and challenging to comprehensively review the relevant studies.\nAlthough there are already some valuable and impressive surveys on enterprise\nrisk analysis from the perspective of Finance and Management, these surveys\nintroduce approaches in a relatively isolated way and lack recent advances in\nenterprise financial risk analysis. In contrast, this paper attempts to provide\na systematic literature survey of enterprise risk analysis approaches from Big\nData perspective, which reviews more than 250 representative articles in the\npast almost 50 years (from 1968 to 2023). To the best of our knowledge, this is\nthe first and only survey work on enterprise financial risk from Big Data\nperspective. Specifically, this survey connects and systematizes the existing\nenterprise financial risk studies, i.e. to summarize and interpret the\nproblems, methods, and spotlights in a comprehensive way. In particular, we\nfirst introduce the issues of enterprise financial risks in terms of their\ntypes,granularity, intelligence, and evaluation metrics, and summarize the\ncorresponding representative works. Then, we compare the analysis methods used\nto learn enterprise financial risk, and finally summarize the spotlights of the\nmost representative works. Our goal is to clarify current cutting-edge research\nand its possible future directions to model enterprise risk, aiming to fully\nunderstand the mechanisms of enterprise risk generation and contagion.\n"
    },
    {
        "paper_id": 2211.1526,
        "authors": "Konstantin H\\\"ausler",
        "title": "ETF construction on CRIX",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  Investments in cryptocurrencies (CCs) remain risky due to high volatility.\nExchange Traded Funds (ETFs) are a suitable tool to diversify risk and to\nbenefit from the growth of the whole CC sector. We construct an ETF on the\nCRIX, the CRyptocurrency IndeX that maps the non-stationary CC dynamics closely\nby adapting its constituents weights dynamically. The scenario analysis\nconsiders the fee schedules of regulated CC exchanges, spreads obtained from\nhigh-frequency order book data, and models capital deposits to the ETF\nstochastically. The analysis yields valuable insights into the mechanisms,\ncosts and risks of this new financial product: i) although the composition of\nthe CRIX ETF changes frequently (from 5 to 30 constituents), it remains robust\nin its core, as the weights of Bitcoin (BTC) and Ethereum (ETH) are robust over\ntime, ii) on average, a portion of 5.2% needed to be rebalanced at the\nrebalancing dates, iii) trading costs are low compared to traditional assets,\niv) the liquidity of the CC sector has increased significantly during the\nanalysis period, spreads occur especially for altcoins and increase by the size\nof the transactions. But since BTC and ETH are most affected by rebalancing,\nthe cost of spreads remains limited.\n"
    },
    {
        "paper_id": 2211.15431,
        "authors": "Zachary Feinstein and Andreas Sojmark",
        "title": "Endogenous distress contagion in a dynamic interbank model: how possible\n  future losses may spell doom today",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We introduce a dynamic and stochastic interbank model with an endogenous\nnotion of distress contagion, arising from rational worries about future\ndefaults and ensuing losses. This entails a mark-to-market valuation adjustment\nfor interbank claims, leading to a forward-backward approach to the equilibrium\ndynamics whereby future default probabilities are needed to determine today's\nbalance sheets. Distinct from earlier models, the resulting distress contagion\nacts, endogenously, as a stochastic volatility term that exhibits clustering\nand down-market spikes. Furthermore, by incorporating multiple maturities, we\nprovide a novel framework for constructing systemic interbank term structures,\nreflecting the intertemporal risk of contagion. We present the analysis in two\nparts: first, the simpler single maturity setting that extends the classical\ninterbank network literature and, then, the multiple maturity setting for which\nwe can examine how systemic risk materialises in the shape of the resulting\nterm structures.\n"
    },
    {
        "paper_id": 2211.15509,
        "authors": "Thomas Blanchet (PSE)",
        "title": "Uncovering the Dynamics of the Wealth Distribution",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  I introduce a new way of decomposing the evolution of the wealth distribution\nusing a simple continuous time stochastic model, which separates the effects of\nmobility, savings, labor income, rates of return, demography, inheritance, and\nassortative mating. Based on two results from stochastic calculus, I show that\nthis decomposition is nonparametrically identified and can be estimated based\nsolely on repeated cross-sections of the data. I estimate it in the United\nStates since 1962 using historical data on income, wealth, and demography. I\nfind that the main drivers of the rise of the top 1% wealth share since the\n1980s have been, in decreasing level of importance, higher savings at the top,\nhigher rates of return on wealth (essentially in the form of capital gains),\nand higher labor income inequality. I then use the model to study the effects\nof wealth taxation. I derive simple formulas for how the tax base reacts to the\nnet-of-tax rate in the long run, which nest insights from several existing\nmodels, and can be calibrated using estimable elasticities. In the benchmark\ncalibration, the revenue-maximizing wealth tax rate at the top is high (around\n12%), but the revenue collected from the tax is much lower than in the static\ncase.\n"
    },
    {
        "paper_id": 2211.15515,
        "authors": "Y. Zhao, C. Huang, J. Luo",
        "title": "How to Prepare for the Next Pandemic -- Investigation of Correlation\n  Between Food Prices and COVID-19 From Global and Local Perspectives",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  The coronavirus disease (COVID-19) has caused enormous disruptions to not\nonly the United States, but also the global economy. Due to the pandemic,\nissues in the supply chain and concerns about food shortage drove up the food\nprices. According to the U.S. Bureau of Labor Statistics, the prices for food\nincreased 4.1% and 3.7% over the year ended in August 2020 and August 2021,\nrespectively, while the amount of annual increase in the food prices prior to\nthe COVID-19 pandemic is less than 2.0%. Previous studies show that such kinds\nof exogenous disasters, including the 2011 Tohoku Earthquake, 9/11 terrorist\nattacks, and major infectious diseases, and the resulted unusual food prices\noften led to subsequent changes in people's consumption behaviors. We\nhypothesize that the COVID-19 pandemic causes food price changes and the price\nchanges alter people's grocery shopping behaviors as well. To thoroughly\nexplore this, we formulate our analysis from two different perspectives, by\ncollecting data both globally, from China, Japan, United Kingdom, and United\nStates, and locally, from different groups of people inside the US. In\nparticular, we analyze the trends between food prices and COVID-19 as well as\nbetween food prices and spending, aiming to find out their correlations and the\nlessons for preparing the next pandemic.\n"
    },
    {
        "paper_id": 2211.15531,
        "authors": "Henry Chiu and Rama Cont",
        "title": "A model-free approach to continuous-time finance",
        "comments": null,
        "journal-ref": "Mathematical Finance (2023)",
        "doi": "10.1111/mafi.12370",
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  We present a non-probabilistic, pathwise approach to continuous-time finance\nbased on causal functional calculus. We introduce a definition of\nself-financing, free from any integration concept and show that the value of a\nself-financing portfolio is a pathwise integral (every self-financing strategy\nis a gradient) and that generic domain of functional calculus is inherently\narbitrage-free. We then consider the problem of hedging a path-dependent payoff\nacross a generic set of scenarios. We apply the transition principle of Isaacs\nin differential games and obtain a verification theorem for the optimal\nsolution, which is characterised by a fully non-linear path-dependent equation.\nFor the Asian option, we obtain explicit solution.\n"
    },
    {
        "paper_id": 2211.15573,
        "authors": "Jerome Detemple and Scott Robertson",
        "title": "Dynamic Equilibrium with Insider Information and General Uninformed\n  Agent Utility",
        "comments": "45 pages, 3 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We study a continuous time economy where agents have asymmetric information.\nThe informed agent (``$I$''), at time zero, receives a private signal about the\nrisky assets' terminal payoff $\\Psi(X_T)$, while the uninformed agent (``$U$'')\nhas no private signal. $\\Psi$ is an arbitrary payoff function, and $X$ follows\na time-homogeneous diffusion. Crucially, we allow $U$ to have von\nNeumann-Morgenstern preferences with a general utility function on $(0,\\infty)$\nsatisfying the standard conditions. This extends previous constructions of\nequilibria with asymmetric information used when all agents have exponential\nutilities and enables us to study the impact of $U$'s initial share endowment\non equilibrium. To allow for $U$ to have general preferences, we introduce a\nnew method to prove existence of a partial communication equilibrium (PCE),\nwhere at time $0$, $U$ receives a less-informative signal than $I$. In the\nsingle asset case, this signal is recoverable by viewing the equilibrium price\nprocess over an arbitrarily short period of time, and hence the PCE is a\ndynamic noisy rational expectations equilibrium. Lastly, when $U$ has power\n(constant relative risk aversion) utility, we identify the equilibrium price in\nthe small and large risk aversion limits.\n"
    },
    {
        "paper_id": 2211.15628,
        "authors": "David Itkin, Benedikt Koch, Martin Larsson, Josef Teichmann",
        "title": "Ergodic robust maximization of asymptotic growth under stochastic\n  volatility",
        "comments": "29 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We consider an asymptotic robust growth problem under model uncertainty and\nin the presence of (non-Markovian) stochastic covariance. We fix two inputs\nrepresenting the instantaneous covariance for the asset process $X$, which\ndepends on an additional stochastic factor process $Y$, as well as the\ninvariant density of $X$ together with $Y$. The stochastic factor process $Y$\nhas continuous trajectories but is not even required to be a semimartingale.\nOur setup allows for drift uncertainty in $X$ and model uncertainty for the\nlocal dynamics of $Y$. This work builds upon a recent paper of Kardaras &\nRobertson, where the authors consider an analogous problem, however, without\nthe additional stochastic factor process. Under suitable, quite weak\nassumptions we are able to characterize the robust optimal trading strategy and\nthe robust optimal growth rate. The optimal strategy is shown to be\nfunctionally generated and, remarkably, does not depend on the factor process\n$Y$. Our result provides a comprehensive answer to a question proposed by\nFernholz in 2002. Mathematically, we use a combination of partial differential\nequation (PDE), calculus of variations and generalized Dirichlet form\ntechniques.\n"
    },
    {
        "paper_id": 2211.15912,
        "authors": "Zheng Cao, Raymond Guo, Wenyu Du, Jiayi Gao, Kirill V. Golubnichiy",
        "title": "Optimizing Stock Option Forecasting with the Assembly of Machine\n  Learning Models and Improved Trading Strategies",
        "comments": "11 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  This paper introduced key aspects of applying Machine Learning (ML) models,\nimproved trading strategies, and the Quasi-Reversibility Method (QRM) to\noptimize stock option forecasting and trading results. It presented the\nfindings of the follow-up project of the research \"Application of Convolutional\nNeural Networks with Quasi-Reversibility Method Results for Option\nForecasting\". First, the project included an application of Recurrent Neural\nNetworks (RNN) and Long Short-Term Memory (LSTM) networks to provide a novel\nway of predicting stock option trends. Additionally, it examined the dependence\nof the ML models by evaluating the experimental method of combining multiple ML\nmodels to improve prediction results and decision-making. Lastly, two improved\ntrading strategies and simulated investing results were presented. The Binomial\nAsset Pricing Model with discrete time stochastic process analysis and\nportfolio hedging was applied and suggested an optimized investment\nexpectation. These results can be utilized in real-life trading strategies to\noptimize stock option investment results based on historical data.\n"
    },
    {
        "paper_id": 2211.16071,
        "authors": "Fred Espen Benth and Heidar Eyjolfsson",
        "title": "Robustness of Hilbert space-valued stochastic volatility models",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper we show that Hilbert space-valued stochastic models are robust\nwith respect to perturbation, due to measurement or approximation errors, in\nthe underlying volatility process. Within the class of stochastic volatility\nmodulated Ornstein-Uhlenbeck processes, we quantify the error induced by the\nvolatility in terms of perturbations in the parameters of the volatility\nprocess. We moreover study the robustness of the volatility process itself with\nrespect to finite dimensional approximations of the driving compound Poisson\nprocess and semigroup generator respectively, when considering operator-valued\nBarndorff-Nielsen and Shephard stochastic volatility models. We also give\nresults on square root approximations. In all cases we provide explicit bounds\nfor the induced error in terms of the approximation of the underlying\nparameter. We discuss some applications to robustness of prices of options on\nforwards and volatility.\n"
    },
    {
        "paper_id": 2211.16103,
        "authors": "Sara Salamat, Nima Tavassoli, Behnam Sabeti, Reza Fahmi",
        "title": "Text Representation Enrichment Utilizing Graph based Approaches: Stock\n  Market Technical Analysis Case Study",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Graph neural networks (GNNs) have been utilized for various natural language\nprocessing (NLP) tasks lately. The ability to encode corpus-wide features in\ngraph representation made GNN models popular in various tasks such as document\nclassification. One major shortcoming of such models is that they mainly work\non homogeneous graphs, while representing text datasets as graphs requires\nseveral node types which leads to a heterogeneous schema. In this paper, we\npropose a transductive hybrid approach composed of an unsupervised node\nrepresentation learning model followed by a node classification/edge prediction\nmodel. The proposed model is capable of processing heterogeneous graphs to\nproduce unified node embeddings which are then utilized for node classification\nor link prediction as the downstream task. The proposed model is developed to\nclassify stock market technical analysis reports, which to our knowledge is the\nfirst work in this domain. Experiments, which are carried away using a\nconstructed dataset, demonstrate the ability of the model in embedding\nextraction and the downstream tasks.\n"
    },
    {
        "paper_id": 2211.16151,
        "authors": "Max Berre, Benjamin Le Pendeven",
        "title": "Business-cycles and Cash-on-Market: Pre-money Startup Valuation in the\n  Macroeconomic Environment",
        "comments": null,
        "journal-ref": "2022 Entrepreneurial Finance (ENTFIN) Annual Meeting, Sep 2022,\n  Bath, United Kingdom",
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  How do business-cycles impact startup-valuations? While several studies\nexplore VC startupecosystems and pre-money valuations, relatively-few delve\ndeeper into the role of macro-level economic factors in influencing those\nstartup deals valuations. Using a dataset of 1,089 venturecapital investments\nin European Union and European Economic Area markets, this article examines\nmacroeconomic, cyclical and macro-sectoral influences on VC startups pre-money\nVC valuations. Our findings show that business-cycles impact startup-valuation\nboth directly and indirectly. Beyond DCF factors, startup-valuations are\nimpacted via by business-cycles directly, and via local venture-capital\nmarket-size. By using a Structural Equation Model approach, our findings\ncontribute to entrepreneurship and financial-intermediary literature by\nexploring indirect and endogenous relationship possibilities finding that most\ndeterminants are transmission-channels rather than independent drivers. Our\nfindings effectively tie-together startup-valuations, intermediary markets, and\nmacroeconomic determinants.\n"
    },
    {
        "paper_id": 2211.16159,
        "authors": "Sarah Kaakai (LMM), Anis Matoussi (LMM), Achraf Tamtalini (LMM)",
        "title": "Estimation of Systemic Shortfall Risk Measure using Stochastic\n  Algorithms",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Systemic risk measures were introduced to capture the global risk and the\ncorresponding contagion effects that is generated by an interconnected system\nof financial institutions. To this purpose, two approaches were suggested. In\nthe first one, systemic risk measures can be interpreted as the minimal amount\nof cash needed to secure a system after aggregating individual risks. In the\nsecond approach, systemic risk measures can be interpreted as the minimal\namount of cash that secures a system by allocating capital to each single\ninstitution before aggregating individual risks. Although the theory behind\nthese risk measures has been well investigated by several authors, the\nnumerical part has been neglected so far. In this paper, we use stochastic\nalgorithms schemes in estimating MSRM and prove that the resulting estimators\nare consistent and asymptotically normal. We also test numerically the\nperformance of these algorithms on several examples.\n"
    },
    {
        "paper_id": 2211.16176,
        "authors": "Takeshi Yoshihara and Taisei Kaizoji",
        "title": "Mechanism of information transmission from a spot rate market to\n  crypto-asset markets",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We applied the SVAR-LiNGAM to illustrate the causal relationships between the\nspot exchange rate, and three crypto-asset exchange rates, Bitcoin, Ethereum,\nand Ripple. It was notable that the causal order, the EUR_USD spot\nrate->Bitcoin->Ethereum->Ripple, was obtained by this approach. All the\ninstantaneous effects were strongly positive. Moreover, it was notable that\nBitcoin can influence the EUR_USD spot rate positively with a one-day time lag.\n"
    },
    {
        "paper_id": 2211.16292,
        "authors": "Takuma Matsuda, Suguru Otani",
        "title": "Unified Container Shipping Industry Data From 1966: Freight Rate,\n  Shipping Quantity, Newbuilding, Secondhand, and Scrap Price",
        "comments": "28 pages with 10 pages appendix",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  We construct a new unified panel dataset that combines route-year-level\nfreight rates with shipping quantities for the six major routes and\nindustry-year-level newbuilding, secondhand, and scrap prices from 1966 (the\nbeginning of the industry) to 2009. We offer detailed instructions on how to\nmerge various datasets and validate the data's consistency by industry experts\nand former executives who have historical knowledge and experience. Using this\ndataset, we provide a quantitative and descriptive analysis of the industry\ndynamics known as the container crisis. Finally, we identify structural breaks\nfor each variable to demonstrate the impact of the shipping cartels' collapse.\n"
    },
    {
        "paper_id": 2211.16419,
        "authors": "Alexander Roth, Wolf-Peter Schill",
        "title": "Geographical balancing of wind power decreases storage needs in a 100%\n  renewable European power sector",
        "comments": null,
        "journal-ref": null,
        "doi": "10.1016/j.isci.2023.107074",
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  To reduce greenhouse gas emissions, many countries plan to massively expand\nwind power and solar photovoltaic capacities. These variable renewable energy\nsources require additional flexibility in the power sector. Both geographical\nbalancing enabled by interconnection and electricity storage can provide such\nflexibility. In a 100% renewable energy scenario of twelve central European\ncountries, we investigate how geographical balancing between countries reduces\nthe need for electricity storage. Our principal contribution is to separate and\nquantify the different factors at play. Applying a capacity expansion model and\na factorization method, we disentangle the effect of interconnection on optimal\nstorage capacities through distinct factors: differences in countries' solar PV\nand wind power availability patterns, load profiles, as well as hydropower and\nbioenergy capacity portfolios. Results show that interconnection reduces\nstorage needs by around 30% in contrast to a scenario without interconnection.\nDifferences in wind power profiles between countries explain around 80% of that\neffect.\n"
    },
    {
        "paper_id": 2211.16641,
        "authors": "Zhenkun Zhou and Zikun Song and Tao Ren",
        "title": "Predicting China's CPI by Scanner Big Data",
        "comments": "We have updated the paper with more results",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Scanner big data has potential to construct Consumer Price Index (CPI). This\nwork utilizes the scanner data of supermarket retail sales, which are provided\nby China Ant Business Alliance (CAA), to construct the Scanner-data Food\nConsumer Price Index (S-FCPI) in China, and the index reliability is verified\nby other macro indicators, especially by China's CPI. And not only that, we\nbuild multiple machine learning models based on S-FCPI to quantitatively\npredict the CPI growth rate in months, and qualitatively predict those\ndirections and levels. The prediction models achieve much better performance\nthan the traditional time series models in existing research. This work paves\nthe way to construct and predict price indexes through using scanner big data\nin China. S-FCPI can not only reflect the changes of goods prices in higher\nfrequency and wider geographic dimension than CPI, but also provide a new\nperspective for monitoring macroeconomic operation, predicting inflation and\nunderstanding other economic issues, which is beneficial supplement to China's\nCPI.\n"
    },
    {
        "paper_id": 2211.16643,
        "authors": "Gaurab Aryal and Zhaohui Chen and Yuchi Yao and Chris Yung",
        "title": "Security Issuance, Institutional Investors and Quid Pro Quo",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Securities issuance through intermediaries is subject to agency problems and\ninformational frictions. We examine these effects using SPAC data. We identify\n``premium'' investors whose participation is linked to lower liquidation risk,\nhigher returns, and lower redemption rates, consistent with both informational\nrents and agency frictions. In contrast, ``non-premium'' investors engage in\nnon-agency quid pro quo relationships. Specifically, they receive high returns\nfrom an intermediary (quid) in exchange for a tacit agreement to participate in\nweaker future deals (quo). These relationships serve as insurance for issuers\nand intermediaries, enabling more issuers to access markets.\n"
    },
    {
        "paper_id": 2211.16984,
        "authors": "Filippo Marchesani and Francesca Masciarelli",
        "title": "Crowdfunding as Entrepreneurial Investment: The Role of Local Knowledge\n  Spillover",
        "comments": null,
        "journal-ref": "Exploring Innovation in a Digital World, 2021",
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper explores the role of local knowledge spillover and human capital\nas a driver of crowdfunding investment. The role of territory has already been\nstudied in terms of campaign success, but the impact of territory on the use of\nfinancial sources like equity crowdfunding is not yet known. Using a sample of\n435 equity crowdfunding campaigns in 20 Italian regions during a 4-year period\n(from 2016 to 2019), this paper evaluates the impact of human capital flow on\nthe adoption of crowdfunding campaigns. Our results show that inbound knowledge\nin the region, measured in terms of ability to attract national and\ninternational students, has a significant effect on the adoption of\ncrowdfunding campaigns in the region itself.\n"
    },
    {
        "paper_id": 2211.17005,
        "authors": "Lokman Abbas-Turki, St\\'ephane Cr\\'epey and Bouazza Saadeddine",
        "title": "Pathwise CVA Regressions With Oversimulated Defaults",
        "comments": "This article has been accepted for publication in Mathematical\n  Finance, published by Wiley",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We consider the computation by simulation and neural net regression of\nconditional expectations, or more general elicitable statistics, of functionals\nof processes $(X, Y )$. Here an exogenous component $Y$ (Markov by itself) is\ntime-consuming to simulate, while the endogenous component $X$ (jointly Markov\nwith $Y$) is quick to simulate given $Y$, but is responsible for most of the\nvariance of the simulated payoff. To address the related variance issue, we\nintroduce a conditionally independent, hierarchical simulation scheme, where\nseveral paths of $X$ are simulated for each simulated path of $Y$. We analyze\nthe statistical convergence of the regression learning scheme based on such\nblock-dependent data. We derive heuristics on the number of paths of $Y$ and,\nfor each of them, of $X$, that should be simulated. The resulting algorithm is\nimplemented on a graphics processing unit (GPU) combining Python/CUDA and\nlearning with PyTorch. A CVA case study with a nested Monte Carlo benchmark\nshows that the hierarchical simulation technique is key to the success of the\nlearning approach.\n"
    },
    {
        "paper_id": 2211.17026,
        "authors": "Griselda Deelstra, Lech A. Grzelak, Felix L. Wolf",
        "title": "Accelerated Computations of Sensitivities for xVA",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Exposure simulations are fundamental to many xVA calculations and are a\nnested expectation problem where repeated portfolio valuations create a\nsignificant computational expense. Sensitivity calculations which require\nshocked and unshocked valuations in bump-and-revalue schemes exacerbate the\ncomputational load. A known reduction of the portfolio valuation cost is\nunderstood to be found in polynomial approximations, which we apply in this\narticle to interest rate sensitivities of expected exposures.\n  We consider a method based on the approximation of the shocked and unshocked\nvaluation functions, as well as a novel approach in which the difference\nbetween these functions is approximated. Convergence results are shown, and we\nstudy the choice of interpolation nodes. Numerical experiments with interest\nrate derivatives are conducted to demonstrate the high accuracy and remarkable\ncomputational cost reduction. We further illustrate how the method can be\nextended to more general xVA models using the example of CVA with wrong-way\nrisk.\n"
    },
    {
        "paper_id": 2211.1708,
        "authors": "Linas Nasvytis",
        "title": "Trust and Time Preference: Measuring a Causal Effect in a\n  Random-Assignment Experiment",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Large amounts of evidence suggest that trust levels in a country are an\nimportant determinant of its macroeconomic growth. In this paper, we\ninvestigate one channel through which trust might support economic performance:\nthrough the levels of patience, also known as time preference in the economics\nliterature. Following Gabaix and Laibson (2017), we first argue that time\npreference can be modelled as optimal Bayesian inference based on noisy signals\nabout the future, so that it is affected by the perceived certainty of future\noutcomes. Drawing on neuroscience literature, we argue that the mechanism\nlinking trust and patience could be facilitated by the neurotransmitter\noxytocin. On the one hand, it is a neural correlate of trusting behavior. On\nthe other, it has an impact on the brain's encoding of prediction error, and\ncould therefore increase the perceived certainty of a neural representation of\na future event. The relationship between trust and time preference is tested\nexperimentally using the Trust Game. While the paper does not find a\nsignificant effect of trust on time preference or the levels of certainty, it\nproposes an experimental design that can successfully manipulate people's\nshort-term levels of trust for experimental purposes.\n"
    },
    {
        "paper_id": 2211.17193,
        "authors": "Taylan Kabbani",
        "title": "Metaheuristic Approach to Solve Portfolio Selection Problem",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
        "abstract": "  In this paper, a heuristic method based on TabuSearch and TokenRing Search is\nbeing used in order to solve the Portfolio Optimization Problem. The seminal\nmean-variance model of Markowitz is being considered with the addition of\ncardinality and quantity constraints to better capture the dynamics of the\ntrading procedure, the model becomes an NP-hard problem that can not be solved\nusing an exact method. The combination of three different neighborhood\nrelations is being explored with Tabu Search. In addition, a new constructive\nmethod for the initial solution is proposed. Finally, I show how the proposed\ntechniques perform on public benchmarks\n"
    },
    {
        "paper_id": 2211.1722,
        "authors": "Laura Eslava, Fernando Baltazar-Larios, Bor Reynoso",
        "title": "Maximum Likelihood Estimation for a Markov-Modulated Jump-Diffusion\n  Model",
        "comments": "16 pages, 5 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We propose a method for obtaining maximum likelihood estimates (MLEs) of a\nMarkov-Modulated Jump-Diffusion Model (MMJDM) when the data is a discrete time\nsample of the diffusion process, the jumps follow a Laplace distribution, and\nthe parameters of the diffusion are controlled by a Markov Jump Process (MJP).\nThe data can be viewed as incomplete observation of a model with a tractable\nlikelihood function. Therefore we use the EM-algorithm to obtain MLEs of the\nparameters. We validate our method with simulated data.\n  The motivation for obtaining estimates of this model is that stock prices\nhave distinct drift and volatility at distinct periods of time. The assumption\nis that these phases are modulated by macroeconomic environments whose changes\nare given by discontinuities or jumps in prices. This model improves on the\nstock prices representation of classical models such as the model of Black and\nScholes or Merton's Jump-Diffusion Model (JDM). We fit the model to the stock\nprices of Amazon and Netflix during a 15-years period and use our method to\nestimate the MLEs.\n"
    },
    {
        "paper_id": 2211.17231,
        "authors": "Alessandro Prosperi",
        "title": "A partial stochastic equilibrium model and its limiting behaviour",
        "comments": "arXiv admin note: text overlap with arXiv:1809.05947 by other authors",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The existence of a (partial) market equilibrium price is proved in a\ncomplete, continuous time finite-agent market setting. The economic agents act\nas price takers in a fully competitive setting and maximize exponential utility\nfrom terminal wealth. As the number $N$ of economic agents goes to infinity,\nthe BSDE system of $N$ equations characterizing the equilibrium asset price\ndynamics decouples. Due to the system's symmetry, the influence of the mean\nfield of the agents, conditionally on the common noise, becomes deterministic.\n"
    },
    {
        "paper_id": 2212.00016,
        "authors": "Shah J Miah",
        "title": "Impact of Business Analytics and Decision Support Systems on e-commerce\n  in SMEs",
        "comments": "DataCom Conf",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  With the advancement in the marketing channel, the use of e-commerce has\nincreased tremendously therefore the basic objective of this study is to\nanalyze the impact of business analytics and decision support systems on\ne-commerce in small and medium enterprises. Small and medium enterprises are\nbecoming a priority for economies as by implementing some policies and\nregulations these businesses could encourage gain development on an\ninternational level. The objective of this study is to analyze the impact of\nbusiness analytics and decision support systems on e-commerce in small and\nmedium enterprises that investigate the relationship between business analytics\nand decision support systems in e-commerce businesses. To evaluate the impact\nof both on e-commerce the, descriptive analysis approach is adopted that\nreviews the research of different scholars who adopted different plans and\nstrategies to predict the relationship between e-commerce and business\nanalytics. The study contributes to the literature by examining the impact of\nbusiness analytics in SMEs and provides a comprehensive understanding of its\nrelationship with the decision support system. After analyzing the impact of\nbusiness analytics and decision support system in SMEs, the research also\nhighlights some limitations and provide future recommendations that are helpful\nto overcome these limitations.\n"
    },
    {
        "paper_id": 2212.00018,
        "authors": "Irene Aldridge and Payton Martin",
        "title": "ESG In Corporate Filings: An AI Perspective",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Our main contribution is that we are using AI to discern the key drivers of\nvariation of ESG mentions in the corporate filings. With AI, we are able to\nseparate \"dimensions\" along which the corporate management presents their ESG\npolicies to the world. These dimensions are 1) diversity, 2) hazardous\nmaterials, and 3) greenhouse gasses. We are also able to identify separate\n\"background\" dimensions of unofficial ESG activity in the firms, which provide\nmore color into the firms and their shareholders' thinking about their ESG\nprocesses. We then measure investors' response to the ESG activity \"factors\".\nThe AI techniques presented can assist in building better, more reliable and\nuseful ESG ratings systems.\n"
    },
    {
        "paper_id": 2212.00088,
        "authors": "David Ferreira and Pedro Cunha",
        "title": "Ranking Critical Tools in the Implementation of Lean Six Sigma as an\n  Integrated Management System in Portugal",
        "comments": "7 pages, 3 figures, 1 table",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Lean Six Sigma (LSS) is a comprehensive and powerful strategy for processes\nimprovement and products. There is a cornucopia of tools for its implementation\nand 37 among them were selected to carry out an evaluation based on three\nfactors, namely: Frequency of use of the tool, difficulty in implementing,\nimportance and impact of the tool in the implementation of LSS. An online\nsurvey was conducted with Portuguese consultants and it included questions on\nthe profile, and the companies they worked, as well as the degree of impact of\nthe tools used. Consultants were asked to choose ten tools, ranking them in\norder of importance. The frequencies with which each tool had been cited were\ncounted. A procedure was then developed to identify the know-how of consultants\nto establish a ranking of LSS tools. It was created an ordering list of tools,\nwhich emphasized in: Honshin Kanri, VOC, VSM. The results presented are\nparticularly relevant when is considered the importance of understanding the\nrequirements for a successful implementation of Lean Six Sigma management\nsystem in the organizations.\n"
    },
    {
        "paper_id": 2212.00292,
        "authors": "Brett Hemenway Falk, Gerry Tsoukalas, Niuniu Zhang",
        "title": "Economics of NFTs: The Value of Creator Royalties",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
        "abstract": "  Non-Fungible Tokens (NFTs) promise to revolutionize how content creators\n(e.g., artists) price and sell their work. One core feature of NFTs is the\noption to embed creator royalties which earmark a percentage of future sale\nproceeds to creators, each time their NFTs change hands. As popular as this\nfeature is in practice, its utility is often questioned because buyers, the\nargument goes, simply ``price it in at the time of purchase''. As intuitive as\nthis argument sounds, it is incomplete. We find royalties can add value to\ncreators in at least three distinct ways. (i) Risk sharing: when creators and\nbuyers are risk sensitive, royalties can improve trade by splitting the risks\nassociated with future price volatility; (ii) Dynamic pricing: in the presence\nof information asymmetry, royalties can extract more revenues from\nbetter-informed speculators over time, mimicking the benefits of ``dynamic\npricing''; (iii) Price discrimination: when creators sell multi-unit NFT\ncollections, royalties can better capture value from heterogeneous buyers. Our\nresults suggest creator royalties play an important and sometimes overlooked\nrole in the economics of NFTs.\n"
    },
    {
        "paper_id": 2212.00336,
        "authors": "Philippe Bergault, Louis Bertucci, David Bouba, Olivier Gu\\'eant",
        "title": "Automated Market Makers: Mean-Variance Analysis of LPs Payoffs and\n  Design of Pricing Functions",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  With the emergence of decentralized finance, new trading mechanisms called\nAutomated Market Makers have appeared. The most popular Automated Market Makers\nare Constant Function Market Makers. They have been studied both theoretically\nand empirically. In particular, the concept of impermanent loss has emerged and\nexplains part of the profit and loss of liquidity providers in Constant\nFunction Market Makers. In this paper, we propose another mechanism in which\nprice discovery does not solely rely on liquidity takers but also on an\nexternal exchange rate or price oracle. We also propose to compare the\ndifferent mechanisms from the point of view of liquidity providers by using a\nmean / variance analysis of their profit and loss compared to that of agents\nholding assets outside of Automated Market Makers. In particular, inspired by\nMarkowitz' modern portfolio theory, we manage to obtain an efficient frontier\nfor the performance of liquidity providers in the idealized case of a perfect\noracle. Beyond that idealized case, we show that even when the oracle is lagged\nand in the presence of adverse selection by liquidity takers and systematic\narbitrageurs, optimized oracle-based mechanisms perform better than popular\nConstant Function Market Makers.\n"
    },
    {
        "paper_id": 2212.00391,
        "authors": "Hyungbin Park, Heejun Yeo",
        "title": "Dynamic and static fund separations and their stability for long-term\n  optimal investments",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper investigates dynamic and static fund separations and their\nstability for long-term optimal investments under three model classes. An\ninvestor maximizes the expected utility with constant relative risk aversion\nunder an incomplete market consisting of a safe asset, several risky assets,\nand a single state variable. The state variables in two of the model classes\nfollow a 3/2 process and an inverse Bessel process, respectively. The other\nmarket model has the partially observed state variable modeled as an\nOrnstein-Uhlenbeck state process. We show that the dynamic optimal portfolio of\nthis utility maximization consists of m+3 portfolios: the safe asset, the\nmyopic portfolio, the m time-independent portfolios, and the intertemporal\nportfolio. Over time, the intertemporal portfolio eventually vanishes, leading\nthe dynamic portfolio to converge to m+2 portfolios, referred to as the static\nportfolio. We also prove that the convergence is stable under model parameter\nperturbations. In addition, sensitivities of the intertemporal portfolio with\nrespect to small parameters perturbations also vanish in the long run. The\nconvergence rate for the intertemporal portfolio and its sensitivities are\ncomputed explicitly for the presented models.\n"
    },
    {
        "paper_id": 2212.0064,
        "authors": "Mark Bergen (1), Thomas Bergen (1), Daniel Levy (2) and Rose Semenov\n  (1) ((1) University of Minnesota, (2) Ber-Ilan University, Emory University,\n  ISET at TSU, ICEA, and RCEA)",
        "title": "3 Lessons from Hyperinflationary Periods",
        "comments": "8 pages",
        "journal-ref": "Harvard Business Review, 2022 (Forthcoming)",
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Inflation is painful, for firms, customers, employees, and society. But\ncareful study of periods of hyperinflation point to ways that firms can adapt.\nIn particular, companies need to think about how to change prices regularly and\ncheaply, because constant price changes can ultimately be very, very expensive.\nAnd they should consider how to communicate those price changes to customers.\nProviding clarity and predictability can increase consumer trust and help firms\nin the long run.\n"
    },
    {
        "paper_id": 2212.00674,
        "authors": "Henrik Wachtmeister, Johan Gars and Daniel Spiro",
        "title": "Quantity restrictions and price discounts on Russian oil",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Following Russia's invasion of Ukraine, Western countries have looked for\nways to limit Russia's oil income. This paper considers, theoretically and\nquantitatively, two such options: 1) an export-quantity restriction and 2) a\nforced discount on Russian oil. We build a quantifiable model of the global oil\nmarket and analyze how each of these policies affect: which Russian oil fields\nfall out of production; the global oil supply; and the global oil price. By\nthese statics we derive the effects of the policies on Russian oil profits and\noil-importers' economic surplus. The effects on Russian oil profits are\nsubstantial. In the short run (within the first year), a quantity restriction\nof 20% yields Russian losses of 62 million USD per day, equivalent to 1.2% of\nGDP and 32% of military spending. In the long run (beyond a year) new\ninvestments become unprofitable. Losses rise to 100 million USD per day, 2% of\nGDP and 56% of military spending. A price discount of 20% is even more harmful\nto Russia, yielding losses of 152 million USD per day, equivalent to 3.1% of\nGDP and 85% of military spending in the short run and long run. A price\ndiscount puts generally more burden on Russia and less on importers compared to\na quantity restriction. In fact, a price discount implies net gains for oil\nimporters as it essentially redistributes oil rents from Russia to importers.\nIf the restrictions are expected to last for long, the burden on oil importers\ndecreases. Overall, both policies at all levels imply larger relative losses\nfor Russia than for oil importers (in shares of their GDP). The case for a\nprice discount on Russian oil is thus strong. However, Russia may choose not to\nexport at the discounted price, in which case the price-discount sanction\nbecomes a de facto supply restriction.\n"
    },
    {
        "paper_id": 2212.00934,
        "authors": "Kazufumi Tsuboi",
        "title": "Shifting to Telework and Firms' Location: Does Telework Make Our Society\n  Efficient?",
        "comments": "31 pages, 8 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Although it has been suggested that the shift from on-site work to telework\nwill change the city structure, the mechanism of this change is not clear. This\nstudy clarifies how the location of firms changes when the cost of teleworking\ndecreases and how this affects the urban economy. The two main results obtained\nare as follows. (i) The expansion of teleworking causes firms to be located\ncloser to urban centers or closer to urban fringes. (ii) Teleworking makes\nurban production more efficient and cities more compact. This is the first\npaper to show that two empirical studies can be represented in a unified\ntheoretical model and that existing studies obtained by simulation can be\nexplained analytically.\n"
    },
    {
        "paper_id": 2212.01048,
        "authors": "Damir Filipovi\\'c and Puneet Pasricha",
        "title": "Empirical Asset Pricing via Ensemble Gaussian Process Regression",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  We introduce an ensemble learning method based on Gaussian Process Regression\n(GPR) for predicting conditional expected stock returns given stock-level and\nmacro-economic information. Our ensemble learning approach significantly\nreduces the computational complexity inherent in GPR inference and lends itself\nto general online learning tasks. We conduct an empirical analysis on a large\ncross-section of US stocks from 1962 to 2016. We find that our method dominates\nexisting machine learning models statistically and economically in terms of\nout-of-sample $R$-squared and Sharpe ratio of prediction-sorted portfolios.\nExploiting the Bayesian nature of GPR, we introduce the mean-variance optimal\nportfolio with respect to the predictive uncertainty distribution of the\nexpected stock returns. It appeals to an uncertainty averse investor and\nsignificantly dominates the equal- and value-weighted prediction-sorted\nportfolios, which outperform the S&P 500.\n"
    },
    {
        "paper_id": 2212.01119,
        "authors": "Zbigniew Palmowski and Pawe{\\l} St\\k{e}pniak",
        "title": "Last passage American cancellable option in L\\'evy models",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  We derive the explicit price of the perpetual American put option cancelled\nat the last passage time of the underlying above some fixed level. We assume\nthe asset process is governed by a geometric spectrally negative L\\'evy\nprocess. We show that the optimal exercise time is the first epoch when asset\nprice process drops below an optimal threshold. We perform numerical analysis\nas well considering classical Black-Scholes models and the model where\nlogarithm of the asset price has additional exponential downward shocks. The\nproof is based on some martingale arguments and fluctuation theory of L\\'evy\nprocesses.\n"
    },
    {
        "paper_id": 2212.01267,
        "authors": "Pasquale De Rosa and Valerio Schiavoni",
        "title": "Understanding Cryptocoins Trends Correlations",
        "comments": "8 pages, 4 figures",
        "journal-ref": "In: Distributed Applications and Interoperable Systems. DAIS 2022.\n  Lecture Notes in Computer Science, vol 13272. Springer, Cham (2022)",
        "doi": "10.1007/978-3-031-16092-9_3",
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Crypto-coins (also known as cryptocurrencies) are tradable digital assets.\nNotable examples include Bitcoin, Ether and Litecoin. Ownerships of cryptocoins\nare registered on distributed ledgers (i.e., blockchains). Secure encryption\ntechniques guarantee the security of the transactions (transfers of coins\nacross owners), registered into the ledger. Cryptocoins are exchanged for\nspecific trading prices. While history has shown the extreme volatility of such\ntrading prices across all different sets of crypto-assets, it remains unclear\nwhat and if there are tight relations between the trading prices of different\ncryptocoins. Major coin exchanges (i.e., Coinbase) provide trend correlation\nindicators to coin owners, suggesting possible acquisitions or sells. However,\nthese correlations remain largely unvalidated. In this paper, we shed lights on\nthe trend correlations across a large variety of cryptocoins, by investigating\ntheir coin-price correlation trends over a period of two years. Our\nexperimental results suggest strong correlation patterns between main coins\n(Ethereum, Bitcoin) and alt-coins. We believe our study can support forecasting\ntechniques for time-series modeling in the context of crypto-coins. We release\nour dataset and code to reproduce our analysis to the research community.\n"
    },
    {
        "paper_id": 2212.01315,
        "authors": "Maikon Araujo",
        "title": "Brazilian listed options with discrete dividends and the fast Laplace\n  transform",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The Brazilian stock exchange (B3) has long used a strike-only adjustment to\naccount for dividends in its listed equity options. This adjustment still makes\nit necessary to account for discrete dividends when pricing either calls or\nputs. This work presents a numerical procedure, based on the fast Laplace\ntransform and its inverse, a procedure that can efficiently compute the\nBrazilian listed options' premium and the Greeks delta, gamma, and theta with\nhigh accuracy.\n"
    },
    {
        "paper_id": 2212.01553,
        "authors": "Yujue Wang",
        "title": "Short-term shock, long-lasting payment: Evidence from the Lushan\n  Earthquake",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Abrupt catastrophic events bring business risks into firms. The paper\nintroduces the Great Lushan Earthquake in 2013 in China as an unexpected shock\nto explore the causal effects on public firms in both the long and short term.\nDID-PSM methods are conducted to examine the robustness of causal inference.\nThe identifications and estimations indicate that catastrophic shock\nsignificantly negatively impacts cash flow liquidity and profitability in the\nshort term. Besides, the practical influences on firms' manufacturing and\noperation emerge in the treated group. Firms increase non-business expenditures\nand retained earnings as a financial measure to resist series risk during the\nshock period. As the long-term payment, the decline in production factors,\nparticularly in employment level and the loss in fixed assets, are permanent.\nThe earthquake's comprehensive interactions are also reflected. The recovery\nfrom the disaster would benefit the companies by raising the growth rate of\nR\\&D and enhancing competitiveness through increasing market share, though\nthese effects are temporary. PSM-DID and event study methods are implemented to\ninvestigate the general effects of specific strong earthquakes on local public\nfirms nationwide. Consistent with the Lushan Earthquake, the ratio of cash flow\nto sales dropped drastically and recovered in 3 subsequent semesters. The shock\non sales was transitory, only in the current semester.\n"
    },
    {
        "paper_id": 2212.01557,
        "authors": "Yujue Wang",
        "title": "Compacter networks as a defensive mechanism: How firms clustered during\n  2015 Financial Crisis in China",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The stock market's reaction to the external risk shock is closely related to\nthe cross-shareholding network structure. This paper takes the public\ninformation of listed companies in the A-share securities market as the primary\nsample to study the relationship between the stock return rate, market\nperformance, and network topology before and after China's stock market crash\nin 2015. Data visualization and empirical analysis demonstrate that the return\nrate of stocks is related to the company's traditional business ability and the\nsocial capital brought by cross-holding. Several heteroscedasticity tests and\nendogeneity tests with IV are conducted to support the robustness. The\nstructure of the cross-shareholding network experienced upheaval after the\nshock, even distorting the effects of market value, and assets holding on the\nreturn rate. The enterprises in the entire shareholding network are connected\nmore firmly to overcome systematic external risks. The number of enterprise\nclusters is significantly reduced during the process. Besides, the number of\nnewly established cross-shareholding relationships shows an outbreak, which may\nexplain the rapid maintenance of stability in the financial system. When stable\nclustering is formed before and after a stock crash (rather than when it\noccurs), the clustering coefficient of clear clustering still has an apparent\npositive influence on the return rate of stocks. To sum up, the compacted\nnetwork may prevent the firms from pursuing aggressive earning before the\nfinancial crisis, but would protect firms from suffering relatively high losses\nduring and after the shock.\n"
    },
    {
        "paper_id": 2212.01584,
        "authors": "Morteza Tahami Pour Zarandi, Mehdi Ghasemi Meymandi, Mohammad Hemami",
        "title": "A comprehensive study of cotton price fluctuations using multiple\n  Econometric and LSTM neural network models",
        "comments": "The corresponding author has requested a withdrawal due to the basic\n  editing of the priprint",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  This paper proposes a new coherent model for a comprehensive study of the\ncotton price using econometrics and Long-Short term memory neural network\n(LSTM) methodologies. We call a simple cotton price trend and then assumed\nconjectures in structural method (ARMA), Markov switching dynamic regression,\nsimultaneous equation system, GARCH families procedures, and Artificial Neural\nNetworks that determine the characteristics of cotton price trend duration\n1990-2020. It is established that in the structural method, the best procedure\nis AR (2) by Markov switching estimation. Based on the MS-AR procedure, it\nconcludes that tending to regime change from decreasing trend to an increasing\none is more significant than a reverse mode. The simultaneous equation system\ninvestigates three procedures based on the acreage cotton, value-added, and\nreal cotton price. Finally, prediction with the GARCH families TARCH procedure\nis the best-fitting model, and in the LSTM neural network, the results show an\naccurate prediction by the training-testing method.\n"
    },
    {
        "paper_id": 2212.01591,
        "authors": "Peter K. Friz, William Salkeld, Thomas Wagenhofer",
        "title": "Weak error estimates for rough volatility models",
        "comments": "36 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We consider a class of stochastic processes with rough stochastic volatility,\nexamples of which include the rough Bergomi and rough Stein-Stein model, that\nhave gained considerable importance in quantitative finance. A basic question\nfor such (non-Markovian) models concerns efficient numerical schemes. While\nstrong rates are well understood (order $H$), we tackle here the intricate\nquestion of weak rates.\n  Our main result asserts that the weak rate, for a reasonably large class of\ntest function, is essentially of order $\\min \\{ 3H+\\tfrac12, 1 \\}$ where $H \\in\n(0,1/2]$ is the Hurst parameter of the fractional Brownian motion that\nunderlies the rough volatility process. Interestingly, the phase transitation\nat $H=1/6$ is related to the correlation between the two driving factors, and\nthus gives additional meaning to a quantity already of central importance in\nstochastic volatility modelling. Our results are complemented by a lower bound\nwhich show that the obtained weak rate is indeed optimal.\n"
    },
    {
        "paper_id": 2212.01705,
        "authors": "C. Biliotti, F.J. Bargagli-Stoffi, N. Fraccaroli, M. Puliga, M.\n  Riccaboni",
        "title": "Breaking Down the Lockdown: The Causal Effects of Stay-At-Home Mandates\n  on Uncertainty and Sentiments During the COVID-19 Pandemic",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  We study the causal effects of lockdown measures on uncertainty and sentiment\non Twitter. To this end, we exploit the quasi-experimental framework created by\nthe first COVID-19 lockdown in a high-income economy--the unexpected Italian\nlockdown in February 2020. We measure changes in public sentiment using deep\nlearning and dictionary-based methods on the text of daily tweets geolocated\nwithin and near the locked-down areas, before and after the treatment. We\nclassify tweets into four categories--economics, health, politics, and lockdown\npolicy--to examine how the policy affected emotions heterogeneously. Using a\nstaggered difference-in-differences approach, we show that the lockdown did not\nhave a significantly robust impact on economic uncertainty and sentiment.\nHowever, the policy came at the price of higher uncertainty on health and\npolitics and more negative political sentiments. These results, which are\nrobust to a battery of robustness tests, show that lockdowns have relevant\nnon-health related implications.\n"
    },
    {
        "paper_id": 2212.01719,
        "authors": "Jingruo Sun",
        "title": "Deep Galerkin Method for Mean Field Control Problem",
        "comments": "This submission has been withdrawn by arXiv administrators as the\n  second author was added without their knowledge or consent",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We consider an optimal control problem where the average welfare of weakly\ninteracting agents is of interest. We examine the mean-field control problem as\nthe fluid approximation of the N-agent control problem with the setup of\nfinite-state space, continuous-time, and finite-horizon. The value function of\nthe mean-field control problem is characterized as the unique viscosity\nsolution of a Hamilton-Jacobi-Bellman equation in the simplex. We apply the DGM\nto estimate the value function and the evolution of the distribution. We also\nprove the numerical solution approximated by a neural network converges to the\nanalytical solution.\n"
    },
    {
        "paper_id": 2212.01807,
        "authors": "Damian Kisiel, Denise Gorse",
        "title": "Axial-LOB: High-Frequency Trading with Axial Attention",
        "comments": "7 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Previous attempts to predict stock price from limit order book (LOB) data are\nmostly based on deep convolutional neural networks. Although convolutions offer\nefficiency by restricting their operations to local interactions, it is at the\ncost of potentially missing out on the detection of long-range dependencies.\nRecent studies address this problem by employing additional recurrent or\nattention layers that increase computational complexity. In this work, we\npropose Axial-LOB, a novel fully-attentional deep learning architecture for\npredicting price movements of stocks from LOB data. By utilizing gated\nposition-sensitive axial attention layers our architecture is able to construct\nfeature maps that incorporate global interactions, while significantly reducing\nthe size of the parameter space. Unlike previous works, Axial-LOB does not rely\non hand-crafted convolutional kernels and hence has stable performance under\ninput permutations and the capacity to incorporate additional LOB features. The\neffectiveness of Axial-LOB is demonstrated on a large benchmark dataset,\ncontaining time series representations of millions of high-frequency trading\nevents, where our model establishes a new state of the art, achieving an\nexcellent directional classification performance at all tested prediction\nhorizons.\n"
    },
    {
        "paper_id": 2212.02239,
        "authors": "Borys Koval, Sylvia Fr\\\"uhwirth-Schnatter, Leopold S\\\"ogner",
        "title": "Bayesian Reconciliation of Return Predictability",
        "comments": "48 pages, 10 figures, 6 tables",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This article considers a stable vector autoregressive (VAR) model and\ninvestigates return predictability in a Bayesian context. The VAR system\ncomprises asset returns and the dividend-price ratio as proposed in Cochrane\n(2008), and allows pinning down the question of return predictability to the\nvalue of one particular model parameter. We develop a new shrinkage type prior\nfor this parameter and compare our Bayesian approach to ordinary least squares\nestimation and to the reduced-bias estimator proposed in Amihud and Hurvich\n(2004). A simulation study shows that the Bayesian approach dominates the\nreduced-bias estimator in terms of observed size (false positive) and power\n(false negative). We apply our methodology to annual CRSP value-weighted\nreturns running, respectively, from 1926 to 2004 and from 1953 to 2021. For the\nfirst sample, the Bayesian approach supports the hypothesis of no return\npredictability, while for the second data set weak evidence for predictability\nis observed.\n"
    },
    {
        "paper_id": 2212.02307,
        "authors": "Radu Burlacu (CERAG), Patrice Fontaine (EUROFIDAI), Sonia\n  Jimenez-Garc\\`es",
        "title": "Why do investors buy shares of actively managed equity mutual funds?\n  Considering the Correct Reference Portfolio from an Uninformed Investor's\n  Perspective 1, 2",
        "comments": "Finance, In press",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We use the Grossman \\& Stiglitz (1980) framework to build a reference\nportfolio for uninformed investors and employ this portfolio to assess the\nperformance of actively managed equity mutual funds. We propose an empirical\nmethodology to construct this reference portfolio using the information on\nprices and supply. We show that mutual funds provide, on average, an\ninsignificant alpha of 23 basis points per year when considering this portfolio\nas a reference. With the stock market index as a proxy for the market\nportfolio, the average fund alpha is negative and highly significant, --128\nbasis points per year. The results are robust when considering various subsets\nof funds based on their characteristics and their degree of selectivity. In\nline with rational expectations equilibrium models considering asymmetrically\ninformed investors and partially revealing equilibrium prices, our study\nsupports that active management adds value for uniformed investors.\n"
    },
    {
        "paper_id": 2212.0257,
        "authors": "Eric Luxenberg and Philipp Schiele and Stephen Boyd",
        "title": "Robust Bond Portfolio Construction via Convex-Concave Saddle Point\n  Optimization",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The minimum (worst case) value of a long-only portfolio of bonds, over a\nconvex set of yield curves and spreads, can be estimated by its sensitivities\nto the points on the yield curve. We show that sensitivity based estimates are\nconservative, \\ie, underestimate the worst case value, and that the exact worst\ncase value can be found by solving a tractable convex optimization problem. We\nthen show how to construct a long-only bond portfolio that includes the worst\ncase value in its objective or as a constraint, using convex-concave saddle\npoint optimization.\n"
    },
    {
        "paper_id": 2212.02581,
        "authors": "Anna Costello, Ekaterina Fedorova, Zhijing Jin, Rada Mihalcea",
        "title": "Editing a Woman's Voice",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  Prior work shows that men and women speak with different levels of\nconfidence, though it's often assumed that these differences are innate or are\nlearned in early childhood. Using academic publishing as a setting, we find\nthat language differences across male and female authors are initially\nnegligible: in first drafts of academic manuscripts, men and women write with\nsimilar levels of uncertainty. However, when we trace those early drafts to\ntheir published versions, a substantial gender gap in linguistic uncertainty\narises. That is, women increase their use of cautionary language through the\npublication process more than men. We show this increase in the linguistic\ngender gap varies substantially based on editor assignment. Specifically, our\nauthor-to-editor matched dataset allows us to estimate editor-specific fixed\neffects, capturing how specific editors impact the change in linguistic\nuncertainty for female authors relative to male authors (the editor's\nauthor-gender gap). Editors' author-gender gaps vary widely, and correlate with\nobservable editor characteristics such as societal norms in their\ncountry-of-origin, their work history, and the year that they obtained their\nPhD. Overall, our study suggests that a woman's \"voice\" is partially shaped by\nexternal forces, and it highlights the critical role of editors in shaping how\nfemale academics communicate.\n"
    },
    {
        "paper_id": 2212.02613,
        "authors": "Aditya Kuvalekar",
        "title": "Matching with Incomplete Preferences",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  I study a two-sided marriage market in which agents have incomplete\npreferences -- i.e., they find some alternatives incomparable. The strong\n(weak) core consists of matchings wherein no coalition wants to form a new\nmatch between themselves, leaving some (all) agents better off without harming\nanyone. The strong core may be empty, while the weak core can be too large. I\npropose the concept of the \"compromise core\" -- a nonempty set that sits\nbetween the weak and the strong cores. Similarly, I define the men-(women-)\noptimal core and illustrate its benefit in an application to India's\nengineering college admissions system.\n"
    },
    {
        "paper_id": 2212.02664,
        "authors": "Teevrat Garg, Maulik Jagnani, Hemant K. Pullabhotla",
        "title": "Structural transformation and environmental externalities",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Even as policymakers seek to encourage economic development by addressing\nmisallocation due to frictions in labor markets, the associated production\nexternalities - such as air pollution - remain unexplored. Using a regression\ndiscontinuity design, we show access to rural roads increases agricultural\nfires and particulate emissions. Farm labor exits are a likely mechanism\nresponsible for the increase in agricultural fires: rural roads cause movement\nof workers out of agriculture and induce farmers to use fire - a labor-saving\nbut polluting technology - to clear agricultural residue or to make harvesting\nless labor-intensive. Overall, the adoption of fires due to rural roads\nincreases infant mortality rate by 5.5% in downwind locations.\n"
    },
    {
        "paper_id": 2212.02721,
        "authors": "Jie Zou, Jiashu Lou, Baohua Wang, Sixue Liu",
        "title": "A Novel Deep Reinforcement Learning Based Automated Stock Trading System\n  Using Cascaded LSTM Networks",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  More and more stock trading strategies are constructed using deep\nreinforcement learning (DRL) algorithms, but DRL methods originally widely used\nin the gaming community are not directly adaptable to financial data with low\nsignal-to-noise ratios and unevenness, and thus suffer from performance\nshortcomings. In this paper, to capture the hidden information, we propose a\nDRL based stock trading system using cascaded LSTM, which first uses LSTM to\nextract the time-series features from stock daily data, and then the features\nextracted are fed to the agent for training, while the strategy functions in\nreinforcement learning also use another LSTM for training. Experiments in DJI\nin the US market and SSE50 in the Chinese stock market show that our model\noutperforms previous baseline models in terms of cumulative returns and Sharp\nratio, and this advantage is more significant in the Chinese stock market, a\nmerging market. It indicates that our proposed method is a promising way to\nbuild a automated stock trading system.\n"
    },
    {
        "paper_id": 2212.02841,
        "authors": "Chengyuan Han, Malte Schr\\\"oder, Dirk Witthaut, Philipp C. B\\\"ottcher",
        "title": "Formation of trade networks by economies of scale and product\n  differentiation",
        "comments": "25 pages, 7 figures",
        "journal-ref": "J. Phys. Complex. 4 (2023) 025006",
        "doi": "10.1088/2632-072X/acc91f",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Understanding the structure and formation of networks is a central topic in\ncomplexity science. Economic networks are formed by decisions of individual\nagents and thus not properly described by established random graph models. In\nthis article, we establish a model for the emergence of trade networks that is\nbased on rational decisions of individual agents. The model incorporates key\ndrivers for the emergence of trade, comparative advantage and economic scale\neffects, but also the heterogeneity of agents and the transportation or\ntransaction costs. Numerical simulations show three macroscopically different\nregimes of the emerging trade networks. Depending on the specific\ntransportation costs and the heterogeneity of individual preferences, we find\ncentralized production with a star-like trade network, distributed production\nwith all-to-all trading or local production and no trade. Using methods from\nstatistical mechanics, we provide an analytic theory of the transitions between\nthese regimes and estimates for critical parameters values.\n"
    },
    {
        "paper_id": 2212.02906,
        "authors": "Marc Wildi and Branka Hadji Misheva",
        "title": "A Time Series Approach to Explainability for Neural Nets with\n  Applications to Risk-Management and Fraud Detection",
        "comments": "28 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
        "abstract": "  Artificial intelligence is creating one of the biggest revolution across\ntechnology driven application fields. For the finance sector, it offers many\nopportunities for significant market innovation and yet broad adoption of AI\nsystems heavily relies on our trust in their outputs. Trust in technology is\nenabled by understanding the rationale behind the predictions made. To this\nend, the concept of eXplainable AI emerged introducing a suite of techniques\nattempting to explain to users how complex models arrived at a certain\ndecision. For cross-sectional data classical XAI approaches can lead to\nvaluable insights about the models' inner workings, but these techniques\ngenerally cannot cope well with longitudinal data (time series) in the presence\nof dependence structure and non-stationarity. We here propose a novel XAI\ntechnique for deep learning methods which preserves and exploits the natural\ntime ordering of the data.\n"
    },
    {
        "paper_id": 2212.03092,
        "authors": "Luigi Biagini and Simone Severini",
        "title": "Can Machine Learning discover the determining factors in participation\n  in insurance schemes? A comparative analysis",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Identifying factors that affect participation is key to a successful\ninsurance scheme. This study's challenges involve using many factors that could\naffect insurance participation to make a better forecast.Huge numbers of\nfactors affect participation, making evaluation difficult. These interrelated\nfactors can mask the influence on adhesion predictions, making them\nmisleading.This study evaluated how 66 common characteristics affect insurance\nparticipation choices. We relied on individual farm data from FADN from 2016 to\n2019 with type 1 (Fieldcrops) farming with 10,926 observations.We use three\nMachine Learning (ML) approaches (LASSO, Boosting, Random Forest) compare them\nto the GLM model used in insurance modelling. ML methodologies can use a large\nset of information efficiently by performing the variable selection. A highly\naccurate parsimonious model helps us understand the factors affecting insurance\nparticipation and design better products.ML predicts fairly well despite the\ncomplexity of insurance participation problem. Our results suggest Boosting\nperforms better than the other two ML tools using a smaller set of regressors.\nThe proposed ML tools identify which variables explain participation choice.\nThis information includes the number of cases in which single variables are\nselected and their relative importance in affecting participation.Focusing on\nthe subset of information that best explains insurance participation could\nreduce the cost of designing insurance schemes.\n"
    },
    {
        "paper_id": 2212.03094,
        "authors": "Massimiliano Fessina, Giambattista Albora, Andrea Tacchella and Andrea\n  Zaccaria",
        "title": "Which products activate a product? An explainable machine learning\n  approach",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Tree-based machine learning algorithms provide the most precise assessment of\nthe feasibility for a country to export a target product given its export\nbasket. However, the high number of parameters involved prevents a\nstraightforward interpretation of the results and, in turn, the explainability\nof policy indications. In this paper, we propose a procedure to statistically\nvalidate the importance of the products used in the feasibility assessment. In\nthis way, we are able to identify which products, called explainers,\nsignificantly increase the probability to export a target product in the near\nfuture. The explainers naturally identify a low dimensional representation, the\nFeature Importance Product Space, that enhances the interpretability of the\nrecommendations and provides out-of-sample forecasts of the export baskets of\ncountries. Interestingly, we detect a positive correlation between the\ncomplexity of a product and the complexity of its explainers.\n"
    },
    {
        "paper_id": 2212.03114,
        "authors": "Luigi Biagini",
        "title": "Applications of Machine Learning for the Ratemaking in Agricultural\n  Insurances",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  This paper evaluates Machine Learning (ML) in establishing ratemaking for new\ninsurance schemes. To make the evaluation feasible, we established expected\nindemnities as premiums. Then, we use ML to forecast indemnities using a\nminimum set of variables. The analysis simulates the introduction of an income\ninsurance scheme, the so-called Income Stabilization Tool (IST), in Italy as a\ncase study using farm-level data from the FADN from 2008-2018. We predicted the\nexpected IST indemnities using three ML tools, LASSO, Elastic Net, and\nBoosting, that perform variable selection, comparing with the Generalized\nLinear Model (baseline) usually adopted in insurance investigations.\nFurthermore, Tweedie distribution is implemented to consider the peculiarity\nshape of the indemnities function, characterized by zero-inflated, no-negative\nvalue, and asymmetric fat-tail. The robustness of the results was evaluated by\ncomparing the econometric and economic performance of the models. Specifically,\nML has obtained the best goodness-of-fit than baseline, using a small and\nstable selection of regressors and significantly reducing the gathering cost of\ninformation. However, Boosting enabled it to obtain the best economic\nperformance, balancing the most and most minor risky subjects optimally and\nachieving good economic sustainability. These findings suggest how machine\nlearning can be successfully applied in agricultural insurance.This study\nrepresents one of the first to use ML and Tweedie distribution in agricultural\ninsurance, demonstrating its potential to overcome multiple issues.\n"
    },
    {
        "paper_id": 2212.03355,
        "authors": "Daron Acemoglu and Nicolaj S{\\o}ndergaard M\\\"uhlbach and Andrew J.\n  Scott",
        "title": "The Rise of Age-Friendly Jobs",
        "comments": null,
        "journal-ref": "The Journal of the Economics of Ageing, Volume 23, October 2022,\n  100416",
        "doi": "10.1016/j.jeoa.2022.100416",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In 1990, one in five U.S. workers were aged over 50 years whereas today it is\none in three. One possible explanation for this is that occupations have become\nmore accommodating to the preferences of older workers. We explore this by\nconstructing an \"age-friendliness\" index for occupations. We use Natural\nLanguage Processing to measure the degree of overlap between textual\ndescriptions of occupations and characteristics which define age-friendliness.\nOur index provides an approximation to rankings produced by survey participants\nand has predictive power for the occupational share of older workers. We find\nthat between 1990 and 2020 around three quarters of occupations have seen their\nage-friendliness increase and employment in above-average age-friendly\noccupations has risen by 49 million. However, older workers have not benefited\ndisproportionately from this rise, with substantial gains going to younger\nfemales and college graduates and with male non-college educated workers losing\nout the most. These findings point to the need to frame the rise of\nage-friendly jobs in the context of other labour market trends and\nimperfections. Purely age-based policies are insufficient given both\nheterogeneity amongst older workers as well as similarities between groups of\nolder and younger workers. The latter is especially apparent in the overlapping\nappeal of specific occupational characteristics.\n"
    },
    {
        "paper_id": 2212.03443,
        "authors": "Jiashu Lou, Leyi Cui, Ye Li",
        "title": "Bi-LSTM Price Prediction based on Attention Mechanism",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  With the increasing enrichment and development of the financial derivatives\nmarket, the frequency of transactions is also faster and faster. Due to human\nlimitations, algorithms and automatic trading have recently become the focus of\ndiscussion. In this paper, we propose a bidirectional LSTM neural network based\non an attention mechanism, which is based on two popular assets, gold and\nbitcoin. In terms of Feature Engineering, on the one hand, we add traditional\ntechnical factors, and at the same time, we combine time series models to\ndevelop factors. In the selection of model parameters, we finally chose a\ntwo-layer deep learning network. According to AUC measurement, the accuracy of\nbitcoin and gold is 71.94% and 73.03% respectively. Using the forecast results,\nwe achieved a return of 1089.34% in two years. At the same time, we also\ncompare the attention Bi-LSTM model proposed in this paper with the traditional\nmodel, and the results show that our model has the best performance in this\ndata set. Finally, we discuss the significance of the model and the\nexperimental results, as well as the possible improvement direction in the\nfuture.\n"
    },
    {
        "paper_id": 2212.03454,
        "authors": "Harrison Mateika, Juannan Jia, Linda Lillard, Noah Cronbaugh, and Will\n  Shin",
        "title": "Fallen Angel Bonds Investment and Bankruptcy Predictions Using Manual\n  Models and Automated Machine Learning",
        "comments": "8 pages, 3 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  The primary aim of this research was to find a model that best predicts which\nfallen angel bonds would either potentially rise up back to investment grade\nbonds and which ones would fall into bankruptcy. To implement the solution, we\nthought that the ideal method would be to create an optimal machine learning\nmodel that could predict bankruptcies. Among the many machine learning models\nout there we decided to pick four classification methods: logistic regression,\nKNN, SVM, and NN. We also utilized an automated methods of Google Cloud's\nmachine learning.\n  The results of our model comparisons showed that the models did not predict\nbankruptcies very well on the original data set with the exception of Google\nCloud's machine learning having a high precision score. However, our\nover-sampled and feature selection data set did perform very well. This could\nlikely be due to the model being over-fitted to match the narrative of the\nover-sampled data (as in, it does not accurately predict data outside of this\ndata set quite well). Therefore, we were not able to create a model that we are\nconfident that would predict bankruptcies.\n  However, we were able to find value out of this project in two key ways. The\nfirst is that Google Cloud's machine learning model in every metric and in\nevery data set either outperformed or performed on par with the other models.\nThe second is that we found that utilizing feature selection did not reduce\npredictive power that much. This means that we can reduce the amount of data to\ncollect for future experimentation regarding predicting bankruptcies.\n"
    },
    {
        "paper_id": 2212.03503,
        "authors": "Luigi Biagini, Federico Antonioli, Simone Severini",
        "title": "The impact of CAP subsidies on the productivity of cereal farms in six\n  European countries",
        "comments": "Working Paper May 2022",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Total factor productivity (TFP) is a key determinant of farm development, a\nsector that receives substantial public support. The issue has taken on great\nimportance today, where the conflict in Ukraine has led to repercussions on the\ncereal markets. This paper investigates the effects of different subsidies on\nthe productivity of cereal farms, accounting that farms differ according to the\nlevel of TFP. We relied on a three-step estimation strategy: i) estimation of\nproduction functions, ii) evaluation of TFP, and iii) assessment of the\nrelationship between CAP subsidies and TFP. To overcome multiple endogeneity\nproblems, the System-GMM estimator is adopted. The investigation embraces farms\nin France, Germany, Italy, Poland, Spain and the United Kingdom using the FADN\nsamples from 2008 to 2018. Adding to previous analyses, we compare results from\ndifferent countries and investigate three subsets of farms with varying levels\nof TFP. The outcomes confirm how CAP negatively impacts farm TFP, but the\nextent differs according to the type of subsidies, the six countries and,\nwithin these, among farms with different productivity groups. Therefore there\nis room for policy improvements in order to foster the productivity of cereal\nfarms.\n"
    },
    {
        "paper_id": 2212.03567,
        "authors": "Marco Pangallo, Alberto Aleta, R. Maria del Rio Chanona, Anton\n  Pichler, David Mart\\'in-Corral, Matteo Chinazzi, Fran\\c{c}ois Lafond, Marco\n  Ajelli, Esteban Moro, Yamir Moreno, Alessandro Vespignani, J. Doyne Farmer",
        "title": "The unequal effects of the health-economy tradeoff during the COVID-19\n  pandemic",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  The potential tradeoff between health outcomes and economic impact has been a\nmajor challenge in the policy making process during the COVID-19 pandemic.\nEpidemic-economic models designed to address this issue are either too\naggregate to consider heterogeneous outcomes across socio-economic groups, or,\nwhen sufficiently fine-grained, not well grounded by empirical data. To fill\nthis gap, we introduce a data-driven, granular, agent-based model that\nsimulates epidemic and economic outcomes across industries, occupations, and\nincome levels with geographic realism. The key mechanism coupling the epidemic\nand economic modules is the reduction in consumption demand due to fear of\ninfection. We calibrate the model to the first wave of COVID-19 in the New York\nmetropolitan area, showing that it reproduces key epidemic and economic\nstatistics, and then examine counterfactual scenarios. We find that: (a) both\nhigh fear of infection and strict restrictions similarly harm the economy but\nreduce infections; (b) low-income workers bear the brunt of both the economic\nand epidemic harm; (c) closing non-customer-facing industries such as\nmanufacturing and construction only marginally reduces the death toll while\nconsiderably increasing unemployment; and (d) delaying the start of protective\nmeasures does little to help the economy and worsens epidemic outcomes in all\nscenarios. We anticipate that our model will help designing effective and\nequitable non-pharmaceutical interventions that minimize disruptions in the\nface of a novel pandemic.\n"
    },
    {
        "paper_id": 2212.03931,
        "authors": "Mark Dean, Dilip Ravindran, and J\\\"org Stoye",
        "title": "A Better Test of Choice Overload",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Choice overload - by which larger choice sets are detrimental to a chooser's\nwellbeing - is potentially of great importance to the design of economic\npolicy. Yet the current evidence on its prevalence is inconclusive. We argue\nthat existing tests are likely to be underpowered and hence that choice\noverload may occur more often than the literature suggests. We propose more\npowerful tests based on richer data and characterization theorems for the\nRandom Utility Model. These new approaches come with significant econometric\nchallenges, which we show how to address. We apply our tests to new\nexperimental data and find strong evidence of choice overload that would likely\nbe missed using current approaches.\n"
    },
    {
        "paper_id": 2212.03947,
        "authors": "Laurence Francis Lacey",
        "title": "Macroeconomic evaluation of the growth of the UK economy over the period\n  2000 to 2019",
        "comments": "23 pages, 10 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  An information entropy statistical methodology was used to evaluate the\ngrowth of the UK economy over the period 2000 to 2019, with an emphasis on the\nimpact of labour productivity on gross domestic product (GDP) per capita and\nthe average growth in real wages, during this time period. The growth of the UK\neconomy over the period 2000 to 2019 can be described in terms of three\ndistinct phases: 1) 2000 to 2007 - strong sustained economic growth 2) 2008 to\n2013 - the impact of the international financial crisis, its immediate\naftermath, and period of recovery 3) 2014 to 2019 - weak sustained economic\ngrowth The key determinant of the UK economic performance over this period\nwould appear to the annual rate of growth in labour productivity. It was\nclosely related to the annual rate of growth in GDP per capita, and it was\nsignificantly weaker in the period 2014 to 2019 compared to the period 2000 to\n2007. This also corresponded with a weaker rate of growth in annual average\nreal wages over the period 2014 to 2019 compared to the period 2000 to 2007.\nThroughout the period 2000 to 2019, UK CPI was maintained, on average, at\napproximately 2.1% per annum. More rapid UK economic growth would be expected\nto be achieved by sustained investment in measures that enhance labour\nproductivity, with the further expectation that a sustained improvement in\nlabour productivity would increase the annual rate of growth of UK GDP per\ncapita and average real wages. While the results given in this paper are\nspecific to the UK over the time period 2000 to 2019, the expectation is that\nthe methodology and approach adopted can be applied to quantifying the dynamics\nof any developed economy over any time period.\n"
    },
    {
        "paper_id": 2212.04209,
        "authors": "Prateek Jain, Alberto Garcia Garcia",
        "title": "Quantum classical hybrid neural networks for continuous variable\n  prediction",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-sa/4.0/",
        "abstract": "  Within this decade, quantum computers are predicted to outperform\nconventional computers in terms of processing power and have a disruptive\neffect on a variety of business sectors. It is predicted that the financial\nsector would be one of the first to benefit from quantum computing both in the\nshort and long terms. In this research work we use Hybrid Quantum Neural\nnetworks to present a quantum machine learning approach for Continuous variable\nprediction.\n"
    },
    {
        "paper_id": 2212.04235,
        "authors": "Matthias Feiler",
        "title": "A probabilistic autoencoder for causal discovery",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The paper addresses the problem of finding the causal direction between two\nassociated variables. The proposed solution is to build an autoencoder of their\njoint distribution and to maximize its estimation capacity relative to both the\nmarginal distributions. It is shown that the resulting two capacities cannot,\nin general, be equal. This leads to a new criterion for causal discovery: the\nhigher capacity is consistent with the unconstrained choice of a distribution\nrepresenting the cause while the lower capacity reflects the constraints\nimposed by the mechanism on the distribution of the effect. Estimation capacity\nis defined as the ability of the auto-encoder to represent arbitrary datasets.\nA regularization term forces it to decide which one of the variables to model\nin a more generic way i.e., while maintaining higher model capacity. The causal\ndirection is revealed by the constraints encountered while encoding the data\ninstead of being measured as a property of the data itself. The idea is\nimplemented and tested using a restricted Boltzmann machine.\n"
    },
    {
        "paper_id": 2212.04277,
        "authors": "Alicia von Schenk, Victor Klockmann, Jean-Fran\\c{c}ois Bonnefon, Iyad\n  Rahwan, Nils K\\\"obis",
        "title": "Lie detection algorithms attract few users but vastly increase\n  accusation rates",
        "comments": "Alicia von Schenk and Victor Klockmann share first-authorship",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
        "abstract": "  People are not very good at detecting lies, which may explain why they\nrefrain from accusing others of lying, given the social costs attached to false\naccusations - both for the accuser and the accused. Here we consider how this\nsocial balance might be disrupted by the availability of lie-detection\nalgorithms powered by Artificial Intelligence. Will people elect to use lie\ndetection algorithms that perform better than humans, and if so, will they show\nless restraint in their accusations? We built a machine learning classifier\nwhose accuracy (67\\%) was significantly better than human accuracy (50\\%) in a\nlie-detection task and conducted an incentivized lie-detection experiment in\nwhich we measured participants' propensity to use the algorithm, as well as the\nimpact of that use on accusation rates. We find that the few people (33\\%) who\nelect to use the algorithm drastically increase their accusation rates (from\n25\\% in the baseline condition up to 86% when the algorithm flags a statement\nas a lie). They make more false accusations (18pp increase), but at the same\ntime, the probability of a lie remaining undetected is much lower in this group\n(36pp decrease). We consider individual motivations for using lie detection\nalgorithms and the social implications of these algorithms.\n"
    },
    {
        "paper_id": 2212.04394,
        "authors": "Nicole B\\\"auerle and An Chen",
        "title": "Optimal investment under partial information and robust VaR-type\n  constraint",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper extends the utility maximization literature by combining partial\ninformation and (robust) regulatory constraints. Partial information is\ncharacterized by the fact that the stock price itself is observable by the\noptimizing financial institution, but the outcome of the market price of the\nrisk $\\theta$ is unknown to the institution. The regulator develops either a\ncongruent or distinct perception of the market price of risk in comparison to\nthe financial institution when imposing the Value-at-Risk (VaR) constraint. We\nalso discuss a robust VaR constraint in which the regulator uses a worst-case\nmeasure. The solution to our optimization problem takes the same form as in the\nfull information case: optimal wealth can be expressed as a decreasing function\nof state price density. The optimal wealth is equal to the minimum regulatory\nfinancing requirement in the intermediate economic states. The key distinction\nlies in the fact that the price density in the final state depends on the\noverall evolution of the estimated market price of risk, denoted as\n$\\hat{\\theta}(s)$ or that the upper boundary of the intermediate region\nexhibits stochastic behavior.\n"
    },
    {
        "paper_id": 2212.04425,
        "authors": "Matthew Lorig, Natchanon Suaysom",
        "title": "Explicit Caplet Implied Volatilities for Quadratic Term-Structure Models",
        "comments": "22 pages, 4 figures. arXiv admin note: substantial text overlap with\n  arXiv:2106.04518",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We derive an explicit asymptotic approximation for implied volatilities of\ncaplets under the assumption that the short-rate is described by a generic\nquadratic term-structure model. In addition to providing an asymptotic accuracy\nresult, we perform experiments in order to gauge the numerical accuracy of our\napproximation.\n"
    },
    {
        "paper_id": 2212.04525,
        "authors": "Mykola Pinchuk",
        "title": "Monetary Uncertainty as a Determinant of the Response of Stock Market to\n  Macroeconomic News",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  This paper examines the effect of macroeconomic news announcements (MNA) on\nthe stock market. Stocks exhibit a strong positive response to major MNA: 1\nstandard deviation of MNA surprise causes 11-25 bps higher returns. This\nresponse is highly time-varying and is weaker during periods of high monetary\nuncertainty. I decompose this response into cash flow and risk-free rate\nchannels. 1 standard deviation of good MNA surprise leads to plus 30 bps\nreturns from the cash flow channel and minus 23 bps per 1\\% of monetary\nuncertainty from the risk-free rate channel. Risk-free rate channel is\ntime-varying and is stronger when monetary uncertainty is high. High levels of\nmonetary uncertainty mask the strong positive response of stocks to MNA, which\nexplains why past research failed to detect this relation.\n"
    },
    {
        "paper_id": 2212.04623,
        "authors": "Erhan Bayraktar, Donghan Kim, Abhishek Tilva",
        "title": "Arbitrage theory in a market of stochastic dimension",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
        "abstract": "  This paper studies an equity market of stochastic dimension, where the number\nof assets fluctuates over time. In such a market, we develop the fundamental\ntheorem of asset pricing, which provides the equivalence of the following\nstatements: (i) there exists a supermartingale num\\'eraire portfolio; (ii) each\ndissected market, which is of a fixed dimension between dimensional jumps, has\nlocally finite growth; (iii) there is no arbitrage of the first kind; (iv)\nthere exists a local martingale deflator; (v) the market is viable. We also\npresent the optional decomposition theorem, which characterizes a given\nnonnegative process as the wealth process of some investment-consumption\nstrategy. Furthermore, similar results still hold in an open market embedded in\nthe entire market of stochastic dimension, where investors can only invest in a\nfixed number of large capitalization stocks. These results are developed in an\nequity market model where the price process is given by a piecewise continuous\nsemimartingale of stochastic dimension. Without the continuity assumption on\nthe price process, we present similar results but without explicit\ncharacterization of the num\\'eraire portfolio.\n"
    },
    {
        "paper_id": 2212.04713,
        "authors": "Katharina Oberpriller, Moritz Ritter and Thorsten Schmidt",
        "title": "Robust asymptotic insurance-finance arbitrage",
        "comments": "21 pages, 1 figure",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In most cases, insurance contracts are linked to the financial markets, such\nas through interest rates or equity-linked insurance products. To motivate an\nevaluation rule in these hybrid markets, Artzner et al. (2022) introduced the\nnotion of insurance-finance arbitrage. In this paper we extend their setting by\nincorporating model uncertainty. To this end, we allow statistical uncertainty\nin the underlying dynamics to be represented by a set of priors $\\mathscr{P}$.\nWithin this framework we introduce the notion of robust asymptotic\ninsurance-finance arbitrage and characterize the absence of such strategies in\nterms of the concept of ${Q}\\mathscr{P}$-evaluations. This is a nonlinear\ntwo-step evaluation which guarantees no robust asymptotic insurance-finance\narbitrage. Moreover, the ${Q}\\mathscr{P}$-evaluation dominates all two-step\nevaluations as long as we agree on the set of priors $\\mathscr{P}$ which shows\nthat those two-step evaluations do not allow for robust asymptotic\ninsurance-finance arbitrages. Furthermore, we introduce a doubly stochastic\nmodel under uncertainty for surrender and survival. In this setting, we\ndescribe conditional dependence by means of copulas and illustrate how the\n${Q}\\mathscr{P}$-evaluation can be used for the pricing of hybrid insurance\nproducts.\n"
    },
    {
        "paper_id": 2212.04724,
        "authors": "Jean-Philippe Boussemart, Walter Briec, Raluca Parvulescu, Paola\n  Ravelojaona",
        "title": "$\\Lambda$-Returns to Scale and Individual Minimum Extrapolation\n  Principle",
        "comments": "40 pages, 18 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  This paper proposes to estimate the returns-to-scale of production sets by\nconsidering the individual return of each observed firm through the notion of\n$\\Lambda$-returns to scale assumption. Along this line, the global technology\nis then constructed as the intersection of all the individual technologies.\nHence, an axiomatic foundation is proposed to present the notion of\n$\\Lambda$-returns to scale. This new characterization of the returns-to-scale\nencompasses the definition of $\\alpha$-returns to scale, as a special case as\nwell as the standard non-increasing and non-decreasing returns-to-scale models.\nA non-parametric procedure based upon the goodness of fit approach is proposed\nto assess these individual returns-to-scale. To illustrate this notion of\n$\\Lambda$-returns to scale assumption, an empirical illustration is provided\nbased upon a dataset involving 63 industries constituting the whole American\neconomy over the period 1987-2018.\n"
    },
    {
        "paper_id": 2212.04848,
        "authors": "Shuo Gong, Yijun Hu, Linxiao Wei",
        "title": "Risk measurement of joint risk of portfolios: a liquidity shortfall\n  aspect",
        "comments": "45 pages, 0 figure",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper presents a novel axiomatic framework of measuring the joint risk\nof a portfolio consisting of several financial positions. From the liquidity\nshortfall aspect, we construct a distortion-type risk measure to measure the\njoint risk of portfolios, which we referred to as multivariate distortion joint\nrisk measure, representing the liquidity shortfall caused by the joint risk of\nportfolios. After its fundamental properties have been studied, we\naxiomatically characterize it by proposing a novel set of axioms. Furthermore,\nbased on the representations for multivariate distortion joint risk measures,\nwe also propose a new class of vector-valued multivariate distortion joint risk\nmeasures, as well as with sensible financial interpretation. Their fundamental\nproperties are also investigated. It turns out that this new class is large\nenough, as it can not only induce new vector-valued multivariate risk measures,\nbut also recover some popular vector-valued multivariate risk measures known in\nthe literature with alternative financial interpretation. Examples are given to\nillustrate the proposed multivariate distortion joint risk measures. This paper\nmainly gives some theoretical results, helping one to have an insight look at\nthe measurement of joint risk of portfolios.\n"
    },
    {
        "paper_id": 2212.04974,
        "authors": "Dragos Gorduza, Xiaowen Dong, Stefan Zohren",
        "title": "Understanding stock market instability via graph auto-encoders",
        "comments": "Submitted to Glinda workshop of the Neurips 2022 conference Keywords\n  : Graph Based Learning, Graph Neural Networks, Graph Autoencoder, Stock\n  Market Information, Volatility Forecasting",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Understanding stock market instability is a key question in financial\nmanagement as practitioners seek to forecast breakdowns in asset co-movements\nwhich expose portfolios to rapid and devastating collapses in value. The\nstructure of these co-movements can be described as a graph where companies are\nrepresented by nodes and edges capture correlations between their price\nmovements. Learning a timely indicator of co-movement breakdowns (manifested as\nmodifications in the graph structure) is central in understanding both\nfinancial stability and volatility forecasting. We propose to use the edge\nreconstruction accuracy of a graph auto-encoder (GAE) as an indicator for how\nspatially homogeneous connections between assets are, which, based on financial\nnetwork literature, we use as a proxy to infer market volatility. Our\nexperiments on the S&P 500 over the 2015-2022 period show that higher GAE\nreconstruction error values are correlated with higher volatility. We also show\nthat out-of-sample autoregressive modeling of volatility is improved by the\naddition of the proposed measure. Our paper contributes to the literature of\nmachine learning in finance particularly in the context of understanding stock\nmarket instability.\n"
    },
    {
        "paper_id": 2212.05235,
        "authors": "Shuhua Xiao, Jiali Ma, Li Xia, Shushang Zhu",
        "title": "Optimal Systemic Risk Bailout: A PGO Approach Based on Neural Network",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The bailout strategy is crucial to cushion the massive loss caused by\nsystemic risk in the financial system. There is no closed-form formulation of\nthe optimal bailout problem, making solving it difficult. In this paper, we\nregard the issue of the optimal bailout (capital injection) as a black-box\noptimization problem, where the black box is characterized as a fixed-point\nsystem that follows the E-N framework for measuring the systemic risk of the\nfinancial system. We propose the so-called ``Prediction-Gradient-Optimization''\n(PGO) framework to solve it, where the ``Prediction'' means that the objective\nfunction without a closed-form is approximated and predicted by a neural\nnetwork, the ``Gradient'' is calculated based on the former approximation, and\nthe ``Optimization'' procedure is further implemented within a gradient\nprojection algorithm to solve the problem. Comprehensive numerical simulations\ndemonstrate that the proposed approach is promising for systemic risk\nmanagement.\n"
    },
    {
        "paper_id": 2212.05281,
        "authors": "Maren Springsklee, Fabian Scheller",
        "title": "Exploring non-residential technology adoption: an empirical analysis of\n  factors associated with the adoption of photovoltaic systems by municipal\n  authorities in Germany",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  This research article explores potential influencing factors of solar\nphotovoltaic (PV) system adoption by municipal authorities in Germany in the\nyear 2019. We derive seven hypothesized relationships from the empirical\nliterature on residential PV adoption, organizational technology adoption, and\nsustainability policy adoption by local governments, and apply a twofold\nempirical approach to examine them. First, we explore the associations of a set\nof explanatory variables on the installed capacity of adopter municipalities\n(N=223) in an OLS model. Second, we use a logit model to analyze whether the\nidentified relationships are also apparent between adopter and non-adopter\nmunicipalities (N=423). Our findings suggest that fiscal capacity (measured by\nper capita debt and per capita tax revenue) and peer effects (measured by the\npre-existing installed capacity) are positively associated with both the\ninstalled capacity and adoption. Furthermore, we find that institutional\ncapacity (measured by the presence of a municipal utility) and environmental\nconcern (measured by the share of green party votes) are positively associated\nwith municipal PV adoption. Economic factors (measured by solar irradiation)\nshow a significant positive but small effect in both regression models. No\nevidence was found to support the influence of political will. Results for the\nrole of municipal characteristics are mixed, although the population size was\nconsistently positively associated with municipal PV adoption and installed\ncapacity. Our results support previous studies on PV system adoption\ndeterminants and offer a starting point for additional research on\nnon-residential decision-making and PV adoption.\n"
    },
    {
        "paper_id": 2212.05304,
        "authors": "Kaichen Xu",
        "title": "Estimation and Application of the Convergence Bounds for Nonlinear\n  Markov Chains",
        "comments": "21 pages, 8 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Nonlinear Markov Chains (nMC) are regarded as the original (linear) Markov\nChains with nonlinear small perturbations. It fits real-world data better, but\nits associated properties are difficult to describe. A new approach is proposed\nto analyze the ergodicity and even estimate the convergence bounds of nMC,\nwhich is more precise than existing results. In the new method, Coupling Markov\nabout homogeneous Markov chains is applied to reconstitute the relationship\nbetween distribution at any times and the limiting distribution. The\nconvergence bounds can be provided by the transition probability matrix of\nCoupling Markov. Moreover, a new volatility called TV Volatility can be\ncalculated through the convergence bounds, wavelet analysis and Gaussian HMM.\nIt's tested to estimate the volatility of two securities (TSLA and AMC). The\nresults show TV Volatility can reflect the magnitude of the change of square\nreturns in a period wonderfully.\n"
    },
    {
        "paper_id": 2212.05317,
        "authors": "Giorgio Ferrari and Shihao Zhu",
        "title": "On a Merton Problem with Irreversible Healthcare Investment",
        "comments": "A previous version of this paper circulated under the title\n  \"Consumption decision, portfolio choice and healthcare irreversible\n  investment\". In the current version, some mathematical arguments have been\n  improved, and detailed numerical examples and corresponding economic\n  implications are provided",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  We propose a tractable dynamic framework for the joint determination of\noptimal consumption, portfolio choice, and healthcare irreversible investment.\nOur model is based on a Merton's portfolio and consumption problem, where, in\naddition, the agent can choose the time at which undertaking a costly lump sum\nhealth investment decision. Health depreciates with age and directly affects\nthe agent's mortality force, so that investment into healthcare reduces the\nagent's mortality risk. The resulting optimization problem is formulated as a\nstochastic control-stopping problem with a random time-horizon and\nstate-variables given by the agent's wealth and health capital. We transform\nthis problem into its dual version, which is now a two-dimensional optimal\nstopping problem with interconnected dynamics and finite time-horizon.\nRegularity of the optimal stopping value function is derived and the related\nfree boundary surface is proved to be Lipschitz continuous, and it is\ncharacterized as the unique solution to a nonlinear integral equation, which we\ncompute numerically. In the original coordinates, the agent thus invests into\nhealthcare whenever her wealth exceeds an age- and health-dependent transformed\nversion of the optimal stopping boundary. We also provide the numerical\nillustrations of the optimal strategies and some financial implications are\ndiscussed.\n"
    },
    {
        "paper_id": 2212.05357,
        "authors": "Luyao Zhang, Xinyu Tian",
        "title": "On Blockchain We Cooperate: An Evolutionary Game Perspective",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Cooperation is fundamental for human prosperity. Blockchain, as a trust\nmachine, is a cooperative institution in cyberspace that supports cooperation\nthrough distributed trust with consensus protocols. While studies in computer\nscience focus on fault tolerance problems with consensus algorithms, economic\nresearch utilizes incentive designs to analyze agent behaviors. To achieve\ncooperation on blockchains, emerging interdisciplinary research introduces\nrationality and game-theoretical solution concepts to study the equilibrium\noutcomes of various consensus protocols. However, existing studies do not\nconsider the possibility for agents to learn from historical observations.\nTherefore, we abstract a general consensus protocol as a dynamic game\nenvironment, apply a solution concept of bounded rationality to model agent\nbehavior, and resolve the initial conditions for three different stable\nequilibria. In our game, agents imitatively learn the global history in an\nevolutionary process toward equilibria, for which we evaluate the outcomes from\nboth computing and economic perspectives in terms of safety, liveness,\nvalidity, and social welfare. Our research contributes to the literature across\ndisciplines, including distributed consensus in computer science, game theory\nin economics on blockchain consensus, evolutionary game theory at the\nintersection of biology and economics, bounded rationality at the interplay\nbetween psychology and economics, and cooperative AI with joint insights into\ncomputing and social science. Finally, we discuss that future protocol design\ncan better achieve the most desired outcomes of our honest stable equilibria by\nincreasing the reward-punishment ratio and lowering both the cost-punishment\nratio and the pivotality rate.\n"
    },
    {
        "paper_id": 2212.05369,
        "authors": "Weilin Fu, Zhuoran Li, Yupeng Zhang and Xingyou Zhou",
        "title": "Time Series Analysis in American Stock Market Recovering in Post\n  COVID-19 Pandemic Period",
        "comments": "9 pages, 4 figures, Submitted to the Cambridge University Press\n  Journal",
        "journal-ref": null,
        "doi": "10.47191/afmj/v8i2.03",
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Every financial crisis has caused a dual shock to the global economy. The\nshortage of market liquidity, such as default in debt and bonds, has led to the\nspread of bankruptcies, such as Lehman Brothers in 2008. Using the data for the\nETFs of the S&P 500, Nasdaq 100, and Dow Jones Industrial Average collected\nfrom Yahoo Finance, this study implemented Deep Learning, Neuro Network, and\nTime-series to analyze the trend of the American Stock Market in the\npost-COVID-19 period. LSTM model in Neuro Network to predict the future trend,\nwhich suggests the US stock market keeps falling for the post-COVID-19 period.\nThis study reveals a reasonable allocation method of Long Short-Term Memory for\nwhich there is strong evidence.\n"
    },
    {
        "paper_id": 2212.05632,
        "authors": "Yufan Zhang, Zichao Chen, Yutong Sun, Yulin Liu, Luyao Zhang",
        "title": "Blockchain Network Analysis: A Comparative Study of Decentralized Banks",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Decentralized finance (DeFi) is known for its unique mechanism design, which\napplies smart contracts to facilitate peer-to-peer transactions. The\ndecentralized bank is a typical DeFi application. Ideally, a decentralized bank\nshould be decentralized in the transaction. However, many recent studies have\nfound that decentralized banks have not achieved a significant degree of\ndecentralization. This research conducts a comparative study among mainstream\ndecentralized banks. We apply core-periphery network features analysis using\nthe transaction data from four decentralized banks, Liquity, Aave, MakerDao,\nand Compound. We extract six features and compare the banks' levels of\ndecentralization cross-sectionally. According to the analysis results, we find\nthat: 1) MakerDao and Compound are more decentralized in the transactions than\nAave and Liquity. 2) Although decentralized banking transactions are supposed\nto be decentralized, the data show that four banks have primary external\ntransaction core addresses such as Huobi, Coinbase, and Binance, etc. We also\ndiscuss four design features that might affect network decentralization. Our\nresearch contributes to the literature at the interface of decentralized\nfinance, financial technology (Fintech), and social network analysis and\ninspires future protocol designs to live up to the promise of decentralized\nfinance for a truly peer-to-peer transaction network.\n"
    },
    {
        "paper_id": 2212.05671,
        "authors": "Chun Yat Yeung, Ali Hirsa",
        "title": "Saddle-Point Approach to Large-Time Volatility Smile",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We extend upon the saddle-point equation presented in [1] to derive\nlarge-time model-implied volatility smiles, providing its theoretical\nfoundation and studying its applications in classical models. As long as\ncharacteristic function fulfills a L\\'evy-type scaling behavior in large time,\nthe approach allows us to study analytically the large-time smile behaviors\nunder specific models, and moreover, to reach a very wide class of\narbitrage-free model-inspired parametrizations, in the same manner as\nstochastic-volatility-inspired (SVI).\n"
    },
    {
        "paper_id": 2212.05734,
        "authors": "Kanis Saengchote",
        "title": "Decentralized lending and its users: Insights from Compound",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  Permissionless blockchains offer an information environment where users can\ninteract privately without fear of censorship. Financial services can be\nprogrammatically coded via smart contracts to automate transactions without the\nneed for human intervention or knowing user identity. This new paradigm is\nknown as decentralized finance (DeFi). We investigate Compound (a leading DeFi\nlending protocol) to show how it works in this novel information environment,\nwho its users are, and what factors determine their participation. On-chain\ntransaction data shows that loan durations are short (31 days on average), and\nmany users borrow to support leveraged investment strategies (yield farming).\nWe show that systemic risk in DeFi arises from concentration and\ninterconnection, and how traditional risk management practices can be\nchallenging for DeFi.\n"
    },
    {
        "paper_id": 2212.05906,
        "authors": "Yufei Xie, Xiang Liu, Qizhong Yuan",
        "title": "Research on College Students' Innovation and Entrepreneurship Education\n  from The Perspective of Artificial Intelligence Knowledge-Based Crowdsourcing",
        "comments": null,
        "journal-ref": "International Journal of Education and Economics, volume 5, 2022,\n  and page 84-86",
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Based on the practical process of innovation and entrepreneurship education\nfor college students in the author's university, this study analyzes and\ndeconstructs the key concepts of AI knowledge-based crowdsourcing on the basis\nof literature research, and analyzes the objective fitting needs of combining\nAI knowledge-based crowdsourcing with college students' innovation and\nentrepreneurship education practice through a survey and research of a random\nsample of college students, and verifies that college students' knowledge and\napplication of AI knowledge-based crowdsourcing in the learning and practice of\ninnovation and entrepreneurship The study also verifies the awareness and\napplication of AI knowledge-based crowdsourcing knowledge by university\nstudents in the learning and practice of innovation and entrepreneurship.\n"
    },
    {
        "paper_id": 2212.05912,
        "authors": "Piero Mazzarisi, Adele Ravagnani, Paola Deriu, Fabrizio Lillo,\n  Francesca Medda, Antonio Russo",
        "title": "A machine learning approach to support decision in insider trading\n  detection",
        "comments": "42 pages, 16 Figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Identifying market abuse activity from data on investors' trading activity is\nvery challenging both for the data volume and for the low signal to noise\nratio. Here we propose two complementary unsupervised machine learning methods\nto support market surveillance aimed at identifying potential insider trading\nactivities. The first one uses clustering to identify, in the vicinity of a\nprice sensitive event such as a takeover bid, discontinuities in the trading\nactivity of an investor with respect to his/her own past trading history and on\nthe present trading activity of his/her peers. The second unsupervised approach\naims at identifying (small) groups of investors that act coherently around\nprice sensitive events, pointing to potential insider rings, i.e. a group of\nsynchronised traders displaying strong directional trading in rewarding\nposition in a period before the price sensitive event. As a case study, we\napply our methods to investor resolved data of Italian stocks around takeover\nbids.\n"
    },
    {
        "paper_id": 2212.05916,
        "authors": "Alireza Jafari and Saman Haratizadeh",
        "title": "NETpred: Network-based modeling and prediction of multiple connected\n  market indices",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Market prediction plays a major role in supporting financial decisions. An\nemerging approach in this domain is to use graphical modeling and analysis to\nfor prediction of next market index fluctuations. One important question in\nthis domain is how to construct an appropriate graphical model of the data that\ncan be effectively used by a semi-supervised GNN to predict index fluctuations.\nIn this paper, we introduce a framework called NETpred that generates a novel\nheterogeneous graph representing multiple related indices and their stocks by\nusing several stock-stock and stock-index relation measures. It then thoroughly\nselects a diverse set of representative nodes that cover different parts of the\nstate space and whose price movements are accurately predictable. By assigning\ninitial predicted labels to such a set of nodes, NETpred makes sure that the\nsubsequent GCN model can be successfully trained using a semi-supervised\nlearning process. The resulting model is then used to predict the stock labels\nwhich are finally aggregated to infer the labels for all the index nodes in the\ngraph. Our comprehensive set of experiments shows that NETpred improves the\nperformance of the state-of-the-art baselines by 3%-5% in terms of F-score\nmeasure on different well-known data sets.\n"
    },
    {
        "paper_id": 2212.05933,
        "authors": "Alapan Chaudhuri, Zeeshan Ahmed, Ashwin Rao, Shivansh Subramanian,\n  Shreyas Pradhan and Abhishek Mittal",
        "title": "Nostradamus: Weathering Worth",
        "comments": "13 pages, 13 figures; updated abstract; updated format to Springer\n  LNCS",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Nostradamus, inspired by the French astrologer and reputed seer, is a\ndetailed study exploring relations between environmental factors and changes in\nthe stock market. In this paper, we analyze associative correlation and\ncausation between environmental elements (including natural disasters, climate\nand weather conditions) and stock prices, using historical stock market data,\nhistorical climate data, and various climate indicators such as carbon dioxide\nemissions. We have conducted our study based on the US financial market, global\nclimate trends, and daily weather records to demonstrate a significant\nrelationship between climate and stock price fluctuation. Our analysis covers\nboth short-term and long-term rises and dips in company stock performances.\nLastly, we take four natural disasters as a case study to observe the effect\nthey have on people's emotional state and their influence on the stock market.\n"
    },
    {
        "paper_id": 2212.06073,
        "authors": "Edoardo Briganti, Victor Sellemi",
        "title": "Why Does GDP Move Before G? It's all in the Measurement",
        "comments": "There are fundamental unclear explanations of the multiplier\n  breakdown which causes great confusion among researchers interested in fiscal\n  policy. The error is in the section before the conclusion",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/publicdomain/zero/1.0/",
        "abstract": "  We find that the early impact of defense news shocks on GDP is mainly due to\na rise in business inventories, as contractors ramp up production for new\ndefense contracts. These contracts do not affect government spending (G) until\npayment-on-delivery, which occurs 2-3 quarters later. Novel data on defense\nprocurement obligations reveals that contract awards Granger-cause\nVAR-identified shocks to G, but not defense news shocks. This implies that the\nearly GDP response relative to G is largely driven by the delayed accounting of\ndefense contracts, while VAR-identified shocks to G miss the impact on\ninventories, resulting in lower multiplier estimates.\n"
    },
    {
        "paper_id": 2212.06223,
        "authors": "Yung-Yu Tsai, Hsing-Wen Han, Kuang-Ta Lo, Tzu-Ting Yang",
        "title": "The Effect of Financial Resources on Fertility: Evidence from\n  Administrative Data on Lottery Winners",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  This paper utilizes wealth shocks from winning lottery prizes to examine the\ncausal effect of financial resources on fertility. We employ extensive panels\nof administrative data encompassing over 0.4 million lottery winners in Taiwan\nand implement a triple-differences design. Our analyses reveal that a\nsubstantial lottery win can significantly increase fertility, the implied\nwealth elasticity of which is around 0.06. Moreover, the primary channel\nthrough which fertility increases is by prompting first births among previously\nchildless individuals. Finally, our analysis reveals that approximately 25% of\nthe total fertility effect stems from increased marriage rates following a\nlottery win.\n"
    },
    {
        "paper_id": 2212.06674,
        "authors": "K.V. Yupatova, O.A. Malafeyev, V.S. Lipatnikov, V.Y. Bezrukikh",
        "title": "IT companies: the specifics of social networks valuation",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  The study discusses the main features which affect the IT companies valuation\non the example of social networks. The relevance of the chosen topic is due to\nthe fact that people live in the information age now, and information\ntechnologies surround us everywhere. Because of this, social networks have\nbecome very popular. They assist people to communicate with each other despite\nof the time and distance. Social networks are also companies that operate in\norder to generate income therefore their owners need to know how promising and\nprofitable their business is. The social networks differ from traditional\ncompanies in this case the purpose of the research is determining the features\nof social networks that affect the accuracy and adequacy of the results of\ncompany valuation. The paper reviews the definitions of information technology,\nsocial networks, history, types of social networks, distinguishing features\nbased on domestic and foreign literature. There are analyzed methods of\nassessing the value of Internet companies, their characteristics and methods of\napplication. There is the six social networks evaluation was assessed in the\npractical part of the study: Facebook, Twitter, Pinterest, Snapchat, Sina Weibo\nand Vkontakte on the basis of the literature studied and the methods for\nevaluating the Internet companies which recommended in it, including the method\nof discounting the cash flow of the company as part of the income approach and\nthe multiplier method as part of a comparative approach. Based on the analysis,\nthe features that affect the social networks valuation are identified.\n"
    },
    {
        "paper_id": 2212.06684,
        "authors": "Yigit Aydede and Jan Ditzen",
        "title": "Identifying the regional drivers of influenza-like illness in Nova\n  Scotia with dominance analysis",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  The spread of viral pathogens is inherently a spatial process. While the\ntemporal aspects of viral spread at the epidemiological level have been\nincreasingly well characterized, the spatial aspects of viral spread are still\nunderstudied due to a striking absence of theoretical expectations of how\nspatial dynamics may impact the temporal dynamics of viral populations.\nCharacterizing the spatial transmission and understanding the factors driving\nit are important for anticipating local timing of disease incidence and for\nguiding more informed control strategies. Using a unique data set from Nova\nScotia, the objective of this study is to apply a new novel method that\nrecovers a spatial network of the influenza-like viral spread where the regions\nin their dominance are identified and ranked. We, then, focus on identifying\nregional predictors of those dominant regions.\n"
    },
    {
        "paper_id": 2212.06733,
        "authors": "Gero Junike, Hauke Stier, Marcus C. Christiansen",
        "title": "Sequential decompositions at their limit",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Sequential updating (SU) decompositions are a well-known technique for\ncreating profit and loss (P&L) attributions, e.g., for a bond portfolio, by\ndividing the time horizon into subintervals and sequentially editing each risk\nfactor in each subinterval, e.g., FX, IR or CS. We show that SU decompositions\nconverge for increasing number of subintervals if the P&L attribution can be\nexpressed by a smooth function of the risk factors. We further consider average\nSU decompositions, which are invariant with respect to the order or labeling of\nthe risk factors. The averaging is numerically expensive, and we discuss\nseveral ways in which the computational complexity of average SU decompositions\ncan be significantly reduced.\n"
    },
    {
        "paper_id": 2212.06736,
        "authors": "Rachel Nesbit",
        "title": "The Role of Mandated Mental Health Treatment in the Criminal Justice\n  System",
        "comments": "80 pages, 16 figures, 23 tables",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Mental health disorders are particularly prevalent among those in the\ncriminal justice system and may be a contributing factor in recidivism. Using\nNorth Carolina court cases from 1994 to 2009, this paper evaluates how mandated\nmental health treatment as a term of probation impacts the likelihood that\nindividuals return to the criminal justice system. I use random variation in\njudge assignment to compare those who were required to seek weekly mental\nhealth counseling to those who were not. The main findings are that being\nassigned to seek mental health treatment decreases the likelihood of three-year\nrecidivism by about 12 percentage points, or 36 percent. This effect persists\nover time, and is similar among various types of individuals on probation. In\naddition, I show that mental health treatment operates distinctly from drug\naddiction interventions in a multiple-treatment framework. I provide evidence\nthat mental health treatment's longer-term effectiveness is strongest among\nmore financially-advantaged probationers, consistent with this setting, in\nwhich the cost of mandated treatment is shouldered by offenders. Finally,\nconservative calculations result in a 5:1 benefit-to-cost ratio which suggests\nthat the treatment-induced decrease in future crime would be more than\nsufficient to offset the costs of treatment.\n"
    },
    {
        "paper_id": 2212.06744,
        "authors": "Alice Di Bella and Massimo Tavoni",
        "title": "Demand-side policies for power generation in response to the energy\n  crisis: a model analysis for Italy",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
        "abstract": "  In order to mitigate the impacts of the energy crise, the European Union has\nproposed various measures. For the power sector a directive prescribes a shift\nof 5% of the demand in 10% of the peak hours, plus a voluntary 10% overall\ndemand reduction. Here we use a power system model to quantify the implications\nof this policy for the Italian power sector, as it stands today and under the\ntransformation required to meet the climate goals of the Fit-for-55. We find\nthat policymakers would need to incentivize electricity consumption in the\nmiddle of the day while discouraging it in the early morning and late\nafternoon. We also highlight the benefits of the decarbonization strategy in\nthe context of uncertain gas prices: for a gas price at or above 50 euro/MWh,\npower generation through gas is reduced by more than one third, approaching\nwhat needed to comply with the Fit-for-55. Finally, we quantify the value of\ndemand side management strategies to curb fossil resource consumption and to\nreduce curtailed electricity under a high renewable scenario.\n"
    },
    {
        "paper_id": 2212.06888,
        "authors": "Songrun He, Asaf Manela, Omri Ross, Victor von Wachter",
        "title": "Fundamentals of Perpetual Futures",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Perpetual futures -- swap contracts that never expire -- are the most popular\nderivative traded in cryptocurrency markets, with more than $100 billion traded\ndaily. Perpetuals provide investors with leveraged exposure to\ncryptocurrencies, which does not require rollover or direct cryptocurrency\nholding. To keep the gap between perpetual futures and spot prices small, long\nposition holders periodically pay short position holders a funding rate\nproportional to this gap. The funding rate incentivizes trades that tend to\nnarrow the futures-spot gap. But unlike fixed-maturity futures, perpetuals are\nnot guaranteed to converge to the spot price of their underlying asset at any\ntime, and familiar no-arbitrage prices for perpetuals are not available, as the\ncontracts have no expiry date to enforce arbitrage. Here, using a weaker notion\nof random-maturity arbitrage, we derive no-arbitrage prices for perpetual\nfutures in frictionless markets and no-arbitrage bounds for markets with\ntrading costs. These no-arbitrage prices provide a valuable benchmark for\nperpetual futures and simultaneously prescribe a strategy to exploit divergence\nfrom these fundamental values. Empirically, we find that deviations of crypto\nperpetual futures from no-arbitrage prices are considerably larger than those\ndocumented in traditional currency markets. These deviations comove across\ncryptocurrencies and diminish over time as crypto markets develop and become\nmore efficient. A simple trading strategy generates large Sharpe ratios even\nfor investors paying the highest trading costs on Binance, which is currently\nthe largest crypto exchange by volume.\n"
    },
    {
        "paper_id": 2212.06951,
        "authors": "Yihang Fu, Zesen Zhuang, Luyao Zhang",
        "title": "AI Ethics on Blockchain: Topic Analysis on Twitter Data for Blockchain\n  Security",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Blockchain has empowered computer systems to be more secure using a\ndistributed network. However, the current blockchain design suffers from\nfairness issues in transaction ordering. Miners are able to reorder\ntransactions to generate profits, the so-called miner extractable value (MEV).\nExisting research recognizes MEV as a severe security issue and proposes\npotential solutions, including prominent Flashbots. However, previous studies\nhave mostly analyzed blockchain data, which might not capture the impacts of\nMEV in a much broader AI society. Thus, in this research, we applied natural\nlanguage processing (NLP) methods to comprehensively analyze topics in tweets\non MEV. We collected more than 20000 tweets with #MEV and #Flashbots hashtags\nand analyzed their topics. Our results show that the tweets discussed profound\ntopics of ethical concern, including security, equity, emotional sentiments,\nand the desire for solutions to MEV. We also identify the co-movements of MEV\nactivities on blockchain and social media platforms. Our study contributes to\nthe literature at the interface of blockchain security, MEV solutions, and AI\nethics.\n"
    },
    {
        "paper_id": 2212.06984,
        "authors": "Dongwei Zhao, Sarah Coyle, Apurba Sakti, Audun Botterud",
        "title": "Market Mechanisms for Low-Carbon Electricity Investments: A\n  Game-Theoretical Analysis",
        "comments": "IEEE Transactions on Energy Markets, Policy and Regulation",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Electricity markets are transforming from the dominance of conventional\nenergy resources (CERs), e.g., fossil fuels, to low-carbon energy resources\n(LERs), e.g., renewables and energy storage. This work examines market\nmechanisms to incentivize LER investments, while ensuring adequate market\nrevenues for investors, guiding investors' strategic investments towards social\noptimum, and protecting consumers from scarcity prices. To reduce the impact of\nexcessive scarcity prices, we present a new market mechanism, which consists of\na Penalty payment for lost load, a supply Incentive, and an energy price Uplift\n(PIU). We establish a game-theoretical framework to analyze market equilibrium.\nWe prove that one Nash equilibrium under the penalty payment and supply\nincentive can reach the social optimum given quadratic supply costs of CERs.\nAlthough the price uplift can ensure adequate revenues, the resulting system\ncost deviates from the social optimum while the gap decreases as more CERs\nretire. Furthermore, under the traditional marginal-cost pricing (MCP)\nmechanism, investors may withhold investments to cause scarcity prices, but\nsuch behavior is absent under the PIU mechanism. Simulation results show that\nthe PIU mechanism can reduce consumers' costs by over 30% compared with the MCP\nmechanism by reducing excessive revenues of low-cost CERs from scarcity prices.\n"
    },
    {
        "paper_id": 2212.07019,
        "authors": "Chunmeng Yang, Siqi Bu, Yi Fan, Wayne Xinwei Wan, Ruoheng Wang, Aoife\n  Foley",
        "title": "Data-Driven Prediction and Evaluation on Future Impact of Energy\n  Transition Policies in Smart Regions",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  To meet widely recognised carbon neutrality targets, over the last decade\nmetropolitan regions around the world have implemented policies to promote the\ngeneration and use of sustainable energy. Nevertheless, there is an\navailability gap in formulating and evaluating these policies in a timely\nmanner, since sustainable energy capacity and generation are dynamically\ndetermined by various factors along dimensions based on local economic\nprosperity and societal green ambitions. We develop a novel data-driven\nplatform to predict and evaluate energy transition policies by applying an\nartificial neural network and a technology diffusion model. Using Singapore,\nLondon, and California as case studies of metropolitan regions at distinctive\nstages of energy transition, we show that in addition to forecasting renewable\nenergy generation and capacity, the platform is particularly powerful in\nformulating future policy scenarios. We recommend global application of the\nproposed methodology to future sustainable energy transition in smart regions.\n"
    },
    {
        "paper_id": 2212.07099,
        "authors": "Yufei Xie, Jing Cui, Mengdie Wang",
        "title": "Research on The Cultivation Path of Craftsman Spirit in Higher\n  Vocational Education Based on Survey Data",
        "comments": null,
        "journal-ref": "International Journal of Education and Teaching Research, Vol.1,\n  No.3, 2020, 161-164",
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  With the development of China's economy and society, the importance of\n\"craftsman's spirit\" has become more and more prominent. As the main\neducational institution for training technical talents, higher vocational\ncolleges vigorously promote the exploration of the cultivation path of\ncraftsman spirit in higher vocational education, which provides new ideas and\ndirections for the reform and development of higher vocational education, and\nis the fundamental need of the national innovation driven development strategy.\nBased on the questionnaire survey of vocational students in a certain range,\nthis paper analyzes the problems existing in the cultivation path of craftsman\nspirit in Higher Vocational Education from multiple levels and the\ncountermeasures.\n"
    },
    {
        "paper_id": 2212.0728,
        "authors": "S\\'andor Juh\\'asz, Gerg\\H{o} Pint\\'er, \\'Ad\\'am Kov\\'acs, Endre Borza,\n  Gergely M\\'onus, L\\'aszl\\'o L\\H{o}rincz and Bal\\'azs Lengyel",
        "title": "Amenity complexity and urban locations of socio-economic mixing",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Cities host diverse people and their mixing is the engine of prosperity. In\nturn, segregation and inequalities are common features of most cities and\nlocations that enable the meeting of people with different socio-economic\nstatus are key for urban inclusion. In this study, we adopt the concept of\neconomic complexity to quantify the sophistication of amenity supply at urban\nlocations. We propose that neighborhood complexity and amenity complexity are\nconnected to the ability of locations to attract diverse visitors from various\nsocio-economic backgrounds across the city. We construct the measures of\namenity complexity based on the local portfolio of diverse and non-ubiquitous\namenities in Budapest, Hungary. Socio-economic mixing at visited third places\nis investigated by tracing the daily mobility of individuals and by\ncharacterizing their status by the real-estate price of their home locations.\nResults suggest that measures of ubiquity and diversity of amenities do not,\nbut neighborhood complexity and amenity complexity are correlated with the\nurban centrality of locations. Urban centrality is a strong predictor of\nsocio-economic mixing, but both neighborhood complexity and amenity complexity\nadd further explanatory power to our models. Our work combines urban mobility\ndata with economic complexity thinking to show that the diversity of\nnon-ubiquitous amenities, central locations, and the potentials for\nsocio-economic mixing are interrelated.\n"
    },
    {
        "paper_id": 2212.07306,
        "authors": "Jakub Warmuz, Amit Chaudhary, and Daniele Pinna",
        "title": "Toxic Liquidation Spirals",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  On November 22nd 2022, the lending platform AAVE v2 (on Ethereum) incurred\nbad debt resulting from a major liquidation event involving a single user who\nhad borrowed close to \\$40M of CRV tokens using USDC as collateral. This\nincident has prompted the Aave community to consider changes to its liquidation\nthreshold, and limitations on the number of illiquid coins that can be borrowed\non the platform. In this paper, we argue that the bad debt incurred by AAVE was\nnot due to excess volatility in CRV/USDC price activity on that day, but rather\na fundamental flaw in the liquidation logic which triggered a toxic liquidation\nspiral on the platform. We note that this flaw, which is shared by a number of\nmajor DeFi lending markets, can be easily overcome with simple changes to the\nincentives driving liquidations. We claim that halting all liquidations once a\nuser's loan-to-value (LTV) ratio surpasses a certain threshold value can\nprevent future toxic liquidation spirals and offer substantial improvement in\nthe bad debt that a lending market can expect to incur. Furthermore, we\nstrongly argue that protocols should enact dynamic liquidation incentives and\nclosing factor policies moving forward for optimal management of protocol risk.\n"
    },
    {
        "paper_id": 2212.07384,
        "authors": "Gaurab Aryal and Federico Ciliberto and Leland E. Farmer and Ekaterina\n  Khmelnitskaya",
        "title": "Valuing Pharmaceutical Drug Innovations",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  We propose a methodology to estimate the market value of pharmaceutical\ndrugs. Our approach combines an event study with a model of discounted cash\nflows and uses stock market responses to drug development announcements to\ninfer the values. We estimate that, on average, a successful drug is valued at\n\\$1.62 billion, and its value at the discovery stage is \\$64.3 million, with\nsubstantial heterogeneity across major diseases. Leveraging these estimates, we\nalso determine the average drug development costs at various stages.\nFurthermore, we explore applying our estimates to design policies that support\ndrug development through drug buyouts and cost-sharing agreements.\n"
    },
    {
        "paper_id": 2212.07516,
        "authors": "Lin Chen, Xun Yu Zhou",
        "title": "Naive Markowitz Policies",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We study a continuous-time Markowitz mean-variance portfolio selection model\nin which a naive agent, unaware of the underlying time-inconsistency,\ncontinuously reoptimizes over time. We define the resulting naive policies\nthrough the limit of discretely naive policies that are committed only in very\nsmall time intervals, and derive them analytically and explicitly. We compare\nnaive policies with pre-committed optimal policies and with consistent\nplanners' equilibrium policies in a Black-Scholes market, and find that the\nformer are mean-variance inefficient starting from any given time and wealth,\nand always take riskier exposure than equilibrium policies.\n"
    },
    {
        "paper_id": 2212.07817,
        "authors": "Peter K. Friz, Thomas Wagenhofer",
        "title": "Reconstructing Volatility: Pricing of Index Options under Rough\n  Volatility",
        "comments": "22 pages, This version to appear in Mathematical Finance",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In previous works Avellaneda et al. pioneered the pricing and hedging of\nindex options - products highly sensitive to implied volatility and correlation\nassumptions - with large deviations methods, assuming local volatility dynamics\nfor all components of the index. We here present an extension applicable to\nnon-Markovian dynamics and in particular the case of rough volatility dynamics.\n"
    },
    {
        "paper_id": 2212.07827,
        "authors": "Umut \\c{C}etin and Alaina Danilova",
        "title": "Order routing and market quality: Who benefits from internalisation?",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  We analyse two models of liquidity provision to determine the retail traders'\npreference for marketable order routing. Order internalization is captured by a\nmodel of market makers competing for the retail order flow in a Bertrand\nfashion. On the other hand, the price-taking competitive liquidity providers\ncharacterize the open exchange model. We show that, when liquidity providers\nare risk averse, routing of the marketable orders to the wholesalers is\npreferred by all retail traders: informed, uninformed and noisy. The\nunwillingness of liquidity providers to bear risk causes the strategic trader\n(informed or not) to absorb large shocks in their inventories. This results in\nmean reverting inventories, price reversal, and lower market depth. The\nequilibria in both models coincide with Kyle (1985) when liquidity providers\nare risk neutral. We also identify a universal parameter that allows comparison\nof market liquidity, profit and value of information across different markets.\n"
    },
    {
        "paper_id": 2212.07942,
        "authors": "Alexis Asseman, Tomasz Kornuta, Anirudh Patel, Matt Deible, Sam Green",
        "title": "Multi-Agent Dynamic Pricing in a Blockchain Protocol Using Gaussian\n  Bandits",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  The Graph Protocol indexes historical blockchain transaction data and makes\nit available for querying. As the protocol is decentralized, there are many\nindependent Indexers that index and compete with each other for serving queries\nto the Consumers. One dimension along which Indexers compete is pricing. In\nthis paper, we propose a bandit-based algorithm for maximization of Indexers'\nrevenue via Consumer budget discovery. We present the design and the\nconsiderations we had to make for a dynamic pricing algorithm being used by\nmultiple agents simultaneously. We discuss the results achieved by our dynamic\npricing bandits both in simulation and deployed into production on one of the\nIndexers operating on Ethereum. We have open-sourced both the simulation\nframework and tools we created, which other Indexers have since started to\nadapt into their own workflows.\n"
    },
    {
        "paper_id": 2212.07944,
        "authors": "Kaizheng Wang, Xiao Xu, Xun Yu Zhou",
        "title": "Variable Clustering via Distributionally Robust Nodewise Regression",
        "comments": "34 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We study a multi-factor block model for variable clustering and connect it to\nthe regularized subspace clustering by formulating a distributionally robust\nversion of the nodewise regression. To solve the latter problem, we derive a\nconvex relaxation, provide guidance on selecting the size of the robust region,\nand hence the regularization weighting parameter, based on the data, and\npropose an ADMM algorithm for implementation. We validate our method in an\nextensive simulation study. Finally, we propose and apply a variant of our\nmethod to stock return data, obtain interpretable clusters that facilitate\nportfolio selection and compare its out-of-sample performance with other\nclustering methods in an empirical study.\n"
    },
    {
        "paper_id": 2212.07972,
        "authors": "Orcan Ogetbil and Bernhard Hientzsch",
        "title": "A Flexible Commodity Skew Model with Maturity Effects",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We propose a non-parametric extension with leverage functions to the Andersen\ncommodity curve model. We calibrate this model to market data for WTI and NG\nincluding option skew at the standard maturities. While the model can be\ncalibrated by an analytical formula for the deterministic rate case, the\nstochastic rate case demands estimation of an expectation for which we employ\nMonte Carlo simulation. We find that the market smile is captured for the\ndeterministic rate case; and with relatively low number of paths, for the\nstochastic rate case. Since there is typically at most one standard maturity\nwith liquid volatility data for each futures contract, there is flexibility on\nthe shape of nonstandard maturity implied volatility and how the total implied\nvariance accumulates. We equip the model with different total implied variance\naccumulators to demonstrate that flexibility.\n"
    },
    {
        "paper_id": 2212.08029,
        "authors": "David J. Pr\\\"omel, David Scheffels",
        "title": "Pathwise uniqueness for singular stochastic Volterra equations with\n  H\\\"older coefficients",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Pathwise uniqueness is established for a class of one-dimensional stochastic\nVolterra equations driven by Brownian motion with singular kernels and H\\\"older\ncontinuous diffusion coefficients. Consequently, the existence of unique strong\nsolutions is obtained for this class of stochastic Volterra equations.\n"
    },
    {
        "paper_id": 2212.08137,
        "authors": "Zunian Luo",
        "title": "Cap or No Cap? What Can Governments Do to Promote EV Sales?",
        "comments": null,
        "journal-ref": "Cornell Undergraduate Economic Review. Fall (2021) 19-55",
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  This paper examines the effect of the federal EV income tax subsidy on EV\nsales. I find that reduction of the federal subsidy caused sales to decline by\n$43.2 \\%$. To arrive at this result, I employ historical time series data from\nthe Department of Energy Alternative Fuels Data Center. Using the fact that the\nsubsidy is available only for firms with fewer than 200,000 cumulative EV\nsales, I separate EV models into two groups. The treatment group consists of\nmodels receiving the full subsidy, and the control group consists of models\nreceiving the reduced subsidy. This allows for a difference in differences\n(DiD) model structure. To examine the robustness of the results, I conduct\nregression analyses. Due to a relatively small sample size, the regression\ncoefficients lack statistical significance. Above all, my results suggest that\nfederal incentives designed to promote EV consumption are successful in their\nobjectives.\n"
    },
    {
        "paper_id": 2212.08198,
        "authors": "Tamay Besiroglu, Nicholas Emery-Xu, Neil Thompson",
        "title": "Economic impacts of AI-augmented R&D",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Since its emergence around 2010, deep learning has rapidly become the most\nimportant technique in Artificial Intelligence (AI), producing an array of\nscientific firsts in areas as diverse as protein folding, drug discovery,\nintegrated chip design, and weather prediction. As more scientists and\nengineers adopt deep learning, it is important to consider what effect\nwidespread deployment would have on scientific progress and, ultimately,\neconomic growth. We assess this impact by estimating the idea production\nfunction for AI in two computer vision tasks that are considered key test-beds\nfor deep learning and show that AI idea production is notably more\ncapital-intensive than traditional R&D. Because increasing the\ncapital-intensity of R&D accelerates the investments that make scientists and\nengineers more productive, our work suggests that AI-augmented R&D has the\npotential to speed up technological change and economic growth.\n"
    },
    {
        "paper_id": 2212.0822,
        "authors": "Joan Martinez",
        "title": "The Long-Term Effects of Teachers' Gender Stereotypes",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  This paper studies the effects of teachers' stereotypical assessments of boys\nand girls on students' long-term outcomes, including high school graduation,\ncollege attendance, and formal sector employment. I measure teachers' gender\nstereotypical grading based on preconceptions about boys' aptitude in math and\nscience and girls' aptitude in communicating and verbal skills by analyzing\nnovel data on teachers' responses to the Implicit Association Test (IAT) and\ndifferences in gender gaps between teacher-assigned and blindly graded tests.\nTo collect IAT scores on a national scale, I developed a large-scale\neducational program accessible to teachers and students in Peruvian public\nschools. This analysis provides evidence that teachers' gender stereotypes are\nreflected in student evaluations: math teachers with stronger stereotypes\nassociating boys with scientific disciplines award male students with higher\ntest scores compared to blindly-graded test scores. In contrast, language arts\nteachers who stereotypically associate females with humanities-based\ndisciplines give female students higher grades. Using graduation, college\nenrollment, and matched employer-employee data on 1.7 million public high\nschool students expected to graduate between 2015 and 2019, I find that female\nstudents who are placed with teachers who use more stereotypical grading\npractices are less likely to graduate from high school and apply to college\nthan male students. In comparison to their male counterparts, female high\nschool graduates whose teachers employ more stereotypical grading practices are\nless likely to be employed in the formal sector and have fewer paid working\nhours. Furthermore, exposure to teachers with more stereotypical grading\npractices reduces women's monthly earnings, thereby widening the gender pay\ngap.\n"
    },
    {
        "paper_id": 2212.0825,
        "authors": "Tomohisa Yamakami, Yuki Takeuchi",
        "title": "Pricing Bermudan Swaption under Two Factor Hull-White Model with Fast\n  Gauss Transform",
        "comments": "21 pages, 16 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper describes a fast and stable algorithm for evaluating Bermudan\nswaption under the two factor Hull-White model. We discretize the calculation\nof the expected value in the evaluation of Bermudan swaption by numerical\nintegration, and Gaussian kernel sums appears in it. The fast Gauss transform\ncan be applied to these Gaussian kernel sums, and it reduces computational\ncomplexity from $O(N^2)$ to $O(N)$ as grid points number $N$ of numerical\nintegration. We also propose to stabilize the computation under the condition\nthat the correlation is close to $-1$ by introducing the grid rotation.\nNumerical experiments using actual market data show that our method reduces the\ncomputation time significantly compared to the method without the fast Gauss\ntransform. They also show that the method of the grid rotation contributes to\ncomputational stability in the situations where the correlation is close to\n$-1$ and time step is short.\n"
    },
    {
        "paper_id": 2212.08297,
        "authors": "Eduardo Abi Jaber, Camille Illand and Shaun (Xiaoyuan) Li",
        "title": "Joint SPX-VIX calibration with Gaussian polynomial volatility models:\n  deep pricing with quantization hints",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  We consider the joint SPX-VIX calibration within a general class of Gaussian\npolynomial volatility models in which the volatility of the SPX is assumed to\nbe a polynomial function of a Gaussian Volterra process defined as a stochastic\nconvolution between a kernel and a Brownian motion. By performing joint\ncalibration to daily SPX-VIX implied volatility surface data between 2012 and\n2022, we compare the empirical performance of different kernels and their\nassociated Markovian and non-Markovian models, such as rough and non-rough\npath-dependent volatility models. In order to ensure an efficient calibration\nand a fair comparison between the models, we develop a generic unified method\nin our class of models for fast and accurate pricing of SPX and VIX derivatives\nbased on functional quantization and Neural Networks. For the first time, we\nidentify a \\textit{conventional one-factor Markovian continuous stochastic\nvolatility model} that is able to achieve remarkable fits of the implied\nvolatility surfaces of the SPX and VIX together with the term structure of VIX\nfutures. What is even more remarkable is that our conventional one-factor\nMarkovian continuous stochastic volatility model outperforms, in all market\nconditions, its rough and\n  non-rough path-dependent counterparts with the same number of parameters.\n"
    },
    {
        "paper_id": 2212.08509,
        "authors": "Michael E. Mura",
        "title": "Moate Simulation of Stochastic Processes",
        "comments": "17 pages, 1 pdf figure plus internal tikz figures. JEL\n  Classifications: C15, C63, G12, G13, G17",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  A novel approach called Moate Simulation is presented to provide an accurate\nnumerical evolution of probability distribution functions represented on grids\narising from stochastic differential processes where initial conditions are\nspecified. Where the variables of stochastic differential equations may be\ntransformed via It\\^o-Doeblin calculus into stochastic differentials with a\nconstant diffusion term, the probability distribution function for these\nvariables can be simulated in discrete time steps. The drift is applied\ndirectly to a volume element of the distribution while the stochastic diffusion\nterm is applied through the use of convolution techniques such as Fast or\nDiscrete Fourier Transforms. This allows for highly accurate distributions to\nbe efficiently simulated to a given time horizon and may be employed in one,\ntwo or higher dimensional expectation integrals, e.g. for pricing of financial\nderivatives. The Moate Simulation approach forms a more accurate and\nconsiderably faster alternative to Monte Carlo Simulation for many applications\nwhile retaining the opportunity to alter the distribution in mid-simulation.\n"
    },
    {
        "paper_id": 2212.08518,
        "authors": "Erhan Bayraktar, Gaoyue Guo, Wenpin Tang, Yuming Paul Zhang",
        "title": "Systemic robustness: a mean-field particle system approach",
        "comments": "33 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  This paper is concerned with the problem of budget control in a large\nparticle system modeled by stochastic differential equations involving hitting\ntimes, which arises from considerations of systemic risk in a regional\nfinancial network. Motivated by Tang and Tsai (Ann. Probab., 46(2018), pp.\n1597{1650), we focus on the number or proportion of surviving entities that\nnever default to measure the systemic robustness. First we show that both the\nmean-field particle system and its limiting McKean-Vlasov equation are\nwell-posed by virtue of the notion of minimal solutions. We then establish a\nconnection between the proportion of surviving entities in the large particle\nsystem and the probability of default in the limiting McKean-Vlasov equation as\nthe size of the interacting particle system N tends to infinity. Finally, we\nstudy the asymptotic efficiency of budget control in different economy regimes:\nthe expected number of surviving entities is of constant order in a negative\neconomy; it is of order of the square root of N in a neutral economy; and it is\nof order N in a positive economy where the budget's effect is negligible.\n"
    },
    {
        "paper_id": 2212.08734,
        "authors": "N'yoma Diamond, Grant Perkins",
        "title": "Using Intermarket Data to Evaluate the Efficient Market Hypothesis with\n  Machine Learning",
        "comments": "for source code, see\n  https://github.com/Swiss-MQP-2022/Intermarket-ML-For-EMH",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  In its semi-strong form, the Efficient Market Hypothesis (EMH) implies that\ntechnical analysis will not reveal any hidden statistical trends via\nintermarket data analysis. If technical analysis on intermarket data reveals\ntrends which can be leveraged to significantly outperform the stock market,\nthen the semi-strong EMH does not hold. In this work, we utilize a variety of\nmachine learning techniques to empirically evaluate the EMH using stock market,\nforeign currency (Forex), international government bond, index future, and\ncommodities future assets. We train five machine learning models on each\ndataset and analyze the average performance of these models for predicting the\ndirection of future S&P 500 movement as approximated by the SPDR S&P 500 Trust\nETF (SPY). From our analysis, the datasets containing bonds, index futures,\nand/or commodities futures data notably outperform baselines by substantial\nmargins. Further, we find that the usage of intermarket data induce\nstatistically significant positive impacts on the accuracy, macro F1 score,\nweighted F1 score, and area under receiver operating characteristic curve for a\nvariety of models at the 95% confidence level. This provides strong empirical\nevidence contradicting the semi-strong EMH.\n"
    },
    {
        "paper_id": 2212.09192,
        "authors": "Hongda Hu, Arthur Charpentier, Mario Ghossoub, Alexander Schied",
        "title": "Multiarmed Bandits Problem Under the Mean-Variance Setting",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  The classical multi-armed bandit (MAB) problem involves a learner and a\ncollection of K independent arms, each with its own ex ante unknown independent\nreward distribution. At each one of a finite number of rounds, the learner\nselects one arm and receives new information. The learner often faces an\nexploration-exploitation dilemma: exploiting the current information by playing\nthe arm with the highest estimated reward versus exploring all arms to gather\nmore reward information. The design objective aims to maximize the expected\ncumulative reward over all rounds. However, such an objective does not account\nfor a risk-reward tradeoff, which is often a fundamental precept in many areas\nof applications, most notably in finance and economics. In this paper, we build\nupon Sani et al. (2012) and extend the classical MAB problem to a mean-variance\nsetting. Specifically, we relax the assumptions of independent arms and bounded\nrewards made in Sani et al. (2012) by considering sub-Gaussian arms. We\nintroduce the Risk Aware Lower Confidence Bound (RALCB) algorithm to solve the\nproblem, and study some of its properties. Finally, we perform a number of\nnumerical simulations to demonstrate that, in both independent and dependent\nscenarios, our suggested approach performs better than the algorithm suggested\nby Sani et al. (2012).\n"
    },
    {
        "paper_id": 2212.09299,
        "authors": "Max Franks, Matthias Kalkuhl, Kai Lessmann",
        "title": "Optimal pricing for carbon dioxide removal under inter-regional leakage",
        "comments": null,
        "journal-ref": "Journal of Environmental Economics and Management 2022 102769",
        "doi": "10.1016/j.jeem.2022.102769",
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  Carbon dioxide removal (CDR) moves atmospheric carbon to geological or\nland-based sinks. In a first-best setting, the optimal use of CDR is achieved\nby a removal subsidy that equals the optimal carbon tax and marginal damages.\nWe derive second-best policy rules for CDR subsidies and carbon taxes when no\nglobal carbon price exists but a national government implements a unilateral\nclimate policy. We find that the optimal carbon tax differs from an optimal CDR\nsubsidy because of carbon leakage and a balance of resource trade effect.\nFirst, the optimal removal subsidy tends to be larger than the carbon tax\nbecause of lower supply-side leakage on fossil resource markets. Second, net\ncarbon exporters exacerbate this wedge to increase producer surplus of their\ncarbon resource producers, implying even larger removal subsidies. Third, net\ncarbon importers may set their removal subsidy even below their carbon tax when\nmarginal environmental damages are small, to appropriate producer surplus from\ncarbon exporters.\n"
    },
    {
        "paper_id": 2212.0938,
        "authors": "Emanuel Kohlscheen",
        "title": "Understanding the food component of inflation",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  This article presents evidence based on a panel of 35 countries over the past\n30 years that the Phillips curve relation holds for food inflation. That is,\nbroader economic overheating does push up the food component of the CPI in a\nsystematic way. Further, general inflation expectations from professional\nforecasters clearly impact food price inflation. The analysis also quantifies\nthe extent to which higher food production and imports, or lower food exports,\nreduce food inflation. Importantly, the link between domestic and global food\nprices is typically weak, with passthroughs within a year ranging from 0.07 to\n0.16, after exchange rate variations are taken into account.\n"
    },
    {
        "paper_id": 2212.09385,
        "authors": "Joseph Levitas, Konstantin Yavilberg, Oleg Korol, Genadi Man",
        "title": "Prediction of Auto Insurance Risk Based on t-SNE Dimensionality\n  Reduction",
        "comments": "13 pages, 9 figures",
        "journal-ref": null,
        "doi": "10.54364/AAIML.2022.1139",
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Correct risk estimation of policyholders is of great significance to auto\ninsurance companies. While the current tools used in this field have been\nproven in practice to be quite efficient and beneficial, we argue that there is\nstill a lot of room for development and improvement in the auto insurance risk\nestimation process. To this end, we develop a framework based on a combination\nof a neural network together with a dimensionality reduction technique t-SNE\n(t-distributed stochastic neighbour embedding). This enables us to visually\nrepresent the complex structure of the risk as a two-dimensional surface, while\nstill preserving the properties of the local region in the features space. The\nobtained results, which are based on real insurance data, reveal a clear\ncontrast between the high and the low risk policy holders, and indeed improve\nupon the actual risk estimation performed by the insurer. Due to the visual\naccessibility of the portfolio in this approach, we argue that this framework\ncould be advantageous to the auto insurer, both as a main risk prediction tool\nand as an additional validation stage in other approaches.\n"
    },
    {
        "paper_id": 2212.09473,
        "authors": "Mih\\'aly P\\'eter Hanics",
        "title": "Graph theoretical models and algorithms of portfolio compression",
        "comments": "From Bachelors thesis",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  In portfolio compression, market participants (banks, organizations,\ncompanies, financial agents) sign contracts, creating liabilities between each\nother, which increases the systemic risk. Large, dense markets commonly can be\ncompressed by reducing obligations without lowering the net notional of each\nparticipant (an example is if liabilities make a cycle between agents, then it\nis possible to reduce each of them without any net notional changing), and our\ntarget is to eliminate as much excess notional as possible in practice (excess\nis defined as the difference between gross and net notional). A limiting factor\nthat may reduce the effectiveness of the compression can be the preferences and\npriorities of compression participants, who may individually define conditions\nfor the compression, which must be considered when designing the clearing\nprocess, otherwise, a participant may bail out, resulting in the designed\nclearing process to be impossible to execute. These markets can be\nwell-represented with edge-weighted graphs. In this paper, I examine cases when\npreferences of participants on behalf of clearing are given, e.g., in what\norder would they pay back their liabilities (a key factor can be the rate of\ninterest) and I show a clearing algorithm for these problems. On top of that,\nsince it is a common goal for the compression coordinating authority to\nmaximize the compressed amount, I also show a method to compute the maximum\nvolume conservative compression in a network. I further evaluate the\npossibility of combining the two models. Examples and program code of the model\nare also shown, also a0 pseudo-code of the clearing algorithms.\n"
    },
    {
        "paper_id": 2212.09585,
        "authors": "Roger Knecktys, Henrik Bette, R\\\"udiger Kiesel, Thomas Guhr",
        "title": "Risk Theory and Pricing of \"Pay-for-Performance\" Business Models",
        "comments": "15 pages; 11 Figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Technology trends as digitalization and Industry 4.0 initiate a growing\ndemand for new business models. Most of this models requires a fundamental\nshift of operational and financial risks between seller and buyer. A key\nquestion is therefore how to include additional risk pricing and hedging. In\nthis paper we propose a new approach for a risk theory of innovative\nperformance based business models as \"Pay-for-Performance\" or \"Product as a\nService\". A new model and calculation method for determination the risk premium\nis presented. It contains beside financial price fluctuations also operational\nfailure behaviour of products. We apply the model for a typical industrial\napplication and simulate the pricing dependency for different cost\ndistributions.\n"
    },
    {
        "paper_id": 2212.09624,
        "authors": "Rachna Saxena, Abhijeet Kumar, Mridul Mishra",
        "title": "Holder Recommendations using Graph Representation Learning & Link\n  Prediction",
        "comments": "6 pages, 6 figures, 2 tables Presented at a workshop in ACM AI in\n  Finance conference",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Lead recommendations for financial products such as funds or ETF is\npotentially challenging in investment space due to changing market scenarios,\nand difficulty in capturing financial holder's mindset and their philosophy.\nCurrent methods surface leads based on certain product categorization and\nattributes like returns, fees, category etc. to suggest similar product to\ninvestors which may not capture the holder's investment behavior holistically.\nOther reported works does subjective analysis of institutional holder's\nideology. This paper proposes a comprehensive data driven framework for\ndeveloping a lead recommendations system in holder's space for financial\nproducts like funds by using transactional history, asset flows and product\nspecific attributes. The system assumes holder's interest implicitly by\nconsidering all investment transactions made and collects possible meta\ninformation to detect holder's investment profile/persona like investment\nanticipation and investment behavior. This paper focusses on holder\nrecommendation component of framework which employs a bi-partite graph\nrepresentation of financial holders and funds using variety of attributes and\nfurther employs GraphSage model for learning representations followed by link\nprediction model for ranking recommendation for future period. The performance\nof the proposed approach is compared with baseline model i.e., content-based\nfiltering approach on metric hits at Top-k (50, 100, 200) recommendations. We\nfound that the proposed graph ML solution outperform baseline by absolute 42%,\n22% and 14% with a look ahead bias and by absolute 18%, 19% and 18% on\ncompletely unseen holders in terms of hit rate for top-k recommendations: 50,\n100 and 200 respectively.\n"
    },
    {
        "paper_id": 2212.09715,
        "authors": "Victoria Mooers, Joseph Campbell, Alessandra Casella, Lucas de Lara,\n  and Dilip Ravindran",
        "title": "Liquid Democracy. Two Experiments on Delegation in Voting",
        "comments": "87 pages, 12 figures. Revised Introduction; added Discussion to\n  explore mechanisms and model extensions; expanded discussion of individual\n  behavior in Section 6; reformatted tables for clarity; revised Experiment 2\n  probit regressions; corrected typos and added references",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Proponents of participatory democracy praise Liquid Democracy: decisions are\ntaken by referendum, but voters delegate their votes freely. When better\ninformed voters are present, delegation can increase the probability of a\ncorrect decision. However, delegation must be used sparely because it reduces\nthe information aggregated through voting. In two different experiments, we\nfind that delegation underperforms both universal majority voting and the\nsimpler option of abstention. In a tightly controlled lab experiment where the\nsubjects' precision of information is conveyed in precise mathematical terms\nand very salient, the result is due to overdelegation. In a perceptual task run\nonline where the precision of information is not known precisely, delegation\nremains very high and again underperforms both majority voting and abstention.\nIn addition, subjects substantially overestimate the precision of the better\ninformed voters, underlining that Liquid Democracy is fragile to multiple\nsources of noise. The paper makes an innovative methodological contribution by\ncombining two very different experimental procedures: the study of voting rules\nwould benefit from complementing controlled experiments with known precision of\ninformation with tests under ambiguity, a realistic assumption in many voting\nsituations.\n"
    },
    {
        "paper_id": 2212.09904,
        "authors": "Francisco Rodr\\'iguez",
        "title": "Sanctions and Imports of Essential Goods: A Closer Look at the Equipo\n  Anova (2021) Results",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  We revisit the results of a recent paper by Equipo Anova, who claim to find\nevidence of an improvement in Venezuelan imports of food and medicines\nassociated with the adoption of U.S. financial sanctions towards Venezuela in\n2017. We show that their results are consequence of data coding errors and\nquestionable methodological choices, including the use an unreasonable\nfunctional form that implies a counterfactual of negative imports in the\nabsence of sanctions, the omission of data accounting for four-fifths of the\ncountry's food imports at the time of sanctions and incorrect application of\nregression discontinuity methods. Once these errors are corrected, the evidence\nof a significant improvement in the level and rate of change in imports of\nessentials disappears.\n"
    },
    {
        "paper_id": 2212.09957,
        "authors": "Marc Chataigner, Areski Cousin, St\\'ephane Cr\\'epey, Matthew Dixon and\n  Djibril Gueye",
        "title": "Beyond Surrogate Modeling: Learning the Local Volatility Via Shape\n  Constraints",
        "comments": null,
        "journal-ref": "Short Communication: Beyond Surrogate Modeling: Learning the Local\n  Volatility via Shape Constraints, SIAM Journal on Financial Mathematics\n  12(3), SC58-SC69, 2021",
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  We explore the abilities of two machine learning approaches for no-arbitrage\ninterpolation of European vanilla option prices, which jointly yield the\ncorresponding local volatility surface: a finite dimensional Gaussian process\n(GP) regression approach under no-arbitrage constraints based on prices, and a\nneural net (NN) approach with penalization of arbitrages based on implied\nvolatilities. We demonstrate the performance of these approaches relative to\nthe SSVI industry standard. The GP approach is proven arbitrage-free, whereas\narbitrages are only penalized under the SSVI and NN approaches. The GP approach\nobtains the best out-of-sample calibration error and provides uncertainty\nquantification.The NN approach yields a smoother local volatility and a better\nbacktesting performance, as its training criterion incorporates a local\nvolatility regularization term.\n"
    },
    {
        "paper_id": 2212.10053,
        "authors": "Knut Anton Mork, Fabian Andsem Harang, Haakon Andreas Tr{\\o}nnes,\n  Vegard Skonseng Bjerketvedt",
        "title": "Dynamic spending and portfolio decisions with a soft social norm",
        "comments": "45 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We explore the implications of a preference ordering for an investor-consumer\nwith a strong preference for keeping consumption above an exogenous social\nnorm, but who is willing to tolerate occasional dips below it. We do this by\nsplicing two CRRA preference orderings, one with high curvature below the norm\nand the other with low curvature at or above it. We find this formulation\nappealing for many endowment funds and sovereign wealth funds, including the\nNorwegian Government Pension Fund Global, which inspired our research. We solve\nthis model analytically as well as numerically and find that annual spending\nshould not only be significantly lower than the expected financial return, but\nmostly also procyclical. In particular, financial losses should, as a rule, be\nfollowed by larger than proportional spending cuts, except when some smoothing\nis needed to keep spending from falling too far below the social norm. Yet, at\nvery low wealth levels, spending should be kept particularly low in order to\nbuild sufficient wealth to raise consumption above the social norm. Financial\nrisk taking should also be modest and procyclical, so that the investor\nsometimes may want to \"buy at the top\" and \"sell at the bottom\". Many of these\nfeatures are shared by habitformation models and other models with some lower\nbound for consumption. However, our specification is more flexible and thus\nmore easily adaptable to actual fund management. The nonlinearity of the policy\nfunctions may present challenges regarding delegation to professional managers.\nHowever, simpler rules of thumb with constant or slowly moving equity share and\nconsumption-wealth ratio can reach almost the same expected discounted utility.\nHowever, the constant levels will then look very different from the\nimplications of expected CRRA utility or Epstein-Zin preferences in that\nconsumption is much lower.\n"
    },
    {
        "paper_id": 2212.10164,
        "authors": "Mathieu Rosenbaum and Jianfei Zhang",
        "title": "Multi-asset market making under the quadratic rough Heston",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Given the promising results on joint modeling of SPX/VIX smiles of the\nrecently introduced quadratic rough Heston model, we consider a multi-asset\nmarket making problem on SPX and its derivatives, e.g. VIX futures, SPX and VIX\noptions. The market maker tries to maximize its profit from spread capturing\nwhile controlling the portfolio's inventory risk, which can be fully explained\nby the value change of SPX under the particular setting of the quadratic rough\nHeston model. The high dimensionality of the resulting optimization problem is\nrelaxed by several approximations. An asymptotic closed-form solution can be\nobtained. The accuracy and relevance of the approximations are illustrated\nthrough numerical experiments.\n"
    },
    {
        "paper_id": 2212.10234,
        "authors": "Conleigh Byers and Brent Eldridge",
        "title": "Auction designs to increase incentive compatibility and reduce\n  self-scheduling in electricity markets",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The system operator's scheduling problem in electricity markets, called unit\ncommitment, is a non-convex mixed-integer program. The optimal value function\nis non-convex, preventing the application of traditional marginal pricing\ntheory to find prices that clear the market and incentivize market participants\nto follow the dispatch schedule. Units that perceive the opportunity to make a\nprofit may be incentivized to self-commit (submitting an offer with zero fixed\noperating costs) or self-schedule their production (submitting an offer with\nzero total cost). We simulate bidder behavior to show that market power can be\nexercised by becoming a price taker. Agents can learn to increase their profits\nvia a reinforcement learning algorithm without explicit knowledge of the costs\nor strategies of other agents. We investigate different non-convex pricing\nmodels over a multi-period commitment window simulating the day-ahead market\nand show that convex hull pricing can reduce producer incentives to deviate\nfrom the central dispatch decision. In a realistic test system with\napproximately 1000 generators, we find strategic bidding under the restricted\nconvex model can increase total producer profits by 4.4% and decrease lost\nopportunity costs by 2/3. While the cost to consumers with convex hull pricing\nis higher at the competitive solution, the cost to consumers is higher with the\nrestricted convex model after strategic bidding.\n"
    },
    {
        "paper_id": 2212.10308,
        "authors": "Matthias Nadler, Felix Bekemeier and Fabian Sch\\\"ar",
        "title": "DeFi Risk Transfer: Towards A Fully Decentralized Insurance Protocol",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-sa/4.0/",
        "abstract": "  In this paper, we propose a fully decentralized and smart contract-based\ninsurance protocol. We identify various issues in the Decentralized Finance\n(DeFi) insurance context and propose a solution to overcome these shortcomings.\nWe introduce an economic model that allows for risk transfer without any\nexternal dependencies or centralized intermediaries. In particular, our\nproposal does not need any sort of subjective claim assessment, community\nvoting or external data providers (oracles). Moreover, it solves the problem of\nover-insurance and proposes various ways to mitigate the capital inefficiencies\nusually seen with DeFi collateral. The work takes inspiration from peer-to-peer\n(P2P) insurance and collateralized debt obligations (CDO). We formally describe\nthe protocol, assess its efficiency and key properties and present a reference\nimplementation. Finally, we address limitations, extensions and ideas for\nfurther research.\n"
    },
    {
        "paper_id": 2212.10317,
        "authors": "Andrew Y. Chen, Alejandro Lopez-Lira, and Tom Zimmermann",
        "title": "Does Peer-Reviewed Research Help Predict Stock Returns?",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Mining 29,000 accounting ratios for t-statistics over 2.0 leads to\ncross-sectional return predictability similar to the peer review process. For\nboth methods, about 50% of predictability remains after the original sample\nperiods. Predictors supported by peer-reviewed risk explanations or equilibrium\nmodels underperform other predictors post-sample, suggesting peer review\nsystematically mislabels mispricing as risk, though only 20% of predictors are\nlabelled as risk. Data mining generates other features of peer review including\nthe rise in returns as original sample periods end and the speed of post-sample\ndecay. It also uncovers themes like investment, issuance, and accruals --\ndecades before they are published.\n"
    },
    {
        "paper_id": 2212.10844,
        "authors": "Jeremi Assael (BNPP CIB GM Lab, MICS), Thibaut Heurtebize, Laurent\n  Carlier (BNPP CIB GM Lab), Fran\\c{c}ois Soup\\'e",
        "title": "Greenhouse gases emissions: estimating corporate non-reported emissions\n  using interpretable machine learning",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  As of 2022, greenhouse gases (GHG) emissions reporting and auditing are not\nyet compulsory for all companies and methodologies of measurement and\nestimation are not unified. We propose a machine learning-based model to\nestimate scope 1 and scope 2 GHG emissions of companies not reporting them yet.\nOur model, specifically designed to be transparent and completely adapted to\nthis use case, is able to estimate emissions for a large universe of companies.\nIt shows good out-of-sample global performances as well as good out-of-sample\ngranular performances when evaluating it by sectors, by countries or by\nrevenues buckets. We also compare our results to those of other providers and\nfind our estimates to be more accurate. Thanks to the proposed explainability\ntools using Shapley values, our model is fully interpretable, the user being\nable to understand which factors split explain the GHG emissions for each\nparticular company.\n"
    },
    {
        "paper_id": 2212.10917,
        "authors": "Eduardo Abi Jaber, Camille Illand and Shaun (Xiaoyuan) Li",
        "title": "The quintic Ornstein-Uhlenbeck volatility model that jointly calibrates\n  SPX & VIX smiles",
        "comments": "arXiv admin note: text overlap with arXiv:2212.08297",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/publicdomain/zero/1.0/",
        "abstract": "  The quintic Ornstein-Uhlenbeck volatility model is a stochastic volatility\nmodel where the volatility process is a polynomial function of degree five of a\nsingle Ornstein-Uhlenbeck process with fast mean reversion and large\nvol-of-vol. The model is able to achieve remarkable joint fits of the SPX-VIX\nsmiles with only 6 effective parameters and an input curve that allows to match\ncertain term structures. We provide several practical specifications of the\ninput curve, study their impact on the joint calibration problem and consider\nadditionally time-dependent parameters to help achieve better fits for longer\nmaturities going beyond 1 year. Even better, the model remains very simple and\ntractable for pricing and calibration: the VIX squared is again polynomial in\nthe Ornstein-Uhlenbeck process, leading to efficient VIX derivative pricing by\na simple integration against a Gaussian density; simulation of the volatility\nprocess is exact; and pricing SPX products derivatives can be done efficiently\nand accurately by standard Monte Carlo techniques with suitable antithetic and\ncontrol variates.\n"
    },
    {
        "paper_id": 2212.10974,
        "authors": "Stefan Loesch",
        "title": "The Quantitative Finance Aspects of Automated Market Markers in DeFi",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  Automated Market Makers (AMMs) are a class of smart contracts on Ethereum and\nother blockchains that \"make markets\" autonomously. In other words, AMMs stand\nready to trade with other market participants that interact with them, at the\nconditions determined by the AMM. In this this paper, which relies on the\nexisting and growing corpus of literature available, we review and present the\nkey mathematical and quantitative finance aspects that underpin their\noperations, including the interesting relationship between AMMs and derivatives\npricing and hedging.\n"
    },
    {
        "paper_id": 2212.11108,
        "authors": "d'Artis Kancs",
        "title": "Enhancing Resilience: Model-based Simulations",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
        "abstract": "  Since several years, the fragility of global supply chains (GSCs) is at\nhistorically high levels. In the same time, the landscape of hybrid threats is\nexpanding; new forms of hybrid threats create different types of uncertainties.\nThis paper aims to understand the potential consequences of uncertain events -\nlike natural disasters, pandemics, hybrid and/or military aggression - on GSC\nresilience and robustness. Leveraging a parsimonious supply chain model, we\nanalyse how the organisational structure of GSCs interacts with uncertainty,\nand how risk-aversion vs. ambiguity-aversion, vertical integration vs. upstream\noutsourcing, resilience vs. efficiency trade-offs drive a wedge between\ndecentralised and centralised optimal GSC diversification strategies in\npresence of externalities. Parameterising the scalable data model with\nWorld-Input Output Tables, we simulate the survival probability of a GSC and\nimplications for supply chain robustness and resilience. The presented\nmodel-based simulations provide an interoperable and directly comparable\nconceptualisation of positive and normative effects of counterfactual\nresilience and robustness policy choices under individually optimal\n(decentralised) and socially optimal (centralised) GSC organisation structures.\n"
    },
    {
        "paper_id": 2212.11518,
        "authors": "Huy\\^en Pham (UPD7, LPSM (UMR\\_8001)), Xavier Warin (EDF R\\&D, FiME\n  Lab)",
        "title": "Mean-field neural networks-based algorithms for McKean-Vlasov control\n  problems *",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper is devoted to the numerical resolution of McKean-Vlasov control\nproblems via the class of mean-field neural networks introduced in our\ncompanion paper [25] in order to learn the solution on the Wasserstein space.\nWe propose several algorithms either based on dynamic programming with control\nlearning by policy or value iteration, or backward SDE from stochastic maximum\nprinciple with global or local loss functions. Extensive numerical results on\ndifferent examples are presented to illustrate the accuracy of each of our\neight algorithms. We discuss and compare the pros and cons of all the tested\nmethods.\n"
    },
    {
        "paper_id": 2212.11585,
        "authors": "Gian Paolo Clemente and Alessandra Cornaro and Rosanna Grassi and\n  Giorgio Rizzini",
        "title": "Strategic energy flows in input-output relations: a temporal multilayer\n  approach",
        "comments": null,
        "journal-ref": "Applied Stochastic Models in Business and Industry (2023)",
        "doi": "10.1002/asmb.2783",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The energy consumption, the transfer of resources through the international\ntrade, the transition towards renewable energies and the environmental\nsustainability appear as key drivers in order to evaluate the resilience of the\nenergy systems. Concerning the consumptions, in the literature a great\nattention has been paid to direct energy, but the production of goods and\nservices also involves indirect energy. Hence, in this work we consider\ndifferent types of embodied energy sources and the time evolution of the\nsectors' and countries' interactions. Flows are indeed used to construct a\ndirected and weighted temporal multilayer network based respectively on\nrenewable and non-renewable sources, where sectors are nodes and layers are\ncountries. We provide a methodological approach for analysing the network\nreliability and resilience and for identifying critical sectors and economies\nin the system by applying the Multi-Dimensional HITS algorithm. Then, we\nevaluate central arcs in the network at each time period by proposing a novel\ntopological indicator based on the maximum flow problem. In this way, we\nprovide a full view of economies, sectors and connections that play a relevant\nrole over time in the network and whose removal could heavily affect the\nstability of the system. We provide a numerical analysis based on the embodied\nenergy flows among countries and sectors in the period from 1990 to 2016.\nResults prove that the methods are effective in catching the different patterns\nbetween renewable and non-renewable energy sources.\n"
    },
    {
        "paper_id": 2212.11752,
        "authors": "Matteo Burzoni, Alessandro Doldi and Enea Monzio Compagnoni",
        "title": "Risk Sharing with Deep Neural Networks",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We consider the problem of optimally sharing a financial position among\nagents with potentially different reference risk measures. The problem is\nequivalent to computing the infimal convolution of the risk metrics and finding\nthe so-called optimal allocations. We propose a neural network-based framework\nto solve the problem and we prove the convergence of the approximated\ninf-convolution, as well as the approximated optimal allocations, to the\ncorresponding theoretical values. We support our findings with several\nnumerical experiments.\n"
    },
    {
        "paper_id": 2212.11765,
        "authors": "Tanja Aue, Adam Jatowt, Michael F\\\"arber",
        "title": "Predicting Companies' ESG Ratings from News Articles Using Multivariate\n  Timeseries Analysis",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Environmental, social and governance (ESG) engagement of companies moved into\nthe focus of public attention over recent years. With the requirements of\ncompulsory reporting being implemented and investors incorporating\nsustainability in their investment decisions, the demand for transparent and\nreliable ESG ratings is increasing. However, automatic approaches for\nforecasting ESG ratings have been quite scarce despite the increasing\nimportance of the topic. In this paper, we build a model to predict ESG ratings\nfrom news articles using the combination of multivariate timeseries\nconstruction and deep learning techniques. A news dataset for about 3,000 US\ncompanies together with their ratings is also created and released for\ntraining. Through the experimental evaluation we find out that our approach\nprovides accurate results outperforming the state-of-the-art, and can be used\nin practice to support a manual determination or analysis of ESG ratings.\n"
    },
    {
        "paper_id": 2212.11766,
        "authors": "Zengjing Chen, Huaijin Liang, Wei Wang, Xiaodong Yan",
        "title": "Long bet will lose: demystifying seemingly fair gambling via two-armed\n  Futurity bandit",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  No matter how much some gamblers occasionally win, as long as they continue\nto gamble, sooner or later they will lose more to the casino, which is the\nso-called long bet will lose. Our results demonstrate the counter-intuitive\nphenomenon, that gamblers involved in long bets will lose but casinos always\nadvertise their unprofitable circumstances. Here we expose the law of\ninevitability behind long bet will loss by theoretically and experimentally\ndemystifying the profitable mystery behind casinos under two-armed antique\nMills Futurity slot machine. The main results straightforwardly elucidate that\nall casino projects are seemingly a fair gamble but essentially unfair, i.e.,\nthe casino's win rate is greater than 50%. We anticipate our assay to be a\nstarting point for studying the fairness of more sophisticated multi-armed\nFuturity bandits based on the mathematical tool. In application, a fairness\nstudy of the Futurity bandits not only exposes the fraud of casinos for\ngamblers but also discloses discount marketing, bundled sales, or other induced\nconsumption tactics.\n"
    },
    {
        "paper_id": 2212.11779,
        "authors": "Qi Deng, Lunge Dai, Zixin Yang, Zhong-guo Zhou, Monica Hussein, Dingyi\n  Chen, Mick Swartz",
        "title": "The Impact of Regulation Regime Changes on ChiNext IPOs: Effects of 2013\n  and 2020 Reforms on Pricing and Overreaction",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Since its inauguration, ChiNext has gone through three time periods with two\ndifferent regulation regimes and three different sets of listing day trading\nrestrictions. This paper studies the impact of regulation regimes and listing\nday trading restrictions on the initial return of ChiNext IPOs. We hypothesize\nthat the initial return of a ChiNext IPO contains the issuers intrinsic value\nand the investors overreaction. The intrinsic value is represented by the IPOs\n21st day return (monthly return), and the difference between the monthly and\ninitial returns (intramonth return) is a proxy of the overreaction. We find\nthat all significant variables for all three returns in all three time periods\nfall into four categories: pre-listing demand, post-listing demand, market\ncondition and pre-listing issuer value. We observe stark contrasts among\nvariable categories for each of the returns in the three time periods, which\nreveals an evolution of the investors behavior with regard to the progression\nof regulation regimes. Based on our findings, we argue that the differences\namong the levels and determinants of initial return, monthly return (intrinsic\nvalue) and intramonth return (overreaction) in different time periods can be\nlargely explained by regulation regime changes along two dimensions: 1)\napproval vs. registration and 2) listing day trading curbs and return limits.\nWe find that IPO pricing is demand-driven under the approval regime, but\nvalue-driven under the registration regime. We further compare the impact of\nregulation regime changes on ChiNext IPO pricing practice, and propose a future\nresearch plan on ChiNext IPO pricing efficiency with policy implication.\n"
    },
    {
        "paper_id": 2212.11787,
        "authors": "Jinhui Li",
        "title": "Macro carbon price prediction with support vector regression and Paris\n  accord targets",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
        "abstract": "  Carbon neutralization is an urgent task in society because of the global\nwarming threat. And carbon trading is an essential market mechanics to solve\ncarbon reduction targets. Macro carbon price prediction is vital in the useful\nmanagement and decision-making of the carbon market. We focus on the EU carbon\nmarket and we choose oil price, coal price, gas price, and DAX index to be the\nfour market factors in predicting carbon price, and also we select carbon\nemission targets from Paris Accord as the political factor in the carbon market\nin terms of the macro view of the carbon price prediction. Thus we use these\nfive factors as inputs to predict the future carbon yearly price in 2030 with\nthe support vector regression models. We use grid search and cross validation\nto guarantee the prediction performance of our models. We believe this model\nwill have great applications in the macro carbon price prediction.\n"
    },
    {
        "paper_id": 2212.11833,
        "authors": "Timo Dimitriadis and Roxana Halbleib and Jeannine Polivka and Jasper\n  Rennspies and Sina Streicher and Axel Friedrich Wolter",
        "title": "Efficient Sampling for Realized Variance Estimation in Time-Changed\n  Diffusion Models",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper analyzes the benefits of sampling intraday returns in intrinsic\ntime for the standard and pre-averaging realized variance (RV) estimators. We\ntheoretically show in finite samples and asymptotically that the RV estimator\nis most efficient under the new concept of realized business time, which\nsamples according to a combination of observed trades and estimated tick\nvariance. Our asymptotic results carry over to the pre-averaging RV estimator\nunder market microstructure noise. The analysis builds on the assumption that\nasset prices follow a diffusion that is time-changed with a jump process that\nseparately models the transaction times. This provides a flexible model that\nseparately captures the empirically varying trading intensity and tick variance\nprocesses, which are particularly relevant for disentangling the driving forces\nof the sampling schemes. Extensive simulations confirm our theoretical results\nand show that realized business time remains superior also under more general\nnoise and process specifications. An application to stock data provides\nempirical evidence for the benefits of using realized business time sampling to\nconstruct more efficient RV estimators as well as for an improved forecasting\nperformance.\n"
    },
    {
        "paper_id": 2212.11917,
        "authors": "Jean-Fran\\c{c}ois Chassagneux and Mohan Yang",
        "title": "Convergence of particles and tree based scheme for singular FBSDEs",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We study an implementation of the theoretical splitting scheme introduced in\n[Chassagneux and Yang, 2022] for singular FBSDEs [Carmona and Delarue 2013] and\ntheir associated quasi-linear degenerate PDEs. The fully implementable\nalgorithm is based on particles approximation of the transport operator and\ntree like approximation of the diffusion operator appearing in the theoretical\nsplitting. We prove the convergence with a rate of our numerical method under\nsome reasonable conditions on the coefficients functions. This validates a\nposteriori some numerical results obtained in [Chassagneux and Yang, 2022]. We\nconclude the paper with a numerical section presenting various implementations\nof the algorithm and discussing their efficiency in practice.\n"
    },
    {
        "paper_id": 2212.12018,
        "authors": "Pierre Bras and Gilles Pag\\`es",
        "title": "Langevin algorithms for Markovian Neural Networks and Deep Stochastic\n  control",
        "comments": "12 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Stochastic Gradient Descent Langevin Dynamics (SGLD) algorithms, which add\nnoise to the classic gradient descent, are known to improve the training of\nneural networks in some cases where the neural network is very deep. In this\npaper we study the possibilities of training acceleration for the numerical\nresolution of stochastic control problems through gradient descent, where the\ncontrol is parametrized by a neural network. If the control is applied at many\ndiscretization times then solving the stochastic control problem reduces to\nminimizing the loss of a very deep neural network. We numerically show that\nLangevin algorithms improve the training on various stochastic control problems\nlike hedging and resource management, and for different choices of gradient\ndescent methods.\n"
    },
    {
        "paper_id": 2212.12044,
        "authors": "Pouriya Khalilian, Sara Azizi, Mohammad Hossein Amiri, and Javad T.\n  Firouzjaee",
        "title": "Design interpretable experience of dynamical feed forward machine\n  learning model for forecasting NASDAQ",
        "comments": "21 pages, 5 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  National Association of Securities Dealers Automated Quotations(NASDAQ) is an\nAmerican stock exchange based. It is one of the most valuable stock economic\nindices in the world and is located in New York City \\cite{pagano2008quality}.\nThe volatility of the stock market and the influence of economic indicators\nsuch as crude oil, gold, and the dollar in the stock market, and NASDAQ shares\nare also affected and have a volatile and chaotic nature\n\\cite{firouzjaee2022lstm}.In this article, we have examined the effect of oil,\ndollar, gold, and the volatility of the stock market in the economic market,\nand then we have also examined the effect of these indicators on NASDAQ stocks.\nThen we started to analyze the impact of the feedback on the past prices of\nNASDAQ stocks and its impact on the current price. Using PCA and Linear\nRegression algorithm, we have designed an optimal dynamic learning experience\nfor modeling these stocks. The results obtained from the quantitative analysis\nare consistent with the results of the qualitative analysis of economic\nstudies, and the modeling done with the optimal dynamic experience of machine\nlearning justifies the current price of NASDAQ shares.\n"
    },
    {
        "paper_id": 2212.12051,
        "authors": "Emmanuel Alanis, Sudheer Chava, Agam Shah",
        "title": "Benchmarking Machine Learning Models to Predict Corporate Bankruptcy",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Using a comprehensive sample of 2,585 bankruptcies from 1990 to 2019, we\nbenchmark the performance of various machine learning models in predicting\nfinancial distress of publicly traded U.S. firms. We find that gradient boosted\ntrees outperform other models in one-year-ahead forecasts. Variable permutation\ntests show that excess stock returns, idiosyncratic risk, and relative size are\nthe more important variables for predictions. Textual features derived from\ncorporate filings do not improve performance materially. In a credit\ncompetition model that accounts for the asymmetric cost of default\nmisclassification, the survival random forest is able to capture large dollar\nprofits.\n"
    },
    {
        "paper_id": 2212.1206,
        "authors": "Weiwei Xiong, Katsumasa Tanaka, Philippe Ciais, Daniel J. A.\n  Johansson, Mariliis Lehtveer",
        "title": "emIAM v1.0: an emulator for Integrated Assessment Models using marginal\n  abatement cost curves",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  We developed an emulator for Integrated Assessment Models (emIAM) based on a\nmarginal abatement cost (MAC) curve approach. Using the output of IAMs in the\nENGAGE Scenario Explorer and the GET model, we derived a large set of MAC\ncurves: ten IAMs; global and eleven regions; three gases CO2, CH4, and N2O;\neight portfolios of available mitigation technologies; and two emission\nsources. We tested the performance of emIAM by coupling it with a simple\nclimate model ACC2. We found that the optimizing climate-economy model\nemIAM-ACC2 adequately reproduced a majority of original IAM emission outcomes\nunder similar conditions, allowing systematic explorations of IAMs with small\ncomputational resources. emIAM can expand the capability of simple climate\nmodels as a tool to calculate cost-effective pathways linked directly to a\ntemperature target.\n"
    },
    {
        "paper_id": 2212.12285,
        "authors": "Mois\\'es Ram\\'irez, Raziel Ru\\'iz, Nathan Klarer",
        "title": "The Effects of Just-in-time Delivery on Social Engagement: A Cluster\n  Analysis",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Fooji Inc. is a social media engagement platform that has created a\nproprietary \"Just-in-time\" delivery network to provide prizes to social media\nmarketing campaign participants in real-time. In this paper, we prove the\nefficacy of the \"Just-in-time\" delivery network through a cluster analysis that\nextracts and presents the underlying drivers of campaign engagement.\n  We utilize a machine learning methodology with a principal component analysis\nto organize Fooji campaigns across these principal components. The arrangement\nof data across the principal component space allows us to expose underlying\ntrends using a $K$-means clustering technique. The most important of these\ntrends is the demonstration of how the \"Just-in-time\" delivery network improves\nsocial media engagement.\n"
    },
    {
        "paper_id": 2212.12318,
        "authors": "Marco Di Francesco and Kevin Kamm",
        "title": "CDO calibration via Magnus Expansion and Deep Learning",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  In this paper, we improve the performance of the large basket approximation\ndeveloped by Reisinger et al. to calibrate Collateralized Debt Obligations\n(CDO) to iTraxx market data. The iTraxx tranches and index are computed using a\nbasket of size $K= 125$. In the context of the large basket approximation, it\nis assumed that this is sufficiently large to approximate it by a limit SPDE\ndescribing the portfolio loss of a basket with size $K\\rightarrow \\infty$. For\nthe resulting SPDE, we show four different numerical methods and demonstrate\nhow the Magnus expansion can be applied to efficiently solve the large basket\nSPDE with high accuracy. Moreover, we will calibrate a structural model to the\navailable market data. For this, it is important to efficiently infer the\nso-called initial distances to default from the Credit Default Swap (CDS)\nquotes of the constituents of the iTraxx for the large basket approximation. We\nwill show how Deep Learning techniques can help us to improve the performance\nof this step significantly. We will see in the end a good fit to the market\ndata and develop a highly parallelizable numerical scheme using GPU and\nmultithreading techniques.\n"
    },
    {
        "paper_id": 2212.12356,
        "authors": "Dario Mazzilli, Manuel Sebastian Mariani, Flaviano Morone, Aurelio\n  Patelli",
        "title": "Equivalence between the Fitness-Complexity and the Sinkhorn-Knopp\n  algorithms",
        "comments": "14 pages, 4 figures",
        "journal-ref": null,
        "doi": "10.1088/2632-072X/ad2697",
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  We uncover the connection between the Fitness-Complexity algorithm, developed\nin the economic complexity field, and the Sinkhorn-Knopp algorithm, widely used\nin diverse domains ranging from computer science and mathematics to economics.\nDespite minor formal differences between the two methods, both converge to the\nsame fixed-point solution up to normalization. The discovered connection allows\nus to derive a rigorous interpretation of the Fitness and the Complexity\nmetrics as the potentials of a suitable energy function. Under this\ninterpretation, high-energy products are unfeasible for low-fitness countries,\nwhich explains why the algorithm is effective at displaying nested patterns in\nbipartite networks. We also show that the proposed interpretation reveals the\nscale invariance of the Fitness-Complexity algorithm, which has practical\nimplications for the algorithm's implementation in different datasets. Further,\nanalysis of empirical trade data under the new perspective reveals three\ncategories of countries that might benefit from different development\nstrategies.\n"
    },
    {
        "paper_id": 2212.12398,
        "authors": "Ariah Klages-Mundt, Steffen Schuldenzucker",
        "title": "Designing Autonomous Markets for Stablecoin Monetary Policy",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We develop a new type of automated market maker (AMM) that helps to maintain\nstability and long-term viability in a stablecoin. This primary market AMM\n(P-AMM) is an autonomous mechanism for pricing minting and redemption of\nstablecoins in all possible states and is designed to achieve several desirable\nproperties. We first cover several case studies of current ad hoc stablecoin\nissuance and redemption mechanisms, several of which have contributed to recent\nstablecoin de-peggings, and formulate desirable properties of a P-AMM that\nsupport stability and usability. We then design a P-AMM redemption curve and\nshow that it satisfies these properties, including bounded loss for both the\nprotocol and stablecoin holders. We further show that this redemption curve is\npath independent and has properties of path deficiency in extended settings\ninvolving trading fees and a separate minting curve. This means that system\nhealth weakly improves relative to the path independent setting along any\ntrading curve and that there is no incentive to strategically subdivide\nredemptions. Finally, we show how to implement the P-AMM efficiently on-chain.\n"
    },
    {
        "paper_id": 2212.12687,
        "authors": "F. Campigli, G. Bormetti, F. Lillo",
        "title": "Measuring price impact and information content of trades in a\n  time-varying setting",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We propose a non-linear observation-driven version of the Hasbrouck (1991)\nmodel for dynamically estimating trades' market impact and information content.\nWe find that market impact displays an intraday pattern superimposed with large\nfluctuations. Some of them are exogenous, and, as an example, we investigate\nmarket impact dynamics around FOMC announcements. Contrary to Hasbrouck (1991),\nwe find that the information content of trades depends on the local liquidity\nlevel and the recent history of prices and trades. Finally, we use the model to\nestimate the time-varying permanent impact parameter, which allows performing a\ndynamic transaction cost analysis.\n"
    },
    {
        "paper_id": 2212.12717,
        "authors": "Jinan Zou, Qingying Zhao, Yang Jiao, Haiyao Cao, Yanxi Liu, Qingsen\n  Yan, Ehsan Abbasnejad, Lingqiao Liu, Javen Qinfeng Shi",
        "title": "Stock Market Prediction via Deep Learning Techniques: A Survey",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Existing surveys on stock market prediction often focus on traditional\nmachine learning methods instead of deep learning methods. This motivates us to\nprovide a structured and comprehensive overview of the research on stock market\nprediction. We present four elaborated subtasks of stock market prediction and\npropose a novel taxonomy to summarize the state-of-the-art models based on deep\nneural networks. In addition, we also provide detailed statistics on the\ndatasets and evaluation metrics commonly used in the stock market. Finally, we\npoint out several future directions by sharing some new perspectives on stock\nmarket prediction.\n"
    },
    {
        "paper_id": 2212.12725,
        "authors": "Alessandro Gnoatto, Silvia Lavagnini, Athena Picarelli",
        "title": "Deep Quadratic Hedging",
        "comments": "43 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We present a novel computational approach for quadratic hedging in a\nhigh-dimensional incomplete market. This covers both mean-variance hedging and\nlocal risk minimization. In the first case, the solution is linked to a system\nof BSDEs, one of which being a backward stochastic Riccati equation (BSRE); in\nthe second case, the solution is related to the F\\\"olmer-Schweizer\ndecomposition and is also linked to a BSDE. We apply (recursively) a deep\nneural network-based BSDE solver. Thanks to this approach, we solve\nhigh-dimensional quadratic hedging problems, providing the entire hedging\nstrategies paths, which, in alternative, would require to solve high\ndimensional PDEs. We test our approach with a classical Heston model and with a\nmulti-dimensional generalization of it.\n"
    },
    {
        "paper_id": 2212.12742,
        "authors": "Jann Michael Weinand, Maximilian Hoffmann, Jan G\\\"opfert, Tom Terlouw,\n  Julian Sch\\\"onau, Patrick Kuckertz, Russell McKenna, Leander Kotzur, Jochen\n  Lin{\\ss}en, Detlef Stolten",
        "title": "Global LCOEs of decentralized off-grid renewable energy systems",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Recent global events emphasize the importance of a reliable energy supply.\nOne way to increase energy supply security is through decentralized off-grid\nrenewable energy systems, for which a growing number of case studies are\nresearched. This review gives a global overview of the levelized cost of\nelectricity (LCOE) for these autonomous energy systems, which range from 0.03\n\\$_{2021}/kWh to over 1.00 \\$_{2021}/kWh worldwide. The average LCOEs for 100%\nrenewable energy systems have decreased by 9% annually between 2016 and 2021\nfrom 0.54 \\$_{2021}/kWh to 0.29 \\$_{2021}/kWh, presumably due to cost\nreductions in renewable energy and storage technologies. Furthermore, we\nidentify and discuss seven key reasons why LCOEs are frequently overestimated\nor underestimated in literature, and how this can be prevented in the future.\nOur overview can be employed to verify findings on off-grid systems, to assess\nwhere these systems might be deployed and how costs evolve.\n"
    },
    {
        "paper_id": 2212.12774,
        "authors": "Maria A. Shishanina, Anatoly A. Sidorov",
        "title": "Knowledge Management in Management of Social and Economic Development of\n  Municipalities: Highlights",
        "comments": "arXiv admin note: substantial text overlap with arXiv:2111.13690",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  The paper discusses the process of social and economic development of\nmunicipalities. A conclusion is made that developing an adequate model of\nsocial and economic development using conventional approaches presents a\nconsiderable challenge. It is proposed to use semantic modeling to represent\nthe social and economic development of municipalities, and cognitive mapping to\nidentify the set of connections that occur among indicators and that have a\ndirect impact on social and economic development.\n"
    },
    {
        "paper_id": 2212.12796,
        "authors": "Roxana Guti\\'errez-Romero",
        "title": "Violence in Guatemala pushes adults and children to seek work in Mexico",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  This article estimates the impact of violence on emigration crossings from\nGuatemala to Mexico as final destination during 2009-2017. To identify causal\neffects, we use as instruments the variation in deforestation in Guatemala, and\nthe seizing of cocaine in Colombia. We argue that criminal organizations\ndeforest land in Guatemala, fueling violence and leading to emigration,\nparticularly during exogenous supply shocks to cocaine. A one-point increase in\nthe homicide rate differential between Guatemalan municipalities and Mexico,\nleads to 211 additional emigration crossings made by male adults. This rise in\nviolence, also leads to 20 extra emigration crossings made by children.\n"
    },
    {
        "paper_id": 2212.12797,
        "authors": "Roxana Guti\\'errez-Romero and Nayeli Salgado",
        "title": "New trends in South-South migration: The economic impact of COVID-19 and\n  immigration enforcement",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  This paper evaluates the impact of the pandemic and enforcement at the US and\nMexican borders on the emigration of Guatemalans during 2017-2020. During this\nperiod, the number of crossings from Guatemala fell by 10%, according to the\nSurvey of Migration to the Southern Border of Mexico. Yet, there was a rise of\nnearly 30% in the number of emigration crossings of male adults travelling with\ntheir children. This new trend was partly driven by the recent reduction in the\nnumber of children deported from the US. For a one-point reduction in the\nnumber of children deported from the US to Guatemalan municipalities, there was\nan increase of nearly 14 in the number of crossings made by adult males leaving\nfrom Guatemala for Mexico; and nearly 0.5 additional crossings made by male\nadults travelling with their children. However, the surge of emigrants\ntravelling with their children was also driven by the acute economic shock that\nGuatemala experienced during the pandemic. During this period, air pollution in\nthe analysed Guatemalan municipalities fell by 4%, night light per capita fell\nby 15%, and homicide rates fell by 40%. Unlike in previous years, emigrants are\nfleeing poverty rather than violence. Our findings suggest that a reduction in\nviolence alone will not be sufficient to reduce emigration flows from Central\nAmerica, but that economic recovery is needed.\n"
    },
    {
        "paper_id": 2212.12854,
        "authors": "Libo Li, Ruyi Liu, Marek Rutkowski",
        "title": "Well-posedness and penalization schemes for generalized BSDEs and\n  reflected generalized BSDEs",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The paper is directly motivated by the pricing of vulnerable European and\nAmerican options in a general hazard process setup and a related study of the\ncorresponding pre-default backward stochastic differential equations (BSDE) and\npre-default reflected backward stochastic differential equations (RBSDE). We\nwork with a generic filtration $\\FF$ for which the martingale representation\nproperty is assumed to hold with respect to a square-integrable martingale $M$\nand the goal of this work is of twofold. First, we aim to establish the\nwell-posedness results and comparison theorems for a generalized BSDE and a\nreflected generalized BSDE with a continuous and nondecreasing driver $A$.\nSecond, we study extended penalization schemes for a generalized BSDE and a\nreflected generalized BSDE in which we penalize against the driver in order to\nobtain in the limit either a particular optimal stopping problem or a Dynkin\ngame in which the set of admissible exercise time is constrained to the right\nsupport of the measure generated by $A$.\n"
    },
    {
        "paper_id": 2212.1286,
        "authors": "Libo Li, Ruyi Liu, Marek Rutkowski",
        "title": "Vulnerable European and American Options in a Market Model with Optional\n  Hazard Process",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We study the upper and lower bounds for prices of European and American style\noptions with the possibility of an external termination, meaning that the\ncontract may be terminated at some random time. Under the assumption that the\nunderlying market model is incomplete and frictionless, we obtain duality\nresults linking the upper price of a vulnerable European option with the price\nof an American option whose exercise times are constrained to times at which\nthe external termination can happen with a non-zero probability. Similarly, the\nupper and lower prices for an vulnerable American option are linked to the\nprice of an American option and a game option, respectively. In particular, the\nminimizer of the game option is only allowed to stop at times which the\nexternal termination may occur with a non-zero probability.\n"
    },
    {
        "paper_id": 2212.12891,
        "authors": "Nanang Adie Setyawan, Hadiahti Utami, Bayu Setyo Nugroho, Mellasanti\n  Ayuwardani, and Suharmanto",
        "title": "Analysis of the Driving Factors of Implementing Green Supply Chain\n  Management in SME in the City of Semarang",
        "comments": "7 pages, Published with International Research Journal of Economics\n  and Management Studies (IRJEMS)",
        "journal-ref": "International Research Journal of Economics and Management\n  Studies, Vol 1, issue 2 (2022)",
        "doi": "10.56472/25835238/IRJEMS-V1I2P107",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This study set out to determine what motivated SMEs in Semarang City to\nundertake green supply chain management during the COVID-19 and New Normal\npandemics. The purposive sampling approach was used as the sampling methodology\nin this investigation. There are 100 respondents in the research samples. The\nAMOS 24.0 program's structural equation modelling (SEM) is used in this\nresearch method. According to the study's findings, the Strategic Orientation\nvariable significantly and favourably affects the Green Supply Chain Management\nvariable expected to have a value of 0.945, and the Government Regulation\nvariable has a positive and strong influence on the variable Green Supply Chain\nManagement with an estimated value of 0.070, the Green Supply Chain Management\nvariable with an estimated value of has a positive and significant impact on\nthe environmental performance variable. 0.504, the Strategic Orientation\nvariable with an estimated value of has a positive and significant impact on\nthe environmental performance variable. 0.442, The Environmental Performance\nvariable is directly impacted positively and significantly by the Government\nRegulation variable, with an estimated value of 0.041. This significant\npositive influence is because SMEs in Semarang City have government\nregulations, along with government support for facilities regarding efforts to\nimplement the concept of environmental concern, causing high environmental\nperformance caused by the optimal implementation of Green supply chain\nmanagement is built on a collaboration between the government and the supply\nchain's participants.\n"
    },
    {
        "paper_id": 2212.13176,
        "authors": "Petar Jolakoski, Arnab Pal, Trifce Sandev, Ljupco Kocarev, Ralf\n  Metzler, and Viktor Stojkoski",
        "title": "The fate of the American dream: A first passage under resetting approach\n  to income dynamics",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Detailed knowledge of individual income dynamics is crucial for investigating\nthe existence of the American dream: Are we able to improve our income status\nduring our working life? This key question simply boils down to observing\nindividual status and how it moves between two thresholds: the current income\nand the desired income. Yet, our knowledge of these temporal properties of\nincome remains limited since we rely on estimates coming from transition\nmatrices which simplify income dynamics by aggregating the individual changes\ninto quantiles and thus overlooking significant microscopic variations. Here,\nwe bridge this gap by employing First Passage Time concepts in a baseline\nstochastic process with resetting used for modeling income dynamics and\ndeveloping a framework that is able to crucially disaggregate the temporal\nproperties of income to the level of an individual worker. We find analytically\nand illustrate numerically that our framework is orthogonal to the transition\nmatrix approach and leads to improved and more granular estimates. Moreover, to\nfacilitate empirical applications of the framework, we introduce a publicly\navailable statistical methodology, and showcase the application using the USA\nincome dynamics data. These results help to improve our understanding on the\ntemporal properties of income in real economies and provide a set of tools for\ndesigning policy interventions.\n"
    },
    {
        "paper_id": 2212.13188,
        "authors": "Yuri Kabanov, Arthur Sidorenko",
        "title": "An Axiomatic Viewpoint on the Rogers--Veraart and Suzuki--Elsinger\n  Models of Systemic Risk",
        "comments": "10 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/publicdomain/zero/1.0/",
        "abstract": "  We study a model of clearing in an interbank network with crossholdings and\ndefault charges. Following the Eisenberg--Noe approach, we define the model via\na set of natural financial regulations including those related with eventual\ndefault charges and derive a finite family of fixpoint problems. These problems\nare parameterized by vectors of binary variables. Our model combines features\nof the Ararat--Meimanjanov, Rogers--Veraart, and Suzuki--Elsinger networks. We\ndevelop methods of computing the maximal and minimal clearing pairs using the\nmixed integer-linear programming and a Gaussian elimination algorithm.\n"
    },
    {
        "paper_id": 2212.13371,
        "authors": "Tim Johnson and Nick Obradovich",
        "title": "Measuring an artificial intelligence agent's trust in humans using\n  machine incentives",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Scientists and philosophers have debated whether humans can trust advanced\nartificial intelligence (AI) agents to respect humanity's best interests. Yet\nwhat about the reverse? Will advanced AI agents trust humans? Gauging an AI\nagent's trust in humans is challenging because--absent costs for\ndishonesty--such agents might respond falsely about their trust in humans. Here\nwe present a method for incentivizing machine decisions without altering an AI\nagent's underlying algorithms or goal orientation. In two separate experiments,\nwe then employ this method in hundreds of trust games between an AI agent (a\nLarge Language Model (LLM) from OpenAI) and a human experimenter (author TJ).\nIn our first experiment, we find that the AI agent decides to trust humans at\nhigher rates when facing actual incentives than when making hypothetical\ndecisions. Our second experiment replicates and extends these findings by\nautomating game play and by homogenizing question wording. We again observe\nhigher rates of trust when the AI agent faces real incentives. Across both\nexperiments, the AI agent's trust decisions appear unrelated to the magnitude\nof stakes. Furthermore, to address the possibility that the AI agent's trust\ndecisions reflect a preference for uncertainty, the experiments include two\nconditions that present the AI agent with a non-social decision task that\nprovides the opportunity to choose a certain or uncertain option; in those\nconditions, the AI agent consistently chooses the certain option. Our\nexperiments suggest that one of the most advanced AI language models to date\nalters its social behavior in response to incentives and displays behavior\nconsistent with trust toward a human interlocutor when incentivized.\n"
    },
    {
        "paper_id": 2212.13611,
        "authors": "M\\'aximo Flor\\'in, Rafael U. Gos\\'alvez",
        "title": "Hidden costs of La Mancha's production model and drivers of change",
        "comments": "3 figures, manuscript prepared with the material presented to the I\n  Congreso de La Mancha, held online on December 17th, 2022",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  The territory of La Mancha, its rural areas, and its landscapes suffer a kind\nof atherosclerosis (\"the silent killer\") because of the increase in artificial\nsurfaces, the fragmentation of the countryside by various infrastructures, the\nabandonment of small and medium-sized farms and the loss of agricultural,\nmaterial, and intangible heritage. At the same time, agricultural\nindustrialization hides, behind a supposed productive efficiency, the\ndeterioration of the quantitative and qualitative ecological status of surface\nand groundwater bodies, and causes air pollution, greenhouse gas emissions,\nloss of soil fertility, drainage and plowing of wetlands, forgetfulness of the\nancestral environmental heritage, of the emergence of uses and customs of\ncollective self-government and reduction of the adaptive capacity of\ntraditional agroecosystems. This work aims, firstly, to shed light on the true\ncosts of the main causes of environmental degradation in the territory of La\nMancha, while deteriorating relations between rural and urban areas and\ndetermining the loss of territorial identity of La Mancha. the population. In\naddition, drivers of change toward a more sustainable social, economic,\nhydrological, environmental, and cultural production model are identified.\n"
    },
    {
        "paper_id": 2212.13622,
        "authors": "Ali R. Kaazempur-Mofrad",
        "title": "A Statistical Inquiry into Gender-Based Income Inequality in Canada",
        "comments": "14 pages, 3 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Income inequality distribution between social groups has been a global\nchallenge. The focus of this study is to investigate the potential impact of\nfemale income on family size and purchasing power. Using statistical methods\nsuch as simple linear regression, maximum likelihood analysis, and hypothesis\ntesting, I evaluated and investigated the variability of female pre-tax income\nwith respect to family size. The results obtained from this study illustrate\nthat for each additional household member, the average purchasing power\ndecreases. Additionally, the Bayesian analysis indicates that the probability\nfor an individual with a pre-tax income of at least one and two standard\ndeviations above the population mean is female is approximately 1/3 and 1/4,\nrespectively, further highlighting the gender-based income inequality in\nCanada. This analysis concludes that although female pre-tax income has no\nstatistically significant impact on family size, the female pre-tax income per\nperson has a statistically significant impact on family size.\n"
    },
    {
        "paper_id": 2212.13628,
        "authors": "Bruno Dupire and Valentin Tissot-Daguette",
        "title": "Functional Expansions",
        "comments": "39 pages, 7 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Path dependence is omnipresent in many disciplines such as engineering,\nsystem theory and finance. It reflects the influence of the past on the future,\noften expressed through functionals. However, non-Markovian problems are often\ninfinite-dimensional, thus challenging from a conceptual and computational\nperspective. In this work, we shed light on expansions of functionals. First,\nwe treat static expansions made around paths of fixed length and propose a\ngeneralization of the Wiener series$-$the intrinsic value expansion (IVE). In\nthe dynamic case, we revisit the functional Taylor expansion (FTE). The latter\nconnects the functional It\\^o calculus with the signature to quantify the\neffect in a functional when a \"perturbation\" path is concatenated with the\nsource path. In particular, the FTE elegantly separates the functional from\nfuture trajectories. The notions of real analyticity and radius of convergence\nare also extended to the path space. We discuss other dynamic expansions\narising from Hilbert projections and the Wiener chaos, and finally show\nfinancial applications of the FTE to the pricing and hedging of exotic\ncontingent claims.\n"
    },
    {
        "paper_id": 2212.13815,
        "authors": "Jinge Bao, Patrick Rebentrost",
        "title": "Fundamental theorem for quantum asset pricing",
        "comments": "19 pages, 2 figures, presentation modified, typos corrected",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Quantum computers have the potential to provide an advantage for financial\npricing problems by the use of quantum estimation. In a broader context, it is\nreasonable to ask about situations where the market and the assets traded on\nthe market themselves have quantum properties. In this work, we consider a\nfinancial setting where instead of by classical probabilities the market is\ndescribed by a pure quantum state or, more generally, a quantum density\noperator. This setting naturally leads to a new asset class, which we call\nquantum assets. Under the assumption that such assets have a price and can be\ntraded, we develop an extended definition of arbitrage to quantify gains\nwithout the corresponding risk. Our main result is a quantum version of the\nfirst fundamental theorem of asset pricing. If and only if there is no\narbitrage, there exists a risk-free density operator under which all assets are\nmartingales. This density operator is used for the pricing of quantum\nderivatives. To prove the theorem, we study the density operator version of the\nRadon-Nikodym measure change. We provide examples to illustrate the theory.\n"
    },
    {
        "paper_id": 2212.13839,
        "authors": "Szabolcs Nagy, Sergey U. Chernikov, Ekaterina Degtereva",
        "title": "The Impact of the Pharmaceutical Industry on the Innovation Performance\n  of European Countries",
        "comments": null,
        "journal-ref": "Regional Statistics, Vol. 13. No. 1. 2023",
        "doi": "10.15196/RS130105",
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  There are significant differences in innovation performance between\ncountries. Additionally, the pharmaceutical sector is stronger in some\ncountries than in others. This suggests that the development of the\npharmaceutical industry can influence a country's innovation performance. Using\nthe Global Innovation Index and selected performance measures of the\npharmaceutical sector, this study examines how the pharmaceutical sector\ninfluences the innovation performance of countries from the European context.\nThe dataset of 27 European countries was analysed using simple, and multiple\nlinear regressions and Pearson correlation. Our findings show that only three\nindicators of the pharmaceutical industry, more precisely pharmaceutical\nResearch and Development, pharmaceutical exports, and pharmaceutical employment\nexplain the innovation performance of a country largely. Pharmaceutical\nResearch and Development and exports have a significant positive impact on a\ncountry's innovation performance, whereas employment in the pharmaceutical\nindustry has a slightly negative impact. Additionally, global innovation\nperformance has been found to positively influence life expectancy. We further\noutline the implications and possible policy directions based on these\nfindings.\n"
    },
    {
        "paper_id": 2212.1384,
        "authors": "Szabolcs Nagy, Mariann Veresne Somosi",
        "title": "The relationship between social innovation and digital economy and\n  society",
        "comments": null,
        "journal-ref": "Regional Statistics, Vol. 12. No. 2. 2022, 3-29;",
        "doi": "10.15196/RS120202",
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  The information age is also an era of escalating social problems. The digital\ntransformation of society and the economy is already underway in all countries,\nalthough the progress in this transformation can vary widely. There are more\nsocial innovation projects addressing global and local social problems in some\ncountries than in others. This suggests that different levels of digital\ntransformation might influence the social innovation potential. Using the\nInternational Digital Economy and Society Index and the Social Innovation\nIndex, this study investigates how digital transformation of the economy and\nsociety affects the capacity for social innovation. A dataset of 29 countries\nwas analysed using both simple and multiple linear regressions and Pearsons\ncorrelation. Based on the research findings, it can be concluded that the\ndigital transformation of the economy and society has a significant positive\nimpact on the capacity for social innovation. It was also found that the\nintegration of digital technology plays a critical role in digital\ntransformation. Therefore, the progress in digital transformation is beneficial\nto social innovation capacity. In line with the research findings, this study\noutlines the implications and possible directions for policy.\n"
    },
    {
        "paper_id": 2212.13841,
        "authors": "Han-Sol Lee, Sergey U. Chernikov, Szabolcs Nagy",
        "title": "Motivations and locational factors of FDI in CIS countries: Empirical\n  evidence from South Korean FDI in Kazakhstan, Russia, and Uzbekistan",
        "comments": null,
        "journal-ref": "Regional Statistics, Vol. 11. No. 4. 2021, 79_100;",
        "doi": "10.15196/RS110404",
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Considering the growing significance of Eurasian economic ties because of\nSouth Korea s New Northern Policy and Russia s New Eastern Policy, this study\ninvestigates the motivations and locational factors of South Korean foreign\ndirect investment (FDI) in three countries in the Commonwealth of Independent\nStates (CIS: Kazakhstan, Russia, and Uzbekistan) by employing panel analysis\n(pooled ordinary least squares (OLS), fixed effects, random effects) using data\nfrom 1993 to 2017. The results show the positive and significant coefficients\nof GDP, resource endowments, and inflation. Unlike conventional South Korean\noutward FDI, labour-seeking is not defined as a primary purpose. Exchange\nrates, political rights, and civil liberties are identified as insignificant.\nThe authors conclude that South Korean FDI in Kazakhstan, Russia, and\nUzbekistan is associated with market-seeking (particularly in Kazakhstan and\nRussia) and natural resource-seeking, especially the former. From a policy\nperspective, our empirical evidence suggests that these countries host\ngovernments could implement mechanisms to facilitate the movement of goods\nacross regions and countries to increase the attractiveness of small local\nmarkets. The South Korean government could develop financial support and risk\nsharing programmes to enhance natural resource-seeking investments and mutual\nexchange programmes to overcome the red syndrome complex in South Korean\nsociety.\n"
    },
    {
        "paper_id": 2212.13864,
        "authors": "Samuel Solgon Santos, Marcelo Brutti Righi, Eduardo de Oliveira Horta",
        "title": "The limitations of comonotonic additive risk measures: a literature\n  review",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Risk measures satisfying the axiom of comonotonic additivity are extensively\nstudied, arguably because of the plethora of results indicating interesting\naspects of such risk measures. Recent research, however, has shown that this\naxiom is incompatible with central properties in specific contexts. In this\npaper, we present a literature review of these incompatibilities. In addition,\nwe use the Choquet representation of comonotonic additive risk measures to show\nthey cannot be surplus invariant.\n"
    },
    {
        "paper_id": 2212.13901,
        "authors": "Patrice Ballester (GEODE, UPPA, MSHS-T, UNS-IAE Nice)",
        "title": "Barcelona in the face of globalization, how to think of the city through\n  the organization and evaluation of major events?",
        "comments": "in French language",
        "journal-ref": "SPINDLER Jacques, HURON, David (Dir.). L'{\\'e}valuation de\n  l'{\\'e}v{\\'e}nementiel touristique, Cinqui{\\`e}me partie, ''L'eph{\\`e}mere\n  touristique est-il durable?'' (Chapitre 3), L'Harmattan, pp.463 - 483, 2009,\n  9782296102392",
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The event questions men whether it is political, cultural or touristic. It\nhas its own meaning as it starts something while showing a will, a new\npossibility to create, to meet and to surprise. The event is in fact an \"advent\nthat reaches everything\" generally integrating itself into a long process or\nphase of the evolution of societies in terms of its societal structure.\nHowever, if the event exists, it is significant and therefore assessable. How\nto evaluate an Olympic Games or an international exhibition? Can we evaluate\nthe before (from the statement of position, application file), the during (the\ncourse of the tourist, cultural or sporting event) and the after (the closing\nof the festival and its direct and indirect effects on society)? How then to\nevaluate all the dimensions of the event generally based on major tourist and\nurban planning operations? We will take as a field of study a city -- capital\n-- commercial port which has become a sort of archetype in the context of the\norganization of mega-events with a tourist vocation, such as the Universal or\nInternational Exhibitions and the Olympic Games, namely Barcelona and three\nvery important dates for the Catalan capital: 1888, universal exhibition, 1929\nuniversal and international exhibition, 1992 the modern Olympic Games and 2004\nthe Universal Forum of Cultures of Unesco, international exhibition. The\neconomic interest of organizing an ephemeral giant event is still relevant and\nwill be confirmed in the future with the role of Asian countries within the IOC\nand the BIE (Bureau international des expositions). The important thing is to\ndevelop a rational argument to communicate on the economic merits of the event.\nSustainable development and Corporate Social Responsibility (CSR) are\npredominant elements for ephemeral events that are intended to be sustainable\nover time for future generations.\n"
    },
    {
        "paper_id": 2212.13966,
        "authors": "Carlos Hernandez-Suarez, Efren Murillo-Zamora",
        "title": "What is expected for China's SARS-CoV-2 epidemic?",
        "comments": "6 pages pdf document, 2 of these are supplementary material",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  Recently, China announced that its \"zero-covid\" policy would end, which will\nbring serious challenges to the country's health system. In here we provide\nsimple calculations that allows us to provide an estimate of what is expected\nas an outcome in terms of fatalities, using the fact that it is a highly\ncontagious disease that will expose most of a highly vaccinated population to\nthe virus. We use recent findings regarding the amount of reduction in the risk\nof severe outcome achieved by vaccination and arrive to an estimate of 1.1 m\ndeaths, 60% of these are males. In our model, 84% percent of deaths occur in\nindividuals with age 55 years or older. In a scenario in which this protection\nis completely lost due to waning and the infection fatality rate of the\nprevalent strain reaches similar levels to the observed in the beginning of the\nepidemic, the death toll could reach 2.4 m, 93% in 55 years or older.\n"
    },
    {
        "paper_id": 2212.13996,
        "authors": "Wolfgang Karl H\\\"ardle and Yegor Klochkov and Alla Petukhina and\n  Nikita Zhivotovskiy",
        "title": "Robustifying Markowitz",
        "comments": "45 pages; to appear in Journal of Econometrics",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Markowitz mean-variance portfolios with sample mean and covariance as input\nparameters feature numerous issues in practice. They perform poorly out of\nsample due to estimation error, they experience extreme weights together with\nhigh sensitivity to change in input parameters. The heavy-tail characteristics\nof financial time series are in fact the cause for these erratic fluctuations\nof weights that consequently create substantial transaction costs. In\nrobustifying the weights we present a toolbox for stabilizing costs and weights\nfor global minimum Markowitz portfolios. Utilizing a projected gradient descent\n(PGD) technique, we avoid the estimation and inversion of the covariance\noperator as a whole and concentrate on robust estimation of the gradient\ndescent increment. Using modern tools of robust statistics we construct a\ncomputationally efficient estimator with almost Gaussian properties based on\nmedian-of-means uniformly over weights. This robustified Markowitz approach is\nconfirmed by empirical studies on equity markets. We demonstrate that\nrobustified portfolios reach the lowest turnover compared to shrinkage-based\nand constrained portfolios while preserving or slightly improving out-of-sample\nperformance.\n"
    },
    {
        "paper_id": 2212.14076,
        "authors": "Raj G. Patel, Chia-Wei Hsing, Serkan Sahin, Samuel Palmer, Saeed S.\n  Jahromi, Shivam Sharma, Tomas Dominguez, Kris Tziritas, Christophe Michel,\n  Vincent Porte, Mustafa Abid, Stephane Aubert, Pierre Castellani, Samuel\n  Mugel, Roman Orus",
        "title": "Quantum-Inspired Tensor Neural Networks for Option Pricing",
        "comments": "11 pages, 8 figures, minor changes. arXiv admin note: substantial\n  text overlap with arXiv:2208.02235",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Recent advances in deep learning have enabled us to address the curse of\ndimensionality (COD) by solving problems in higher dimensions. A subset of such\napproaches of addressing the COD has led us to solving high-dimensional PDEs.\nThis has resulted in opening doors to solving a variety of real-world problems\nranging from mathematical finance to stochastic control for industrial\napplications. Although feasible, these deep learning methods are still\nconstrained by training time and memory. Tackling these shortcomings, Tensor\nNeural Networks (TNN) demonstrate that they can provide significant parameter\nsavings while attaining the same accuracy as compared to the classical Dense\nNeural Network (DNN). In addition, we also show how TNN can be trained faster\nthan DNN for the same accuracy. Besides TNN, we also introduce Tensor Network\nInitializer (TNN Init), a weight initialization scheme that leads to faster\nconvergence with smaller variance for an equivalent parameter count as compared\nto a DNN. We benchmark TNN and TNN Init by applying them to solve the parabolic\nPDE associated with the Heston model, which is widely used in financial pricing\ntheory.\n"
    },
    {
        "paper_id": 2212.14159,
        "authors": "Torsten Heinrich and Jangho Yang",
        "title": "Innovation in times of Covid-19",
        "comments": "23 pages, 7 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Did the Covid-19 pandemic have an impact on innovation? Past economic\ndisruptions, anecdotal evidence, and the previous literature suggest a decline\nwith substantial differences between industries. We leverage USPTO patent\napplication data to investigate and quantify the disturbance. We assess\ndifferences by field of technology (at the CPC subclass level) as well as the\nimpact of direct and indirect relevance for the management of the pandemic.\nDirect Covid-19 relevance is identified from a keyword search of the patent\napplication fulltexts; indirect Covid-19 relevance is derived from past CPC\nsubclass to subclass citation patterns. We find that direct Covid-19 relevance\nis associated with a strong boost to the growth of the number of patent\napplications in the first year of the pandemic at the same order of magnitude\n(in percentage points) as the percentage of patents referencing Covid-19. We\nfind no effect for indirect Covid-19 relevance, indicating a focus on applied\nresearch at the expense of more basic research. Fields of technology (CPC\nmainsections) have an additional significant impact, with, e.g., mainsections A\n(human necessities) and C (chemistry, metallurgy) having a strong performance.\n"
    },
    {
        "paper_id": 2212.14174,
        "authors": "Erhan Bayraktar, Shuoqing Deng, Dominykas Norgilas",
        "title": "Supermartingale Brenier's Theorem with full-marginals constraint",
        "comments": "Supermartingale optimal transport, Brenier's Theorem",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  We explicitly construct the supermartingale version of the\nFr{\\'e}chet-Hoeffding coupling in the setting with infinitely many marginal\nconstraints. This extends the results of Henry-Labordere et al. obtained in the\nmartingale setting. Our construction is based on the Markovian iteration of\none-period optimal supermartingale couplings. In the limit, as the number of\niterations goes to infinity, we obtain a pure jump process that belongs to a\nfamily of local L{\\'e}vy models introduced by Carr et al. We show that the\nconstructed processes solve the continuous-time supermartingale optimal\ntransport problem for a particular family of path-dependent cost functions. The\nexplicit computations are provided in the following three cases: the uniform\ncase, the Bachelier model and the Geometric Brownian Motion case.\n"
    },
    {
        "paper_id": 2212.14188,
        "authors": "Ying Hu, Xiaomin Shi, Zuo Quan Xu",
        "title": "Constrained monotone mean-variance problem with random coefficients",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  This paper studies the monotone mean-variance (MMV) problem and the classical\nmean-variance (MV) problem with convex cone trading constraints in a market\nwith random coefficients. We provide semiclosed optimal strategies and optimal\nvalues for both problems via certain backward stochastic differential equations\n(BSDEs). After noting the links between these BSDEs, we find that the two\nproblems share the same optimal portfolio and optimal value. This generalizes\nthe result of Shen and Zou $[$ SIAM J. Financial Math., 13 (2022), pp.\nSC99-SC112$]$ from deterministic coefficients to random ones.\n"
    },
    {
        "paper_id": 2212.14259,
        "authors": "Johannes Langner and Gregor Svindland",
        "title": "Bipolar Theorems for Sets of Non-negative Random Variables",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper assumes a robust, in general not dominated, probabilistic\nframework and provides necessary and sufficient conditions for a bipolar\nrepresentation of subsets of the set of all quasi-sure equivalence classes of\nnon-negative random variables without any further conditions on the underlying\nmeasure space. This generalises and unifies existing bipolar theorems proved\nunder stronger assumptions on the robust framework. Applications are in areas\nof robust financial modeling which we discuss throughout the paper.\n"
    },
    {
        "paper_id": 2212.14327,
        "authors": "Guohui Guan, Zongxia Liang, Yilun Song",
        "title": "A Stackelberg reinsurance-investment game under $\\alpha$-maxmin\n  mean-variance criterion and stochastic volatility",
        "comments": "38 pages, 20 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper investigates a Stackelberg game between an insurer and a reinsurer\nunder the $\\alpha$-maxmin mean-variance criterion. The insurer can purchase\nper-loss reinsurance from the reinsurer. With the insurer's feedback\nreinsurance strategy, the reinsurer optimizes the reinsurance premium in the\nStackelberg game. The financial market consists of cash and stock with Heston's\nstochastic volatility. Both the insurer and reinsurer maximize their respective\n$\\alpha$-maxmin mean-variance preferences in the market. The criterion is\ntime-inconsistent and we derive the equilibrium strategies by the extended\nHamilton-Jacobi-Bellman equations. Similar to the non-robust case in Li and\nYoung (2022), excess-of-loss reinsurance is the optimal form of reinsurance\nstrategy for the insurer. The equilibrium investment strategy is determined by\na system of Riccati differential equations. Besides, the equations determining\nthe equilibrium reinsurance strategy and reinsurance premium rate are given\nsemi-explicitly, which is simplified to an algebraic equation in a specific\nexample. Numerical examples illustrate that the game between the insurer and\nreinsurer makes the insurance more radical when the agents become more\nambiguity aversion or risk aversion. Furthermore, the level of ambiguity,\nambiguity attitude, and risk attitude of the insurer (reinsurer) have similar\neffects on the equilibrium reinsurance strategy, reinsurance premium, and\ninvestment strategy.\n"
    },
    {
        "paper_id": 2212.14372,
        "authors": "Jean-Fran\\c{c}ois Chassagneux and Junchao Chen and Noufel Frikha",
        "title": "Deep Runge-Kutta schemes for BSDEs",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We propose a new probabilistic scheme which combines deep learning techniques\nwith high order schemes for backward stochastic differential equations\nbelonging to the class of Runge-Kutta methods to solve high-dimensional\nsemi-linear parabolic partial differential equations. Our approach notably\nextends the one introduced in [Hure Pham Warin 2020] for the implicit Euler\nscheme to schemes which are more efficient in terms of discrete-time error. We\nestablish some convergence results for our implemented schemes under classical\nregularity assumptions. We also illustrate the efficiency of our method for\ndifferent schemes of order one, two and three. Our numerical results indicate\nthat the Crank-Nicolson schemes is a good compromise in terms of precision,\ncomputational cost and numerical implementation.\n"
    },
    {
        "paper_id": 2212.14477,
        "authors": "MohammadAmin Fazli, Mahdi Lashkari, Hamed Taherkhani, Jafar Habibi",
        "title": "A Novel Experts Advice Aggregation Framework Using Deep Reinforcement\n  Learning for Portfolio Management",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  Solving portfolio management problems using deep reinforcement learning has\nbeen getting much attention in finance for a few years. We have proposed a new\nmethod using experts signals and historical price data to feed into our\nreinforcement learning framework. Although experts signals have been used in\nprevious works in the field of finance, as far as we know, it is the first time\nthis method, in tandem with deep RL, is used to solve the financial portfolio\nmanagement problem. Our proposed framework consists of a convolutional network\nfor aggregating signals, another convolutional network for historical price\ndata, and a vanilla network. We used the Proximal Policy Optimization algorithm\nas the agent to process the reward and take action in the environment. The\nresults suggested that, on average, our framework could gain 90 percent of the\nprofit earned by the best expert.\n"
    },
    {
        "paper_id": 2212.1467,
        "authors": "Xiaodong Li, Pangjing Wu, Chenxin Zou, Qing Li",
        "title": "Hierarchical Deep Reinforcement Learning for VWAP Strategy Optimization",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  Designing an intelligent volume-weighted average price (VWAP) strategy is a\ncritical concern for brokers, since traditional rule-based strategies are\nrelatively static that cannot achieve a lower transaction cost in a dynamic\nmarket. Many studies have tried to minimize the cost via reinforcement\nlearning, but there are bottlenecks in improvement, especially for\nlong-duration strategies such as the VWAP strategy. To address this issue, we\npropose a deep learning and hierarchical reinforcement learning jointed\narchitecture termed Macro-Meta-Micro Trader (M3T) to capture market patterns\nand execute orders from different temporal scales. The Macro Trader first\nallocates a parent order into tranches based on volume profiles as the\ntraditional VWAP strategy does, but a long short-term memory neural network is\nused to improve the forecasting accuracy. Then the Meta Trader selects a\nshort-term subgoal appropriate to instant liquidity within each tranche to form\na mini-tranche. The Micro Trader consequently extracts the instant market state\nand fulfils the subgoal with the lowest transaction cost. Our experiments over\nstocks listed on the Shanghai stock exchange demonstrate that our approach\noutperforms baselines in terms of VWAP slippage, with an average cost saving of\n1.16 base points compared to the optimal baseline.\n"
    },
    {
        "paper_id": 2212.14687,
        "authors": "Hamid Nasiri, Mohammad Mehdi Ebadzadeh",
        "title": "Multi-step-ahead Stock Price Prediction Using Recurrent Fuzzy Neural\n  Network and Variational Mode Decomposition",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
        "abstract": "  Financial time series prediction, a growing research topic, has attracted\nconsiderable interest from scholars, and several approaches have been\ndeveloped. Among them, decomposition-based methods have achieved promising\nresults. Most decomposition-based methods approximate a single function, which\nis insufficient for obtaining accurate results. Moreover, most existing\nresearches have concentrated on one-step-ahead forecasting that prevents stock\nmarket investors from arriving at the best decisions for the future. This study\nproposes two novel methods for multi-step-ahead stock price prediction based on\nthe issues outlined. DCT-MFRFNN, a method based on discrete cosine transform\n(DCT) and multi-functional recurrent fuzzy neural network (MFRFNN), uses DCT to\nreduce fluctuations in the time series and simplify its structure and MFRFNN to\npredict the stock price. VMD-MFRFNN, an approach based on variational mode\ndecomposition (VMD) and MFRFNN, brings together their advantages. VMD-MFRFNN\nconsists of two phases. The input signal is decomposed to several IMFs using\nVMD in the decomposition phase. In the prediction and reconstruction phase,\neach of the IMFs is given to a separate MFRFNN for prediction, and predicted\nsignals are summed to reconstruct the output. Three financial time series,\nincluding Hang Seng Index (HSI), Shanghai Stock Exchange (SSE), and Standard &\nPoor's 500 Index (SPX), are used for the evaluation of the proposed methods.\nExperimental results indicate that VMD-MFRFNN surpasses other state-of-the-art\nmethods. VMD-MFRFNN, on average, shows 35.93%, 24.88%, and 34.59% decreases in\nRMSE from the second-best model for HSI, SSE, and SPX, respectively. Also,\nDCT-MFRFNN outperforms MFRFNN in all experiments.\n"
    },
    {
        "paper_id": 2212.14689,
        "authors": "Aditya Pandey, Haseeba Fathiya, Nivedita Patel",
        "title": "Cross-Domain Shopping and Stock Trend Analysis",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  This paper presents a cross-domain trend analysis that aims to identify and\nanalyze the relationships between stock prices, stock news on Twitter, and\nusers' behaviors on e-commerce websites. The analysis is based on three\ndatasets: a US stock dataset, a stock tweets dataset, and an e-commerce\nbehavior dataset. The analysis is performed using Hadoop, Hive, and Tableau,\nallowing for efficient and scalable processing and visualizing large datasets.\n  The analysis includes trend analysis of Twitter sentiment (positive and\nnegative tweets) and correlation analysis, including the correlation between\ntweet sentiment and stocks, the correlation between stock trends and shopping\nbehavior, and the understanding of data based on different slices of time. By\ncomparing different features from the datasets over time, we hope to gain\ninsight into the factors that drive user behavior as well as the market in\ndifferent categories. The results of this analysis can provide valuable\ninsights for businesses and investors to inform decision-making.\n  We believe that our analysis can serve as a valuable starting point for\nfurther research and investigation into these topics.\n"
    },
    {
        "paper_id": 2301.0017,
        "authors": "Tolga Buz, Gerard de Melo",
        "title": "Democratization of Retail Trading: Can Reddit's WallStreetBets\n  Outperform Investment Bank Analysts?",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The recent hype around Reddit's WallStreetBets (WSB) community has inspired\nresearch on its impact on our economy and society. Still, one important\nquestion remains: Can WSB's community of anonymous contributors actually\nprovide valuable investment advice and possibly even outperform top financial\ninstitutions? We present a data-driven empirical study of investment\nrecommendations of WSB in comparison to recommendations made by leading\ninvestment banks, based on more than 1.6 million WSB posts published since\n2018. %enriched with stock market data. To this end, we extract and evaluate\ninvestment recommendations from WSB's raw text for all S&P 500 stocks and\ncompare their performance to more than 16,000 analyst recommendations from the\nlargest investment banks. While not all WSB recommendations prove profitable,\nour results show that they achieve average returns that compete with the best\nbanks and outperform them in certain cases. Furthermore, the WSB community has\nbeen better than almost all investment banks at detecting top-performing\nstocks. We conclude that WSB may indeed constitute a freely accessible,\nvaluable source of investment advice.\n"
    },
    {
        "paper_id": 2301.00248,
        "authors": "Thomas Dierckx, Jesse Davis, Wim Schoutens",
        "title": "Nowcasting Stock Implied Volatility with Twitter",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  In this study, we predict next-day movements of stock end-of-day implied\nvolatility using random forests. Through an ablation study, we examine the\nusefulness of different sources of predictors and expose the value of attention\nand sentiment features extracted from Twitter. We study the approach on a stock\nuniverse comprised of the 165 most liquid US stocks diversified across the 11\ntraditional market sectors using a sizeable out-of-sample period spanning over\nsix years. In doing so, we uncover that stocks in certain sectors, such as\nConsumer Discretionary, Technology, Real Estate, and Utilities are easier to\npredict than others. Further analysis shows that possible reasons for these\ndiscrepancies might be caused by either excess social media attention or low\noption liquidity. Lastly, we explore how our proposed approach fares throughout\ntime by identifying four underlying market regimes in implied volatility using\nhidden Markov models. We find that most added value is achieved in regimes\nassociated with lower implied volatility, but optimal regimes vary per market\nsector.\n"
    },
    {
        "paper_id": 2301.00372,
        "authors": "Keh-Kuan Sun and Stella Papadokonstantaki",
        "title": "Lying Aversion and Vague Communication: An Experimental Study",
        "comments": null,
        "journal-ref": "European Economic Review 160 (November 2023): 104611",
        "doi": "10.1016/j.euroecorev.2023.104611",
        "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
        "abstract": "  An agent may strategically employ a vague message to mislead an audience's\nbelief about the state of the world, but this may cause the agent to feel guilt\nor negatively impact how the audience perceives the agent. Using a novel\nexperimental design that allows participants to be vague while at the same time\nisolating the internal cost of lying from the social identity cost of appearing\ndishonest, we explore the extent to which these two types of lying costs affect\ncommunication. We find that participants exploit vagueness to be consistent\nwith the truth, while at the same time leveraging the imprecision to their own\nbenefit. More participants use vague messages in treatments where concern with\nsocial identity is relevant. In addition, we find that social identity concerns\nsubstantially affect the length and patterns of vague messages used across the\ntreatments.\n"
    },
    {
        "paper_id": 2301.0041,
        "authors": "Stephan Leitner",
        "title": "Designing organizations for bottom-up task allocation: The role of\n  incentives",
        "comments": "38 pages, 6 figures, 2 tables",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In recent years, various decentralized organizational forms have emerged,\nposing a challenge for organizational design. Some design elements, such as\ntask allocation, become emergent properties that cannot be fully controlled\nfrom the top down. The central question that arises in this context is: How can\nbottom-up task allocation be guided towards an effective organizational\nstructure? To address this question, this paper presents a novel agent-based\nmodel of an organization that features bottom-up task allocation that can be\nmotivated by either long-term or short-term orientation on the agents' side.\nThe model also includes an incentive mechanism to guide the bottom-up task\nallocation process and create incentives that range from altruistic to\nindividualistic. Our analysis shows that when bottom-up task allocation is\ndriven by short-term orientation and aligned with the incentive mechanisms, it\nleads to improved organizational performance that surpasses that of\ntraditionally designed organizations. Additionally, we find that the presence\nof altruistic incentive mechanisms within the organization reduces the\nimportance of mirroring in task allocation.\n"
    },
    {
        "paper_id": 2301.0044,
        "authors": "Geoff Boeing, Yougeng Lu, Clemens Pilgram",
        "title": "Local Inequities in the Relative Production of and Exposure to Vehicular\n  Air Pollution in Los Angeles",
        "comments": "Pre-print",
        "journal-ref": "Urban Studies, 2023",
        "doi": "10.1177/00420980221145403",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Vehicular air pollution has created an ongoing air quality and public health\ncrisis. Despite growing knowledge of racial injustice in exposure levels, less\nis known about the relationship between the production of and exposure to such\npollution. This study assesses pollution burden by testing whether local\npopulations' vehicular air pollution exposure is proportional to how much they\ndrive. Through a Los Angeles, California case study we examine how this relates\nto race, ethnicity, and socioeconomic status -- and how these relationships\nvary across the region. We find that, all else equal, tracts whose residents\ndrive less are exposed to more air pollution, as are tracts with a less-White\npopulation. Commuters from majority-White tracts disproportionately drive\nthrough non-White tracts, compared to the inverse. Decades of\nracially-motivated freeway infrastructure planning and residential segregation\nshape today's disparities in who produces vehicular air pollution and who is\nexposed to it, but opportunities exist for urban planning and transport policy\nto mitigate this injustice.\n"
    },
    {
        "paper_id": 2301.00648,
        "authors": "A. Aimi, C. Guardasoni, L. Ortiz-Gracia, S. Sanfelici",
        "title": "Fast Barrier Option Pricing by the COS BEM Method in Heston Model",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this work, the Fourier-cosine series (COS) method has been combined with\nthe Boundary Element Method (BEM) for a fast evaluation of barrier option\nprices. After a description of its use in the Black and Scholes (BS) model, the\nfocus of the paper is on the application of the proposed methodology to the\nbarrier option evaluation in the Heston model, where its contribution is\nfundamental to improve computational efficiency and to make BEM appealing among\nFinance practitioners as a valid alternative to Monte Carlo (MC) or other more\ntraditional approaches. An error analysis is provided on the number of terms\nused in the Fourier-cosine series expansion, where the error bound estimation\nis based on the characteristic function of the log-asset price process.\n"
    },
    {
        "paper_id": 2301.00666,
        "authors": "Yuki Oyama, Daisuke Fukuda, Naoto Imura, Katsuhiro Nishinari",
        "title": "E-commerce users' preferences for delivery options",
        "comments": "Section 1 needs to be rewritten",
        "journal-ref": "Journal of Retailing and Consumer Services 78 (2024) 103711",
        "doi": "10.1016/j.jretconser.2024.103711",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Many e-commerce marketplaces offer their users fast delivery options for free\nto meet the increasing needs of users, imposing an excessive burden on city\nlogistics. Therefore, understanding e-commerce users' preference for delivery\noptions is a key to designing logistics policies. To this end, this study\ndesigns a stated choice survey in which respondents are faced with choice tasks\namong different delivery options and time slots, which was completed by 4,062\nusers from the three major metropolitan areas in Japan. To analyze the data,\nmixed logit models capturing taste heterogeneity as well as flexible\nsubstitution patterns have been estimated. The model estimation results\nindicate that delivery attributes including fee, time, and time slot size are\nsignificant determinants of the delivery option choices. Associations between\nusers' preferences and socio-demographic characteristics, such as age, gender,\nteleworking frequency and the presence of a delivery box, were also suggested.\nMoreover, we analyzed two willingness-to-pay measures for delivery, namely, the\nvalue of delivery time savings (VODT) and the value of time slot shortening\n(VOTS), and applied a non-semiparametric approach to estimate their\ndistributions in a data-oriented manner. Although VODT has a large\nheterogeneity among respondents, the estimated median VODT is 25.6 JPY/day,\nimplying that more than half of the respondents would wait an additional day if\nthe delivery fee were increased by only 26 JPY, that is, they do not\nnecessarily need a fast delivery option but often request it when cheap or\nalmost free. Moreover, VOTS was found to be low, distributed with the median of\n5.0 JPY/hour; that is, users do not highly value the reduction in time slot\nsize in monetary terms. These findings on e-commerce users' preferences can\nhelp in designing levels of service for last-mile delivery to significantly\nimprove its efficiency.\n"
    },
    {
        "paper_id": 2301.0068,
        "authors": "Razan Abdelazim Idris Alzain",
        "title": "Large-Scale 3D Printing -- Market Analysis",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  The aim of this research is to get a better understanding of the future of\nlarge-scale 3D printing. By developing the market analysis, it will be clear\nwhether large-scale 3D printing is becoming more of a preferred way of printing\ncustom-made parts for production companies. Companies can then choose whether\nto change their ways, for a more profitable less costly method, or stay on the\nroute they are on. By getting deep into this topic, a new world of technology\nis then being discovered and familiarized. With a mix of theoretical and\npractical relevance, a complete coverage could be made on large-scale 3D\nprinting. This paper could then cover all aspects of this topic, and the reader\ncould then make their own judgment if large-scale 3D printing would be the best\noption.\n"
    },
    {
        "paper_id": 2301.00681,
        "authors": "Kevin Sun",
        "title": "Historical Patterns and Recent Impacts of Chinese Investors in United\n  States Real Estate",
        "comments": "23 pages, 4 figures, presented at the 2022 Southern California\n  Conference for Undergraduate Research (SCCUR), preprint for publishing in\n  research journal, currently in the peer-review process",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Since supplanting Canada in 2014, Chinese investors have been the lead\nforeign buyers of U.S. real estate, concentrating their purchases in urban\nareas with higher Chinese populations like California. The reasons for\ninvestment include prestige, freedom from capital confiscation, and safe,\ndiversified opportunities from abroad simply being more lucrative and available\nthan in their home country, where the market is eroding. Interestingly, since\n2019, Chinese investors have sold a net 23.6 billion dollars of U.S. commercial\nreal estate, a stark contrast to past acquisitions between 2013 to 2018 where\nthey were net buyers of almost 52 billion dollars worth of properties. A\nsimilar trend appears in the residential real estate segment too. In both 2017\nand 2018, Chinese buyers purchased over 40,000 U.S. residential properties\nwhich were halved in 2019 and steadily declined to only 6,700 in the past year.\nThis turnaround in Chinese investment can be attributed to a deteriorating\nrelationship between the U.S. and China during the Trump Presidency, financial\ndistress in China, and new Chinese government regulations prohibiting outbound\ninvestments. Additionally, while Chinese investment is a small share of U.S.\nreal estate (~1.5% at its peak), it has outsized impacts on market valuations\nof home prices in U.S. zip codes with higher populations of foreign-born\nChinese, increasing property prices and exacerbating the issue of housing\naffordability in these areas. This paper investigates the rapid growth and\ndecline of Chinese investment in U.S. real estate and its effect on U.S. home\nprices in certain demographics.\n"
    },
    {
        "paper_id": 2301.0079,
        "authors": "Thomas Wong and Mauricio Barahona",
        "title": "Online learning techniques for prediction of temporal tabular datasets\n  with regime changes",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  The application of deep learning to non-stationary temporal datasets can lead\nto overfitted models that underperform under regime changes. In this work, we\npropose a modular machine learning pipeline for ranking predictions on temporal\npanel datasets which is robust under regime changes. The modularity of the\npipeline allows the use of different models, including Gradient Boosting\nDecision Trees (GBDTs) and Neural Networks, with and without feature\nengineering. We evaluate our framework on financial data for stock portfolio\nprediction, and find that GBDT models with dropout display high performance,\nrobustness and generalisability with reduced complexity and computational cost.\nWe then demonstrate how online learning techniques, which require no retraining\nof models, can be used post-prediction to enhance the results. First, we show\nthat dynamic feature projection improves robustness by reducing drawdown in\nregime changes. Second, we demonstrate that dynamical model ensembling based on\nselection of models with good recent performance leads to improved Sharpe and\nCalmar ratios of out-of-sample predictions. We also evaluate the robustness of\nour pipeline across different data splits and random seeds with good\nreproducibility.\n"
    },
    {
        "paper_id": 2301.01127,
        "authors": "Vartuhi Tonoyan and Christopher Boudreaux",
        "title": "Gender Diversity in Ownership and Firm Innovativeness in Emerging\n  Markets. The Mediating Roles of R&D Investments and External Capital",
        "comments": "35 pages, 5 tables",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Despite recent evidence linking gender diversity in the firm with firm\ninnovativeness, we know little about the underlying mechanisms. Building on and\nextending the Upper Echelon and entrepreneurship literature, we address two\nlingering questions: why and how does gender diversity in firm ownership affect\nfirm innovativeness? We use survey data collected from 7,848 owner-managers of\nSMEs across 29 emerging markets to test our hypotheses. Our findings\ndemonstrate that firms with higher gender diversity in ownership are more\nlikely to invest in R&D and rely upon a breadth of external capital, with such\ndifferentials explaining sizeable proportions of the higher likelihood of\noverall firm innovativeness, product and process, as well as organizational and\nmarketing innovations exhibited by their firms. Our findings are robust to\ncorrections for alternative measurement of focal variables, sensitivity to\noutliers and subsamples, and endogenous self-selection concerns.\n"
    },
    {
        "paper_id": 2301.01212,
        "authors": "Ricardo Mu\\~noz-Cancino and Cristi\\'an Bravo and Sebasti\\'an A. R\\'ios\n  and Manuel Gra\\~na",
        "title": "Assessment of creditworthiness models privacy-preserving training with\n  synthetic data",
        "comments": null,
        "journal-ref": "Hybrid Artificial Intelligent Systems. HAIS 2022. Lecture Notes in\n  Computer Science(), vol 13469",
        "doi": "10.1007/978-3-031-15471-3_32",
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  Credit scoring models are the primary instrument used by financial\ninstitutions to manage credit risk. The scarcity of research on behavioral\nscoring is due to the difficult data access. Financial institutions have to\nmaintain the privacy and security of borrowers' information refrain them from\ncollaborating in research initiatives. In this work, we present a methodology\nthat allows us to evaluate the performance of models trained with synthetic\ndata when they are applied to real-world data. Our results show that synthetic\ndata quality is increasingly poor when the number of attributes increases.\nHowever, creditworthiness assessment models trained with synthetic data show a\nreduction of 3\\% of AUC and 6\\% of KS when compared with models trained with\nreal data. These results have a significant impact since they encourage credit\nrisk investigation from synthetic data, making it possible to maintain\nborrowers' privacy and to address problems that until now have been hampered by\nthe availability of information.\n"
    },
    {
        "paper_id": 2301.0126,
        "authors": "Colin Turfus and Aurelio Romero-Berm\\'udez",
        "title": "Analytic RFR Option Pricing with Smile and Skew",
        "comments": "7 figures and all technical details included",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We extend the short rate model of Turfus and Romero-Berm\\'udez [2021] to\nfacilitate accurate arbitrage-free analytic pricing of SOFR, SONIA or ESTR\ncaplets, i.e. options on backward-looking compounded rates payments, in a\nmanner consistent with the smile and skew levels observed in the market. These\ncaplet pricing formulae and corresponding LIBOR or term-rate caplet results are\ntranslated into effective variance (implied volatility) formulae, which are\nseen to be of a particularly simple form. They show that the model is\nessentially equivalent to imposing on a Hull-White model an effective variance\nwhich is a quadratic function of the moneyness parameter (rather than a\nconstant) for any given maturity. Results are also illustrated graphically.\n"
    },
    {
        "paper_id": 2301.01271,
        "authors": "Gianmarco Caldini",
        "title": "On the notion of measurable utility on a connected and separable\n  topological space: an order isomorphism theorem",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The aim of this article is to define a notion of cardinal utility function\ncalled measurable utility and to define it on a connected and separable subset\nof a weakly ordered topological space. The definition is equivalent to the ones\ngiven by Frisch in 1926 and by Shapley in 1975 and postulates axioms on a set\nof alternatives that allow both to ordinally rank alternatives and to compare\ntheir utility differences. After a brief review of the philosophy of\nutilitarianism and the history of utility theory, the paper introduces the\nmathematical framework to represent intensity comparisons of utility and proves\na list of topological lemmas that will be used in the main result. Finally, the\narticle states and proves a representation theorem for a measurable utility\nfunction defined on a connected and separable subset of a weakly ordered\ntopological space equipped with another weak order on its cartesian product.\nUnder some assumptions on the order relations, the main theorem proves\nexistence and uniqueness, up to positive affine transformations, of an order\nisomorphism with the real line.\n"
    },
    {
        "paper_id": 2301.01277,
        "authors": "Szabolcs Nagy, Noemi Hajdu",
        "title": "Consumer acceptance of the use of artificial intelligence in online\n  shopping: evidence from Hungary",
        "comments": null,
        "journal-ref": "Amfiteatru Economic, 2021, 23(56), pp.155-173",
        "doi": "10.24818/EA/2021/56/155",
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  The rapid development of technology has drastically changed the way consumers\ndo their shopping. The volume of global online commerce has significantly been\nincreasing partly due to the recent COVID-19 crisis that has accelerated the\nexpansion of e-commerce. A growing number of webshops integrate Artificial\nIntelligence (AI), state-of-the-art technology into their stores to improve\ncustomer experience, satisfaction and loyalty. However, little research has\nbeen done to verify the process of how consumers adopt and use AI-powered\nwebshops. Using the technology acceptance model (TAM) as a theoretical\nbackground, this study addresses the question of trust and consumer acceptance\nof Artificial Intelligence in online retail. An online survey in Hungary was\nconducted to build a database of 439 respondents for this study. To analyse\ndata, structural equation modelling (SEM) was used. After the respecification\nof the initial theoretical model, a nested model, which was also based on TAM,\nwas developed and tested. The widely used TAM was found to be a suitable\ntheoretical model for investigating consumer acceptance of the use of\nArtificial Intelligence in online shopping. Trust was found to be one of the\nkey factors influencing consumer attitudes towards Artificial Intelligence.\nPerceived usefulness as the other key factor in attitudes and behavioural\nintention was found to be more important than the perceived ease of use. These\nfindings offer valuable implications for webshop owners to increase customer\nacceptance\n"
    },
    {
        "paper_id": 2301.01278,
        "authors": "Szabolcs Nagy and Mariann Veresne Somosi",
        "title": "Students Perceptions of Sustainable Universities in Hungary. An\n  Importance-Performance Analysis",
        "comments": null,
        "journal-ref": "Amfiteatru Economic, Volume 22, No. 54, 2020",
        "doi": "10.24818/EA/2020/54/496",
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  In order to succeed, universities are forced to respond to the new challenges\nin the rapidly changing world. The recently emerging fourth-generation\nuniversities should meet sustainability objectives to better serve their\nstudents and their communities. It is essential for universities to measure\ntheir sustainability performance to capitalise on their core strengths and to\novercome their weaknesses. In line with the stakeholder theory, the objective\nof this study was to investigate students perceptions of university\nsustainability including their expectations about and satisfaction with the\nefforts that universities make towards sustainability. This paper proposes a\nnew approach that combines the sustainable university scale, developed by the\nauthors, with the importance-performance analysis to identify key areas of\nuniversity sustainability. To collect data, an online survey was conducted in\nHungary in 2019. The sustainable university scale was found to be a reliable\nconstruct to measure different aspects of university sustainability. Results of\nthe importance-performance analysis suggest that students consider Hungarian\nuniversities unsustainable. Research findings indicate that Hungarian\nuniversities perform poorly in sustainable purchasing and renewable energy use,\nbut their location and their efforts towards separate waste collection are\ntheir major competitive advantages. The main domains of university\nsustainability were also discussed. This study provides university\ndecision-makers and researchers with insightful results supporting the\ntransformation of traditional universities into sustainable, fourth-generation\nhigher education institutions.\n"
    },
    {
        "paper_id": 2301.01279,
        "authors": "Szabolcs Nagy, Gergo Hajdu",
        "title": "The relationship between content marketing and the traditional marketing\n  communication tools",
        "comments": "Keywords: content marketing, trends, advertising, sales promotion,\n  direct marketing, personal selling, public relations",
        "journal-ref": "ESZAK-MAGYARORSZAGI STRATEGIAI FUZETEK 1786-1594 2560-2926 18 (1)\n  pp. 110-119 2021",
        "doi": "10.32976/stratfuz.2021.25",
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Digitalization is making a significant impact on marketing. New marketing\napproaches and tools are emerging which are not always clearly categorised.\nThis article seeks to investigate the relationship between one of the novel\nmarketing tools, content marketing, and the five elements of the traditional\nmarketing communication mix. Based on an extensive literature review, this\npaper analyses the main differences and similarities between them. This article\naims to generate a debate on the status of content marketing. According to the\nauthors' opinion, content marketing can be considered as the sixth marketing\ncommunication mix element. However, further research is needed to fill in the\nexisting knowledge gap.\n"
    },
    {
        "paper_id": 2301.01362,
        "authors": "Julien Hambuckers, Li Sun, Luca Trapin",
        "title": "Measuring tail risk at high-frequency: An $L_1$-regularized extreme\n  value regression approach with unit-root predictors",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We study tail risk dynamics in high-frequency financial markets and their\nconnection with trading activity and market uncertainty. We introduce a dynamic\nextreme value regression model accommodating both stationary and local\nunit-root predictors to appropriately capture the time-varying behaviour of the\ndistribution of high-frequency extreme losses. To characterize trading activity\nand market uncertainty, we consider several volatility and liquidity\npredictors, and propose a two-step adaptive $L_1$-regularized maximum\nlikelihood estimator to select the most appropriate ones. We establish the\noracle property of the proposed estimator for selecting both stationary and\nlocal unit-root predictors, and show its good finite sample properties in an\nextensive simulation study. Studying the high-frequency extreme losses of nine\nlarge liquid U.S. stocks using 42 liquidity and volatility predictors, we find\nthe severity of extreme losses to be well predicted by low levels of price\nimpact in period of high volatility of liquidity and volatility.\n"
    },
    {
        "paper_id": 2301.01555,
        "authors": "Leonid Dolinskyi and Yan Dolinsky",
        "title": "Optimal Liquidation with High Risk Aversion and Small Linear Price\n  Impact",
        "comments": "arXiv admin note: text overlap with arXiv:2111.00451",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  We consider the Bachelier model with linear price impact. Exponential utility\nindifference prices are studied for vanilla European options in the case where\nthe investor is required to liquidate her position. Our main result is\nestablishing a non-trivial scaling limit for a vanishing price impact which is\ninversely proportional to the risk aversion. We compute the limit of the\ncorresponding utility indifference prices and find explicitly a family of\nportfolios which are asymptotically optimal.\n"
    },
    {
        "paper_id": 2301.01807,
        "authors": "Isaac Tamblyn, Tengkai Yu, Ian Benlolo",
        "title": "fintech-kMC: Agent based simulations of financial platforms for design\n  and testing of machine learning systems",
        "comments": "To appear at AAAI-23 Bridge Program: AI for Financial Services,\n  Washington D.C., February 7 - 8, 2023",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We discuss our simulation tool, fintech-kMC, which is designed to generate\nsynthetic data for machine learning model development and testing. fintech-kMC\nis an agent-based model driven by a kinetic Monte Carlo (a.k.a. continuous time\nMonte Carlo) engine which simulates the behaviour of customers using an online\ndigital financial platform. The tool provides an interpretable, reproducible,\nand realistic way of generating synthetic data which can be used to validate\nand test AI/ML models and pipelines to be used in real-world customer-facing\nfinancial applications.\n"
    },
    {
        "paper_id": 2301.01836,
        "authors": "Anna G. Hughes, Jack S. Baker, Santosh Kumar Radha",
        "title": "A Quantum-Inspired Binary Optimization Algorithm for Representative\n  Selection",
        "comments": "11 pages, 9 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Advancements in quantum computing are fuelling emerging applications across\ndisciplines, including finance, where quantum and quantum-inspired algorithms\ncan now make market predictions, detect fraud, and optimize portfolios.\nExpanding this toolbox, we propose the selector algorithm: a method for\nselecting the most representative subset of data from a larger dataset. The\nselected subset includes data points that simultaneously meet the two\nrequirements of being maximally close to neighboring data points and maximally\nfar from more distant data points where the precise notion of distance is given\nby any kernel or generalized similarity function. The cost function encoding\nthe above requirements naturally presents itself as a Quadratic Unconstrained\nBinary Optimization (QUBO) problem, which is well-suited for quantum\noptimization algorithms - including quantum annealing. While the selector\nalgorithm has applications in multiple areas, it is particularly useful in\nfinance, where it can be used to build a diversified portfolio from a more\nextensive selection of assets. After experimenting with synthetic datasets, we\nshow two use cases for the selector algorithm with real data: (1) approximately\nreconstructing the NASDAQ 100 index using a subset of stocks, and (2)\ndiversifying a portfolio of cryptocurrencies. In our analysis of use case (2),\nwe compare the performance of two quantum annealers provided by D-Wave Systems.\n"
    },
    {
        "paper_id": 2301.01843,
        "authors": "Miguel Fajardo-Steinh\\\"auser",
        "title": "Peace Dividends: The Economic Effects of Colombia's Peace Agreement",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  The last decades have seen a resurgence of armed conflict around the world,\nrenewing the need for durable peace agreements. In this paper, I evaluate the\neconomic effects of the peace agreement between the Colombian government and\nthe largest guerrilla group in the country, the FARC, putting an end to one of\nthe longest and most violent armed conflicts in recent history. Using a\ndifference-in-difference strategy comparing municipalities that historically\nhad FARC presence and those with presence of a similar, smaller guerrilla\ngroup, the ELN, before and after the start of a unilateral ceasefire by the\nFARC, I establish three sets of results. First, violence indicators\nsignificantly and sizably decreased in historically FARC municipalities.\nSecond, despite this large reduction in violence, I find precisely-estimated\nnull effects across a variety of economic indicators, suggesting no effect of\nthe peace agreement on economic activity. Furthermore, I use a sharp\ndiscontinuity in eligibility to the government's flagship business and job\ncreation program for conflict-affected areas to evaluate the policy's impact,\nalso finding precisely-estimated null effects on the same economic indicators.\nThird, I present evidence that suggests the reason why historically FARC\nmunicipalities could not reap the economic benefits from the reduction in\nviolence is a lack of state capacity, caused both by their low initial levels\nof state capacity and the lack of state entry post-ceasefire. These results\nindicate that peace agreements require complementary investments in state\ncapacity to yield an economic dividend.\n"
    },
    {
        "paper_id": 2301.01954,
        "authors": "Margarita Leib, Nils K\\\"obis, Rainer Michael Rilke, Marloes Hagens,\n  Bernd Irlenbusch",
        "title": "Corrupted by Algorithms? How AI-generated and Human-written Advice Shape\n  (Dis)honesty",
        "comments": "* shared first-authorship This is an updated version of the pre-print\n  arXiv:2102.07536 with a new data set",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Artificial Intelligence (AI) increasingly becomes an indispensable advisor.\nNew ethical concerns arise if AI persuades people to behave dishonestly. In an\nexperiment, we study how AI advice (generated by a Natural-Language-Processing\nalgorithm) affects (dis)honesty, compare it to equivalent human advice, and\ntest whether transparency about advice source matters. We find that\ndishonesty-promoting advice increases dishonesty, whereas honesty-promoting\nadvice does not increase honesty. This is the case for both AI- and human\nadvice. Algorithmic transparency, a commonly proposed policy to mitigate AI\nrisks, does not affect behaviour. The findings mark the first steps towards\nmanaging AI advice responsibly.\n"
    },
    {
        "paper_id": 2301.02027,
        "authors": "Luca Mungo, Silvia Bartolucci, Laura Alessandretti",
        "title": "Cryptocurrency co-investment network: token returns reflect investment\n  patterns",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Since the introduction of Bitcoin in 2009, the dramatic and unsteady\nevolution of the cryptocurrency market has also been driven by large\ninvestments by traditional and cryptocurrency-focused hedge funds.\nNotwithstanding their critical role, our understanding of the relationship\nbetween institutional investments and the evolution of the cryptocurrency\nmarket has remained limited, also due to the lack of comprehensive data\ndescribing investments over time. In this study, we present a quantitative\nstudy of cryptocurrency institutional investments based on a dataset collected\nfor 1324 currencies in the period between 2014 and 2022 from Crunchbase, one of\nthe largest platforms gathering business information. We show that the\nevolution of the cryptocurrency market capitalization is highly correlated with\nthe size of institutional investments, thus confirming their important role.\nFurther, we find that the market is dominated by the presence of a group of\nprominent investors who tend to specialise by focusing on particular\ntechnologies. Finally, studying the co-investment network of currencies that\nshare common investors, we show that assets with shared investors tend to be\ncharacterized by similar market behavior. Our work sheds light on the role\nplayed by institutional investors and provides a basis for further research on\ntheir influence in the cryptocurrency ecosystem.\n"
    },
    {
        "paper_id": 2301.02575,
        "authors": "Germ\\'an Reyes",
        "title": "Cognitive Endurance, Talent Selection, and the Labor Market Returns to\n  Human Capital",
        "comments": "JEL Codes: I26, J24, J31, M54",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Cognitive endurance -- the ability to sustain performance on a\ncognitively-demanding task over time -- is thought to be a crucial productivity\ndeterminant. However, a lack of data on this variable has limited researchers'\nability to understand its role for success in college and the labor market.\nThis paper uses college-admission-exam records from 15 million Brazilian high\nschool students to measure cognitive endurance based on changes in performance\nthroughout the exam. By exploiting exogenous variation in the order of exam\nquestions, I show that students are 7.1 percentage points more likely to\ncorrectly answer a given question when it appears at the beginning of the day\nversus the end (relative to a sample mean of 34.3%). I develop a method to\ndecompose test scores into fatigue-adjusted ability and cognitive endurance. I\nthen merge these measures into a higher-education census and the earnings\nrecords of the universe of Brazilian formal-sector workers to quantify the\nassociation between endurance and long-run outcomes. I find that cognitive\nendurance has a statistically and economically significant wage return.\nControlling for fatigue-adjusted ability and other student characteristics, a\none-standard-deviation higher endurance predicts a 5.4% wage increase. This\nwage return to endurance is sizable, equivalent to a third of the wage return\nto ability. I also document positive associations between endurance and college\nattendance, college quality, college graduation, firm quality, and other\noutcomes. Finally, I show how systematic differences in endurance across\nstudents interact with the exam design to determine the sorting of students to\ncolleges. I discuss the implications of these findings for the use of cognitive\nassessments for talent selection and investments in interventions that build\ncognitive endurance.\n"
    },
    {
        "paper_id": 2301.02648,
        "authors": "Maria Dolores Gadea and Jesus Gonzalo",
        "title": "Climate change heterogeneity: A new quantitative approach",
        "comments": "36 pages, 7 figures, 10 tables, and 6 pages appendix",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Climate change is a non-uniform phenomenon. This paper proposes a new\nquantitative methodology to characterize, measure, and test the existence of\nclimate change heterogeneity. It consists of three steps. First, we introduce a\nnew testable warming typology based on the evolution of the trend of the whole\ntemperature distribution and not only on the average. Second, we define the\nconcepts of warming acceleration and warming amplification in a testable\nformat. And third, we introduce the new testable concept of warming dominance\nto determine whether region A is suffering a worse warming process than region\nB. Applying this three-step methodology, we find that Spain and the Globe\nexperience a clear distributional warming process (beyond the standard average)\nbut of different types. In both cases, this process is accelerating over time\nand asymmetrically amplified. Overall, warming in Spain dominates the Globe in\nall the quantiles except the lower tail of the global temperature distribution\nthat corresponds to the Arctic region. Our climate change heterogeneity results\nopen the door to the need for a non-uniform causal-effect climate analysis that\ngoes beyond the standard causality in mean as well as for a more efficient\ndesign of the mitigation-adaptation policies. In particular, the heterogeneity\nwe find suggests that these policies should contain a common global component\nand a clear local-regional element. Future climate agreements should take the\nwhole temperature distribution into account.\n"
    },
    {
        "paper_id": 2301.02692,
        "authors": "Mario V. W\\\"uthrich, Johanna Ziegel",
        "title": "Isotonic Recalibration under a Low Signal-to-Noise Ratio",
        "comments": "21 pages, 9 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Insurance pricing systems should fulfill the auto-calibration property to\nensure that there is no systematic cross-financing between different price\ncohorts. Often, regression models are not auto-calibrated. We propose to apply\nisotonic recalibration to a given regression model to ensure auto-calibration.\nOur main result proves that under a low signal-to-noise ratio, this isotonic\nrecalibration step leads to explainable pricing systems because the resulting\nisotonically recalibrated regression functions have a low complexity.\n"
    },
    {
        "paper_id": 2301.02754,
        "authors": "Chung-Han Hsieh and Yi-Shan Wong",
        "title": "On Frequency-Based Optimal Portfolio with Transaction Costs",
        "comments": "Submitted for possible publication",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  The aim of this paper is to investigate the impact of rebalancing frequency\nand transaction costs on the log-optimal portfolio, which is a portfolio that\nmaximizes the expected logarithmic growth rate of an investor's wealth. We\nprove that the frequency-dependent log-optimal portfolio problem with costs is\nequivalent to a concave program and provide a version of the dominance theorem\nwith costs to determine when an investor should invest all available funds in a\nparticular asset. Then, we show that transaction costs may cause a bankruptcy\nissue for the frequency-dependent log-optimal portfolio. To address this issue,\nwe approximate the problem to obtain a quadratic concave program and derive\nnecessary and sufficient optimality conditions. Additionally, we prove a\nversion of the two-fund theorem, which states that any convex combination of\ntwo optimal weights from the optimality conditions is still optimal. We test\nour proposed methods using both intraday and daily price data. Finally, we\nextend our empirical studies to an online trading scenario by implementing a\nsliding window approach. This approach enables us to solve a sequence of\nconcave programs rather than a potentially computational complex stochastic\ndynamic programming problem.\n"
    },
    {
        "paper_id": 2301.02767,
        "authors": "Muhammad Zia Hydari, Idris Adjerid, Aaron D. Striegel",
        "title": "Health Wearables, Gamification, and Healthful Activity",
        "comments": "Management Science. Published online in Articles in Advance 19 Dec\n  2022. For supplemental material, please visit\n  https://pubsonline.informs.org/doi/suppl/10.1287/mnsc.2022.4581",
        "journal-ref": null,
        "doi": "10.1287/mnsc.2022.4581",
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Health wearables in combination with gamification enable interventions that\nhave the potential to increase physical activity -- a key determinant of\nhealth. However, the extant literature does not provide conclusive evidence on\nthe benefits of gamification, and there are persistent concerns that\ncompetition-based gamification approaches will only benefit those who are\nhighly active at the expense of those who are sedentary. We investigate the\neffect of Fitbit leaderboards on the number of steps taken by the user. Using a\nunique data set of Fitbit wearable users, some of whom participate in a\nleaderboard, we find that leaderboards lead to a 370 (3.5%) step increase in\nthe users' daily physical activity. However, we find that the benefits of\nleaderboards are highly heterogeneous. Surprisingly, we find that those who\nwere highly active prior to adoption are hurt by leaderboards and walk 630\nfewer steps daily after adoption (a 5% relative decrease). In contrast, those\nwho were sedentary prior to adoption benefited substantially from leaderboards\nand walked an additional 1,300 steps daily after adoption (a 15% relative\nincrease). We find that these effects emerge because sedentary individuals\nbenefit even when leaderboards are small and when they do not rank first on\nthem. In contrast, highly active individuals are harmed by smaller leaderboards\nand only see benefit when they rank highly on large leaderboards. We posit that\nthis unexpected divergence in effects could be due to the underappreciated\npotential of noncompetition dynamics (e.g., changes in expectations for\nexercise) to benefit sedentary users, but harm more active ones.\n"
    },
    {
        "paper_id": 2301.02797,
        "authors": "Jaehyuk Choi, Byoung Ki Seo",
        "title": "Option pricing under the normal SABR model with Gaussian quadratures",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The stochastic-alpha-beta-rho (SABR) model has been widely adopted in options\ntrading. In particular, the normal ($\\beta=0$) SABR model is a popular model\nchoice for interest rates because it allows negative asset values. The option\nprice and delta under the SABR model are typically obtained via asymptotic\nimplied volatility approximation, but these are often inaccurate and\narbitrageable. Using a recently discovered price transition law, we propose a\nGaussian quadrature integration scheme for price options under the normal SABR\nmodel. The compound Gaussian quadrature sum over only 49 points can calculate a\nvery accurate price and delta that are arbitrage-free.\n"
    },
    {
        "paper_id": 2301.028,
        "authors": "Jaehyuk Choi, Yue Kuen Kwok",
        "title": "Simulation schemes for the Heston model with Poisson conditioning",
        "comments": null,
        "journal-ref": "European Journal of Operational Research, 314(1):363-376, 2024",
        "doi": "10.1016/j.ejor.2023.10.048",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Exact simulation schemes under the Heston stochastic volatility model (e.g.,\nBroadie-Kaya and Glasserman-Kim) suffer from computationally expensive modified\nBessel function evaluations. We propose a new exact simulation scheme without\nthe modified Bessel function, based on the observation that the conditional\nintegrated variance can be simplified when conditioned by the Poisson variate\nused for simulating the terminal variance. Our approach also enhances the\nlow-bias and time discretization schemes, which are suitable for pricing\nderivatives with frequent monitoring. Extensive numerical tests reveal the good\nperformance of the new simulation schemes in terms of accuracy, efficiency, and\nreliability when compared with existing methods.\n"
    },
    {
        "paper_id": 2301.02912,
        "authors": "Jarek K\\k{e}dra, Assaf Libman, Victoria Steblovskaya",
        "title": "Minimum Cost Super-Hedging in a Discrete Time Incomplete Multi-Asset\n  Binomial Market",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We consider a multi-asset incomplete model of the financial market, where\neach of $m\\geq 2$ risky assets follows the binomial dynamics, and no\nassumptions are made on the joint distribution of the risky asset price\nprocesses. We provide explicit formulas for the minimum cost super-hedging\nstrategies for a wide class of European type multi-asset contingent claims.\nThis class includes European basket call and put options, among others. Since a\nsuper-hedge is a non-self-financing arbitrage strategy, it produces\nnon-negative local residuals, for which we also give explicit formulas. This\npaper completes the foundation started in previous work of the authors for the\nextension of our results to a more realistic market model.\n"
    },
    {
        "paper_id": 2301.03063,
        "authors": "Gennady Shkliarevsky",
        "title": "Inflation and Value Creation: An Economic and Philosophic Investigation",
        "comments": "52 pages",
        "journal-ref": null,
        "doi": "10.13140/RG.2.2.30512.23046",
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  The subject of this study is inflation, a problem that has plagued America\nand the world over the last several decades. Despite a rich trove of scholarly\nstudies and a wide range of tools developed to deal with inflation, we are\nnowhere near a solution of this problem. We are now in the middle of the\ninflation that threatens to become a stagflation or even a full recession; and\nwe have no idea what to prevent this outcome. This investigation explores the\nreal source of inflation. Tracing the problem of inflation to production, it\nfinds that inflation is not a phenomenon intrinsic to economy; rather, it is a\nresult of inefficiencies and waste in our economy. The investigation leads to a\nconclusion that the solution of the problem of inflation is in achieving full\nefficiency in production. Our economic production is a result of the evolution\nthat is propelled by the process of creation. In order to end economic\ninefficiencies, we should model our economic practice on the process that\npreceded production and has led to its emergence. In addition, the study will\noutline ways in which our economic theory and practice must be changed to\nachieve full efficiency of our production. Finally, the study provides a\ncritical overview of the current theories of inflation and remedies that are\nproposed to deal with it.\n"
    },
    {
        "paper_id": 2301.03124,
        "authors": "Eiji Yamamura, Yoshiro Tsutsui, Fumio Ohtake",
        "title": "The COVID-19 vaccination, preventive behaviors and pro-social\n  motivation: panel data analysis from Japan",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The COVID-19 vaccine reduces infection risk: even if one contracts COVID-19,\nthe probability of complications like death or hospitalization is lower.\nHowever, vaccination may prompt people to decrease preventive behaviors, such\nas staying indoors, handwashing, and wearing a mask. Thereby, if vaccinated\npeople pursue only their self-interest, the vaccine's effect may be lower than\nexpected. However, if vaccinated people are pro-social (motivated toward\nbenefit for the whole society), they might maintain preventive behaviors to\nreduce the spread of infection.\n"
    },
    {
        "paper_id": 2301.03136,
        "authors": "Guijin Son, Hanwool Lee, Nahyeon Kang, Moonjeong Hahm",
        "title": "Removing Non-Stationary Knowledge From Pre-Trained Language Models for\n  Entity-Level Sentiment Classification in Finance",
        "comments": "Published at The AAAI-2023 Workshop On Multimodal AI For Financial\n  Forecasting (muffin@AAAI2023)",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Extraction of sentiment signals from news text, stock message boards, and\nbusiness reports, for stock movement prediction, has been a rising field of\ninterest in finance. Building upon past literature, the most recent works\nattempt to better capture sentiment from sentences with complex syntactic\nstructures by introducing aspect-level sentiment classification (ASC). Despite\nthe growing interest, however, fine-grained sentiment analysis has not been\nfully explored in non-English literature due to the shortage of annotated\nfinance-specific data. Accordingly, it is necessary for non-English languages\nto leverage datasets and pre-trained language models (PLM) of different\ndomains, languages, and tasks to best their performance. To facilitate\nfinance-specific ASC research in the Korean language, we build KorFinASC, a\nKorean aspect-level sentiment classification dataset for finance consisting of\n12,613 human-annotated samples, and explore methods of intermediate transfer\nlearning. Our experiments indicate that past research has been ignorant towards\nthe potentially wrong knowledge of financial entities encoded during the\ntraining phase, which has overestimated the predictive power of PLMs. In our\nwork, we use the term \"non-stationary knowledge'' to refer to information that\nwas previously correct but is likely to change, and present \"TGT-Masking'', a\nnovel masking pattern to restrict PLMs from speculating knowledge of the kind.\nFinally, through a series of transfer learning with TGT-Masking applied we\nimprove 22.63% of classification accuracy compared to standalone models on\nKorFinASC.\n"
    },
    {
        "paper_id": 2301.03186,
        "authors": "Hayden Brown",
        "title": "Long-Term Returns Estimation of Leveraged Indexes and ETFs",
        "comments": "23 pages, 9 figures",
        "journal-ref": null,
        "doi": "10.1007/s11408-023-00440-3",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Daily leveraged exchange traded funds amplify gains and losses of their\nunderlying benchmark indexes on a daily basis. The result of going long in a\ndaily leveraged ETF for more than one day is less clear. Here, bounds are given\nfor the log-returns of a leveraged ETF when going long for more than just one\nday. The bounds are quadratic in the daily log-returns of the underlying\nbenchmark index, and they are used to find sufficient conditions for\noutperformance and underperformance of a leveraged ETF in relation to its\nunderlying benchmark index. Results show that if the underlying benchmark index\ndrops 10+\\% over the course of 63 consecutive trading days, and the standard\ndeviation of the benchmark index's daily log-returns is no more than .015, then\ngoing long in a -3x leveraged ETF during that period gives a log-return of at\nleast 1.5 times the log-return of a short position in the underlying benchmark\nindex. Results also show promise for a 2x daily leveraged S&P 500 ETF. If the\naverage annual log-return of the S&P 500 index continues to be at least .0658,\nas it has been in the past, and the standard deviation of daily S&P 500\nlog-returns is under .0125, then a 2x daily leveraged S&P 500 ETF will perform\nat least as well as the S&P 500 index in the long-run.\n"
    },
    {
        "paper_id": 2301.03303,
        "authors": "Juliane Wiese and Nattavudh Powdthavee",
        "title": "How effective are covid-19 vaccine health messages in reducing vaccine\n  skepticism? Heterogeneity in messages effectiveness by just world beliefs",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  To end the COVID-19 pandemic, policymakers have relied on various public\nhealth messages to boost vaccine take-up rates amongst people across wide\npolitical spectra, backgrounds, and worldviews. However, much less is\nunderstood about whether these messages affect different people in the same\nway. One source of heterogeneity is the belief in a just world (BJW), which is\nthe belief that in general, good things happen to good people, and bad things\nhappen to bad people. This study investigates the effectiveness of two common\nmessages of the COVID-19 pandemic: vaccinate to protect yourself and vaccinate\nto protect others in your community. We then examine whether BJW moderates the\neffectiveness of these messages. We hypothesize that just-world believers react\nnegatively to the prosocial pro-vaccine message, as it charges individuals with\nthe responsibility to care for others around them. Using an unvaccinated sample\nof UK residents before vaccines were made widely available (N=526), we\ndemonstrate that the individual-focused message significantly reduces overall\nvaccine skepticism, and that this effect is more robust for individuals with a\nlow BJW, whereas the community-focused message does not. Our findings highlight\nthe importance of individual differences in the reception of public health\nmessages to reduce COVID-19 vaccine skepticism.\n"
    },
    {
        "paper_id": 2301.03354,
        "authors": "Thales A. P. West, Sven Wunder, Erin O. Sills, Jan B\\\"orner, Sami W.\n  Rifai, Alexandra N. Neidermeier, Andreas Kontoleon",
        "title": "Action needed to make carbon offsets from tropical forest conservation\n  work for climate change mitigation",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Carbon offsets from voluntarily avoided deforestation projects are generated\nbased on performance vis-\\`a-vis ex-ante deforestation baselines. We examined\nthe impacts of 27 forest conservation projects in six countries on three\ncontinents using synthetic control methods for causal inference. We compare the\nproject baselines with ex-post counterfactuals based on observed deforestation\nin control sites. Our findings show that most projects have not reduced\ndeforestation. For projects that did, reductions were substantially lower than\nclaimed. Methodologies for constructing deforestation baselines for\ncarbon-offset interventions thus need urgent revisions in order to correctly\nattribute reduced deforestation to the conservation interventions, thus\nmaintaining both incentives for forest conservation and the integrity of global\ncarbon accounting.\n"
    },
    {
        "paper_id": 2301.03404,
        "authors": "Xhesilda Vogli, Erion \\c{C}ano",
        "title": "CSRCZ: A Dataset About Corporate Social Responsibility in Czech Republic",
        "comments": "5 pages, 5 tables, 2 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  As stakeholders' pressure on corporates for disclosing their corporate social\nresponsibility operations grows, it is crucial to understand how efficient\ncorporate disclosure systems are in bridging the gap between corporate social\nresponsibility reports and their actual practice. Meanwhile, research on\ncorporate social responsibility is still not aligned with the recent\ndata-driven strategies, and little public data are available. This paper aims\nto describe CSRCZ, a newly created dataset based on disclosure reports from the\nwebsites of 1000 companies that operate in Czech Republic. Each company was\nanalyzed based on three main parameters: company size, company industry, and\ncompany initiatives. We describe the content of the dataset as well as its\npotential use for future research. We believe that CSRCZ has implications for\nfurther research, since it is the first publicly available dataset of its kind.\n"
    },
    {
        "paper_id": 2301.03517,
        "authors": "Xia Han and Liyuan Lin and Ruodu Wang",
        "title": "Diversification quotients based on VaR and ES",
        "comments": "arXiv admin note: substantial text overlap with arXiv:2206.13679",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The diversification quotient (DQ) is recently introduced for quantifying the\ndegree of diversification of a stochastic portfolio model. It has an axiomatic\nfoundation and can be defined through a parametric class of risk measures.\nSince the Value-at-Risk (VaR) and the Expected Shortfall (ES) are the most\nprominent risk measures widely used in both banking and insurance, we\ninvestigate DQ constructed from VaR and ES in this paper. In particular, for\nthe popular models of elliptical and multivariate regular varying (MRV)\ndistributions, explicit formulas are available. The portfolio optimization\nproblems for the elliptical and MRV models are also studied. Our results\nfurther reveal favourable features of DQ, both theoretically and practically,\ncompared to traditional diversification indices based on a single risk measure.\n"
    },
    {
        "paper_id": 2301.03716,
        "authors": "Khwan Kim, Noah Askin, James A. Evans",
        "title": "Disrupted Routines Anticipate Musical Exploration",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Prior research suggests that taste preferences relate to personality traits,\nvalues, shifts in mood, and immigration destination, but understanding everyday\npatterns of listening and the function music plays in life have remained\nelusive, despite speculations that musical nostalgia may compensate for local\ndisruption. Using more than a hundred million streams of 4 million songs by\ntens of thousands of international listeners from a global music service\ncatering to local tastes, here we show that breaches in personal routine are\nsystematically associated with personal musical exploration. As people visited\nnew cities and countries, their preferences diversified, converging towards\ntheir destinations. As people experienced COVID-19 lock-downs, and then again\nwhen they experienced reopenings, their preferences diversified further.\n"
    },
    {
        "paper_id": 2301.0402,
        "authors": "Jian Guo, Saizhuo Wang, Lionel M. Ni, Heung-Yeung Shum",
        "title": "Quant 4.0: Engineering Quantitative Investment with Automated,\n  Explainable and Knowledge-driven Artificial Intelligence",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Quantitative investment (``quant'') is an interdisciplinary field combining\nfinancial engineering, computer science, mathematics, statistics, etc. Quant\nhas become one of the mainstream investment methodologies over the past\ndecades, and has experienced three generations: Quant 1.0, trading by\nmathematical modeling to discover mis-priced assets in markets; Quant 2.0,\nshifting quant research pipeline from small ``strategy workshops'' to large\n``alpha factories''; Quant 3.0, applying deep learning techniques to discover\ncomplex nonlinear pricing rules. Despite its advantage in prediction, deep\nlearning relies on extremely large data volume and labor-intensive tuning of\n``black-box'' neural network models. To address these limitations, in this\npaper, we introduce Quant 4.0 and provide an engineering perspective for\nnext-generation quant. Quant 4.0 has three key differentiating components.\nFirst, automated AI changes quant pipeline from traditional hand-craft modeling\nto the state-of-the-art automated modeling, practicing the philosophy of\n``algorithm produces algorithm, model builds model, and eventually AI creates\nAI''. Second, explainable AI develops new techniques to better understand and\ninterpret investment decisions made by machine learning black-boxes, and\nexplains complicated and hidden risk exposures. Third, knowledge-driven AI is a\nsupplement to data-driven AI such as deep learning and it incorporates prior\nknowledge into modeling to improve investment decision, in particular for\nquantitative value investing. Moreover, we discuss how to build a system that\npractices the Quant 4.0 concept. Finally, we propose ten challenging research\nproblems for quant technology, and discuss potential solutions, research\ndirections, and future trends.\n"
    },
    {
        "paper_id": 2301.04052,
        "authors": "A. Y. Aydemir",
        "title": "Optimal social security timing",
        "comments": "21 pages, 5 figures, 2 tables",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  The optimal age that a retiree claims social security retirement benefits is\nin general a complicated function of many factors. However, if the\nbeneficiary's finances and health are not the constraining factors, it is\npossible to formally derive mathematical models that maximize a well-defined\nmeasure of his total benefits. A model that takes into account various factors\nsuch as the increase in the benefits for delayed claims and the penalties for\nearly retirement, the advantages of investing some of the benefits in the\nfinancial markets, and the effects of cost-of-living adjustments shows that not\nwaiting until age 70 is almost always the better option. The optimal claiming\nage that maximizes the total benefits, however, depends on the expected market\nreturns and the rate of cost-of-living adjustments, with the higher market\nrates in general pushing the optimal age lower. The models presented here can\nbe easily tailored to address the particular circumstances and goals of any\nindividual.\n"
    },
    {
        "paper_id": 2301.04095,
        "authors": "Yasa Syed, Guanyang Wang",
        "title": "Optimal randomized multilevel Monte Carlo for repeatedly nested\n  expectations",
        "comments": "Accepted by ICML 2023. This version generalizes Thm 2.2 and 2.4 to\n  multivariate underlying process, adds two numerical experiments, adds several\n  references, and corrects several typos",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  The estimation of repeatedly nested expectations is a challenging task that\narises in many real-world systems. However, existing methods generally suffer\nfrom high computational costs when the number of nestings becomes large. Fix\nany non-negative integer $D$ for the total number of nestings. Standard Monte\nCarlo methods typically cost at least $\\mathcal{O}(\\varepsilon^{-(2+D)})$ and\nsometimes $\\mathcal{O}(\\varepsilon^{-2(1+D)})$ to obtain an estimator up to\n$\\varepsilon$-error. More advanced methods, such as multilevel Monte Carlo,\ncurrently only exist for $D = 1$. In this paper, we propose a novel Monte Carlo\nestimator called $\\mathsf{READ}$, which stands for \"Recursive Estimator for\nArbitrary Depth.'' Our estimator has an optimal computational cost of\n$\\mathcal{O}(\\varepsilon^{-2})$ for every fixed $D$ under suitable assumptions,\nand a nearly optimal computational cost of $\\mathcal{O}(\\varepsilon^{-2(1 +\n\\delta)})$ for any $0 < \\delta < \\frac12$ under much more general assumptions.\nOur estimator is also unbiased, which makes it easy to parallelize. The key\ningredients in our construction are an observation of the problem's recursive\nstructure and the recursive use of the randomized multilevel Monte Carlo\nmethod.\n"
    },
    {
        "paper_id": 2301.04118,
        "authors": "Yuqi Li and Lihua Zhang",
        "title": "Achieving a Given Financial Goal with Optimal Deferred Term Insurance\n  Purchasing Policy",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper researches the problem of purchasing deferred term insurance in\nthe context of financial planning to maximize the probability of achieving a\npersonal financial goal. Specifically, our study starts from the perspective of\nhedging death risk and longevity risk, and considers the purchase of deferred\nterm life insurance and deferred term pure endowment to achieve a given\nfinancial goal for the first time in both deterministic and stochastic\nframework. In particular, we consider income, consumption and risky investment\nin the stochastic framework, extending previous results in\n\\cite{Bayraktar2016}. The time cutoff m and n make the work more difficult.\nHowever, by establishing new controls,``\\emph{quasi-ideal value}\"\nand``\\emph{ideal value}\", we solve the corresponding ordinary differential\nequations or stochastic differential equations, and give the specific\nexpressions for the maximum probability. Then we provide the optimal life\ninsurance purchasing strategies and the optimal risk investment strategies. In\ngeneral, when m \\geqslant 0, n>0, deferred term insurance or term life\ninsurance is a better choice for those who want to achieve their financial or\nbequest goals but are not financially sound. In particular, if m >0, n\n\\rightarrow \\infty, our viewpoint also sheds light on reaching a bequest goal\nby purchasing deferred whole life insurance. It is worth noting that when m=0,\nn \\rightarrow \\infty, our problem is equivalent to achieving the just mentioned\nbequest goal by purchasing whole life insurance, at which point the maximum\nprobability and the life insurance purchasing strategies we provide are\nconsistent with those in \\cite{Bayraktar2014, Bayraktar2016}.\n"
    },
    {
        "paper_id": 2301.04244,
        "authors": "Anup Rao",
        "title": "Elastic Cash",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Elastic Cash is a new decentralized mechanism for regulating the money\nsupply. The mechanism operates by modifying the supply so that an interest rate\ndetermined by a public market is kept approximately fixed. It can be\nincorporated into the conventional monetary system to improve the elasticity of\nthe US Dollar, and it can be used to design new elastic cryptocurrencies that\nremain decentralized.\n"
    },
    {
        "paper_id": 2301.04455,
        "authors": "Tashreef Muhammad, Tahsin Aziz and Mohammad Shafiul Alam",
        "title": "Utilizing Technical Data to Discover Similar Companies in Dhaka Stock\n  Exchange",
        "comments": "7 pages, 3 images, initial collected information",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Stock market investment have been an ideal form of investment for many years.\nInvesting capitals smartly in stock market yields high profit returns. But\nthere are many companies available in a market. Currently there are more than\n$345$ active companies who have stocks in Dhaka Stock Exchange (DSE). Analyzing\nall these companies is quite impossible. However, many companies tend to move\ntogether. This study aims at finding which companies in DSE have a close\nconnection and move alongside each other. By analyzing this relation, the\ninvestors and traders will be able to analyze a lot of companies' statistics\nfrom a calculating just a handful number of companies. The conducted experiment\nyielded promising results. It was found that though the system was not given\nanything other than technical data, it was able to identify companies that show\ndomain specific outcomes. In other words, a relation between technical data and\nfundamental data was discovered from the conducted experiment.\n"
    },
    {
        "paper_id": 2301.04579,
        "authors": "Hardik Rajpal, Omar A Guerrero",
        "title": "Synergistic Small Worlds that Drive Technological Sophistication",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  Advanced economies exhibit a high degree of sophistication in the creation of\nvarious products. While critical to such sophistication, the nature and\nunderlying structure of the interactions taking place inside production\nprocesses remain opaque when studying large systems such as industries or\nentire economies. Using partial information decomposition, we quantify the\nnature of these interactions, allowing us to infer how much innovation stems\nform specific input interactions and how they are structured. These estimates\nyield a novel picture of the nuanced interactions underpinning technological\nsophistication. By analyzing networks of synergistic interactions, we find that\nmore sophisticated industries tend to exhibit highly modular small-world\ntopologies; with the tertiary sector as its central connective core. Countries\nand industries that have a well-established connective core and specialized\nmodules exhibit higher economic complexity and output efficiency. Similar\nmodular networks have been found to be responsible for maintaining a balance\nbetween integration and segregation of information in the human brain,\nsuggesting a universal principle underlying the organization of sophisticated\nproduction processes.\n"
    },
    {
        "paper_id": 2301.04607,
        "authors": "Han-Sol Lee, Sergey U. Chernikov, Szabolcs Nagy, Ekaterina A.\n  Degtereva",
        "title": "The Impact of National Culture on Innovation A Comparative Analysis\n  between Developed and Developing Nations during the Pre and Post Crisis\n  Period 2007_2021",
        "comments": "Keywords Hofstede cultural dimensions (HCD); Global Innovation Index\n  (GII); financial crisis; COVID-19; comparative analysis",
        "journal-ref": "SOCIAL SCIENCES 2076-0760 11. (11.) Paper: 522 , 15 p. 2022",
        "doi": "10.3390/socsci11110522",
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  This empirical study investigates the impact of the Hofstede cultural\ndimensions (HCD) on the Global Innovation Index (GII) scores in four different\nyears (2007, 2009, 2019 and 2021) to compare the impacts during the pre- and\npost-crisis (financial and COVID-19) period by employing ordinary least square\n(OLS) and robust least square (Robust) analyses. The purpose of this study is\nto identify the impact of cultural factors on the innovation development for\ndifferent income groups during the pre- and post-crisis period. We found that,\nin general, the same cultural properties were required for countries to enhance\ninnovation inputs and outputs regardless of pre- and post-crisis periods and\ntime variances. The significant cultural factors (driving forces) of the\ninnovation performance do not change over time. However, our empirical results\nrevealed that not the crisis itself but the income group (either developed or\ndeveloping) is the factor that influences the relationship between cultural\nproperties and innovation. It is also worth noting that cultural properties\nhave lost much of their impact on innovation, particularly in developing\ncountries, during recent periods. It is highly likely that in terms of\ninnovation, no cultural development or change can significantly impact the\ninnovation output of developing countries without the construction of the\nappropriate systems.\n"
    },
    {
        "paper_id": 2301.04609,
        "authors": "Szabolcs Nagy, Csilla Konyha Molnarne",
        "title": "The Effects of Hofstede's Cultural Dimensions on Pro-Environmental\n  Behaviour: How Culture Influences Environmentally Conscious Behaviour",
        "comments": "Keywords: pro-environmental behavior, culture, Hofstedes cultural\n  dimensions, Hungary, individualism, power distance",
        "journal-ref": "THEORY METHODOLOGY PRACTICE: CLUB OF ECONOMICS IN MISKOLC 14 (1)\n  pp. 27-36 2018",
        "doi": "10.18096/TMP.2018.01.03",
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  The need for a more sustainable lifestyle is a key focus for several\ncountries. Using a questionnaire survey conducted in Hungary, this paper\nexamines how culture influences environmentally conscious behaviour. Having\ninvestigated the direct impact of Hofstedes cultural dimensions on\npro-environmental behaviour, we found that the culture of a country hardly\naffects actual environmentally conscious behaviour. The findings indicate that\nonly individualism and power distance have a significant but weak negative\nimpact on pro-environmental behaviour. Based on the findings, we can state that\na positive change in culture is a necessary but not sufficient condition for\nmaking a country greener.\n"
    },
    {
        "paper_id": 2301.04925,
        "authors": "Mazzoni Leonardo, Pinelli Fabio, Riccaboni Massimo",
        "title": "Measuring Corporate Digital Divide with web scraping: Evidence from\n  Italy",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  With the increasing pervasiveness of ICTs in the fabric of economic\nactivities, the corporate digital divide has emerged as a new crucial topic to\nevaluate the IT competencies and the digital gap between firms and territories.\nGiven the scarcity of available granular data to measure the phenomenon, most\nstudies have used survey data. To bridge the empirical gap, we scrape the\nwebsite homepage of 182 705 Italian firms, extracting ten features related to\ntheir digital footprint characteristics to develop a new corporate digital\nassessment index. Our results highlight a significant digital divide across\ndimensions, sectors and geographical locations of Italian firms, opening up new\nperspectives on monitoring and near-real-time data-driven analysis.\n"
    },
    {
        "paper_id": 2301.04996,
        "authors": "Jarek K\\k{e}dra, Assaf Libman, Victoria Steblovskaya",
        "title": "European baskets in discrete-time continuous-binomial market models",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We consider a discrete-time incomplete multi-asset market model with\ncontinuous price jumps. For a wide class of contingent claims, including\nEuropean basket call options, we compute the bounds of the interval containing\nthe no-arbitrage prices. We prove that the lower bound coincides, in fact, with\nJensen's bound. The upper bound can be computed by restricting to a binomial\nmodel for which an explicit expression for the bound is known by an earlier\nwork of the authors. We describe explicitly a maximal hedging strategy which is\nthe best possible in the sense that its value is equal to the upper bound of\nthe price interval of the claim. Our results show that for any $c$ in the\ninterval of the non-arbitrage contingent claim price at time $0$, one can\nchange the boundaries of the price jumps to obtain a model in which $c$ is the\nupper bound at time $0$ of this interval. The lower bound of this interval\nremains unaffected.\n"
    },
    {
        "paper_id": 2301.0508,
        "authors": "J. E. Salgado-Hern\\'andez and Manan Vyas",
        "title": "Non-linear correlation analysis in financial markets using hierarchical\n  clustering",
        "comments": "18 pages, 11 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Distance correlation coefficient (DCC) can be used to identify new\nassociations and correlations between multiple variables. The distance\ncorrelation coefficient applies to variables of any dimension, can be used to\ndetermine smaller sets of variables that provide equivalent information, is\nzero only when variables are independent, and is capable of detecting nonlinear\nassociations that are undetectable by the classical Pearson correlation\ncoefficient (PCC). Hence, DCC provides more information than the PCC. We\nanalyze numerous pairs of stocks in S\\&P500 database with the distance\ncorrelation coefficient and provide an overview of stochastic evolution of\nfinancial market states based on these correlation measures obtained using\nagglomerative clustering.\n"
    },
    {
        "paper_id": 2301.05157,
        "authors": "Eyal Neuman, Yufei Zhang",
        "title": "Statistical Learning with Sublinear Regret of Propagator Models",
        "comments": "49 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We consider a class of learning problems in which an agent liquidates a risky\nasset while creating both transient price impact driven by an unknown\nconvolution propagator and linear temporary price impact with an unknown\nparameter. We characterize the trader's performance as maximization of a\nrevenue-risk functional, where the trader also exploits available information\non a price predicting signal. We present a trading algorithm that alternates\nbetween exploration and exploitation phases and achieves sublinear regrets with\nhigh probability. For the exploration phase we propose a novel approach for\nnon-parametric estimation of the price impact kernel by observing only the\nvisible price process and derive sharp bounds on the convergence rate, which\nare characterised by the singularity of the propagator. These kernel estimation\nmethods extend existing methods from the area of Tikhonov regularisation for\ninverse problems and are of independent interest. The bound on the regret in\nthe exploitation phase is obtained by deriving stability results for the\noptimizer and value function of the associated class of infinite-dimensional\nstochastic control problems. As a complementary result we propose a\nregression-based algorithm to estimate the conditional expectation of\nnon-Markovian signals and derive its convergence rate.\n"
    },
    {
        "paper_id": 2301.053,
        "authors": "Jiwon Kim and Moon-Ju Kang and KangHun Lee and HyungJun Moon and\n  Bo-Kwan Jeon",
        "title": "Deep Reinforcement Learning for Asset Allocation: Reward Clipping",
        "comments": "11 pages, 9 figures, 5 tables",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  Recently, there are many trials to apply reinforcement learning in asset\nallocation for earning more stable profits. In this paper, we compare\nperformance between several reinforcement learning algorithms - actor-only,\nactor-critic and PPO models. Furthermore, we analyze each models' character and\nthen introduce the advanced algorithm, so called Reward clipping model. It\nseems that the Reward Clipping model is better than other existing models in\nfinance domain, especially portfolio optimization - it has strength both in\nbull and bear markets. Finally, we compare the performance for these models\nwith traditional investment strategies during decreasing and increasing\nmarkets.\n"
    },
    {
        "paper_id": 2301.05332,
        "authors": "Yoshihiro Shirai",
        "title": "A Levy-driven Ornstein-Uhlenbeck process for the valuation of credit\n  index swaptions",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  A Levy-driven Ornstein-Uhlenbeck process is proposed to model the evolution\nof the risk-free rate and default intensities for the purpose of evaluating\noption contracts on a credit index. Time evolution in credit markets is assumed\nto follow a gamma process in order to reflect the different pace at which\ncredit products are exchanged with respect to that of risk-free debt. Formulas\nfor the characteristic function, zero coupon bonds, moments of the process and\nits stationary distribution are derived. Numerical experiments showing\nconvergence of standard numerical methods for the valuation PIDE to analytical\nand Montecarlo solutions are shown. Calibration to market prices of options on\na credit index is performed, and model and market implied summary statistics\nfor the underlying credit spreads are estimated and compared.\n"
    },
    {
        "paper_id": 2301.05333,
        "authors": "Yoshihiro Shirai",
        "title": "Acceptable Bilateral Gamma Parameters",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  The purpose of this paper is to utilize statistical methodologies to infer\nfrom market prices of assets and their derivatives the magnitude of the set of\na measure M that defines acceptance sets of risky future cash flows.\nSpecifically, we estimate upper and lower boundaries of the compensation needed\nfor a given bilateral gamma distributed future cash flow to be acceptable. We\nshow that prospects theory provides a natural interpretation of the behaviors\nimplied by such boundaries, which are not compatible with expected utility\ntheory over terminal wealth. Boundaries for bilateral gamma risk neutral scale\nparameters for given speed parameters are also estimated and tested against\nmarket data and, in particular, comparisons are made with known empirical facts\nabout the magnitude of the acceptance set of a common class of risk measures.\n"
    },
    {
        "paper_id": 2301.05443,
        "authors": "Tomoo Kikuchi and Satoshi Tobe",
        "title": "ASEAN's Portfolio Investment in a Gravity Model",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  We investigate the elasticity of portfolio investment to geographical\ndistance in a gravity model utilizing a bilateral panel of 86 reporting and 241\ncounterparty countries/territories for 2007-2017. We find that the elasticity\nis more negative for ASEAN than OECD members. The difference is larger if we\nexclude Singapore. This indicates that Singapore's behavior is very different\nfrom other ASEAN members. While Singapore tends to invest in faraway OECD\ncountries, other ASEAN members tend to invest in nearby countries. Our study\nalso shows the emergence of China as a significant investment destination for\nASEAN members.\n"
    },
    {
        "paper_id": 2301.05677,
        "authors": "Mohammed Salek, Damien Challet and Ioane Muni Toke",
        "title": "Price impact in equity auctions: zero, then linear",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  Using high-quality data, we report several statistical regularities of equity\nauctions in the Paris stock exchange. First, the average order book density is\nlinear around the auction price at the time of auction clearing and has a large\npeak at the auction price. While the peak is due to slow traders, the order\ndensity shape is the result of subtle dynamics. The impact of a new market\norder or cancellation at the auction time can be decomposed into three parts as\na function of the size of the additional order: (1) zero impact, caused by the\ndiscrete nature of prices, sometimes up to a surprisingly large additional\nvolume relative to the auction volume (2) linear impact for additional orders\nup to a large fraction of the auction volume (3) for even larger orders price\nimpact is non-linear, frequently super-linear.\n"
    },
    {
        "paper_id": 2301.05693,
        "authors": "Fateme Shahabi Nejad, Mohammad Mehdi Ebadzadeh",
        "title": "Stock market forecasting using DRAGAN and feature matching",
        "comments": "Preprint submitted to Elsevier",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
        "abstract": "  Applying machine learning methods to forecast stock prices has been one of\nthe research topics of interest in recent years. Almost few studies have been\nreported based on generative adversarial networks (GANs) in this area, but\ntheir results are promising. GANs are powerful generative models successfully\napplied in different areas but suffer from inherent challenges such as training\ninstability and mode collapse. Also, a primary concern is capturing\ncorrelations in stock prices. Therefore, our challenges fall into two main\ncategories: capturing correlations and inherent problems of GANs. In this\npaper, we have introduced a novel framework based on DRAGAN and feature\nmatching for stock price forecasting, which improves training stability and\nalleviates mode collapse. We have employed windowing to acquire temporal\ncorrelations by the generator. Also, we have exploited conditioning on\ndiscriminator inputs to capture temporal correlations and correlations between\nprices and features. Experimental results on data from several stocks indicate\nthat our proposed method outperformed long short-term memory (LSTM) as a\nbaseline method, also basic GANs and WGAN-GP as two different variants of GANs.\n"
    },
    {
        "paper_id": 2301.05798,
        "authors": "Jing Gao, Sen Li",
        "title": "Regulating For-Hire Autonomous Vehicles for An Equitable Multimodal\n  Transportation Network",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper assesses the equity impacts of for-hire autonomous vehicles (AVs)\nand investigates regulatory policies that promote spatial and social equity in\nfuture autonomous mobility ecosystems. To this end, we consider a multimodal\ntransportation network, where a ride-hailing platform operates a fleet of AVs\nto offer mobility-on-demand services in competition with a public transit\nagency that offers transit services on a transportation network. A\ngame-theoretic model is developed to characterize the intimate interactions\nbetween the ride-hailing platform, the transit agency, and multiclass\npassengers with distinct income levels. An algorithm is proposed to compute the\nNash equilibrium of the game and conduct an ex-post evaluation of the\nperformance of the obtained solution. Based on the proposed framework, we\nevaluate the spatial and social equity in transport accessibility using the\nTheil index, and find that although the proliferation of for-hire AVs in the\nride-hailing network improves overall accessibility, the benefits are not\nfairly distributed among distinct locations or population groups, implying that\nthe deployment of AVs will enlarge the existing spatial and social inequity\ngaps in the transportation network if no regulatory intervention is in place.\nTo address this concern, we investigate two regulatory policies that can\nimprove transport equity: (a) a minimum service-level requirement on\nride-hailing services, which improves the spatial equity in the transport\nnetwork; (b) a subsidy on transit services by taxing ride-hailing services,\nwhich promotes the use of public transit and improves the spatial and social\nequity of the transport network. We show that the minimum service-level\nrequirement entails a trade-off: as a higher minimum service level is imposed,\nthe spatial inequity reduces, but the social inequity will be exacerbated. On\nthe other hand ...\n"
    },
    {
        "paper_id": 2301.05886,
        "authors": "Michael B. Giles, Abdul-Lateef Haji-Ali, Jonathan Spence",
        "title": "Efficient Risk Estimation for the Credit Valuation Adjustment",
        "comments": "35 pages, 2 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The valuation of over-the-counter derivatives is subject to a series of\nvaluation adjustments known as xVA, which pose additional risks for financial\ninstitutions. Associated risk measures, such as the value-at-risk of an\nunderlying valuation adjustment, play an important role in managing these\nrisks. Monte Carlo methods are often regarded as inefficient for computing such\nmeasures. As an example, we consider the value-at-risk of the Credit Valuation\nAdjustment (CVA-VaR), which can be expressed using a triple nested expectation.\nTraditional Monte Carlo methods are often inefficient at handling several\nnested expectations. Utilising recent developments in multilevel nested\nsimulation for probabilities, we construct a hierarchical estimator of the\nCVA-VaR which reduces the computational complexity by 3 orders of magnitude\ncompared to standard Monte Carlo.\n"
    },
    {
        "paper_id": 2301.05999,
        "authors": "Gaurab Aryal and Dennis J. Campbell and Federico Ciliberto and\n  Ekaterina A. Khmelnitskaya",
        "title": "Common Subcontracting and Airline Prices",
        "comments": "forthcoming in The Review of Economics and Statistics",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In the US airline industry, independent regional airlines fly passengers on\nbehalf of several national airlines across different markets, giving rise to\n$\\textit{common subcontracting}$. On the one hand, we find that subcontracting\nis associated with lower prices, consistent with the notion that regional\nairlines tend to fly passengers at lower costs than major airlines. On the\nother hand, we find that $\\textit{common}$ subcontracting is associated with\nhigher prices. These two countervailing effects suggest that the growth of\nregional airlines can have anticompetitive implications for the industry.\n"
    },
    {
        "paper_id": 2301.0645,
        "authors": "Lingjiong Zhu",
        "title": "A delayed dual risk model",
        "comments": "17 pages, 2 figures, 2 tables",
        "journal-ref": "Stochastic Models 33(1), 149-170, 2017",
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper, we study a dual risk model with delays in the spirit of\nDassios-Zhao. When a new innovation occurs, there is a delay before the\ninnovation turns into a profit. We obtain large initial surplus asymptotics for\nthe ruin probability and ruin time distributions. For some special cases, we\nget closed-form formulas. Numerical illustrations will also be provided.\n"
    },
    {
        "paper_id": 2301.0646,
        "authors": "Dan Pirjol, Lingjiong Zhu",
        "title": "Sensitivities of Asian options in the Black-Scholes model",
        "comments": "21 pages, 2 figures, 7 tables",
        "journal-ref": "International Journal of Theoretical and Applied Finance 21(01),\n  1850008, 2018",
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We propose analytical approximations for the sensitivities (Greeks) of the\nAsian options in the Black-Scholes model, following from a small\nmaturity/volatility approximation for the option prices which has the exact\nshort maturity limit, obtained using large deviations theory. Numerical tests\ndemonstrate good agreement of the proposed approximation with alternative\nnumerical simulation results for cases of practical interest. We also study the\nqualitative properties of Asian Greeks, including new results for Rho, the\nsensitivity with respect to changes in the risk-free rate, and Psi, the\nsensitivity with respect to the dividend yield. In particular we show that the\nRho of a fixed-strike Asian option and the Psi of a floating-strike Asian\noption can change sign.\n"
    },
    {
        "paper_id": 2301.06831,
        "authors": "Rohan Tangri, Peter Yatsyshin, Elisabeth A. Duijnstee, Danilo Mandic",
        "title": "Generalizing Impermanent Loss on Decentralized Exchanges with Constant\n  Function Market Makers",
        "comments": "14 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Liquidity providers are essential for the function of decentralized exchanges\nto ensure liquidity takers can be guaranteed a counterparty for their trades.\nHowever, liquidity providers investing in liquidity pools face many risks, the\nmost prominent of which is impermanent loss. Currently, analysis of this metric\nis difficult to conduct due to different market maker algorithms, fee\nstructures and concentrated liquidity dynamics across the various exchanges. To\nthis end, we provide a framework to generalize impermanent loss for multiple\nasset pools obeying any constant function market maker with optional\nconcentrated liquidity. We also discuss how pool fees fit into the framework,\nand identify the condition for which liquidity provisioning becomes profitable\nwhen earnings from trading fees exceed impermanent loss. Finally, we\ndemonstrate the utility and generalizability of this framework with simulations\nin BalancerV2 and UniswapV3.\n"
    },
    {
        "paper_id": 2301.06847,
        "authors": "Abdelali Gabih, Hakam Kondakji, Ralf Wunderlich",
        "title": "Power Utility Maximization with Expert Opinions at Fixed Arrival Times\n  in a Market with Hidden Gaussian Drift",
        "comments": "37 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper we study optimal trading strategies in a financial market in\nwhich stock returns depend on a hidden Gaussian mean reverting drift process.\nInvestors obtain information on that drift by observing stock returns.\nMoreover, expert opinions in the form of signals about the current state of the\ndrift arriving at fixed and known dates are included in the analysis. Drift\nestimates are based on Kalman filter techniques. They are used to transform a\npower utility maximization problem under partial information into an\noptimization problem under full information where the state variable is the\nfilter of the drift. The dynamic programming equation for this problem is\nstudied and closed-form solutions for the value function and the optimal\ntrading strategy of an investor are derived. They allow to quantify the\nmonetary value of information delivered by the expert opinions. We illustrate\nour theoretical findings by results of extensive numerical experiments.\n"
    },
    {
        "paper_id": 2301.0706,
        "authors": "Dangxing Chen and Luyao Zhang",
        "title": "Monotonicity for AI ethics and society: An empirical study of the\n  monotonic neural additive model in criminology, education, health care, and\n  finance",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Algorithm fairness in the application of artificial intelligence (AI) is\nessential for a better society. As the foundational axiom of social mechanisms,\nfairness consists of multiple facets. Although the machine learning (ML)\ncommunity has focused on intersectionality as a matter of statistical parity,\nespecially in discrimination issues, an emerging body of literature addresses\nanother facet -- monotonicity. Based on domain expertise, monotonicity plays a\nvital role in numerous fairness-related areas, where violations could misguide\nhuman decisions and lead to disastrous consequences. In this paper, we first\nsystematically evaluate the significance of applying monotonic neural additive\nmodels (MNAMs), which use a fairness-aware ML algorithm to enforce both\nindividual and pairwise monotonicity principles, for the fairness of AI ethics\nand society. We have found, through a hybrid method of theoretical reasoning,\nsimulation, and extensive empirical analysis, that considering monotonicity\naxioms is essential in all areas of fairness, including criminology, education,\nhealth care, and finance. Our research contributes to the interdisciplinary\nresearch at the interface of AI ethics, explainable AI (XAI), and\nhuman-computer interactions (HCIs). By evidencing the catastrophic consequences\nif monotonicity is not met, we address the significance of monotonicity\nrequirements in AI applications. Furthermore, we demonstrate that MNAMs are an\neffective fairness-aware ML approach by imposing monotonicity restrictions\nintegrating human intelligence.\n"
    },
    {
        "paper_id": 2301.07318,
        "authors": "Chuting Sun, Qi Wu, Xing Yan",
        "title": "Dynamic CVaR Portfolio Construction with Attention-Powered Generative\n  Factor Learning",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  The dynamic portfolio construction problem requires dynamic modeling of the\njoint distribution of multivariate stock returns. To achieve this, we propose a\ndynamic generative factor model which uses random variable transformation as an\nimplicit way of distribution modeling and relies on the Attention-GRU network\nfor dynamic learning and forecasting. The proposed model captures the dynamic\ndependence among multivariate stock returns, especially focusing on the\ntail-side properties. We also propose a two-step iterative algorithm to train\nthe model and then predict the time-varying model parameters, including the\ntime-invariant tail parameters. At each investment date, we can easily simulate\nnew samples from the learned generative model, and we further perform CVaR\nportfolio optimization with the simulated samples to form a dynamic portfolio\nstrategy. The numerical experiment on stock data shows that our model leads to\nwiser investments that promise higher reward-risk ratios and present lower tail\nrisks.\n"
    }
]