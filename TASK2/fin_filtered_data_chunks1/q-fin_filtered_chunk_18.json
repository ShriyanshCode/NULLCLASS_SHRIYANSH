[
    {
        "paper_id": 2401.04378,
        "authors": "Zan Yu and Lianzeng Zhang",
        "title": "Computing the Gerber-Shiu function with interest and a constant dividend\n  barrier by physics-informed neural networks",
        "comments": "23 pages; 5 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
        "abstract": "  In this paper, we propose a new efficient method for calculating the\nGerber-Shiu discounted penalty function. Generally, the Gerber-Shiu function\nusually satisfies a class of integro-differential equation. We introduce the\nphysics-informed neural networks (PINN) which embed a differential equation\ninto the loss of the neural network using automatic differentiation. In\naddition, PINN is more free to set boundary conditions and does not rely on the\ndetermination of the initial value. This gives us an idea to calculate more\ngeneral Gerber-Shiu functions. Numerical examples are provided to illustrate\nthe very good performance of our approximation.\n"
    },
    {
        "paper_id": 2401.04521,
        "authors": "Arman Abgaryan, Utkarsh Sharma, Joshua Tobkin",
        "title": "Proof of Efficient Liquidity: A Staking Mechanism for Capital Efficient\n  Liquidity",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  The Proof of Efficient Liquidity (PoEL) protocol, designed for specialised\nProof of Stake (PoS) consensus-based blockchains that incorporate intrinsic\nDeFi applications, aims to support sustainable liquidity bootstrapping and\nnetwork security. This concept seeks to efficiently utilise budgeted staking\nrewards to attract and sustain liquidity through a risk-structuring engine and\nincentive allocation strategy, both of which are designed to maximise capital\nefficiency. The proposed protocol serves the dual objective of: (i) capital\ncreation by attracting risk capital efficiently and maximising its operational\nutility for intrinsic DeFi applications, thereby asserting sustainability; and\n(ii) enhancing the adopting blockchain network's economic security by\naugmenting their staking (PoS) mechanism with a harmonious layer seeking to\nattract a diversity of digital assets. Finally, the protocol's conceptual\nframework, as detailed in the appendix, is extended to encompass service fee\ncredits. This extension capitalises on the network's auxiliary services to\ndisperse incentives and attract liquidity, ensuring the network achieves and\nmaintains the critical usage threshold essential for its sustained operational\nviability and progressive growth.\n"
    },
    {
        "paper_id": 2401.04526,
        "authors": "Lei Shen, Qingyue Shi, Vinit Parida, Marin Jovanovic",
        "title": "Ecosystem orchestration practices for industrial firms: A qualitative\n  meta-analysis, framework development and research agenda",
        "comments": null,
        "journal-ref": null,
        "doi": "10.1016/j.jbusres.2023.114463",
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  This study ventures into the dynamic realm of ecosystem orchestration for\nindustrial firms, emphasizing its significance in maintaining competitive\nadvantage in the digital era. The fragmented research on this important subject\nposes challenges for firms aiming to navigate and capitalize on ecosystem\norchestration. To bridge this knowledge gap, we conducted a comprehensive\nqualitative meta-analysis of 31 case studies and identified multifaceted\norchestration practices employed by industrial firms. The core contribution of\nthis research is the illumination of five interdependent but interrelated\norchestration practices: strategic design, relational, resource integration,\ntechnological, and innovation. Together, these practices are synthesized into\nan integrative framework termed the \"Stirring Model,\" which serves as a\npractical guide to the orchestration practices. Furthermore, the conceptual\nframework clarifies the synergy between the identified practices and highlights\ntheir collective impact. This study proposes theoretical and practical\nimplications for ecosystem orchestration literature and suggests avenues for\nfurther research.\n"
    },
    {
        "paper_id": 2401.04573,
        "authors": "Nestor Gandelman, Osiris Jorge Parcero, Flavia Roldan",
        "title": "Opportunities to upgrade the scientific disciplines space",
        "comments": "Pages 32, 11 tables, 3 figure",
        "journal-ref": null,
        "doi": "10.4067/S0718-88702022000200049",
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  Knowledge generated in a given scientific domain may spill over into other\nclose scientific disciplines, thereby improving performance. Using bibliometric\ndata from the SCImago database drawn from a sample of 174 countries, we\nimplement a measure of proximity based on revealed comparative advantage (RCA).\nOur estimates show that proximity between disciplines positively and\nsignificantly affects the RCA growth rate. This impact is larger on disciplines\nthat currently do not have RCA.\n"
    },
    {
        "paper_id": 2401.04605,
        "authors": "Mustaqim Adamrah, Yos Sunitiyoso",
        "title": "Effect on New Loan Repayment Fine Clause on Bank Jaya Artha's Customer\n  Satisfaction and Recommendation",
        "comments": "11 pages, 10 figures, 6 tables, Published with International Research\n  Journal of Economics and Management Studies (IRJEMS)",
        "journal-ref": "IRJEMS 2(4): 718-728, December 2023. Published by Eternal\n  Scientific Publications",
        "doi": "10.56472/25835238/IRJEMS-V2I4P185",
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  The growing population of older people in Indonesia--the world's fourth-most\npopulous country--makes a larger cake for the pension business, including in\nthe banking sector. PT Bank Jaya Artha is one of the Indonesian banks that\nprovide products and services for people who are set to enter retirement age.\nIn the wake of tight competition in the pension business market, Bank Jaya\nArtha has since 2019 imposed a fine of three times of installments in addition\nto 5% of outstanding debt on customers planning to repay their debt in a\nmission to prevent them from leaving for competitors. While the clause is not\nincluded in loan agreements signed before the implementation, it applies to\npast loan agreements as well. This, in turn, has led to customer complaints.\nThe research is meant to find out how the implementation of the unconsented\nclause has affected customer satisfaction and willingness to recommend the bank\nand what the bank should do to become more customer-centric, according to\ncustomers. Using a design thinking framework, the research collects\nquantitative and qualitative data from the bank's pension customers through\nquestionnaires and forum group discussions. Statistical analysis is utilized on\nquantitative data from questionnaires, and content analysis is utilized on\nqualitative data from questionnaires. A narrative analysis is also used to\nexplain qualitative data from forum group discussions. The result shows that\nthere are problems in the way the bank communicates information to customers,\nparticularly information about the loan repayment fine. Lack of transparency, a\nreactive approach instead of a proactive one, the obscurity of the information,\nand the time the information is delivered have affected customers' satisfaction\ntoward the bank.\n"
    },
    {
        "paper_id": 2401.04696,
        "authors": "Osiris Jorge Parcero, Emiliano Villanueva",
        "title": "World Wine Exports: What Determines the Success of New World Wine\n  Producers?",
        "comments": "Pages 24, 2 tables, 2 charts",
        "journal-ref": null,
        "doi": "10.1515/1542-0485.1376",
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  By using an econometric approach this paper looks at the evolution of the\nworld wine industry in the period 1961-2005. A particular stylized fact is the\nappearance of nontraditional producing and exporting countries of wine from the\nbeginning of the nineties. We show that the success of these new producing and\nexporting countries can be explained by the importance of the demand from\nnon-producing countries with little or no tradition of wine consumption,\nrelative to the world demand. This stylized fact is consistent with a testable\nimplication of the switching cost literature and to the best of our knowledge\nthis is the first time that this implication is tested.\n"
    },
    {
        "paper_id": 2401.04702,
        "authors": "Didier Sornette, Yu Zhang",
        "title": "Scaling Laws And Statistical Properties of The Transaction Flows And\n  Holding Times of Bitcoin",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  We study the temporal evolution of the holding-time distribution of bitcoins\nand find that the average distribution of holding-time is a heavy-tailed power\nlaw extending from one day to over at least $200$ weeks with an exponent\napproximately equal to $0.9$, indicating very long memory effects. We also\nreport significant sample-to-sample variations of the distribution of holding\ntimes, which can be best characterized as multiscaling, with power-law\nexponents varying between $0.3$ and $2.5$ depending on bitcoin price regimes.\nWe document significant differences between the distributions of book-to-market\nand of realized returns, showing that traders obtain far from optimal\nperformance. We also report strong direct qualitative and quantitative evidence\nof the disposition effect in the Bitcoin Blockchain data. Defining\nage-dependent transaction flows as the fraction of bitcoins that are traded at\na given time and that were born (last traded) at some specific earlier time, we\ndocument that the time-averaged transaction flow fraction has a power law\ndependence as a function of age, with an exponent close to $-1.5$, a value\ncompatible with priority queuing theory. We document the existence of\nmultifractality on the measure defined as the normalized number of bitcoins\nexchanged at a given time.\n"
    },
    {
        "paper_id": 2401.04752,
        "authors": "Nestor Gandelman, Osiris Jorge Parcero, Matilde Pereira, Flavia Roldan",
        "title": "Revealed comparative advantages in scientific and technological\n  disciplines in Uruguay",
        "comments": "Pages 48, in Spanish language, 2 tables, 20 figure",
        "journal-ref": null,
        "doi": "10.3989/redc.2022.4.1915",
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  Based on bibliometric information from Scopus for the period 1996-2019, this\ndocument characterizes the evolution of Uruguayan scientific production and\nestablishes the areas in which the country has a revealed comparative advantage\n(RCA). Methodologically, it is proposed that there is a RCA in an area if this\narea has a greater share in national scientific production than the share of\nthe area in world scientific production. The evidence presented considers two\nmeasurements of scientific production (published articles and citations) and\nthree levels of aggregation in the areas (a minor one with 5 large areas, a\nmore detailed one with 27 disciplines and another even more granular with more\nthan 300 disaggregations). Within Health Sciences there is a RCA in Veterinary,\nNursing and Medicine. Within Life Sciences there is a RCA in Agricultural and\nBiological Sciences, Immunology and Microbiology and Biochemistry, Genetics and\nMolecular Biology. In Physical Sciences there is only a RCA in Environmental\nScience and in Social Sciences only in Economics, Econometrics and Finance.\n"
    },
    {
        "paper_id": 2401.0508,
        "authors": "Stephen Boyd and Kasper Johansson and Ronald Kahn and Philipp Schiele\n  and Thomas Schmelzer",
        "title": "Markowitz Portfolio Construction at Seventy",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  More than seventy years ago Harry Markowitz formulated portfolio construction\nas an optimization problem that trades off expected return and risk, defined as\nthe standard deviation of the portfolio returns. Since then the method has been\nextended to include many practical constraints and objective terms, such as\ntransaction cost or leverage limits. Despite several criticisms of Markowitz's\nmethod, for example its sensitivity to poor forecasts of the return statistics,\nit has become the dominant quantitative method for portfolio construction in\npractice. In this article we describe an extension of Markowitz's method that\naddresses many practical effects and gracefully handles the uncertainty\ninherent in return statistics forecasting. Like Markowitz's original\nformulation, the extension is also a convex optimization problem, which can be\nsolved with high reliability and speed.\n"
    },
    {
        "paper_id": 2401.05107,
        "authors": "Adolfo Cristobal Campoamor, Osiris Jorge Parcero",
        "title": "Behind the Eastern-Western European convergence path: the role of\n  geography and trade liberalization",
        "comments": "27 pages and 13 figures",
        "journal-ref": "Annals of Regional Science, Vol. 51, No. 3, 871-891 (2013)",
        "doi": "10.1007/s00168-013-0565-1",
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  This paper proposes a two blocks and three regions economic geography model\nthat can account for the most salient stylized facts experienced by Eastern\nEuropean transition economies during the period 1990 2005. In contrast to the\nexisting literature, which has favored technological explanations, trade\nliberalization is the only driving force. The model correctly predicts that in\nthe first half of the period, trade liberalization led to divergence in GDP per\ncapita, both between the West and the East and within the East. Consistent with\nthe data, in the second half of the period, this process was reversed and\nconvergence became the dominant force.\n"
    },
    {
        "paper_id": 2401.05116,
        "authors": "Jo\\v{z}e P. Damijan, Sandra Damijan, Osiris Jorge Parcero",
        "title": "Is there a size premium for nations?",
        "comments": "Pages 44, 9 tables, 5 figure",
        "journal-ref": null,
        "doi": "10.1007/s13132-022-01021-x",
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  This paper examines whether there is a premium in country size. We study\nwhether there are significant gains from being a small or a large country in\nterms of certain socioeconomic indicators and how large this premium is. Using\npanel data for 200 countries over 50 years, we estimate premia for various\nsizes of nations across a variety of key economic and socioeconomic performance\nindicators. We find that smaller countries are richer, have larger governments,\nand are more prudent in terms of fiscal policies than larger ones. On the other\nhand, smaller countries seem to be subject to higher absolute and per capita\ncosts for the provision of essential public goods, which may lower their\nsocioeconomic performance in terms of health and education. In terms of\neconomic performance, small countries seem to do better than large countries,\ncompensating for smallness by relying on foreign trade and foreign direct\ninvestment. The latter comes at the cost of higher vulnerability to external\nshocks, resulting in higher volatility of growth rates. This paper's findings\noffer essential guidance to policymakers, international organizations, and\nbusiness researchers, especially those assessing a country's economic or\nsocioeconomic performance or potential. The study implies that comparisons with\nmedium-sized or large countries may be of little utility in predicting the\nperformance of small countries.\n"
    },
    {
        "paper_id": 2401.05183,
        "authors": "Nils Braakmann, Barbara Eberth",
        "title": "Can unions impose costs on employers in education strikes? Evidence from\n  pension disputes in UK universities",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The impact of strikes in educational institutions, specifically universities,\non employers remains understudied. This paper investigates the impact of\neducation strikes in UK universities from 2018 to 2022, primarily due to\npension disputes. Using data from the Guardian University Guide and the 2014\nand 2021 Research Excellence Frameworks and leveraging\ndifference-in-differences and regression discontinuity approaches, our findings\nsuggest significant declines in several student related outcomes, such as\nstudent satisfaction, and a more mixed picture for student attainment and\nresearch performance. These results highlight the substantial, albeit indirect,\ncost unions can impose on university employers during strikes.\n"
    },
    {
        "paper_id": 2401.05209,
        "authors": "Marcel Nutz, Johannes Wiesel",
        "title": "On the Martingale Schr\\\"odinger Bridge between Two Distributions",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We study a martingale Schr\\\"odinger bridge problem: given two probability\ndistributions, find their martingale coupling with minimal relative entropy.\nOur main result provides Schr\\\"odinger potentials for this coupling. Namely,\nunder certain conditions, the log-density of the optimal coupling is given by a\ntriplet of real functions representing the marginal and martingale constraints.\nThe potentials are also described as the solution of a dual problem.\n"
    },
    {
        "paper_id": 2401.0521,
        "authors": "Enzo Brox and Daniel Goller",
        "title": "Tournaments, Contestant Heterogeneity and Performance",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Tournaments are frequently used incentive mechanisms to enhance performance.\nIn this paper, we use field data and show that skill disparities among\ncontestants asymmetrically affect the performance of contestants. Skill\ndisparities have detrimental effects on the performance of the lower-ability\ncontestant but positive effects on the performance of the higher-ability\ncontestant. We discuss the potential of different behavioral approaches to\nexplain our findings and discuss the implications of our results for the\noptimal design of contests. Beyond that, our study reveals two important\nempirical results: (a) affirmative action-type policies may help to mitigate\nthe adverse effects on lower-ability contestants, and (b) the skill level of\npotential future contestants in subsequent tournament stages can detrimentally\ninfluence the performance of higher-ability contestants but does not affect the\nlower-ability contestant.\n"
    },
    {
        "paper_id": 2401.05257,
        "authors": "Philippe Bergault and Leandro S\\'anchez-Betancourt",
        "title": "A Mean Field Game between Informed Traders and a Broker",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  We find closed-form solutions to the stochastic game between a broker and a\nmean-field of informed traders. In the finite player game, the informed traders\nobserve a common signal and a private signal. The broker, on the other hand,\nobserves the trading speed of each of his clients and provides liquidity to the\ninformed traders. Each player in the game optimises wealth adjusted by\ninventory penalties. In the mean field version of the game, using a G\\^ateaux\nderivative approach, we characterise the solution to the game with a system of\nforward-backward stochastic differential equations that we solve explicitly. We\nfind that the optimal trading strategy of the broker is linear on his own\ninventory, on the average inventory among informed traders, and on the common\nsignal or the average trading speed of the informed traders. The Nash\nequilibrium we find helps informed traders decide how to use private\ninformation, and helps brokers decide how much of the order flow they should\nexternalise or internalise when facing a large number of clients.\n"
    },
    {
        "paper_id": 2401.05264,
        "authors": "Zhang Chern Lee, Wei Yun Tan, Hoong Khen Koo, Wilson Pang",
        "title": "Comparison of Markowitz Model and Single-Index Model on Portfolio\n  Selection of Malaysian Stocks",
        "comments": "19 pages, 5 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Our article is focused on the application of Markowitz Portfolio Theory and\nthe Single Index Model on 10-year historical monthly return data for 10 stocks\nincluded in FTSE Bursa Malaysia KLCI, which is also our market index, as well\nas a risk-free asset which is the monthly fixed deposit rate. We will calculate\nthe minimum variance portfolio and maximum Sharpe portfolio for both the\nMarkowitz model and Single Index model subject to five different constraints,\nwith the results presented in the form of tables and graphs such that\ncomparisons between the different models and constraints can be made. We hope\nthis article will help provide useful information for future investors who are\ninterested in the Malaysian stock market and would like to construct an\nefficient investment portfolio. Keywords: Markowitz Portfolio Theory, Single\nIndex Model, FTSE Bursa Malaysia KLCI, Efficient Portfolio\n"
    },
    {
        "paper_id": 2401.05337,
        "authors": "Pierre Renucci",
        "title": "Optimal Linear Signal: An Unsupervised Machine Learning Framework to\n  Optimize PnL with Linear Signals",
        "comments": "The code of the model and the empiric strategy are available on my\n  GitHub: Cnernc/OptimalLinearSignal",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/publicdomain/zero/1.0/",
        "abstract": "  This study presents an unsupervised machine learning approach for optimizing\nProfit and Loss (PnL) in quantitative finance. Our algorithm, akin to an\nunsupervised variant of linear regression, maximizes the Sharpe Ratio of PnL\ngenerated from signals constructed linearly from exogenous variables. The\nmethodology employs a linear relationship between exogenous variables and the\ntrading signal, with the objective of maximizing the Sharpe Ratio through\nparameter optimization. Empirical application on an ETF representing U.S.\nTreasury bonds demonstrates the model's effectiveness, supported by\nregularization techniques to mitigate overfitting. The study concludes with\npotential avenues for further development, including generalized time steps and\nenhanced corrective terms.\n"
    },
    {
        "paper_id": 2401.05347,
        "authors": "Mikkel Bennedsen",
        "title": "Income and emotional well-being: Evidence for well-being plateauing\n  around $200,000 per year",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Is emotional well-being monotonically increasing in the level of income or\ndoes it reach a plateau at some income threshold, whereafter additional income\ndoes not contribute to further well-being? Conflicting answers to this question\nhas been suggested in the academic literature. In a recent paper, using an\nincome threshold of $100,000 per year, Killingsworth et al. (2023) appears to\nhave resolved these conflicts, concluding that emotional well-being is\nmonotonically increasing in income for all but the unhappiest individuals. In\nthis paper, we show that this conclusion is sensitive to the placement of the\nincome threshold at which the relationship between emotional well-being and\nincome is allowed to plateau. Using standard econometric methods, we propose a\ndata-driven approach to detect the placement of the threshold. Using this\ndata-driven income threshold, a flat relationship between household income and\nemotional well-being above a threshold around $200,000 per year is found. While\nour analysis relaxes the assumption of a pre-specified income threshold, it\nrelies on a number of other assumptions, which we briefly discuss. We conclude\nthat although the analysis of this paper provides some evidence for well-being\nplateauing around $200,000 per year, more research is needed before any\ndefinite conclusions about the relationship between emotional well-being and\nincome can be drawn.\n"
    },
    {
        "paper_id": 2401.05393,
        "authors": "Roberto Rivera, Guido Rocco, Massimiliano Marzo, Enrico Talin",
        "title": "RIVCoin: an alternative, integrated, CeFi/DeFi-Vaulted Cryptocurrency",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  This whitepaper introduces RIVCoin, a cryptocurrency built on Cosmos, fully\nstabilized by a diversified portfolio of both CeFi and DeFi assets, available\nin a digital, non-custodial wallet called RIV Wallet, that aims to provide\nUsers an easy way to access the cryptocurrency markets, compliant to the\nstrictest AML laws and regulations up to date. The token is a cryptocurrency at\nany time stabilized by a basket of assets: reserves are invested in a portfolio\ncomposed long term by 50% of CeFi assets, comprised of Fixed Income, Equity,\nMutual and Hedge Funds and 50% of diversified strategies focused on digital\nassets, mainly staking and LP farming on the major, battle tested DeFi\nprotocols. The cryptocurrency, as well as the dollar before Bretton Woods, is\nalways fully stabilized by vaulted proof of assets: it is born and managed as a\ndecentralized token, minted by a Decentralized Autonomous Organization, and\nentirely stabilized by assets evaluated by professional independent third\nparties. Users will trade, pool, and exchange the token without any\nintermediary, being able to merge them into a Liquidity Pool whose rewards will\nbe composed by both the trading fees and the liquidity rewards derived from the\nreserve's seigniorage.\n  Users who wish and decide to pool RIVCoin in the Liquidity Pool will receive\nadditional RIVCoin for themselves, and new RIVCoin are minted when the reserves\nincrease in value or in case of purchase of new RIVCoin. The proposed model\nallows for alignment of incentives: decreasing the risk exposure by wealthier\nUsers, but implicitly increasing that of smaller ones to a level perceived by\nthem as still sustainable. Users indirectly benefit from the access to the\nrewards of sophisticated cryptocurrency portfolios hitherto precluded to them,\nwithout this turning into a disadvantage for the wealthy User.\n"
    },
    {
        "paper_id": 2401.05395,
        "authors": "Ruixin Ding and Bowei Chen and James M. Wilson and Zhi Yan and Yufei\n  Huang",
        "title": "SRNI-CAR: A comprehensive dataset for analyzing the Chinese automotive\n  market",
        "comments": null,
        "journal-ref": "Proceedings of 2023 IEEE International Conference on Big Data\n  (BigData), page 3405-3412",
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  The automotive industry plays a critical role in the global economy, and\nparticularly important is the expanding Chinese automobile market due to its\nimmense scale and influence. However, existing automotive sector datasets are\nlimited in their coverage, failing to adequately consider the growing demand\nfor more and diverse variables. This paper aims to bridge this data gap by\nintroducing a comprehensive dataset spanning the years from 2016 to 2022,\nencompassing sales data, online reviews, and a wealth of information related to\nthe Chinese automotive industry. This dataset serves as a valuable resource,\nsignificantly expanding the available data. Its impact extends to various\ndimensions, including improving forecasting accuracy, expanding the scope of\nbusiness applications, informing policy development and regulation, and\nadvancing academic research within the automotive sector. To illustrate the\ndataset's potential applications in both business and academic contexts, we\npresent two application examples. Our developed dataset enhances our\nunderstanding of the Chinese automotive market and offers a valuable tool for\nresearchers, policymakers, and industry stakeholders worldwide.\n"
    },
    {
        "paper_id": 2401.05414,
        "authors": "Xinshuai Dong, Haoyue Dai, Yewen Fan, Songyao Jin, Sathyamoorthy\n  Rajendran, Kun Zhang",
        "title": "On the Three Demons in Causality in Finance: Time Resolution,\n  Nonstationarity, and Latent Factors",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Financial data is generally time series in essence and thus suffers from\nthree fundamental issues: the mismatch in time resolution, the time-varying\nproperty of the distribution - nonstationarity, and causal factors that are\nimportant but unknown/unobserved. In this paper, we follow a causal perspective\nto systematically look into these three demons in finance. Specifically, we\nreexamine these issues in the context of causality, which gives rise to a novel\nand inspiring understanding of how the issues can be addressed. Following this\nperspective, we provide systematic solutions to these problems, which hopefully\nwould serve as a foundation for future research in the area.\n"
    },
    {
        "paper_id": 2401.05417,
        "authors": "Sanaz Behzadi, Mahmonir Bayanati, Hamed Nozari",
        "title": "Multiple-bubble testing in the cryptocurrency market: a case study of\n  bitcoin",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-sa/4.0/",
        "abstract": "  Economic periods and financial crises have highlighted the importance of\nevaluating financial markets to investors and researchers in recent decades.\n"
    },
    {
        "paper_id": 2401.05423,
        "authors": "Victor Ujaldon Garcia",
        "title": "Introduction of L0 norm and application of L1 and C1 norm in the study\n  of time-series",
        "comments": "14 pages 8 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Four markets are considered: Cryptocurrencies / South American exchange rate\n/ Spanish Banking indices and European Indices and studied using TDA\n(Topological Data Analysis) tools. These tools are used to predict and showcase\nboth strengths and weakness of the current TDA tools. In this paper a new tool\n$L0$ norm is defined and complemented with the already existing $C1$ norm.\n"
    },
    {
        "paper_id": 2401.0543,
        "authors": "Zinuo You, Pengju Zhang, Jin Zheng, John Cartlidge",
        "title": "Multi-relational Graph Diffusion Neural Network with Parallel Retention\n  for Stock Trends Classification",
        "comments": "5 pages, 2 figures. Author manuscript accepted for ICASSP 2024 (IEEE\n  International Conference on Acoustics, Speech and Signal Processing)",
        "journal-ref": "49th IEEE International Conference on Acoustics, Speech and Signal\n  Processing (ICASSP), 2024, pp. 6545-6549",
        "doi": "10.1109/ICASSP48485.2024.10447394",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Stock trend classification remains a fundamental yet challenging task, owing\nto the intricate time-evolving dynamics between and within stocks. To tackle\nthese two challenges, we propose a graph-based representation learning approach\naimed at predicting the future movements of multiple stocks. Initially, we\nmodel the complex time-varying relationships between stocks by generating\ndynamic multi-relational stock graphs. This is achieved through a novel edge\ngeneration algorithm that leverages information entropy and signal energy to\nquantify the intensity and directionality of inter-stock relations on each\ntrading day. Then, we further refine these initial graphs through a stochastic\nmulti-relational diffusion process, adaptively learning task-optimal edges.\nSubsequently, we implement a decoupled representation learning scheme with\nparallel retention to obtain the final graph representation. This strategy\nbetter captures the unique temporal features within individual stocks while\nalso capturing the overall structure of the stock graph. Comprehensive\nexperiments conducted on real-world datasets from two US markets (NASDAQ and\nNYSE) and one Chinese market (Shanghai Stock Exchange: SSE) validate the\neffectiveness of our method. Our approach consistently outperforms\nstate-of-the-art baselines in forecasting next trading day stock trends across\nthree test periods spanning seven years. Datasets and code have been released\n(https://github.com/pixelhero98/MGDPR).\n"
    },
    {
        "paper_id": 2401.05441,
        "authors": "Ali Mehrban, Pegah Ahadian",
        "title": "An adaptive network-based approach for advanced forecasting of\n  cryptocurrency values",
        "comments": "11 pages",
        "journal-ref": "International Journal of Computer Science and Information\n  Technology (IJCSIT), 2023",
        "doi": "10.5121/ijcsit.2023.15601",
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  This paper describes an architecture for predicting the price of\ncryptocurrencies for the next seven days using the Adaptive Network Based Fuzzy\nInference System (ANFIS). Historical data of cryptocurrencies and indexes that\nare considered are Bitcoin (BTC), Ethereum (ETH), Bitcoin Dominance (BTC.D),\nand Ethereum Dominance (ETH.D) in a daily timeframe. The methods used to teach\nthe data are hybrid and backpropagation algorithms, as well as grid partition,\nsubtractive clustering, and Fuzzy C-means clustering (FCM) algorithms, which\nare used in data clustering. The architectural performance designed in this\npaper has been compared with different inputs and neural network models in\nterms of statistical evaluation criteria. Finally, the proposed method can\npredict the price of digital currencies in a short time.\n"
    },
    {
        "paper_id": 2401.05447,
        "authors": "Baptiste Lefort, Eric Benhamou, Jean-Jacques Ohana, David Saltiel,\n  Beatrice Guez, Damien Challet",
        "title": "Can ChatGPT Compute Trustworthy Sentiment Scores from Bloomberg Market\n  Wraps?",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  We used a dataset of daily Bloomberg Financial Market Summaries from 2010 to\n2023, reposted on large financial media, to determine how global news headlines\nmay affect stock market movements using ChatGPT and a two-stage prompt\napproach. We document a statistically significant positive correlation between\nthe sentiment score and future equity market returns over short to medium term,\nwhich reverts to a negative correlation over longer horizons. Validation of\nthis correlation pattern across multiple equity markets indicates its\nrobustness across equity regions and resilience to non-linearity, evidenced by\ncomparison of Pearson and Spearman correlations. Finally, we provide an\nestimate of the optimal horizon that strikes a balance between reactivity to\nnew information and correlation.\n"
    },
    {
        "paper_id": 2401.05549,
        "authors": "Yukihiro Tsuzuki",
        "title": "Boundary conditions at infinity for Black-Scholes equations",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We propose numerical procedures for computing the prices of forward\ncontracts, in which the underlying asset price is a Markovian local martingale.\nIf the underlying process is a strict local martingale, multiple solutions\nexist for the corresponding Black-Scholes equations, and the derivative prices\nare characterized as the minimal solutions. Our prices are upper and lower\nbounds obtained using numerical methods on a finite grid under the respective\nboundary conditions. These bounds and the boundary values converge to the exact\nvalue as the underlying price approaches infinity. The proposed procedures are\ndemonstrated through numerical tests.\n"
    },
    {
        "paper_id": 2401.05713,
        "authors": "Tahir Choulli and Emmanuel Lepinette",
        "title": "Super-hedging-pricing formulas and Immediate-Profit arbitrage for market\n  models under random horizon",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
        "abstract": "  In this paper, we consider the discrete-time setting, and the market model\ndescribed by (S,F,T)$. Herein F is the ``public\" flow of information which is\navailable to all agents overtime, S is the discounted price process of\nd-tradable assets, and T is an arbitrary random time whose occurrence might not\nbe observable via F. Thus, we consider the larger flow G which incorporates F\nand makes T an observable random time. This framework covers the credit risk\ntheory setting, the life insurance setting and the setting of employee stock\noption valuation. For the stopped model (S^T,G) and for various vulnerable\nclaims, based on this model, we address the super-hedging pricing valuation\nproblem and its intrinsic Immediate-Profit arbitrage (IP hereafter for short).\nOur first main contribution lies in singling out the impact of change of prior\nand/or information on conditional essential supremum, which is a vital tool in\nsuper-hedging pricing. The second main contribution consists of describing as\nexplicit as possible how the set of super-hedging prices expands under the\nstochasticity of T and its risks, and we address the IP arbitrage for (S^T,G)\nas well. The third main contribution resides in elaborating as explicit as\npossible pricing formulas for vulnerable claims, and singling out the various\ninformational risks in the prices' dynamics.\n"
    },
    {
        "paper_id": 2401.0576,
        "authors": "Khalil Liouane",
        "title": "Follow The Money: Exploring the Key Factors Influencing Investment in\n  African Startups",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-sa/4.0/",
        "abstract": "  The African continent has witnessed a notable surge in entrepreneurial\nactivity, with the number of startups and investments made in the ecosystem\ngrowing significantly in recent years. Against this backdrop, this paper\npresents an in-depth analysis of the critical key factors influencing funding\namounts in African startup deals. A comprehensive analysis of 2,521 startup\ninvestment deals, spanning from January 2019 to March 2023, was conducted using\na combination of statistical and several machine learning techniques. The\nresults of this study highlight a significant gender diversity gap, the\nimportance of professional experience, and the impact of founders' academic\nbackgrounds. The study reveals that human capital, a diversified sector\napproach, and cross-border collaboration strategies are crucial for a robust\nstartup ecosystem. Additionally, we identified the potential positive impact of\n'Y combinators' for African startups, the implications of exit strategies on\ndeal amounts, and the heterogeneity as well as the incongruity of investment\nrounds across the continent. In light of these findings, we propose an\nassortment of policy recommendations aimed at fostering a propitious milieu for\nAfrican entrepreneurial ventures, promoting equitable investment distribution,\nand enhancing cross-border collaboration. By providing a rigorous empirical\nanalysis, this study not only contributes to the existing body of literature\nbut also lays the foundation for future research aimed at promoting investment\nand catalyzing socio-economic development throughout the African continent.\n"
    },
    {
        "paper_id": 2401.05799,
        "authors": "Frank Xing",
        "title": "Designing Heterogeneous LLM Agents for Financial Sentiment Analysis",
        "comments": "15 pages",
        "journal-ref": null,
        "doi": "10.1145/3688399",
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  Large language models (LLMs) have drastically changed the possible ways to\ndesign intelligent systems, shifting the focuses from massive data acquisition\nand new modeling training to human alignment and strategical elicitation of the\nfull potential of existing pre-trained models. This paradigm shift, however, is\nnot fully realized in financial sentiment analysis (FSA), due to the\ndiscriminative nature of this task and a lack of prescriptive knowledge of how\nto leverage generative models in such a context. This study investigates the\neffectiveness of the new paradigm, i.e., using LLMs without fine-tuning for\nFSA. Rooted in Minsky's theory of mind and emotions, a design framework with\nheterogeneous LLM agents is proposed. The framework instantiates specialized\nagents using prior domain knowledge of the types of FSA errors and reasons on\nthe aggregated agent discussions. Comprehensive evaluation on FSA datasets show\nthat the framework yields better accuracies, especially when the discussions\nare substantial. This study contributes to the design foundations and paves new\navenues for LLMs-based FSA. Implications on business and management are also\ndiscussed.\n"
    },
    {
        "paper_id": 2401.05823,
        "authors": "Li Lin",
        "title": "Quantum Probability Theoretic Asset Return Modeling: A Novel\n  Schr\\\"odinger-Like Trading Equation and Multimodal Distribution",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Quantum theory provides a comprehensive framework for quantifying\nuncertainty, often applied in quantum finance to explore the stochastic nature\nof asset returns. This perspective likens returns to microscopic particle\nmotion, governed by quantum probabilities akin to physical laws. However, such\napproaches presuppose specific microscopic quantum effects in return changes, a\npremise criticized for lack of guarantee. This paper diverges by asserting that\nquantum probability is a mathematical extension of classical probability to\ncomplex numbers. It isn't exclusively tied to microscopic quantum phenomena,\nbypassing the need for quantum effects in returns.By directly linking quantum\nprobability's mathematical structure to traders' decisions and market\nbehaviors, it avoids assuming quantum effects for returns and invoking the wave\nfunction. The complex phase of quantum probability, capturing transitions\nbetween long and short decisions while considering information interaction\namong traders, offers an inherent advantage over classical probability in\ncharacterizing the multimodal distribution of asset returns.Utilizing Fourier\ndecomposition, we derive a Schr\\\"odinger-like trading equation, where each term\nexplicitly corresponds to implications of market trading. The equation\nindicates discrete energy levels in financial trading, with returns following a\nnormal distribution at the lowest level. As the market transitions to higher\ntrading levels, a phase shift occurs in the return distribution, leading to\nmultimodality and fat tails. Empirical research on the Chinese stock market\nsupports the existence of energy levels and multimodal distributions derived\nfrom this quantum probability asset returns model.\n"
    },
    {
        "paper_id": 2401.05832,
        "authors": "Dar\\'io Blanco-Fern\\'andez, Stephan Leitner, Alexandra Rausch",
        "title": "Interactions between dynamic team composition and coordination: An\n  agent-based modeling approach",
        "comments": "45 pages, submitted and accepted in the Review of Managerial Science\n  Journal",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  This paper examines the interactions between selected coordination modes and\ndynamic team composition, and their joint effects on task performance under\ndifferent task complexity and individual learning conditions. Prior research\noften treats dynamic team composition as a consequence of suboptimal\norganizational design choices. The emergence of new organizational forms that\nconsciously employ teams that change their composition periodically challenges\nthis perspective. In this paper, we follow the contingency theory and\ncharacterize dynamic team composition as a design choice that interacts with\nother choices such as the coordination mode, and with additional contextual\nfactors such as individual learning and task complexity. We employ an\nagent-based modeling approach based on the NK framework, which includes a\nreinforcement learning mechanism, a recurring team formation mechanism based on\nsignaling, and three different coordination modes. Our results suggest that by\nimplementing lateral communication or sequential decision-making, teams may\nexploit the benefits of dynamic composition more than if decision-making is\nfully autonomous. The choice of a proper coordination mode, however, is partly\nmoderated by the task complexity and individual learning. Additionally, we show\nthat only a coordination mode based on lateral communication may prevent the\nnegative effects of individual learning.\n"
    },
    {
        "paper_id": 2401.06134,
        "authors": "Shengwen Shi (1), Jian'an Zhang (2) ((1) School of Economics, Shanghai\n  University of Finance and Economics, Shanghai, China, (2) School of\n  Economics, Shanghai University of Finance and Economics, Shanghai, China)",
        "title": "Synergy or Rivalry? Glimpses of Regional Modernization and Public\n  Service Equalization: A Case Study from China",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  For most developing countries, increasing the equalization of basic public\nservices is widely recognized as an effective channel to improve people's sense\nof contentment. However, for many emerging economies like China, the\nequalization level of basic public services may often be neglected in the\ntrade-off between the speed and quality of development. Taking the Yangtze\nRiver Delta region of China as an example, this paper first adopts the coupling\ncoordination degree model to explore current status of basic public services in\nthis region, and then uses Moran's I index to study the overall equalization\nlevel of development there. Moreover, this paper uses the Theil index to\nanalyze the main reasons for the spatial differences in the level of public\nservices, followed by the AF method to accurately identify the exact weaknesses\nof the 40 counties of 10 cities with the weakest level of basic public service\ndevelopment. Based on this, this paper provides targeted optimization\ninitiatives and continues to explore the factors affecting the growth of the\nlevel of public service equalization through the convergence model, verifying\nthe convergence trend of the degree of public service equalization, and\nultimately providing practical policy recommendations for promoting the\nequalization of basic public services.\n"
    },
    {
        "paper_id": 2401.06139,
        "authors": "Bohan Ma, Yushan Xue, Yuan Lu, Jing Chen",
        "title": "Stockformer: A Price-Volume Factor Stock Selection Model Based on\n  Wavelet Transform and Multi-Task Self-Attention Networks",
        "comments": "Currently under consideration for publication in the Expert Systems\n  With Applications",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  As the Chinese stock market continues to evolve and its market structure\ngrows increasingly complex, traditional quantitative trading methods are facing\nescalating challenges. Particularly, due to policy uncertainty and the frequent\nmarket fluctuations triggered by sudden economic events, existing models often\nstruggle to accurately predict market dynamics. To address these challenges,\nthis paper introduces Stockformer, a price-volume factor stock selection model\nthat integrates wavelet transformation and a multitask self-attention network,\naimed at enhancing responsiveness and predictive accuracy regarding market\ninstabilities. Through discrete wavelet transform, Stockformer decomposes stock\nreturns into high and low frequencies, meticulously capturing long-term market\ntrends and short-term fluctuations, including abrupt events. Moreover, the\nmodel incorporates a Dual-Frequency Spatiotemporal Encoder and graph embedding\ntechniques to effectively capture complex temporal and spatial relationships\namong stocks. Employing a multitask learning strategy, it simultaneously\npredicts stock returns and directional trends. Experimental results show that\nStockformer outperforms existing advanced methods on multiple real stock market\ndatasets. In strategy backtesting, Stockformer consistently demonstrates\nexceptional stability and reliability across market conditions-whether rising,\nfalling, or fluctuating-particularly maintaining high performance during\ndownturns or volatile periods, indicating a high adaptability to market\nfluctuations. To foster innovation and collaboration in the financial analysis\nsector, the Stockformer model's code has been open-sourced and is available on\nthe GitHub repository: https://github.com/Eric991005/Multitask-Stockformer.\n"
    },
    {
        "paper_id": 2401.06141,
        "authors": "Jos\\'e Miguel Flores-Contr\\'o and S\\'everine Arnold",
        "title": "The Role of Direct Capital Cash Transfers Towards Poverty and Extreme\n  Poverty Alleviation -- An Omega Risk Process",
        "comments": "47 pages, 14 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Trapping refers to the event when a household falls into the area of poverty.\nHouseholds that live or fall into the area of poverty are said to be in a\npoverty trap, where a poverty trap is a state of poverty from which it is\ndifficult to escape without external help. Similarly, extreme poverty is\nconsidered as the most severe type of poverty, in which households experience\nsevere deprivation of basic human needs. In this article, we consider an Omega\nrisk process with deterministic growth and a multiplicative jump (collapse)\nstructure to model the capital of a household. It is assumed that, when a\nhousehold's capital level is above a certain capital barrier level that\ndetermines a household's eligibility for a capital cash transfer programme, its\ncapital grows exponentially. As soon as its capital falls below the capital\nbarrier level, the capital dynamics incorporate external support in the form of\ndirect transfers (capital cash transfers) provided by donors or governments.\nOtherwise, once trapped, the capital grows only due to the capital cash\ntransfers. Under this model, we first derive closed-form expressions for the\ntrapping probability and then do the same for the probability of extreme\npoverty, which only depends on the current value of the capital given by some\nextreme poverty rate function. Numerical examples illustrate the role of\ncapital cash transfers on poverty and extreme poverty dynamics.\n"
    },
    {
        "paper_id": 2401.06142,
        "authors": "Pierre Gosselin (IF), A\\\"ileen Lotz",
        "title": "A Statistical Field Perspective on Capital Allocation and Accumulation:\n  Individual dynamics",
        "comments": "arXiv admin note: substantial text overlap with arXiv:2312.16173,\n  arXiv:2205.03087",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We have shown, in a series of articles, that a classical description of a\nlarge number of economic agents can be replaced by a statistical fields\nformalism. To better understand the accumulation and allocation of capital\namong different sectors, the present paper applies this statistical fields\ndescription to a large number of heterogeneous agents divided into two groups.\nThe first group is composed of a large number of firms in different sectors\nthat collectively own the entire physical capital. The second group, investors,\nholds the entire financial capital and allocates it between firms across\nsectors according to investment preferences, expected returns, and stock prices\nvariations on financial markets. In return, firms pay dividends to their\ninvestors. Financial capital is thus a function of dividends and stock\nvaluations, whereas physical capital is a function of the total capital\nallocated by the financial sector. Whereas our previous work focused on the\nbackground fields that describe potential long-term equilibria, here we compute\nthe transition functions of individual agents and study their probabilistic\ndynamics in the background field, as a function of their initial state. We show\nthat capital accumulation depends on various factors. The probability\nassociated with each firm's trajectories is the result of several contradictory\neffects: the firm tends to shift towards sectors with the greatest long-term\nreturn, but must take into account the impact of its shift on its\nattractiveness for investors throughout its trajectory. Since this trajectory\ndepends largely on the average capital of transition sectors, a firm's\nattractiveness during its relocation depends on the relative level of capital\nin those sectors. Thus, an under-capitalized firm reaching a high-capital\nsector will experience a loss of attractiveness, and subsequently, in\ninvestors. Moreover, the firm must also consider the effects of competition in\nthe intermediate sectors. An under-capitalized firm will tend to be ousted out\ntowards sectors with lower average capital, while an over-capitalized firm will\ntend to shift towards higher averagecapital sectors. For investors, capital\nallocation depends on their short and long-term returns. These returns are not\nindependent: in the short-term, returns are composed of both the firm's\ndividends and the increase in its stock prices. In the long-term, returns are\nbased on the firm's growth expectations, but also, indirectly, on expectations\nof higher stock prices. Investors' capital allocation directly depends on the\nvolatility of stock prices and {\\ldots}rms'dividends. Investors will tend to\nreallocate their capital to maximize their short and long-term returns. The\nhigher their level of capital, the stronger the reallocation will be.\n"
    },
    {
        "paper_id": 2401.06163,
        "authors": "Marcelo S. Tedesco and Francisco Javier Ramos Soria",
        "title": "Grassroots Innovation Actors: Their Role and Positioning in Economic\n  Ecosystems -- A Comparative Study Through Complex Network Analysis",
        "comments": "21 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This study offers an examination of grassroots innovation actors and their\nintegration within larger economic ecosystems. Through a comparative analysis\nin Oaxaca, Mexico; La Plata, Argentina; and Araucania, Chile, this research\nsheds light on the vital role that grassroots innovation plays in broader\neconomic ecosystems. Using Complex Network Analysis and the TE-SER model, the\nstudy unveils how these actors interact, collaborate, and influence major\neconomic ecosystems in the context of complex social challenges. The findings\nhighlight that actors from the grassroots innovation ecosystem make up a\nsignificant portion of the larger innovation-driven entrepreneurial economic\necosystem, accounting for between 20% and 30% in all three cases and are\nstrategically positioned within the ecosystem's structural network.\nAdditionally, this study emphasizes the potential for greater integration of\ngrassroots innovation actors to leverage resources and foster socio-economic\ndevelopment. The research concludes by advocating for further studies in\nsimilar socio-economic contexts to enhance our understanding of integration\ndynamics and mutual benefits between grassroots innovation ecosystems and other\nlarger economic systems.\n"
    },
    {
        "paper_id": 2401.06164,
        "authors": "Lezhi Li, Ting-Yu Chang, Hai Wang",
        "title": "Multimodal Gen-AI for Fundamental Investment Research",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This report outlines a transformative initiative in the financial investment\nindustry, where the conventional decision-making process, laden with\nlabor-intensive tasks such as sifting through voluminous documents, is being\nreimagined. Leveraging language models, our experiments aim to automate\ninformation summarization and investment idea generation. We seek to evaluate\nthe effectiveness of fine-tuning methods on a base model (Llama2) to achieve\nspecific application-level goals, including providing insights into the impact\nof events on companies and sectors, understanding market condition\nrelationships, generating investor-aligned investment ideas, and formatting\nresults with stock recommendations and detailed explanations. Through\nstate-of-the-art generative modeling techniques, the ultimate objective is to\ndevelop an AI agent prototype, liberating human investors from repetitive tasks\nand allowing a focus on high-level strategic thinking. The project encompasses\na diverse corpus dataset, including research reports, investment memos, market\nnews, and extensive time-series market data. We conducted three experiments\napplying unsupervised and supervised LoRA fine-tuning on the llama2_7b_hf_chat\nas the base model, as well as instruction fine-tuning on the GPT3.5 model.\nStatistical and human evaluations both show that the fine-tuned versions\nperform better in solving text modeling, summarization, reasoning, and finance\ndomain questions, demonstrating a pivotal step towards enhancing\ndecision-making processes in the financial domain. Code implementation for the\nproject can be found on GitHub: https://github.com/Firenze11/finance_lm.\n"
    },
    {
        "paper_id": 2401.06172,
        "authors": "Yue Chen, Xingyi Andrew, Salintip Supasanya",
        "title": "CRISIS ALERT:Forecasting Stock Market Crisis Events Using Machine\n  Learning Methods",
        "comments": "14 pages, 9 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Historically, the economic recession often came abruptly and disastrously.\nFor instance, during the 2008 financial crisis, the SP 500 fell 46 percent from\nOctober 2007 to March 2009. If we could detect the signals of the crisis\nearlier, we could have taken preventive measures. Therefore, driven by such\nmotivation, we use advanced machine learning techniques, including Random\nForest and Extreme Gradient Boosting, to predict any potential market crashes\nmainly in the US market. Also, we would like to compare the performance of\nthese methods and examine which model is better for forecasting US stock market\ncrashes. We apply our models on the daily financial market data, which tend to\nbe more responsive with higher reporting frequencies. We consider 75\nexplanatory variables, including general US stock market indexes, SP 500 sector\nindexes, as well as market indicators that can be used for the purpose of\ncrisis prediction. Finally, we conclude, with selected classification metrics,\nthat the Extreme Gradient Boosting method performs the best in predicting US\nstock market crisis events.\n"
    },
    {
        "paper_id": 2401.06179,
        "authors": "Sina Montazeri, Akram Mirzaeinia, Haseebullah Jumakhan, Amir\n  Mirzaeinia",
        "title": "CNN-DRL for Scalable Actions in Finance",
        "comments": "10th Annual Conf. on Computational Science & Computational\n  Intelligence",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
        "abstract": "  The published MLP-based DRL in finance has difficulties in learning the\ndynamics of the environment when the action scale increases. If the buying and\nselling increase to one thousand shares, the MLP agent will not be able to\neffectively adapt to the environment. To address this, we designed a CNN agent\nthat concatenates the data from the last ninety days of the daily feature\nvector to create the CNN input matrix. Our extensive experiments demonstrate\nthat the MLP-based agent experiences a loss corresponding to the initial\nenvironment setup, while our designed CNN remains stable, effectively learns\nthe environment, and leads to an increase in rewards.\n"
    },
    {
        "paper_id": 2401.06249,
        "authors": "Alessio Brini and Giacomo Toscano",
        "title": "SpotV2Net: Multivariate Intraday Spot Volatility Forecasting via\n  Vol-of-Vol-Informed Graph Attention Networks",
        "comments": "34 pages, 5 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper introduces SpotV2Net, a multivariate intraday spot volatility\nforecasting model based on a Graph Attention Network architecture. SpotV2Net\nrepresents assets as nodes within a graph and includes non-parametric\nhigh-frequency Fourier estimates of the spot volatility and co-volatility as\nnode features. Further, it incorporates Fourier estimates of the spot\nvolatility of volatility and co-volatility of volatility as features for node\nedges, to capture spillover effects. We test the forecasting accuracy of\nSpotV2Net in an extensive empirical exercise, conducted with the components of\nthe Dow Jones Industrial Average index. The results we obtain suggest that\nSpotV2Net yields statistically significant gains in forecasting accuracy, for\nboth single-step and multi-step forecasts, compared to a panel heterogenous\nauto-regressive model and alternative machine-learning models. To interpret the\nforecasts produced by SpotV2Net, we employ GNNExplainer\n\\citep{ying2019gnnexplainer}, a model-agnostic interpretability tool, and\nthereby uncover subgraphs that are critical to a node's predictions.\n"
    },
    {
        "paper_id": 2401.06457,
        "authors": "Ruimin Song, Tiantian Zhao, Chunhui Zhou",
        "title": "Analysis of the Impact of Central bank Digital Currency on the Demand\n  for Transactional Currency",
        "comments": "Central bank digital currencies; transactional money demand; ARDL\n  model. arXiv admin note: text overlap with arXiv:2310.07326",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper takes the development of Central bank digital currencies as a\nperspective, introduces it into the Baumol-Tobin money demand theoretical\nframework, establishes the transactional money demand model under Central bank\nDigital Currency, and qualitatively analyzes the influence mechanism of Central\nbank digital currencies on transactional money demand; meanwhile, quarterly\ndata from 2010-2022 are selected to test the relationship between Central bank\ndigital currencies and transactional money demand through the ARDL model. The\nlong-run equilibrium and short-run dynamics between the demand for Central bank\ndigital currencies and transactional currency are examined by ARDL model. The\nempirical results show that the issuance and circulation of Central bank\ndigital currencies will reduce the demand for transactional money. Based on the\ntheoretical analysis and empirical test, this paper proposes that China should\nexplore a more effective Currency policy in the context of Central bank digital\ncurrencies while promoting the development of Central bank digital currencies\nin a prudent manner in the future.\n"
    },
    {
        "paper_id": 2401.06724,
        "authors": "Mohammed Salek, Damien Challet, Ioane Muni Toke",
        "title": "Equity auction dynamics: latent liquidity models with activity\n  acceleration",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  Equity auctions display several distinctive characteristics in contrast to\ncontinuous trading. As the auction time approaches, the rate of events\naccelerates causing a substantial liquidity buildup around the indicative\nprice. This, in turn, results in a reduced price impact and decreased\nvolatility of the indicative price. In this study, we adapt the latent/revealed\norder book framework to the specifics of equity auctions. We provide precise\nmeasurements of the model parameters, including order submissions,\ncancellations, and diffusion rates. Our setup allows us to describe the full\ndynamics of the average order book during closing auctions in Euronext Paris.\nThese findings support the relevance of the latent liquidity framework in\ndescribing limit order book dynamics. Lastly, we analyze the factors\ncontributing to a sub-diffusive indicative price and demonstrate the absence of\nindicative price predictability.\n"
    },
    {
        "paper_id": 2401.0674,
        "authors": "Emmanuil H. Georgoulis, Antonis Papapantoleon, Costas Smaragdakis",
        "title": "A deep implicit-explicit minimizing movement method for option pricing\n  in jump-diffusion models",
        "comments": "16 pages, 11 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  We develop a novel deep learning approach for pricing European basket options\nwritten on assets that follow jump-diffusion dynamics. The option pricing\nproblem is formulated as a partial integro-differential equation, which is\napproximated via a new implicit-explicit minimizing movement time-stepping\napproach, involving approximation by deep, residual-type Artificial Neural\nNetworks (ANNs) for each time step. The integral operator is discretized via\ntwo different approaches: a) a sparse-grid Gauss--Hermite approximation\nfollowing localised coordinate axes arising from singular value decompositions,\nand b) an ANN-based high-dimensional special-purpose quadrature rule.\nCrucially, the proposed ANN is constructed to ensure the asymptotic behavior of\nthe solution for large values of the underlyings and also leads to consistent\noutputs with respect to a priori known qualitative properties of the solution.\nThe performance and robustness with respect to the dimension of the methods are\nassessed in a series of numerical experiments involving the Merton\njump-diffusion model.\n"
    },
    {
        "paper_id": 2401.06835,
        "authors": "Hannes Wallimann",
        "title": "Austria's KlimaTicket: Assessing the short-term impact of a cheap\n  nationwide travel pass on demand",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Measures to reduce transport-related greenhouse gas emissions are of great\nimportance to policy-makers. A recent example is the nationwide KlimaTicket in\nAustria, a country with a relatively high share of transport-related emissions.\nThe cheap yearly season ticket introduced in October 2021 allows unlimited\naccess to Austria's public transport network. Using the synthetic control and\nsynthetic difference-in-differences methods, I assess the causal effect of this\npolicy on public transport demand by constructing a data-driven counterfactual\nout of European railway companies to mimic the number of passengers of the\nAustrian Federal Railways without the KlimaTicket. The results indicate public\ntransport demand grew slightly faster in Austria, i.e., 3.3 or 6.8 percentage\npoints, depending on the method, than it would have in the absence of the\nKlimaTicket. However, the growth effect after the COVID-19 pandemic appears\nonly statistically significant when applying the synthetic control method, and\nthe positive effect on public transport demand growth disappears in 2022.\n"
    },
    {
        "paper_id": 2401.06876,
        "authors": "Alessandro V. M. Oliveira, Thiago Caliari, Rodolfo R. Narcizo",
        "title": "An empirical model of fleet modernization: on the relationship between\n  market concentration and innovation adoption in the Brazilian airline\n  industry",
        "comments": null,
        "journal-ref": "Research in Transportation Business & Management, 43, 100704\n  (2022)",
        "doi": "10.1016/j.rtbm.2021.100704",
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  The modernization of an airline's fleet can reduce its operating costs,\nimprove the perceived quality of service offered to passengers, and mitigate\nemissions. The present paper investigates the market incentives that airlines\nhave to adopt technological innovation from manufacturers by acquiring new\ngeneration aircraft. We develop an econometric model of fleet modernization in\nthe Brazilian commercial aviation over two decades. We examine the hypothesis\nof an inverted-U relationship between market concentration and fleet\nmodernization and find evidence that both the extremes of competition and\nconcentration may inhibit innovation adoption by carriers. We find limited\nevidence associating either hubbing activity or low-cost carriers with the more\nintense introduction of new types of aircraft models and variants in the\nindustry. Finally, our results suggest that energy cost rises may provoke\nboosts in fleet modernization in the long term, with carriers possibly\ntargeting more eco-efficient operations up to two years after an upsurge in\nfuel price.\n"
    },
    {
        "paper_id": 2401.07075,
        "authors": "Patrick Rehill and Nicholas Biddle",
        "title": "Heterogeneous treatment effect estimation with high-dimensional data in\n  public policy evaluation -- an application to the conditioning of cash\n  transfers in Morocco using causal machine learning",
        "comments": "24 pages, 8 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Causal machine learning methods can be used to search for treatment effect\nheterogeneity in high-dimensional datasets even where we lack a strong enough\ntheoretical framework to select variables or make parametric assumptions about\ndata. This paper uses causal machine learning methods to estimate heterogeneous\ntreatment effects in the case of an experimental study carried out in Morocco\nwhich evaluated the effect of conditionalizing a cash transfer program on\nseveral outcomes including maths test scores which is the focus of this work.\nWe explore treatment effects across a dataset of 1936 pre-treatment variables.\nFor the most part, heterogeneity is modelled by two different factors,\nparticipation in education (at the baseline) and more general measures of\npoverty. Those who are more disadvantaged at the baseline benefit less from any\ntreatment. While conditioning generally has a negative effect this more\ndisadvantaged group is also hurt more by conditioning. The second purpose of\nthis paper is to demonstrate and reflect upon a causal machine learning\napproach to policy evaluation. We propose a novel causal tree method for\ninterpretable modelling of causal effects and reflect on the difficulty of\nexplaining atheoretical results.\n"
    },
    {
        "paper_id": 2401.07183,
        "authors": "Huisheng Wang, H. Vicky Zhao",
        "title": "Optimal Investment with Herd Behaviour Using Rational Decision\n  Decomposition",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper, we study the optimal investment problem considering the herd\nbehaviour between two agents, including one leading expert and one following\nagent whose decisions are influenced by those of the leading expert. In the\nobjective functional of the optimal investment problem, we introduce the\naverage deviation term to measure the distance between the two agents'\ndecisions and use the variational method to find its analytical solution. To\ntheoretically analyze the impact of the following agent's herd behaviour on\nhis/her decision, we decompose his/her optimal decision into a convex linear\ncombination of the two agents' rational decisions, which we call the rational\ndecision decomposition. Furthermore, we define the weight function in the\nrational decision decomposition as the following agent's investment opinion to\nmeasure the preference of his/her own rational decision over that of the\nleading expert. We use the investment opinion to quantitatively analyze the\nimpact of the herd behaviour, the following agent's initial wealth, the excess\nreturn, and the volatility of the risky asset on the optimal decision. We\nvalidate our analyses through numerical experiments on real stock data. This\nstudy is crucial to understanding investors' herd behaviour in decision-making\nand designing effective mechanisms to guide their decisions.\n"
    },
    {
        "paper_id": 2401.07345,
        "authors": "Jeongbin Kim, Matthew Kovach, Kyu-Min Lee, Euncheol Shin, Hector\n  Tzavellas",
        "title": "Learning to be Homo Economicus: Can an LLM Learn Preferences from Choice",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper explores the use of Large Language Models (LLMs) as decision aids,\nwith a focus on their ability to learn preferences and provide personalized\nrecommendations. To establish a baseline, we replicate standard economic\nexperiments on choice under risk (Choi et al., 2007) with GPT, one of the most\nprominent LLMs, prompted to respond as (i) a human decision maker or (ii) a\nrecommendation system for customers. With these baselines established, GPT is\nprovided with a sample set of choices and prompted to make recommendations\nbased on the provided data. From the data generated by GPT, we identify its\n(revealed) preferences and explore its ability to learn from data. Our analysis\nyields three results. First, GPT's choices are consistent with (expected)\nutility maximization theory. Second, GPT can align its recommendations with\npeople's risk aversion, by recommending less risky portfolios to more\nrisk-averse decision makers, highlighting GPT's potential as a personalized\ndecision aid. Third, however, GPT demonstrates limited alignment when it comes\nto disappointment aversion.\n"
    },
    {
        "paper_id": 2401.07483,
        "authors": "Partha Sen, Sumana Sen",
        "title": "Graph database while computationally efficient filters out quickly the\n  ESG integrated equities in investment management",
        "comments": "10 pages, 17 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Design/methodology/approach This research evaluated the databases of SQL,\nNo-SQL and graph databases to compare and contrast efficiency and performance.\nTo perform this experiment the data were collected from multiple sources\nincluding stock price and financial news. Python is used as an interface to\nconnect and query databases (to create database structures according to the\nfeed file structure, to load data into tables, objects, to read data , to\nconnect PostgreSQL, ElasticSearch, Neo4j. Purpose Modern applications of LLM\n(Large language model) including RAG (Retrieval Augmented Generation) with\nMachine Learning, deep learning, NLP (natural language processing) or Decision\nAnalytics are computationally expensive. Finding a better option to consume\nless resources and time to get the result. Findings The Graph database of ESG\n(Environmental, Social and Governance) is comparatively better and can be\nconsidered for extended analytics to integrate ESG in business and investment.\nPractical implications A graph ML with a RAG architecture model can be\nintroduced as a new framework with less computationally expensive LLM\napplication in the equity filtering process for portfolio management.\nOriginality/value Filtering out selective stocks out of two thousand or more\nlisted companies in any stock exchange for active investment, consuming less\nresource consumption especially memory and energy to integrate artificial\nintelligence and ESG in business and investment.\n"
    },
    {
        "paper_id": 2401.07682,
        "authors": "Samuel Vandak, Geoffrey Goodell",
        "title": "Cash and Card Acceptance in Retail Payments: Motivations and Factors",
        "comments": "34 pages, 19 figures, 5 tables",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The landscape of payment methods in retail is a complex and evolving area.\nVendors are motivated to conduct an appropriate analysis to decide what payment\nmethods to accept out of a vast range of options. Many factors are included in\nthis decision process, some qualitative and some quantitative. The following\nresearch project investigates vendors' acceptance of cards and cash from\nvarious viewpoints, all chosen to represent a novel perspective, including the\nbarriers and preferences for each and correlations with external demographic\nfactors. We observe that lower interchange fees, limited in this instance by\nthe regulatory framework, play a crucial role in facilitating merchants'\nacceptance of card payments. The regulatory constraints on interchange fees\ncreate a favorable cost structure for merchants, making card payment adoption\nfinancially feasible. However, additional factors like technological readiness\nand consumer preferences might also play a significant role in their\ndecision-making process. We also note that aggregate Merchant Service Providers\n(MSPs) have positively impacted the payment landscape by offering more\ncompetitive fee rates, particularly beneficial for small merchants and\nentrepreneurs. However, associated risks, such as account freezes or abrupt\nterminations, pose challenges and often lack transparency. Last, the\nquantitative analysis of the relationship between demographic variables and\nacceptance of payment types is presented. This analysis combines the current\nlandscape of payment acceptance in the UK with data from the most recent census\nfrom 2021. We show that the unemployment rates shape card and cash acceptance,\nage affects contactless preference, and work-from-home impacts credit card\npreference.\n"
    },
    {
        "paper_id": 2401.07689,
        "authors": "Matthias Hafner and Helmut Dietl",
        "title": "Impermanent Loss Conditions: An Analysis of Decentralized Exchange\n  Platforms",
        "comments": "This paper was presented at the CfC 2024 Academic Track Conference",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Decentralized exchanges are widely used platforms for trading crypto assets.\nThe most common types work with automated market makers (AMM), allowing traders\nto exchange assets without needing to find matching counterparties. Thereby,\ntraders exchange against asset reserves managed by smart contracts. These\nassets are provided by liquidity providers in exchange for a fee. Static\nanalysis shows that small price changes in one of the assets can result in\nlosses for liquidity providers. Despite the success of AMMs, it is claimed that\nliquidity providers often suffer losses. However, the literature does not\nadequately consider the dynamic effects of fees over time. Therefore, we\ninvestigate the impermanent loss problem in a dynamic setting using Monte Carlo\nsimulations. Our findings indicate that price changes do not necessarily lead\nto losses. Fees paid by traders and arbitrageurs are equally important. In this\nrespect, we can show that an arbitrage-friendly environment benefits the\nliquidity provider. Thus, we suggest that AMM developers should promote an\narbitrage-friendly environment rather than trying to prevent arbitrage.\n"
    },
    {
        "paper_id": 2401.07728,
        "authors": "Dorinel Bastide (LaMME), St\\'ephane Cr\\'epey (LPSM)",
        "title": "Provisions and Economic Capital for Credit Losses",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Based on supermodularity ordering properties, we show that convex risk\nmeasures of credit losses are nondecreasing w.r.t. credit-credit and, in a\nwrong-way risk setup, credit-market, covariances of elliptically distributed\nlatent factors. These results support the use of such setups for computing\ncredit provisions and economic capital or for conducting stress test exercises\nand risk management analysis.\n"
    },
    {
        "paper_id": 2401.08013,
        "authors": "Jiayang Li and Qianni Wang and Liyang Feng and Jun Xie and Yu Marco\n  Nie",
        "title": "A Day-to-Day Dynamical Approach to the Most Likely User Equilibrium\n  Problem",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The lack of a unique user equilibrium (UE) route flow in traffic assignment\nhas posed a significant challenge to many transportation applications. The\nmaximum-entropy principle, which advocates for the consistent selection of the\nmost likely solution as a representative, is often used to address the\nchallenge. Built on a recently proposed day-to-day (DTD) discrete-time\ndynamical model called cumulative logit (CULO), this study provides a new\nbehavioral underpinning for the maximum-entropy UE (MEUE) route flow. It has\nbeen proven that CULO can reach a UE state without presuming travelers are\nperfectly rational. Here, we further establish that CULO always converges to\nthe MEUE route flow if (i) travelers have zero prior information about routes\nand thus are forced to give all routes an equal choice probability, or (ii) all\ntravelers gather information from the same source such that the so-called\ngeneral proportionality condition is satisfied. Thus, CULO may be used as a\npractical solution algorithm for the MEUE problem. To put this idea into\npractice, we propose to eliminate the route enumeration requirement of the\noriginal CULO model through an iterative route discovery scheme. We also\nexamine the discrete-time versions of four popular continuous-time dynamical\nmodels and compare them to CULO. The analysis shows that the replicator dynamic\nis the only one that has the potential to reach the MEUE solution with some\nregularity. The analytical results are confirmed through numerical experiments.\n"
    },
    {
        "paper_id": 2401.08064,
        "authors": "Scott E. Allen, Ren\\'e F. Kizilcec, A. David Redish",
        "title": "A new model of trust based on neural information processing",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
        "abstract": "  More than 30 years of research has firmly established the vital role of trust\nin human organizations and relationships, but the underlying mechanisms by\nwhich people build, lose, and rebuild trust remains incompletely understood. We\npropose a mechanistic model of trust that is grounded in the modern\nneuroscience of decision making. Since trust requires anticipating the future\nactions of others, any mechanistic model must be built upon up-to-date theories\non how the brain learns, represents, and processes information about the future\nwithin its decision-making systems. Contemporary neuroscience has revealed that\ndecision making arises from multiple parallel systems that perform distinct,\ncomplementary information processing. Each system represents information in\ndifferent forms, and therefore learns via different mechanisms. When an act of\ntrust is reciprocated or violated, this provides new information that can be\nused to anticipate future actions. The taxonomy of neural information\nrepresentations that is the basis for the system boundaries between neural\ndecision-making systems provides a taxonomy for categorizing different forms of\ntrust and generating mechanistic predictions about how these forms of trust are\nlearned and manifested in human behavior. Three key predictions arising from\nour model are (1) strategic risk-taking can reveal how to best proceed in a\nrelationship, (2) human organizations and environments can be intentionally\ndesigned to encourage trust among their members, and (3) violations of trust\nneed not always degrade trust, but can also provide opportunities to build\ntrust.\n"
    },
    {
        "paper_id": 2401.08077,
        "authors": "Shubham Singh, Mayur Bhat",
        "title": "Transformer-based approach for Ethereum Price Prediction Using\n  Crosscurrency correlation and Sentiment Analysis",
        "comments": "12 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  The research delves into the capabilities of a transformer-based neural\nnetwork for Ethereum cryptocurrency price forecasting. The experiment runs\naround the hypothesis that cryptocurrency prices are strongly correlated with\nother cryptocurrencies and the sentiments around the cryptocurrency. The model\nemploys a transformer architecture for several setups from single-feature\nscenarios to complex configurations incorporating volume, sentiment, and\ncorrelated cryptocurrency prices. Despite a smaller dataset and less complex\narchitecture, the transformer model surpasses ANN and MLP counterparts on some\nparameters. The conclusion presents a hypothesis on the illusion of causality\nin cryptocurrency price movements driven by sentiments.\n"
    },
    {
        "paper_id": 2401.08093,
        "authors": "Ce Wang",
        "title": "A Two-Step Longstaff Schwartz Monte Carlo Approach to Game Option\n  Pricing",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  We proposed a two-step Longstaff Schwartz Monte Carlo (LSMC) method with two\nregression models fitted at each time step to price game options. Although the\noriginal LSMC can be used to price game options with an enlarged range of path\nin regression and a modified cashflow updating rule, we identified a drawback\nof such approach, which motivated us to propose our approach. We implemented\nnumerical examples with benchmarks using binomial tree and numerical PDE, and\nit showed that our method produces more reliable results comparing to the\noriginal LSMC.\n"
    },
    {
        "paper_id": 2401.08094,
        "authors": "Jingyi Cao, Dongchen Li, Virginia R. Young, and Bin Zou",
        "title": "Optimal Insurance to Maximize Exponential Utility when Premium is\n  Computed by a Convex Functional",
        "comments": "12 pages, 3 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  We find the optimal indemnity to maximize the expected utility of terminal\nwealth of a buyer of insurance whose preferences are modeled by an exponential\nutility. The insurance premium is computed by a convex functional. We obtain a\nnecessary condition for the optimal indemnity; then, because the candidate\noptimal indemnity is given implicitly, we use that necessary condition to\ndevelop a numerical algorithm to compute it. We prove that the numerical\nalgorithm converges to a unique indemnity that, indeed, equals the optimal\npolicy. We also illustrate our results with numerical examples.\n"
    },
    {
        "paper_id": 2401.08251,
        "authors": "Alberto Pliego Marug\\'an, Fausto Pedro Garc\\'ia M\\'arquez and Jes\\'us\n  Mar\\'ia Pinar P\\'erez",
        "title": "A techno-economic model for avoiding conflicts of interest between\n  owners of offshore wind farms and maintenance suppliers",
        "comments": "Published in Renewable and Sustainable Energy Reviews (ELSEVIER) 10\n  July 2022. DOI: https://doi.org/10.1016/j.rser.2022.112753 Cite as:\n  Marug\\'an, A. P., M\\'arquez, F. P. G., & P\\'erez, J. M. P. (2022). A\n  techno-economic model for avoiding conflicts of interest between owners of\n  offshore wind farms and maintenance suppliers. Renewable and Sustainable\n  Energy Reviews, 168, 112753",
        "journal-ref": null,
        "doi": "10.1016/j.rser.2022.112753",
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  Currently, wind energy is one of the most important sources of renewable\nenergy. Offshore locations for wind turbines are increasingly exploited because\nof their numerous advantages. However, offshore wind farms require high\ninvestment in maintenance service. Due to its complexity and special\nrequirements, maintenance service is usually outsourced by wind farm owners. In\nthis paper, we propose a novel approach to determine, quantify, and reduce the\npossible conflicts of interest between owners and maintenance suppliers. We\ncreated a complete techno-economic model to address this problem from an\nimpartial point of view. An iterative process was developed to obtain\nstatistical results that can help stakeholders negotiate the terms of the\ncontract, in which the availability of the wind farm is the reference parameter\nby which to determine penalisations and incentives. Moreover, a multi-objective\nprogramming problem was addressed that maximises the profits of both parties\nwithout losing the alignment of their interests. The main scientific\ncontribution of this paper is the maintenance analysis of offshore wind farms\nfrom two perspectives: that of the owner and the maintenance supplier. This\nanalysis evaluates the conflicts of interest of both parties. In addition, we\ndemonstrate that proper adjustment of some parameters, such as penalisation,\nincentives, and resources, and adequate control of availability can help reduce\nthis conflict of interests.\n"
    },
    {
        "paper_id": 2401.08302,
        "authors": "Andrew W. Macpherson",
        "title": "Do backrun auctions protect traders?",
        "comments": "Keywords: MEV, queue discipline, sandwich, CFMM, arbitrage,\n  blockchain, Ethereum",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We study a new \"laminated\" queueing model for orders on batched trading\nvenues such as decentralised exchanges. The model aims to capture and\ngeneralise transaction queueing infrastructure that has arisen to organise MEV\nactivity on public blockchains such as Ethereum, providing convenient channels\nfor sophisticated agents to extract value by acting on end-user order flow by\nperforming arbitrage and related HFT activities. In our model, market orders\nare interspersed with orders created by arbitrageurs that under idealised\nconditions reset the marginal price to a global equilibrium between each trade,\nimproving predictability of execution for liquidity traders.\n  If an arbitrageur has a chance to land multiple opportunities in a row, he\nmay attempt to manipulate the execution price of the intervening market order\nby a probabilistic blind sandwiching strategy. To study how bad this\nmanipulation can get, we introduce and bound a price manipulation coefficient\nthat measures the deviation from global equilibrium of local pricing quoted by\na rational arbitrageur. We exhibit cases in which this coefficient is well\napproximated by a \"zeta value' with interpretable and empirically measurable\nparameters.\n"
    },
    {
        "paper_id": 2401.08323,
        "authors": "Zongxia Liang, Sheng Wang, Jianming Xia, Fengyi Yuan",
        "title": "Dynamic portfolio selection under generalized disappointment aversion",
        "comments": "38 pages, 5 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper addresses the continuous-time portfolio selection problem under\ngeneralized disappointment aversion (GDA). The implicit definition of the\ncertainty equivalent within GDA preferences introduces time inconsistency to\nthis problem. We provide the sufficient and necessary condition for a strategy\nto be an equilibrium by a fully nonlinear integral equation. Investigating the\nexistence and uniqueness of the solution to the integral equation, we establish\nthe existence and uniqueness of the equilibrium. Our findings indicate that\nunder disappointment aversion preferences, non-participation in the stock\nmarket is the unique equilibrium. The semi-analytical equilibrium strategies\nobtained under the constant relative risk aversion utility functions reveal\nthat, under GDA preferences, the investment proportion in the stock market\nconsistently remains smaller than the investment proportion under classical\nexpected utility theory. The numerical analysis shows that the equilibrium\nstrategy's monotonicity concerning the two parameters of GDA preference aligns\nwith the monotonicity of the degree of risk aversion.\n"
    },
    {
        "paper_id": 2401.08548,
        "authors": "Francisco Salas-Molina",
        "title": "Fitting random cash management models to data",
        "comments": "19 pages,6 figures, 1 table",
        "journal-ref": null,
        "doi": "10.1016/j.cor.2018.04.007",
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  Organizations use cash management models to control balances to both avoid\noverdrafts and obtain a profit from short-term investments. Most management\nmodels are based on control bounds which are derived from the assumption of a\nparticular cash flow probability distribution. In this paper, we relax this\nstrong assumption to fit cash management models to data by means of stochastic\nand linear programming. We also introduce ensembles of random cash management\nmodels which are built by randomly selecting a subsequence of the original cash\nflow data set. We illustrate our approach by means of a real case study showing\nthat a small random sample of data is enough to fit sufficiently good\nbound-based models.\n"
    },
    {
        "paper_id": 2401.0859,
        "authors": "Alvear Guzman Katherine, Campozano Buele Jenner, Duran Canarte\n  Paulette, Holguin Cedeno Roger, Mejia Crespin Fernando",
        "title": "Incremento del precio de los combustibles y su incidencia en los\n  productos de la canasta basica del canton el triunfo, provincia del guayas",
        "comments": "in Spanish language",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  The objective of this research was to analyze the impact of the increase in\nthe price of fuels and its incidence on the products of the basic basket of the\nEl Triunfo, the province of Guayas. In the present study, the non-experimental\nquantitative method was used. The study population was limited to the families\nof the town, seeking to determine how their level of consumption was impacted\nafter the increase in fuels. Just 95 people were randomly taken. The study\ninstrument that was used was surveys, with a focus on the purchasing power of\nfamilies with respect to the basic basket after the increase in fuel prices.\nThe results were processed through Cronbach's Alpha and reflected in pie\ncharts. The independent and dependent variable that make up our study, were\nrelated through a simple linear regression, to determine if they correlate with\neach other.\n  Keywords: Fuels, Basic basket, Linear regression, Subsidies, Inflation\n"
    },
    {
        "paper_id": 2401.08594,
        "authors": "Satoshi Nakano and Kazuhiko Nishimura",
        "title": "How do we measure trade elasticity for services?",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper is about our attempt of identifying trade elasticities through the\nvariations in the exchange rate, for possible applications to the case of\nservices whose physical transactions are veiled in the trade statistics. The\nregression analysis to estimate the elasticity entails a situation where the\nexplanatory variable is leaked into the error term through the latent supply\nequation, causing an endogeneity problem for which an instrumental variable\ncannot be found. Our identification strategy is to utilize the normalizing\ncondition, which enables the supply parameter to be identified, along with the\nreduced-form equation of the system of demand and supply equations. We evaluate\nthe performances of the method proposed by applying to several different\ntangible goods, whose benchmark trade elasticities are estimable by utilizing\nthe information on their physical transactions.\n"
    },
    {
        "paper_id": 2401.08596,
        "authors": "Leonard Mushunje and Maxwell Mashasha",
        "title": "Non-Banking Sector development effect on Economic Growth. A Nighttime\n  light data approach",
        "comments": "28 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  This paper uses nighttime light(NTL) data to measure the nexus of the\nnon-banking sector, particularly insurance, and economic growth in South\nAfrica. We hypothesize that insurance sector growth positively propels economic\ngrowth due to its economic growth-supportive traits like investment protection\nand optimal risk mitigation. We also claim that Nighttime light data is a good\neconomic measure than Gross domestic product (GDP). We used weighted\nregressions to measure the relationships between nighttime light data, GDP, and\ninsurance sector development. We used time series South African GDP data\ncollected from the World Bank for the period running from 2000 to 2018, and the\nnighttime lights data from the National Geophysical Data Centre (NGDC) in\npartnership with the National Oceanic and Atmospheric Administration (NOAA).\nFrom the models fitted and the reported BIC, AIC, and likelihood ratios, the\ninsurance sector proved to have more predictive power on economic development\nin South Africa, and radiance light explained economic growth better than GDP\nand GDP/Capita. We concluded that nighttime data is a good proxy for economic\ngrowth than GDP/Capita in emerging economies like South Africa, where secondary\ndata needs to be more robust and sometimes inflated. The findings will guide\nresearchers and policymakers on what drives economic development and what\npolicies to put in place. It would be interesting to extend the current study\nto other sectors such as micro-finances, mutual and hedge funds.\n"
    },
    {
        "paper_id": 2401.086,
        "authors": "Bernhard Hientzsch",
        "title": "Reinforcement Learning and Deep Stochastic Optimal Control for Final\n  Quadratic Hedging",
        "comments": "arXiv admin note: substantial text overlap with arXiv:2302.07996",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We consider two data driven approaches, Reinforcement Learning (RL) and Deep\nTrajectory-based Stochastic Optimal Control (DTSOC) for hedging a European call\noption without and with transaction cost according to a quadratic hedging P&L\nobjective at maturity (\"variance-optimal hedging\" or \"final quadratic\nhedging\"). We study the performance of the two approaches under various market\nenvironments (modeled via the Black-Scholes and/or the log-normal SABR model)\nto understand their advantages and limitations. Without transaction costs and\nin the Black-Scholes model, both approaches match the performance of the\nvariance-optimal Delta hedge. In the log-normal SABR model without transaction\ncosts, they match the performance of the variance-optimal Barlett's Delta\nhedge. Agents trained on Black-Scholes trajectories with matching initial\nvolatility but used on SABR trajectories match the performance of Bartlett's\nDelta hedge in average cost, but show substantially wider variance. To apply RL\napproaches to these problems, P&L at maturity is written as sum of step-wise\ncontributions and variants of RL algorithms are implemented and used that\nminimize expectation of second moments of such sums.\n"
    },
    {
        "paper_id": 2401.08606,
        "authors": "Guillaume Coqueret",
        "title": "Forking paths in financial economics",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  We argue that spanning large numbers of degrees of freedom in empirical\nanalysis allows better characterizations of effects and thus improves the\ntrustworthiness of conclusions. Our ideas are illustrated in three studies:\nequity premium prediction, asset pricing anomalies and risk premia estimation.\nIn the first, we find that each additional degree of freedom in the protocol\nexpands the average range of $t$-statistics by at least 30%. In the second, we\nshow that resorting to forking paths instead of bootstrapping in multiple\ntesting raises the bar of significance for anomalies: at the 5% confidence\nlevel, the threshold for bootstrapped statistics is 4.5, whereas with paths, it\nis at least 8.2, a bar much higher than those currently used in the literature.\nIn our third application, we reveal the importance of particular steps in the\nestimation of premia. In addition, we use paths to corroborate prior findings\nin the three topics. We document heterogeneity in our ability to replicate\nprior studies: some conclusions seem robust, others do not align with the paths\nwe were able to generate.\n"
    },
    {
        "paper_id": 2401.0861,
        "authors": "Xihan Xiong, Zhipeng Wang, Xi Chen, William Knottenbelt, Michael Huth",
        "title": "Leverage Staking with Liquid Staking Derivatives (LSDs): Opportunities\n  and Risks",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  In the Proof of Stake (PoS) Ethereum ecosystem, users can stake ETH on Lido\nto receive stETH, a Liquid Staking Derivative (LSD) that represents staked ETH\nand accrues staking rewards. LSDs improve the liquidity of staked assets by\nfacilitating their use in secondary markets, such as for collateralized\nborrowing on Aave or asset exchanges on Curve. The composability of Lido, Aave,\nand Curve enables an emerging strategy known as leverage staking, where users\nsupply stETH as collateral on Aave to borrow ETH and then acquire more stETH.\nThis can be done directly by initially staking ETH on Lido or indirectly by\nswapping ETH for stETH on Curve. While this iterative process enhances\nfinancial returns, it also introduces potential risks.\n  This paper explores the opportunities and risks of leverage staking. We\nestablish a formal framework for leverage staking with stETH and identify 442\nsuch positions on Ethereum over 963 days. These positions represent a total\nvolume of 537,123 ETH (877m USD). Our data reveal that the majority (81.7%) of\nleverage staking positions achieved an Annual Percentage Rate (APR) higher than\nthat of conventional staking on Lido. Despite the high returns, we also\nrecognize the risks of leverage staking. From the Terra crash incident, we\nunderstand that token devaluation can greatly impact the market. Therefore, we\nconduct stress tests under extreme conditions, particularly during stETH\ndevaluations, to thoroughly evaluate the associated risks. Our simulations\nindicate that leverage staking can exacerbate the risk of cascading\nliquidations by introducing additional selling pressures from liquidation and\ndeleveraging activities. Moreover, this strategy poses broader systemic risks\nas it undermines the stability of ordinary positions by intensifying their\nliquidations.\n"
    },
    {
        "paper_id": 2401.08645,
        "authors": "Adam R. Swietek",
        "title": "Automated Design Appraisal: Estimating Real Estate Price Growth and\n  Value at Risk due to Local Development",
        "comments": "18 pages, 7 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
        "abstract": "  Financial criteria in architectural design evaluation are limited to cost\nperformance. Here, I introduce a method, Automated Design Appraisal (ADA), to\npredict the market price of a generated building design concept within a local\nurban context. Integrating ADA with 3D building performance simulations enables\nfinancial impact assessment that exceeds the spatial resolution of previous\nwork. Within an integrated impact assessment, ADA measures the direct and\nlocalized effect of urban development. To demonstrate its practical utility, I\nstudy local devaluation risk due to nearby development associated with changes\nto visual landscape quality. The results shed light on the relationship between\namenities and property value, identifying clusters of properties physically\nexposed or financially sensitive to local land-use change. Beyond its\napplication as a financial sensitivity tool, ADA serves as a blueprint for\narchitectural design optimization procedures, in which economic performance is\nevaluated based on learned preferences derived from financial market data.\n"
    },
    {
        "paper_id": 2401.08892,
        "authors": "Bernd Engelmann",
        "title": "Spurious Default Probability Projections in Credit Risk Stress Testing\n  Models",
        "comments": "15 pages, 4 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Credit risk stress testing has become an important risk management device\nwhich is used both by banks internally and by regulators. Stress testing is\ncomplex because it essentially means projecting a bank's full balance sheet\nconditional on a macroeconomic scenario over multiple years. Part of the\ncomplexity stems from using a wide range of model parameters for, e.g., rating\ntransition, write-off rules, prepayment, or origination of new loans. A typical\nparameterization of a credit risk stress test model specifies parameters linked\nto an average economic, the through-the-cycle, state. These parameters are\ntransformed to a stressed state by utilizing a macroeconomic model. It will be\nshown that the model parameterization implies a unique through-the-cycle\nportfolio which is unrelated to a bank's current portfolio. Independent of the\nstress imposed to the model, the current portfolio will have a tendency to\npropagate towards the through-the-cycle portfolio. This could create unwanted\nspurious effects on projected portfolio default rates especially when a stress\ntest model's parameterization is inconsistent with a bank's current portfolio.\n"
    },
    {
        "paper_id": 2401.09054,
        "authors": "Edoardo Berton, Alessandro Doldi, Marco Maggis",
        "title": "On conditioning and consistency for nonlinear functionals",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  We consider a family of conditional nonlinear expectations defined on the\nspace of bounded random variables and indexed by the class of all the\nsub-sigma-algebras of a given underlying sigma-algebra. We show that if this\nfamily satisfies a natural consistency property, then it collapses to a\nconditional certainty equivalent defined in terms of a state-dependent utility\nfunction. This result is obtained by embedding our problem in a decision\ntheoretical framework and providing a new characterization of the Sure-Thing\nPrinciple. In particular we prove that this principle characterizes those\npreference relations which admit consistent backward conditional projections.\nWe build our analysis on state-dependent preferences for a general state space\nas in Wakker and Zank (1999) and show that their numerical representation\nadmits a continuous version of the state-dependent utility. In this way, we\nalso answer positively to a conjecture posed in the aforementioned paper.\n"
    },
    {
        "paper_id": 2401.09056,
        "authors": "Abraham Ramos Torres, Laura N Montoya",
        "title": "AI Thrust: Ranking Emerging Powers for Tech Startup Investment in Latin\n  America",
        "comments": "9 pages, 4 tables, 9 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
        "abstract": "  Artificial intelligence (AI) is rapidly transforming the global economy, and\nLatin America is no exception. In recent years, there has been a growing\ninterest in AI development and implementation in the region. This paper\npresents a ranking of Latin American (LATAM) countries based on their potential\nto become emerging powers in AI. The ranking is based on three pillars:\ninfrastructure, education, and finance. Infrastructure is measured by the\navailability of electricity, high-speed internet, the quality of\ntelecommunications networks, and the availability of supercomputers. Education\nis measured by the quality of education and the research status. Finance is\nmeasured by the cost of investments, history of investments, economic metrics,\nand current implementation of AI.\n  While Brazil, Chile, and Mexico have established themselves as major players\nin the AI industry in Latin America, our ranking demonstrates the new emerging\npowers in the region. According to the results, Argentina, Colombia, Uruguay,\nCosta Rica, and Ecuador are leading as new emerging powers in AI in Latin\nAmerica. These countries have strong education systems, well-developed\ninfrastructure, and growing financial resources. The ranking provides a useful\ntool for policymakers, investors, and businesses interested in AI development\nin Latin America. It can help to identify emerging LATAM countries with the\ngreatest potential for AI growth and success.\n"
    },
    {
        "paper_id": 2401.09113,
        "authors": "Karl-Wilhelm Georg Bollweg, Thilo Meyer-Brandis",
        "title": "Mean-Field SDEs driven by $G$-Brownian Motion",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We extend the notion of mean-field SDEs to SDEs driven by $G$-Brownian\nmotion. More precisely, we consider a $G$-SDE where the coefficients depend not\nonly on time and the current state but also on the solution as random variable.\n"
    },
    {
        "paper_id": 2401.09174,
        "authors": "William E. Bendinelli, Humberto F. A. J. Bettini, Alessandro V. M.\n  Oliveira",
        "title": "Airline delays, congestion internalization and non-price spillover\n  effects of low cost carrier entry",
        "comments": null,
        "journal-ref": "Transportation Research Part A: Policy and Practice, 85, 39-52\n  (2016)",
        "doi": "10.1016/j.tra.2016.01.001",
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  This paper develops an econometric model of flight delays to investigate the\ninfluence of competition and dominance on the incentives of carriers to\nmaintain on-time performance. We consider both the route and the airport levels\nto inspect the local and global effects of competition, with a unifying\nframework to test the hypotheses of 1. airport congestion internalization and\n2. the market competition-quality relationship in a single econometric model.\nIn particular, we examine the impacts of the entry of low cost carriers (LCC)\non the flight delays of incumbent full service carriers in the Brazilian\nairline industry. The main results indicate a highly significant effect of\nairport congestion self-internalization in parallel with route-level quality\ncompetition. Additionally, the potential competition caused by LCC presence\nprovokes a global effect that suggests the existence of non-price spillovers of\nthe LCC entry to non-entered routes.\n"
    },
    {
        "paper_id": 2401.09233,
        "authors": "Christoph J. B\\\"orner, Ingo Hoffmann and John H. Stiebel",
        "title": "A closer look at the chemical potential of an ideal agent system",
        "comments": "11 Pages, 0 Figures, Working Paper, Theoretical Contribution",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Models for spin systems known from statistical physics are used in\neconometrics in the form of agent-based models. Econophysics research in\neconometrics is increasingly developing general market models that describe\nexchange phenomena and use the chemical potential $\\mu$ known from physics in\nthe context of particle number changes. In statistical physics, equations of\nstate are known for the chemical potential, which take into account the\nrespective model framework and the corresponding state variables. A simple\ntransfer of these equations of state to problems in econophysics appears\ndifficult. To the best of our knowledge, the equation of state for the chemical\npotential is currently missing even for the simplest conceivable model of an\nideal agent system. In this paper, this research gap is closed and the equation\nof state for the chemical potential is derived from the econophysical model\nassumptions of the ideal agent system. An interpretation of the equation of\nstate leads to fundamental relationships that could also have been guessed, but\nare shown here by the theory.\n"
    },
    {
        "paper_id": 2401.09265,
        "authors": "B.N. Kausik",
        "title": "Equity Premium in Efficient Markets",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Equity premium, the surplus returns of stocks over bonds, has been an\nenduring puzzle. While numerous prior works approach the problem assuming the\nutility of money is invariant across contexts, our approach implies that in\nefficient markets the utility of money is polymorphic, with risk aversion\ndependent on the information available in each context, i.e. the discount on\neach future cash flow depends on all information available on that cash flow.\nSpecifically, we prove that in efficient markets, informed investors maximize\nreturn on volatility by being risk-neutral with riskless bonds, and risk-averse\nwith equities, thereby resolving the puzzle. We validate our results on\nhistorical data with surprising consistency.\n  JEL Classification: C58, G00, G12, G17\n"
    },
    {
        "paper_id": 2401.09361,
        "authors": "Timoth\\'ee Fabre and Ioane Muni Toke",
        "title": "Neural Hawkes: Non-Parametric Estimation in High Dimension and Causality\n  Analysis in Cryptocurrency Markets",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We propose a novel approach to marked Hawkes kernel inference which we name\nthe moment-based neural Hawkes estimation method. Hawkes processes are fully\ncharacterized by their first and second order statistics through a Fredholm\nintegral equation of the second kind. Using recent advances in solving partial\ndifferential equations with physics-informed neural networks, we provide a\nnumerical procedure to solve this integral equation in high dimension. Together\nwith an adapted training pipeline, we give a generic set of hyperparameters\nthat produces robust results across a wide range of kernel shapes. We conduct\nan extensive numerical validation on simulated data. We finally propose two\napplications of the method to the analysis of the microstructure of\ncryptocurrency markets. In a first application we extract the influence of\nvolume on the arrival rate of BTC-USD trades and in a second application we\nanalyze the causality relationships and their directions amongst a universe of\n15 cryptocurrency pairs in a centralized exchange.\n"
    },
    {
        "paper_id": 2401.09718,
        "authors": "Guy Ben-Ishai, Jeff Dean, James Manyika, Ruth Porat, Hal Varian and\n  Kent Walker",
        "title": "AI and the Opportunity for Shared Prosperity: Lessons from the History\n  of Technology and the Economy",
        "comments": "37 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Recent progress in artificial intelligence (AI) marks a pivotal moment in\nhuman history. It presents the opportunity for machines to learn, adapt, and\nperform tasks that have the potential to assist people, from everyday\nactivities to their most creative and ambitious projects. It also has the\npotential to help businesses and organizations harness knowledge, increase\nproductivity, innovate, transform, and power shared prosperity. This tremendous\npotential raises two fundamental questions: (1) Will AI actually advance\nnational and global economic transformation to benefit society at large? and\n(2) What issues must we get right to fully realize AI's economic value, expand\nprosperity and improve lives everywhere? We explore these questions by\nconsidering the recent history of technology and innovation as a guide for the\nlikely impact of AI and what we must do to realize its economic potential to\nbenefit society. While we do not presume the future will be entirely like that\npast, for reasons we will discuss, we do believe prior experience with\ntechnological change offers many useful lessons. We conclude that while\nprogress in AI presents a historic opportunity to advance our economic\nprosperity and future wellbeing, its economic benefits will not come\nautomatically and that AI risks exacerbating existing economic challenges\nunless we collectively and purposefully act to enable its potential and address\nits challenges. We suggest a collective policy agenda - involving developers,\ndeployers and users of AI, infrastructure providers, policymakers, and those\ninvolved in workforce training - that may help both realize and harness AI's\neconomic potential and address its risks to our shared prosperity.\n"
    },
    {
        "paper_id": 2401.09778,
        "authors": "O. Didkovskyi, N. Jean, G. Le Pera and C. Nordio",
        "title": "Cross-Domain Behavioral Credit Modeling: transferability from private to\n  central data",
        "comments": "25 pages, 15 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  This paper introduces a credit risk rating model for credit risk assessment\nin quantitative finance, aiming to categorize borrowers based on their\nbehavioral data. The model is trained on data from Experian, a widely\nrecognized credit bureau, to effectively identify instances of loan defaults\namong bank customers. Employing state-of-the-art statistical and machine\nlearning techniques ensures the model's predictive accuracy. Furthermore, we\nassess the model's transferability by testing it on behavioral data from the\nBank of Italy, demonstrating its potential applicability across diverse\ndatasets during prediction. This study highlights the benefits of incorporating\nexternal behavioral data to improve credit risk assessment in financial\ninstitutions.\n"
    },
    {
        "paper_id": 2401.09811,
        "authors": "Gabriel Bizama, Alexander Wu, Bernardo Paniagua, Max Mitre",
        "title": "A Framework for Digital Currencies for Financial Inclusion in Latin\n  America and the Caribbean",
        "comments": "32 pages, 7 figures, 3 tables and 3 boxes",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This research aims to provide a framework to assess the contribution of\ndigital currencies to promote financial inclusion, based on a diagnosis of the\nlandscape of financial inclusion and domestic and cross-border payments in\nLatin America and the Caribbean. It also provides insights from central banks\nin the region on key aspects regarding a possible implementation of central\nbank digital currencies. Findings show that although digital currencies\ndevelopment is at an early stage, a well-designed system could reduce the cost\nof domestic and cross-border payments, improve the settlement of transactions\nto achieve real-time payments, expand the accessibility of central bank money,\nincorporate programmable payments and achieve system performance demands.\n"
    },
    {
        "paper_id": 2401.09955,
        "authors": "Felix L. Wolf, Griselda Deelstra, Lech A. Grzelak",
        "title": "Consistent asset modelling with random coefficients and switches between\n  regimes",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We explore a stochastic model that enables capturing external influences in\ntwo specific ways. The model allows for the expression of uncertainty in the\nparametrisation of the stochastic dynamics and incorporates patterns to account\nfor different behaviours across various times or regimes. To establish our\nframework, we initially construct a model with random parameters, where the\nswitching between regimes can be dictated either by random variables or\ndeterministically. Such a model is highly interpretable. We further ensure\nmathematical consistency by demonstrating that the framework can be elegantly\nexpressed through local volatility models taking the form of standard jump\ndiffusions. Additionally, we consider a Markov-modulated approach for the\nswitching between regimes characterised by random parameters. For all\nconsidered models, we derive characteristic functions, providing a versatile\ntool with wide-ranging applications. In a numerical experiment, we apply the\nframework to the financial problem of option pricing. The impact of parameter\nuncertainty is analysed in a two-regime model, where the asset process switches\nbetween periods of high and low volatility imbued with high and low\nuncertainty, respectively.\n"
    },
    {
        "paper_id": 2401.10162,
        "authors": "Garvit Arora, Shubhangi Tiwari, Ying Wu, Xuan Mei",
        "title": "An Exploration to the Correlation Structure and Clustering of\n  Macroeconomic Variables",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  As a quantitative characterization of the complicated economy, Macroeconomic\nVariables (MEVs), including GDP, inflation, unemployment, income, spending,\ninterest rate, etc., are playing a crucial role in banks' portfolio management\nand stress testing exercise. In recent years, especially during the COVID-19\nperiod and the current high inflation environment, people are frequently\ntalking about the changing \"correlation structure\" of MEVs. In this paper, we\nuse a principal component based algorithm to perform unsupervised clustering on\nMEVs so we can quantify and better understand MEVs' correlation structure in\nany given period. We also demonstrate how this method can be used to visualize\nhistorical MEVs pattern changes between 2000 and 2022. Further, we use this\nmethod to compare different hypothetical and/or historical macroeconomic\nscenarios and present our key findings. One of these interesting observations\nis that, for a list of 132 transformations derived from 44 targeted MEVs that\ncover 5 different aspects of the U.S. economy (which takes as a subset the 10+\nkey MEVs published by FRB), compared to benign years where there are typically\n20-25 clusters, during the great financial crisis (GFC), i.e., 2007-2010, they\nexhibited a more synchronized and less diversified pattern of movement, forming\nroughly 15 clusters. We also see this contrast in the hypothetical CCAR2023 FRB\nscenarios where the Severely Adverse scenario has 15 clusters and the Baseline\nscenario has 21 clusters. We provide our interpretation to this observation and\nhope this research can inspire and benefit researchers from different domains\nall over the world.\n"
    },
    {
        "paper_id": 2401.10181,
        "authors": "Amine C-L. Ouazad",
        "title": "Equilibrium Multiplicity: A Systematic Approach using Homotopies, with\n  an Application to Chicago",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Discrete choice models with social interactions or spillovers may exhibit\nmultiple equilibria. This paper provides a systematic approach to enumerating\nthem for a quantitative spatial model with discrete locations, social\ninteractions, and elastic housing supply. The approach relies on two\nhomotopies. A homotopy is a smooth function that transforms the solutions of a\nsimpler city where solutions are known, to a city with heterogeneous locations\nand finite supply elasticity. The first homotopy is that, in the set of cities\nwith perfectly elastic floor surface supply, an economy with heterogeneous\nlocations is homotopic to an economy with homogeneous locations, whose\nsolutions can be comprehensively enumerated. Such an economy is epsilon close\nto an economy whose equilibria are the zeros of a system of polynomials. This\nis a well-studied area of mathematics where the enumeration of equilibria can\nbe guaranteed. The second homotopy is that a city with perfectly elastic\nhousing supply is homotopic to a city with an arbitrary supply elasticity. In a\nsmall number of cases, the path may bifurcate and a single path yields two or\nmore equilibria. By running the method on thousands of cities, we obtain a\nlarge number of equilibria. Each equilibrium has different population\ndistributions. We provide a method that is computationally feasible for\neconomies with a large number of locations choices, with an empirical\napplication to the City of Chicago. There exist multiple ``counterfactual\nChicagos'' consistent with the estimated parameters. Population distribution,\nprices, and welfare are not uniquely pinned down by amenities. The paper's\nmethod can be applied to models in trade and IO. Further applications of\nalgebraic geometry are suggested.\n"
    },
    {
        "paper_id": 2401.10238,
        "authors": "Ana Fern\\'andez Vilas and Rebeca P. D\\'iaz Redondo and Daniel Couto\n  Cancela and Alejandro Torrado Pazos",
        "title": "Interplay between Cryptocurrency Transactions and Online Financial\n  Forums",
        "comments": null,
        "journal-ref": "Mathematics 2021, 9(4), 411;",
        "doi": "10.3390/math9040411",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Cryptocurrencies are a type of digital money meant to provide security and\nanonymity while using cryptography techniques. Although cryptocurrencies\nrepresent a breakthrough and provide some important benefits, their usage poses\nsome risks that are a result of the lack of supervising institutions and\ntransparency. Because disinformation and volatility is discouraging for\npersonal investors, cryptocurrencies emerged hand-in-hand with the\nproliferation of online users' communities and forums as places to share\ninformation that can alleviate users' mistrust. This research focuses on the\nstudy of the interplay between these cryptocurrency forums and fluctuations in\ncryptocurrency values. In particular, the most popular cryptocurrency Bitcoin\n(BTC) and a related active discussion community, Bitcointalk, are analyzed.\nThis study shows that the activity of Bitcointalk forum keeps a direct\nrelationship with the trend in the values of BTC, therefore analysis of this\ninteraction would be a perfect base to support personal investments in a\nnon-regulated market and, to confirm whether cryptocurrency forums show\nevidences to detect abnormal behaviors in BTC values as well as to predict or\nestimate these values. The experiment highlights that forum data can explain\nspecific events in the financial field. It also underlines the relevance of\nquotes (regular mechanism to response a post) at periods: (1) when there is a\nhigh concentration of posts around certain topics; (2) when peaks in the BTC\nprice are observed; and, (3) when the BTC price gradually shifts downwards and\nusers intend to sell.\n"
    },
    {
        "paper_id": 2401.10255,
        "authors": "Franck Ramaharo and Gerzhino Rasolofomanana",
        "title": "Nowcasting Madagascar's real GDP using machine learning algorithms",
        "comments": "13 pages, 6 figures, 5 tables",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  We investigate the predictive power of different machine learning algorithms\nto nowcast Madagascar's gross domestic product (GDP). We trained popular\nregression models, including linear regularized regression (Ridge, Lasso,\nElastic-net), dimensionality reduction model (principal component regression),\nk-nearest neighbors algorithm (k-NN regression), support vector regression\n(linear SVR), and tree-based ensemble models (Random forest and XGBoost\nregressions), on 10 Malagasy quarterly macroeconomic leading indicators over\nthe period 2007Q1--2022Q4, and we used simple econometric models as a\nbenchmark. We measured the nowcast accuracy of each model by calculating the\nroot mean square error (RMSE), mean absolute error (MAE), and mean absolute\npercentage error (MAPE). Our findings reveal that the Ensemble Model, formed by\naggregating individual predictions, consistently outperforms traditional\neconometric models. We conclude that machine learning models can deliver more\naccurate and timely nowcasts of Malagasy economic performance and provide\npolicymakers with additional guidance for data-driven decision making.\n"
    },
    {
        "paper_id": 2401.10261,
        "authors": "Vahidin Jeleskovic and Steffen Loeber",
        "title": "How industrial clusters influence the growth of the regional GDP: A\n  spatial-approach",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
        "abstract": "  In this paper, we employ spatial econometric methods to analyze panel data\nfrom German NUTS 3 regions. Our goal is to gain a deeper understanding of the\nsignificance and interdependence of industry clusters in shaping the dynamics\nof GDP. To achieve a more nuanced spatial differentiation, we introduce\nindicator matrices for each industry sector which allows for extending the\nspatial Durbin model to a new version of it. This approach is essential due to\nboth the economic importance of these sectors and the potential issue of\nomitted variables. Failing to account for industry sectors can lead to omitted\nvariable bias and estimation problems. To assess the effects of the major\nindustry sectors, we incorporate eight distinct branches of industry into our\nanalysis. According to prevailing economic theory, these clusters should have a\npositive impact on the regions they are associated with. Our findings indeed\nreveal highly significant impacts, which can be either positive or negative, of\nspecific sectors on local GDP growth. Spatially, we observe that direct and\nindirect effects can exhibit opposite signs, indicative of heightened\ncompetitiveness within and between industry sectors. Therefore, we recommend\nthat industry sectors should be taken into consideration when conducting\nspatial analysis of GDP. Doing so allows for a more comprehensive understanding\nof the economic dynamics at play.\n"
    },
    {
        "paper_id": 2401.1037,
        "authors": "Lars Ericson, Xuejun Zhu, Xusi Han, Rao Fu, Shuang Li, Steve Guo, Ping\n  Hu",
        "title": "Deep Generative Modeling for Financial Time Series with Application in\n  VaR: A Comparative Review",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In the financial services industry, forecasting the risk factor distribution\nconditional on the history and the current market environment is the key to\nmarket risk modeling in general and value at risk (VaR) model in particular. As\none of the most widely adopted VaR models in commercial banks, Historical\nsimulation (HS) uses the empirical distribution of daily returns in a\nhistorical window as the forecast distribution of risk factor returns in the\nnext day. The objectives for financial time series generation are to generate\nsynthetic data paths with good variety, and similar distribution and dynamics\nto the original historical data. In this paper, we apply multiple existing deep\ngenerative methods (e.g., CGAN, CWGAN, Diffusion, and Signature WGAN) for\nconditional time series generation, and propose and test two new methods for\nconditional multi-step time series generation, namely Encoder-Decoder CGAN and\nConditional TimeVAE. Furthermore, we introduce a comprehensive framework with a\nset of KPIs to measure the quality of the generated time series for financial\nmodeling. The KPIs cover distribution distance, autocorrelation and\nbacktesting. All models (HS, parametric and neural networks) are tested on both\nhistorical USD yield curve data and additional data simulated from GARCH and\nCIR processes. The study shows that top performing models are HS, GARCH and\nCWGAN models. Future research directions in this area are also discussed.\n"
    },
    {
        "paper_id": 2401.10473,
        "authors": "Thomas J. Sargent and John Stachurski",
        "title": "Dynamic Programming: Finite States",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  This book is about dynamic programming and its applications in economics,\nfinance, and adjacent fields. It brings together recent innovations in the\ntheory of dynamic programming and provides applications and code that can help\nreaders approach the research frontier. The book is aimed at graduate students\nand researchers, although most chapters are accessible to undergraduate\nstudents with solid quantitative backgrounds.\n"
    },
    {
        "paper_id": 2401.10722,
        "authors": "Hamza Bodor, Laurent Carlier",
        "title": "Stylized Facts and Market Microstructure: An In-Depth Exploration of\n  German Bond Futures Market",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/publicdomain/zero/1.0/",
        "abstract": "  This paper presents an in-depth analysis of stylized facts in the context of\nfutures on German bonds. The study examines four futures contracts on German\nbonds: Schatz, Bobl, Bund and Buxl, using tick-by-tick limit order book\ndatasets. It uncovers a range of stylized facts and empirical observations,\nincluding the distribution of order sizes, patterns of order flow, and\ninter-arrival times of orders. The findings reveal both commonalities and\nunique characteristics across the different futures, thereby enriching our\nunderstanding of these markets. Furthermore, the paper introduces insightful\nrealism metrics that can be used to benchmark market simulators. The study\ncontributes to the literature on financial stylized facts by extending\nempirical observations to this class of assets, which has been relatively\nunderexplored in existing research. This work provides valuable guidance for\nthe development of more accurate and realistic market simulators.\n"
    },
    {
        "paper_id": 2401.10872,
        "authors": "Federico Echenique, Alejandro Robinson-Cort\\'es, Leeat Yariv",
        "title": "An Experimental Study of Decentralized Matching",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We present an experimental study of decentralized two-sided matching markets\nwith no transfers. Experimental participants are informed of everyone's\npreferences and can make arbitrary non-binding match offers that get finalized\nwhen a period of market inactivity has elapsed. Several insights emerge. First,\nstable outcomes are prevalent. Second, while centralized clearinghouses\ncommonly aim at implementing extremal stable matchings, our decentralized\nmarkets most frequently culminate in the median stable matching. Third,\npreferences' cardinal representations impact the stable partners participants\nmatch with. Last, the dynamics underlying our results exhibit strategic\nsophistication, with agents successfully avoiding cycles of blocking pairs.\n"
    },
    {
        "paper_id": 2401.10903,
        "authors": "Dengxin Huang",
        "title": "Application of Machine Learning in Stock Market Forecasting: A Case\n  Study of Disney Stock",
        "comments": "9 pages, 7 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  This document presents a stock market analysis conducted on a dataset\nconsisting of 750 instances and 16 attributes donated in 2014-10-23. The\nanalysis includes an exploratory data analysis (EDA) section, feature\nengineering, data preparation, model selection, and insights from the analysis.\nThe Fama French 3-factor model is also utilized in the analysis. The results of\nthe analysis are presented, with linear regression being the best-performing\nmodel.\n"
    },
    {
        "paper_id": 2401.10931,
        "authors": "Sauren Gupta, Apoorva Hathi Katharaki, Yifan Xu, Bhaskar\n  Krishnamachari, Rajarshi Gupta",
        "title": "Forecasting Cryptocurrency Staking Rewards",
        "comments": "9 pages, 18 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This research explores a relatively unexplored area of predicting\ncryptocurrency staking rewards, offering potential insights to researchers and\ninvestors. We investigate two predictive methodologies: a) a straightforward\nsliding-window average, and b) linear regression models predicated on\nhistorical data. The findings reveal that ETH staking rewards can be forecasted\nwith an RMSE within 0.7% and 1.1% of the mean value for 1-day and 7-day\nlook-aheads respectively, using a 7-day sliding-window average approach.\nAdditionally, we discern diverse prediction accuracies across various\ncryptocurrencies, including SOL, XTZ, ATOM, and MATIC. Linear regression is\nidentified as superior to the moving-window average for perdicting in the short\nterm for XTZ and ATOM. The results underscore the generally stable and\npredictable nature of staking rewards for most assets, with MATIC presenting a\nnoteworthy exception.\n"
    },
    {
        "paper_id": 2401.11011,
        "authors": "Valentina Aparicio, Daniel Gordon, Sebastian G. Huayamares, Yuhuai Luo",
        "title": "BioFinBERT: Finetuning Large Language Models (LLMs) to Analyze Sentiment\n  of Press Releases and Financial Text Around Inflection Points of Biotech\n  Stocks",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Large language models (LLMs) are deep learning algorithms being used to\nperform natural language processing tasks in various fields, from social\nsciences to finance and biomedical sciences. Developing and training a new LLM\ncan be very computationally expensive, so it is becoming a common practice to\ntake existing LLMs and finetune them with carefully curated datasets for\ndesired applications in different fields. Here, we present BioFinBERT, a\nfinetuned LLM to perform financial sentiment analysis of public text associated\nwith stocks of companies in the biotechnology sector. The stocks of biotech\ncompanies developing highly innovative and risky therapeutic drugs tend to\nrespond very positively or negatively upon a successful or failed clinical\nreadout or regulatory approval of their drug, respectively. These clinical or\nregulatory results are disclosed by the biotech companies via press releases,\nwhich are followed by a significant stock response in many cases. In our\nattempt to design a LLM capable of analyzing the sentiment of these press\nreleases,we first finetuned BioBERT, a biomedical language representation model\ndesigned for biomedical text mining, using financial textual databases. Our\nfinetuned model, termed BioFinBERT, was then used to perform financial\nsentiment analysis of various biotech-related press releases and financial text\naround inflection points that significantly affected the price of biotech\nstocks.\n"
    },
    {
        "paper_id": 2401.111,
        "authors": "David Roodman",
        "title": "Long-term Effects of India's Childhood Immunization Program on Earnings\n  and Consumption Expenditure: Comment",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
        "abstract": "  Summan, Nandi, and Bloom (2023), hereafter SNB, find that exposure during\ninfancy to India's Universal Immunization Programme (UIP) increased wages and\nper-capita household expenditure in early adulthood. SNB regress these outcomes\non a treatment indicator that depends upon year and district of birth while\ncontrolling for age at follow-up. Because year of birth and age are nearly\ncollinear, SNB's identifying variation does not come from the staggered\nintroduction of the UIP, but rather from the progression of time during the\nfollow-up period. Within the 12-month follow-up period, those interviewed later\nwere more likely to have been treated and, on average, reported higher wages\nand household expenditure. Wages and household expenditure, however, rose by at\nleast as much in a control group composed of people too old to have been\nexposed as infants to the UIP as in the treated group. SNB's results are best\nexplained by inflation, economic growth, and non-random survey sequencing\nduring the follow-up survey period.\n"
    },
    {
        "paper_id": 2401.11158,
        "authors": "Min Dai, Hanqing Jin, Xi Yang",
        "title": "Data-driven Option Pricing",
        "comments": "15 pages, 3 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  We propose an innovative data-driven option pricing methodology that relies\nexclusively on the dataset of historical underlying asset prices. While the\ndataset is rooted in the objective world, option prices are commonly expressed\nas discounted expectations of their terminal payoffs in a risk-neutral world.\nBridging this gap motivates us to identify a pricing kernel process,\ntransforming option pricing into evaluating expectations in the objective\nworld. We recover the pricing kernel by solving a utility maximization problem,\nand evaluate the expectations in terms of a functional optimization problem.\nLeveraging the deep learning technique, we design data-driven algorithms to\nsolve both optimization problems over the dataset. Numerical experiments are\npresented to demonstrate the efficiency of our methodology.\n"
    },
    {
        "paper_id": 2401.11343,
        "authors": "Shawn Berry",
        "title": "An income-based approach to modeling commuting distance in the Toronto\n  area",
        "comments": "pp.1-40, 8 tables, 5 figures",
        "journal-ref": "Research in Transportation Business & Management, Volume 43, June\n  2022, 100720",
        "doi": "10.1016/j.rtbm.2021.100720",
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  The purpose of this article is to propose a novel model of the effects of\nchanges in shelter and driving costs on car commuting distances in the\noverheated Toronto housing market from 2011 to 2016. The model borrows from\ntheoretical concepts of microeconomics and urban geography to examine the\nToronto housing market. Using 2011 and 2016 Census data for census metropolitan\nareas (CMAs) and census agglomerations (CAs) in Southern Ontario and computed\ndriving costs, the model of car commuting distance is based on variables of\nallocation of monthly household income to monthly shelter costs and driving\ncosts as a function of the car driving distance to Toronto. Using this model,\nwe can predict the effect on car commuting distance due to changes in any of\nthe variables. The model also offers an explanation for communities of Toronto\ncar commuters beyond a driving radius that we might expect for daily commuting.\nThe model confirms that increases in shelter costs in the Toronto housing\nmarket from 2011 to 2016 have forced the boundaries of feasible housing\nlocations outward, and forced households to move farther away, thus increasing\ncar commuting distance.\n"
    },
    {
        "paper_id": 2401.11345,
        "authors": "Shawn Berry",
        "title": "Fake Google restaurant reviews and the implications for consumers and\n  restaurants",
        "comments": "pp.1-158, 41 tables, 11 figures. Doctor of Business Administration\n  Dissertation",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  The use of online reviews to aid with purchase decisions is popular among\nconsumers as it is a simple heuristic tool based on the reported experiences of\nother consumers. However, not all online reviews are written by real consumers\nor reflect actual experiences, and present implications for consumers and\nbusinesses. This study examines the effects of fake online reviews written by\nartificial intelligence (AI) on consumer decision making. Respondents were\nsurveyed about their attitudes and habits concerning online reviews using an\nonline questionnaire (n=351), and participated in a restaurant choice\nexperiment using varying proportions of fake and real reviews. While the\nfindings confirm prior studies, new insights are gained about the confusion for\nconsumers and consequences for businesses when reviews written by AI are\nbelieved rather than real reviews. The study presents a fake review detection\nmodel using logistic regression modeling to score and flag reviews as a\nsolution.\n"
    },
    {
        "paper_id": 2401.11495,
        "authors": "Ulrich Horst and Wei Xu",
        "title": "Functional Limit Theorems for Hawkes Processes",
        "comments": "59 pages; Keywords and phrases: Hawkes process, functional limit\n  theorem, regular variation, convergence rate",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We prove that the long-run behavior of Hawkes processes is fully determined\nby the average number and the dispersion of child events. For subcritical\nprocesses we provide FLLNs and FCLTs under minimal conditions on the kernel of\nthe process with the precise form of the limit theorems depending strongly on\nthe dispersion of child events. For a critical Hawkes process with weakly\ndispersed child events, functional central limit theorems do not hold. Instead,\nwe prove that the rescaled intensity processes and rescaled Hawkes processes\nbehave like CIR-processes without mean-reversion, respectively integrated\nCIR-processes. We provide the rate of convergence by establishing an upper\nbound on the Wasserstein distance between the distributions of rescaled Hawkes\nprocess and the corresponding limit process. By contrast, critical Hawkes\nprocess with heavily dispersed child events share many properties of\nsubcritical ones. In particular, functional limit theorems hold. However,\nunlike subcritical processes critical ones with heavily dispersed child events\ndisplay long-range dependencies.\n"
    },
    {
        "paper_id": 2401.11585,
        "authors": "Ganapati Kumar Biswas",
        "title": "Analyzing the Impact of Financial Inclusion on Economic Growth in\n  Bangladesh",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-sa/4.0/",
        "abstract": "  Financial inclusion is touted one of the principal drivers for economic\ngrowth for an economy. The study aims to explore the impact of financial\ninclusion on economic growth in Bangladesh. In my study, I used the number of\nloan accounts as the proxy for financial inclusion. Using time series data from\nspans from 2004-2021, the study revealed that there exists a long-run\nrelationship between GDP, financial inclusion, and other macroeconomic\nvariables in Bangladesh. The study also found that financial inclusion had a\npositive impact on economic growth of Bangladesh during the study period.\nTherefore, the policymakers and the central bank of Bangladesh as the apex\nauthority of financial system should promote financial inclusion activities to\nachieve sustainable economic growth.\n"
    },
    {
        "paper_id": 2401.11619,
        "authors": "Claudio Fontana, Giacomo Lanaro, Agatha Murgoci",
        "title": "The geometry of multi-curve interest rate models",
        "comments": "revised version, 29 pages, 2 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We study the problems of consistency and of the existence of\nfinite-dimensional realizations for multi-curve interest rate models of\nHeath-Jarrow-Morton type, generalizing the geometric approach developed by T.\nBj\\\"ork and co-authors in the classical single-curve setting. We characterize\nwhen a multi-curve interest rate model is consistent with a given parameterized\nfamily of forward curves and spreads and when a model can be realized by a\nfinite-dimensional state process. We illustrate the general theory in a number\nof model classes and examples, providing explicit constructions of\nfinite-dimensional realizations. Based on these theoretical results, we perform\nthe calibration of a three-curve Hull-White model to market data and analyse\nthe stability of the estimated parameters.\n"
    },
    {
        "paper_id": 2401.11621,
        "authors": "Riaz Ud Din, Salman Ahmed, Saddam Hussain Khan",
        "title": "A Novel Decision Ensemble Framework: Customized Attention-BiLSTM and\n  XGBoost for Speculative Stock Price Forecasting",
        "comments": "30 pages, 16 Figures, 4 Tables",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
        "abstract": "  Forecasting speculative stock prices is essential for effective investment\nrisk management that drives the need for the development of innovative\nalgorithms. However, the speculative nature, volatility, and complex sequential\ndependencies within financial markets present inherent challenges which\nnecessitate advanced techniques. This paper proposes a novel framework, CAB-XDE\n(customized attention BiLSTM-XGB decision ensemble), for predicting the daily\nclosing price of speculative stock Bitcoin-USD (BTC-USD). CAB-XDE framework\nintegrates a customized bi-directional long short-term memory (BiLSTM) with the\nattention mechanism and the XGBoost algorithm. The customized BiLSTM leverages\nits learning capabilities to capture the complex sequential dependencies and\nspeculative market trends. Additionally, the new attention mechanism\ndynamically assigns weights to influential features, thereby enhancing\ninterpretability, and optimizing effective cost measures and volatility\nforecasting. Moreover, XGBoost handles nonlinear relationships and contributes\nto the proposed CAB-XDE framework robustness. Additionally, the weight\ndetermination theory-error reciprocal method further refines predictions. This\nrefinement is achieved by iteratively adjusting model weights. It is based on\ndiscrepancies between theoretical expectations and actual errors in individual\ncustomized attention BiLSTM and XGBoost models to enhance performance. Finally,\nthe predictions from both XGBoost and customized attention BiLSTM models are\nconcatenated to achieve diverse prediction space and are provided to the\nensemble classifier to enhance the generalization capabilities of CAB-XDE. The\nproposed CAB-XDE framework is empirically validated on volatile Bitcoin market,\nsourced from Yahoo Finance and outperforms state-of-the-art models with a MAPE\nof 0.0037, MAE of 84.40, and RMSE of 106.14.\n"
    },
    {
        "paper_id": 2401.11701,
        "authors": "Takaaki Koike, Cathy W.S. Chen, Edward M.H. Lin",
        "title": "Forecasting and Backtesting Gradient Allocations of Expected Shortfall",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Capital allocation is a procedure for quantifying the contribution of each\nsource of risk to aggregated risk. The gradient allocation rule, also known as\nthe Euler principle, is a prevalent rule of capital allocation under which the\nallocated capital captures the diversification benefit of the marginal risk as\na component of overall risk. This research concentrates on Expected Shortfall\n(ES) as a regulatory standard and focuses on the gradient allocations of ES,\nalso called ES contributions (ESCs). We present the comprehensive treatment of\nbacktesting the tuple of ESCs in the framework of the traditional and\ncomparative backtests based on the concepts of joint identifiability and\nmulti-objective elicitability. For robust forecast evaluation against the\nchoice of scoring function, we also extend the Murphy diagram, a graphical tool\nto check whether one forecast dominates another under a class of scoring\nfunctions, to the case of ESCs. Finally, leveraging the recent concept of\nmulti-objective elicitability, we propose a novel semiparametric model for\nforecasting dynamic ESCs based on a compositional regression model. In an\nempirical analysis of stock returns we evaluate and compare a variety of models\nfor forecasting dynamic ESCs and demonstrate the outstanding performance of the\nproposed model.\n"
    },
    {
        "paper_id": 2401.11958,
        "authors": "Daniel Kr\\v{s}ek and Gudmund Pammer",
        "title": "General duality and dual attainment for adapted transport",
        "comments": "32 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We investigate duality and existence of dual optimizers for several adapted\noptimal transport problems under minimal assumptions. This includes the causal\nand bicausal transport, the barycenter problem, and a general multimarginal\nproblem incorporating causality constraints. Moreover, we discuss applications\nof our results in robust finance. We consider a non-dominated model of several\nfinancial markets where stocks are traded dynamically, but the joint stock\ndynamics are unknown. We show that a no-arbitrage assumption in a quasi-sure\nsense naturally leads to sets of multicausal couplings. Consequently, computing\nthe robust superhedging price is equivalent to solving an adapted transport\nproblem, and finding a superhedging strategy means solving the corresponding\ndual.\n"
    },
    {
        "paper_id": 2401.12064,
        "authors": "Chen Liang, Murat Tunc, Gordon Burtch",
        "title": "Market Responses to Genuine Versus Strategic Generosity: An Empirical\n  Examination of NFT Charity Fundraisers",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Crypto donations now represent a significant fraction of charitable giving\nworldwide. Nonfungible token (NFT) charity fundraisers, which involve the sale\nof NFTs of artistic works with the proceeds donated to philanthropic causes,\nhave emerged as a novel development in this space. A unique aspect of NFT\ncharity fundraisers is the significant potential for donors to reap financial\ngains from the rising value of purchased NFTs. Questions may arise about the\nmotivations of donors in these charity fundraisers, resulting in a negative\nsocial image. NFT charity fundraisers thus offer a unique opportunity to\nunderstand the economic consequences of a donor's social image. We investigate\nthese effects in the context of a large NFT charity fundraiser. We identify the\ncausal effect of purchasing an NFT within the charity fundraiser on a donor's\nlater market outcomes by leveraging random variation in transaction processing\ntimes on the blockchain. Further, we demonstrate a clear pattern of\nheterogeneity, based on an individual's decision to relist (versus hold) the\npurchased charity NFTs (a sign of strategic generosity), and based on an\nindividual's degree of social exposure within the NFT marketplace. We show that\ncharity-NFT \"relisters\" experience significant penalties in the market, in\nterms of the prices they are able to command on other NFT listings,\nparticularly among those who relist quickly and those who are more socially\nexposed. Our study underscores the growing importance of digital visibility and\ntraceability, features that characterize crypto-philanthropy, and online\nphilanthropy more broadly.\n"
    },
    {
        "paper_id": 2401.121,
        "authors": "Herv\\'e Bercegol and Paul E. Brockway",
        "title": "Metrics matter, a Formal comment on Ward et al Plos-One 2016 paper : Is\n  decoupling GDP growth from environmental impact possible?",
        "comments": "6 pages, 4 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The Ward et al. (2016) Plos-One paper is an important, heavily-cited paper in\nthe decoupling literature. The authors present evidence of 1990-2015 growth in\nmaterial and energy consumption and GDP at a world level, and for selected\ncountries. They find only relative decoupling has occurred, leading to their\ncentral claim that future absolute decoupling is implausible. However, the\nauthors have made two key errors in their collected data: GDP data is in\ncurrent prices which includes inflation, and their global material use data is\nthe total mass of fossil energy materials. Strictly, GDP data should be in\nconstant prices to allow for its comparison over time, and material inputs to\nan economy should be the sum of mineral raw materials. Amending for these\nerrors, we find much smaller levels of energy-GDP relative decoupling, and no\nmaterials-GDP decoupling at all at a global level. We check these new results\nby adding data for 1900-1990 to provide a longer time series, and find\nconsistently low (and even no) levels of global relative decoupling of material\nuse. The central claim for materials over the implausibility of future absolute\ndecoupling therefore not only remains valid but is reinforced by the corrected\ndatasets.\n"
    },
    {
        "paper_id": 2401.12118,
        "authors": "Ben Klemens",
        "title": "Measures of the Capital Network of the U.S. Economy",
        "comments": "18 pages. JEL classifications: L14; C81; M42; G34",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/publicdomain/zero/1.0/",
        "abstract": "  About two million U.S. corporations and partnerships are linked to each other\nand human investors by about 15 million owner-subsidiary links. Comparable\nsocial networks such as corporate board memberships and socially-built systems\nsuch as the network of Internet links are \"small worlds,\" meaning a network\nwith a small diameter and link densities with a power-law distribution, but\nthese properties had not yet been measured for the business entity network.\nThis article shows that both inbound links and outbound links display a\npower-law distribution with a coefficient of concentration estimable to within\na generally narrow confidence interval, overall, for subnetworks including only\nbusiness entities, only for the great connected component of the network, and\nin subnetworks with edges associated with certain industries, for all years\n2009-2021. In contrast to other networks with power-law distributed link\ndensities, the network is mostly a tree, and has a diameter an order of\nmagnitude larger than a small-world network with the same link distribution.\nThe regularity of the power-law distribution indicates that its coefficient can\nbe used as a new, well-defined macroeconomic metric for the concentration of\ncapital flows in an economy. Economists might use it as a new measure of market\nconcentration which is more comprehensive than measures based only on the few\nbiggest firms. Comparing capital link concentrations across countries would\nfacilitate modeling the relationship between business network characteristics\nand other macroeconomic indicators.\n"
    },
    {
        "paper_id": 2401.12274,
        "authors": "Juan Aparicio, Miguel A. Duran, Ana Lozano-Vivas, and Jesus T. Pastor",
        "title": "Are Charter Value and Supervision Aligned? A Segmentation Analysis",
        "comments": "46 pages, 4 tables, 5 figures, accepted version of a paper published\n  in the Journal of Financial Stability",
        "journal-ref": "Journal of Financial Stability 37 (2018) 60-73",
        "doi": "10.1016/j.jfs.2018.05.004",
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  Previous work suggests that the charter value hypothesis is theoretically\ngrounded and empirically supported, but not universally. Accordingly, this\npaper aims to perform an analysis of the relations between charter value, risk\ntaking, and supervision, taking into account the relations' complexity.\nSpecifically, using the CAMELS rating system as a general framework for\nsupervision, we study how charter value relates to risk and supervision by\nmeans of classification and regression tree analysis. The sample covers the\nperiod 2005-2016 and consists of listed banks in countries that were members of\nthe Eurozone when it came into existence, along with Greece. To evaluate the\ncrisis consequences, we also separately analyze four subperiods and countries\nthat required financial aid from third parties and those that did not so, along\nwith large and small banks. Our results reflect the complexity of the relations\nbetween charter value, supervision, and risk. Indeed, supervision and charter\nvalue seem aligned regarding only some types of risk\n"
    },
    {
        "paper_id": 2401.12301,
        "authors": "Miguel A. Duran",
        "title": "Pricing and Usage: An Empirical Analysis of Lines of Credit",
        "comments": "32 pages, 7 tables, accepted version of a paper published in the\n  Journal of International Financial Markets, Institutions and Money",
        "journal-ref": "Journal of International Financial Markets, Institutions and Money\n  50 (2017) 219-234",
        "doi": "10.1016/j.intfin.2017.08.012",
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  The hypothesis that committed revolving credit lines with fixed spreads can\nprovide firms with interest rate insurance is a standard feature of models on\nthese credit facilities' interest rate structure. Nevertheless, this hypothesis\nhas not been tested. Its empirical examination is the main contribution of this\npaper. To perform this analysis, and given the unavailability of data, we\nhand-collect data on usage at the credit line level itself. The resulting\ndataset enables us also to take into account characteristics of credit lines\nthat have been ignored by previous research. One of them is that credit lines\ncan have simultaneously fixed and performance-based spreads.\n"
    },
    {
        "paper_id": 2401.12315,
        "authors": "Miguel A. Duran",
        "title": "The Risk-Return Relation in the Corporate Loan Market",
        "comments": "56 pages, 3 figurees, 7 tables, accepted version of a paper published\n  in the North American Journal of Economics and Finance",
        "journal-ref": "North American Journal of Economics and Finance 60 (2020) 1-24",
        "doi": "10.1016/j.najef.2022.101643",
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  This paper analyzes the hypothesis that returns play a risk-compensating role\nin the market for corporate revolving lines of credit. Specifically, we test\nwhether borrower risk and the expected return on these debt instruments are\npositively related. Our main findings support this prediction, in contrast to\nthe only previous work that examined this problem two decades ago.\nNevertheless, we find evidence of mispricing regarding the risk of\ndeteriorating firms using their facilities more intensively and during the\nsubprime crisis.\n"
    },
    {
        "paper_id": 2401.12323,
        "authors": "F. Bolivar, M. A. Duran, and A. Lozano-Vivas",
        "title": "Bank Business Models, Size, and Profitability",
        "comments": "14 pages, 1 figure, 3 tables, accepted version of an article\n  published in Finance Research Letters",
        "journal-ref": "Finance Research Letters 53 (2023)",
        "doi": "10.1016/j.frl.2022.103605",
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  To examine the relation between profitability and business models (BMs)\nacross bank sizes, the paper proposes a research strategy based on machine\nlearning techniques. This strategy allows for analyzing whether size and profit\nperformance underlie BM heterogeneity, with BM identification being based on\nhow the components of the bank portfolio contribute to profitability. The\nempirical exercise focuses on the European Union banking system. Our results\nsuggest that banks with analogous levels of performance and different sizes\nshare strategic features. Additionally, high capital ratios seem compatible\nwith high profitability if banks, relative to their size peers, adopt a\nstandard retail BM.\n"
    },
    {
        "paper_id": 2401.12334,
        "authors": "F. Bolivar, Miguel A. Duran, and A. Lozano-Vivas",
        "title": "Business Model Contributions to Bank Profit Performance: A Machine\n  Learning Approach",
        "comments": "46 pages, 10 tables, 3 figures, submitted version of a paper\n  published in Research in International Business and Finance",
        "journal-ref": "Research in International Business and Finance 64 (2023)",
        "doi": "10.1016/j.ribaf.2022.101870",
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  This paper analyzes the relation between bank profit performance and business\nmodels. Using a machine learning-based approach, we propose a methodological\nstrategy in which balance sheet components' contributions to profitability are\nthe identification instruments of business models. We apply this strategy to\nthe European Union banking system from 1997 to 2021. Our main findings indicate\nthat the standard retail-oriented business model is the profile that performs\nbest in terms of profitability, whereas adopting a non-specialized business\nprofile is a strategic decision that leads to poor profitability. Additionally,\nour findings suggest that the effect of high capital ratios on profitability\ndepends on the business profile. The contributions of business models to\nprofitability decreased during the Great Recession. Although the situation\nshowed signs of improvement afterward, the European Union banking system's\nability to yield returns is still problematic in the post-crisis period, even\nfor the best-performing group.\n"
    },
    {
        "paper_id": 2401.12652,
        "authors": "Henri Arno, Klaas Mulier, Joke Baeck, Thomas Demeester",
        "title": "From Numbers to Words: Multi-Modal Bankruptcy Prediction Using the ECL\n  Dataset",
        "comments": "Presented at the 6th Workshop on Financial Technology and Natural\n  Language Processing (FinNLP) @ IJCNLP-AACL 2023 in Bali, Indonesia",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-sa/4.0/",
        "abstract": "  In this paper, we present ECL, a novel multi-modal dataset containing the\ntextual and numerical data from corporate 10K filings and associated binary\nbankruptcy labels. Furthermore, we develop and critically evaluate several\nclassical and neural bankruptcy prediction models using this dataset. Our\nfindings suggest that the information contained in each data modality is\ncomplementary for bankruptcy prediction. We also see that the binary bankruptcy\nprediction target does not enable our models to distinguish next year\nbankruptcy from an unhealthy financial situation resulting in bankruptcy in\nlater years. Finally, we explore the use of LLMs in the context of our task. We\nshow how GPT-based models can be used to extract meaningful summaries from the\ntextual data but zero-shot bankruptcy prediction results are poor. All\nresources required to access and update the dataset or replicate our\nexperiments are available on github.com/henriarnoUG/ECL.\n"
    },
    {
        "paper_id": 2401.12669,
        "authors": "Francesco Cesarone, Justo Puerto",
        "title": "New approximate stochastic dominance approaches for Enhanced Indexation\n  models",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  In this paper, we discuss portfolio selection strategies for Enhanced\nIndexation (EI), which are based on stochastic dominance relations. The goal is\nto select portfolios that stochastically dominate a given benchmark but that,\nat the same time, must generate some excess return with respect to a benchmark\nindex. To achieve this goal, we propose a new methodology that selects\nportfolios using the ordered weighted average (OWA) operator, which generalizes\nprevious approaches based on minimax selection rules and still leads to solving\nlinear programming models. We also introduce a new type of approximate\nstochastic dominance rule and show that it implies the almost Second-order\nStochastic Dominance (SSD) criterion proposed by Lizyayev and Ruszczynski\n(2012). We prove that our EI model based on OWA selects portfolios that\ndominate a given benchmark through this new form of stochastic dominance\ncriterion. We test the performance of the obtained portfolios in an extensive\nempirical analysis based on real-world datasets. The computational results show\nthat our proposed approach outperforms several SSD-based strategies widely used\nin the literature, as well as the global minimum variance portfolio.\n"
    },
    {
        "paper_id": 2401.12748,
        "authors": "Beatrice Acciaio and Daniel Kr\\v{s}ek and Gudmund Pammer",
        "title": "Multicausal transport: barycenters and dynamic matching",
        "comments": "26 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We introduce a multivariate version of adapted transport, which we name\nmulticausal transport, involving several filtered processes among which\ncausality constraints are imposed. Subsequently, we consider the barycenter\nproblem for stochastic processes with respect to causal and bicausal optimal\ntransport, and study its connection to specific multicausal transport problems.\nAttainment and duality of the aforementioned problems are provided. As an\napplication, we study a matching problem in a dynamic setting where agents'\ntypes evolve over time. We link this to a causal barycenter problem and thereby\nshow existence of equilibria.\n"
    },
    {
        "paper_id": 2401.12773,
        "authors": "Fabian Dvorak, Regina Stumpf, Sebastian Fehrler, Urs Fischbacher",
        "title": "Generative AI Triggers Welfare-Reducing Decisions in Humans",
        "comments": "19 pages, 2 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Generative artificial intelligence (AI) is poised to reshape the way\nindividuals communicate and interact. While this form of AI has the potential\nto efficiently make numerous human decisions, there is limited understanding of\nhow individuals respond to its use in social interaction. In particular, it\nremains unclear how individuals engage with algorithms when the interaction\nentails consequences for other people. Here, we report the results of a\nlarge-scale pre-registered online experiment (N = 3,552) indicating diminished\nfairness, trust, trustworthiness, cooperation, and coordination by human\nplayers in economic twoplayer games, when the decision of the interaction\npartner is taken over by ChatGPT. On the contrary, we observe no adverse\nwelfare effects when individuals are uncertain about whether they are\ninteracting with a human or generative AI. Therefore, the promotion of AI\ntransparency, often suggested as a solution to mitigate the negative impacts of\ngenerative AI on society, shows a detrimental effect on welfare in our study.\nConcurrently, participants frequently delegate decisions to ChatGPT,\nparticularly when the AI's involvement is undisclosed, and individuals struggle\nto discern between AI and human decisions.\n"
    },
    {
        "paper_id": 2401.12856,
        "authors": "Luca De Gennaro Aquino, Xuedong He, Moris Simon Strub, Yuting Yang",
        "title": "Reference-dependent asset pricing with a stochastic consumption-dividend\n  ratio",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We study a discrete-time consumption-based capital asset pricing model under\nexpectations-based reference-dependent preferences. More precisely, we consider\nan endowment economy populated by a representative agent who derives utility\nfrom current consumption and from gains and losses in consumption with respect\nto a forward-looking, stochastic reference point. First, we consider a general\nmodel in which the agent's preferences include both contemporaneous gain-loss\nutility, that is, utility from the difference between current consumption and\npreviously held expectations about current consumption, and prospective\ngain-loss utility, that is, utility from the difference between intertemporal\nbeliefs about future consumption. A semi-closed form solution for equilibrium\nasset prices is derived for this case. We then specialize to a model in which\nthe agent derives contemporaneous gain-loss utility only, obtaining equilibrium\nasset prices in closed form. Extensive numerical experiments show that, with\nplausible values of risk aversion and loss aversion, our models can generate\nequity premia that match empirical estimates. Interestingly, the models turn\nout to be consistent with some well-known empirical facts, namely procyclical\nvariation in the price-dividend ratio and countercyclical variation in the\nconditional expected equity premium and in the conditional volatility of the\nequity premium. Furthermore, we find that prospective gain-loss utility is\nnecessary for the model to predict reasonable values of the price-dividend\nratio.\n"
    },
    {
        "paper_id": 2401.13126,
        "authors": "Nakul Upadhya and Alexandre Granzer-Guay",
        "title": "Optimizing Transition Strategies for Small to Medium Sized Portfolios",
        "comments": "All of the discussed experiments and presented results can be\n  reproduced using our code at\n  https://github.com/upadhyan/Portfolio-Changeover-Optimization",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  This work discusses the benefits of constrained portfolio turnover strategies\nfor small to medium-sized portfolios. We propose a dynamic multi-period model\nthat aims to minimize transaction costs and maximize terminal wealth levels\nwhilst adhering to strict portfolio turnover constraints. Our results\ndemonstrate that using our framework in combination with a reasonable forecast,\ncan lead to higher portfolio values and lower transaction costs on average when\ncompared to a naive, single-period model. Such results were maintained given\ndifferent problem cases, such as, trading horizon, assets under management,\nwealth levels, etc. In addition, the proposed model lends itself to a\nreformulation that makes use of the column generation algorithm which can be\nstrategically leveraged to reduce complexity and solving times.\n"
    },
    {
        "paper_id": 2401.13159,
        "authors": "Elena M. Martinez, Nicole Tichenor Blackstone, Parke E. Wilde, Anna W.\n  Herforth, William A. Masters",
        "title": "Environmental impacts, nutritional profiles, and retail prices of\n  commonly sold retail food items in 181 countries: an observational study",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Affordability is often seen as a barrier to consuming sustainable diets. This\nstudy provides the first worldwide test of how retail food prices relate to\nempirically estimated environmental impacts and nutritional profile scores\nbetween and within food groups. We use prices for 811 retail food items\ncommonly sold in 181 countries during 2011 and 2017, matched to estimated\ncarbon and water footprints and nutritional profiles, to test whether healthier\nand more environmentally sustainable foods are more expensive between and\nwithin food groups. We find that within almost all groups, less expensive items\nhave significantly lower carbon and water footprints. Associations are\nstrongest for animal source foods, where each 10% lower price is associated\nwith 20 grams lower CO2-equivalent carbon and 5 liters lower water footprint\nper 100kcal. Gradients between price and nutritional profile vary by food\ngroup, price range, and nutritional attribute. In contrast, lower-priced items\nhave lower nutritional value in only some groups over some price ranges, and\nthat relationship is sometimes reversed. These findings reveal opportunities to\nreduce financial and environmental costs of diets, contributing to transitions\ntowards healthier, more environmentally sustainable food systems.\n"
    },
    {
        "paper_id": 2401.13314,
        "authors": "Lokman Abbas-Turki (LPSM), St\\'ephane Cr\\'epey (LPSM), Botao Li\n  (LPSM), Bouazza Saadeddine (LPSM, LaMME)",
        "title": "An Explicit Scheme for Pathwise XVA Computations",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Motivated by the equations of cross valuation adjustments (XVAs) in the\nrealistic case where capital is deemed fungible as a source of funding for\nvariation margin, we introduce a simulation/regression scheme for a class of\nanticipated BSDEs, where the coefficient entails a conditional expected\nshortfall of the martingale part of the solution. The scheme is explicit in\ntime and uses neural network least-squares and quantile regressions for the\nembedded conditional expectations and expected shortfall computations. An a\nposteriori Monte Carlo validation procedure allows assessing the regression\nerror of the scheme at each time step. The superiority of this scheme with\nrespect to Picard iterations is illustrated in a high-dimensional and hybrid\nmarket/default risks XVA use-case.\n"
    },
    {
        "paper_id": 2401.13399,
        "authors": "Marcel Bluhm (1), Adrian Cachinero Vasiljevi\\'c (2), S\\'ebastien\n  Derivaux (2), S{\\o}ren Terp H{\\o}rl\\\"uck Jessen (3) ((1) The Block, (2)\n  Steakhouse Financial Limited, (3) Balloonist ApS)",
        "title": "Real-time Risk Metrics for Programmatic Stablecoin Crypto\n  Asset-Liability Management (CALM)",
        "comments": "The authors would like to thank Professor Moorad Choudhry for review\n  comments on an earlier draft. Submitted for the SNB-CIF Conference on\n  Cryptoassets and Financial Innovation, 24 May 2024",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  Stablecoins have turned out to be the \"killer\" use case of the growing\ndigital asset space. However, risk management frameworks, including regulatory\nones, have been largely absent. In this paper, we address the critical question\nof measuring and managing risk in stablecoin protocols, which operate on public\nblockchain infrastructure. The on-chain environment makes it possible to\nmonitor risk and automate its management via transparent smart-contracts in\nreal-time. We propose two risk metrics covering capitalization and liquidity of\nstablecoin protocols. We then explore in a case-study type analysis how our\nrisk management framework can be applied to DAI, the biggest decentralized\nstablecoin by market capitalisation to-date, governed by MakerDAO. Based on our\nfindings, we recommend that the protocol explores implementing automatic\ncapital buffer adjustments and dynamic maturity gap matching. Our analysis\ndemonstrates the practical benefits for scalable (prudential) risk management\nstemming from real-time availability of high-quality, granular,\ntamper-resistant on-chain data in the digital asset space. We name this\napproach Crypto Asset-Liability Management (CALM).\n"
    },
    {
        "paper_id": 2401.1367,
        "authors": "Wenbo Lyu",
        "title": "\"The Roller Conduction Effect\" from the A-share Data Evidence",
        "comments": "11 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In the post-epidemic era, consumption recovery has obvious time and space\ntransmission laws, and there are different valuation criteria for consumption\nsegments. Using the A-share data of the consumption recovery stage from January\nto April 2022, this paper quantitatively compares the rotation effect between\ndifferent consumption sectors when the valuation returns to the reasonable\nrange. According to the new classification of \"sensory-based consumption\", it\ninterprets the internal logic of digital consumption as A consumption upgrade\ntool and a higher valuation target, and expounds the \"the roller conduction\neffect\". The law of consumption recovery and valuation return period is\nexplained from the perspective of time and space conduction. The study found\nthat in the early stage of consumption recovery, the recovery of consumer\nconfidence was slow. In this period, A-shares were mainly dominated by the\nstock capital game, and there was an obvious plate rotation law in the game.\nBeing familiar with this law has strong significance, which not only helps\npolicy makers to adjust the direction of policy guidance, but also helps\nfinancial investors to make better investment strategies. The disadvantage of\nthis paper is that it has not yet studied the roller conduction effect of the\nglobal financial market, and more rigorous mathematical models are still needed\nto support the definition of stock funds, which is also the main direction of\nthe author's future research.\n"
    },
    {
        "paper_id": 2401.13671,
        "authors": "Franck Ramaharo and Fitiavana Randriamifidy",
        "title": "Determinants of renewable energy consumption in Madagascar: Evidence\n  from feature selection algorithms",
        "comments": "21 pages, 4 tables, 1 figure",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  The aim of this note is to identify the factors influencing renewable energy\nconsumption in Madagascar. We tested 12 features covering macroeconomic,\nfinancial, social, and environmental aspects, including economic growth,\ndomestic investment, foreign direct investment, financial development,\nindustrial development, inflation, income distribution, trade openness,\nexchange rate, tourism development, environmental quality, and urbanization. To\nassess their significance, we assumed a linear relationship between renewable\nenergy consumption and these features over the 1990-2021 period. Next, we\napplied different machine learning feature selection algorithms classified as\nfilter-based (relative importance for linear regression, correlation method),\nembedded (LASSO), and wrapper-based (best subset regression, stepwise\nregression, recursive feature elimination, iterative predictor weighting\npartial least squares, Boruta, simulated annealing, and genetic algorithms)\nmethods. Our analysis revealed that the five most influential drivers stem from\nmacroeconomic aspects. We found that domestic investment, foreign direct\ninvestment, and inflation positively contribute to the adoption of renewable\nenergy sources. On the other hand, industrial development and trade openness\nnegatively affect renewable energy consumption in Madagascar.\n"
    },
    {
        "paper_id": 2401.13673,
        "authors": "Neha Deopa and Daniele Rinaldo",
        "title": "Sacred Ecology: The Environmental Impact of African Traditional\n  Religions",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
        "abstract": "  Do religions codify ecological principles? This paper explores theoretically\nand empirically the role religious beliefs play in shaping environmental\ninteractions. We study African Traditional Religions (ATR) which place forests\nwithin a sacred sphere. We build a model of non-market interactions of the\nmean-field type where the actions of agents with heterogeneous religious\nbeliefs continuously affect the spatial density of forest cover. The\nequilibrium extraction policy shows how individual beliefs and their\ndistribution among the population can be a key driver of forest conservation.\nThe model also characterizes the role of resource scarcity in both individual\nand population extraction decisions. We test the model predictions empirically\nrelying on the unique case of Benin, where ATR adherence is freely reported.\nUsing an instrumental variable strategy that exploits the variation in\nproximity to the Benin-Nigerian border, we find that a 1 standard deviation\nincrease in ATR adherence has a 0.4 standard deviation positive impact on\nforest cover change. We study the impact of historically belonging to the\nancient Kingdom of Dahomey, birthplace of the Vodun religion. Using the\noriginal boundaries as a spatial discontinuity, we find positive evidence of\nDahomey affiliation on contemporary forest change. Lastly, we compare observed\nforest cover to counterfactual outcomes by simulating the absence of ATR\nbeliefs across the population.\n"
    },
    {
        "paper_id": 2401.13674,
        "authors": "Alvear Guzman Katherine, Campozano Buele Jenner, Duran Canarte\n  Paulette, Holguin Cedeno Roger, Mejia Crespin Fernando",
        "title": "Analisis de la incidencia de la inversion extranjera directa y la\n  inversion nacional, en el crecimiento economico de Chile",
        "comments": "in Spanish language",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  The research aims to assess the impact of foreign direct investment (FDI) and\ndomestic investment on Chile's economic growth. By elucidating the relationship\nbetween FDI and domestic investment, the study contributes valuable insights\nfor economic policy formulation and future investments. The findings hold\nsignificance in shaping Chile's international perception as an investment\ndestination, potentially influencing its standing in the global economic\nlandscape. Demonstrating that FDI is a significant driver of economic growth\ncould enhance confidence among foreign investors. The project's importance lies\nin contributing to economic knowledge and guiding strategic decisions for\nsustainable economic growth in Chile. Understanding the interplay of FDI and\ndomestic investment allows for a balanced approach, promoting stable economic\ndevelopment and mitigating issues like excessive reliance on foreign\ninvestment. The study highlights the theory of internationalization as a\nconceptual framework for understanding the motives and strategies of\nmultinational companies investing abroad. Leveraging data from sources like the\nCentral Bank of Chile, the research analyzes variables such as Chile's economic\ngrowth (GDP), FDI, and domestic investment. The hypothesis posits a significant\nlong-term causal relationship between FDI, National Investment (NI), and\nChile's Economic Growth (GDP). Statistical analysis using the Eviews 6 software\ntool confirms that attracting foreign investments and promoting internal\ninvestment are imperative for sustainable economic growth in Chile.\n"
    },
    {
        "paper_id": 2401.13675,
        "authors": "Shteryo Nozharov",
        "title": "Social costs of curcular economy in European Union",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Two fundamental issues are incorporated in the present monograph: the issue\nrelated to the quantification of the social costs and the issue, related to the\ndefining of the circular economy concept as a theoretical model. The analysis\nis based on the methodology of the new institutional economics, which fact\ndistinguishes it from the many other circular economy analysis based on the\nneo-classical methodological apparatus.\n"
    },
    {
        "paper_id": 2401.13676,
        "authors": "Ziqi Wang",
        "title": "The impact of Hong Kong's anti-ELAB movement on political related firms",
        "comments": "34 pages, 13 tables",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/publicdomain/zero/1.0/",
        "abstract": "  Hong Kong's anti-ELAB movement had a significant impact on the stock market\nthe stock price of listed companies. Using the number of protestors as the\nmeasurement of daily protesting intensity from 2019/6/6 to 2020/1/17, this\npaper documents that the stock price of listed companies associated with the\npan-democratic parties were more negatively affected by protesting than other\ncompanies. Furthermore, this paper finds that after the implementation of the\nanti-mask law, protesting had a positive impact on red chips but a negative\nimpact on companies related to pan-democracy parties. Therefore, this paper\nbelieves that after the central government and the HKSAR government adopted\nstrict measures to stop violence and chaos, the value of the political\nconnection of red chips became positive while the value of the connection with\npan-democracy parties became negative.\n"
    },
    {
        "paper_id": 2401.13678,
        "authors": "Jose Aurelio Medina-Garrido, Jose Maria Biedma-Ferrer, Jaime\n  Sanchez-Ortiz",
        "title": "I Can't Go to Work Tomorrow! Work-Family Policies, Well-Being and\n  Absenteeism",
        "comments": null,
        "journal-ref": "Sustainability 12:5519 (2020)",
        "doi": "10.3390/su12145519",
        "license": "http://creativecommons.org/publicdomain/zero/1.0/",
        "abstract": "  Among the main causes of absenteeism are health problems, emotional problems,\nand inadequate work-family policies (WFP). This paper analyses the impact of\nthe existence and accessibility of WFP on work absenteeism, by considering the\nmediating role of the well-being, which includes emotional as well as physical\nor health problems, that is generated by these policies. We differentiate\nbetween the existence of the WFP and its accessibility, as the mere existence\nof the WFP in an organisation is not enough. Additionally, workers must be able\nto access these policies easily and without retaliation of any kind. The model\nincludes the hierarchy and the gender as moderating variables. To test the\nproposed hypotheses, a structural equation model based on the partial least\nsquares structural equation modelling (PLS-SEM) approach is applied to a sample\nof employees in the service sector in Spain. On the one hand, the findings show\nthat the existence of WFP has no direct effect on absenteeism; however,\naccessibility to these policies does have a direct effect on absenteeism. On\nthe other hand, both the existence and accessibility of WFP have positive\ndirect effects on emotional well-being. In addition, emotional well-being is\npositively related to physical well-being which, in turn, promotes a reduction\nin absenteeism. Finally, significant differences in the relationship between\nthe existence of WFP and emotional well-being confirm the special difficulty of\nfemale managers in reconciling family life and work life.\n"
    },
    {
        "paper_id": 2401.13679,
        "authors": "Miguel Angel Montanes-Del-Rio, Jose Aurelio Medina-Garrido",
        "title": "Determinants of the Propensity for Innovation among Entrepreneurs in the\n  Tourism Industry",
        "comments": null,
        "journal-ref": "Sustainability 12:5003 (2020)",
        "doi": "10.3390/su12125003",
        "license": "http://creativecommons.org/publicdomain/zero/1.0/",
        "abstract": "  Tourism's increasing share of Gross Domestic Product throughout the world,\nits impact on employment and its continuous growth justifies the interest it\nraises amongst entrepreneurs and public authorities. However, this growth\ncoexists with intense competition; as a result of which, tourism companies must\ncontinuously innovate in order to survive and grow. This is evident in the\ndiversification of tourism products and destinations, the improvement of\nbusiness processes and the incorporation of new technologies for\nintermediation, amongst other examples. This paper expounds on the factors that\nexplain the propensity for innovation amongst tourism entrepreneurs and it may\nhelp governments to promote innovation that is based on those determining\nfactors. The hypotheses are tested using a logistic regression on 699\ninternational tourism entrepreneurs, taken from the 2014 Global Adult\nPopulation Survey of the Global Entrepreneurship Monitor project. The\npropensity for innovation amongst tourism entrepreneurs has a statistically\nsignificant relationship to gender, age, level of education and informal\ninvestments in previous businesses.\n"
    },
    {
        "paper_id": 2401.13681,
        "authors": "Jose Aurelio Medina-Garrido, Jose Maria Biedma-Ferrer, Antonio Rafael\n  Ramos-Rodriguez",
        "title": "Moderating effects of gender and family responsibilities on the\n  relations between work-family policies and job performance",
        "comments": null,
        "journal-ref": "International Journal of Human Resource Management, 32 (2021)",
        "doi": "10.1080/09585192.2018.1505762",
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  This study analyzes the impact of work-family policies (WFP) on job\nperformance, and the possible moderating role of gender and family\nresponsibilities. Hypothesis testing was performed using a structural equation\nmodel based on a PLS-SEM approach applied to a sample of 1,511 employees of the\nSpanish banking sector. The results show that neither the existence nor the\naccessibility of the WFP has a direct, positive impact on performance, unlike\nwhat we expected, but both have an indirect effect via the well-being generated\nby these policies. We also find that neither gender nor family responsibilities\nhave a significant moderating role on these relations, contrary to what we\ninitially expected.\n"
    },
    {
        "paper_id": 2401.13682,
        "authors": "Antonio Rafael Ramos-Rodriguez, Jose Aurelio Medina-Garrido, Jose\n  Ruiz-Navarro",
        "title": "Why not now? Intended timing in entrepreneurial intentions",
        "comments": null,
        "journal-ref": "International Entrepreneurship and Management Journal,\n  15:1221-1246 (2019)",
        "doi": "10.1007/s11365-019-00586-5",
        "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
        "abstract": "  Purpose: Understanding the formation of entrepreneurial intentions is\ncritical, given that it is the first step in the entrepreneurial process.\nAlthough entrepreneurial intention has been extensively studied, little\nattention has been paid on the intended timing of future entrepreneurial\nprojects. This paper analyses entrepreneurial intentions among final-year\nuniversity students after graduation in terms of the timeframe to start a\nbusiness. Potentially rapid entrepreneurs and entrepreneurs-in-waiting were\ncompared using the Theory of Planned Behaviour (TPB). Methodology: A\nvariance-based structural equation modelling approach was used for the sample\nof 851 final-year university students with entrepreneurial intentions who\nparticipated in GUESSS project. Findings: The results obtained contribute to\nthe understanding of how entrepreneurial intentions are formed, particularly,\nhow intended timing plays a moderating role in the relationships of the\nvariables of the theoretical model of TPB. This study provides empirical\nevidence that significant differences exist between potential rapid\nentrepreneurs and entrepreneurs-in-waiting. Practical implications: The\nfindings of this study have practical implications for entrepreneurship\neducation, and they can help policy makers develop more effective policies and\nprograms to promote entrepreneurship. Originality: Intention-based models have\ntraditionally examined the intent -- but not the timing -- of new venture\ncreation. However, the time elapsed between the formation of the\nentrepreneurial intent and the identification of a business opportunity can\nvary considerably. Therefore, analysing the moderating role of intended timing\ncould be relevant to entrepreneurial intention research.\n"
    },
    {
        "paper_id": 2401.13683,
        "authors": "Jose Aurelio Medina-Garrido, Jose Maria Biedma-Ferrer, Antonio Rafael\n  Ramos-Rodriguez",
        "title": "Relationship between work-family balance, employee well-being and job\n  performance",
        "comments": null,
        "journal-ref": "Academia Revista Latinoamericana de Administracion, 30(1), pp.\n  40-58 (2017)",
        "doi": "10.1108/ARLA-08-2015-0202",
        "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
        "abstract": "  Purpose: To assess the impact of the existence of and access to different\nwork-family policies on employee well-being and job performance.\n  Design-methodology-approach: Hypothesis testing was performed using a\nstructural equation model based on a PLS-SEM approach applied to a sample of\n1,511 employees of the Spanish banking sector.\n  Findings: The results obtained demonstrate that the existence and true access\nto different types of work-family policies such as flexible working hours\n(flexi-time), long leaves, and flexible work location (flexi-place) are not\ndirectly related to job performance, but indirectly so, when mediated by the\nwell-being of employees generated by work-family policies. In a similar vein,\ntrue access to employee and family support services also has an indirect\npositive impact on job performance mediated by the well-being produced. In\ncontrast, the mere existence of employee and family support services does not\nhave any direct or indirect effect on job performance.\n  Originality-value: This study makes a theoretical and empirical contribution\nto better understand the impact that of the existence of and access to\nwork-family policies on job performance mediated by employee well-being. In\nthis sense, we posited and tested an unpublished theoretical model where the\nconcept of employee well-being gains special relevance at academic and\norganizational level due to its implications for human resource management.\n"
    },
    {
        "paper_id": 2401.13684,
        "authors": "Antonio Rafael Ramos-Rodriguez, Salustiano Martinez-Fierro, Jose\n  Aurelio Medina-Garrido, Jose Ruiz-Navarro",
        "title": "Global Entrepreneurship Monitor versus Panel Study of Entrepreneurial\n  Dynamics: comparing their intellectual structures",
        "comments": null,
        "journal-ref": "(2015). Global entrepreneurship monitor versus panel study of\n  entrepreneurial dynamics: comparing their intellectual structures.\n  International Entrepreneurship and Management Journal, 11(3), 571-597",
        "doi": "10.1007/S11365-013-0292-1",
        "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
        "abstract": "  In the past 15 years, two international observatories have been intensively\nstudying entrepreneurship using empirical studies with different methodologies:\nGEM and PSED. Both projects have generated a considerable volume of scientific\nproduction, and their intellectual structures are worth analyzing. The current\nwork is an exploratory study of the knowledge base of the articles generated by\neach of these two observatories and published in prestigious journals. The\nvalue added of this work lies in its novel characterization of the intellectual\nstructure of entrepreneurship according to the academic production of these two\ninitiatives. The results may be of interest to the managers and members of\nthese observatories, as well as to academics, researchers, sponsors and\npolicymakers interested in entrepreneurship.\n"
    },
    {
        "paper_id": 2401.13685,
        "authors": "Antonio Rafael Ramos-Rodriguez, Jose Aurelio Medina-Garrido, Jose\n  Ruiz-Navarro",
        "title": "Determinants of Hotels and Restaurants entrepreneurship: A study using\n  GEM data",
        "comments": null,
        "journal-ref": "International Journal of Hospitality Management, 31(2), 579-587\n  (2012)",
        "doi": "10.1016/j.ijhm.2011.08.003",
        "license": "http://creativecommons.org/publicdomain/zero/1.0/",
        "abstract": "  The objective of this work is to assess the influence of certain factors on\nthe likelihood of being a Hotels and Restaurants (H&R) entrepreneur. The\nfactors evaluated are demographic and economic variables, variables related to\nperceptions of the environment and personal traits, and variables measuring the\nindividual's intellectual and social capital. The work uses logistic regression\ntechniques to analyze a sample of 33,711 individuals in the countries\nparticipating in the GEM project in 2008. The findings show that age, gender,\nincome, perception of opportunities, fear of failure, entrepreneurial ability,\nknowing other entrepreneurs and being a business angel are explanatory factors\nof the probability of being an H&R entrepreneur.\n"
    },
    {
        "paper_id": 2401.13686,
        "authors": "Leonard Mushunje",
        "title": "Capturing the Tax-Revenue Bracketing System via a predator-prey model:\n  Evidence from South Africa",
        "comments": "18 Pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Revenues obtained from the corporate tax heads play significant roles in any\neconomy as they can be prioritized for producing public goods and employment\ncreations, among others. As such, corporate tax revenue should be paid enough\nattention. This study, therefore, explores the tax-revenue harvesting system of\nan economy where we focused on the corporate tax head. The system comprises\nthree players; the government and formal and informal firms. We applied the\npredator-prey model to model the effect of the government-gazetted tax rate on\ncorporate survivability. It is a new approach to modeling economic system\nrelations and games. Critical combinatory points are derived, with stability\nanalysis provided after that. Dynamics associated with the tax-revenue system\nare established and critically analyzed. Lastly, we provide the mathematical\nway the system can be optimized for the government to harvest as much Revenue\nas possible, including optimal conditions.\n"
    },
    {
        "paper_id": 2401.13687,
        "authors": "Anika Dixit",
        "title": "Econometric Approach to Analyzing Determinants of Sustained Prosperity",
        "comments": "15 pages including 7 tables, and references",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Every year, substantial resources are allocated to foreign aid with the aim\nof catalyzing prosperity and development in recipient countries. The diverse\nbody of research on the relationship between aid and gross domestic product\n(GDP) has yielded varying results, finding evidence of both positive, negative,\nand negligible associations between the two. This study employs econometric\ntechniques, namely Fully Modified Ordinary Least Squares Regression (FMOLS) and\nthe Generalized Method of Moments (GMM), to explore the intricate links between\ninnovation and different types of official development assistance (ODA) with\nthe overarching construct of prosperity. The paper also reviews the linkages\nbetween foundational metrics, such as the rule of law, education, and economic\ninfrastructure and services, in enabling self-sustaining prosperity. Drawing\nupon panel data of relevant determinants for 74 countries across the years 2013\nto 2021, the study found that there was a negligible relationship between both\nODA and innovation indices with prosperity. Notably, foreign aid targeted\nspecifically toward education was observed to have a positive impact on\nprosperity, as was the presence of rule of law in a state. The results of the\nstudy are then examined through the lens of a case-study on Reliance Jio,\nexemplifying how the company engineered an ecosystem that harnessed resources\nand facilitated infrastructure development, thereby contributing to\nself-sustaining economic growth and prosperity in India.\n"
    },
    {
        "paper_id": 2401.13688,
        "authors": "Osama A. Marzouk",
        "title": "In the Aftermath of Oil Prices Fall of 2014/2015-Socioeconomic Facts and\n  Changes in the Public Policies in the Sultanate of Oman",
        "comments": "17 pages, 16 figures, 5 tables, 44 references",
        "journal-ref": "International Journal of Management And Economics Invention,\n  Volume 3, Issue 11, Pages 1463-1479, 2017",
        "doi": "10.47191/ijmei/v3i11.09",
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Since the start of its national renaissance in 1970, the Sultanate of Oman\n(Oman) has gone over a major development in several areas, such as education,\ninfrastructure, and urbanization. This has been powered by the revenues from\nexporting crude oil and natural gas, which together form the skeleton of the\ncountry's economy. In the second half of 2014, the oil prices declined strongly\nto about 50% of its price. This was followed by another moderate decline in the\nsecond half of 2015 and the beginning of 2016, leaving the barrel price at a\nlow level below 30 US$ in January 2016 (as compared to above 110 US$ in June\n2014). This drop had direct impacts on the economy of Oman, manifested in a\nlarge budget deficit, reduced governmental expenditure, reduced or cancelled\nsubsidy of fuels and electricity, increase in the water tariff, and decline in\ndeposits in banks. The country is coping with this through its 9th five-year\nplan (2016-2020), which adopts a strategy of diversifying the income and\nrelying less on the traditional oil and gas sector. The country has also taken\nmeasures to facilitate private businesses. This article sheds light on these\ntopics as well as miscellaneous data about Oman.\n"
    },
    {
        "paper_id": 2401.13694,
        "authors": "David Roodman",
        "title": "The Arrival of Fast Internet and Employment in Africa: Comment",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
        "abstract": "  Hjort and Poulsen (2019) frames the staggered arrival of submarine Internet\ncables on the shores of Africa circa 2010 as a difference-in-differences\nnatural experiment. The paper finds positive impacts of broadband on\nindividual- and firm-level employment and nighttime light emissions. These\nresults largely are not robust to alternative geocoding of survey locations, to\ncorrecting for a satellite changeover at end-2009, and to revisiting a\ndefinition of the treated zone that has no clear technological basis, is\nnarrower than the spatial resolution of nearly all the data sources, and is\nempirically suboptimal as a representation of the geography of broadband.\n"
    },
    {
        "paper_id": 2401.1389,
        "authors": "Kyungsub Lee",
        "title": "Discrete Hawkes process with flexible residual distribution and filtered\n  historical simulation",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  We introduce a new model which can be considered as a extended version of the\nHawkes process in a discrete sense. This model enables the integration of\nvarious residual distributions while preserving the fundamental properties of\nthe original Hawkes process. The rich nature of this model enables a filtered\nhistorical simulation which incorporate the properties of original time series\nmore accurately. The process naturally extends to multi-variate models with\neasy implementations of estimation and simulation. We investigate the effect of\nflexible residual distribution on estimation of high frequency financial data\ncompared with the Hawkes process.\n"
    },
    {
        "paper_id": 2401.13977,
        "authors": "Tanmay Ghosh and Nithin Nagaraj",
        "title": "Evaluating the Determinants of Mode Choice Using Statistical and Machine\n  Learning Techniques in the Indian Megacity of Bengaluru",
        "comments": "65 pages, 26 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The decision making involved behind the mode choice is critical for\ntransportation planning. While statistical learning techniques like discrete\nchoice models have been used traditionally, machine learning (ML) models have\ngained traction recently among the transportation planners due to their higher\npredictive performance. However, the black box nature of ML models pose\nsignificant interpretability challenges, limiting their practical application\nin decision and policy making. This study utilised a dataset of $1350$\nhouseholds belonging to low and low-middle income bracket in the city of\nBengaluru to investigate mode choice decision making behaviour using\nMultinomial logit model and ML classifiers like decision trees, random forests,\nextreme gradient boosting and support vector machines. In terms of accuracy,\nrandom forest model performed the best ($0.788$ on training data and $0.605$ on\ntesting data) compared to all the other models. This research has adopted\nmodern interpretability techniques like feature importance and individual\nconditional expectation plots to explain the decision making behaviour using ML\nmodels. A higher travel costs significantly reduce the predicted probability of\nbus usage compared to other modes (a $0.66\\%$ and $0.34\\%$ reduction using\nRandom Forests and XGBoost model for $10\\%$ increase in travel cost). However,\nreducing travel time by $10\\%$ increases the preference for the metro ($0.16\\%$\nin Random Forests and 0.42% in XGBoost). This research augments the ongoing\nresearch on mode choice analysis using machine learning techniques, which would\nhelp in improving the understanding of the performance of these models with\nreal-world data in terms of both accuracy and interpretability.\n"
    },
    {
        "paper_id": 2401.14199,
        "authors": "Junwei Su, Shan Wu, Jinhui Li",
        "title": "MTRGL:Effective Temporal Correlation Discerning through Multi-modal\n  Temporal Relational Graph Learning",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  In this study, we explore the synergy of deep learning and financial market\napplications, focusing on pair trading. This market-neutral strategy is\nintegral to quantitative finance and is apt for advanced deep-learning\ntechniques. A pivotal challenge in pair trading is discerning temporal\ncorrelations among entities, necessitating the integration of diverse data\nmodalities. Addressing this, we introduce a novel framework, Multi-modal\nTemporal Relation Graph Learning (MTRGL). MTRGL combines time series data and\ndiscrete features into a temporal graph and employs a memory-based temporal\ngraph neural network. This approach reframes temporal correlation\nidentification as a temporal graph link prediction task, which has shown\nempirical success. Our experiments on real-world datasets confirm the superior\nperformance of MTRGL, emphasizing its promise in refining automated pair\ntrading strategies.\n"
    },
    {
        "paper_id": 2401.1439,
        "authors": "\\'Alvaro Guinea Juli\\'a and Alet Roux",
        "title": "Higher order approximation of option prices in Barndorff-Nielsen and\n  Shephard models",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We present an approximation method based on the mixing formula (Hull & White\n1987, Romano & Touzi 1997) for pricing European options in Barndorff-Nielsen\nand Shephard models. This approximation is based on a Taylor expansion of the\noption price. It is implemented using a recursive algorithm that allows us to\nobtain closed form approximations of the option price of any order (subject to\ntechnical conditions on the background driving L\\'evy process). This method can\nbe used for any type of Barndorff-Nielsen and Shephard stochastic volatility\nmodel. Explicit results are presented in the case where the stationary\ndistribution of the background driving L\\'evy process is inverse Gaussian or\ngamma. In both of these cases, the approximation compares favorably to option\nprices produced by the characteristic function. In particular, we also perform\nan error analysis of the approximation, which is partially based on the results\nof Das & Langren\\'e (2022). We obtain asymptotic results for the error of the\n$N^{\\text{th}}$ order approximation and error bounds when the variance process\nsatisfies an inverse Gaussian Ornstein-Uhlenbeck process or a gamma\nOrnstein-Uhlenbeck process.\n"
    },
    {
        "paper_id": 2401.14435,
        "authors": "Hans-Bernd Schaefer, Rok Spruk",
        "title": "Islamic Law, Western European Law and the Roots of Middle East's Long\n  Divergence: a Comparative Empirical Investigation (800-1600)",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  We examine the contribution of Islamic legal institutions to the comparative\neconomic decline of the Middle East behind Latin Europe, which can be observed\nsince the late Middle Ages. To this end, we explore whether the sacralization\nof Islamic law and its focus on the Sharia as supreme, sacred and unchangeable\nlegal text, which reached its culmination in the 13th century had an impact on\neconomic development. We use the population size of 145 cities in Islamic\ncountries and 648 European cities for the period 800-1800 as proxies for the\nlevel of economic development, and construct novel estimates of the number of\nlaw schools (i.e. madaris) and estimate their contribution to the\npre-industrial economic development. Our triple-differences estimates show that\na higher density of madrasas before the sacralization of Islamic law predicts a\nmore vibrant urban economy characterized by higher urban growth. After the\nconsolidation of the sharia sacralization of law in the 13th century, greater\ndensity of law schools is associated with stagnating population size. We show\nthat the economic decline of the Middle East can be partly explained by the\nabsence of legal innovations or substitutes of them, which paved the way for\nthe economic rise of Latin Europe, where ground-breaking legal reforms\nintroduced a series of legal innovations conducive for economic growth. We find\nthat the number of learned lawyers trained in universities with law schools is\nhighly and positively correlated with the western European city population. Our\ncounterfactual estimates show that almost all Islamic cities under\nconsideration would have had much larger size by the year 1700 if legal\ninnovations comparable to those in Western Europe were introduced. By making\nuse of a series of synthetic control and difference-in-differences estimators\nour findings are robust against a large number of model specification checks.\n"
    },
    {
        "paper_id": 2401.14443,
        "authors": "Giulia Di Nunno, Emanuela Rosazza Gianin",
        "title": "Cash non-additive risk measures: horizon risk and generalized entropy",
        "comments": "13 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Horizon risk (see arXiv:2301.04971) is studied in the context of cash\nnon-additive fully-dynamic risk measures induced by BSDEs. Furthermore, we\nintroduce a risk measure based on generalized Tsallis entropy which can\ndynamically evaluate the riskiness of losses considering both horizon risk and\ninterest rate uncertainty. The new q-entropic risk measure on losses can be\nused as a quantification of capital requirement.\n"
    },
    {
        "paper_id": 2401.14553,
        "authors": "Pepa Ram\\'irez-Cobo, Emilio Carrizosa and Rosa Elvira Lillo",
        "title": "Analysis of an aggregate loss model in a Markov renewal regime",
        "comments": null,
        "journal-ref": "Applied Mathematics and Computation (2021)",
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
        "abstract": "  In this article we consider an aggregate loss model with dependent losses.\nThe losses occurrence process is governed by a two-state Markovian arrival\nprocess (MAP2), a Markov renewal process process that allows for (1) correlated\ninter-losses times, (2) non-exponentially distributed inter-losses times and,\n(3) overdisperse losses counts. Some quantities of interest to measure\npersistence in the loss occurrence process are obtained. Given a real\noperational risk database, the aggregate loss model is estimated by fitting\nseparately the inter-losses times and severities. The MAP2 is estimated via\ndirect maximization of the likelihood function, and severities are modeled by\nthe heavy-tailed, double-Pareto Lognormal distribution. In comparison with the\nfit provided by the Poisson process, the results point out that taking into\naccount the dependence and overdispersion in the inter-losses times\ndistribution leads to higher capital charges.\n"
    },
    {
        "paper_id": 2401.14593,
        "authors": "Chudamani Poudyal",
        "title": "Robust Estimation of the Tail Index of a Single Parameter Pareto\n  Distribution from Grouped Data",
        "comments": "18 pages, 1 figure, 6 tables",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Numerous robust estimators exist as alternatives to the maximum likelihood\nestimator (MLE) when a completely observed ground-up loss severity sample\ndataset is available. However, the options for robust alternatives to MLE\nbecome significantly limited when dealing with grouped loss severity data, with\nonly a handful of methods like least squares, minimum Hellinger distance, and\noptimal bounded influence function available. This paper introduces a novel\nrobust estimation technique, the Method of Truncated Moments (MTuM),\nspecifically designed to estimate the tail index of a Pareto distribution from\ngrouped data. Inferential justification of MTuM is established by employing the\ncentral limit theorem and validating them through a comprehensive simulation\nstudy.\n"
    },
    {
        "paper_id": 2401.14672,
        "authors": "Wenyuan Wang, Kaixin Yan, Xiang Yu",
        "title": "Optimal portfolio under ratio-type periodic evaluation in incomplete\n  markets with stochastic factors",
        "comments": "28 pages, 33 conference",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper studies a type of periodic utility maximization for portfolio\nmanagement in an incomplete market model, where the underlying price diffusion\nprocess depends on some external stochastic factors. The portfolio performance\nis periodically evaluated on the relative ratio of two adjacent wealth levels\nover an infinite horizon. For both power and logarithmic utilities, we\nformulate the auxiliary one-period optimization problems with modified utility\nfunctions, for which we develop the martingale duality approach to establish\nthe existence of the optimal portfolio processes and the dual minimizers can be\nidentified as the \"least favorable\" completion of the market. With the help of\nthe duality results in the auxiliary problems and some fixed point arguments,\nwe further derive and verify the optimal portfolio processes in a periodic\nmanner for the original periodic evaluation problems over an infinite horizon.\n"
    },
    {
        "paper_id": 2401.14757,
        "authors": "Hannes Wallimann and Silvio Sticher",
        "title": "How to Use Data Science in Economics -- a Classroom Game Based on Cartel\n  Detection",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  We present a classroom game that integrates economics and data-science\ncompetencies. In the first two parts of the game, participants assume the roles\nof firms in a procurement market, where they must either adopt competitive\nbehaviors or have the option to engage in collusion. Success in these parts\nhinges on their comprehension of market dynamics. In the third part of the\ngame, participants transition to the role of competition-authority members.\nDrawing from recent literature on machine-learning-based cartel detection, they\nanalyze the bids for patterns indicative of collusive (cartel) behavior. In\nthis part of the game, success depends on data-science skills. We offer a\ndetailed discussion on implementing the game, emphasizing considerations for\naccommodating diverging levels of preexisting knowledge in data science.\n"
    },
    {
        "paper_id": 2401.14761,
        "authors": "Eeshaan Dutta and Sarthak Diwan and Siddhartha P. Chakrabarty",
        "title": "ESG driven pairs algorithm for sustainable trading: Analysis from the\n  Indian market",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  This paper proposes an algorithmic trading framework integrating\nEnvironmental, Social, and Governance (ESG) ratings with a pairs trading\nstrategy. It addresses the demand for socially responsible investment solutions\nby developing a unique algorithm blending ESG data with methods for identifying\nco-integrated stocks. This allows selecting profitable pairs adhering to ESG\nprinciples. Further, it incorporates technical indicators for optimal trade\nexecution within this sustainability framework. Extensive back-testing provides\nevidence of the model's effectiveness, consistently generating positive returns\nexceeding conventional pairs trading strategies, while upholding ESG\nprinciples. This paves the way for a transformative approach to algorithmic\ntrading, offering insights for investors, policymakers, and academics.\n"
    },
    {
        "paper_id": 2401.14945,
        "authors": "Kevin Bl\\\"attler, Hannes Wallimann and Widar von Arx",
        "title": "Free public transport to the destination: A causal analysis of tourists'\n  travel mode choice",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  In this paper, we assess the impact of a fare-free public transport policy\nfor overnight guests on travel mode choice to a Swiss tourism destination. The\npolicy directly targets domestic transport to and from a destination, the\nsubstantial contributor to the CO2 emissions of overnight trips. Based on a\nsurvey sample, we identify the effect with the help of the random element that\nthe information on the offer from a hotelier to the guest varies in day-to-day\nbusiness. We estimate a shift from private cars to public transport due to the\npolicy of, on average, 14.8 and 11.6 percentage points, depending on the\napplication of propensity score matching and causal forest. This knowledge is\nrelevant for policy-makers to design future offers that include more\nsustainable travels to a destination. Overall, our paper exemplifies how such\nan effect of comparable natural experiments in the travel and tourism industry\ncan be properly identified with a causal framework and underlying assumptions.\n"
    },
    {
        "paper_id": 2401.15069,
        "authors": "Aditya Ramji, Daniel Sperling, Lewis Fulton",
        "title": "Sustainable Market Incentives -- Lessons from European Feebates for a\n  ZEV Future",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Strong policies with sustainable incentives are needed to accelerate the EV\ntransition. This paper assesses various feebate designs assessing recent policy\nevolution in five European countries. While there are key design elements that\nshould be considered, there is no optimal feebate design. Different policy\nobjectives could be served by feebates influencing its design and\neffectiveness. Using feebates to transition to EVs has emerged a key objective.\nWith the financial sustainability of EV incentive programs being questioned, a\nself financing market mechanism could be the need of the hour solution.\nIrrespective of the policy goals, a feebate will impact both the supply side,\ni.e., the automotive industry and the consumer side. Globally, feebates can be\nused to effect technology leapfrogging while navigating the political economy\nof clean transportation policy in different country contexts. This paper\nhighlights thirteen design elements of an effective feebate policy that can\nserve as a foundation for policymakers.\n"
    },
    {
        "paper_id": 2401.15108,
        "authors": "Diwas Paudel and Tapas K. Das",
        "title": "Tacit algorithmic collusion in deep reinforcement learning guided price\n  competition: A study using EV charge pricing game",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-sa/4.0/",
        "abstract": "  Players in pricing games with complex structures are increasingly adopting\nartificial intelligence (AI) aided learning algorithms to make pricing\ndecisions for maximizing profits. This is raising concern for the antitrust\nagencies as the practice of using AI may promote tacit algorithmic collusion\namong otherwise independent players. Recent studies of games in canonical forms\nhave shown contrasting claims ranging from none to a high level of tacit\ncollusion among AI-guided players. In this paper, we examine the concern for\ntacit collusion by considering a practical game where EV charging hubs compete\nby dynamically varying their prices. Such a game is likely to be commonplace in\nthe near future as EV adoption grows in all sectors of transportation. The hubs\nsource power from the day-ahead (DA) and real-time (RT) electricity markets as\nwell as from in-house battery storage systems. Their goal is to maximize\nprofits via pricing and efficiently managing the cost of power usage. To aid\nour examination, we develop a two-step data-driven methodology. The first step\nobtains the DA commitment by solving a stochastic model. The second step\ngenerates the pricing strategies by solving a competitive Markov decision\nprocess model using a multi-agent deep reinforcement learning (MADRL)\nframework. We evaluate the resulting pricing strategies using an index for the\nlevel of tacit algorithmic collusion. An index value of zero indicates no\ncollusion (perfect competition) and one indicates full collusion (monopolistic\nbehavior). Results from our numerical case study yield collusion index values\nbetween 0.14 and 0.45, suggesting a low to moderate level of collusion.\n"
    },
    {
        "paper_id": 2401.15139,
        "authors": "Jasin Machkour, Daniel P. Palomar, Michael Muma",
        "title": "FDR-Controlled Portfolio Optimization for Sparse Financial Index\n  Tracking",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In high-dimensional data analysis, such as financial index tracking or\nbiomedical applications, it is crucial to select the few relevant variables\nwhile maintaining control over the false discovery rate (FDR). In these\napplications, strong dependencies often exist among the variables (e.g., stock\nreturns), which can undermine the FDR control property of existing methods like\nthe model-X knockoff method or the T-Rex selector. To address this issue, we\nhave expanded the T-Rex framework to accommodate overlapping groups of highly\ncorrelated variables. This is achieved by integrating a nearest neighbors\npenalization mechanism into the framework, which provably controls the FDR at\nthe user-defined target level. A real-world example of sparse index tracking\ndemonstrates the proposed method's ability to accurately track the S&P 500\nindex over the past 20 years based on a small number of stocks. An open-source\nimplementation is provided within the R package TRexSelector on CRAN.\n"
    },
    {
        "paper_id": 2401.15483,
        "authors": "Roberto Baviera and Pietro Manzoni",
        "title": "Fast and General Simulation of L\\'evy-driven OU processes for Energy\n  Derivatives",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  L\\'evy-driven Ornstein-Uhlenbeck (OU) processes represent an intriguing class\nof stochastic processes that have garnered interest in the energy sector for\ntheir ability to capture typical features of market dynamics. However, in the\ncurrent state-of-the-art, Monte Carlo simulations of these processes are not\nstraightforward for two main reasons: i) algorithms are available only for some\nparticular processes within this class; ii) they are often computationally\nexpensive. In this paper, we introduce a new simulation technique designed to\naddress both challenges. It relies on the numerical inversion of the\ncharacteristic function, offering a general methodology applicable to all\nL\\'evy-driven OU processes. Moreover, leveraging FFT, the proposed methodology\nensures fast and accurate simulations, providing a solid basis for the\nwidespread adoption of these processes in the energy sector. Lastly, the\nalgorithm allows an optimal control of the numerical error. We apply the\ntechnique to the pricing of energy derivatives, comparing the results with\nexisting benchmarks. Our findings indicate that the proposed methodology is at\nleast one order of magnitude faster than existing algorithms, all while\nmaintaining an equivalent level of accuracy.\n"
    },
    {
        "paper_id": 2401.15493,
        "authors": "Daniel H. Karney, Khyati Malik",
        "title": "The WTP-WTA Gap for Public Goods: New Insights from Compensating and\n  Equivalent Variation Closed-Form Solutions",
        "comments": "New title; New results related to EV (and associated theorems) and\n  certainty equivalence; Edits to Introduction with minor edits elsewhere;\n  Additional references",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  This study finds exact closed-form solutions for compensating variation (CV)\nand equivalent variation (EV) for both marginal and non-marginal changes in\npublic goods given homothetic utility. The parameters for these solutions are\nrecoverable from observable data in empirical applications as a single\nsufficient statistic summarizes consumer preferences. The closed-form CV and EV\nexpressions identify three economic mechanisms that determine the magnitudes of\nCV and EV. One of these mechanisms, the relative preference effect, helps\nexplain the disparity between willingness to pay (WTP) and willingness to\naccept (WTA) for public goods.\n"
    },
    {
        "paper_id": 2401.15552,
        "authors": "Erhan Bayraktar, Bingyan Han, Dominykas Norgilas",
        "title": "The McCormick martingale optimal transport",
        "comments": "29 pages, 2 figures, add new numerical results",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Martingale optimal transport (MOT) often yields broad price bounds for\noptions, constraining their practical applicability. In this study, we extend\nMOT by incorporating causality constraints among assets, inspired by the\nnonanticipativity condition of stochastic processes. However, this introduces a\ncomputationally challenging bilinear program. To tackle this issue, we propose\nMcCormick relaxations to ease the bicausal formulation and refer to it as\nMcCormick MOT. The primal attainment and strong duality of McCormick MOT are\nestablished under standard assumptions. Empirically, using the lower and upper\nbounds derived from marginal constraints, the McCormick relaxations reduce the\nprice gap by an average of 1% for stocks with liquid option markets and 4% for\nthose with moderately liquid markets. When tighter bounds on probability masses\nare applied, the average reduction increases to 12.66%.\n"
    },
    {
        "paper_id": 2401.1557,
        "authors": "Anindya Goswami and Kuldip Singh Patel",
        "title": "Estimation of domain truncation error for a system of PDEs arising in\n  option pricing",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  In this paper, a multidimensional system of parabolic partial differential\nequations arising in European option pricing under a regime-switching market\nmodel is studied in details. For solving that numerically, one must truncate\nthe domain and impose an artificial boundary data. By deriving an estimate of\nthe domain truncation error at all the points in the truncated domain, we\nextend some results in the literature those deal with option pricing equation\nunder constant regime case only. We differ from the existing approach to obtain\nthe error estimate that is sharper in certain region of the domain. Hence, the\nminimum of proposed and existing gives a strictly sharper estimate. A\ncomprehensive comparison with the existing literature is carried out by\nconsidering some numerical examples. Those examples confirm that the\nimprovement in the error estimates is significant.\n"
    },
    {
        "paper_id": 2401.15659,
        "authors": "Zongxia Liang, Keyu Zhang",
        "title": "A Mean Field Game Approach to Relative Investment-Consumption Games with\n  Habit Formation",
        "comments": "arXiv admin note: substantial text overlap with arXiv:2206.13341 by\n  other authors",
        "journal-ref": null,
        "doi": "10.1007/s11579-024-00360-4",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper studies an optimal investment-consumption problem for competitive\nagents with exponential or power utilities and a common finite time horizon.\nEach agent regards the average of habit formation and wealth from all peers as\nbenchmarks to evaluate the performance of her decision. We formulate the\nn-agent game problems and the corresponding mean field game problems under the\ntwo utilities. One mean field equilibrium is derived in a closed form in each\nproblem. In each problem with n agents, an approximate Nash equilibrium is then\nconstructed using the obtained mean field equilibrium when n is sufficiently\nlarge. The explicit convergence order in each problem can also be obtained. In\naddition, we provide some numerical illustrations of our results.\n"
    },
    {
        "paper_id": 2401.15728,
        "authors": "Aurelio Romero-Berm\\'udez and Colin Turfus",
        "title": "Analytic Pricing of SOFR Futures Contracts with Smile and Skew",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We introduce a perturbative formalism to solve the backward-looking futures\npricing problem. The formalism is based on a time-ordered exponential series\nwhich allows to derive the functional form of the integral kernel associated to\nthe backward-Kolmogorov diffusion PDE. We present an analytic pricing formula\nfor SOFR futures contracts under an extension of the Hull-White model which\nincorporates not only the intrinsic convexity adjustments captured by Mercurio\n[2018], but also the skew and smile observed in options markets as done in\nTurfus and Romero-Berm\\'udez [2023].\n"
    },
    {
        "paper_id": 2401.1603,
        "authors": "Yi Jiang and Richard S.J. Tol",
        "title": "Does green innovation crowd out other innovation of firms? Based on the\n  extended CDM model and unconditional quantile regressions",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In the era of sustainability, firms grapple with the decision of how much to\ninvest in green innovation and how it influences their economic trajectory.\nThis study employs the Crepon, Duguet, and Mairesse (CDM) framework to examine\nthe conversion of R&D funds into patents and their impact on productivity,\neffectively addressing endogeneity by utilizing predicted dependent variables\nat each stage to exclude unobservable factors. Extending the classical CDM\nmodel, this study contrasts green and non-green innovations' economic effects.\nThe results show non-green patents predominantly drive productivity gains,\nwhile green patents have a limited impact in non-heavy polluting firms.\nHowever, in high-pollution and manufacturing sectors, both innovation types\nequally enhance productivity. Using unconditional quantile regression, I found\ngreen innovation's productivity impact follows an inverse U-shape, unlike the\nU-shaped pattern of non-green innovation. Significantly, in the 50th to 80th\nproductivity percentiles of manufacturing and high-pollution firms, green\ninnovation not only contributes to environmental sustainability but also\noutperforms non-green innovation economically.\n"
    },
    {
        "paper_id": 2401.16286,
        "authors": "Dennis Schroers",
        "title": "Robust Functional Data Analysis for Stochastic Evolution Equations in\n  Infinite Dimensions",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We develop an asymptotic theory for the jump robust measurement of\ncovariations in the context of stochastic evolution equation in infinite\ndimensions. Namely, we identify scaling limits for realized covariations of\nsolution processes with the quadratic covariation of the latent random process\nthat drives the evolution equation which is assumed to be a Hilbert\nspace-valued semimartingale. We discuss applications to dynamically consistent\nand outlier-robust dimension reduction in the spirit of functional principal\ncomponents and the estimation of infinite-dimensional stochastic volatility\nmodels.\n"
    },
    {
        "paper_id": 2401.16455,
        "authors": "Namasi G. Sankar and Suryadeepto Nag and Siddhartha P. Chakrabarty and\n  Sankarshan Basu",
        "title": "The Carbon Premium: Correlation or Causation? Evidence from S&P 500\n  Companies",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  In the context of whether investors are aware of carbon-related risks, it is\noften hypothesized that there may be a carbon premium in the value of stocks of\nfirms, conferring an abnormal excess value to firms' shares as a form of\ncompensation to investors for their transition risk exposure through the\nownership of carbon instensive stocks. However, there is little consensus in\nthe literature regarding the existence of such a premium. Moreover few studies\nhave examined whether the correlation that is often observed is actually\ncausal. The pertinent question is whether more polluting firms give higher\nreturns or do firms with high returns have less incentive to decarbonize? In\nthis study, we investigate whether firms' emissions is causally linked to the\npresence of a carbon premium in a panel of 141 firms listed in the S\\&P500\nindex using fixed-effects analysis, with propensity score weighting to control\nfor selection bias in which firms increase their emissions. We find that there\nis a statistically significant positive carbon premium associated with Scope 1\nemissions, while there is no significant premium associated with Scope 2\nemissions, implying that risks associated with direct emissions by the firm are\npriced, while bought emissions are not.\n"
    },
    {
        "paper_id": 2401.16458,
        "authors": "Mario Sanz-Guerrero, Javier Arroyo",
        "title": "Credit Risk Meets Large Language Models: Building a Risk Indicator from\n  Loan Descriptions in P2P Lending",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Peer-to-peer (P2P) lending has emerged as a distinctive financing mechanism,\nlinking borrowers with lenders through online platforms. However, P2P lending\nfaces the challenge of information asymmetry, as lenders often lack sufficient\ndata to assess the creditworthiness of borrowers. This paper proposes a novel\napproach to address this issue by leveraging the textual descriptions provided\nby borrowers during the loan application process. Our methodology involves\nprocessing these textual descriptions using a Large Language Model (LLM), a\npowerful tool capable of discerning patterns and semantics within the text.\nTransfer learning is applied to adapt the LLM to the specific task at hand.\n  Our results derived from the analysis of the Lending Club dataset show that\nthe risk score generated by BERT, a widely used LLM, significantly improves the\nperformance of credit risk classifiers. However, the inherent opacity of\nLLM-based systems, coupled with uncertainties about potential biases,\nunderscores critical considerations for regulatory frameworks and engenders\ntrust-related concerns among end-users, opening new avenues for future research\nin the dynamic landscape of P2P lending and artificial intelligence.\n"
    },
    {
        "paper_id": 2401.16723,
        "authors": "Zhiyu Quan, Changyue Hu, Panyi Dong, Emiliano A. Valdez",
        "title": "Improving Business Insurance Loss Models by Leveraging InsurTech\n  Innovation",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Recent transformative and disruptive advancements in the insurance industry\nhave embraced various InsurTech innovations. In particular, with the rapid\nprogress in data science and computational capabilities, InsurTech is able to\nintegrate a multitude of emerging data sources, shedding light on opportunities\nto enhance risk classification and claims management. This paper presents a\ngroundbreaking effort as we combine real-life proprietary insurance claims\ninformation together with InsurTech data to enhance the loss model, a\nfundamental component of insurance companies' risk management. Our study\nfurther utilizes various machine learning techniques to quantify the predictive\nimprovement of the InsurTech-enhanced loss model over that of the insurance\nin-house. The quantification process provides a deeper understanding of the\nvalue of the InsurTech innovation and advocates potential risk factors that are\nunexplored in traditional insurance loss modeling. This study represents a\nsuccessful undertaking of an academic-industry collaboration, suggesting an\ninspiring path for future partnerships between industry and academic\ninstitutions.\n"
    },
    {
        "paper_id": 2401.16752,
        "authors": "Frank Y. Huang and Po-Chun Huang",
        "title": "Enhancing Urban Traffic Safety: An Evaluation on Taipei's Neighborhood\n  Traffic Environment Improvement Program",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  In densely populated urban areas, where interactions between pedestrians,\nvehicles, and motorcycles are frequent and complex, traffic safety is a\ncritical concern. This paper evaluates the Neighborhood Traffic Environment\nImprovement Program in Taipei, which involved painting green pedestrian paths,\nadjusting no-parking red/yellow lines, and painting speed limit and stop/slow\nsigns on lanes and alleys. Exploiting staggered rollout of policy\nimplementation and administrative traffic accident data, we found that the\nprogram reduced daytime traffic accidents by 5 percent and injuries by 8\npercent, while having no significant impact on nighttime incidents. The\neffectiveness of the program during the day is mainly attributed to the painted\ngreen sidewalks, with adequate sunlight playing a part in the program's\nsuccess. Our findings indicate that cost-effective strategies like green\npedestrian lanes can be effective in areas with dense populations and high\nmotorcycle traffic, as they improve safety by encouraging pedestrians to use\nmarked areas and deterring vehicles from these zones.\n"
    },
    {
        "paper_id": 2401.16754,
        "authors": "David Almog, Romain Gauriot, Lionel Page, Daniel Martin",
        "title": "AI Oversight and Human Mistakes: Evidence from Centre Court",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Powered by the increasing predictive capabilities of machine learning\nalgorithms, artificial intelligence (AI) systems have begun to be used to\noverrule human mistakes in many settings. We provide the first field evidence\nthis AI oversight carries psychological costs that can impact human\ndecision-making. We investigate one of the highest visibility settings in which\nAI oversight has occurred: the Hawk-Eye review of umpires in top tennis\ntournaments. We find that umpires lowered their overall mistake rate after the\nintroduction of Hawk-Eye review, in line with rational inattention given\npsychological costs of being overruled by AI. We also find that umpires\nincreased the rate at which they called balls in, which produced a shift from\nmaking Type II errors (calling a ball out when in) to Type I errors (calling a\nball in when out). We structurally estimate the psychological costs of being\noverruled by AI using a model of rational inattentive umpires, and our results\nsuggest that because of these costs, umpires cared twice as much about Type II\nerrors under AI oversight.\n"
    },
    {
        "paper_id": 2401.1692,
        "authors": "Anubha Goel, Damir Filipovi\\'c, Puneet Pasricha",
        "title": "Sparse Portfolio Selection via Topological Data Analysis based\n  Clustering",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  This paper uses topological data analysis (TDA) tools and introduces a\ndata-driven clustering-based stock selection strategy tailored for sparse\nportfolio construction. Our asset selection strategy exploits the topological\nfeatures of stock price movements to select a subset of topologically similar\n(different) assets for a sparse index tracking (Markowitz) portfolio. We\nintroduce new distance measures, which serve as an input to the clustering\nalgorithm, on the space of persistence diagrams and landscapes that consider\nthe time component of a time series. We conduct an empirical analysis on the\nS\\&P index from 2009 to 2020, including a study on the COVID-19 data to\nvalidate the robustness of our methodology. Our strategy to integrate TDA with\nthe clustering algorithm significantly enhanced the performance of sparse\nportfolios across various performance measures in diverse market scenarios.\n"
    },
    {
        "paper_id": 2401.17047,
        "authors": "Cristina Pereira, Herm\\'inia Gon\\c{c}alves, Teresa Sequeira",
        "title": "Determinants of well-being",
        "comments": "23 pages, 4 figures, 5 tables",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Traditionally, European social policies have focused on material well-being\nand social justice, neglecting subjective indicators. This review\nsystematically examines the scientific understanding of well-being, its\nindicators, and its relationship with governance. It suggests that political\nsystems and institutions significantly impact well-being, and that subjective\nindicators should be incorporated into public policy decisions. The findings\nadvocate for a more holistic approach to well-being measurement, encompassing\nboth objective and subjective dimensions.\n"
    },
    {
        "paper_id": 2401.17265,
        "authors": "Yi Shen, Zachary Van Oosten, Ruodu Wang",
        "title": "Partial Law Invariance and Risk Measures",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  We introduce the concept of partial law invariance, generalizing the concepts\nof law invariance and probabilistic sophistication widely used in decision\ntheory, as well as statistical and financial applications. This new concept is\nmotivated by practical considerations of decision making under uncertainty,\nthus connecting the literature on decision theory and that on financial risk\nmanagement. We fully characterize partially law-invariant coherent risk\nmeasures via a novel representation formula. Strong partial law invariance is\ndefined to bridge the gap between the above characterization and the classic\nrepresentation formula of Kusuoka. We propose a few classes of new risk\nmeasures, including partially law-invariant versions of the Expected Shortfall\nand the entropic risk measures, and illustrate their applications in risk\nassessment under different types of uncertainty. We provide a tractable\noptimization formula for computing a class of partially law-invariant coherent\nrisk measures and give a numerical example.\n"
    },
    {
        "paper_id": 2401.17329,
        "authors": "Sina Sahebi, Sahand Heshami, Mohammad Khojastehpour, Ali Rahimi,\n  Mahyar Mollajani",
        "title": "Assessing Public Perception of Car Automation in Iran: Acceptance and\n  Willingness to Pay for Adaptive Cruise Control",
        "comments": null,
        "journal-ref": null,
        "doi": "10.1016/j.iatssr.2024.04.002",
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  Adaptive cruise control (ACC) is a technology that can reduce fuel\nconsumption and air pollution in the automotive industry. However, its\navailability in Iran is low compared to industrialized countries. This study\nexamines the acceptance and willingness to pay (WTP) for ACC among Iranian\ndrivers. Data from an online survey of 453 respondents were analyzed using the\nTechnology Acceptance Model (TAM) and an ordered logit model. The results show\nthat perceived ease of use and perceived usefulness affect attitudes toward\nusing ACC, which in turn influence behavioral intentions. The logit model also\nshows that drivers who find ACC easy and useful, who have higher vehicle\nprices, and who are women with cruise control (CC) experience are more likely\nto pay for ACC. To increase the adoption of ACC in Iran, it is suggested to\ntarget early adopters, especially women and capitalists, who can influence\nothers with their positive feedback. The benefits of ACC for traffic safety and\nenvironmental sustainability should also be emphasized.\n"
    },
    {
        "paper_id": 2401.17334,
        "authors": "Ivan Medovikov and Valentyn Panchenko and Artem Prokhorov",
        "title": "Efficient estimation of parameters in marginals in semiparametric\n  multivariate models",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  We consider a general multivariate model where univariate marginal\ndistributions are known up to a parameter vector and we are interested in\nestimating that parameter vector without specifying the joint distribution,\nexcept for the marginals. If we assume independence between the marginals and\nmaximize the resulting quasi-likelihood, we obtain a consistent but inefficient\nQMLE estimator. If we assume a parametric copula (other than independence) we\nobtain a full MLE, which is efficient but only under a correct copula\nspecification and may be biased if the copula is misspecified. Instead we\npropose a sieve MLE estimator (SMLE) which improves over QMLE but does not have\nthe drawbacks of full MLE. We model the unknown part of the joint distribution\nusing the Bernstein-Kantorovich polynomial copula and assess the resulting\nimprovement over QMLE and over misspecified FMLE in terms of relative\nefficiency and robustness. We derive the asymptotic distribution of the new\nestimator and show that it reaches the relevant semiparametric efficiency\nbound. Simulations suggest that the sieve MLE can be almost as efficient as\nFMLE relative to QMLE provided there is enough dependence between the\nmarginals. We demonstrate practical value of the new estimator with several\napplications. First, we apply SMLE in an insurance context where we build a\nflexible semi-parametric claim loss model for a scenario where one of the\nvariables is censored. As in simulations, the use of SMLE leads to tighter\nparameter estimates. Next, we consider financial risk management examples and\nshow how the use of SMLE leads to superior Value-at-Risk predictions. The paper\ncomes with an online archive which contains all codes and datasets.\n"
    },
    {
        "paper_id": 2401.17384,
        "authors": "Molly J Doruska, Christopher B Barrett, Jason R Rohr",
        "title": "Modeling how and why aquatic vegetation removal can free rural\n  households from poverty-disease traps",
        "comments": "30 pages, 4 figures, Supplemental materials: 27 pages, 10 tables, 4\n  figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  Infectious disease can reduce labor productivity and incomes, trapping\nsubpopulations in a vicious cycle of ill health and poverty. Efforts to boost\nAfrican farmers' agricultural production through fertilizer use can\ninadvertently promote the growth of aquatic vegetation that hosts disease\nvectors. Recent trials established that removing aquatic vegetation habitat for\nsnail intermediate hosts reduces schistosomiasis infection rates in children,\nwhile converting the harvested vegetation into compost boosts agricultural\nproductivity and incomes. Our model illustrates how this ecological\nintervention changes the feedback between the human and natural systems,\npotentially freeing rural households from poverty-disease traps. We develop a\nbioeconomic model that interacts an analytical microeconomic model of\nagricultural households' behavior, health status and incomes over time with a\ndynamic model of schistosomiasis disease ecology. We calibrate the model with\nfield data from northern Senegal. We show analytically and via simulation that\nlocal conversion of invasive aquatic vegetation to compost changes the\nfeedbacks among interlinked disease, aquatic and agricultural systems, reducing\nschistosomiasis infection and increasing incomes relative to the current status\nquo, in which villagers rarely remove vegetation. Aquatic vegetation removal\ndisrupts the poverty-disease trap by reducing habitat for snails that vector\nthe infectious helminth and by promoting production of compost that returns to\nagricultural soils nutrients that currently leach into surface water from\non-farm fertilizer applications. The result is healthier people, more\nproductive labor, cleaner water, more productive agriculture, and higher\nincomes.\n"
    },
    {
        "paper_id": 2401.17391,
        "authors": "Christelle Zozoungbo",
        "title": "Education Policy and Intergenerational Educational Persistence: Evidence\n  from rural Benin",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper employs a nonlinear difference-in-differences approach to\nempirically examine the Maximally Maintained Inequality (MMI) hypothesis in\nrural Benin. The findings of this study confirm the MMI hypothesis. In\nparticular, it is observed that when 76% of educated parents choose to educate\ntheir daughters in the absence of educational programs, in contrast to only 37%\namong non-educated parents, the average impact of tuition fee subsidy on\nenrollment probability in primary schools stands at 3.8\\% for non-educated\nhouseholds and 0.27% for educated households. Conversely, in cases where only\n27% of educated parents decide to educate their daughters without education\nprograms, the average effect of tuition fee waivers on enrollment probability\nin primary schools increases to 19.64\\% for non-educated households and 24\\%\nfor educated households. From the analysis of household education decisions\ninfluenced by a preference for education and budget constraints, three key\nconclusions emerge to explain the mechanism behind the MMI. Firstly, when the\nincome advantage of educated households compared to non-educated households is\nsignificantly high, irrespective of the level of their preference advantage,\nreducing the financial cost of education induces a greater shift in education\ndecisions among non-educated households. Secondly, in situations where educated\nhouseholds do not possess an income advantage relative to non-educated\nhouseholds, the reduction in education-related financial costs leads to a more\npronounced change in education decisions among educated households. Lastly, for\nthe low-income advantage of educated households, as the income advantage of\neducated households increases, non-educated households respond more to\neducation policy than educated parents, if the preference advantage of educated\nhouseholds is relatively smaller.\n"
    },
    {
        "paper_id": 2401.17448,
        "authors": "Antonio Rafael Ramos-Rodriguez, Jose Aurelio Medina-Garrido, Jose\n  Daniel Lorenzo-Gomez, Jose Ruiz-Navarro",
        "title": "What you know or who you know? The role of intellectual and social\n  capital in opportunity recognition",
        "comments": null,
        "journal-ref": "International Small Business Journal (2010) 28(6) 566-582",
        "doi": "10.1177/0266242610369753",
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  The recognition of business opportunities is the first stage in the\nentrepreneurial process. The current work analyzes the effects of individuals'\npossession of and access to knowledge on the probability of recognizing good\nbusiness opportunities in their area of residence. The authors use an eclectic\ntheoretical framework consisting of intellectual and social capital concepts.\nIn particular, they analyze the role of individuals' educational level, their\nperception that they have the right knowledge and skills to start a business,\nwhether they own and manage a firm, their contacts with other entrepreneurs,\nand whether they have been business angels. The hypotheses proposed here are\ntested using data collected for the GEM project in Spain in 2007. The results\nshow that individuals' access to external knowledge through the social networks\nin which they participate is fundamental for developing the capacity to\nrecognize new business opportunities.\n"
    },
    {
        "paper_id": 2401.17472,
        "authors": "Zhipeng Huang, Balint Negyesi, Cornelis W. Oosterlee",
        "title": "Convergence of the deep BSDE method for stochastic control problems\n  formulated through the stochastic maximum principle",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  It is well-known that decision-making problems from stochastic control can be\nformulated by means of a forward-backward stochastic differential equation\n(FBSDE). Recently, the authors of Ji et al. 2022 proposed an efficient deep\nlearning algorithm based on the stochastic maximum principle (SMP). In this\npaper, we provide a convergence result for this deep SMP-BSDE algorithm and\ncompare its performance with other existing methods. In particular, by adopting\na strategy as in Han and Long 2020, we derive a-posteriori estimate, and show\nthat the total approximation error can be bounded by the value of the loss\nfunctional and the discretization error. We present numerical examples for\nhigh-dimensional stochastic control problems, both in case of drift- and\ndiffusion control, which showcase superior performance compared to existing\nalgorithms.\n"
    },
    {
        "paper_id": 2401.17578,
        "authors": "Cassidy Shubatt and Jeffrey Yang",
        "title": "Tradeoffs and Comparison Complexity",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  This paper develops a theory of how tradeoffs govern comparison complexity,\nand how this complexity generates systematic mistakes in choice. In our model,\noptions are easier to compare when they involve less pronounced tradeoffs, in\nparticular when they are 1) more similar feature-by-feature and 2) closer to\ndominance. These two postulates yield tractable measures of comparison\ncomplexity in the domains of multiattribute, lottery, and intertemporal choice.\nWe then show how behavioral regularities in choice and valuation, such as\ncontext effects, preference reversals, and apparent probability weighting and\nhyperbolic discounting in valuations, can be understood as responses to\ncomparison complexity. We test our model experimentally by varying the strength\nand nature of tradeoffs. First, we show that our complexity measures predict\nchoice errors, choice inconsistency, and cognitive uncertainty in binary choice\ndata across all three domains. Second, we document that manipulations of\ncomparison complexity can reverse classic behavioral regularities, in line with\nthe predictions of the theory.\n"
    },
    {
        "paper_id": 2401.17886,
        "authors": "John Paul P. Miranda, Maria Anna D. Cruz, Dina D. Gonzales, Ma.\n  Rebecca G. Del Rosario, Aira May B. Canlas, Joseph Alexander Bansil",
        "title": "Filipino Use of Designer and Luxury Perfumes: A Pilot Study of Consumer\n  Behavior",
        "comments": "14 pages, 4 tables, journal article, peer-reviewed",
        "journal-ref": "Puissant, 5, 2014-2027, 2024",
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  This study investigates the usage patterns and purposes of designer perfumes\namong Filipino consumers, employing purposive and snowball sampling methods as\nnon-probability sampling techniques. Data was collected using Google Forms, and\nthe majority of respondents purchased full bottles of designer perfumes from\nretailers, wholesalers, and physical stores, with occasional \"blind purchases.\"\nDaily usage was common, with respondents applying an average of 5.88 sprays in\nthe morning, favoring fresh scent notes and Eau De Parfum concentration. They\ntended to alternate perfumes daily, selecting different scent profiles\naccording to the Philippine climate. The study reveals that Filipino\nrespondents primarily use designer perfumes to achieve a pleasant and fresh\nfragrance. Additionally, these perfumes play a role in boosting self-esteem,\nelevating mood, and enhancing personal presentation. Some respondents reported\nfewer common applications, such as using perfume to address insomnia and\nmigraines. Overall, the research highlights the significant role of perfume in\nthe grooming routine of Filipino consumers. This study represents the first\nattempt to comprehend perfume usage patterns and purposes specifically within\nthe Filipino context. Consequently, its findings are invaluable for\nmanufacturers and marketers targeting the Filipino market, providing insights\ninto consumer preferences and motivations.\n"
    },
    {
        "paper_id": 2401.17929,
        "authors": "Alexander Erlei, Lukas Meub",
        "title": "Technological Shocks and Algorithmic Decision Aids in Credence Goods\n  Markets",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In credence goods markets such as health care or repair services, consumers\nrely on experts with superior information to adequately diagnose and treat\nthem. Experts, however, are constrained in their diagnostic abilities, which\nhurts market efficiency and consumer welfare. Technological breakthroughs that\nsubstitute or complement expert judgments have the potential to alleviate\nconsumer mistreatment. This article studies how competitive experts adopt novel\ndiagnostic technologies when skills are heterogeneously distributed and\nobfuscated to consumers. We differentiate between novel technologies that\nincrease expert abilities, and algorithmic decision aids that complement expert\njudgments, but do not affect an expert's personal diagnostic precision. We show\nthat high-ability experts may be incentivized to forego the decision aid in\norder to escape a pooling equilibrium by differentiating themselves from\nlow-ability experts. Results from an online experiment support our hypothesis,\nshowing that high-ability experts are significantly less likely than\nlow-ability experts to invest into an algorithmic decision aid. Furthermore, we\ndocument pervasive under-investments, and no effect on expert honesty.\n"
    },
    {
        "paper_id": 2401.17971,
        "authors": "Davide Fiaschi, Cristina Tealdi",
        "title": "Let's roll back! The challenging task of regulating temporary contracts",
        "comments": "30 pages, 10 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper, we evaluate the impact of a reform introduced in Italy in 2018\n(Decreto Dignit\\`a), which increased the rigidity of employment protection\nlegislation (EPL) of temporary contracts, rolling back previous policies, to\nreduce job instability. We use longitudinal labour force data from 2016 to 2019\nand adopt a time-series technique within a Rubin Casual Model (RCM) framework\nto estimate the causal effect of the reform. We find that the reform was\nsuccessful in reducing persistence into temporary employment and increasing the\nflow from temporary to permanent employment, in particular among women and\nyoung workers in the North of Italy, with significant effects on the stocks of\npermanent employment (+), temporary employment (-) and unemployment (-).\nHowever, this positive outcome came at the cost of higher persistence into\ninactivity, lower outflows from unemployment to temporary employment and higher\noutflows from unemployment to inactivity among males and low-educated workers.\n"
    },
    {
        "paper_id": 2402.00299,
        "authors": "Sahab Zandi, Kamesh Korangi, Mar\\'ia \\'Oskarsd\\'ottir, Christophe\n  Mues, Cristi\\'an Bravo",
        "title": "Attention-based Dynamic Multilayer Graph Neural Networks for Loan\n  Default Prediction",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  Whereas traditional credit scoring tends to employ only individual borrower-\nor loan-level predictors, it has been acknowledged for some time that\nconnections between borrowers may result in default risk propagating over a\nnetwork. In this paper, we present a model for credit risk assessment\nleveraging a dynamic multilayer network built from a Graph Neural Network and a\nRecurrent Neural Network, each layer reflecting a different source of network\nconnection. We test our methodology in a behavioural credit scoring context\nusing a dataset provided by U.S. mortgage financier Freddie Mac, in which\ndifferent types of connections arise from the geographical location of the\nborrower and their choice of mortgage provider. The proposed model considers\nboth types of connections and the evolution of these connections over time. We\nenhance the model by using a custom attention mechanism that weights the\ndifferent time snapshots according to their importance. After testing multiple\nconfigurations, a model with GAT, LSTM, and the attention mechanism provides\nthe best results. Empirical results demonstrate that, when it comes to\npredicting probability of default for the borrowers, our proposed model brings\nboth better results and novel insights for the analysis of the importance of\nconnections and timestamps, compared to traditional methods.\n"
    },
    {
        "paper_id": 2402.00445,
        "authors": "Takuji Arai and Yuto Imai",
        "title": "Option pricing for Barndorff-Nielsen and Shephard model by supervised\n  deep learning",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  This paper aims to develop a supervised deep-learning scheme to compute call\noption prices for the Barndorff-Nielsen and Shephard model with a\nnon-martingale asset price process having infinite active jumps. In our deep\nlearning scheme, teaching data is generated through the Monte Carlo method\ndeveloped by Arai and Imai (2024). Moreover, the BNS model includes many\nvariables, which makes the deep learning accuracy worse. Therefore, we will\ncreate another input variable using the Black-Scholes formula. As a result, the\naccuracy is improved dramatically.\n"
    },
    {
        "paper_id": 2402.00515,
        "authors": "Zhenglong Li, Vincent Tam, Kwan L. Yeung",
        "title": "Developing A Multi-Agent and Self-Adaptive Framework with Deep\n  Reinforcement Learning for Dynamic Portfolio Risk Management",
        "comments": "Accepted by The 23rd International Conference on Autonomous Agents\n  and Multi-Agent Systems",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
        "abstract": "  Deep or reinforcement learning (RL) approaches have been adapted as reactive\nagents to quickly learn and respond with new investment strategies for\nportfolio management under the highly turbulent financial market environments\nin recent years. In many cases, due to the very complex correlations among\nvarious financial sectors, and the fluctuating trends in different financial\nmarkets, a deep or reinforcement learning based agent can be biased in\nmaximising the total returns of the newly formulated investment portfolio while\nneglecting its potential risks under the turmoil of various market conditions\nin the global or regional sectors. Accordingly, a multi-agent and self-adaptive\nframework namely the MASA is proposed in which a sophisticated multi-agent\nreinforcement learning (RL) approach is adopted through two cooperating and\nreactive agents to carefully and dynamically balance the trade-off between the\noverall portfolio returns and their potential risks. Besides, a very flexible\nand proactive agent as the market observer is integrated into the MASA\nframework to provide some additional information on the estimated market trends\nas valuable feedbacks for multi-agent RL approach to quickly adapt to the\never-changing market conditions. The obtained empirical results clearly reveal\nthe potential strengths of our proposed MASA framework based on the multi-agent\nRL approach against many well-known RL-based approaches on the challenging data\nsets of the CSI 300, Dow Jones Industrial Average and S&P 500 indexes over the\npast 10 years. More importantly, our proposed MASA framework shed lights on\nmany possible directions for future investigation.\n"
    },
    {
        "paper_id": 2402.00543,
        "authors": "Reza Salimi, Kamran Pakizeh",
        "title": "The extension of Pearson correlation coefficient, measuring noise, and\n  selecting features",
        "comments": "24 pages, 40 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Not a matter of serious contention, Pearson's correlation coefficient is\nstill the most important statistical association measure. Restricted to just\ntwo variables, this measure sometimes doesn't live up to users' needs and\nexpectations. Specifically, a multivariable version of the correlation\ncoefficient can greatly contribute to better assessment of the risk in a\nmulti-asset investment portfolio. Needless to say, the correlation coefficient\nis derived from another concept: covariance. Even though covariance can be\nextended naturally by its mathematical formula, such an extension is to no use.\nMaking matters worse, the correlation coefficient can never be extended based\non its mathematical definition. In this article, we briefly explore random\nmatrix theory to extend the notion of Pearson's correlation coefficient to an\narbitrary number of variables. Then, we show that how useful this measure is at\ngauging noise, thereby selecting features particularly in classification.\n"
    },
    {
        "paper_id": 2402.00787,
        "authors": "Benjamin Patrick Evans, Sumitra Ganesh",
        "title": "Learning and Calibrating Heterogeneous Bounded Rational Market Behaviour\n  with Multi-Agent Reinforcement Learning",
        "comments": "Accepted as a full paper at AAMAS 2024",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Agent-based models (ABMs) have shown promise for modelling various real world\nphenomena incompatible with traditional equilibrium analysis. However, a\ncritical concern is the manual definition of behavioural rules in ABMs. Recent\ndevelopments in multi-agent reinforcement learning (MARL) offer a way to\naddress this issue from an optimisation perspective, where agents strive to\nmaximise their utility, eliminating the need for manual rule specification.\nThis learning-focused approach aligns with established economic and financial\nmodels through the use of rational utility-maximising agents. However, this\nrepresentation departs from the fundamental motivation for ABMs: that realistic\ndynamics emerging from bounded rationality and agent heterogeneity can be\nmodelled. To resolve this apparent disparity between the two approaches, we\npropose a novel technique for representing heterogeneous processing-constrained\nagents within a MARL framework. The proposed approach treats agents as\nconstrained optimisers with varying degrees of strategic skills, permitting\ndeparture from strict utility maximisation. Behaviour is learnt through\nrepeated simulations with policy gradients to adjust action likelihoods. To\nallow efficient computation, we use parameterised shared policy learning with\ndistributions of agent skill levels. Shared policy learning avoids the need for\nagents to learn individual policies yet still enables a spectrum of bounded\nrational behaviours. We validate our model's effectiveness using real-world\ndata on a range of canonical $n$-agent settings, demonstrating significantly\nimproved predictive capability.\n"
    },
    {
        "paper_id": 2402.00855,
        "authors": "Jan L.M. Dhaene and Moshe A. Milevsky",
        "title": "'Egalitarian pooling and sharing of longevity risk', a.k.a. 'The many\n  ways to skin a tontine cat'",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  There is little disagreement among insurance actuaries and financial\neconomists about the societal benefits of longevity-risk pooling in the form of\nlife annuities, defined benefit pensions, self-annuitization funds, and even\ntontine schemes. Indeed, the discounted value or cost of providing an income\nfor life is lower -- in other words, the amount of upfront capital required to\ngenerate a similar income stream with the same level of statistical safety is\nlower -- when participants pool their financial resources versus going it\nalone. Moreover, when participants' financial circumstances and lifespans are\nhomogenous, there is consensus on how to share the \"winnings\" among survivors,\nnamely by distributing them equally among survivors, a.k.a. a uniform rule.\nAlas, what is lesser-known and much more problematic is allocating the winnings\nin such a pool when participants differ in wealth (contributions) and health\n(longevity), especially when the pools are relatively small in size. The same\nproblems arise when viewed from the dual perspective of decentralized risk\nsharing (DRS). The positive correlation between health and income and the fact\nthat wealthier participants are likely to live longer is a growing concern\namong pension and retirement policymakers. With that motivation in mind, this\npaper offers a modelling framework for distributing longevity-risk pools'\nincome and benefits (or tontine winnings) when participants are heterogeneous.\nSimilar to the nascent literature on decentralized risk sharing, there are\nseveral equally plausible arrangements for sharing benefits (a.k.a. \"skinning\nthe cat\") among survivors. Moreover, the selected rule depends on the extent of\nsocial cohesion within the longevity risk pool, ranging from solidarity and\naltruism to pure individualism. In sum, actuarial science cannot really offer\nor guarantee uniqueness, only a methodology.\n"
    },
    {
        "paper_id": 2402.01007,
        "authors": "Avital Baral, Taylor Reynolds, Lawrence Susskind, Daniel J. Weitzner,\n  Angelina Wu",
        "title": "Municipal cyber risk modeling using cryptographic computing to inform\n  cyber policymaking",
        "comments": "Working Draft for Presentation at the Cybersecurity Law and Policy\n  Scholars Conference - September 29, 2023",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  Municipalities are vulnerable to cyberattacks with devastating consequences,\nbut they lack key information to evaluate their own risk and compare their\nsecurity posture to peers. Using data from 83 municipalities collected via a\ncryptographically secure computation platform about their security posture,\nincidents, security control failures, and losses, we build data-driven cyber\nrisk models and cyber security benchmarks for municipalities. We produce\nbenchmarks of the security posture in a sector, the frequency of cyber\nincidents, forecasted annual losses for organizations based on their defensive\nposture, and a weighting of cyber controls based on their individual failure\nrates and associated losses. Combined, these four items can help guide cyber\npolicymaking by quantifying the cyber risk in a sector, identifying gaps that\nneed to be addressed, prioritizing policy interventions, and tracking progress\nof those interventions over time. In the case of the municipalities, these\nnewly derived risk measures highlight the need for continuous measured\nimprovement of cybersecurity readiness, show clear areas of weakness and\nstrength, and provide governments with some early targets for policy focus such\nas security education, incident response, and focusing efforts first on\nmunicipalities at the lowest security levels that have the highest risk\nreduction per security dollar invested.\n"
    },
    {
        "paper_id": 2402.01141,
        "authors": "Thitithep Sitthiyot and Kanyarat Holasut",
        "title": "Income distribution in Thailand is scale-invariant",
        "comments": "9 pages, 2 figures, 3 tables",
        "journal-ref": "PLoS ONE 18(7): e0288265, 2023",
        "doi": "10.1371/journal.pone.0288265",
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  This study examines whether income distribution in Thailand has a property of\nscale invariance or self-similarity across years. By using the data on income\nshares by quintile and by decile of Thailand from 1988 to 2021, the results\nfrom 306-pairwise Kolmogorov-Smirnov tests indicate that income distribution in\nThailand is statistically scale-invariant or self-similar across years with\np-values ranging between 0.988 and 1.000. Based on these empirical findings,\nthis study would like to propose that, in order to change income distribution\nin Thailand whose pattern had persisted for over three decades, the change\nitself cannot be gradual but has to be like a phase transition of substance in\nphysics.\n"
    },
    {
        "paper_id": 2402.01142,
        "authors": "Thitithep Sitthiyot and Kanyarat Holasut",
        "title": "A simple method for joint evaluation of skill in directional forecasts\n  of multiple variables",
        "comments": "10 pages, 8 tables",
        "journal-ref": "Heliyon 9: e19729, 2023",
        "doi": "10.1016/j.heliyon.2023.e19729",
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Forecasts for key macroeconomic variables are almost always made\nsimultaneously by the same organizations, presented together, and used together\nin policy analyses and decision-makings. It is therefore important to know\nwhether the forecasters are skillful enough to forecast the future values of\nthose variables. Here a method for joint evaluation of skill in directional\nforecasts of multiple variables is introduced. The method is simple to use and\ndoes not rely on complicated assumptions required by the conventional\nstatistical methods for measuring accuracy of directional forecast. The data on\nGDP growth and inflation forecasts of three organizations from Thailand,\nnamely, the Bank of Thailand, the Fiscal Policy Office, and the Office of the\nNational Economic and Social Development Council as well as the actual data on\nGDP growth and inflation of Thailand between 2001 and 2021 are employed in\norder to demonstrate how the method could be used to evaluate the skills of\nforecasters in practice. The overall results indicate that these three\norganizations are somewhat skillful in forecasting the direction-of-changes of\nGDP growth and inflation when no band and a band of +/- 1 standard deviation of\nthe forecasted outcome are considered. However, when a band of +/- 0.5% of the\nforecasted outcome is introduced, the skills in forecasting the\ndirection-of-changes of GDP growth and inflation of these three organizations\nare, at best, little better than intelligent guess work.\n"
    },
    {
        "paper_id": 2402.01337,
        "authors": "Chenguang Liu, Antonis Papapantoleon, Alexandros Saplaouras",
        "title": "Convergence rates for Backward SDEs driven by L\\'evy processes",
        "comments": "17 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We consider L\\'evy processes that are approximated by compound Poisson\nprocesses and, correspondingly, BSDEs driven by L\\'evy processes that are\napproximated by BSDEs driven by their compound Poisson approximations. We are\ninterested in the rate of convergence of the approximate BSDEs to the ones\ndriven by the L\\'evy processes. The rate of convergence of the L\\'evy processes\ndepends on the Blumenthal--Getoor index of the process. We derive the rate of\nconvergence for the BSDEs in the $\\mathbb L^2$-norm and in the Wasserstein\ndistance, and show that, in both cases, this equals the rate of convergence of\nthe corresponding L\\'evy process, and thus is optimal.\n"
    },
    {
        "paper_id": 2402.01354,
        "authors": "Jozef Barunik and Lukas Vacha",
        "title": "Predicting the volatility of major energy commodity prices: the dynamic\n  persistence model",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Time variation and persistence are crucial properties of volatility that are\noften studied separately in energy volatility forecasting models. Here, we\npropose a novel approach that allows shocks with heterogeneous persistence to\nvary smoothly over time, and thus model the two together. We argue that this is\nimportant because such dynamics arise naturally from the dynamic nature of\nshocks in energy commodities. We identify such dynamics from the data using\nlocalised regressions and build a model that significantly improves volatility\nforecasts. Such forecasting models, based on a rich persistence structure that\nvaries smoothly over time, outperform state-of-the-art benchmark models and are\nparticularly useful for forecasting over longer horizons.\n"
    },
    {
        "paper_id": 2402.01441,
        "authors": "Andrew Ye, James Xu, Yi Wang, Yifan Yu, Daniel Yan, Ryan Chen, Bosheng\n  Dong, Vipin Chaudhary, Shuai Xu",
        "title": "Learning the Market: Sentiment-Based Ensemble Trading Agents",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We propose the integration of sentiment analysis and deep-reinforcement\nlearning ensemble algorithms for stock trading, and design a strategy capable\nof dynamically altering its employed agent given concurrent market sentiment.\nIn particular, we create a simple-yet-effective method for extracting news\nsentiment and combine this with general improvements upon existing works,\nresulting in automated trading agents that effectively consider both\nqualitative market factors and quantitative stock data. We show that our\napproach results in a strategy that is profitable, robust, and risk-minimal --\noutperforming the traditional ensemble strategy as well as single agent\nalgorithms and market metrics. Our findings determine that the conventional\npractice of switching ensemble agents every fixed-number of months is\nsub-optimal, and that a dynamic sentiment-based framework greatly unlocks\nadditional performance within these agents. Furthermore, as we have designed\nour algorithm with simplicity and efficiency in mind, we hypothesize that the\ntransition of our method from historical evaluation towards real-time trading\nwith live data should be relatively simple.\n"
    },
    {
        "paper_id": 2402.01648,
        "authors": "Soheila Khajoui, Saeid Dehyadegari, Sayyed Abdolmajid Jalaee",
        "title": "Forecasting Imports in OECD Member Countries and Iran by Using Neural\n  Network Algorithms of LSTM",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Artificial Neural Networks (ANN) which are a branch of artificial\nintelligence, have shown their high value in lots of applications and are used\nas a suitable forecasting method. Therefore, this study aims at forecasting\nimports in OECD member selected countries and Iran for 20 seasons from 2021 to\n2025 by means of ANN. Data related to the imports of such countries collected\nover 50 years from 1970 to 2019 from valid resources including World Bank, WTO,\nIFM,the data turned into seasonal data to increase the number of collected data\nfor better performance and high accuracy of the network by using Diz formula\nthat there were totally 200 data related to imports. This study has used LSTM\nto analyse data in Pycharm. 75% of data considered as training data and 25%\nconsidered as test data and the results of the analysis were forecasted with\n99% accuracy which revealed the validity and reliability of the output. Since\nthe imports is consumption function and since the consumption is influenced\nduring Covid-19 Pandemic, so it is time-consuming to correct and improve it to\nbe influential on the imports, thus the imports in the years after Covid-19\nPandemic has had a fluctuating trend.\n"
    },
    {
        "paper_id": 2402.01734,
        "authors": "Kei Nakagawa, Kohei Hayashi, Yugo Fujimoto",
        "title": "CFTM: Continuous time fractional topic model",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper, we propose the Continuous Time Fractional Topic Model (cFTM),\na new method for dynamic topic modeling. This approach incorporates fractional\nBrownian motion~(fBm) to effectively identify positive or negative correlations\nin topic and word distribution over time, revealing long-term dependency or\nroughness. Our theoretical analysis shows that the cFTM can capture these\nlong-term dependency or roughness in both topic and word distributions,\nmirroring the main characteristics of fBm. Moreover, we prove that the\nparameter estimation process for the cFTM is on par with that of LDA,\ntraditional topic models. To demonstrate the cFTM's property, we conduct\nempirical study using economic news articles. The results from these tests\nsupport the model's ability to identify and track long-term dependency or\nroughness in topics over time.\n"
    },
    {
        "paper_id": 2402.01766,
        "authors": "Joshua C. Yang, Damian Dailisan, Marcin Korecki, Carina I. Hausladen,\n  and Dirk Helbing",
        "title": "LLM Voting: Human Choices and AI Collective Decision Making",
        "comments": "Accepted in AAAI Conference on AI, Ethics, and Society (AIES)",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper investigates the voting behaviors of Large Language Models (LLMs),\nspecifically GPT-4 and LLaMA-2, their biases, and how they align with human\nvoting patterns. Our methodology involved using a dataset from a human voting\nexperiment to establish a baseline for human preferences and conducting a\ncorresponding experiment with LLM agents. We observed that the choice of voting\nmethods and the presentation order influenced LLM voting outcomes. We found\nthat varying the persona can reduce some of these biases and enhance alignment\nwith human choices. While the Chain-of-Thought approach did not improve\nprediction accuracy, it has potential for AI explainability in the voting\nprocess. We also identified a trade-off between preference diversity and\nalignment accuracy in LLMs, influenced by different temperature settings. Our\nfindings indicate that LLMs may lead to less diverse collective outcomes and\nbiased assumptions when used in voting scenarios, emphasizing the need for\ncautious integration of LLMs into democratic processes.\n"
    },
    {
        "paper_id": 2402.01784,
        "authors": "Mar\\'ia Jos\\'e Presno, Manuel Landajo, Paula Fern\\'andez Gonz\\'alez",
        "title": "GHG emissions in the EU-28. A multilevel club convergence study of the\n  Emission Trading System and Effort Sharing Decision mechanisms",
        "comments": "This manuscript is an early draft of Presno, M.J., Landajo, M.,\n  Fern\\'andez Gonz\\'alez, P. (2021): \"GHG emissions in the EU-28. A multilevel\n  club convergence study of the Emission Trading System and Effort Sharing\n  Decision mechanisms,\" Sustainable Production and Consumption, Vol. 27, pp.\n  998-1009. The final version of the paper is available at the Journal's web\n  page",
        "journal-ref": "Sustainable Production and Consumption, 27, 998-1009 (2021)",
        "doi": "10.1016/j.spc.2021.02.032",
        "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
        "abstract": "  The European Union is engaged in the fight against climate change. A crucial\nissue to enforce common environmental guidelines is environmental convergence.\nStates converging in environmental variables are expected to be able to jointly\ndevelop and implement environmental policies. Convergence in environmental\nindicators may also help determine the efficiency and speed of those policies.\nThis paper employs a multilevel club convergence approach to analyze\nconvergence in the evolution of GHG emissions among the EU-28 members, on a\nsearch for countries transitioning from disequilibrium to specific steady-state\npositions. Overall convergence is rejected, with club composition depending on\nthe specific period (1990-2017, 2005-2017) and emissions categories (global,\nETS, ESD) analyzed. Some countries (e.g. the United Kingdom and Denmark) are\nconsistently located in clubs outperforming the EU's average in terms of speed\nof emissions reductions, for both the whole and the most recent periods, and\nfor both ETS and ESD emissions. At the other end, Germany (with a large\nindustrial and export basis), Ireland (with the strongest GDP growth in the EU\nin recent years) and most Eastern EU members underperform after 2005, almost\nreversing their previous positions when the study begins in 1990.\n"
    },
    {
        "paper_id": 2402.0182,
        "authors": "Eduardo Abi Jaber, Louis-Amand G\\'erard",
        "title": "Signature volatility models: pricing and hedging with Fourier",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  We consider a stochastic volatility model where the dynamics of the\nvolatility are given by a possibly infinite linear combination of the elements\nof the time extended signature of a Brownian motion. First, we show that the\nmodel is remarkably universal, as it includes, but is not limited to, the\ncelebrated Stein-Stein, Bergomi, and Heston models, together with some\npath-dependent variants. Second, we derive the joint characteristic functional\nof the log-price and integrated variance provided that some infinite\ndimensional extended tensor algebra valued Riccati equation admits a solution.\nThis allows us to price and (quadratically) hedge certain European and\npath-dependent options using Fourier inversion techniques. We highlight the\nefficiency and accuracy of these Fourier techniques in a comprehensive\nnumerical study.\n"
    },
    {
        "paper_id": 2402.01938,
        "authors": "Ihor Kendiukhov",
        "title": "Present Value of the Future Consumer Goods Multiplier",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  In this paper, we derive a formula for the present value of future consumer\ngoods multiplier based on the assumption that a constant share of investment in\nthe production of consumer goods is expected. The present value appears to be\nan infinite geometric sequence. Moreover, we investigate how the notion of the\nmultiplier can help us in macroeconomic analysis of capital and investment\ndynamics and in understanding some general principles of capital market\nequilibrium. Using the concept of this multiplier, we build a macroeconomic\nmodel of capital market dynamics which is consistent with the implications of\nclassical models and with the market equilibrium condition but gives additional\nquantitative and qualitative predictions regarding the dynamics of shares of\ninvestment into the production of consumer goods and the production of means of\nproduction. The investment volume is modeled as a function of the multiplier:\ninvestments adjust when the value of the multiplier fluctuates around its\nequilibrium value of one. In addition, we suggest possible connections between\nthe investment volume and the multiplier value in the form of differential\nequations. We also present the formula for the rate of growth of the\nmultiplier. Independently of the implications of capital market dynamics\nmodels, the formula for the multiplier itself can be applied for the evaluation\nof the present value of capital or the estimation of the macroeconomic impact\nof changes in investment volumes. Our findings show that both the exponential\nand hyperbolic discounting in combination with empirical evidence available\nlead to the value of the multiplier that is close to one.\n"
    },
    {
        "paper_id": 2402.01951,
        "authors": "Stelios Arvanitis, Olivier Scaillet, Nikolas Topaloglou",
        "title": "Sparse spanning portfolios and under-diversification with second-order\n  stochastic dominance",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  We develop and implement methods for determining whether relaxing sparsity\nconstraints on portfolios improves the investment opportunity set for\nrisk-averse investors. We formulate a new estimation procedure for sparse\nsecond-order stochastic spanning based on a greedy algorithm and Linear\nProgramming. We show the optimal recovery of the sparse solution asymptotically\nwhether spanning holds or not. From large equity datasets, we estimate the\nexpected utility loss due to possible under-diversification, and find that\nthere is no benefit from expanding a sparse opportunity set beyond 45 assets.\nThe optimal sparse portfolio invests in 10 industry sectors and cuts tail risk\nwhen compared to a sparse mean-variance portfolio. On a rolling-window basis,\nthe number of assets shrinks to 25 assets in crisis periods, while standard\nfactor models cannot explain the performance of the sparse portfolios.\n"
    },
    {
        "paper_id": 2402.02197,
        "authors": "Nicol\\'as Ure\\~na and Antonio M. Vargas",
        "title": "Numerical solution to a Parabolic-ODE Solow model with spatial diffusion\n  and technology-induced motility",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  This work studies a parabolic-ODE PDE's system which describes the evolution\nof the physical capital \"$k$\" and technological progress \"$A$\", using a\nmeshless in one and two dimensional bounded domain with regular boundary. The\nwell-known Solow model is extended by considering the spatial diffusion of both\ncapital anf technology. Moreover, we study the case in which no spatial\ndiffusion of the technology progress occurs. For such models, we propound\nschemes based on the Generalized Finite Difference method and proof the\nconvergence of the numerical solution to the continuous one. Several examples\nshow the dynamics of the model for a wide range of parameters. These examples\nillustrate the accuary of the numerical method.\n"
    },
    {
        "paper_id": 2402.02315,
        "authors": "Jean Lee, Nicholas Stevens, Soyeon Caren Han, Minseok Song",
        "title": "A Survey of Large Language Models in Finance (FinLLMs)",
        "comments": "More information on https://github.com/adlnlp/FinLLMs",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Large Language Models (LLMs) have shown remarkable capabilities across a wide\nvariety of Natural Language Processing (NLP) tasks and have attracted attention\nfrom multiple domains, including financial services. Despite the extensive\nresearch into general-domain LLMs, and their immense potential in finance,\nFinancial LLM (FinLLM) research remains limited. This survey provides a\ncomprehensive overview of FinLLMs, including their history, techniques,\nperformance, and opportunities and challenges. Firstly, we present a\nchronological overview of general-domain Pre-trained Language Models (PLMs)\nthrough to current FinLLMs, including the GPT-series, selected open-source\nLLMs, and financial LMs. Secondly, we compare five techniques used across\nfinancial PLMs and FinLLMs, including training methods, training data, and\nfine-tuning methods. Thirdly, we summarize the performance evaluations of six\nbenchmark tasks and datasets. In addition, we provide eight advanced financial\nNLP tasks and datasets for developing more sophisticated FinLLMs. Finally, we\ndiscuss the opportunities and the challenges facing FinLLMs, such as\nhallucination, privacy, and efficiency. To support AI research in finance, we\ncompile a collection of accessible datasets and evaluation benchmarks on\nGitHub.\n"
    },
    {
        "paper_id": 2402.02402,
        "authors": "Marcos Lacasa Cazcarra",
        "title": "Machine Learning Analysis of the Impact of Increasing the Minimum Wage\n  on Income Inequality in Spain from 2001 to 2021",
        "comments": "3 tables, 7 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  This paper analyzes the impact of the National Minimum Wage from 2001 to\n2021. The MNW increased from 505.7/month (2001) to 1,108.3/month (2021). Using\nthe data provided by the Spanish Tax Administration Agency, databases that\nrepresent the entire population studied can be analyzed. More accurate results\nand more efficient predictive models are provided by these counts. This work is\ncharacterized by the database used, which is a national census and not a sample\nor projection. Therefore, the study reflects results and analyses based on\nhistorical data from the Spanish Salary Census 2001-2021. Various\nmachine-learning models show that income inequality has been reduced by raising\nthe minimum wage. Raising the minimum wage has not led to inflation or\nincreased unemployment. On the contrary, it has been consistent with increased\nnet employment, contained prices, and increased corporate profit margins. The\nmost important conclusion is that an increase in the minimum wage in the period\nanalyzed has led to an increase in the wealth of the country, increasing\nemployment and company profits, and is postulated, under the conditions\nanalyzed, as an effective method for the redistribution of wealth.\n"
    },
    {
        "paper_id": 2402.02714,
        "authors": "Changqing Teng and Guanglian Li",
        "title": "Neural option pricing for rough Bergomi model",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The rough Bergomi (rBergomi) model can accurately describe the historical and\nimplied volatilities, and has gained much attention in the past few years.\nHowever, there are many hidden unknown parameters or even functions in the\nmodel. In this work, we investigate the potential of learning the forward\nvariance curve in the rBergomi model using a neural SDE. To construct an\nefficient solver for the neural SDE, we propose a novel numerical scheme for\nsimulating the volatility process using the modified summation of exponentials.\nUsing the Wasserstein 1-distance to define the loss function, we show that the\nlearned forward variance curve is capable of calibrating the price process of\nthe underlying asset and the price of the European-style options\nsimultaneously. Several numerical tests are provided to demonstrate its\nperformance.\n"
    },
    {
        "paper_id": 2402.02745,
        "authors": "Chi Truong (Department of Actuarial Studies and Business Analytics,\n  Macquarie Business School, Macquarie University), Matteo Malavasi (School of\n  Risk and Actuarial Studies, UNSW Business School, UNSW), Han Li (Department\n  of Economics, University of Melbourne), Stefan Trueck (Department of\n  Actuarial Studies and Business Analytics, Macquarie Business School,\n  Macquarie University), and Pavel V. Shevchenko (Department of Actuarial\n  Studies and Business Analytics, Macquarie Business School, Macquarie\n  University)",
        "title": "Optimal dynamic climate adaptation pathways: a case study of New York\n  City",
        "comments": "29 pages, 5 figures, and 4 tables",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Assessing climate risk and its potential impacts on our cities and economies\nis of fundamental importance. Extreme weather events, such as hurricanes,\nfloods, and storm surges can lead to catastrophic damages. We propose a\nflexible approach based on real options analysis and extreme value theory,\nwhich enables the selection of optimal adaptation pathways for a portfolio of\nclimate adaptation projects. We model the severity of extreme sea level events\nusing the block maxima approach from extreme value theory, and then develop a\nreal options framework, factoring in climate change, sea level rise\nuncertainty, and the growth in asset exposure. We then apply the proposed\nframework to a real-world problem, considering sea level data as well as\ndifferent adaptation investment options for New York City. Our research can\nassist governments and policy makers in taking informed decisions about optimal\nadaptation pathways and more specifically about reducing flood and storm surge\nrisk in a dynamic settings.\n"
    },
    {
        "paper_id": 2402.03338,
        "authors": "Sina Montazeri, Akram Mirzaeinia, Amir Mirzaeinia",
        "title": "CNN-DRL with Shuffled Features in Finance",
        "comments": "10th Annual Conf. on Computational Science & Computational\n  Intelligence (CSCI'23). arXiv admin note: substantial text overlap with\n  arXiv:2401.06179",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
        "abstract": "  In prior methods, it was observed that the application of Convolutional\nNeural Networks agent in Deep Reinforcement Learning to financial data resulted\nin an enhanced reward. In this study, a specific permutation was applied to the\nfeature vector, thereby generating a CNN matrix that strategically positions\nmore pertinent features in close proximity. Our comprehensive experimental\nevaluations unequivocally demonstrate a substantial enhancement in reward\nattainment.\n"
    },
    {
        "paper_id": 2402.03353,
        "authors": "C. Sarai R. Avila",
        "title": "Tweet Influence on Market Trends: Analyzing the Impact of Social Media\n  Sentiment on Biotech Stocks",
        "comments": "This submission includes 51 pages and 24 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  This study investigates the relationship between tweet sentiment across\ndiverse categories: news, company opinions, CEO opinions, competitor opinions,\nand stock market behavior in the biotechnology sector, with a focus on\nunderstanding the impact of social media discourse on investor sentiment and\ndecision-making processes. We analyzed historical stock market data for ten of\nthe largest and most influential pharmaceutical companies alongside Twitter\ndata related to COVID-19, vaccines, the companies, and their respective CEOs.\nUsing VADER sentiment analysis, we examined the sentiment scores of tweets and\nassessed their relationships with stock market performance. We employed ARIMA\n(AutoRegressive Integrated Moving Average) and VAR (Vector AutoRegression)\nmodels to forecast stock market performance, incorporating sentiment covariates\nto improve predictions. Our findings revealed a complex interplay between tweet\nsentiment, news, biotech companies, their CEOs, and stock market performance,\nemphasizing the importance of considering diverse factors when modeling and\npredicting stock prices. This study provides valuable insights into the\ninfluence of social media on the financial sector and lays a foundation for\nfuture research aimed at refining stock price prediction models.\n"
    },
    {
        "paper_id": 2402.03411,
        "authors": "Zachary Porreca",
        "title": "Bride Kidnapping and Informal Governance Institutions",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Bride kidnapping is a form of forced marriage in which a woman is taken\nagainst her will and coerced into accepting marriage with her captor.\nPost-Soviet Kyrgyzstan has seen a large increase in the prominence of this\npractice alongside a revitalization of traditional values and culture. As part\nof this resurgence of Kyrgyz identity and culture, the central government has\nformalized the authority of councils of elders called aksakals as an arbitrator\nfor local dispute resolution -- guided by informal principles of tradition and\ncultural norm adherence. Bride kidnapping falls within the domain of aksakal\nauthority. In this study, I leverage data from a nationally representative\nsurvey and specify a latent class nested logit model of mens' marriage modality\nchoice to analyze the impacts that aksakal governance has on the decision to\nkidnap. Based on value assessment questions on the survey, men are assigned to\na probability distribution over latent class membership. Utility function\nparameters for each potential marriage modality are estimated for each latent\nclass of men. Results suggest that living under aksakal governance makes men 9%\nmore likely to obtain a wife through bride capture, with men substituting\nkidnapping for choice marriage modalities such as elopement and standard love\nmarriages.\n"
    },
    {
        "paper_id": 2402.03659,
        "authors": "Kelvin J.L. Koa, Yunshan Ma, Ritchie Ng, Tat-Seng Chua",
        "title": "Learning to Generate Explainable Stock Predictions using Self-Reflective\n  Large Language Models",
        "comments": "WWW 2024",
        "journal-ref": null,
        "doi": "10.1145/3589334.3645611",
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Explaining stock predictions is generally a difficult task for traditional\nnon-generative deep learning models, where explanations are limited to\nvisualizing the attention weights on important texts. Today, Large Language\nModels (LLMs) present a solution to this problem, given their known\ncapabilities to generate human-readable explanations for their decision-making\nprocess. However, the task of stock prediction remains challenging for LLMs, as\nit requires the ability to weigh the varying impacts of chaotic social texts on\nstock prices. The problem gets progressively harder with the introduction of\nthe explanation component, which requires LLMs to explain verbally why certain\nfactors are more important than the others. On the other hand, to fine-tune\nLLMs for such a task, one would need expert-annotated samples of explanation\nfor every stock movement in the training set, which is expensive and\nimpractical to scale. To tackle these issues, we propose our\nSummarize-Explain-Predict (SEP) framework, which utilizes a self-reflective\nagent and Proximal Policy Optimization (PPO) to let a LLM teach itself how to\ngenerate explainable stock predictions in a fully autonomous manner. The\nreflective agent learns how to explain past stock movements through\nself-reasoning, while the PPO trainer trains the model to generate the most\nlikely explanations from input texts. The training samples for the PPO trainer\nare also the responses generated during the reflective process, which\neliminates the need for human annotators. Using our SEP framework, we fine-tune\na LLM that can outperform both traditional deep-learning and LLM methods in\nprediction accuracy and Matthews correlation coefficient for the stock\nclassification task. To justify the generalization capability of our framework,\nwe further test it on the portfolio construction task, and demonstrate its\neffectiveness through various portfolio metrics.\n"
    },
    {
        "paper_id": 2402.03755,
        "authors": "Saizhuo Wang, Hang Yuan, Lionel M. Ni, Jian Guo",
        "title": "QuantAgent: Seeking Holy Grail in Trading by Self-Improving Large\n  Language Model",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Autonomous agents based on Large Language Models (LLMs) that devise plans and\ntackle real-world challenges have gained prominence.However, tailoring these\nagents for specialized domains like quantitative investment remains a\nformidable task. The core challenge involves efficiently building and\nintegrating a domain-specific knowledge base for the agent's learning process.\nThis paper introduces a principled framework to address this challenge,\ncomprising a two-layer loop.In the inner loop, the agent refines its responses\nby drawing from its knowledge base, while in the outer loop, these responses\nare tested in real-world scenarios to automatically enhance the knowledge base\nwith new insights.We demonstrate that our approach enables the agent to\nprogressively approximate optimal behavior with provable\nefficiency.Furthermore, we instantiate this framework through an autonomous\nagent for mining trading signals named QuantAgent. Empirical results showcase\nQuantAgent's capability in uncovering viable financial signals and enhancing\nthe accuracy of financial forecasts.\n"
    },
    {
        "paper_id": 2402.038,
        "authors": "Emanuel Kohlscheen, Richhild Moessner, Elod Takats",
        "title": "Effects of carbon pricing and other climate policies on CO2 emissions",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  We provide ex-post empirical analysis of the effects of climate policies on\ncarbon dioxide emissions at the aggregate national level. Our results are based\non a comprehensive database of 121 countries. As climate policies we examine\ncarbon taxes and emissions trading systems (ETS), as well as the overall\nstringency of climate policies. We use dynamic panel regressions, controlling\nfor macroeconomic factors such as economic development, GDP growth,\nurbanisation, as well as the energy mix. We find that higher carbon taxes and\nprices of permits in ETS reduce carbon emissions. An increase in carbon taxes\nby $10 per ton of CO2 reduces CO2 emissions per capita by 1.3% in the short run\nand by 4.6% in the long run.\n"
    },
    {
        "paper_id": 2402.03806,
        "authors": "Marc Schmitt",
        "title": "Explainable Automated Machine Learning for Credit Decisions: Enhancing\n  Human Artificial Intelligence Collaboration in Financial Engineering",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper explores the integration of Explainable Automated Machine Learning\n(AutoML) in the realm of financial engineering, specifically focusing on its\napplication in credit decision-making. The rapid evolution of Artificial\nIntelligence (AI) in finance has necessitated a balance between sophisticated\nalgorithmic decision-making and the need for transparency in these systems. The\nfocus is on how AutoML can streamline the development of robust machine\nlearning models for credit scoring, while Explainable AI (XAI) methods,\nparticularly SHapley Additive exPlanations (SHAP), provide insights into the\nmodels' decision-making processes. This study demonstrates how the combination\nof AutoML and XAI not only enhances the efficiency and accuracy of credit\ndecisions but also fosters trust and collaboration between humans and AI\nsystems. The findings underscore the potential of explainable AutoML in\nimproving the transparency and accountability of AI-driven financial decisions,\naligning with regulatory requirements and ethical considerations.\n"
    },
    {
        "paper_id": 2402.03894,
        "authors": "Benedikt V. Meylahn, Arnoud V. den Boer, and Michel Mandjes",
        "title": "Interpersonal trust: Asymptotic analysis of a stochastic coordination\n  game with multi-agent learning",
        "comments": "17 pages, 4 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  We study the interpersonal trust of a population of agents, asking whether\nchance may decide if a population ends up in a high trust or low trust state.\nWe model this by a discrete time, random matching stochastic coordination game.\nAgents are endowed with an exponential smoothing learning rule about the\nbehaviour of their neighbours. We find that, with probability one in the long\nrun the whole population either always cooperates or always defects. By\nsimulation we study the impact of the distributions of the payoffs in the game\nand of the exponential smoothing learning (memory of the agents). We find, that\nas the agent memory increases or as the size of the population increases, the\nactual dynamics start to resemble the expectation of the process. We conclude\nthat it is indeed possible that different populations may converge upon high or\nlow trust between its citizens simply by chance, though the game parameters\n(context of the society) may be quite telling.\n"
    },
    {
        "paper_id": 2402.03953,
        "authors": "Erdong Chen, Mengzhong Ma, Zixin Nie",
        "title": "Exploring the Impact: How Decentralized Exchange Designs Shape Traders'\n  Behavior on Perpetual Future Contracts",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  In this paper, we analyze traders' behavior within both centralized exchanges\n(CEXs) and decentralized exchanges (DEXs), focusing on the volatility of\nBitcoin prices and the trading activity of investors engaged in perpetual\nfuture contracts. We categorize the architecture of perpetual future exchanges\ninto three distinct models, each exhibiting unique patterns of trader behavior\nin relation to trading volume, open interest, liquidation, and leverage. Our\ndetailed examination of DEXs, especially those utilizing the Virtual Automated\nMarket Making (VAMM) Model, uncovers a differential impact of open interest on\nlong versus short positions. In exchanges which operate under the Oracle\nPricing Model, we find that traders primarily act as price takers, with their\ntrading actions reflecting direct responses to price movements of the\nunderlying assets. Furthermore, our research highlights a significant\npropensity among less informed traders to overreact to positive news, as\ndemonstrated by an increase in long positions. This study contributes to the\nunderstanding of market dynamics in digital asset exchanges, offering insights\ninto the behavioral finance for future innovation of decentralized finance.\n"
    },
    {
        "paper_id": 2402.04138,
        "authors": "Javier Cabello S\\'anchez, Juan Antonio Fern\\'andez Torvisco, Mariano\n  R. Arias",
        "title": "TAC Method for Fitting Exponential Autoregressive Models and Others:\n  Applications in Economy and Finance",
        "comments": null,
        "journal-ref": "Mathematics. 2021; 9(8):862",
        "doi": "10.3390/math9080862",
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  There are a couple of purposes in this paper: to study a problem of\napproximation with exponential functions and to show its relevance for the\neconomic science. We present results that completely solve the problem of the\nbest approximation by means of exponential functions and we will be able to\ndetermine what kind of data is suitable to be fitted. Data will be approximated\nusing TAC (implemented in the R-package nlstac), a numerical algorithm for\nfitting data by exponential patterns without initial guess designed by the\nauthors. We check one more time the robustness of this algorithm by\nsuccessfully applying it to two very distant areas of economy: demand curves\nand nonlinear time series. This shows TAC's utility and highlights how far this\nalgorithm could be used.\n"
    },
    {
        "paper_id": 2402.04166,
        "authors": "Taylor Reynolds, Sarah Scheffler, Daniel J. Weitzner, Angelina Wu",
        "title": "Mind the Gap: Securely modeling cyber risk based on security deviations\n  from a peer group",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  There are two strategic and longstanding questions about cyber risk that\norganizations largely have been unable to answer: What is an organization's\nestimated risk exposure and how does its security compare with peers? Answering\nboth requires industry-wide data on security posture, incidents, and losses\nthat, until recently, have been too sensitive for organizations to share. Now,\nprivacy enhancing technologies (PETs) such as cryptographic computing can\nenable the secure computation of aggregate cyber risk metrics from a peer group\nof organizations while leaving sensitive input data undisclosed. As these new\naggregate data become available, analysts need ways to integrate them into\ncyber risk models that can produce more reliable risk assessments and allow\ncomparison to a peer group. This paper proposes a new framework for\nbenchmarking cyber posture against peers and estimating cyber risk within\nspecific economic sectors using the new variables emerging from secure\ncomputations. We introduce a new top-line variable called the Defense Gap Index\nrepresenting the weighted security gap between an organization and its peers\nthat can be used to forecast an organization's own security risk based on\nhistorical industry data. We apply this approach in a specific sector using\ndata collected from 25 large firms, in partnership with an industry ISAO, to\nbuild an industry risk model and provide tools back to participants to estimate\ntheir own risk exposure and privately compare their security posture with their\npeers.\n"
    },
    {
        "paper_id": 2402.04429,
        "authors": "Chiaki Moriguchi, Yusuke Narita, Mari Tanaka",
        "title": "Meritocracy and Its Discontents: Long-run Effects of Repeated School\n  Admission Reforms",
        "comments": "Keywords: Elite Education, Market Design, Strategic Behavior,\n  Regional Mobility, Universal Access, Persistent Effects",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  What happens if selective colleges change their admission policies? We study\nthis question by analyzing the world's first implementation of nationally\ncentralized meritocratic admissions in the early twentieth century. We find a\npersistent meritocracy-equity tradeoff. Compared to the decentralized system,\nthe centralized system admitted more high-achievers and produced more\noccupational elites (such as top income earners) decades later in the labor\nmarket. This gain came at a distributional cost, however. Meritocratic\ncentralization also increased the number of urban-born elites relative to\nrural-born ones, undermining equal access to higher education and career\nadvancement.\n"
    },
    {
        "paper_id": 2402.04472,
        "authors": "Damien Echevin, Bernard Fortin, Aristide Houndetoungan",
        "title": "Healthcare Quality by Specialists under a Mixed Compensation System: an\n  Empirical Analysis",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  We analyze the effects of a mixed compensation (MC) scheme for specialists on\nthe quality of their healthcare services. We exploit a reform implemented in\nQuebec (Canada) in 1999. The government introduced a payment mechanism\ncombining a per diem with a reduced fee per clinical service. Using a large\npatient/physician panel dataset, we estimate a multi-state multi-spell hazard\nmodel analogous to a difference-in-differences approach. We compute quality\nindicators from our model. Our results suggest that the reform reduced the\nquality of MC specialist services measured by the risk of re-hospitalization\nand mortality after discharge. These effects vary across specialties.\n"
    },
    {
        "paper_id": 2402.04474,
        "authors": "Aristide Houndetoungan, Asad Islam, Michael Vlassopoulos, Yves Zenou",
        "title": "The Role of Child Gender in the Formation of Parents' Social Networks",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Social networks play an important role in various aspects of life. While\nextensive research has explored factors such as gender, race, and education in\nnetwork formation, one dimension that has received less attention is the gender\nof one's child. Children tend to form friendships with same-gender peers,\npotentially leading their parents to interact based on their child's gender.\nFocusing on households with children aged 3-5, we leverage a rich dataset from\nrural Bangladesh to investigate the role of children's gender in parental\nnetwork formation. We estimate an equilibrium model of network formation that\nconsiders a child's gender alongside other socioeconomic factors.\nCounterfactual analyses reveal that children's gender significantly shapes\nparents' network structure. Specifically, if all children share the same\ngender, households would have approximately 15% more links, with a stronger\neffect for families having girls. Importantly, the impact of children's gender\non network structure is on par with or even surpasses that of factors such as\nincome distribution, parental occupation, education, and age. These findings\ncarry implications for debates surrounding coed versus single-sex schools, as\nwell as policies that foster inter-gender social interactions among children.\n"
    },
    {
        "paper_id": 2402.04662,
        "authors": "Guangye Cao",
        "title": "Token vs Equity for Startup Financing",
        "comments": "11 pages, 1 figure",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Why would a blockchain-based startup and its venture capital investors choose\nto finance by issuing tokens instead of equity? What would be their rates of\nreturn for each asset? This paper focuses on the liquidity difference between\nthe two fundraising methods. I build a three-period model of an entrepreneur,\ntwo types of investors, and users. Some investors have unforeseen liquidity\nneeds in the middle period that can only be met with tokens. The entrepreneur\nobtains higher payoff by issuing tokens instead of equity, and the payoff\ndifference increases with investors risk-aversion and need for liquidity in the\nmiddle period, as well as the depth of the token market.\n"
    },
    {
        "paper_id": 2402.0474,
        "authors": "Sobin Joseph and Shashi Jain",
        "title": "Non-Parametric Estimation of Multi-dimensional Marked Hawkes Processes",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  An extension of the Hawkes process, the Marked Hawkes process distinguishes\nitself by featuring variable jump size across each event, in contrast to the\nconstant jump size observed in a Hawkes process without marks. While extensive\nliterature has been dedicated to the non-parametric estimation of both the\nlinear and non-linear Hawkes process, there remains a significant gap in the\nliterature regarding the marked Hawkes process. In response to this, we propose\na methodology for estimating the conditional intensity of the marked Hawkes\nprocess. We introduce two distinct models: \\textit{Shallow Neural Hawkes with\nmarks}- for Hawkes processes with excitatory kernels and \\textit{Neural Network\nfor Non-Linear Hawkes with Marks}- for non-linear Hawkes processes. Both these\napproaches take the past arrival times and their corresponding marks as the\ninput to obtain the arrival intensity. This approach is entirely\nnon-parametric, preserving the interpretability associated with the marked\nHawkes process. To validate the efficacy of our method, we subject the method\nto synthetic datasets with known ground truth. Additionally, we apply our\nmethod to model cryptocurrency order book data, demonstrating its applicability\nto real-world scenarios.\n"
    },
    {
        "paper_id": 2402.04765,
        "authors": "Lo\\\"ic Mar\\'echal, Alain Mermoud, Dimitri Percia David, and Mathias\n  Humbert",
        "title": "Measuring the performance of investments in information security\n  startups: An empirical analysis by cybersecurity sectors using Crunchbase\n  data",
        "comments": "This document results from a research project funded by the\n  Cyber-Defence Campus, armasuisse Science and Technology. We appreciate\n  helpful comments from seminar participants at the Cyber Alp Retreat 2022 and\n  WEIS 2023",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Early-stage firms play a significant role in driving innovation and creating\nnew products and services, especially for cybersecurity. Therefore, evaluating\ntheir performance is crucial for investors and policymakers. This work presents\na financial evaluation of early-stage firms' performance in 19 cybersecurity\nsectors using a private-equity dataset from 2010 to 2022 retrieved from\nCrunchbase. We observe firms, their primary and secondary activities, funding\nrounds, and pre and post-money valuations. We compare cybersecurity sectors\nregarding the amount raised over funding rounds and post-money valuations while\ninferring missing observations. We observe significant investor interest\nvariations across categories, periods, and locations. In particular, we find\nthe average capital raised (valuations) to range from USD 7.24 mln (USD 32.39\nmln) for spam filtering to USD 45.46 mln (USD 447.22 mln) for the private cloud\nsector. Next, we assume a log process for returns computed from post-money\nvaluations and estimate the expected returns, systematic and specific risks,\nand risk-adjusted returns of investments in early-stage firms belonging to\ncybersecurity sectors. Again, we observe substantial performance variations\nwith annualized expected returns ranging from 9.72\\% for privacy to 177.27\\%\nfor the blockchain sector. Finally, we show that overall, the cybersecurity\nindustry performance is on par with previous results found in private equity.\nOur results shed light on the performance of cybersecurity investments and,\nthus, on investors' expectations about cybersecurity.\n"
    },
    {
        "paper_id": 2402.04773,
        "authors": "Daniel Celeny, Lo\\\"ic Mar\\'echal, Evgueni Rousselot, Alain Mermoud,\n  and Mathias Humbert",
        "title": "Prioritizing Investments in Cybersecurity: Empirical Evidence from an\n  Event Study on the Determinants of Cyberattack Costs",
        "comments": "This document results from a research project funded by the\n  Cyber-Defence Campus, armasuisse Science and Technology. Our code will be\n  available at\n  https://github.com/technometrics-lab/17-The-determinants-of-cyberattack-costs-An-event-study.\n  Corresponding author: Daniel Celeny e-mail: daniel.celeny@alumni.epfl.ch,\n  Cyber-Defence Campus, Innovation Park, EPFL, 1015 Lausanne",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Along with the increasing frequency and severity of cyber incidents,\nunderstanding their economic implications is paramount. In this context, listed\nfirms' reactions to cyber incidents are compelling to study since they (i) are\na good proxy to estimate the costs borne by other organizations, (ii) have a\ncritical position in the economy, and (iii) have their financial information\npublicly available. We extract listed firms' cyber incident dates and\ncharacteristics from newswire headlines. We use an event study over 2012--2022,\nusing a three-day window around events and standard benchmarks. We find that\nthe magnitude of abnormal returns around cyber incidents is on par with\nprevious studies using newswire or alternative data to identify cyber\nincidents. Conversely, as we adjust the standard errors accounting for\nevent-induced variance and residual cross-correlation, we find that the\npreviously claimed significance of abnormal returns vanishes. Given these\nresults, we run a horse race of specifications, in which we test for the\nmarginal effects of type of cyber incidents, target firm sector, periods, and\ntheir interactions. Data breaches are the most detrimental incident type with\nan average loss of -1.3\\% or (USD -1.9 billion) over the last decade. The\nhealth sector is the most sensitive to cyber incidents, with an average loss of\n-5.21\\% (or USD -1.2 billion), and even more so when these are data breaches.\nInstead, we cannot show any time-varying effect of cyber incidents or a\nspecific effect of the type of news as had previously been advocated.\n"
    },
    {
        "paper_id": 2402.04775,
        "authors": "Daniel Celeny and Lo\\\"ic Mar\\'echal",
        "title": "Cyber risk and the cross-section of stock returns",
        "comments": "This document results from a research project funded by the\n  Cyber-Defence Campus, armasuisse Science and Technology. We appreciate\n  helpful comments from seminar participants at the Cyber Alp Retreat 2023. We\n  also thank Pierre Collin-Dufresne and Michel Dubois for their invaluable\n  comments",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  We extract firms' cyber risk with a machine learning algorithm measuring the\nproximity between their disclosures and a dedicated cyber corpus. Our approach\noutperforms dictionary methods, uses full disclosure and not devoted-only\nsections, and generates a cyber risk measure uncorrelated with other firms'\ncharacteristics. We find that a portfolio of US-listed stocks in the high cyber\nrisk quantile generates an excess return of 18.72% p.a. Moreover, a long-short\ncyber risk portfolio has a significant and positive risk premium of 6.93% p.a.,\nrobust to all factors' benchmarks. Finally, using a Bayesian asset pricing\nmethod, we show that our cyber risk factor is the essential feature that allows\nany multi-factor model to price the cross-section of stock returns.\n"
    },
    {
        "paper_id": 2402.05024,
        "authors": "Yulin Yu, Daniel M. Romero",
        "title": "Does the Use of Unusual Combinations of Datasets Contribute to Greater\n  Scientific Impact?",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  Scientific datasets play a crucial role in contemporary data-driven research,\nas they allow for the progress of science by facilitating the discovery of new\npatterns and phenomena. This mounting demand for empirical research raises\nimportant questions on how strategic data utilization in research projects can\nstimulate scientific advancement. In this study, we examine the hypothesis\ninspired by the recombination theory, which suggests that innovative\ncombinations of existing knowledge, including the use of unusual combinations\nof datasets, can lead to high-impact discoveries. We investigate the scientific\noutcomes of such atypical data combinations in more than 30,000 publications\nthat leverage over 6,000 datasets curated within one of the largest social\nscience databases, ICPSR. This study offers four important insights. First,\ncombining datasets, particularly those infrequently paired, significantly\ncontributes to both scientific and broader impacts (e.g., dissemination to the\ngeneral public). Second, the combination of datasets with atypically combined\ntopics has the opposite effect -- the use of such data is associated with fewer\ncitations. Third, younger and less experienced research teams tend to use\natypical combinations of datasets in research at a higher frequency than their\nolder and more experienced counterparts. Lastly, despite the benefits of data\ncombination, papers that amalgamate data remain infrequent. This finding\nsuggests that the unconventional combination of datasets is an under-utilized\nbut powerful strategy correlated with the scientific and broader impact of\nscientific discoveries.\n"
    },
    {
        "paper_id": 2402.05113,
        "authors": "Oumar Mbodji and Traian A. Pirvu",
        "title": "Portfolio Time Consistency and Utility Weighted Discount Rates",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Merton portfolio management problem is studied in this paper within a\nstochastic volatility, non constant time discount rate, and power utility\nframework. This problem is time inconsistent and the way out of this\npredicament is to consider the subgame perfect strategies. The later are\ncharacterized through an extended Hamilton Jacobi Bellman (HJB) equation. A\nfixed point iteration is employed to solve the extended HJB equation. This is\ndone in a two stage approach: in a first step the utility weighted discount\nrate is introduced and characterized as the fixed point of a certain operator;\nin the second step the value function is determined through a linear parabolic\npartial differential equation. Numerical experiments explore the effect of the\ntime discount rate on the subgame perfect and precommitment strategies.\n"
    },
    {
        "paper_id": 2402.05152,
        "authors": "Shawn Berry",
        "title": "Is the price right? Reconceptualizing price and income elasticity to\n  anticipate price perception issues",
        "comments": "12 pages, 2 figures, 6 tables",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
        "abstract": "  Price perception by consumers represents a challenge to the ability of a\nbusiness to correctly and profitably price and sell their products or services\nin a given market and any new target market. Complicating the perception of\nprices is the dynamics of price and income elasticity, both of which are key\nfor estimating demand. This article proposes a novel and elegant identity that\nconceptualizes elasticity as a means of quantifying the potential for price\nperception problems in a market. Elasticity studies from 1990 to 2023 (n=30)\nwere sampled to evaluate the relationship between price and income elasticity\nfor various consumer commodities using the identity. The results suggest that,\ngiven known price and income elasticity values, a business can anticipate\npricing perception problems in advance and address the potential for damaging\ndistortion of their value proposition. Further, the business can use this\ninsight to correctly choose a strategic pricing approach.\n"
    },
    {
        "paper_id": 2402.05272,
        "authors": "Yizhan Shu, Chenyu Yu, John M. Mulvey",
        "title": "Downside Risk Reduction Using Regime-Switching Signals: A Statistical\n  Jump Model Approach",
        "comments": "30 pages, 6 figures. Revised manuscript",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This article investigates a regime-switching investment strategy aimed at\nmitigating downside risk by reducing market exposure during anticipated\nunfavorable market regimes. We highlight the statistical jump model (JM) for\nmarket regime identification, a recently developed robust model that\ndistinguishes itself from traditional Markov-switching models by enhancing\nregime persistence through a jump penalty applied at each state transition. Our\nJM utilizes a feature set comprising risk and return measures derived solely\nfrom the return series, with the optimal jump penalty selected through a\ntime-series cross-validation method that directly optimizes strategy\nperformance. Our empirical analysis evaluates the realistic out-of-sample\nperformance of various strategies on major equity indices from the US, Germany,\nand Japan from 1990 to 2023, in the presence of transaction costs and trading\ndelays. The results demonstrate the consistent outperformance of the JM-guided\nstrategy in reducing risk metrics such as volatility and maximum drawdown, and\nenhancing risk-adjusted returns like the Sharpe ratio, when compared to both\nhidden Markov model-guided strategy and the buy-and-hold strategy. These\nfindings underline the enhanced persistence, practicality, and versatility of\nstrategies utilizing JMs for regime-switching signals.\n"
    },
    {
        "paper_id": 2402.05364,
        "authors": "M. Mija\\'il Mart\\'inez-Ramos, Parisa Majari, Andres R.\n  Cruz-Hern\\'andez, Hirdesh K. Pharasi, Manan Vyas",
        "title": "Coarse graining correlation matrices according to macrostructures:\n  Financial markets as a paradigm",
        "comments": "15 pages, 16 figures, version as accepted for publication in Physica\n  Scripta (2024)",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  We analyze correlation structures in financial markets by coarse graining the\nPearson correlation matrices according to market sectors to obtain Guhr\nmatrices using Guhr's correlation method according to Ref. [P. Rinn {\\it et.\nal.}, Europhysics Letters 110, 68003 (2015)]. We compare the results for the\nevolution of market states and the corresponding transition matrices with those\nobtained using Pearson correlation matrices. The behavior of market states is\nfound to be similar for both the coarse grained and Pearson matrices. However,\nthe number of relevant variables is reduced by orders of magnitude.\n"
    },
    {
        "paper_id": 2402.05574,
        "authors": "Francesca Cibrario, Or Samimi Golan, Giacomo Ranieri, Emanuele Dri,\n  Mattia Ippoliti, Ron Cohen, Christian Mattia, Bartolomeo Montrucchio, Amir\n  Naveh, Davide Corbelletto",
        "title": "Quantum Amplitude Loading for Rainbow Options Pricing",
        "comments": "9 pages, 5 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-sa/4.0/",
        "abstract": "  This work introduces a novel approach to price rainbow options, a type of\npath-independent multi-asset derivatives, with quantum computers. Leveraging\nthe Iterative Quantum Amplitude Estimation method, we present an end-to-end\nquantum circuit implementation, emphasizing efficiency by delaying the\ntransition to price space. Moreover, we analyze two different amplitude loading\ntechniques for handling exponential functions. Experiments on the IBM QASM\nsimulator validate our quantum pricing model, contributing to the evolving\nfield of quantum finance.\n"
    },
    {
        "paper_id": 2402.05679,
        "authors": "Francesco Scotti, Andrea Flori, Piercesare Secchi, Marika Arena and\n  Giovanni Azzone",
        "title": "Heterogeneous drivers of overnight and same-day visits",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  This paper aims to explore the factors stimulating different tourism\nbehaviours, with specific reference to same-day visits and overnight stays. To\nthis aim, we employ mobile network data referred to the area of Lombardy. The\npaper highlights that larger availability of tourism accommodations, cultural\nand natural endowments are relevant factors explaining overnight stays.\nConversely, temporary entertainment and transportation facilities increase\nmunicipalities attractiveness for same-day visits. The results also highlight a\ntrade-off in the capability of municipalities of being attractive in connection\nto both the tourism behaviours, with higher overnight stays in areas with more\nlimited same-day visits. Mobile data offer a spatial and temporal granularity\nallowing to detect relevant patterns and support the design of tourism\nprecision policies.\n"
    },
    {
        "paper_id": 2402.05729,
        "authors": "Marjan Petreski, Stefan Tanevski and Alejandro D. Jacobo",
        "title": "Monetary Policy and the Gendered Labor Market Dynamics: Evidence from\n  Developing Economies",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Using a Taylor rule amended with official reserves movements, we derive\ncountry-specific monetary shocks and employ a local projections-estimator for\ntracking gender-disaggregated labor-market responses in 99 developing economies\nfrom 2009 to 2021. Results show that women experience more negative post-shock\nemployment responses than men, contributing to a deepening of the gender gaps\non the labor market. After the shock, women leave the labor market more so than\nmen, which results in an apparently intact or even improved unemployment\noutcome for women. We find limited evidence of sector-specific reaction to\ninterest rates. Additionally, we identify an intense worsening of women-s\nposition on the labor market in high-growth environments as well under monetary\npolicy tightening. Developing Asia and Latin America experience the most\nsignificant detrimental effects on women's employment, Africa exhibits a slower\nmanifestation of the monetary shocks-impact and developing Europe shows the\nmildest effects.\n"
    },
    {
        "paper_id": 2402.06032,
        "authors": "Katerina Rigana, Ernst C. Wit, Samantha Cook",
        "title": "Navigating Market Turbulence: Insights from Causal Network Contagion\n  Value at Risk",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Accurately defining, measuring and mitigating risk is a cornerstone of\nfinancial risk management, especially in the presence of financial contagion.\nTraditional correlation-based risk assessment methods often struggle under\nvolatile market conditions, particularly in the face of external shocks,\nhighlighting the need for a more robust and invariant predictive approach.\n  This paper introduces the Causal Network Contagion Value at Risk (Causal-NECO\nVaR), a novel methodology that significantly advances causal inference in\nfinancial risk analysis. Embracing a causal network framework, this method\nadeptly captures and analyses volatility and spillover effects, effectively\nsetting it apart from conventional contagion-based VaR models. Causal-NECO\nVaR's key innovation lies in its ability to derive directional influences among\nassets from observational data, thereby offering robust risk predictions that\nremain invariant to market shocks and systemic changes.\n  A comprehensive simulation study and the application to the Forex market show\nthe robustness of the method. Causal-NECO VaR not only demonstrates predictive\naccuracy, but also maintains its reliability in unstable financial\nenvironments, offering clearer risk assessments even amidst unforeseen market\ndisturbances. This research makes a significant contribution to the field of\nrisk management and financial stability, presenting a causal approach to the\ncomputation of VaR. It emphasises the model's superior resilience and invariant\npredictive power, essential for navigating the complexities of today's\never-evolving financial markets.\n"
    },
    {
        "paper_id": 2402.061,
        "authors": "Md Rabiul Hasan, Muztoba Ahmed Khan, Thorsten Wuest",
        "title": "Towards Industry 5.0: A Systematic Literature Review on Sustainable and\n  Green Composite Materials Supply Chains",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  Sustainable supply chain management is a key objective of Industry 5.0,\nleveraging technologies like real-time data analytics, connectivity, and\nintelligent automation. At the same time, composite materials present benefits\nsuch as lightweight structures, crucial for reducing fuel consumption. This\nstudy investigates the intersection between sustainable supply chains and\ncomposites by analyzing the current status, research gaps, methodologies, and\nfuture research opportunities through bibliometric analysis and a systematic\nreview of the state of the art in the composite materials supply chain. A\nsystematic literature review approach is employed to analyze the Scopus and Web\nof Science (WOS) databases, offering a comprehensive overview of the existing\nliterature. Through bibliometric analysis, the study investigates countries,\nauthors, citations, keywords, subject areas, and article themes within the\nmetadata to provide additional context. An in-depth analysis of thirty selected\npapers (n=30) sheds light on the key contributions, major challenges, and Key\nPerformance Indicators (KPIs) across various instances of composite material\nsupply chains, resulting in a generalized overview. Furthermore, this research\nsuggests future directions to link the sustainability efforts in composite\nmaterials supply chains with current research gaps. The study underscores\ndiverse research themes in the field, highlighting a few influential works and\npresenting opportunities for advancement in this emerging area. Collectively,\nthese findings offer valuable insights and a robust roadmap for future research\nin this domain.\n"
    },
    {
        "paper_id": 2402.06459,
        "authors": "Guangsheng Yu, Qin Wang, Caijun Sun, Lam Duc Nguyen, H.M.N. Dilum\n  Bandara, Shiping Chen",
        "title": "Maximizing NFT Incentives: References Make You Rich",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  In this paper, we study how to optimize existing Non-Fungible Token (NFT)\nincentives. Upon exploring a large number of NFT-related standards and\nreal-world projects, we come across an unexpected finding. That is, the current\nNFT incentive mechanisms, often organized in an isolated and one-time-use\nfashion, tend to overlook their potential for scalable organizational\nstructures.\n  We propose, analyze, and implement a novel reference incentive model, which\nis inherently structured as a Directed Acyclic Graph (DAG)-based NFT network.\nThis model aims to maximize connections (or references) between NFTs, enabling\neach isolated NFT to expand its network and accumulate rewards derived from\nsubsequent or subscribed ones. We conduct both theoretical and practical\nanalyses of the model, demonstrating its optimal utility.\n"
    },
    {
        "paper_id": 2402.06594,
        "authors": "Stuart Baumann and Carl Singleton",
        "title": "They were robbed! Scoring by the middlemost to attenuate biased judging\n  in boxing",
        "comments": "We are grateful for comments and advice from Anwesha Mukherjee",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Boxing has a long-standing problem with biased judging, impacting both\nprofessional and Olympic bouts. \"Robberies\", where boxers are widely seen as\nbeing denied rightful victories, threaten to drive fans and athletes away from\nthe sport. To tackle this problem, we propose a minimalist adjustment in how\nboxing is scored: the winner would be decided by the majority of round-by-round\nvictories according to the judges, rather than relying on the judges' overall\nbout scores. This approach, rooted in social choice theory and utilising\nmajority rule and middlemost aggregation functions, creates a coordination\nproblem for partisan judges and attenuates their influence. Our model analysis\nand simulations demonstrate the potential to significantly decrease the\nlikelihood of a partisan judge swaying the result of a bout.\n"
    },
    {
        "paper_id": 2402.06633,
        "authors": "Hao Qian, Hongting Zhou, Qian Zhao, Hao Chen, Hongxiang Yao, Jingwei\n  Wang, Ziqi Liu, Fei Yu, Zhiqiang Zhang, Jun Zhou",
        "title": "MDGNN: Multi-Relational Dynamic Graph Neural Network for Comprehensive\n  and Dynamic Stock Investment Prediction",
        "comments": "9 pages, 3 figures, accepted by AAAI 2024",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  The stock market is a crucial component of the financial system, but\npredicting the movement of stock prices is challenging due to the dynamic and\nintricate relations arising from various aspects such as economic indicators,\nfinancial reports, global news, and investor sentiment. Traditional sequential\nmethods and graph-based models have been applied in stock movement prediction,\nbut they have limitations in capturing the multifaceted and temporal influences\nin stock price movements. To address these challenges, the Multi-relational\nDynamic Graph Neural Network (MDGNN) framework is proposed, which utilizes a\ndiscrete dynamic graph to comprehensively capture multifaceted relations among\nstocks and their evolution over time. The representation generated from the\ngraph offers a complete perspective on the interrelationships among stocks and\nassociated entities. Additionally, the power of the Transformer structure is\nleveraged to encode the temporal evolution of multiplex relations, providing a\ndynamic and effective approach to predicting stock investment. Further, our\nproposed MDGNN framework achieves the best performance in public datasets\ncompared with state-of-the-art (SOTA) stock investment methods.\n"
    },
    {
        "paper_id": 2402.06635,
        "authors": "Bryan Kelly, Boris Kuznetsov, Semyon Malamud, Teng Andrea Xu",
        "title": "Large (and Deep) Factor Models",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We open up the black box behind Deep Learning for portfolio optimization and\nprove that a sufficiently wide and arbitrarily deep neural network (DNN)\ntrained to maximize the Sharpe ratio of the Stochastic Discount Factor (SDF) is\nequivalent to a large factor model (LFM): A linear factor pricing model that\nuses many non-linear characteristics. The nature of these characteristics\ndepends on the architecture of the DNN in an explicit, tractable fashion. This\nmakes it possible to derive end-to-end trained DNN-based SDFs in closed form\nfor the first time. We evaluate LFMs empirically and show how various\narchitectural choices impact SDF performance. We document the virtue of depth\ncomplexity: With enough data, the out-of-sample performance of DNN-SDF is\nincreasing in the NN depth, saturating at huge depths of around 100 hidden\nlayers.\n"
    },
    {
        "paper_id": 2402.06638,
        "authors": "Chu Myaet Thwal, Ye Lin Tun, Kitae Kim, Seong-Bae Park, Choong Seon\n  Hong",
        "title": "Transformers with Attentive Federated Aggregation for Time Series Stock\n  Forecasting",
        "comments": "Published in IEEE ICOIN 2023",
        "journal-ref": null,
        "doi": "10.1109/ICOIN56518.2023.10048928",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Recent innovations in transformers have shown their superior performance in\nnatural language processing (NLP) and computer vision (CV). The ability to\ncapture long-range dependencies and interactions in sequential data has also\ntriggered a great interest in time series modeling, leading to the widespread\nuse of transformers in many time series applications. However, being the most\ncommon and crucial application, the adaptation of transformers to time series\nforecasting has remained limited, with both promising and inconsistent results.\nIn contrast to the challenges in NLP and CV, time series problems not only add\nthe complexity of order or temporal dependence among input sequences but also\nconsider trend, level, and seasonality information that much of this data is\nvaluable for decision making. The conventional training scheme has shown\ndeficiencies regarding model overfitting, data scarcity, and privacy issues\nwhen working with transformers for a forecasting task. In this work, we propose\nattentive federated transformers for time series stock forecasting with better\nperformance while preserving the privacy of participating enterprises.\nEmpirical results on various stock data from the Yahoo! Finance website\nindicate the superiority of our proposed scheme in dealing with the above\nchallenges and data heterogeneity in federated learning.\n"
    },
    {
        "paper_id": 2402.06642,
        "authors": "Pengfei Zhao, Haoren Zhu, Wilfred Siu Hung NG, Dik Lun Lee",
        "title": "From GARCH to Neural Network for Volatility Forecast",
        "comments": "Accepted by AAAI'24",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Volatility, as a measure of uncertainty, plays a crucial role in numerous\nfinancial activities such as risk management. The Econometrics and Machine\nLearning communities have developed two distinct approaches for financial\nvolatility forecasting: the stochastic approach and the neural network (NN)\napproach. Despite their individual strengths, these methodologies have\nconventionally evolved in separate research trajectories with little\ninteraction between them. This study endeavors to bridge this gap by\nestablishing an equivalence relationship between models of the GARCH family and\ntheir corresponding NN counterparts. With the equivalence relationship\nestablished, we introduce an innovative approach, named GARCH-NN, for\nconstructing NN-based volatility models. It obtains the NN counterparts of\nGARCH models and integrates them as components into an established NN\narchitecture, thereby seamlessly infusing volatility stylized facts (SFs)\ninherent in the GARCH models into the neural network. We develop the GARCH-LSTM\nmodel to showcase the power of the GARCH-NN approach. Experiment results\nvalidate that amalgamating the NN counterparts of the GARCH family models into\nestablished NN models leads to enhanced outcomes compared to employing the\nstochastic and NN models in isolation.\n"
    },
    {
        "paper_id": 2402.06656,
        "authors": "Yuan Gao, Haokun Chen, Xiang Wang, Zhicai Wang, Xue Wang, Jinyang Gao,\n  Bolin Ding",
        "title": "DiffsFormer: A Diffusion Transformer on Stock Factor Augmentation",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Machine learning models have demonstrated remarkable efficacy and efficiency\nin a wide range of stock forecasting tasks. However, the inherent challenges of\ndata scarcity, including low signal-to-noise ratio (SNR) and data homogeneity,\npose significant obstacles to accurate forecasting. To address this issue, we\npropose a novel approach that utilizes artificial intelligence-generated\nsamples (AIGS) to enhance the training procedures. In our work, we introduce\nthe Diffusion Model to generate stock factors with Transformer architecture\n(DiffsFormer). DiffsFormer is initially trained on a large-scale source domain,\nincorporating conditional guidance so as to capture global joint distribution.\nWhen presented with a specific downstream task, we employ DiffsFormer to\naugment the training procedure by editing existing samples. This editing step\nallows us to control the strength of the editing process, determining the\nextent to which the generated data deviates from the target domain. To evaluate\nthe effectiveness of DiffsFormer augmented training, we conduct experiments on\nthe CSI300 and CSI800 datasets, employing eight commonly used machine learning\nmodels. The proposed method achieves relative improvements of 7.2% and 27.8% in\nannualized return ratio for the respective datasets. Furthermore, we perform\nextensive experiments to gain insights into the functionality of DiffsFormer\nand its constituent components, elucidating how they address the challenges of\ndata scarcity and enhance the overall model performance. Our research\ndemonstrates the efficacy of leveraging AIGS and the DiffsFormer architecture\nto mitigate data scarcity in stock forecasting tasks.\n"
    },
    {
        "paper_id": 2402.06689,
        "authors": "Himanshu Gupta and Aditya Jaiswal",
        "title": "A Study on Stock Forecasting Using Deep Learning and Statistical Models",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Predicting a fast and accurate model for stock price forecasting is been a\nchallenging task and this is an active area of research where it is yet to be\nfound which is the best way to forecast the stock price. Machine learning, deep\nlearning and statistical analysis techniques are used here to get the accurate\nresult so the investors can see the future trend and maximize the return of\ninvestment in stock trading. This paper will review many deep learning\nalgorithms for stock price forecasting. We use a record of s&p 500 index data\nfor training and testing. The survey motive is to check various deep learning\nand statistical model techniques for stock price forecasting that are Moving\nAverages, ARIMA which are statistical techniques and LSTM, RNN, CNN, and FULL\nCNN which are deep learning models. It will discuss various models, including\nthe Auto regression integration moving average model, the Recurrent neural\nnetwork model, the long short-term model which is the type of RNN used for long\ndependency for data, the convolutional neural network model, and the full\nconvolutional neural network model, in terms of error calculation or percentage\nof accuracy that how much it is accurate which measures by the function like\nRoot mean square error, mean absolute error, mean squared error. The model can\nbe used to predict the stock price by checking the low MAE value as lower the\nMAE value the difference between the predicting and the actual value will be\nless and this model will predict the price more accurately than other models.\n"
    },
    {
        "paper_id": 2402.06698,
        "authors": "Zihan Dong, Xinyu Fan, Zhiyuan Peng",
        "title": "FNSPID: A Comprehensive Financial News Dataset in Time Series",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Financial market predictions utilize historical data to anticipate future\nstock prices and market trends. Traditionally, these predictions have focused\non the statistical analysis of quantitative factors, such as stock prices,\ntrading volumes, inflation rates, and changes in industrial production. Recent\nadvancements in large language models motivate the integrated financial\nanalysis of both sentiment data, particularly market news, and numerical\nfactors. Nonetheless, this methodology frequently encounters constraints due to\nthe paucity of extensive datasets that amalgamate both quantitative and\nqualitative sentiment analyses. To address this challenge, we introduce a\nlarge-scale financial dataset, namely, Financial News and Stock Price\nIntegration Dataset (FNSPID). It comprises 29.7 million stock prices and 15.7\nmillion time-aligned financial news records for 4,775 S&P500 companies,\ncovering the period from 1999 to 2023, sourced from 4 stock market news\nwebsites. We demonstrate that FNSPID excels existing stock market datasets in\nscale and diversity while uniquely incorporating sentiment information. Through\nfinancial analysis experiments on FNSPID, we propose: (1) the dataset's size\nand quality significantly boost market prediction accuracy; (2) adding\nsentiment scores modestly enhances performance on the transformer-based model;\n(3) a reproducible procedure that can update the dataset. Completed work, code,\ndocumentation, and examples are available at github.com/Zdong104/FNSPID. FNSPID\noffers unprecedented opportunities for the financial research community to\nadvance predictive modeling and analysis.\n"
    },
    {
        "paper_id": 2402.06714,
        "authors": "Ciaran O'Connor and Joseph Collins and Steven Prestwich and Andrea\n  Visentin",
        "title": "Electricity Price Forecasting in the Irish Balancing Market",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Short-term electricity markets are becoming more relevant due to\nless-predictable renewable energy sources, attracting considerable attention\nfrom the industry. The balancing market is the closest to real-time and the\nmost volatile among them. Its price forecasting literature is limited,\ninconsistent and outdated, with few deep learning attempts and no public\ndataset. This work applies to the Irish balancing market a variety of price\nprediction techniques proven successful in the widely studied day-ahead market.\nWe compare statistical, machine learning, and deep learning models using a\nframework that investigates the impact of different training sizes. The\nframework defines hyperparameters and calibration settings; the dataset and\nmodels are made public to ensure reproducibility and to be used as benchmarks\nfor future works. An extensive numerical study shows that well-performing\nmodels in the day-ahead market do not perform well in the balancing one,\nhighlighting that these markets are fundamentally different constructs. The\nbest model is LEAR, a statistical approach based on LASSO, which outperforms\nmore complex and computationally demanding approaches.\n"
    },
    {
        "paper_id": 2402.06731,
        "authors": "Matthew Willetts, Christian Harrington",
        "title": "Closed-form solutions for generic N-token AMM arbitrage",
        "comments": "6 pages plus appendix",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Convex optimisation has provided a mechanism to determine arbitrage trades on\nautomated market markets (AMMs) since almost their inception. Here we outline\ngeneric closed-form solutions for $N$-token geometric mean market maker pool\narbitrage, that in simulation (with synthetic and historic data) provide better\narbitrage opportunities than convex optimisers and is able to capitalise on\nthose opportunities sooner. Furthermore, the intrinsic parallelism of the\nproposed approach (unlike convex optimisation) offers the ability to scale on\nGPUs, opening up a new approach to AMM modelling by offering an alternative to\nnumerical-solver-based methods. The lower computational cost of running this\nnew mechanism can also enable on-chain arbitrage bots for multi-asset pools.\n"
    },
    {
        "paper_id": 2402.06758,
        "authors": "Martin Kittel, Wolf-Peter Schill",
        "title": "Measuring the Dunkelflaute: How (not) to analyze variable renewable\n  energy shortage",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  As variable renewable energy sources increasingly gain importance in global\nenergy systems, there is a growing interest in understanding periods of\nvariable renewable energy shortage (\"Dunkelflauten\"). Defining, quantifying,\nand comparing such shortage events across different renewable generation\ntechnologies and locations presents a surprisingly intricate challenge. Various\nmethodological approaches exist in different bodies of literature, which have\nbeen applied to single technologies in specific locations or technology\nportfolios across multiple regions. We provide an overview of various methods\nfor quantifying variable renewable energy shortage, focusing either on supply\nfrom variable renewables or its mismatch with electricity demand. We explain\nand critically discuss the merits and challenges of different approaches for\ndefining and identifying shortage events and propose further methodological\nimprovements for more accurate shortage determination. Additionally, we\nelaborate on comparability requirements for multi-technological and\nmulti-regional energy shortage analysis. In doing so, we aim to contribute to\nunifying disparate methodologies, harmonizing terminologies, and providing\nguidance for future research.\n"
    },
    {
        "paper_id": 2402.0684,
        "authors": "Duy-Minh Dang and Hao Zhou",
        "title": "A monotone piecewise constant control integration approach for the\n  two-factor uncertain volatility model",
        "comments": "31 pages, 2 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Prices of option contracts on two assets within uncertain volatility models\nfor worst and best-case scenarios satisfy a two-dimensional\nHamilton-Jacobi-Bellman (HJB) partial differential equation (PDE) with cross\nderivatives terms. Traditional methods mainly involve finite differences and\npolicy iteration. This \"discretize, then optimize\" paradigm requires complex\nrotations of computational stencils for monotonicity.\n  This paper presents a novel and more streamlined \"decompose and integrate,\nthen optimize\" approach to tackle the aforementioned HJB PDE. Within each\ntimestep, our strategy employs a piecewise constant control, breaking down the\nHJB PDE into independent linear two-dimensional PDEs. Using known closed-form\nexpressions for the Fourier transforms of the Green's functions associated with\nthese PDEs, we determine an explicit formula for these functions. Since the\nGreen's functions are non-negative, the solutions to the PDEs, cast as\ntwo-dimensional convolution integrals, can be conveniently approximated using a\nmonotone integration method. Such integration methods, including a composite\nquadrature rule, are generally available in popular programming languages. To\nfurther enhance efficiency, we propose an implementation of this monotone\nintegration scheme via Fast Fourier Transforms, exploiting the Toeplitz matrix\nstructure. Optimal control is subsequently obtained by efficiently synthesizing\nthe solutions of the individual PDEs.\n  The proposed monotone piecewise constant control method is demonstrated to be\nboth $\\ell_{\\infty} $-stable and consistent in the viscosity sense, ensuring\nits convergence to the viscosity solution of the HJB equation. Numerical\nresults show remarkable agreement with benchmark solutions obtained by\nunconditionally monotone finite differences, tree methods, and Monte Carlo\nsimulation, underscoring the robustness and effectiveness of our method.\n"
    },
    {
        "paper_id": 2402.0708,
        "authors": "Tao Ren, Ruihan Zhou, Jinyang Jiang, Jiafeng Liang, Qinghao Wang,\n  Yijie Peng",
        "title": "RiskMiner: Discovering Formulaic Alphas via Risk Seeking Monte Carlo\n  Tree Search",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The formulaic alphas are mathematical formulas that transform raw stock data\ninto indicated signals. In the industry, a collection of formulaic alphas is\ncombined to enhance modeling accuracy. Existing alpha mining only employs the\nneural network agent, unable to utilize the structural information of the\nsolution space. Moreover, they didn't consider the correlation between alphas\nin the collection, which limits the synergistic performance. To address these\nproblems, we propose a novel alpha mining framework, which formulates the alpha\nmining problems as a reward-dense Markov Decision Process (MDP) and solves the\nMDP by the risk-seeking Monte Carlo Tree Search (MCTS). The MCTS-based agent\nfully exploits the structural information of discrete solution space and the\nrisk-seeking policy explicitly optimizes the best-case performance rather than\naverage outcomes. Comprehensive experiments are conducted to demonstrate the\nefficiency of our framework. Our method outperforms all state-of-the-art\nbenchmarks on two real-world stock sets under various metrics. Backtest\nexperiments show that our alphas achieve the most profitable results under a\nrealistic trading setting.\n"
    },
    {
        "paper_id": 2402.07124,
        "authors": "Helena Povoa, Alessandro V. M. Oliveira",
        "title": "Econometric analysis to estimate the impact of holidays on airfares",
        "comments": null,
        "journal-ref": "Journal of Transport Literature, 7, 284-296, 2013",
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  The number of air transportation passengers during the holidays in Brazil has\ngrown notably since the late nineties. One of the reasons is greater\ncompetition in airfares made possible by economic liberalization. This paper\npresents an econometric model of airline pricing aiming at estimating the\nimpacts of holiday periods on fares, with special emphasis on three-day holiday\nevents. It makes use of a database with daily collected data from the internet\nbetween 2008 and 2010 for the major Brazilian city, Sao Paulo. The econometric\npanel data model employs a two-way error components \"within\" estimator,\ncontrolling for airline/airport-pair fixed effect along with quotation and\ndeparture months effects. The decomposition of time effects between quotation\nand departure month effects is the main methodological contribution of the\npaper. Results allow for a comparative analysis of the performance of Sao\nPaulo's downtown and international airports - respectively, Congonhas (CGH),\nand Guarulhos (GRU) airports. As a result, the price of tickets bought 60 days\nin advance for flights with two stops leaving from the downtown airport fell by\nmost.\n"
    },
    {
        "paper_id": 2402.07125,
        "authors": "Frederico A. Turolla, Moises D. Vassallo, Alessandro V. M. Oliveira",
        "title": "Intermodal competition in the Brazilian interstate travel market",
        "comments": null,
        "journal-ref": "Revista de Analisis Economico, vol. 23, n. 1, pp. 21-33, June 2008",
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  This paper presents a test of intermodal interaction between coaches and\nairlines in Brazil in order to check for the efficacy of recent liberalization\nmeasures designed to promote competition in both industries. Interstate travel\nservice in the country is heavily provided by coaches, and the system is fully\noperated by the private sector under public delegation through permits and\nauthorizations. Agency-based regulation was introduced in 2002 along with a\nprice cap regime aimed at enhancing the flexibility to change fares in response\nto demand and cost conditions. By making use of a reaction function-based model\nof coach operators' pricing decisions in the interstate travel market, we then\nestimate the sensitivity of the changes in coach fares to the changes in\nairline fares in a simultaneous-equation framework. Intermodal interaction\namong coach operators and airlines is found to be highly significant and\nprobably due to the competition for a small but increasing set of premium,\nquality-sensitive, coach passengers.\n"
    },
    {
        "paper_id": 2402.07134,
        "authors": "Cathy W.S. Chen, Takaaki Koike, Wei-Hsuan Shau",
        "title": "Tail risk forecasting with semi-parametric regression models by\n  incorporating overnight information",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This research incorporates realized volatility and overnight information into\nrisk models, wherein the overnight return often contributes significantly to\nthe total return volatility. Extending a semi-parametric regression model based\non asymmetric Laplace distribution, we propose a family of RES-CAViaR-oc models\nby adding overnight return and realized measures as a nowcasting technique for\nsimultaneously forecasting Value-at-Risk (VaR) and expected shortfall (ES). We\nutilize Bayesian methods to estimate unknown parameters and forecast VaR and ES\njointly for the proposed model family. We also conduct extensive backtests\nbased on joint elicitability of the pair of VaR and ES during the out-of sample\nperiod. Our empirical study on four international stock indices confirms that\novernight return and realized volatility are vital in tail risk forecasting.\n"
    },
    {
        "paper_id": 2402.0717,
        "authors": "Wenbo Lyu",
        "title": "Research on the multi-stage impact of digital economy on rural\n  revitalization in Hainan Province based on GPM model",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The rapid development of the digital economy has had a profound impact on the\nimplementation of the rural revitalization strategy. Based on this, this study\ntakes Hainan Province as the research object to deeply explore the impact of\ndigital economic development on rural revitalization. The study collected panel\ndata from 2003 to 2022 to construct an evaluation index system for the digital\neconomy and rural revitalization and used panel regression analysis and other\nmethods to explore the promotion effect of the digital economy on rural\nrevitalization. Research results show that the digital economy has a\nsignificant positive impact on rural revitalization, and this impact increases\nas the level of fiscal expenditure increases. The issuance of digital RMB has\nfurther exerted a regulatory effect and promoted the development of the digital\neconomy and the process of rural revitalization. At the same time, the\nestablishment of the Hainan Free Trade Port has also played a positive role in\npromoting the development of the digital economy and rural revitalization. In\nthe prediction of the optimal strategy for rural revitalization based on the\ndevelopment levels of the primary, secondary, and tertiary industries (Rate1,\nRate2, and Rate3), it was found that rate1 can encourage Hainan Province to\nimplement digital economic innovation, encourage rate3 to implement promotion\nbehaviors, and increase rate2 can At the level of sustainable development when\nrate3 promotes rate2's digital economic innovation behavior, it can standardize\nrate2's production behavior to the greatest extent, accelerate the faster\napplication of the digital economy to the rural revitalization industry, and\npromote the technological advancement of enterprises.\n"
    },
    {
        "paper_id": 2402.07185,
        "authors": "Paolo Guasoni, Kasper Larsen, Giovanni Leoni",
        "title": "Existence of an equilibrium with limited stock market participation and\n  power utilities",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  For constants $\\gamma \\in (0,1)$ and $A\\in (1,\\infty)$, we prove existence\nand uniqueness of a solution to the singular and path-dependent Riccati-type\nODE \\begin{align*} \\begin{cases} h'(y) = \\frac{1+\\gamma}{y}\\big( \\gamma -\nh(y)\\big)+h(y)\\frac{\\gamma + \\big((A-\\gamma)e^{\\int_y^1\n\\frac{1-h(q)}{1-q}dq}-A\\big)h(y)}{1-y},\\quad y\\in(0,1), h(0) = \\gamma, \\quad\nh(1) = 1. \\end{cases} \\end{align*} As an application, we use the ODE solution\nto prove existence of a Radner equilibrium with homogenous power-utility\ninvestors in the limited participation model from Basak and Cuoco (1998).\n"
    },
    {
        "paper_id": 2402.0719,
        "authors": "Alessandro V. M. Oliveira",
        "title": "Transporte Aereo: Economia e Politicas Publicas",
        "comments": "Pezco Editora & Desenvolvimento - 1st Edition, 2009. In Portuguese",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  This book, written in Portuguese, presents a comprehensive analysis of the\nair transport industry in Brazil, highlighting its vital importance to the\ncountry's economy. It explores the sector's complexity, from economic\ncharacteristics to interaction with the national aeronautical industry, through\nthe specialization of the workforce and market demand analysis. The book delves\ninto the economic regulation of air transport, tracing the evolution from\nperiods of strict regulation to phases of liberalization and deregulation, and\nexamines market dynamics, focusing on concentration and competitiveness. It\nalso analyzes demand and supply through case studies, investigating everything\nfrom tourists traveling within Brazil to the coverage of the national territory\nand the prices of air tickets. Finally, the book proposes principles for the\nregulation and public policies of the air sector, emphasizing the priority of\nthe passenger, the business environment, access to air transport, and economic\nefficiency, culminating in the advocacy for a free market, but with protection\nfor competition and the consumer.\n"
    },
    {
        "paper_id": 2402.0721,
        "authors": "Mingyang Li, Han Pengsihua, Songqing Zhao, Zejun Wang, Limin Yang,\n  Weian Liu",
        "title": "Fukushima Nuclear Wastewater Discharge: An Evolutionary Game Theory\n  Approach to International and Domestic Interaction and Strategic\n  Decision-Making",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  On August 24, 2023, Japan controversially decided to discharge nuclear\nwastewater from the Fukushima Daiichi Nuclear Power Plant into the ocean,\nsparking intense domestic and global debates. This study uses evolutionary game\ntheory to analyze the strategic dynamics between Japan, other countries, and\nthe Japan Fisheries Association. By incorporating economic, legal,\ninternational aid, and environmental factors, the research identifies three\nevolutionarily stable strategies, analyzing them via numerical simulations. The\nfocus is on Japan's shift from wastewater release to its cessation, exploring\nthe myriad factors influencing this transition and their effects on\nstakeholders' decisions. Key insights highlight the need for international\ncooperation, rigorous scientific research, public education, and effective\nwastewater treatment methods. Offering both a fresh theoretical perspective and\npractical guidance, this study aims to foster global consensus on nuclear\nwastewater management, crucial for marine conservation and sustainable\ndevelopment.\n"
    },
    {
        "paper_id": 2402.07227,
        "authors": "Mingyang Li, Han Pengsihua, Fujiao Meng, Zejun Wang, Weian Liu",
        "title": "Time-Delayed Game Strategy Analysis Among Japan, Other Nations, and the\n  International Atomic Energy Agency in the Context of Fukushima Nuclear\n  Wastewater Discharge Decision",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This academic paper examines the strategic interactions between Japan, other\nnations, and the International Atomic Energy Agency (IAEA) regarding Japan's\ndecision to release treated nuclear wastewater from the Fukushima Daiichi\nNuclear Power Plant into the sea. It introduces a payoff matrix and time-delay\nelements in replicator dynamic equations to mirror real-world decision-making\ndelays. The paper analyzes the stability of strategies and conditions for\ndifferent stable states using characteristic roots of a linearized system and\nnumerical simulations. It concludes that time delays significantly affect\ndecision-making stability and evolution trajectories in nuclear wastewater\ndisposal strategies. The study highlights the importance of efficient\nwastewater treatment technology, the impact of export tax revenue losses on\nJapan's strategies, and the role of international cooperation. The novelty of\nthe research lies in integrating time-delay elements from ocean dynamics and\ngovernmental decision-making into the game-theoretical model.\n"
    },
    {
        "paper_id": 2402.07235,
        "authors": "Wei Yang Tham, Joseph Staudt, Elisabeth Ruth Perlman, Stephanie D.\n  Cheng",
        "title": "Scientific Talent Leaks Out of Funding Gaps",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  We study how delays in NIH grant funding affect the career outcomes of\nresearch personnel. Using comprehensive earnings and tax records linked to\nuniversity transaction data along with a difference-in-differences design, we\nfind that a funding interruption of more than 30 days has a substantial effect\non job placements for personnel who work in labs with a single NIH R01 research\ngrant, including a 3 percentage point (40%) increase in the probability of not\nworking in the US. Incorporating information from the full 2020 Decennial\nCensus and data on publications, we find that about half of those induced into\nnonemployment appear to permanently leave the US and are 90% less likely to\npublish in a given year, with even larger impacts for trainees (postdocs and\ngraduate students). Among personnel who continue to work in the US, we find\nthat interrupted personnel earn 20% less than their continuously-funded peers,\nwith the largest declines concentrated among trainees and other non-faculty\npersonnel (such as staff and undergraduates). Overall, funding delays account\nfor about 5% of US nonemployment in our data, indicating that they have a\nmeaningful effect on the scientific labor force at the national level.\n"
    },
    {
        "paper_id": 2402.07266,
        "authors": "Povilas Lastauskas and Anh Dinh Minh Nguyen",
        "title": "Spillover Effects of US Monetary Policy on Emerging Markets Amidst\n  Uncertainty",
        "comments": null,
        "journal-ref": null,
        "doi": "10.1016/j.intfin.2024.101956",
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  This paper examines the impact of US monetary policy tightening on emerging\nmarkets, distinguishing between direct and indirect spillover effects using the\nglobal vector autoregression with stochastic volatility covering 32 countries.\nThe paper demonstrates that an increase in the US interest rate significantly\nreduces output for emerging markets, leading to larger, more prolonged, and\npersistent declines. Such an impact is further intensified by global trade\nintegration, causing a sharper yet slightly quicker rebounding output drop. The\nspillover effects are significantly amplified when US monetary policy\ntightening is accompanied by an increase in monetary policy uncertainty.\nFinally, emerging markets exhibit considerable heterogeneity in their responses\nto US monetary policy shocks.\n"
    },
    {
        "paper_id": 2402.07277,
        "authors": "Gaurab Aryal and Charles Murry and Pallavi Pal and Arnab Palit",
        "title": "Bundling Demand in K-12 Broadband Procurement",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We evaluate the effects of bundling demand for broadband internet by K-12\nschools. In 2014, New Jersey switched from decentralized procurements to a new\nprocurement system that bundled schools into four regional groups. Using an\nevent study approach, we find that, on average, prices for participants\ndecreased by one-third, and the broadband speed purchased increased sixfold.\nBounding the change in school expenditures due to the program, we find that\nparticipants saved at least as much as their total E-rate subsidy from the\nfederal government. Furthermore, under the assumption of log-concave demand, we\nshow that participating schools experienced large welfare gains.\n"
    },
    {
        "paper_id": 2402.07435,
        "authors": "Narayan Tondapu",
        "title": "Analyzing Currency Fluctuations: A Comparative Study of GARCH, EWMA, and\n  IV Models for GBP/USD and EUR/GBP Pairs",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-sa/4.0/",
        "abstract": "  In this study, we examine the fluctuation in the value of the Great Britain\nPound (GBP). We focus particularly on its relationship with the United States\nDollar (USD) and the Euro (EUR) currency pairs. Utilizing data from June 15,\n2018, to June 15, 2023, we apply various mathematical models to assess their\neffectiveness in predicting the 20-day variation in the pairs' daily returns.\nOur analysis involves the implementation of Exponentially Weighted Moving\nAverage (EWMA), Generalized Autoregressive Conditional Heteroskedasticity\n(GARCH) models, and Implied Volatility (IV) models. To evaluate their\nperformance, we compare the accuracy of their predictions using Root Mean\nSquare Error (RMSE) and Mean Absolute Error (MAE) metrics. We delve into the\nintricacies of GARCH models, examining their statistical characteristics when\napplied to the provided dataset. Our findings suggest the existence of\nasymmetric returns in the EUR/GBP pair, while such evidence is inconclusive for\nthe GBP/USD pair. Additionally, we observe that GARCH-type models better fit\nthe data when assuming residuals follow a standard t-distribution rather than a\nstandard normal distribution. Furthermore, we investigate the efficacy of\ndifferent forecasting techniques within GARCH-type models. Comparing rolling\nwindow forecasts to expanding window forecasts, we find no definitive\nsuperiority in either approach across the tested scenarios. Our experiments\nreveal that for the GBP/USD pair, the most accurate volatility forecasts stem\nfrom the utilization of GARCH models employing a rolling window methodology.\nConversely, for the EUR/GBP pair, optimal forecasts are derived from GARCH\nmodels and Ordinary Least Squares (OLS) models incorporating the annualized\nimplied volatility of the exchange rate as an independent variable.\n"
    },
    {
        "paper_id": 2402.07503,
        "authors": "Micha{\\l} Barski, Rafa{\\l} {\\L}ochowski",
        "title": "Affine term structure models driven by independent L\\'evy processes",
        "comments": "arXiv admin note: substantial text overlap with arXiv:2303.08477,\n  arXiv:2204.07245",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  We characterize affine term structure models of non-negative short rate $R$\nwhich may be obtained as solutions of autonomous SDEs driven by independent,\none-dimensional L\\'evy martingales, that is equations of the form $$\ndR(r)=F(R(t))dt+\\sum_{i=1}^{d}G_i(R(t-))dZ_i(t), \\quad R(0)=r_0\\geq 0, \\quad\nt>0, \\quad (1)$$ with deterministic real functions $F,G_1,...,G_d$ and\nindependent one-dimensional L\\'evy martingales $Z_1,...,Z_d$. Using a general\nresult on the form of the generators of affine term structure models due to\nFilipovi\\'c, it is shown, under the assumption that the Laplace transforms of\nthe driving noises are regularly varying, that all possible solutions $R$ of\n(1) may be obtained also as solutions of autonomous SDEs driven by independent\nstable processes with stability indices in the range $(1,2]$. The obtained\nmodels include in particular the $\\alpha$-CIR model, introduced by Jiao et al.,\nwhich proved to be still simple yet more reliable than the classical CIR model.\nResults on heavy tails of $R$ and its limit distribution in terms of the\nstability indices are proven. Finally, results of numerical calibration of the\nobtained models to the market term structure of interest rates are presented\nand compared with the CIR and $\\alpha$-CIR models.\n"
    },
    {
        "paper_id": 2402.0796,
        "authors": "Wenbo Lyu and Jiayi Zhu and Yunan Ding and Keming Zhang",
        "title": "An Analysis of the Recovery Path of the Consumer Sector in the\n  Post-Pandemic Era",
        "comments": null,
        "journal-ref": null,
        "doi": "10.4236/ojbm.2023.113061",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper proposes a referencable pattern of the recovery of the consumption\nsector, a new dimension to observe and evaluate the intrinsic value of the\nconsumption sector, and proposes the concept of sensory-based consumption and\nthe ranking of the weights of different categories;creates the concept of\ndigital consumption index, coupled with digital RMB index and China-style\ndigital economy index. Finally we explain the internal logic of digital\nconsumption as a consumption upgrade tool and a higher valuation target in the\ncontext of China's economic performance in 2022 and the Chinese government's\npolicy in 2023, leading to the investment strategy of roller conduction effect.\n"
    },
    {
        "paper_id": 2402.08071,
        "authors": "Sunday Akukodi Ugwu",
        "title": "Contagion on Financial Networks: An Introduction",
        "comments": "Mfano Africa-Oxford Mathematics 2023 sponsored mini-project",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  This mini-project models propagation of shocks, in time point, through links\nin connected banks. In particular, financial network of 100 banks out of which\n15 are shocked to default (that is, 85.00% of the banks are solvent) is\nmodelled using Erdos and Renyi network -- directed, weighted and randomly\ngenerated network. Shocking some banks in a financial network implies removing\ntheir assets and redistributing their liabilities to other connected ones in\nthe network. The banks are nodes and two ranges of probability values determine\ntendency of having a link between a pair of banks. Our major finding shows that\nthe ranges of probability values and banks' percentage solvency have positive\ncorrelation.\n"
    },
    {
        "paper_id": 2402.08108,
        "authors": "Kasper Johansson and Thomas Schmelzer and Stephen Boyd",
        "title": "Finding Moving-Band Statistical Arbitrages via Convex-Concave\n  Optimization",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We propose a new method for finding statistical arbitrages that can contain\nmore assets than just the traditional pair. We formulate the problem as seeking\na portfolio with the highest volatility, subject to its price remaining in a\nband and a leverage limit. This optimization problem is not convex, but can be\napproximately solved using the convex-concave procedure, a specific sequential\nconvex programming method. We show how the method generalizes to finding\nmoving-band statistical arbitrages, where the price band midpoint varies over\ntime.\n"
    },
    {
        "paper_id": 2402.08143,
        "authors": "Evangelos Katsamakas, Oleg V. Pavlov, and Ryan Saklad",
        "title": "Artificial intelligence and the transformation of higher education\n  institutions",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Artificial intelligence (AI) advances and the rapid adoption of generative AI\ntools like ChatGPT present new opportunities and challenges for higher\neducation. While substantial literature discusses AI in higher education, there\nis a lack of a systemic approach that captures a holistic view of the AI\ntransformation of higher education institutions (HEIs). To fill this gap, this\narticle, taking a complex systems approach, develops a causal loop diagram\n(CLD) to map the causal feedback mechanisms of AI transformation in a typical\nHEI. Our model accounts for the forces that drive the AI transformation and the\nconsequences of the AI transformation on value creation in a typical HEI. The\narticle identifies and analyzes several reinforcing and balancing feedback\nloops, showing how, motivated by AI technology advances, the HEI invests in AI\nto improve student learning, research, and administration. The HEI must take\nmeasures to deal with academic integrity problems and adapt to changes in\navailable jobs due to AI, emphasizing AI-complementary skills for its students.\nHowever, HEIs face a competitive threat and several policy traps that may lead\nto decline. HEI leaders need to become systems thinkers to manage the\ncomplexity of the AI transformation and benefit from the AI feedback loops\nwhile avoiding the associated pitfalls. We also discuss long-term scenarios,\nthe notion of HEIs influencing the direction of AI, and directions for future\nresearch on AI transformation.\n"
    },
    {
        "paper_id": 2402.08233,
        "authors": "Fabian Krause, Jan-Peter Calliess",
        "title": "End-to-End Policy Learning of a Statistical Arbitrage Autoencoder\n  Architecture",
        "comments": "11 pages, 1 figure",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  In Statistical Arbitrage (StatArb), classical mean reversion trading\nstrategies typically hinge on asset-pricing or PCA based models to identify the\nmean of a synthetic asset. Once such a (linear) model is identified, a separate\nmean reversion strategy is then devised to generate a trading signal. With a\nview of generalising such an approach and turning it truly data-driven, we\nstudy the utility of Autoencoder architectures in StatArb. As a first approach,\nwe employ a standard Autoencoder trained on US stock returns to derive trading\nstrategies based on the Ornstein-Uhlenbeck (OU) process. To further enhance\nthis model, we take a policy-learning approach and embed the Autoencoder\nnetwork into a neural network representation of a space of portfolio trading\npolicies. This integration outputs portfolio allocations directly and is\nend-to-end trainable by backpropagation of the risk-adjusted returns of the\nneural policy. Our findings demonstrate that this innovative end-to-end policy\nlearning approach not only simplifies the strategy development process, but\nalso yields superior gross returns over its competitors illustrating the\npotential of end-to-end training over classical two-stage approaches.\n"
    },
    {
        "paper_id": 2402.08387,
        "authors": "Martin Herdegen, David Hobson, Alex S.L. Tse",
        "title": "Portfolio Optimization under Transaction Costs with Recursive\n  Preferences",
        "comments": "67 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  The Merton investment-consumption problem is fundamental, both in the field\nof finance, and in stochastic control. An important extension of the problem\nadds transaction costs, which is highly relevant from a financial perspective\nbut also challenging from a control perspective because the solution now\ninvolves singular control. A further significant extension takes us from\nadditive utility to stochastic differential utility (SDU), which allows time\npreferences and risk preferences to be disentangled.\n  In this paper, we study this extended version of the Merton problem with\nproportional transaction costs and Epstein-Zin SDU. We fully characterise all\nparameter combinations for which the problem is well posed (which may depend on\nthe level of transaction costs) and provide a full verification argument that\nrelies on no additional technical assumptions and uses primal methods only. The\ncase with SDU requires new mathematical techniques as duality methods break\ndown.\n  Even in the special case of (additive) power utility, our arguments are\nsignificantly simpler, more elegant and more far-reaching than the ones in the\nextant literature. This means that we can easily analyse aspects of the problem\nwhich previously have been very challenging, including comparative statics,\nboundary cases which heretofore have required separate treatment and the\nsituation beyond the small transaction cost regime. A key and novel idea is to\nparametrise consumption and the value function in terms of the shadow fraction\nof wealth, which may be of much wider applicability.\n"
    },
    {
        "paper_id": 2402.08513,
        "authors": "Orimar Sauri",
        "title": "Asymptotic Error Distribution of the Euler Scheme for Fractional\n  Stochastic Delay Differential Equations with Additive Noise",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
        "abstract": "  In this paper we consider the Euler scheme for a class of stochastic delay\ndifferential equations driven by a linear fractional $\\alpha$-stable L\\'evy\nmotion with index $H\\in(0,1)$. We establish the consistency of the scheme and\nstudy the limit distribution of the normalized error process. We show that in\nthe rough case, i.e. when $H<1/\\alpha$, the rate of convergence of the\nsimulation error is of order $\\Delta_{n}^{H+1-1/\\alpha}$ and that the limit\nprocess is once again the solution of an (semi-linear) SDDE.\n"
    },
    {
        "paper_id": 2402.08755,
        "authors": "Andrea Coletta, Kshama Dwarakanath, Penghang Liu, Svitlana Vyetrenko,\n  Tucker Balch",
        "title": "LLM-driven Imitation of Subrational Behavior : Illusion or Reality?",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Modeling subrational agents, such as humans or economic households, is\ninherently challenging due to the difficulty in calibrating reinforcement\nlearning models or collecting data that involves human subjects. Existing work\nhighlights the ability of Large Language Models (LLMs) to address complex\nreasoning tasks and mimic human communication, while simulation using LLMs as\nagents shows emergent social behaviors, potentially improving our comprehension\nof human conduct. In this paper, we propose to investigate the use of LLMs to\ngenerate synthetic human demonstrations, which are then used to learn\nsubrational agent policies though Imitation Learning. We make an assumption\nthat LLMs can be used as implicit computational models of humans, and propose a\nframework to use synthetic demonstrations derived from LLMs to model\nsubrational behaviors that are characteristic of humans (e.g., myopic behavior\nor preference for risk aversion). We experimentally evaluate the ability of our\nframework to model sub-rationality through four simple scenarios, including the\nwell-researched ultimatum game and marshmallow experiment. To gain confidence\nin our framework, we are able to replicate well-established findings from prior\nhuman studies associated with the above scenarios. We conclude by discussing\nthe potential benefits, challenges and limitations of our framework.\n"
    },
    {
        "paper_id": 2402.08905,
        "authors": "Takeshi Kato",
        "title": "Time preference, wealth and utility inequality: A microeconomic\n  interaction and dynamic macroeconomic model connection approach",
        "comments": "37 pages, 11 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Based on interactions between individuals and others and references to social\nnorms, this study reveals the impact of heterogeneity in time preference on\nwealth distribution and inequality. We present a novel approach that connects\nthe interactions between microeconomic agents that generate heterogeneity to\nthe dynamic equations for capital and consumption in macroeconomic models.\nUsing this approach, we estimate the impact of changes in the discount rate due\nto microeconomic interactions on capital, consumption and utility and the\ndegree of inequality. The results show that intercomparisons with others\nregarding consumption significantly affect capital, i.e. wealth inequality.\nFurthermore, the impact on utility is never small and social norms can reduce\nthis impact. Our supporting evidence shows that the quantitative results of\ninequality calculations correspond to survey data from cohort and\ncross-cultural studies. This study's micro-macro connection approach can be\ndeployed to connect microeconomic interactions, such as exchange, interest and\ndebt, redistribution, mutual aid and time preference, to dynamic macroeconomic\nmodels.\n"
    },
    {
        "paper_id": 2402.09125,
        "authors": "Richard S.J. Tol",
        "title": "Database for the meta-analysis of the social cost of carbon (v2024.0)",
        "comments": "arXiv admin note: substantial text overlap with arXiv:2105.03656",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  A new version of the database for the meta-analysis of estimates of the\nsocial cost of carbon is presented. New records were added, and new fields on\nthe impact of climate change and the shape of the welfare function. The\ndatabase was extended to co-author and citation networks.\n"
    },
    {
        "paper_id": 2402.09129,
        "authors": "Michael J. Curry, Zhou Fan, David C. Parkes",
        "title": "Optimal Automated Market Makers: Differentiable Economics and Strong\n  Duality",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The role of a market maker is to simultaneously offer to buy and sell\nquantities of goods, often a financial asset such as a share, at specified\nprices. An automated market maker (AMM) is a mechanism that offers to trade\naccording to some predetermined schedule; the best choice of this schedule\ndepends on the market maker's goals. The literature on the design of AMMs has\nmainly focused on prediction markets with the goal of information elicitation.\nMore recent work motivated by DeFi has focused instead on the goal of profit\nmaximization, but considering only a single type of good (traded with a\nnumeraire), including under adverse selection (Milionis et al. 2022). Optimal\nmarket making in the presence of multiple goods, including the possibility of\ncomplex bundling behavior, is not well understood. In this paper, we show that\nfinding an optimal market maker is dual to an optimal transport problem, with\nspecific geometric constraints on the transport plan in the dual. We show that\noptimal mechanisms for multiple goods and under adverse selection can take\nadvantage of bundling, both improved prices for bundled purchases and sales as\nwell as sometimes accepting payment \"in kind.\" We present conjectures of\noptimal mechanisms in additional settings which show further complex behavior.\nFrom a methodological perspective, we make essential use of the tools of\ndifferentiable economics to generate conjectures of optimal mechanisms, and\ngive a proof-of-concept for the use of such tools in guiding theoretical\ninvestigations.\n"
    },
    {
        "paper_id": 2402.09194,
        "authors": "Marah-Lisanne Thormann, Phan Tu Vuong, Alain B. Zemkoho",
        "title": "The Boosted Difference of Convex Functions Algorithm for Value-at-Risk\n  Constrained Portfolio Optimization",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  A highly relevant problem of modern finance is the design of Value-at-Risk\n(VaR) optimal portfolios. Due to contemporary financial regulations, banks and\nother financial institutions are tied to use the risk measure to control their\ncredit, market and operational risks. For a portfolio with a discrete return\ndistribution and finitely many scenarios, a Difference of Convex (DC) functions\nrepresentation of the VaR can be derived. Wozabal (2012) showed that this\nyields a solution to a VaR constrained Markowitz style portfolio selection\nproblem using the Difference of Convex Functions Algorithm (DCA). A recent\nalgorithmic extension is the so-called Boosted Difference of Convex Functions\nAlgorithm (BDCA) which accelerates the convergence due to an additional line\nsearch step. It has been shown that the BDCA converges linearly for solving\nnon-smooth quadratic problems with linear inequality constraints. In this\npaper, we prove that the linear rate of convergence is also guaranteed for a\npiecewise linear objective function with linear equality and inequality\nconstraints using the Kurdyka-{\\L}ojasiewicz property. An extended case study\nunder consideration of best practices for comparing optimization algorithms\ndemonstrates the superiority of the BDCA over the DCA for real-world financial\nmarket data. We are able to show that the results of the BDCA are significantly\ncloser to the efficient frontier compared to the DCA. Due to the open\navailability of all data sets and code, this paper further provides a practical\nguide for transparent and easily reproducible comparisons of VaR constrained\nportfolio selection problems in Python.\n"
    },
    {
        "paper_id": 2402.09243,
        "authors": "Jaehyuk Choi",
        "title": "Exact simulation scheme for the Ornstein-Uhlenbeck driven stochastic\n  volatility model with the Karhunen-Lo\\`eve expansions",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This study proposes a new exact simulation scheme of the Ornstein-Uhlenbeck\ndriven stochastic volatility model. With the Karhunen-Lo\\`eve expansions, the\nstochastic volatility path following the Ornstein-Uhlenbeck process is\nexpressed as a sine series, and the time integrals of volatility and variance\nare analytically derived as the sums of independent normal random variates. The\nnew method is several hundred times faster than Li and Wu [Eur. J. Oper. Res.,\n2019, 275(2), 768-779] that relies on computationally expensive numerical\ntransform inversion. The simulation algorithm is further improved with the\nconditional Monte-Carlo method and the martingale-preserving control variate on\nthe spot price.\n"
    },
    {
        "paper_id": 2402.09317,
        "authors": "Robert Denkert and Ulrich Horst",
        "title": "Extended mean-field games with multi-dimensional singular controls and\n  non-linear jump impact",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We establish a probabilistic framework for analysing extended mean-field\ngames with multi-dimensional singular controls and state-dependent jump\ndynamics and costs. Two key challenges arise when analysing such games: the\nstate dynamics may not depend continuously on the control and the reward\nfunction may not be u.s.c.~Both problems can be overcome by restricting the set\nof admissible singular controls to controls that can be approximated by\ncontinuous ones. We prove that the corresponding set of admissible weak\ncontrols is given by the weak solutions to a Marcus-type SDE and provide an\nexplicit characterisation of the reward function. The reward function will in\ngeneral only be u.s.c.~To address the lack of continuity we introduce a novel\nclass of MFGs with a broader set of admissible controls, called MFGs of\nparametrisations. Parametrisations are laws of state/control processes that\ncontinuously interpolate jumps. We prove that the reward functional is\ncontinuous on the set of parametrisations, establish the existence of\nequilibria in MFGs of parametrisations, and show that the set of Nash\nequilibria in MFGs of parametrisations and in the underlying MFG with singular\ncontrols coincide. This shows that MFGs of parametrisations provide a canonical\nframework for analysing MFGs with singular controls and non-linear jump impact.\n"
    },
    {
        "paper_id": 2402.09495,
        "authors": "Catayoun Azarm, Erman Acar, Mickey van Zeelt",
        "title": "On the Potential of Network-Based Features for Fraud Detection",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Online transaction fraud presents substantial challenges to businesses and\nconsumers, risking significant financial losses. Conventional rule-based\nsystems struggle to keep pace with evolving fraud tactics, leading to high\nfalse positive rates and missed detections. Machine learning techniques offer a\npromising solution by leveraging historical data to identify fraudulent\npatterns. This article explores using the personalised PageRank (PPR) algorithm\nto capture the social dynamics of fraud by analysing relationships between\nfinancial accounts. The primary objective is to compare the performance of\ntraditional features with the addition of PPR in fraud detection models.\nResults indicate that integrating PPR enhances the model's predictive power,\nsurpassing the baseline model. Additionally, the PPR feature provides unique\nand valuable information, evidenced by its high feature importance score.\nFeature stability analysis confirms consistent feature distributions across\ntraining and test datasets.\n"
    },
    {
        "paper_id": 2402.09552,
        "authors": "Narun Raman, Taylor Lundy, Samuel Amouyal, Yoav Levine, Kevin\n  Leyton-Brown, Moshe Tennenholtz",
        "title": "STEER: Assessing the Economic Rationality of Large Language Models",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  There is increasing interest in using LLMs as decision-making \"agents.\" Doing\nso includes many degrees of freedom: which model should be used; how should it\nbe prompted; should it be asked to introspect, conduct chain-of-thought\nreasoning, etc? Settling these questions -- and more broadly, determining\nwhether an LLM agent is reliable enough to be trusted -- requires a methodology\nfor assessing such an agent's economic rationality. In this paper, we provide\none. We begin by surveying the economic literature on rational decision making,\ntaxonomizing a large set of fine-grained \"elements\" that an agent should\nexhibit, along with dependencies between them. We then propose a benchmark\ndistribution that quantitatively scores an LLMs performance on these elements\nand, combined with a user-provided rubric, produces a \"STEER report card.\"\nFinally, we describe the results of a large-scale empirical experiment with 14\ndifferent LLMs, characterizing the both current state of the art and the impact\nof different model sizes on models' ability to exhibit rational behavior.\n"
    },
    {
        "paper_id": 2402.09556,
        "authors": "Mika Sutela, Nino Lindstr\\\"om",
        "title": "A game theoretic approach to lowering incentives to violate speed limits\n  in Finland",
        "comments": "26 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  We expand on earlier research on the topic by discussing an infinitely\nrepeated game model with a subgame perfect equilibrium strategy profile (SPE)\nas a solution concept that diminishes incentives to violate speed limits in a\ncarrot and stick fashion. In attempts to construct an SPE strategy profile, the\ninitial state is chosen such that the drivers are playing a mixed strategy\nwhereas the police is not enforcing with certainty. We also postulate a short\nperiod version of the repeated game with generalized stage game payoffs. For\nthis game, we construct a multistage strategy profile that is a Nash\nequilibrium but not an SPE. Some solution candidates are excluded by showing\nthat they do not satisfy a one shot deviation property that is a necessary\ncondition for an SPE profile in a repeated game of perfect information.\n"
    },
    {
        "paper_id": 2402.09563,
        "authors": "Kshama Dwarakanath, Svitlana Vyetrenko, Peyman Tavallali, Tucker Balch",
        "title": "ABIDES-Economist: Agent-Based Simulation of Economic Systems with\n  Learning Agents",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We introduce a multi-agent simulator for economic systems comprised of\nheterogeneous Households, heterogeneous Firms, Central Bank and Government\nagents, that could be subjected to exogenous, stochastic shocks. The\ninteraction between agents defines the production and consumption of goods in\nthe economy alongside the flow of money. Each agent can be designed to act\naccording to fixed, rule-based strategies or learn their strategies using\ninteractions with others in the simulator. We ground our simulator by choosing\nagent heterogeneity parameters based on economic literature, while designing\ntheir action spaces in accordance with real data in the United States. Our\nsimulator facilitates the use of reinforcement learning strategies for the\nagents via an OpenAI Gym style environment definition for the economic system.\nWe demonstrate the utility of our simulator by simulating and analyzing two\nhypothetical (yet interesting) economic scenarios. The first scenario\ninvestigates the impact of heterogeneous household skills on their learned\npreferences to work at different firms. The second scenario examines the impact\nof a positive production shock to one of two firms on its pricing strategy in\ncomparison to the second firm. We aspire that our platform sets a stage for\nsubsequent research at the intersection of artificial intelligence and\neconomics.\n"
    },
    {
        "paper_id": 2402.09746,
        "authors": "Hang Yuan, Saizhuo Wang, Jian Guo",
        "title": "Alpha-GPT 2.0: Human-in-the-Loop AI for Quantitative Investment",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Recently, we introduced a new paradigm for alpha mining in the realm of\nquantitative investment, developing a new interactive alpha mining system\nframework, Alpha-GPT. This system is centered on iterative Human-AI interaction\nbased on large language models, introducing a Human-in-the-Loop approach to\nalpha discovery. In this paper, we present the next-generation Alpha-GPT 2.0\n\\footnote{Draft. Work in progress}, a quantitative investment framework that\nfurther encompasses crucial modeling and analysis phases in quantitative\ninvestment. This framework emphasizes the iterative, interactive research\nbetween humans and AI, embodying a Human-in-the-Loop strategy throughout the\nentire quantitative investment pipeline. By assimilating the insights of human\nresearchers into the systematic alpha research process, we effectively leverage\nthe Human-in-the-Loop approach, enhancing the efficiency and precision of\nquantitative investment research.\n"
    },
    {
        "paper_id": 2402.0982,
        "authors": "Yulu Gong, Mengran Zhu, Shuning Huo, Yafei Xiang, Hanyi Yu",
        "title": "Utilizing Deep Learning for Enhancing Network Resilience in Finance",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  In the age of the Internet, people's lives are increasingly dependent on\ntoday's network technology. Maintaining network integrity and protecting the\nlegitimate interests of users is at the heart of network construction. Threat\ndetection is an important part of a complete and effective defense system. How\nto effectively detect unknown threats is one of the concerns of network\nprotection. Currently, network threat detection is usually based on rules and\ntraditional machine learning methods, which create artificial rules or extract\ncommon spatiotemporal features, which cannot be applied to large-scale data\napplications, and the emergence of unknown risks causes the detection accuracy\nof the original model to decline. With this in mind, this paper uses deep\nlearning for advanced threat detection to improve protective measures in the\nfinancial industry. Many network researchers have shifted their focus to\nexception-based intrusion detection techniques. The detection technology mainly\nuses statistical machine learning methods - collecting normal program and\nnetwork behavior data, extracting multidimensional features, and training\ndecision machine learning models on this basis (commonly used include naive\nBayes, decision trees, support vector machines, random forests, etc.).\n"
    },
    {
        "paper_id": 2402.09985,
        "authors": "Rangika Peiris, Chao Wang, Richard Gerlach, and Minh-Ngoc Tran",
        "title": "Semi-parametric financial risk forecasting incorporating multiple\n  realized measures",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  A semi-parametric joint Value-at-Risk (VaR) and Expected Shortfall (ES)\nforecasting framework employing multiple realized measures is developed. The\nproposed framework extends the realized exponential GARCH model to be\nsemi-parametrically estimated, via a joint loss function, whilst extending\nexisting quantile time series models to incorporate multiple realized measures.\nA quasi-likelihood is built, employing the asymmetric Laplace distribution that\nis directly linked to a joint loss function, which enables Bayesian inference\nfor the proposed model. An adaptive Markov Chain Monte Carlo method is used for\nthe model estimation. The empirical section evaluates the performance of the\nproposed framework with six stock markets from January 2000 to June 2022,\ncovering the period of COVID-19. Three realized measures, including 5- minute\nrealized variance, bi-power variation, and realized kernel, are incorporated\nand evaluated in the proposed framework. One-step-ahead VaR and ES forecasting\nresults of the proposed model are compared to a range of parametric and\nsemi-parametric models, lending support to the effectiveness of the proposed\nframework.\n"
    },
    {
        "paper_id": 2402.10215,
        "authors": "Manuel Hasenbichler, Wolfgang M\\\"uller and Stefan Thonhauser",
        "title": "The Mean Field Market Model Revisited",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  In this paper, we present an alternative perspective on the mean-field LIBOR\nmarket model introduced by Desmettre et al. in arXiv:2109.10779. Our novel\napproach embeds the mean-field model in a classical setup, but retains the\ncrucial feature of controlling the term rate's variances over large time\nhorizons. This maintains the market model's practicability, since calibrations\nand simulations can be carried out efficiently without nested simulations. In\naddition, we show that our framework can be directly applied to model term\nrates derived from SOFR, ESTR or other nearly risk-free overnight short-term\nrates -- a crucial feature since many IBOR rates are gradually being replaced.\nThese results are complemented by a calibration study and some theoretical\narguments which allow to estimate the probability of unrealistically high rates\nin the presented market models.\n"
    },
    {
        "paper_id": 2402.10253,
        "authors": "Ignas Gasparavi\\v{c}ius, Andrius Grigutis",
        "title": "The Famous American Economist H. Markowitz and Mathematical Overview of\n  his Portfolio Selection Theory",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/publicdomain/zero/1.0/",
        "abstract": "  This survey article is dedicated to the life of the famous American economist\nH. Markowitz (1927--2023). We do revisit the main statements of the portfolio\nselection theory in terms of mathematical completeness including all the\nnecessary auxiliary details.\n"
    },
    {
        "paper_id": 2402.10481,
        "authors": "Xiaorui Zuo, Yao-Tsung Chen, and Wolfgang Karl H\\\"ardle",
        "title": "Emoji Driven Crypto Assets Market Reactions",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In the burgeoning realm of cryptocurrency, social media platforms like\nTwitter have become pivotal in influencing market trends and investor\nsentiments. In our study, we leverage GPT-4 and a fine-tuned transformer-based\nBERT model for a multimodal sentiment analysis, focusing on the impact of emoji\nsentiment on cryptocurrency markets. By translating emojis into quantifiable\nsentiment data, we correlate these insights with key market indicators like BTC\nPrice and the VCRIX index. Our architecture's analysis of emoji sentiment\ndemonstrated a distinct advantage over FinBERT's pure text sentiment analysis\nin such predicting power. This approach may be fed into the development of\ntrading strategies aimed at utilizing social media elements to identify and\nforecast market trends. Crucially, our findings suggest that strategies based\non emoji sentiment can facilitate the avoidance of significant market downturns\nand contribute to the stabilization of returns. This research underscores the\npractical benefits of integrating advanced AI-driven analyses into financial\nstrategies, offering a nuanced perspective on the interplay between digital\ncommunication and market dynamics in an academic context.\n"
    },
    {
        "paper_id": 2402.1076,
        "authors": "Jingyi Gu, Wenlu Du, Guiling Wang",
        "title": "RAGIC: Risk-Aware Generative Adversarial Model for Stock Interval\n  Construction",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Efforts to predict stock market outcomes have yielded limited success due to\nthe inherently stochastic nature of the market, influenced by numerous\nunpredictable factors. Many existing prediction approaches focus on\nsingle-point predictions, lacking the depth needed for effective\ndecision-making and often overlooking market risk. To bridge this gap, we\npropose a novel model, RAGIC, which introduces sequence generation for stock\ninterval prediction to quantify uncertainty more effectively. Our approach\nleverages a Generative Adversarial Network (GAN) to produce future price\nsequences infused with randomness inherent in financial markets. RAGIC's\ngenerator includes a risk module, capturing the risk perception of informed\ninvestors, and a temporal module, accounting for historical price trends and\nseasonality. This multi-faceted generator informs the creation of\nrisk-sensitive intervals through statistical inference, incorporating\nhorizon-wise insights. The interval's width is carefully adjusted to reflect\nmarket volatility. Importantly, our approach relies solely on publicly\navailable data and incurs only low computational overhead. RAGIC's evaluation\nacross globally recognized broad-based indices demonstrates its balanced\nperformance, offering both accuracy and informativeness. Achieving a consistent\n95% coverage, RAGIC maintains a narrow interval width. This promising outcome\nsuggests that our approach effectively addresses the challenges of stock market\nprediction while incorporating vital risk considerations.\n"
    },
    {
        "paper_id": 2402.10803,
        "authors": "Johann Lussange, Stefano Vrizzi, Stefano Palminteri, Boris Gutkin",
        "title": "Modelling crypto markets by multi-agent reinforcement learning",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Building on a previous foundation work (Lussange et al. 2020), this study\nintroduces a multi-agent reinforcement learning (MARL) model simulating crypto\nmarkets, which is calibrated to the Binance's daily closing prices of $153$\ncryptocurrencies that were continuously traded between 2018 and 2022. Unlike\nprevious agent-based models (ABM) or multi-agent systems (MAS) which relied on\nzero-intelligence agents or single autonomous agent methodologies, our approach\nrelies on endowing agents with reinforcement learning (RL) techniques in order\nto model crypto markets. This integration is designed to emulate, with a\nbottom-up approach to complexity inference, both individual and collective\nagents, ensuring robustness in the recent volatile conditions of such markets\nand during the COVID-19 era. A key feature of our model also lies in the fact\nthat its autonomous agents perform asset price valuation based on two sources\nof information: the market prices themselves, and the approximation of the\ncrypto assets fundamental values beyond what those market prices are. Our MAS\ncalibration against real market data allows for an accurate emulation of crypto\nmarkets microstructure and probing key market behaviors, in both the bearish\nand bullish regimes of that particular time period.\n"
    },
    {
        "paper_id": 2402.11066,
        "authors": "Tristan Bester, Benjamin Rosman",
        "title": "Towards Financially Inclusive Credit Products Through Financial Time\n  Series Clustering",
        "comments": "9 Pages, 9 Figures, Published in AAAI W5: AI in Finance for Social\n  Impact",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Financial inclusion ensures that individuals have access to financial\nproducts and services that meet their needs. As a key contributing factor to\neconomic growth and investment opportunity, financial inclusion increases\nconsumer spending and consequently business development. It has been shown that\ninstitutions are more profitable when they provide marginalised social groups\naccess to financial services. Customer segmentation based on consumer\ntransaction data is a well-known strategy used to promote financial inclusion.\nWhile the required data is available to modern institutions, the challenge\nremains that segment annotations are usually difficult and/or expensive to\nobtain. This prevents the usage of time series classification models for\ncustomer segmentation based on domain expert knowledge. As a result, clustering\nis an attractive alternative to partition customers into homogeneous groups\nbased on the spending behaviour encoded within their transaction data. In this\npaper, we present a solution to one of the key challenges preventing modern\nfinancial institutions from providing financially inclusive credit, savings and\ninsurance products: the inability to understand consumer financial behaviour,\nand hence risk, without the introduction of restrictive conventional credit\nscoring techniques. We present a novel time series clustering algorithm that\nallows institutions to understand the financial behaviour of their customers.\nThis enables unique product offerings to be provided based on the needs of the\ncustomer, without reliance on restrictive credit practices.\n"
    },
    {
        "paper_id": 2402.11072,
        "authors": "Mohammad Mehdi Mousavi, Mahdi Kohan Sefidi, Shirin Allahyarkhani",
        "title": "Awareness of self-control",
        "comments": "23 pages, 8 Tables, 2 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  Economists modeled self-control problems in decisions of people with the\ntime-inconsistence preferences model. They argued that the source of\nself-control problems could be uncertainty and temptation. This paper uses an\nexperimental test offered to individuals instantaneous reward and future\nrewards to measure awareness of self-control problems in a tempting condition\nand also measure the effect of commitment and flexibility cost on their\nwelfare. The quasi-hyperbolic discounting model with time discount factor and\npresent bias at the same time was used for making a model for measuring\nawareness and choice reversal conditions. The test showed 66% awareness of\nself-control (partially naive behaviors) in individuals. The welfare\nimplication for individuals increased with commitment and flexibility costs.\nThe result can be useful in marketing and policy-making fields design precisely\noffers for customers and society.\n"
    },
    {
        "paper_id": 2402.11136,
        "authors": "Valentina Macchiati, Piero Mazzarisi and Diego Garlaschelli",
        "title": "Interbank network reconstruction enforcing density and reciprocity",
        "comments": "Accepted to Chaos, Solitons & Fractals Journal",
        "journal-ref": "Chaos Solitons & Fractals 186 (2024)",
        "doi": "10.1016/j.chaos.2024.115279",
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  Networks of financial exposures are the key propagators of risk and distress\namong banks, but their empirical structure is not publicly available because of\nconfidentiality. This limitation has triggered the development of methods of\nnetwork reconstruction from partial, aggregate information. Unfortunately, even\nthe best methods available fail in replicating the number of directed cycles,\nwhich on the other hand play a crucial role in determining graph spectra and\nhence the degree of network stability and systemic risk. Here we address this\nchallenge by exploiting the hypothesis that the statistics of higher-order\ncycles is strongly constrained by that of the shortest ones, i.e. by the amount\nof dyads with reciprocated links. First, we provide a detailed analysis of link\nreciprocity on the e-MID dataset of Italian banks, finding that correlations\nbetween reciprocal links systematically increase with the temporal resolution,\ntypically changing from negative to positive around a timescale of up to 50\ndays. Then, we propose a new network reconstruction method capable of\nenforcing, only from the knowledge of aggregate interbank assets and\nliabilities, both a desired sparsity and a desired link reciprocity. We confirm\nthat the addition of reciprocity dramatically improves the prediction of\nseveral structural and spectral network properties, including the largest real\neigenvalue and the eccentricity of the elliptical distribution of the other\neigenvalues in the complex plane. These results illustrate the importance of\ncorrectly addressing the temporal resolution and the resulting level of\nreciprocity in the reconstruction of financial networks.\n"
    },
    {
        "paper_id": 2402.1117,
        "authors": "Tao Yan, Shengnan Li, Benjamin Kraner, Luyao Zhang, and Claudio J.\n  Tessone",
        "title": "Analyzing Reward Dynamics and Decentralization in Ethereum 2.0: An\n  Advanced Data Engineering Workflow and Comprehensive Datasets for\n  Proof-of-Stake Incentives",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Ethereum 2.0, as the preeminent smart contract blockchain platform,\nguarantees the precise execution of applications without third-party\nintervention. At its core, this system leverages the Proof-of-Stake (PoS)\nconsensus mechanism, which utilizes a stochastic process to select validators\nfor block proposal and validation, consequently rewarding them for their\ncontributions. However, the implementation of blockchain technology often\ndiverges from its central tenet of decentralized consensus, presenting\nsignificant analytical challenges. Our study collects consensus reward data\nfrom the Ethereum Beacon chain and conducts a comprehensive analysis of reward\ndistribution and evolution, categorizing them into attestation, proposer and\nsync committee rewards. To evaluate the degree of decentralization in PoS\nEthereum, we apply several inequality indices, including the Shannon entropy,\nthe Gini Index, the Nakamoto Coefficient, and the Herfindahl-Hirschman Index\n(HHI). Our comprehensive dataset is publicly available on Harvard Dataverse,\nand our analytical methodologies are accessible via GitHub, promoting\nopen-access research. Additionally, we provide insights on utilizing our data\nfor future investigations focused on assessing, augmenting, and refining the\ndecentralization, security, and efficiency of blockchain systems.\n"
    },
    {
        "paper_id": 2402.11231,
        "authors": "Joerg Osterrieder, Stephen Chan, Jeffrey Chu, Yuanyuan Zhang, Branka\n  Hadji Misheva, Codruta Mare",
        "title": "Enhancing Security in Blockchain Networks: Anomalies, Frauds, and\n  Advanced Detection Techniques",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Blockchain technology, a foundational distributed ledger system, enables\nsecure and transparent multi-party transactions. Despite its advantages,\nblockchain networks are susceptible to anomalies and frauds, posing significant\nrisks to their integrity and security. This paper offers a detailed examination\nof blockchain's key definitions and properties, alongside a thorough analysis\nof the various anomalies and frauds that undermine these networks. It describes\nan array of detection and prevention strategies, encompassing statistical and\nmachine learning methods, game-theoretic solutions, digital forensics,\nreputation-based systems, and comprehensive risk assessment techniques. Through\ncase studies, we explore practical applications of anomaly and fraud detection\nin blockchain networks, extracting valuable insights and implications for both\ncurrent practice and future research. Moreover, we spotlight emerging trends\nand challenges within the field, proposing directions for future investigation\nand technological development. Aimed at both practitioners and researchers,\nthis paper seeks to provide a technical, in-depth overview of anomaly and fraud\ndetection within blockchain networks, marking a significant step forward in the\nsearch for enhanced network security and reliability.\n"
    },
    {
        "paper_id": 2402.11371,
        "authors": "Renan P. de Oliveira, Alessandro V. M. Oliveira",
        "title": "Companhias a\\'ereas s\\~ao todas iguais? A converg\\^encia dos modelos de\n  neg\\'ocios no transporte a\\'ereo",
        "comments": "This article is written in Portuguese",
        "journal-ref": "Communications in Airline Economics Research, 1, 10668851, 2024",
        "doi": "10.5281/zenodo.10668851",
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  This study discusses the literature on the convergence of business models of\nairlines in Brazilian air transport, focusing on the formation of flight\nnetworks. Initially, it analyzes the determinants of the network formation\npatterns of the \"fundamental\" business models (archetypes) of airlines in the\nfirst years after the sector's deregulation. Then, it discusses how the\nbusiness models of Brazilian companies resemble these patterns. The literature\nhighlights convergences between the network formation strategies of\nfull-service companies in relation to older low-cost companies, in addition to\nbusiness model redirections after mergers and acquisitions.\n"
    },
    {
        "paper_id": 2402.11372,
        "authors": "Bruno F. Oliveira, Alessandro V. M. Oliveira",
        "title": "Low costs na aviacao: importancia e desdobramentos",
        "comments": "This article is written in Portuguese",
        "journal-ref": "Communications in Airline Economics Research, 1, 10671650, 2024,\n  2024",
        "doi": "10.5281/zenodo.10671650",
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  This study aims to discuss the impacts of a low-cost airline on the air\ntransport market and, especially, to present the most recent findings from\nspecialized literature in the field. To this end, various works on this topic,\npublished since 2015, were selected and analyzed. From this analysis, it was\npossible to categorize the main topics discussed in the papers into five\ngroups: (i) the impacts of a low-cost airline on competing airlines; (ii)\nimpacts on airports; (iii) general impacts on the demand for air transport;\n(iv) effects on passengers' choice process; and (v) general effects on a\ngeographical region.\n"
    },
    {
        "paper_id": 2402.11373,
        "authors": "William E. Bendinelli, Alessandro V. M. Oliveira",
        "title": "Determinantes concorrenciais dos atrasos dos voos no aeroporto e na rota",
        "comments": "This article is written in Portuguese",
        "journal-ref": "Communications in Airline Economics Research, 1, 10671654, 2024",
        "doi": "10.5281/zenodo.10671654",
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  Flight delays are a reality in the modern air industry worldwide. However,\nstudies in the literature have investigated the competitive determinants of\ndelays arising from factors originating at the airport and along the route\nseparately. This work aims to present a national study that used a unifying\napproach from the literature, considering the local and global effects of\ncompetition on delays. The analysis took into account a phenomenon known as the\n\"internalization of externalities\" of airport congestion. Furthermore, it\ndiscusses the relationship between quality and competition on the route and the\nimpacts of the entry of a low-cost carrier (LCC) on the delays of incumbent\nairlines in the Brazilian air market. The literature suggests that there is\nevidence of congestion internalization at Brazilian airports, in parallel with\ncompetition for quality at the route level. Additionally, the potential\ncompetition caused by the presence of the LCC leads to a global effect that\nsuggests the existence of impacts other than prices on routes where it has not\nentered.\n"
    },
    {
        "paper_id": 2402.11374,
        "authors": "Jo\\~ao B. T. Szenczuk, Alessandro V. M. Oliveira",
        "title": "Impactos da Navega\\c{c}\\~ao Baseada em Performance nos Tempos de Voo da\n  Avia\\c{c}\\~ao Comercial",
        "comments": "This article is written in Portuguese",
        "journal-ref": "Communications in Airline Economics Research, 1, 10671656, 2024",
        "doi": "10.5281/zenodo.10671656",
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  This work presents an analysis of recent literature examining factors that\ninfluence flight times in Brazil, with special attention to the impact of new\ntechnology implementations, specifically Performance-Based Navigation (PBN).\nPBN procedures began to be implemented in Brazilian airspace in 2009 and\nrepresent a new concept of air navigation, using satellites to create 3D flight\nroutes that are shorter and more precise, potentially reducing the distances\nflown and consequently flight times and delays. Flight times depend on various\nfactors, such as the size and complexity of the origin and destination\nairports, weather conditions, aircraft models, and how full the flights are.\nTherefore, to assess the impact of the new procedures, the variation in this\nset of factors must be taken into account. The results found in the literature\nsuggest that Performance-Based Navigation had a positive impact, reducing\nflight times by about 1-2%, representing a saving of tens of thousands of\nflight hours from the beginning of implementations in 2009 until the end of\n2018.\n"
    },
    {
        "paper_id": 2402.11379,
        "authors": "Man Chon Iao and Yatheesan J. Selvakumar",
        "title": "Estimating HANK with Micro Data",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We propose an indirect inference strategy for estimating heterogeneous-agent\nbusiness cycle models with micro data. At its heart is a first-order vector\nautoregression that is grounded in linear filtering theory as the cross-section\ngrows large. The result is a fast, simple and robust algorithm for computing an\napproximate likelihood that can be easily paired with standard classical or\nBayesian methods. Importantly, our method is compatible with the popular\nsequence-space solution method, unlike existing state-of-the-art approaches. We\ntest-drive our method by estimating a canonical HANK model with shocks in both\nthe aggregate and cross-section. Not only do simulation results demonstrate the\nappeal of our method, they also emphasize the important information contained\nin the entire micro-level distribution over and above simple moments.\n"
    },
    {
        "paper_id": 2402.11579,
        "authors": "Xiaopeng Si (1), Zi Tang (1) ((1) School of Tourism and Cuisine,\n  Harbin University of Commerce, Harbin)",
        "title": "Assessment of low-carbon tourism development from multi-aspect analysis:\n  A case study of the Yellow River Basin, China",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Climate change has become an unavoidable problem in achieving sustainable\ndevelopment. As one of the major industries worldwide, tourism can make a\nsignificant contribution to mitigating climate change. The main objective of\nthe paper is to assess the development level of low-carbon tourism from\nmulti-aspect, using the Yellow River Basin as an example. Firstly, this study\nquantified tourism carbon dioxide emissions and tourism economy, and analyzed\ntheir evolution characteristics. The interaction and coordination degree\nbetween tourism carbon dioxide emissions and tourism economy were then analyzed\nusing the improved coupling coordination degree model. Finally, this study\nanalyzed the change in total factor productivity of low-carbon tourism by\ncalculating the Malmquist-Luenberger productivity index. The results showed\nthat: (1) The tourism industry in the Yellow River Basin has the\ncharacteristics of the initial environmental Kuznets curve. (2) There was a\nstrong interaction between tourism carbon dioxide emissions and tourism\neconomy, which was manifested as mutual promotion. (3) The total factor\nproductivity of low-carbon tourism was increasing. Based on the above results,\nit could be concluded that the development level of low-carbon tourism in the\nYellow River Basin has been continuously improved from 2000 to 2019, but it is\nstill in the early development stage with the continuous growth of carbon\ndioxide emissions.\n"
    },
    {
        "paper_id": 2402.1158,
        "authors": "Zongxia Liang, Xiaodong Luo",
        "title": "Stackelberg reinsurance and premium decisions with MV criterion and\n  irreversibility",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We study a reinsurance Stackelberg game in which both the insurer and the\nreinsurer adopt the mean-variance (abbr. MV) criterion in their decision-making\nand the reinsurance is irreversible. We apply a unified singular control\nframework where irreversible reinsurance contracts can be signed in both\ndiscrete and continuous times. The results theoretically illustrate that,\nrather than continuous-time contracts or a bunch of discrete-time contracts, a\nsingle once-for-all reinsurance contract is preferred. Moreover, the\nStackelberg game turns out to be centering on the signing time of the single\ncontract. The insurer signs the contract if the premium rate is lower than a\ntime-dependent threshold and the reinsurer designs a premium that triggers the\nsigning of the contract at his preferred time. Further, we find that\nreinsurance preference, discount and reversion have a decreasing dominance in\nthe reinsurer's decision-making, which is not seen for the insurer.\n"
    },
    {
        "paper_id": 2402.11715,
        "authors": "Jos\\'e Miguel Flores-Contr\\'o",
        "title": "The Gerber-Shiu Expected Discounted Penalty Function: An Application to\n  Poverty Trapping",
        "comments": "45 pages, 5 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this article, we consider a risk process with deterministic growth and\nprorated losses to model the capital of a household. Our work focuses on the\nanalysis of the trapping time of such a process, where trapping occurs when a\nhousehold's capital level falls into the poverty area, a region from which it\nis difficult to escape without external help. A function analogous to the\nclassical Gerber-Shiu expected discounted penalty function is introduced, which\nincorporates information on the trapping time, the capital surplus immediately\nbefore trapping and the capital deficit at trapping. Given that the remaining\nproportion of capital upon experiencing a capital loss is\n$Beta(\\alpha,1)-$distributed, closed-form expressions are obtained for\nquantities typically studied in classical risk theory, including the Laplace\ntransform of the trapping time and the distribution of the capital deficit at\ntrapping. In particular, we derive a model belonging to the generalised beta\n(GB) distribution family that describes the distribution of the capital deficit\nat trapping given that trapping occurs. Affinities between the capital deficit\nat trapping and a class of poverty measures, known as the\nFoster-Greer-Thorbecke (FGT) index, are presented. The versatility of this\nmodel to estimate FGT indices is assessed using household microdata from\nBurkina Faso's Enqu\\^ete Multisectorielle Continue (EMC) 2014.\n"
    },
    {
        "paper_id": 2402.11728,
        "authors": "Agam Shah, Arnav Hiray, Pratvi Shah, Arkaprabha Banerjee, Anushka\n  Singh, Dheeraj Eidnani, Bhaskar Chaudhury, Sudheer Chava",
        "title": "Numerical Claim Detection in Finance: A New Financial Dataset,\n  Weak-Supervision Model, and Market Analysis",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  In this paper, we investigate the influence of claims in analyst reports and\nearnings calls on financial market returns, considering them as significant\nquarterly events for publicly traded companies. To facilitate a comprehensive\nanalysis, we construct a new financial dataset for the claim detection task in\nthe financial domain. We benchmark various language models on this dataset and\npropose a novel weak-supervision model that incorporates the knowledge of\nsubject matter experts (SMEs) in the aggregation function, outperforming\nexisting approaches. Furthermore, we demonstrate the practical utility of our\nproposed model by constructing a novel measure ``optimism\". Furthermore, we\nobserved the dependence of earnings surprise and return on our optimism\nmeasure. Our dataset, models, and code will be made publicly (under CC BY 4.0\nlicense) available on GitHub and Hugging Face.\n"
    },
    {
        "paper_id": 2402.11864,
        "authors": "Sebastian Jaimungal, Xiaofei Shi",
        "title": "The Price of Information",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  When an investor is faced with the option to purchase additional information\nregarding an asset price, how much should she pay? To address this question, we\nsolve for the indifference price of information in a setting where a trader\nmaximizes her expected utility of terminal wealth over a finite time horizon.\nIf she does not purchase the information, then she solves a partial information\nstochastic control problem, while, if she does purchase the information, then\nshe pays a cost and receives partial information about the asset's trajectory.\nWe further demonstrate that when the investor can purchase the information at\nany stopping time prior to the end of the trading horizon, she chooses to do so\nat a deterministic time.\n"
    },
    {
        "paper_id": 2402.1193,
        "authors": "Yaoyue Tang, Karina Arias-Calluari, Michael S. Harr\\'e, Fernando\n  Alonso-Marroquin",
        "title": "Stylized Facts of High-Frequency Bitcoin Time Series",
        "comments": "16 pages, 11 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  This paper analyses the high-frequency intraday Bitcoin dataset from 2019 to\n2022. During this time frame, the Bitcoin market index exhibited two distinct\nperiods characterized by abrupt changes in volatility. The Bitcoin price\nreturns for both periods can be described by an anomalous diffusion process,\ntransitioning from subdiffusion for short intervals to weak superdiffusion over\nlonger time intervals. The characteristic features related to this anomalous\nbehavior studied in the present paper include heavy tails, which can be\ndescribed using a $q$-Gaussian distribution and correlations. When we sample\nthe autocorrelation of absolute returns, we observe a power-law relationship,\nindicating time dependency in both periods initially. The ensemble\nautocorrelation of returns decays rapidly and exhibits periodicity. We fitted\nthe autocorrelation with a power law and a cosine function to capture both the\ndecay and the fluctuation and found that the two periods have distinctive\nperiodicity. Further study involves the analysis of endogenous effects within\nthe Bitcoin time series, which are examined through detrending analysis. We\nfound that both periods are multifractal and present self-similarity in the\ndetrended probability density function (PDF). The Hurst exponent over short\ntime intervals shifts from less than 0.5 ($\\sim$ 0.42) in Period 1 to be closer\nto 0.5 in Period 2 ($\\sim$ 0.49), indicating the market is more efficient at\nshort time scales.\n"
    },
    {
        "paper_id": 2402.12049,
        "authors": "Andrea Macr\\`i and Fabrizio Lillo",
        "title": "Reinforcement Learning for Optimal Execution when Liquidity is\n  Time-Varying",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Optimal execution is an important problem faced by any trader. Most solutions\nare based on the assumption of constant market impact, while liquidity is known\nto be dynamic. Moreover, models with time-varying liquidity typically assume\nthat it is observable, despite the fact that, in reality, it is latent and hard\nto measure in real time. In this paper we show that the use of Double Deep\nQ-learning, a form of Reinforcement Learning based on neural networks, is able\nto learn optimal trading policies when liquidity is time-varying. Specifically,\nwe consider an Almgren-Chriss framework with temporary and permanent impact\nparameters following several deterministic and stochastic dynamics. Using\nextensive numerical experiments, we show that the trained algorithm learns the\noptimal policy when the analytical solution is available, and overcomes\nbenchmarks and approximated solutions when the solution is not available.\n"
    },
    {
        "paper_id": 2402.12327,
        "authors": "Zengqing Wu, Run Peng, Shuyuan Zheng, Qianying Liu, Xu Han, Brian\n  Inhyuk Kwon, Makoto Onizuka, Shaojie Tang, Chuan Xiao",
        "title": "Shall We Team Up: Exploring Spontaneous Cooperation of Competing LLM\n  Agents",
        "comments": "Source codes available at\n  https://github.com/wuzengqing001225/SABM_ShallWeTeamUp",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Large Language Models (LLMs) have increasingly been utilized in social\nsimulations, where they are often guided by carefully crafted instructions to\nstably exhibit human-like behaviors during simulations. Nevertheless, we doubt\nthe necessity of shaping agents' behaviors for accurate social simulations.\nInstead, this paper emphasizes the importance of spontaneous phenomena, wherein\nagents deeply engage in contexts and make adaptive decisions without explicit\ndirections. We explored spontaneous cooperation across three competitive\nscenarios and successfully simulated the gradual emergence of cooperation,\nfindings that align closely with human behavioral data. This approach not only\naids the computational social science community in bridging the gap between\nsimulations and real-world dynamics but also offers the AI community a novel\nmethod to assess LLMs' capability of deliberate reasoning.\n"
    },
    {
        "paper_id": 2402.12401,
        "authors": "Ana B. R. Eufr\\'asio, Alessandro V. M. Oliveira",
        "title": "An\\'alise das estrat\\'egias de planejamento de tempos de voo pelas\n  companhias a\\'ereas",
        "comments": "This article is written in Portuguese",
        "journal-ref": "Communications in Airline Economics Research, 1, 10671658, 2024",
        "doi": "10.5281/zenodo.10671658",
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  This study explores the approaches used by airlines in setting flight times.\nIt highlights the need to balance operational and strategic factors, such as\noptimizing the use of resources - including aircraft, crew, and fuel - and\nmanaging the risks related to delays and congestion. The work details a\nnational analysis focused on domestic flights, investigating the factors that\ninfluence companies to adjust scheduled flight times and the impact of this\npractice on punctuality. The results indicate that decisions about flight time\nare influenced by both operational and strategic aspects, being affected by\ncompetition and the sector's policies on punctuality and slot allocation.\nFurthermore, it was found that adding extra time is an effective strategy for\nreducing delays, although it may conceal system deficiencies.\n"
    },
    {
        "paper_id": 2402.12402,
        "authors": "Rodolfo R. Narcizo, Alessandro V. M. Oliveira",
        "title": "A padroniza\\c{c}\\~ao de frota e suas consequ\\^encias para as companhias\n  a\\'ereas",
        "comments": "This article is written in Portuguese",
        "journal-ref": "Communications in Airline Economics Research, 1, 10671660, 2024",
        "doi": "10.5281/zenodo.10671660",
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  This study addresses the growing standardization of airline fleets,\nhighlighting that frequent passengers are more likely to fly on the same\naircraft model more often. The objective is to analyze the fleet management of\nairlines and the impact of a reduced variety of models on company operations.\nThe benefits of standardization, such as operational efficiency, and the risks,\nsuch as vulnerability to specific model failures, are discussed. The work\nreviews international scientific literature on the subject, identifying\nconsensus and disagreements that suggest areas for future research. It also\nincludes a study on the Brazilian market, examining how standardization affects\noperational costs and profitability in terms of model, family, and aircraft\nmanufacturer. Furthermore, the relationship between fleet standardization and\nthe business model of the companies is investigated, concluding that the\nadvantages of standardization are not exclusive to low-cost companies but can\nalso be leveraged by other airlines.\n"
    },
    {
        "paper_id": 2402.12403,
        "authors": "Murillo Massaretto, Alessandro V. M. Oliveira",
        "title": "A literatura das receitas comerciais em aeroportos: discuss\\~oes e\n  principais descobertas",
        "comments": "This article is written in Portuguese",
        "journal-ref": "Communications in Airline Economics Research, 1, 10671662, 2024",
        "doi": "10.5281/zenodo.10671662",
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  The exploration of existing commercial opportunities has been increasing the\nshare of commercial (non-aeronautical) revenues in the total revenues of\nairports worldwide. These revenues, also called commercial, non-aeronautical,\nor non-tariff revenues, come from rentals, duty-free shops, food and beverage\nsales, parking, advertising, etc. In other words, everything that is not the\nmain business of the airport. Consequently, the dependence on commercial\nrevenues has also become increasingly important, and airport managers are\ninterested in understanding how to improve their financial results with this\nnew revenue source, as it not only improves financial outcomes but also\noptimizes passengers' time and money consumption options at airports.\nTherefore, this chapter seeks to discuss the main determinants of commercial\nrevenues at national airports, analyzing the possible impacts of the behavior\nof passengers from low-cost carriers (LCC) on these revenues, combined with\nother determining factors.\n"
    },
    {
        "paper_id": 2402.12404,
        "authors": "Carolina B. Resende, Alessandro V. M. Oliveira",
        "title": "Estudos de cen\\'arios de implanta\\c{c}\\~ao de um imposto ambiental no\n  transporte a\\'ereo no Brasil",
        "comments": "This article is written in Portuguese",
        "journal-ref": "Communications in Airline Economics Research, 1, 10671664, 2024",
        "doi": "10.5281/zenodo.10671664",
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  In recent years, the topic of global warming and greenhouse gas emissions has\nbeen increasingly in the media. This theme has raised various flags, showing\nconcern for the future and seeking alternatives for a more sustainable life\nover the years. When studying greenhouse gas emissions, one of the main\nemitters of such gases is the burning of fuels, in which transport vehicles\nthat depend on gas and oil are included. In this respect, air transport through\naircraft is one of the sources emitting such gases. Aiming to reduce gas\nemissions, one of the ways to do this is by reducing fuel consumption; for\nthis, aircraft would have to be more efficient, or the offer and demand for\nflights would be reduced. However, what if aircraft fuel consumption were\ntaxed? What would be the effects of this on air transport? Could this be one of\nthe alternatives to reduce emissions? To understand this relationship between\ntaxation and a possible reduction in fuel consumption, a study was developed by\nthe Aeronautics Institute of Technology (ITA), using an econometric model to\nunderstand how demand would be affected by changes in airfares. In addition, a\nsimulation of possible environmental taxes on the values of air tickets was\ncarried out to analyze the demand response and to get an idea if this taxation\nwould really solve the emission problems.\n"
    },
    {
        "paper_id": 2402.12487,
        "authors": "Patrick Meyfroidt, Dilini Abeygunawardane, Matthias Baumann, Adia Bey,\n  Ana Buchadas, Cristina Chiarella, Victoria Junquera, Angela Kronenburg\n  Garc\\'ia, Tobias Kuemmerle, Yann le Polain de Waroux, Eduardo Oliveira,\n  Michelle Picoli, Siyu Qin, Virginia Rodriguez Garc\\'ia, Philippe Rufin",
        "title": "Explaining the emergence of land-use frontiers",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Land use expansion is linked to major sustainability concerns including\nclimate change, food security and biodiversity loss. This expansion is largely\nconcentrated in so-called frontiers, defined here as places experiencing marked\ntransformations due to rapid resource exploitation. Understanding the\nmechanisms shaping these frontiers is crucial for sustainability. Previous work\nfocused mainly on explaining how active frontiers advance, in particular into\ntropical forests. Comparatively, our understanding of how frontiers emerge in\nterritories considered marginal in terms of agricultural productivity and\nglobal market integration remains weak. We synthesize conceptual tools\nexplaining resource and land-use frontiers, including theories of land rent and\nagglomeration economies, of frontiers as successive waves, spaces of\nterritorialization, friction, and opportunities, anticipation and expectation.\nWe then propose a new theory of frontier emergence, which identifies exogenous\npushes, legacies of past waves, and actors anticipations as key mechanisms by\nwhich frontiers emerge. Processes of abnormal rent creation and capture and the\nbuilt-up of agglomeration economies then constitute key mechanisms sustaining\nactive frontiers. Finally, we discuss five implications for the governance of\nfrontiers for sustainability. Our theory focuses on agriculture and\ndeforestation frontiers in the tropics, but can be inspirational for other\nfrontier processes including for extractive resources, such as minerals.\n"
    },
    {
        "paper_id": 2402.12528,
        "authors": "Andrzej Daniluk, Evgeny Lakshtanov and Rafal Muchorski",
        "title": "Denoised Monte Carlo for option pricing and Greeks estimation",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We present a novel technique of Monte Carlo error reduction that finds direct\napplication in option pricing and Greeks estimation. The method is applicable\nto any LSV modelling framework and concerns a broad class of payoffs, including\npath-dependent and multi-asset cases. Most importantly, it allows to reduce the\nMonte Carlo error even by an order of magnitude, which is shown in several\nnumerical examples.\n"
    },
    {
        "paper_id": 2402.12561,
        "authors": "Carolin Bauerhenne, Rainer Kolisch, Andreas S. Schulz",
        "title": "Robust Appointment Scheduling with Waiting Time Guarantees",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Appointment scheduling problems under uncertainty encounter a fundamental\ntrade-off between cost minimization and customer waiting times. Most existing\nstudies address this trade-off using a weighted sum approach, which puts little\nemphasis on individual waiting times and, thus, customer satisfaction. In\ncontrast, we study how to minimize total cost while providing waiting time\nguarantees to all customers. Given box uncertainty sets for service times and\nno-shows, we introduce the Robust Appointment Scheduling Problem with Waiting\nTime Guarantees. We show that the problem is NP-hard in general and introduce a\nmixed-integer linear program that can be solved in reasonable computation time.\nFor special cases, we prove that polynomial-time variants of the well-known\nSmallest-Variance-First sequencing rule and the Bailey-Welch scheduling rule\nare optimal. Furthermore, a case study with data from the radiology department\nof a large university hospital demonstrates that the approach not only\nguarantees acceptable waiting times but, compared to existing robust\napproaches, may simultaneously reduce costs incurred by idle time and overtime.\nThis work suggests that limiting instead of minimizing customer waiting times\nis a win-win solution in the trade-off between customer satisfaction and cost\nminimization. Additionally, it provides an easy-to-implement and customizable\nappointment scheduling framework with waiting time guarantees.\n"
    },
    {
        "paper_id": 2402.12575,
        "authors": "Enrique Ide",
        "title": "Cross-Market Mergers with Common Customers: When (and Why) Do They\n  Increase Negotiated Prices?",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  I examine the implications of cross-market mergers of suppliers to\nintermediaries that bundle products for consumers. These mergers are\ncontroversial. Some argue that suppliers' products will be substitutes for\nintermediaries, despite not being substitutes for consumers. Others contend\nthat because bundling makes products complements for consumers, products must\nbe complements for intermediaries. I contribute to this debate by showing that\ntwo products can be complements for consumers but substitutes for\nintermediaries when the products serve a similar role in attracting consumers\nto purchase the bundle. This result leads to new recommendations and helps\nexplain why cross-market hospital mergers raise prices.\n"
    },
    {
        "paper_id": 2402.12848,
        "authors": "Emily Little, Florent Cogen, Quentin Bustarret, Virginie Dussartre,\n  Maxime L\\^aasri, Gabriel Kasmi, Marie Girod, Frederic Bienvenu, Maxime\n  Fortin, Jean-Yves Bourmaud",
        "title": "ATLAS: A Model of Short-term European Electricity Market Processes under\n  Uncertainty",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The ATLAS model simulates the various stages of the electricity market chain\nin Europe, including the formulation of offers by different market actors, the\ncoupling of European markets, strategic optimization of production portfolios\nand, finally, real-time system balancing processes. ATLAS was designed to\nsimulate the various electricity markets and processes that occur from the day\nahead timeframe to real-time with a high level of detail. Its main aim is to\ncapture impacts from imperfect actor coordination, evolving forecast errors and\na high-level of technical constraints--both regarding different production\nunits and the different market constraints.\n"
    },
    {
        "paper_id": 2402.12859,
        "authors": "Florent Cogen, Emily Little, Virginie Dussartre, Quentin Bustarret",
        "title": "ATLAS: A Model of Short-term European Electricity Market Processes under\n  Uncertainty -- Balancing Modules",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The ATLAS model simulates the various stages of the electricity market chain\nin Europe, including the formulation of offers by different market actors, the\ncoupling of European markets, strategic optimization of production portfolios\nand, finally, real-time system balancing processes. ATLAS was designed to\nsimulate the various electricity markets and processes that occur from the day\nahead timeframe to real-time with a high level of detail. Its main aim is to\ncapture impacts from imperfect actor coordination, evolving forecast errors and\na high-level of technical constraints -- both regarding different production\nunits and the different market constraints. This working paper describes the\nsimulated balancing processes in detail and is the second part of the ATLAS\ndocumentation.\n"
    },
    {
        "paper_id": 2402.13177,
        "authors": "Azin Yazdi, Sunder Ramachandran, Hoda Mohsenifard, Khaled Nawaser,\n  Faraz Sasani, Behrooz Gharleghi",
        "title": "The Ebb and Flow of Brand Loyalty: A 28-Year Bibliometric and Content\n  Analysis",
        "comments": "29 pages, 7 tables, 9 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Business research is facing the challenge of scattered knowledge,\nparticularly in the realm of brand loyalty (BL). Although literature reviews on\nBL exist, they predominantly concentrate on the pre-sent state, neglecting\nfuture trends. Therefore, a comprehensive review is imperative to ascertain\nemerging trends in BL This study employs a bibliometric approach, analyzing\n1,468 papers from the Scopus database. Various tools including R software, VOS\nviewer software, and Publish or Perish are utilized. The aim is to portray the\nknowledge map, explore the publication years, identify the top authors and\ntheir co-occurrence, reliable documents, institutions, subjects, research\nhotspots, and pioneering countries and universities in the study of BL. The\nqualitative section of this research identifies gaps and emerging trends in BL\nthrough Word Cloud charts, word growth analysis, and a review of highly cited\narticles from the past four years. Results showed that highly cited articles\nmention topics such as brand love, consumer-brand identification, and social\nnetworks and the U.S. had the most productions in this field. Besides, most\ncitations were related to Keller with 1,173 citations. Furthermore, in the\nqualitative section, social networks and brand experiences were found to be of\ninterest to researchers in the field. Finally, by introducing the antecedents\nand consequences of BL, the gaps and emerging trends in BL were identified, so\nas to present the di-rection of future research in this area.\n"
    },
    {
        "paper_id": 2402.13278,
        "authors": "Bruno F. Oliveira, Alessandro V. M. Oliveira",
        "title": "Determinantes do planejamento estrat\\'egico da rede de uma companhia\n  a\\'erea",
        "comments": "This article is written in Portuguese",
        "journal-ref": "Communications in Airline Economics Research, 1, 10671668, 2024",
        "doi": "10.5281/zenodo.10671668",
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  This work focuses on trying to understand how the construction of an\nairline's network is made. For this purpose, the case of Azul was studied,\ninvestigating which and how factors affect the decision of this airline to\nenter domestic routes, in addition to analyzing how the merger of Azul with the\nregional airline Trip affected the company's network planning. For this, an\nacademic study was conducted using an econometric model to understand the\nairline's entry model. The results show that Azul's business model is based on\nconnecting new destinations, not yet served by its competitors, to one of its\nhubs, and consistently avoiding routes or airports dominated by other airlines.\nRegarding the effects of the merger, the results suggest that Azul moved away\nfrom its original entry model, based on JetBlue, to a model more oriented\ntowards regional aviation, entering shorter routes and regional airports.\n"
    },
    {
        "paper_id": 2402.13279,
        "authors": "Igor R. S. Brito, Alessandro V. M. Oliveira",
        "title": "Privatiza\\c{c}\\~ao de aeroportos: motiva\\c{c}\\~oes, regula\\c{c}\\~ao e\n  efici\\^encia operacional",
        "comments": "This article is written in Portuguese",
        "journal-ref": "Communications in Airline Economics Research, 1, 10671670, 2024",
        "doi": "10.5281/zenodo.10671670",
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  In this study, we will address some topics related to the privatization of\nairports in the scientific literature, in an attempt to provide an answer to\nthe following question: does the privatization of airports bring positive\nresults? Firstly, we turn our attention to the motivations leading to\nprivatization, considering the two main parties involved, the government and\nthe private sector. After all, the success of such a decision will be relative\nto the reasons that justify it. In a second moment, we will consider the\nregulatory issue, whose influence on the airport's financial performance is\nconsolidated in the literature. In the third part, we address the main\ndocumented results of privatization, with special attention to productive\nefficiency, whose improvement is the most popular motivation for privatization.\n"
    },
    {
        "paper_id": 2402.13326,
        "authors": "Andrei Neagu and Fr\\'ed\\'eric Godin and Clarence Simard and Leila\n  Kosseim",
        "title": "Deep Hedging with Market Impact",
        "comments": "13 pages, 5 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Dynamic hedging is the practice of periodically transacting financial\ninstruments to offset the risk caused by an investment or a liability. Dynamic\nhedging optimization can be framed as a sequential decision problem; thus,\nReinforcement Learning (RL) models were recently proposed to tackle this task.\nHowever, existing RL works for hedging do not consider market impact caused by\nthe finite liquidity of traded instruments. Integrating such feature can be\ncrucial to achieve optimal performance when hedging options on stocks with\nlimited liquidity. In this paper, we propose a novel general market impact\ndynamic hedging model based on Deep Reinforcement Learning (DRL) that considers\nseveral realistic features such as convex market impacts, and impact\npersistence through time. The optimal policy obtained from the DRL model is\nanalysed using several option hedging simulations and compared to commonly used\nprocedures such as delta hedging. Results show our DRL model behaves better in\ncontexts of low liquidity by, among others: 1) learning the extent to which\nportfolio rebalancing actions should be dampened or delayed to avoid high\ncosts, 2) factoring in the impact of features not considered by conventional\napproaches, such as previous hedging errors through the portfolio value, and\nthe underlying asset's drift (i.e. the magnitude of its expected return).\n"
    },
    {
        "paper_id": 2402.13355,
        "authors": "Yuanying Guan, Muqiao Huang and Ruodu Wang",
        "title": "A new characterization of second-order stochastic dominance",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We provide a new characterization of second-order stochastic dominance, also\nknown as increasing concave order. The result has an intuitive interpretation\nthat adding a risk with negative expected value in adverse scenarios makes the\nresulting position generally less desirable for risk-averse agents. A similar\ncharacterization is also found for convex order and increasing convex order.\nThe proofs techniques for the main result are based on properties of Expected\nShortfall, a family of risk measures that is popular in financial regulation.\n"
    },
    {
        "paper_id": 2402.13439,
        "authors": "Zakary Rodrigue Diakit\\'e",
        "title": "Estimating Demand for Lamb, Beef, Pork, and Poultry in Canada",
        "comments": null,
        "journal-ref": "Theoretical Economics Letters, 14(1), 67-93",
        "doi": "10.4236/tel.2024.141005",
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  This paper investigates the demand for lamb, beef, pork, and poultry in\nCanada, both at the national level and in disaggregated provinces, to identify\nmeat consumption patterns in different provinces. Meat consumption plays a\nsignificant role in Canada's economy and is an important source of calories for\nthe population. However, meat demand faces several consumption challenges due\nto logistic constraints, as a significant portion of the supply is imported\nfrom other countries. Therefore, there is a need for a better understanding of\nthe causal relationships underlying lamb, beef, pork, and poultry consumption\nin Canada. Until recently, there have been no attempts to estimate meat\nconsumption at the provincial level in Canada. Different Almost Ideal Demand\nSystem (AIDS) models have been applied for testing specifications to circumvent\nseveral econometric and theoretical problems. In particular, generalized AIDS\nand its Quadratic extension QUAIDS methods have been estimated across each\nprovince using the Iterative Linear Least Squares Estimator (ILLE) estimation\nMethod. Weekly retail meat consumption price and quantity data from 2019 to\n2022 have been used for Canada and for each province namely Quebec, Maritime\nprovinces (New Brunswick, Nova Scotia, and Prince Edward Island), Ontario,\ntotal West (Yukon, Northwest Territory and Nunavut), Alberta,\nManitoba-Saskatchewan and Manitoba as well as British Columbia. Consistent\ncoefficients and demand elasticities estimates reveal patterns of substitution\nand/or complementarity between the four categories of meat. Meat consumption\npatterns differ across each province. Results show that the demand for the four\ncategories of meat is responsive to price changes. Overall, lamb expenditure\nwas found to be elastic and thus considered a luxury good during the study\nperiod, while the other three categories are considered normal goods across\nCanada.\n"
    },
    {
        "paper_id": 2402.13627,
        "authors": "Martin Hoefer, Carmine Ventre, Lisa Wilhelmi",
        "title": "Algorithms for Claims Trading",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  The recent banking crisis has again emphasized the importance of\nunderstanding and mitigating systemic risk in financial networks. In this\npaper, we study a market-driven approach to rescue a bank in distress based on\nthe idea of claims trading, a notion defined in Chapter 11 of the U.S.\nBankruptcy Code. We formalize the idea in the context of financial networks by\nEisenberg and Noe. For two given banks v and w, we consider the operation that\nw takes over some claims of v and in return gives liquidity to v to ultimately\nrescue v. We study the structural properties and computational complexity of\ndecision and optimization problems for several variants of claims trading.\n  When trading incoming edges of v, we show that there is no trade in which\nboth banks v and w strictly improve their assets. We therefore consider\ncreditor-positive trades, in which v profits strictly and w remains\nindifferent. For a given set C of incoming edges of v, we provide an efficient\nalgorithm to compute payments by w that result in maximal assets of v. When the\nset C must also be chosen, the problem becomes weakly NP-hard. Our main result\nhere is a bicriteria FPTAS to compute an approximate trade. The approximate\ntrade results in nearly the optimal amount of assets of v in any exact trade.\nOur results extend to the case in which banks use general monotone payment\nfunctions and the emerging clearing state can be computed efficiently.\n  In contrast, for trading outgoing edges of v, the goal is to maximize the\nincrease in assets for the creditors of v. Notably, for these results the\ncharacteristics of the payment functions of the banks are essential. For\npayments ranking creditors one by one, we show NP-hardness of approximation\nwithin a factor polynomial in the network size, when the set of claims C is\npart of the input or not. Instead, for proportional payments, our results\nindicate more favorable conditions.\n"
    },
    {
        "paper_id": 2402.13789,
        "authors": "Alessandro V. M. Oliveira",
        "title": "The seasonality of air ticket prices before and after the pandemic",
        "comments": null,
        "journal-ref": "Communications in Airline Economics Research, 1, 10659441, 2024",
        "doi": "10.5281/zenodo.10659441",
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  This study investigates price seasonality in the Brazilian air transport\nindustry, emphasizing the impact of the COVID-19 pandemic on domestic airline\npricing strategies. Given potential shifts in demand patterns following the\nglobal health crisis, this study explores possible long-term structural changes\nin the seasonality of Brazilian airfare. We analyze an open dataset of domestic\ncity pairs from 2013 to 2023, employing an econometric model developed using\nStata software. Our findings indicate alterations in seasonal patterns and\nlong-term trends in the post-pandemic era. These changes underscore potential\nshifts in the composition of leisure and business travelers, along with the\ncost pressures faced by airlines.\n"
    },
    {
        "paper_id": 2402.13807,
        "authors": "S.J. Newman, K. Schulte, M.M. Morellini, C. Rahal, and D. Leasure",
        "title": "Offshoring Emissions through Used Vehicle Exports",
        "comments": null,
        "journal-ref": null,
        "doi": "10.1038/s41558-024-01943-1",
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Policies to reduce transport emissions often overlook the international flow\nof used vehicles. We quantify the rate at which used vehicles generated CO2 and\npollution for all used vehicles exported from Great Britain; a globally leading\nused vehicle exporter across 2005-2021. Destined for low-middle-income\ncountries, exported vehicles fail roadworthiness standards and, even under\nextremely optimistic functioning as new assumptions, generate at least 13-53\npercent more emissions than scrapped or on-road vehicles.\n"
    },
    {
        "paper_id": 2402.1409,
        "authors": "Edwin Zhang, Sadie Zhao, Tonghan Wang, Safwan Hossain, Henry\n  Gasztowtt, Stephan Zheng, David C. Parkes, Milind Tambe, Yiling Chen",
        "title": "Social Environment Design",
        "comments": "ICML 2024 Position Paper. Website at https://sed.eddie.win",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Artificial Intelligence (AI) holds promise as a technology that can be used\nto improve government and economic policy-making. This paper proposes a new\nresearch agenda towards this end by introducing Social Environment Design, a\ngeneral framework for the use of AI for automated policy-making that connects\nwith the Reinforcement Learning, EconCS, and Computational Social Choice\ncommunities. The framework seeks to capture general economic environments,\nincludes voting on policy objectives, and gives a direction for the systematic\nanalysis of government and economic policy through AI simulation. We highlight\nkey open problems for future research in AI-based policy-making. By solving\nthese challenges, we hope to achieve various social welfare objectives, thereby\npromoting more ethical and responsible decision making.\n"
    },
    {
        "paper_id": 2402.141,
        "authors": "Yan Dolinsky and Doron Greenstein",
        "title": "A Note on Optimal Liquidation with Linear Price Impact",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  In this note we consider the maximization of the expected terminal wealth for\nthe setup of quadratic transaction costs. First, we provide a very simple\nprobabilistic solution to the problem. Although the problem was largely\nstudied, as far as we know up to date this simple and probabilistic form of the\nsolution has not appeared in the literature.\n  Next, we apply the general result for the numerical study of the case where\nthe risky asset is given by a fractional Brownian Motion and the information\nflow of the investor can be diversified.\n"
    },
    {
        "paper_id": 2402.14161,
        "authors": "Dan Pirjol, Lingjiong Zhu",
        "title": "Short-maturity asymptotics for option prices with interest rates effects",
        "comments": "25 pages, 4 figures",
        "journal-ref": "International Journal of Theoretical and Applied Finance 2023,\n  Volume 26, 2350023",
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We derive the short-maturity asymptotics for option prices in the local\nvolatility model in a new short-maturity limit $T\\to 0$ at fixed $\\rho = (r-q)\nT$, where $r$ is the interest rate and $q$ is the dividend yield. In cases of\npractical relevance $\\rho$ is small, however our result holds for any fixed\n$\\rho$. The result is a generalization of the Berestycki-Busca-Florent formula\nfor the short-maturity asymptotics of the implied volatility which includes\ninterest rates and dividend yield effects of $O(((r-q) T)^n)$ to all orders in\n$n$. We obtain analytical results for the ATM volatility and skew in this\nasymptotic limit. Explicit results are derived for the CEV model. The\nasymptotic result is tested numerically against exact evaluation in the\nsquare-root model model $\\sigma(S)=\\sigma/\\sqrt{S}$, which demonstrates that\nthe new asymptotic result is in very good agreement with exact evaluation in a\nwide range of model parameters relevant for practical applications.\n"
    },
    {
        "paper_id": 2402.14189,
        "authors": "Rangrang Zheng, Greg Schivley, Patricia Hidalgo-Gonzalez, Matthias\n  Fripp, Michael J. Roberts",
        "title": "Optimal transmission expansion minimally reduces decarbonization costs\n  of U.S. electricity",
        "comments": "23 pages, 7 figures in main paper. Additional 11 pages including 7\n  additional figures and one table in the appendix",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-sa/4.0/",
        "abstract": "  Solar and wind power are cost-competitive with fossil fuels, yet their\nintermittent nature presents challenges. Significant temporal and geographic\ndifferences in land, wind, and solar resources suggest that long-distance\ntransmission could be particularly beneficial. Using a detailed, open-source\nmodel, we analyze optimal transmission expansion jointly with storage,\ngeneration, and hourly operations across the three primary interconnects in the\nUnited States. Transmission expansion offers far more benefits in a\nhigh-renewable system than in a system with mostly conventional generation. Yet\nwhile an optimal nationwide plan would have more than triple current\ninterregional transmission, transmission decreases the cost of a 100% clean\nsystem by only 4% compared to a plan that relies solely on current\ntransmission. Expanding capacity only within existing interconnects can achieve\nmost of these savings. Adjustments to energy storage and generation mix can\nleverage the current interregional transmission infrastructure to build a clean\npower system at a reasonable cost.\n"
    },
    {
        "paper_id": 2402.14206,
        "authors": "Vahidin Jeleskovic and Yinan Wan",
        "title": "The impact of Facebook-Cambridge Analytica data scandal on the USA tech\n  stock market: An event study based on clustering method",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  This study delves into the intra-industry effects following a firm-specific\nscandal, with a particular focus on the Facebook data leakage scandal and its\nassociated events within the U.S. tech industry and two additional relevant\ngroups. We employ various metrics including daily spread, volatility,\nvolume-weighted return, and CAPM-beta for the pre-analysis clustering, and\nsubsequently utilize CAR (Cumulative Abnormal Return) to evaluate the impact on\nfirms grouped within these clusters. From a broader industry viewpoint,\nsignificant positive CAARs are observed across U.S. sample firms over the three\ndays post-scandal announcement, indicating no adverse impact on the tech sector\noverall. Conversely, after Facebook's initial quarterly earnings report, it\nshowed a notable negative effect despite reported positive performance. The\nclustering principle should aid in identifying directly related companies and\nthus reducing the influence of randomness. This was indeed achieved for the\neffect of the key event, namely \"The Effect of Congressional Hearing on Certain\nClusters across U.S. Tech Stock Market,\" which was identified as delayed and\nsignificantly negative. Therefore, we recommend applying the clustering method\nwhen conducting such or similar event studies.\n"
    },
    {
        "paper_id": 2402.14269,
        "authors": "Jihyeok Jung, Chan-Oi Song, Deok-Joo Lee, Kiho Yoon",
        "title": "Optimal Mechanism in a Dynamic Stochastic Knapsack Environment",
        "comments": "8 pages, 1 figures, presented in AAAI 38th conference on Artificial\n  Intelligence",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  This study introduces an optimal mechanism in a dynamic stochastic knapsack\nenvironment. The model features a single seller who has a fixed quantity of a\nperfectly divisible item. Impatient buyers with a piece-wise linear utility\nfunction arrive randomly and they report the two-dimensional private\ninformation: marginal value and demanded quantity. We derive a\nrevenue-maximizing dynamic mechanism in a finite discrete time framework that\nsatisfies incentive compatibility, individual rationality, and feasibility\nconditions. It is achieved by characterizing buyers' utility and deriving the\nBellman equation. Moreover, we propose the essential penalty scheme for\nincentive compatibility, as well as the allocation and payment policies.\nLastly, we propose algorithms to approximate the optimal policy, based on the\nMonte Carlo simulation-based regression method and reinforcement learning.\n"
    },
    {
        "paper_id": 2402.14322,
        "authors": "Suparna Biswas, Rituparna Sen",
        "title": "Estimation of Spectral Risk Measure for Left Truncated and Right\n  Censored Data",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Left truncated and right censored data are encountered frequently in\ninsurance loss data due to deductibles and policy limits. Risk estimation is an\nimportant task in insurance as it is a necessary step for determining premiums\nunder various policy terms. Spectral risk measures are inherently coherent and\nhave the benefit of connecting the risk measure to the user's risk aversion. In\nthis paper we study the estimation of spectral risk measure based on left\ntruncated and right censored data. We propose a non parametric estimator of\nspectral risk measure using the product limit estimator and establish the\nasymptotic normality for our proposed estimator. We also develop an Edgeworth\nexpansion of our proposed estimator. The bootstrap is employed to approximate\nthe distribution of our proposed estimator and shown to be second order\n``accurate''. Monte Carlo studies are conducted to compare the proposed\nspectral risk measure estimator with the existing parametric and non parametric\nestimators for left truncated and right censored data. Based on our simulation\nstudy we estimate the exponential spectral risk measure for three data sets\nviz; Norwegian fire claims data set, Spain automobile insurance claims and\nFrench marine losses.\n"
    },
    {
        "paper_id": 2402.14389,
        "authors": "Md. Alamin Talukder, Rakib Hossen, Md Ashraf Uddin, Mohammed Nasir\n  Uddin and Uzzal Kumar Acharjee",
        "title": "Securing Transactions: A Hybrid Dependable Ensemble Machine Learning\n  Model using IHT-LR and Grid Search",
        "comments": "Q1, Scopus, ISI, ESCI, IF: 4.8 (Accepted on Jan 19, 2024 -\n  Cybersecurity, Springer Open Journal)",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Financial institutions and businesses face an ongoing challenge from\nfraudulent transactions, prompting the need for effective detection methods.\nDetecting credit card fraud is crucial for identifying and preventing\nunauthorized transactions.Timely detection of fraud enables investigators to\ntake swift actions to mitigate further losses. However, the investigation\nprocess is often time-consuming, limiting the number of alerts that can be\nthoroughly examined each day. Therefore, the primary objective of a fraud\ndetection model is to provide accurate alerts while minimizing false alarms and\nmissed fraud cases. In this paper, we introduce a state-of-the-art hybrid\nensemble (ENS) dependable Machine learning (ML) model that intelligently\ncombines multiple algorithms with proper weighted optimization using Grid\nsearch, including Decision Tree (DT), Random Forest (RF), K-Nearest Neighbor\n(KNN), and Multilayer Perceptron (MLP), to enhance fraud identification. To\naddress the data imbalance issue, we employ the Instant Hardness Threshold\n(IHT) technique in conjunction with Logistic Regression (LR), surpassing\nconventional approaches. Our experiments are conducted on a publicly available\ncredit card dataset comprising 284,807 transactions. The proposed model\nachieves impressive accuracy rates of 99.66%, 99.73%, 98.56%, and 99.79%, and a\nperfect 100% for the DT, RF, KNN, MLP and ENS models, respectively. The hybrid\nensemble model outperforms existing works, establishing a new benchmark for\ndetecting fraudulent transactions in high-frequency scenarios. The results\nhighlight the effectiveness and reliability of our approach, demonstrating\nsuperior performance metrics and showcasing its exceptional potential for\nreal-world fraud detection applications.\n"
    },
    {
        "paper_id": 2402.14476,
        "authors": "Steven Y. K. Wong, Jennifer S. K. Chan, Lamiae Azizi",
        "title": "Quantifying neural network uncertainty under volatility clustering",
        "comments": "38 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  Time-series with time-varying variance pose a unique challenge to uncertainty\nquantification (UQ) methods. Time-varying variance, such as volatility\nclustering as seen in financial time-series, can lead to large mismatch between\npredicted uncertainty and forecast error. Building on recent advances in neural\nnetwork UQ literature, we extend and simplify Deep Evidential Regression and\nDeep Ensembles into a unified framework to deal with UQ under the presence of\nvolatility clustering. We show that a Scale Mixture Distribution is a simpler\nalternative to the Normal-Inverse-Gamma prior that provides favorable\ncomplexity-accuracy trade-off. To illustrate the performance of our proposed\napproach, we apply it to two sets of financial time-series exhibiting\nvolatility clustering: cryptocurrencies and U.S. equities.\n"
    },
    {
        "paper_id": 2402.14555,
        "authors": "Moshe A. Milevsky and Thomas S. Salisbury",
        "title": "The Riccati Tontine: How to Satisfy Regulators on Average",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  This paper presents a new type of modern accumulation-based tontine, called\nthe Riccati tontine, named after two Italians: mathematician Jacobo Riccati (b.\n1676, d. 1754) and financier Lorenzo di Tonti (b. 1602, d. 1684). The Riccati\ntontine is yet another way of pooling and sharing longevity risk, but is\ndifferent from competing designs in two key ways. The first is that in the\nRiccati tontine, the representative investor is expected -- although not\nguaranteed -- to receive their money back if they die, or when the tontine\nlapses. The second is that the underlying funds within the tontine are\ndeliberately {\\em not} indexed to the stock market. Instead, the risky assets\nor underlying investments are selected so that return shocks are negatively\ncorrelated with stochastic mortality, which will maximize the expected payout\nto survivors. This means that during a pandemic, for example, the Riccati\ntontine fund's performance will be impaired relative to the market index, but\nwill not be expected to lose money for participants. In addition to describing\nand explaining the rationale for this non-traditional asset allocation, the\npaper provides a mathematical proof that the recovery schedule that generates\nthis financial outcome satisfies a first-order ODE that is quadratic in the\nunknown function, which (yes) is known as a Riccati equation.\n"
    },
    {
        "paper_id": 2402.14674,
        "authors": "Joachim Meyer",
        "title": "Doing AI: Algorithmic decision support as a human activity",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  Algorithmic decision support (ADS), using Machine-Learning-based AI, is\nbecoming a major part of many processes. Organizations introduce ADS to improve\ndecision-making and use available data, thereby possibly limiting deviations\nfrom the normative \"homo economicus\" and the biases that characterize human\ndecision-making. However, a closer look at the development and use of ADS\nsystems in organizational settings reveals that they necessarily involve a\nseries of largely unspecified human decisions. They begin with deliberations\nfor which decisions to use ADS, continue with choices while developing and\ndeploying the ADS, and end with decisions on how to use the ADS output in an\norganization's operations. The paper presents an overview of these decisions\nand some relevant behavioral phenomena. It points out directions for further\nresearch, which is essential for correctly assessing the processes and their\nvulnerabilities. Understanding these behavioral aspects is important for\nsuccessfully implementing ADS in organizations.\n"
    },
    {
        "paper_id": 2402.14708,
        "authors": "Yifan Duan, Guibin Zhang, Shilong Wang, Xiaojiang Peng, Wang Ziqi,\n  Junyuan Mao, Hao Wu, Xinke Jiang, Kun Wang",
        "title": "CaT-GNN: Enhancing Credit Card Fraud Detection via Causal Temporal Graph\n  Neural Networks",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Credit card fraud poses a significant threat to the economy. While Graph\nNeural Network (GNN)-based fraud detection methods perform well, they often\noverlook the causal effect of a node's local structure on predictions. This\npaper introduces a novel method for credit card fraud detection, the\n\\textbf{\\underline{Ca}}usal \\textbf{\\underline{T}}emporal\n\\textbf{\\underline{G}}raph \\textbf{\\underline{N}}eural \\textbf{N}etwork\n(CaT-GNN), which leverages causal invariant learning to reveal inherent\ncorrelations within transaction data. By decomposing the problem into discovery\nand intervention phases, CaT-GNN identifies causal nodes within the transaction\ngraph and applies a causal mixup strategy to enhance the model's robustness and\ninterpretability. CaT-GNN consists of two key components: Causal-Inspector and\nCausal-Intervener. The Causal-Inspector utilizes attention weights in the\ntemporal attention mechanism to identify causal and environment nodes without\nintroducing additional parameters. Subsequently, the Causal-Intervener performs\na causal mixup enhancement on environment nodes based on the set of nodes.\nEvaluated on three datasets, including a private financial dataset and two\npublic datasets, CaT-GNN demonstrates superior performance over existing\nstate-of-the-art methods. Our findings highlight the potential of integrating\ncausal reasoning with graph neural networks to improve fraud detection\ncapabilities in financial transactions.\n"
    },
    {
        "paper_id": 2402.14939,
        "authors": "Youssef Er-Rays, Meriem M'dioud",
        "title": "Evaluating the Financial Factors Influencing Maternal, Newborn, and\n  Child Health in Africa",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The study investigated the impact of healthcare system efficiency on the\ndelivery of maternal, newborn, and child services in Africa. Data Envelopment\nAnalysis and Tobit regression were employed to assess the efficiency of 46\nhealthcare systems across the continent, utilizing the Variable Returns to\nScale model with Input orientation to evaluate technical efficiency. The Tobit\nmethod was utilized to explore factors contributing to inefficiency, with\ninputs variables including hospital, physician, and paramedical staff, and\noutputs variables encompassing maternal, newborn, and child admissions,\ncesarean interventions, functional competency, and hospitalization days.\nResults revealed that only 26% of countries exhibited efficiency, highlighting\na significant proportion of 74% with inefficiencies. Financial determinants\nsuch as current health expenditures, comprehensive coverage index, and current\nhealth expenditure per capita were found to have a negative impact on the\nefficiency of maternal-child services. These findings underscore a marginal\ndeficiency in technical efficiency within Africa's healthcare systems,\nemphasizing the necessity for policymakers to reassess the roles of both human\nresources and financial dimensions in enhancing healthcare system performance.\n"
    },
    {
        "paper_id": 2402.1494,
        "authors": "Youssef Er-Rays, Meriem M'dioud",
        "title": "Assessment of Technical Efficiency in the Moroccan Public Hospital\n  Network: Using the DEA Method",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Background: The public hospital network in Morocco plays a crucial role in\nproviding healthcare services. However, this network faces challenges in terms\nof technical efficiency in healthcare management. Objectives: This study aimed\nto assess the technical efficiency of the public hospital network in Morocco.\nMethods: This article compares the efficiency of 77 public hospital networks\nfrom 2017 to 2020. Data were collected from the Directorate of Planning and\nFinancial Resources (DPFR) of the Health Ministry Marocco. The Data Envelopment\nAnalysis (DEA) method was employed, using three inputs (Hospital, Physician and\nParamedical) and four outputs (Functional capacity, Hospitalization days and\nAdmission). Additionally, the Malmquist index (MI) is utilized to analyse the\nfactors of production, and peer modelling is incorporated to address hospital\ninefficiency. Results: The average technical efficiency of public hospital\nnetworks under the CRS hypothesis from 2017 to 2020 is 0.697 (71% of DMUs have\na score lower than 1), indicating that these networks need to minimize their\ninputs by approximately 30.3%. The Malmquist index reveals a decline in\nproductivity gain from 2017/2018 (score of 0.980) followed by improvement in\n2018/2019 (score of 1.163). In terms of peer modelling, 72.7% of the DMUs\nshould emulate the most effective DMUs beginning in 2020, whereas the lowest\nscore was observed in 2019. Conclusion: These findings highlight the need for\nthe public hospital network in Morocco to enhance the effective and efficient\nutilization of inputs, such as the number of hospitals and medical and\nparamedical staff, to produce the same outputs, including the number of\nsurgical procedures, hospitalization days, admissions, and functional capacity.\n"
    },
    {
        "paper_id": 2402.14983,
        "authors": "Panyi Dong, Zhiyu Quan, Brandon Edwards, Shih-han Wang, Runhuan Feng,\n  Tianyang Wang, Patrick Foley, Prashant Shah",
        "title": "Privacy-Enhancing Collaborative Information Sharing through Federated\n  Learning -- A Case of the Insurance Industry",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The report demonstrates the benefits (in terms of improved claims loss\nmodeling) of harnessing the value of Federated Learning (FL) to learn a single\nmodel across multiple insurance industry datasets without requiring the\ndatasets themselves to be shared from one company to another. The application\nof FL addresses two of the most pressing concerns: limited data volume and data\nvariety, which are caused by privacy concerns, the rarity of claim events, the\nlack of informative rating factors, etc.. During each round of FL,\ncollaborators compute improvements on the model using their local private data,\nand these insights are combined to update a global model. Such aggregation of\ninsights allows for an increase to the effectiveness in forecasting claims\nlosses compared to models individually trained at each collaborator.\nCritically, this approach enables machine learning collaboration without the\nneed for raw data to leave the compute infrastructure of each respective data\nowner. Additionally, the open-source framework, OpenFL, that is used in our\nexperiments is designed so that it can be run using confidential computing as\nwell as with additional algorithmic protections against leakage of information\nvia the shared model updates. In such a way, FL is implemented as a\nprivacy-enhancing collaborative learning technique that addresses the\nchallenges posed by the sensitivity and privacy of data in traditional machine\nlearning solutions. This paper's application of FL can also be expanded to\nother areas including fraud detection, catastrophe modeling, etc., that have a\nsimilar need to incorporate data privacy into machine learning collaborations.\nOur framework and empirical results provide a foundation for future\ncollaborations among insurers, regulators, academic researchers, and InsurTech\nexperts.\n"
    },
    {
        "paper_id": 2402.15037,
        "authors": "Abhimanyu Nag, Samrat Gupta, Sudipan Sinha and Arka Datta",
        "title": "Analyzing Games in Maker Protocol Part One: A Multi-Agent Influence\n  Diagram Approach Towards Coordination",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Decentralized Finance (DeFi) ecosystems, exemplified by the Maker Protocol,\nrely on intricate games to maintain stability and security. Understanding the\ndynamics of these games is crucial for ensuring the robustness of the system.\nThis motivating research proposes a novel methodology leveraging Multi-Agent\nInfluence Diagrams (MAID), originally proposed by Koller and Milch, to dissect\nand analyze the games within the Maker stablecoin protocol. By representing\nusers and governance of the Maker protocol as agents and their interactions as\nedges in a graph, we capture the complex network of influences governing agent\nbehaviors. Furthermore in the upcoming papers, we will show a Nash Equilibrium\nmodel to elucidate strategies that promote coordination and enhance economic\nsecurity within the ecosystem. Through this approach, we aim to motivate the\nuse of this method to introduce a new method of formal verification of game\ntheoretic security in DeFi platforms.\n"
    },
    {
        "paper_id": 2402.15072,
        "authors": "Andrew Ireland, David Johnston, Rachel Knott",
        "title": "Impacts of Extreme Heat on Labor Force Dynamics",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
        "abstract": "  We use daily longitudinal data and a within-worker identification approach to\nexamine the impacts of heat on labor force dynamics in Australia. High\ntemperatures during 2001-2019 significantly reduced work attendance and hours\nworked, which were not compensated for in subsequent days and weeks. The\nlargest reductions occurred in cooler regions and recent years, and were not\nsolely concentrated amongst outdoor-based workers. Financial and Insurance\nServices was the most strongly affected industry, with temperatures above\n38{\\deg}C (100{\\deg}F) increasing absenteeism by 15 percent. Adverse heat\neffects during the work commute and during outdoor work hours are shown to be\nkey mechanisms.\n"
    },
    {
        "paper_id": 2402.15387,
        "authors": "Alois Pichler",
        "title": "Higher order measures of risk and stochastic dominance",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Higher order risk measures are stochastic optimization problems by design,\nand for this reason they enjoy valuable properties in optimization under\nuncertainties. They nicely integrate with stochastic optimization problems, as\nhas been observed by the intriguing concept of the risk quadrangles, for\nexample. Stochastic dominance is a binary relation for random variables to\ncompare random outcomes. It is demonstrated that the concepts of higher order\nrisk measures and stochastic dominance are equivalent, they can be employed to\ncharacterize the other. The paper explores these relations and connects\nstochastic orders, higher order risk measures and the risk quadrangle.\nExpectiles are employed to exemplify the relations obtained.\n"
    },
    {
        "paper_id": 2402.15498,
        "authors": "Ying Wu, Garvit Arora, Xuan Mei",
        "title": "Using CPI in Loss Given Default Forecasting Models for Commercial Real\n  Estate Portfolio",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Forecasting the loss given default (LGD) for defaulted Commercial Real Estate\n(CRE) loans poses a significant challenge due to the extended resolution and\nworkout time associated with such defaults, particularly in CCAR and CECL\nframework where the utilization of post-default information, including\nmacroeconomic variables (MEVs) such as unemployment (UER) and various rates, is\nrestricted. The current environment of persistent inflation and resultant\nelevated rates further compounds the uncertainty surrounding predictive LGD\nmodels. In this paper, we leverage both internal and public data sources,\nincluding observations from the COVID-19 period, to present a list of evidence\nindicating that the growth rates of the Consumer Price, such as Year-over-Year\n(YoY) growth and logarithmic growth, are good leading indicators for various\nCRE related rates and indices. These include the Federal Funds Effective Rate\nand CRE market sales price indices in key locations such as Los Angeles, New\nYork, and nationwide, encompassing both apartment and office segments.\nFurthermore, with CRE LGD data we demonstrate how incorporating CPI at the time\nof default can improve the accuracy of predicting CRE workout LGD. This is\nparticularly helpful in addressing the common issue of early downturn\nunderestimation encountered in CRE LGD models.\n"
    },
    {
        "paper_id": 2402.15588,
        "authors": "Vuko Vukcevic and Robert Keser",
        "title": "Sizing the bets in a focused portfolio",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  The paper provides a mathematical model and a tool for the focused investing\nstrategy as advocated by Buffett, Munger, and others from this investment\ncommunity. The approach presented here assumes that the investor's role is to\nthink about probabilities of different outcomes for a set of businesses. Based\non these assumptions, the tool calculates the optimal allocation of capital for\neach of the investment candidates. The model is based on a generalized Kelly\nCriterion with options to provide constraints that ensure: no shorting, limited\nuse of leverage, providing a maximum limit to the risk of permanent loss of\ncapital, and maximum individual allocation. The software is applied to an\nexample portfolio from which certain observations about excessive\ndiversification are obtained. In addition, the software is made available for\npublic use.\n"
    },
    {
        "paper_id": 2402.1562,
        "authors": "Tao Wang, Shiying Xiao, Jun Yan",
        "title": "Comparison of sectoral structures between China and Japan: A network\n  perspective",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Economic structure comparisons between China and Japan have long captivated\ndevelopment economists. To delve deeper into their sectoral differences from\n1995 to 2018, we used the annual input-output tables (IOTs) of both nations to\nconstruct weighted and directed input-output networks (IONs). This facilitated\ndeeper network analyses. Strength distributions underscored variations in\ninter-sector economic interactions. Weighted, directed assortativity\ncoefficients encapsulated the homophily among connecting sectors' features. By\nadjusting emphasis in PageRank centrality, key sectors were identified.\nCommunity detection revealed their clustering tendencies among the sectors. As\nanticipated, the analysis pinpointed manufacturing as China's central sector,\nwhile Japan favored services. Yet, at a finer level of the specific sectors,\nboth nations exhibited varied structural evolutions. Contrastingly, sectoral\ncommunities in both China and Japan demonstrated commendable stability over the\nexamined duration.\n"
    },
    {
        "paper_id": 2402.15828,
        "authors": "Florian Aichinger and Sascha Desmettre",
        "title": "Pricing of geometric Asian options in the Volterra-Heston model",
        "comments": "23 pages, 5 tables",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Geometric Asian options are a type of options where the payoff depends on the\ngeometric mean of the underlying asset over a certain period of time. This\npaper is concerned with the pricing of such options for the class of\nVolterra-Heston models, covering the rough Heston model. We are able to derive\nsemi-closed formulas for the prices of geometric Asian options with fixed and\nfloating strikes for this class of stochastic volatility models. These formulas\nrequire the explicit calculation of the conditional joint Fourier transform of\nthe logarithm of the stock price and the logarithm of the geometric mean of the\nstock price over time. Linking our problem to the theory of affine Volterra\nprocesses, we find a representation of this Fourier transform as a suitably\nconstructed stochastic exponential, which depends on the solution of a\nRiccati-Volterra equation. Finally we provide a numerical study for our results\nin the rough Heston model.\n"
    },
    {
        "paper_id": 2402.15936,
        "authors": "Vikranth Lokeshwar Dhandapani, Shashi Jain",
        "title": "Optimizing Neural Networks for Bermudan Option Pricing: Convergence\n  Acceleration, Future Exposure Evaluation and Interpolation in Counterparty\n  Credit Risk",
        "comments": "34 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper presents a Monte-Carlo-based artificial neural network framework\nfor pricing Bermudan options, offering several notable advantages. These\nadvantages encompass the efficient static hedging of the target Bermudan option\nand the effective generation of exposure profiles for risk management. We also\nintroduce a novel optimisation algorithm designed to expedite the convergence\nof the neural network framework proposed by Lokeshwar et al. (2022) supported\nby a comprehensive error convergence analysis. We conduct an extensive\ncomparative analysis of the Present Value (PV) distribution under Markovian and\nno-arbitrage assumptions. We compare the proposed neural network model in\nconjunction with the approach initially introduced by Longstaff and Schwartz\n(2001) and benchmark it against the COS model, the pricing model pioneered by\nFang and Oosterlee (2009), across all Bermudan exercise time points.\nAdditionally, we evaluate exposure profiles, including Expected Exposure and\nPotential Future Exposure, generated by our proposed model and the\nLongstaff-Schwartz model, comparing them against the COS model. We also derive\nexposure profiles at finer non-standard grid points or risk horizons using the\nproposed approach, juxtaposed with the Longstaff Schwartz method with linear\ninterpolation and benchmark against the COS method. In addition, we explore the\neffectiveness of various interpolation schemes within the context of the\nLongstaff-Schwartz method for generating exposures at finer grid horizons.\n"
    },
    {
        "paper_id": 2402.15965,
        "authors": "Lynn Huang",
        "title": "Evolving E-commerce Logistics Planning- Integrating Embedded Technology\n  and Ant Colony Algorithm for Enhanced Efficiency",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Amidst the era of networking, the e-commerce sector has undergone notable\nexpansion, notably with the advent of Cross-border E-commerce (CBEC) in recent\ntimes. This growth trend persists, necessitating robust logistical frameworks\nto sustainably support operations. However, the current e-commerce logistics\nparadigm faces challenges in meeting evolving user demands, prompting a quest\nfor innovative solutions. This research endeavors to address these complexities\nby undertaking a comprehensive analysis of CBEC logistics models and\nintegrating embedded technology into logistical frameworks, resulting in the\ndevelopment of an advanced logistics tracking system. Moreover, employing the\nant colony algorithm, the study conducts experimental investigations into\noptimizing logistics package distribution route planning. Noteworthy\nenhancements are observed in key metrics such as average delivery time,\nsignaling the efficacy of this approach. In essence, this research offers a\npromising pathway towards optimizing logistics package distribution routes and\nbolstering package transportation efficiency within the CBEC domain.\n"
    },
    {
        "paper_id": 2402.15994,
        "authors": "Qishuo Cheng, Le Yang, Jiajian Zheng, Miao Tian, Duan Xin",
        "title": "Optimizing Portfolio Management and Risk Assessment in Digital Assets\n  Using Deep Learning for Predictive Analysis",
        "comments": "10 pages, 5 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Portfolio management issues have been extensively studied in the field of\nartificial intelligence in recent years, but existing deep learning-based\nquantitative trading methods have some areas where they could be improved.\nFirst of all, the prediction mode of stocks is singular; often, only one\ntrading expert is trained by a model, and the trading decision is solely based\non the prediction results of the model. Secondly, the data source used by the\nmodel is relatively simple, and only considers the data of the stock itself,\nignoring the impact of the whole market risk on the stock. In this paper, the\nDQN algorithm is introduced into asset management portfolios in a novel and\nstraightforward way, and the performance greatly exceeds the benchmark, which\nfully proves the effectiveness of the DRL algorithm in portfolio management.\nThis also inspires us to consider the complexity of financial problems, and the\nuse of algorithms should be fully combined with the problems to adapt. Finally,\nin this paper, the strategy is implemented by selecting the assets and actions\nwith the largest Q value. Since different assets are trained separately as\nenvironments, there may be a phenomenon of Q value drift among different assets\n(different assets have different Q value distribution areas), which may easily\nlead to incorrect asset selection. Consider adding constraints so that the Q\nvalues of different assets share a Q value distribution to improve results.\n"
    },
    {
        "paper_id": 2402.16118,
        "authors": "Bruno Ga\\v{s}perov, Marko {\\DJ}urasevi\\'c, Domagoj Jakobovic",
        "title": "Finding Near-Optimal Portfolios With Quality-Diversity",
        "comments": "Preprint of an article accepted for publication in Applications of\n  Evolutionary Computation, 27th International Conference, EvoApplications 2024",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The majority of standard approaches to financial portfolio optimization (PO)\nare based on the mean-variance (MV) framework. Given a risk aversion\ncoefficient, the MV procedure yields a single portfolio that represents the\noptimal trade-off between risk and return. However, the resulting optimal\nportfolio is known to be highly sensitive to the input parameters, i.e., the\nestimates of the return covariance matrix and the mean return vector. It has\nbeen shown that a more robust and flexible alternative lies in determining the\nentire region of near-optimal portfolios. In this paper, we present a novel\napproach for finding a diverse set of such portfolios based on\nquality-diversity (QD) optimization. More specifically, we employ the\nCVT-MAP-Elites algorithm, which is scalable to high-dimensional settings with\npotentially hundreds of behavioral descriptors and/or assets. The results\nhighlight the promising features of QD as a novel tool in PO.\n"
    },
    {
        "paper_id": 2402.16345,
        "authors": "Nina Badulina, Dmitry Shatilovich, Mikhail Zhitlukhin",
        "title": "On convergence of forecasts in prediction markets",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We propose a dynamic model of a prediction market in which agents predict the\nvalues of a sequence of random vectors. The main result shows that if there are\nagents who make correct (or asymptotically correct) next-period forecasts, then\nthe aggregated market forecasts converge to the next-period conditional\nexpectations of the random vectors.\n"
    },
    {
        "paper_id": 2402.16375,
        "authors": "Selim Manka\\\"i (IAE - UCA, UCA), S\\'ebastien Marchand (CERDI, UCA),\n  Ngoc Ha Le (UCA)",
        "title": "Valuing insurance against small probability risks: A meta-analysis",
        "comments": null,
        "journal-ref": "Journal of Behavioral and Experimental Economics, 109, pp.102181",
        "doi": "10.1016/j.socec.2024.102181",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The demand for voluntary insurance against low-probability, high-impact risks\nis lower than expected. To assess the magnitude of the demand, we conduct a\nmeta-analysis of contingent valuation studies using a dataset of experimentally\nelicited and survey-based estimates. We find that the average stated\nwillingness to pay (WTP) for insurance is 87% of expected losses. We perform a\nmeta-regression analysis to examine the heterogeneity in aggregate WTP across\nthese studies. The meta-regression reveals that information about loss\nprobability and probability levels positively influence relative willingness to\npay, whereas respondents' average income and age have a negative effect.\nMoreover, we identify cultural sub-factors, such as power distance and\nuncertainty avoidance, that provided additional explanations for differences in\nWTP across international samples. Methodological factors related to the\nsampling and data collection process significantly influence the stated WTP.\nOur results, robust to model specification and publication bias, are relevant\nto current debates on stated preferences for low-probability risks management.\n"
    },
    {
        "paper_id": 2402.16401,
        "authors": "Felix Dammann and Giorgio Ferrari",
        "title": "A Stationary Equilibrium Model of Green Technology Adoption with\n  Endogenous Carbon Price",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper proposes and analyzes a stationary equilibrium model for a\ncompetitive industry which endogenously determines the carbon price necessary\nto achieve a given emission target. In the model, firms are identified by their\nlevel of technology and make production, entry, and abatement decisions.\nPolluting firms are subject to a carbon price and abatement is formulated as an\nirreversible investment, which entails a sunk cost and results in the firms\nswitching to a carbon neutral technology. In equilibrium, we identify a carbon\nprice and a stationary distribution of incumbent, polluting firms, that\nguarantee the compliance with a certain emission target. Our general\ntheoretical framework is complemented with a case study with Brownian\ntechnology shocks, in which we discuss some implications of our model. We\nobserve that a carbon pricing system alongside installation subsidies and tax\nbenefits for green firms trigger earlier investment, while higher income taxes\nfor polluting firms may be distorting. Moreover, we discuss the role of a\nwelfare maximizing regulator, who, by optimally setting the emission target,\nmay mitigate or revert some parameters' effects observed in the model with\nfixed limit.\n"
    },
    {
        "paper_id": 2402.16428,
        "authors": "Alet Roux and \\'Alvaro Guinea Juli\\'a",
        "title": "Closed form solution to zero coupon bond using a linear stochastic delay\n  differential equation",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We present a short rate model that satisfies a stochastic delay differential\nequation. The model can be considered a delayed version of the Merton model\n(Merton 1970, 1973) or the Vasi\\v{c}ek model (Vasi\\v{c}ek 1977). Using the same\ntechnique as the one used by Flore and Nappo (2019), we show that the bond\nprice is an affine function of the short rate, whose coefficients satisfy a\nsystem of delay differential equations. We give an analytical solution to this\nsystem of delay differential equations, obtaining a closed formula for the zero\ncoupon bond price. Under this model, we can show that the distribution of the\nshort rate is a normal distribution whose mean depends on past values of the\nshort rate. Based on the results of K\\\"uchler and Mensch (1992), we prove the\nexistence of stationary and limiting distributions.\n"
    },
    {
        "paper_id": 2402.16509,
        "authors": "Huy N. Chau, Duy Nguyen, and Thai Nguyen",
        "title": "On short-time behavior of implied volatility in a market model with\n  indexes",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper investigates short-term behaviors of implied volatility of\nderivatives written on indexes in equity markets when the index processes are\nconstructed by using a ranking procedure. Even in simple market settings where\nstock prices follow geometric Brownian motion dynamics, the ranking mechanism\ncan produce the observed term structure of at-the-money (ATM) implied\nvolatility skew for equity indexes. Our proposed models showcase the ability to\nreconcile two seemingly contradictory features found in empirical data from\nequity markets: the long memory of volatilities and the power law of ATM skews.\nFurthermore, the models allow for the capture of a novel phenomenon termed the\nquasi-blow-up phenomenon.\n"
    },
    {
        "paper_id": 2402.16538,
        "authors": "Thomas Dohmen and Georgios Gerasimou",
        "title": "Learning to Maximize (Expected) Utility",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  We study if participants in a choice experiment learn to behave in ways that\nare closer to the predictions of ordinal and expected utility theory as they\nmake decisions from the same menus repeatedly and without receiving feedback of\nany kind. We designed and implemented a non-forced-choice lab experiment with\nmoney lotteries and five repetitions per menu that aimed to test this\nhypothesis from many behavioural angles. In our data from 308 subjects in the\nUK and Germany, significantly more individuals were ordinal- and\nexpected-utility maximizers in their last 15 than in their first 15 identical\ndecision problems. Furthermore, around a quarter and a fifth of all subjects,\nrespectively, decided in those modes throughout the experiment, with nearly\nhalf revealing non-trivial indifferences. A considerable overlap was found\nbetween those consistently rational individuals and the ones who satisfied core\nprinciples of random utility theory. Finally, in addition to finding that\nchoice consistency is positively correlated with cognitive ability, we document\nthat subjects who learned to maximize utility were more cognitively able than\nthose who did not. We discuss potential implications of our analysis.\n"
    },
    {
        "paper_id": 2402.16609,
        "authors": "Ruoyu Sun (1), Angelos Stefanidis (2), Zhengyong Jiang (2), Jionglong\n  Su (2) ((1) Xi'an Jiaotong-Liverpool University, School of Mathematics and\n  Physics, Department of Financial and Actuarial Mathematics (2) Xi'an\n  Jiaotong-Liverpool University Entrepreneur College (Taicang), School of AI\n  and Advanced Computing (1))",
        "title": "Combining Transformer based Deep Reinforcement Learning with\n  Black-Litterman Model for Portfolio Optimization",
        "comments": "46 pages, 15 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  As a model-free algorithm, deep reinforcement learning (DRL) agent learns and\nmakes decisions by interacting with the environment in an unsupervised way. In\nrecent years, DRL algorithms have been widely applied by scholars for portfolio\noptimization in consecutive trading periods, since the DRL agent can\ndynamically adapt to market changes and does not rely on the specification of\nthe joint dynamics across the assets. However, typical DRL agents for portfolio\noptimization cannot learn a policy that is aware of the dynamic correlation\nbetween portfolio asset returns. Since the dynamic correlations among portfolio\nassets are crucial in optimizing the portfolio, the lack of such knowledge\nmakes it difficult for the DRL agent to maximize the return per unit of risk,\nespecially when the target market permits short selling (i.e., the US stock\nmarket). In this research, we propose a hybrid portfolio optimization model\ncombining the DRL agent and the Black-Litterman (BL) model to enable the DRL\nagent to learn the dynamic correlation between the portfolio asset returns and\nimplement an efficacious long/short strategy based on the correlation.\nEssentially, the DRL agent is trained to learn the policy to apply the BL model\nto determine the target portfolio weights. To test our DRL agent, we construct\nthe portfolio based on all the Dow Jones Industrial Average constitute stocks.\nEmpirical results of the experiments conducted on real-world United States\nstock market data demonstrate that our DRL agent significantly outperforms\nvarious comparison portfolio choice strategies and alternative DRL frameworks\nby at least 42% in terms of accumulated return. In terms of the return per unit\nof risk, our DRL agent significantly outperforms various comparative portfolio\nchoice strategies and alternative strategies based on other machine learning\nframeworks.\n"
    },
    {
        "paper_id": 2402.16724,
        "authors": "Svetlana Boyarchenko and Sergei Levendorski\\u{i}",
        "title": "Alternative models for FX: pricing double barrier options in\n  regime-switching L\\'evy models with memory",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper is a supplement to our recent paper ``Alternative models for FX,\narbitrage opportunities and efficient pricing of double barrier options in\nL\\'evy models\". We introduce the class of regime-switching L\\'evy models with\nmemory, which take into account the evolution of the stochastic parameters in\nthe past. This generalization of the class of L\\'evy models modulated by Markov\nchains is similar in spirit to rough volatility models. It is flexible and\nsuitable for application of the machine-learning tools. We formulate the\nmodification of the numerical method in ``Alternative models for FX, arbitrage\nopportunities and efficient pricing of double barrier options in L\\'evy\nmodels\", which has the same number of the main time-consuming blocks as the\nmethod for Markovian regime-switching models.\n"
    },
    {
        "paper_id": 2402.17148,
        "authors": "Nozomu Kobayashi, Yoshiyuki Suimon, Koichi Miyamoto",
        "title": "Time series generation for option pricing on quantum computers using\n  tensor network",
        "comments": "15 pages, 2 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Finance, especially option pricing, is a promising industrial field that\nmight benefit from quantum computing. While quantum algorithms for option\npricing have been proposed, it is desired to devise more efficient\nimplementations of costly operations in the algorithms, one of which is\npreparing a quantum state that encodes a probability distribution of the\nunderlying asset price. In particular, in pricing a path-dependent option, we\nneed to generate a state encoding a joint distribution of the underlying asset\nprice at multiple time points, which is more demanding. To address these\nissues, we propose a novel approach using Matrix Product State (MPS) as a\ngenerative model for time series generation. To validate our approach, taking\nthe Heston model as a target, we conduct numerical experiments to generate time\nseries in the model. Our findings demonstrate the capability of the MPS model\nto generate paths in the Heston model, highlighting its potential for\npath-dependent option pricing on quantum computers.\n"
    },
    {
        "paper_id": 2402.17164,
        "authors": "Hayden Brown",
        "title": "Withdrawal Success Optimization in a Pooled Annuity Fund",
        "comments": "arXiv admin note: text overlap with arXiv:2311.06665",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Consider a closed pooled annuity fund investing in n assets with\ndiscrete-time rebalancing. At time 0, each annuitant makes an initial\ncontribution to the fund, committing to a predetermined schedule of\nwithdrawals. Require annuitants to be homogeneous in the sense that their\ninitial contributions and predetermined withdrawal schedules are identical, and\ntheir mortality distributions are identical and independent. Under the\nforementioned setup, the probability for a particular annuitant to complete the\nprescribed withdrawals until death is maximized over progressively measurable\nportfolio weight functions. Applications consider fund portfolios that mix two\nassets: the S&P Composite Index and an inflation-protected bond. The maximum\nprobability is computed for annually rebalanced schedules consisting of an\ninitial investment and then equal annual withdrawals until death. A\nconsiderable increase in the maximum probability is achieved by increasing the\nnumber of annuitants initially in the pool. For example, when the per-annuitant\ninitial contribution and annual withdrawal amount are held constant, starting\nwith 20 annuitants instead of just 1 can increase the maximum probability\n(measured on a scale from 0 to 1) by as much as .15.\n"
    },
    {
        "paper_id": 2402.17194,
        "authors": "Jiajian Zheng, Duan Xin, Qishuo Cheng, Miao Tian, Le Yang",
        "title": "The Random Forest Model for Analyzing and Forecasting the US Stock\n  Market in the Context of Smart Finance",
        "comments": "10 pages, 8 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The stock market is a crucial component of the financial market, playing a\nvital role in wealth accumulation for investors, financing costs for listed\ncompanies, and the stable development of the national macroeconomy. Significant\nfluctuations in the stock market can damage the interests of stock investors\nand cause an imbalance in the industrial structure, which can interfere with\nthe macro level development of the national economy. The prediction of stock\nprice trends is a popular research topic in academia. Predicting the three\ntrends of stock pricesrising, sideways, and falling can assist investors in\nmaking informed decisions about buying, holding, or selling stocks.\nEstablishing an effective forecasting model for predicting these trends is of\nsubstantial practical importance. This paper evaluates the predictive\nperformance of random forest models combined with artificial intelligence on a\ntest set of four stocks using optimal parameters. The evaluation considers both\npredictive accuracy and time efficiency.\n"
    },
    {
        "paper_id": 2402.17359,
        "authors": "Konark Jain, Nick Firoozye, Jonathan Kochems and Philip Treleaven",
        "title": "Limit Order Book Simulations: A Review",
        "comments": "To be submitted to Quantitative Finance",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Limit Order Books (LOBs) serve as a mechanism for buyers and sellers to\ninteract with each other in the financial markets. Modelling and simulating\nLOBs is quite often necessary for calibrating and fine-tuning the automated\ntrading strategies developed in algorithmic trading research. The recent AI\nrevolution and availability of faster and cheaper compute power has enabled the\nmodelling and simulations to grow richer and even use modern AI techniques. In\nthis review we examine the various kinds of LOB simulation models present in\nthe current state of the art. We provide a classification of the models on the\nbasis of their methodology and provide an aggregate view of the popular\nstylized facts used in the literature to test the models. We additionally\nprovide a focused study of price impact's presence in the models since it is\none of the more crucial phenomena to model in algorithmic trading. Finally, we\nconduct a comparative analysis of various qualities of fits of these models and\nhow they perform when tested against empirical data.\n"
    },
    {
        "paper_id": 2402.17523,
        "authors": "Mehmet Caner, Qingliang Fan, Yingying Li",
        "title": "Navigating Complexity: Constrained Portfolio Analysis in High Dimensions\n  with Tracking Error and Weight Constraints",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  This paper analyzes the statistical properties of constrained portfolio\nformation in a high dimensional portfolio with a large number of assets.\nNamely, we consider portfolios with tracking error constraints, portfolios with\ntracking error jointly with weight (equality or inequality) restrictions, and\nportfolios with only weight restrictions. Tracking error is the portfolio's\nperformance measured against a benchmark (an index usually), {\\color{black}{and\nweight constraints refers to specific allocation of assets within the\nportfolio, which often come in the form of regulatory requirement or fund\nprospectus.}} We show how these portfolios can be estimated consistently in\nlarge dimensions, even when the number of assets is larger than the time span\nof the portfolio. We also provide rate of convergence results for weights of\nthe constrained portfolio, risk of the constrained portfolio and the Sharpe\nRatio of the constrained portfolio. To achieve those results we use a new\nmachine learning technique that merges factor models with nodewise regression\nin statistics. Simulation results and empirics show very good performance of\nour method.\n"
    },
    {
        "paper_id": 2402.17526,
        "authors": "Simon Lodato, Christos Mavridis, Federico Vaccari",
        "title": "The Unelected Hand? Bureaucratic Influence and Electoral Accountability",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  What role do non-elected bureaucrats play when elections provide imperfect\naccountability and create incentives for pandering? We develop a model where\npoliticians and bureaucrats interact to implement policy. Both can either be\ngood, sharing the voters' preferences over policies, or bad, intent on enacting\npolicies that favor private interests. Our analysis identifies the conditions\nunder which good bureaucrats choose to support or oppose political pandering.\nWhen bureaucrats wield significant influence over policy decisions, good\npoliticians lose their incentives to pander, a shift that ultimately benefits\nvoters. An intermediate level of bureaucratic influence over policymaking can\nbe voter-optimal: large enough to prevent pandering but small enough to avoid\ngranting excessive influence to potentially bad bureaucrats.\n"
    },
    {
        "paper_id": 2402.17642,
        "authors": "Ran Wei, Jinjiong Yu",
        "title": "The critical disordered pinning measure",
        "comments": "51 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  In this paper, we study a disordered pinning model induced by a random walk\nwhose increments have a finite fourth moment and vanishing first and third\nmoments. It is known that this model is marginally relevant, and moreover, it\nundergoes a phase transition in an intermediate disorder regime. We show that,\nin the critical window, the point-to-point partition functions converge to a\nunique limiting random measure, which we call the critical disordered pinning\nmeasure. We also obtain an analogous result for a continuous counterpart to the\npinning model, which is closely related to two other models: one is a critical\nstochastic Volterra equation that gives rise to a rough volatility model, and\nthe other is a critical stochastic heat equation with multiplicative noise that\nis white in time and delta in space.\n"
    },
    {
        "paper_id": 2402.17684,
        "authors": "Fabien Le Floc'h",
        "title": "Stochastic Expansion for the Pricing of Asian and Basket Options",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  We present closed analytical approximations for the pricing of basket\noptions, also applicable to Asian options with discrete averaging under the\nBlack-Scholes model with time-dependent parameters. The formulae are obtained\nby using a stochastic Taylor expansion around a log-normal proxy model and are\nfound to be highly accurate for Asian options in practice as well as for\nvanilla options with discrete dividends.\n"
    },
    {
        "paper_id": 2402.17919,
        "authors": "Young Shin Kim and Hyun-Gyoon Kim",
        "title": "Quanto Option Pricing on a Multivariate Levy Process Model with a\n  Generative Artificial Intelligence",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this study, we discuss a machine learning technique to price exotic\noptions with two underlying assets based on a non-Gaussian Levy process model.\nWe introduce a new multivariate Levy process model named the generalized normal\ntempered stable (gNTS) process, which is defined by time-changed multivariate\nBrownian motion. Since the gNTS process does not provide a simple analytic\nformula for the probability density function (PDF), we use the conditional\nreal-valued non-volume preserving (CRealNVP) model, which is a type of\nflow-based generative network. Then, we discuss the no-arbitrage pricing on the\ngNTS model for pricing the quanto option, whose underlying assets consist of a\nforeign index and foreign exchange rate. We present the training of the\nCRealNVP model to learn the PDF of the gNTS process using a training set\ngenerated by Monte Carlo simulation. Next, we estimate the parameters of the\ngNTS model with the trained CRealNVP model using the empirical data observed in\nthe market. Finally, we provide a method to find an equivalent martingale\nmeasure on the gNTS model and to price the quanto option using the CRealNVP\nmodel with the risk-neutral parameters of the gNTS model.\n"
    },
    {
        "paper_id": 2402.17932,
        "authors": "Deepeka Garg, Benjamin Patrick Evans, Leo Ardon, Annapoorani Lakshmi\n  Narayanan, Jared Vann, Udari Madhushani, Makada Henry-Nickie, Sumitra Ganesh",
        "title": "A Heterogeneous Agent Model of Mortgage Servicing: An Income-based\n  Relief Analysis",
        "comments": "AAAI 2024 - AI in Finance for Social Impact",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Mortgages account for the largest portion of household debt in the United\nStates, totaling around \\$12 trillion nationwide. In times of financial\nhardship, alleviating mortgage burdens is essential for supporting affected\nhouseholds. The mortgage servicing industry plays a vital role in offering this\nassistance, yet there has been limited research modelling the complex\nrelationship between households and servicers. To bridge this gap, we developed\nan agent-based model that explores household behavior and the effectiveness of\nrelief measures during financial distress. Our model represents households as\nadaptive learning agents with realistic financial attributes. These households\nexperience exogenous income shocks, which may influence their ability to make\nmortgage payments. Mortgage servicers provide relief options to these\nhouseholds, who then choose the most suitable relief based on their unique\nfinancial circumstances and individual preferences. We analyze the impact of\nvarious external shocks and the success of different mortgage relief strategies\non specific borrower subgroups. Through this analysis, we show that our model\ncan not only replicate real-world mortgage studies but also act as a tool for\nconducting a broad range of what-if scenario analyses. Our approach offers\nfine-grained insights that can inform the development of more effective and\ninclusive mortgage relief solutions.\n"
    },
    {
        "paper_id": 2402.17941,
        "authors": "Vikranth Lokeshwar Dhandapani, Shashi Jain",
        "title": "Neural Networks for Portfolio-Level Risk Management: Portfolio\n  Compression, Static Hedging, Counterparty Credit Risk Exposures and Impact on\n  Capital Requirement",
        "comments": "21 Pages, 8 Figures, 2 Tables",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper, we present an artificial neural network framework for\nportfolio compression of a large portfolio of European options with varying\nmaturities (target portfolio) by a significantly smaller portfolio of European\noptions with shorter or same maturity (compressed portfolio), which also\nrepresents a self-replicating static hedge portfolio of the target portfolio.\nFor the proposed machine learning architecture, which is consummately\ninterpretable by choice of design, we also define the algorithm to learn model\nparameters by providing a parameter initialisation technique and leveraging the\noptimisation methodology proposed in Lokeshwar and Jain (2024), which was\ninitially introduced to price Bermudan options. We demonstrate the convergence\nof errors and the iterative evolution of neural network parameters over the\ncourse of optimization process, using selected target portfolio samples for\nillustration. We demonstrate through numerical examples that the Exposure\ndistributions and Exposure profiles (Expected Exposure and Potential Future\nExposure) of the target portfolio and compressed portfolio align closely across\nfuture risk horizons under risk-neutral and real-world scenarios. Additionally,\nwe benchmark the target portfolio's Financial Greeks (Delta, Gamma, and Vega)\nagainst the compressed portfolio at future time horizons across different\nmarket scenarios generated by Monte-Carlo simulations. Finally, we compare the\nregulatory capital requirement under the standardised approach for counterparty\ncredit risk of the target portfolio against the compressed portfolio and\nhighlight that the capital requirement for the compact portfolio substantially\nreduces.\n"
    },
    {
        "paper_id": 2402.18014,
        "authors": "Bingchu Nie, Dejian Tian, Long Jiang",
        "title": "Set-valued Star-Shaped Risk Measures",
        "comments": "23 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  In this paper, we introduce a new class of set-valued risk measures, named\nset-valued star-shaped risk measures. Motivated by the results of scalar\nmonetary and star-shaped risk measures, this paper investigates the\nrepresentation theorems in the set-valued framework. It is demonstrated that\nset-valued risk measures can be represented as the union of a family of\nset-valued convex risk measures, and set-valued normalized star-shaped risk\nmeasures can be represented as the union of a family of set-valued normalized\nconvex risk measures. The link between set-valued risk measures and set-valued\nstar-shaped risk measures is also established.\n"
    },
    {
        "paper_id": 2402.18047,
        "authors": "Shusaku Sasaki, Takahiro Kubo, Shodai Kitano",
        "title": "Prosocial and Financial Incentives for Biodiversity Conservation: A\n  Field Experiment Using a Smartphone App",
        "comments": "45 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Ascertaining the number, type, and location of plant, insect, and animal\nspecies is essential for biodiversity conservation. However, comprehensively\nmonitoring the situation only through public fixed-point surveys is\nchallenging, and therefore information voluntarily provided by citizens assists\nin ascertaining the species distribution. To effectively encourage the\ncitizens' data sharing behavior, this study proposed a prosocial incentive\nscheme in which, if they provide species information, donations are made to\nactivities for saving endangered species. We conducted a field experiment with\nusers (N=830) of a widely-used Japanese smartphone app to which they post\nspecies photos and measured the incentive's effect on their posting behavior.\nIn addition, we measured the effect of a financial incentive scheme that\nprovides monetary rewards for posting species photos and compared the two\nincentives' effects. The analyses revealed that while the prosocial incentive\ndid not increase the number of posts on average, it did change the contents of\nposts, increasing the proportion of posts on rare species. On the contrary, the\nfinancial incentive statistically significantly increased the number of posts,\nin particular, on less rare and invasive species. Our findings suggest that the\nprosocial and financial incentives could stimulate different motivations and\nencourage different posting behaviors.\n"
    },
    {
        "paper_id": 2402.18119,
        "authors": "Zhenbang Feng, Hardhik Mohanty, and Bhaskar Krishnamachari",
        "title": "Modeling and Analysis of Crypto-Backed Over-Collateralized Stable\n  Derivatives in DeFi",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  In decentralized finance (DeFi), stablecoins like DAI are designed to offer a\nstable value amidst the fluctuating nature of cryptocurrencies. We examine the\nclass of crypto-backed stable derivatives, with a focus on mechanisms for price\nstabilization, which is exemplified by the well-known stablecoin DAI from\nMakerDAO. For simplicity, we focus on a single-collateral setting. We introduce\na belief parameter to the simulation model of DAI in a previous work (DAISIM),\nreflecting market sentiments about the value and stability of DAI, and show\nthat it better matches the expected behavior when this parameter is set to a\nsufficiently high value. We also propose a simple mathematical model of DAI\nprice to explain its stability and dependency on ETH price. Finally, we analyze\npossible risk factors associated with these stable derivatives to provide\nvaluable insights for stakeholders in the DeFi ecosystem.\n"
    },
    {
        "paper_id": 2402.18135,
        "authors": "Rachid Achbah (UL2 UFR SEG)",
        "title": "Manager Characteristics and SMEs' Restructuring Decisions: In-Court vs.\n  Out-of-Court Restructuring",
        "comments": null,
        "journal-ref": "Revue de l'Entrepreneuriat, 2023, 22 (3), pp.45-71",
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This study aims to empirically investigate the impact of managers'\ncharacteristics on their choice between in-court and out-of-court\nrestructuring. Based on the theory of upper echelons, we tested the preferences\nof 342 managers of financially distressed French firms regarding restructuring\ndecisions. The overall findings of this study provide empirical support for the\nupper echelons theory. Specifically, managers with a long tenure and those with\na high level of education are less likely to restructure before the court and\nare more likely to restructure privately. The findings also indicate that\nmanagers' age and gender do not significantly affect their choice between\nin-court and out-of-court restructuring. This study contributes to the\nliterature on bankruptcy and corporate restructuring by turning the focus from\nfirm characteristics to manager characteristics to explain restructuring\ndecisions.\n"
    },
    {
        "paper_id": 2402.18435,
        "authors": "Songyot Kitthamkesorn and Anthony Chen",
        "title": "Stochastic User Equilibrium Model with a Bounded Perceived Travel Time",
        "comments": "22 pages, 13 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Stochastic User Equilibrium (SUE) models depict the perception differences in\ntraffic assignment problems. According to the assumption of an unbounded\nperceived travel time distribution, the conventional SUE problems result in a\npositive choice probability for all available routes, regardless of their\nunappealing travel time. This study provides an eUnit-SUE model to relax this\nassumption. The eUnit model is derived from a bounded probability distribution.\nThis closed-form model aligns with an exponentiated random utility maximization\n(ERUM) paradigm with the exponentiated uniform distributed random error, where\nthe lower and upper bounds endogeneously determine the route usage.\nSpecifically, a Beckmann-type mathematical programming formulation is presented\nfor the eUnit-SUE problem. The equivalency and uniqueness properties are\nrigorously proven. Numerical examples reveal that the eUnit bound range between\nthe lower and upper bounds greatly affects the SUE assignment results. A larger\nbound range increases not only the number of routes in the choice set but also\nthe degree of dispersion in the assignment results due to a larger\nroute-specific perception variance. The misperception is contingent upon the\ndisparity between the shortest and longest travel times and the bounds. As the\nbound range decreases, the shortest route receives significant flow allocation,\nand the assignment result approaches the deterministic user equilibrium (DUE)\nflow pattern.\n"
    },
    {
        "paper_id": 2402.18452,
        "authors": "Fabian Dvorak and Urs Fischbacher",
        "title": "Social Learning with Intrinsic Preferences",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Despite strong evidence for peer effects, little is known about how\nindividuals balance intrinsic preferences and social learning in different\nchoice environments. Using a combination of experiments and discrete choice\nmodeling, we show that intrinsic preferences and social learning jointly\ninfluence participants' decisions, but their relative importance varies across\nchoice tasks and environments. Intrinsic preferences guide participants'\ndecisions in a subjective choice task, while social learning determines\nparticipants' decisions in a task with an objectively correct solution. A\nchoice environment in which people expect to be rewarded for their choices\nreinforces the influence of intrinsic preferences, whereas an environment in\nwhich people expect to be punished for their choices reinforces conformist\nsocial learning. We use simulations to discuss the implications of these\nfindings for the polarization of behavior.\n"
    },
    {
        "paper_id": 2402.18459,
        "authors": "Divyakant Tahlyan, Hani Mahmassani, Amanda Stathopoulos, Maher Said,\n  Susan Shaheen, Joan Walker, Breton Johnson",
        "title": "In-Person, Hybrid or Remote? Employers' Perspectives on the Future of\n  Work Post-Pandemic",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-sa/4.0/",
        "abstract": "  We present an employer-side perspective on remote work through the pandemic\nusing data from top executives of 129 employers in North America. Our analysis\nsuggests that at least some of the pandemic-accelerated changes to the work\nlocation landscape will likely stick; with some form of hybrid work being the\nnorm. However, the patterns will vary by department (HR/legal/sales/IT, etc.)\nand by sector of operations. Top three concerns among employers include:\nsupervision and mentoring, reduction in innovation, and creativity; and the top\nthree benefits include their ability to retain / recruit talent, positive\nimpact on public image and their ability to compete. An Ordered Probit model of\nthe expected April 2024 work location strategy revealed that those in\ntransportation, warehousing, and manufacturing sectors, those with a fully\nin-person approach to work pre-COVID, and those with a negative outlook towards\nthe impact of remote work are likely to be more in-person-centered, while those\nwith fully remote work approach in April 2020 are likely to be less\nin-person-centered. Lastly, we present data on resumption of business travel,\nin-person client interactions and changes in office space reconfigurations that\nemployers have made since the beginning of the pandemic.\n"
    },
    {
        "paper_id": 2402.18485,
        "authors": "Wentao Zhang, Lingxuan Zhao, Haochong Xia, Shuo Sun, Jiaze Sun, Molei\n  Qin, Xinyi Li, Yuqing Zhao, Yilei Zhao, Xinyu Cai, Longtao Zheng, Xinrun\n  Wang, Bo An",
        "title": "A Multimodal Foundation Agent for Financial Trading: Tool-Augmented,\n  Diversified, and Generalist",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Financial trading is a crucial component of the markets, informed by a\nmultimodal information landscape encompassing news, prices, and Kline charts,\nand encompasses diverse tasks such as quantitative trading and high-frequency\ntrading with various assets. While advanced AI techniques like deep learning\nand reinforcement learning are extensively utilized in finance, their\napplication in financial trading tasks often faces challenges due to inadequate\nhandling of multimodal data and limited generalizability across various tasks.\nTo address these challenges, we present FinAgent, a multimodal foundational\nagent with tool augmentation for financial trading. FinAgent's market\nintelligence module processes a diverse range of data-numerical, textual, and\nvisual-to accurately analyze the financial market. Its unique dual-level\nreflection module not only enables rapid adaptation to market dynamics but also\nincorporates a diversified memory retrieval system, enhancing the agent's\nability to learn from historical data and improve decision-making processes.\nThe agent's emphasis on reasoning for actions fosters trust in its financial\ndecisions. Moreover, FinAgent integrates established trading strategies and\nexpert insights, ensuring that its trading approaches are both data-driven and\nrooted in sound financial principles. With comprehensive experiments on 6\nfinancial datasets, including stocks and Crypto, FinAgent significantly\noutperforms 9 state-of-the-art baselines in terms of 6 financial metrics with\nover 36% average improvement on profit. Specifically, a 92.27% return (a 84.39%\nrelative improvement) is achieved on one dataset. Notably, FinAgent is the\nfirst advanced multimodal foundation agent designed for financial trading\ntasks.\n"
    },
    {
        "paper_id": 2402.18764,
        "authors": "Arsham Farzinnia and Corine Boon",
        "title": "An Analytical Approach to (Meta)Relational Models Theory, and its\n  Application to Triple Bottom Line (Profit, People, Planet) -- Towards Social\n  Relations Portfolio Management",
        "comments": "41 pages, 8 pdf figures and tables",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  Investigating the optimal nature of social interactions among generic actors\n(e.g., people or firms), aiming to achieve specifically-agreed objectives, has\nbeen the subject of extensive academic research. Using the relational models\ntheory - comprehensively describing all social interactions among actors as\ncombinations of only four forms of sociality: communal sharing, authority\nranking, equality matching, and market pricing - the common approach within the\nliterature revolves around qualitative assessments of the sociality models'\nconfigurations most effective in realizing predefined purposes, at times\nsupplemented by empirical data. In this treatment, we formulate this question\nas a mathematical optimization problem, in order to quantitatively determine\nthe best possible configurations of sociality forms between dyadic actors which\nwould optimize their mutually-agreed objectives. For this purpose, we develop\nan analytical framework for quantifying the (meta)relational models theory, and\nmathematically demonstrate that combining the four sociality forms within a\nspecific meaningful social interaction inevitably prompts an inherent tension\namong them, through a single elementary and universal metarelation. In analogy\nwith financial portfolio management, we subsequently introduce the concept of\nSocial Relations Portfolio (SRP) management, and propose a generalizable\nprocedural methodology capable of quantitatively identifying the efficient SRP\nfor any objective involving meaningful social relations. As an important\nillustration, the methodology is applied to the Triple Bottom Line paradigm to\nderive its efficient SRP, guiding practitioners in precisely measuring,\nmonitoring, reporting and (proactively) steering stakeholder management efforts\nregarding Corporate Social Responsibility (CSR) and Environmental, Social and\nGovernance (ESG) within and / or across organizations.\n"
    },
    {
        "paper_id": 2402.18872,
        "authors": "Keita Owari",
        "title": "Semistatic robust utility indifference valuation and robust integral\n  functionals",
        "comments": "20 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We consider a discrete-time robust utility maximisation with semistatic\nstrategies, and the associated indifference prices of exotic options. For this\npurpose, we introduce a robust form of convex integral functionals on the space\nof bounded continuous functions on a Polish space, and establish some key\nregularity and representation results, in the spirit of the classical\nRockafellar theorem, in terms of the duality formed with the space of Borel\nmeasures. These results (together with the standard Fenchel duality and minimax\ntheorems) yield a duality for the robust utility maximisation problem as well\nas a representation of associated indifference prices, where the presence of\nstatic positions in the primal problem appears in the dual problem as a\nmarginal constraint on the martingale measures. Consequently, the resulting\nindifference prices are consistent with the observed prices of vanilla options.\n"
    },
    {
        "paper_id": 2402.18959,
        "authors": "Zhuangwei Shi",
        "title": "MambaStock: Selective state space model for stock prediction",
        "comments": "arXiv admin note: substantial text overlap with arXiv:2204.02623",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  The stock market plays a pivotal role in economic development, yet its\nintricate volatility poses challenges for investors. Consequently, research and\naccurate predictions of stock price movements are crucial for mitigating risks.\nTraditional time series models fall short in capturing nonlinearity, leading to\nunsatisfactory stock predictions. This limitation has spurred the widespread\nadoption of neural networks for stock prediction, owing to their robust\nnonlinear generalization capabilities. Recently, Mamba, a structured state\nspace sequence model with a selection mechanism and scan module (S6), has\nemerged as a powerful tool in sequence modeling tasks. Leveraging this\nframework, this paper proposes a novel Mamba-based model for stock price\nprediction, named MambaStock. The proposed MambaStock model effectively mines\nhistorical stock market data to predict future stock prices without handcrafted\nfeatures or extensive preprocessing procedures. Empirical studies on several\nstocks indicate that the MambaStock model outperforms previous methods,\ndelivering highly accurate predictions. This enhanced accuracy can assist\ninvestors and institutions in making informed decisions, aiming to maximize\nreturns while minimizing risks. This work underscores the value of Mamba in\ntime-series forecasting. Source code is available at\nhttps://github.com/zshicode/MambaStock.\n"
    },
    {
        "paper_id": 2402.19203,
        "authors": "Aur\\'elien Alfonsi and Guillaume Szulda",
        "title": "On non-negative solutions of stochastic Volterra equations with jumps\n  and non-Lipschitz coefficients",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We consider one-dimensional stochastic Volterra equations with jumps for\nwhich we establish conditions upon the convolution kernel and coefficients for\nthe strong existence and pathwise uniqueness of a non-negative c\\`adl\\`ag\nsolution. By using the approach recently developed in arXiv:2302.07758, we show\nthe strong existence by using a nonnegative approximation of the equation whose\nconvergence is proved via a variant of the Yamada--Watanabe approximation\ntechnique. We apply our results to L\\'evy-driven stochastic Volterra equations.\nIn particular, we are able to define a Volterra extension of the so-called\nalpha-stable Cox--Ingersoll--Ross process, which is especially used for\napplications in Mathematical Finance.\n"
    },
    {
        "paper_id": 2402.1938,
        "authors": "Adeline Gu\\'eret, Wolf-Peter Schill, Carlos Gaete-Morales",
        "title": "Not flexible enough? Impacts of electric carsharing on a power sector\n  with variable renewables",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Electrifying the car fleet is a major strategy for mitigating greenhouse gas\nemissions in the transport sector. However, electrification alone will not\nsolve all the negative externalities associated with cars. In light of other\nproblems such as street space as well as concerns about the use of mineral\nresources for battery electric cars, reducing the car fleet size would be\nbeneficial, particularly in cities. Carsharing could offer a way to reconcile\ncurrent car usage habits with a reduction in the car fleet size. However, it\ncould also reduce the potential of electric cars to align their grid\ninteractions with variable renewable electricity generation. We investigate how\nelectric carsharing may impact the power sector in the future. We combine three\nopen-source quantitative methods, including sequence clustering of car travel\ndiaries, a probabilistic tool to generate synthetic electric vehicle time\nseries, and an optimization model of the power sector. For 2030 scenarios of\nGermany with a renewable share of at least 80%, we show that switching to\nelectric carsharing only moderately increases power sector costs. In our main\nsetting, carsharing increases yearly power sector costs by less than 100 euros\nper substituted private electric car. This cost effect is largest under the\nassumption of bidirectional charging. It is mitigated when other sources of\nflexibility for the power sector are considered. Carsharing further causes a\nshift from wind power to solar PV in the optimal capacity mix, and may also\ntrigger additional investments in stationary electricity storage. Overall, we\nfind that shared electric cars still have the potential to be operated largely\nin line with variable renewable electricity generation. We conclude that\nelectric carsharing is unlikely to cause much damage to the power sector, but\ncould bring various other benefits, which may outweigh power sector cost\nincreases.\n"
    },
    {
        "paper_id": 2402.19399,
        "authors": "Vahidin Jeleskovic",
        "title": "An Empirical Analysis of Scam Tokens on Ethereum Blockchain",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  This article presents an empirical investigation into the determinants of\ntotal revenue generated by counterfeit tokens on Uniswap. It offers a detailed\noverview of the counterfeit token fraud process, along with a systematic\nsummary of characteristics associated with such fraudulent activities observed\nin Uniswap. The study primarily examines the relationship between revenue from\ncounterfeit token scams and their defining characteristics, and analyzes the\ninfluence of market economic factors such as return on market capitalization\nand price return on Ethereum. Key findings include a significant increase in\noverall transactions of counterfeit tokens on their first day of fraud, and a\nrise in upfront fraud costs leading to corresponding increases in revenue.\nFurthermore, a negative correlation is identified between the total revenue of\ncounterfeit tokens and the volatility of Ethereum market capitalization return,\nwhile price return volatility on Ethereum is found to have a positive impact on\ncounterfeit token revenue, albeit requiring further investigation for a\ncomprehensive understanding. Additionally, the number of subscribers for the\nreal token correlates positively with the realized volume of scam tokens,\nindicating that a larger community following the legitimate token may\ninadvertently contribute to the visibility and success of counterfeit tokens.\nConversely, the number of Telegram subscribers exhibits a negative impact on\nthe realized volume of scam tokens, suggesting that a higher level of scrutiny\nor awareness within Telegram communities may act as a deterrent to fraudulent\nactivities. Finally, the timing of when the scam token is introduced on the\nEthereum blockchain may have a negative impact on its success. Notably, the\ncumulative amount scammed by only 42 counterfeit tokens amounted to almost\n11214 Ether.\n"
    },
    {
        "paper_id": 2402.19421,
        "authors": "Lijia Ma, Xingchen Xu, Yong Tan",
        "title": "Crafting Knowledge: Exploring the Creative Mechanisms of Chat-Based\n  Search Engines",
        "comments": "38 pages, 2 figures, 7 tables",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In the domain of digital information dissemination, search engines act as\npivotal conduits linking information seekers with providers. The advent of\nchat-based search engines utilizing Large Language Models (LLMs) and Retrieval\nAugmented Generation (RAG), exemplified by Bing Chat, marks an evolutionary\nleap in the search ecosystem. They demonstrate metacognitive abilities in\ninterpreting web information and crafting responses with human-like\nunderstanding and creativity. Nonetheless, the intricate nature of LLMs renders\ntheir \"cognitive\" processes opaque, challenging even their designers'\nunderstanding. This research aims to dissect the mechanisms through which an\nLLM-powered chat-based search engine, specifically Bing Chat, selects\ninformation sources for its responses. To this end, an extensive dataset has\nbeen compiled through engagements with New Bing, documenting the websites it\ncites alongside those listed by the conventional search engine. Employing\nnatural language processing (NLP) techniques, the research reveals that Bing\nChat exhibits a preference for content that is not only readable and formally\nstructured, but also demonstrates lower perplexity levels, indicating a unique\ninclination towards text that is predictable by the underlying LLM. Further\nenriching our analysis, we procure an additional dataset through interactions\nwith the GPT-4 based knowledge retrieval API, unveiling a congruent text\npreference between the RAG API and Bing Chat. This consensus suggests that\nthese text preferences intrinsically emerge from the underlying language\nmodels, rather than being explicitly crafted by Bing Chat's developers.\nMoreover, our investigation documents a greater similarity among websites cited\nby RAG technologies compared to those ranked highest by conventional search\nengines.\n"
    },
    {
        "paper_id": 2403.00006,
        "authors": "Sara Ana Solanilla Blanco",
        "title": "Local sensitivity analysis of heating degree day and cooling degree day\n  temperature derivatives prices",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
        "abstract": "  We study the local sensitivity of heating degree day (HDD) and cooling degree\nday (CDD) temperature futures and option prices with respect to perturbations\nin the deseasonalized temperature or in one of its derivatives up to a certain\norder determined by the continuous-time autoregressive process modelling the\ndeseasonalized temperature in the HDD and CDD indexes. We also consider an\nempirical case where a CAR process of autoregressive order 3 is fitted to New\nYork temperatures and we perform a study of the local sensitivity of these\nfinancial contracts and a posterior analysis of the results.\n"
    },
    {
        "paper_id": 2403.00009,
        "authors": "Cyril Bachelard and Apostolos Chalkis and Vissarion Fisikopoulos and\n  Elias Tsigaridas",
        "title": "Randomized Control in Performance Analysis and Empirical Asset Pricing",
        "comments": "57 pages, 7 figures, 2 tables",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  The present article explores the application of randomized control techniques\nin empirical asset pricing and performance evaluation. It introduces geometric\nrandom walks, a class of Markov chain Monte Carlo methods, to construct\nflexible control groups in the form of random portfolios adhering to investor\nconstraints. The sampling-based methods enable an exploration of the\nrelationship between academically studied factor premia and performance in a\npractical setting. In an empirical application, the study assesses the\npotential to capture premias associated with size, value, quality, and momentum\nwithin a strongly constrained setup, exemplified by the investor guidelines of\nthe MSCI Diversified Multifactor index. Additionally, the article highlights\nissues with the more traditional use case of random portfolios for drawing\ninferences in performance evaluation, showcasing challenges related to the\nintricacies of high-dimensional geometry.\n"
    },
    {
        "paper_id": 2403.00139,
        "authors": "Tim Leung, Matthew Lorig, Yoshihiro Shirai",
        "title": "Optimal positioning in derivative securities in incomplete markets",
        "comments": "22 pages, 3 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper analyzes a problem of optimal static hedging using derivatives in\nincomplete markets. The investor is assumed to have a risk exposure to two\nunderlying assets. The hedging instruments are vanilla options written on a\nsingle underlying asset. The hedging problem is formulated as a utility\nmaximization problem whereby the form of the optimal static hedge is\ndetermined. Among our results, a semi-analytical solution for the optimizer is\nfound through variational methods for exponential, power/logarithmic, and\nquadratic utility. When vanilla options are available for each underlying\nasset, the optimal solution is related to the fixed points of a Lipschitz map.\nIn the case of exponential utility, there is only one such fixed point, and\nsubsequent iterations of the map converge to it.\n"
    },
    {
        "paper_id": 2403.00273,
        "authors": "Iv\\'an Belenky",
        "title": "ARED: Argentina Real Estate Dataset",
        "comments": "3 pages, 6 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-sa/4.0/",
        "abstract": "  The Argentinian real estate market presents a unique case study characterized\nby its unstable and rapidly shifting macroeconomic circumstances over the past\ndecades. Despite the existence of a few datasets for price prediction, there is\na lack of mixed modality datasets specifically focused on Argentina. In this\npaper, the first edition of ARED is introduced. A comprehensive real estate\nprice prediction dataset series, designed for the Argentinian market. This\nedition contains information solely for Jan-Feb 2024. It was found that despite\nthe short time range captured by this zeroth edition (44 days), time dependent\nphenomena has been occurring mostly on a market level (market as a whole).\nNevertheless future editions of this dataset, will most likely contain\nhistorical data. Each listing in ARED comprises descriptive features, and\nvariable-length sets of images.\n"
    },
    {
        "paper_id": 2403.00471,
        "authors": "Matthias H\\\"ansel",
        "title": "Idiosyncratic Risk, Government Debt and Inflation",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  How does public debt matter for price stability? If it is useful for the\nprivate sector to insure idiosyncratic risk, government debt expansions can\nincrease the natural rate of interest and create inflation. As I demonstrate\nusing a tractable model, this holds in the presence of an active Taylor rule\nand does not require the absence of future fiscal consolidation. Further\nanalysis using a full-blown 2-asset HANK model reveals the quantitative\nmagnitude of the mechanism to crucially depend on the structure of the asset\nmarket: under standard assumptions, the effect of public debt on the natural\nrate is either overly strong or overly weak. Employing a parsimonious way to\novercome this issue, my framework suggests relevant effects of public debt on\ninflation under active monetary policy: In particular, persistently elevated\npublic debt may make it harder to go the last \"mile of disinflation\" unless\ncentral banks explicitly take its effect on the neutral rate into account.\n"
    },
    {
        "paper_id": 2403.00474,
        "authors": "Peng Yifeng",
        "title": "Volatility-based strategy on Chinese equity index ETF options",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  This study examines the performance of a volatility-based strategy using\nChinese equity index ETF options. Initially successful, the strategy's\neffectiveness waned post-2018. By integrating GARCH models for volatility\nforecasting, the strategy's positions and exposures are dynamically adjusted.\nThe results indicate that such an approach can enhance returns in volatile\nmarkets, suggesting potential for refined trading strategies in China's\nevolving derivatives landscape. The research underscores the importance of\nadaptive strategies in capturing market opportunities amidst changing trading\ndynamics.\n"
    },
    {
        "paper_id": 2403.00523,
        "authors": "Hugo Schnoering, Pierre Porthaux, Michalis Vazirgiannis",
        "title": "Assessing the Efficacy of Heuristic-Based Address Clustering for Bitcoin",
        "comments": "20 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
        "abstract": "  Exploring transactions within the Bitcoin blockchain entails examining the\ntransfer of bitcoins among several hundred million entities. However, it is\noften impractical and resource-consuming to study such a vast number of\nentities. Consequently, entity clustering serves as an initial step in most\nanalytical studies. This process often employs heuristics grounded in the\npractices and behaviors of these entities. In this research, we delve into the\nexamination of two widely used heuristics, alongside the introduction of four\nnovel ones. Our contribution includes the introduction of the\n\\textit{clustering ratio}, a metric designed to quantify the reduction in the\nnumber of entities achieved by a given heuristic. The assessment of this\nreduction ratio plays an important role in justifying the selection of a\nspecific heuristic for analytical purposes. Given the dynamic nature of the\nBitcoin system, characterized by a continuous increase in the number of\nentities on the blockchain, and the evolving behaviors of these entities, we\nextend our study to explore the temporal evolution of the clustering ratio for\neach heuristic. This temporal analysis enhances our understanding of the\neffectiveness of these heuristics over time.\n"
    },
    {
        "paper_id": 2403.00653,
        "authors": "Faustino Prieto, Catalina B. Garc\\'ia-Garc\\'ia, Rom\\'an Salmer\\'on\n  G\\'omez",
        "title": "Modelling Global Fossil CO2 Emissions with a Lognormal Distribution: A\n  Climate Policy Tool",
        "comments": "28 pages, 6 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Carbon dioxide (CO2) emissions have emerged as a critical issue with profound\nimpacts on the environment, human health, and the global economy. The steady\nincrease in atmospheric CO2 levels, largely due to human activities such as\nburning fossil fuels and deforestation, has become a major contributor to\nclimate change and its associated catastrophic effects. To tackle this pressing\nchallenge, a coordinated global effort is needed, which necessitates a deep\nunderstanding of emissions patterns and trends. In this paper, we explore the\nuse of statistical modelling, specifically the lognormal distribution, as a\nframework for comprehending and predicting CO2 emissions. We build on prior\nresearch that suggests a complex distribution of emissions and seek to test the\nhypothesis that a simpler distribution can still offer meaningful insights for\npolicy-makers. We utilize data from three comprehensive databases and analyse\nsix candidate distributions (exponential, Fisk, gamma, lognormal, Lomax,\nWeibull) to identify a suitable model for global fossil CO2 emissions. Our\nfindings highlight the adequacy of the lognormal distribution in characterizing\nemissions across all countries and years studied. Furthermore, to provide\nadditional support for this distribution, we provide statistical evidence\nsupporting the applicability of Gibrat's law to those CO2 emissions. Finally,\nwe employ the lognormal model to predict emission parameters for the coming\nyears and propose two policies for reducing total fossil CO2 emissions. Our\nresearch aims to provide policy-makers with accurate and detailed information\nto support effective climate change mitigation strategies.\n"
    },
    {
        "paper_id": 2403.00707,
        "authors": "Adele Ravagnani, Fabrizio Lillo, Paola Deriu, Piero Mazzarisi,\n  Francesca Medda, Antonio Russo",
        "title": "Dimensionality reduction techniques to support insider trading detection",
        "comments": "This work has been submitted to the IEEE for possible publication.\n  Copyright may be transferred without notice, after which this version may no\n  longer be accessible",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Identification of market abuse is an extremely complicated activity that\nrequires the analysis of large and complex datasets. We propose an unsupervised\nmachine learning method for contextual anomaly detection, which allows to\nsupport market surveillance aimed at identifying potential insider trading\nactivities. This method lies in the reconstruction-based paradigm and employs\nprincipal component analysis and autoencoders as dimensionality reduction\ntechniques. The only input of this method is the trading position of each\ninvestor active on the asset for which we have a price sensitive event (PSE).\nAfter determining reconstruction errors related to the trading profiles,\nseveral conditions are imposed in order to identify investors whose behavior\ncould be suspicious of insider trading related to the PSE. As a case study, we\napply our method to investor resolved data of Italian stocks around takeover\nbids.\n"
    },
    {
        "paper_id": 2403.00746,
        "authors": "Antonis Papapantoleon and Jasper Rou",
        "title": "A time-stepping deep gradient flow method for option pricing in (rough)\n  diffusion models",
        "comments": "18 pages, 10 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  We develop a novel deep learning approach for pricing European options in\ndiffusion models, that can efficiently handle high-dimensional problems\nresulting from Markovian approximations of rough volatility models. The option\npricing partial differential equation is reformulated as an energy minimization\nproblem, which is approximated in a time-stepping fashion by deep artificial\nneural networks. The proposed scheme respects the asymptotic behavior of option\nprices for large levels of moneyness, and adheres to a priori known bounds for\noption prices. The accuracy and efficiency of the proposed method is assessed\nin a series of numerical examples, with particular focus in the lifted Heston\nmodel.\n"
    },
    {
        "paper_id": 2403.0077,
        "authors": "Juan C. King and Roberto Dale and Jos\\'e M. Amig\\'o",
        "title": "Blockchain Metrics and Indicators in Cryptocurrency Trading",
        "comments": "26 pages; 14 figures",
        "journal-ref": "Solitons & Fractals, 178, 114305 (2024)",
        "doi": "10.1016/j.chaos.2023.114305",
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  The objective of this paper is the construction of new indicators that can be\nuseful to operate in the cryptocurrency market. These indicators are based on\npublic data obtained from the blockchain network, specifically from the nodes\nthat make up Bitcoin mining. Therefore, our analysis is unique to that network.\nThe results obtained with numerical simulations of algorithmic trading and\nprediction via statistical models and Machine Learning demonstrate the\nimportance of variables such as the hash rate, the difficulty of mining or the\ncost per transaction when it comes to trade Bitcoin assets or predict the\ndirection of price. Variables obtained from the blockchain network will be\ncalled here blockchain metrics. The corresponding indicators (inspired by the\n\"Hash Ribbon\") perform well in locating buy signals. From our results, we\nconclude that such blockchain indicators allow obtaining information with a\nstatistical advantage in the highly volatile cryptocurrency market.\n"
    },
    {
        "paper_id": 2403.00772,
        "authors": "Ziyuan Ma, Conor Ryan, Jim Buckley, and Muslim Chochlov",
        "title": "Do Weibo platform experts perform better at predicting stock market?",
        "comments": null,
        "journal-ref": "2021, 22nd Engineering Applications of Neural Networks Conference\n  (EANN 2021)",
        "doi": "10.1007/978-3-030-80568-5_40",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Sentiment analysis can be used for stock market prediction. However, existing\nresearch has not studied the impact of a user's financial background on\nsentiment-based forecasting of the stock market using artificial neural\nnetworks. In this work, a novel combination of neural networks is used for the\nassessment of sentiment-based stock market prediction, based on the financial\nbackground of the population that generated the sentiment. The state-of-the-art\nlanguage processing model Bidirectional Encoder Representations from\nTransformers (BERT) is used to classify the sentiment and a Long-Short Term\nMemory (LSTM) model is used for time-series based stock market prediction. For\nevaluation, the Weibo social networking platform is used as a sentiment data\ncollection source. Weibo users (and their comments respectively) are divided\ninto Authorized Financial Advisor (AFA) and Unauthorized Financial Advisor\n(UFA) groups according to their background information, as collected by Weibo.\nThe Hong Kong Hang Seng index is used to extract historical stock market change\ndata. The results indicate that stock market prediction learned from the AFA\ngroup users is 39.67% more precise than that learned from the UFA group users\nand shows the highest accuracy (87%) when compared to existing approaches.\n"
    },
    {
        "paper_id": 2403.00774,
        "authors": "Vasilii Chsherbakov, Ilia Karpov",
        "title": "Regional inflation analysis using social network data",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Inflation is one of the most important macroeconomic indicators that have a\ngreat impact on the population of any country and region. Inflation is\ninfluenced by range of factors, one of which is inflation expectations. Many\ncentral banks take this factor into consideration while implementing monetary\npolicy within the inflation targeting regime. Nowadays, a lot of people are\nactive users of the Internet, especially social networks. There is a hypothesis\nthat people search, read, and discuss mainly only those issues that are of\nparticular interest to them. It is logical to assume that the dynamics of\nprices may also be in the focus of user discussions. So, such discussions could\nbe regarded as an alternative source of more rapid information about inflation\nexpectations. This study is based on unstructured data from Vkontakte social\nnetwork to analyze upward and downward inflationary trends (on the example of\nthe Omsk region). The sample of more than 8.5 million posts was collected\nbetween January 2010 and May 2022. The authors used BERT neural networks to\nsolve the problem. These models demonstrated better results than the benchmarks\n(e.g., logistic regression, decision tree classifier, etc.). It makes possible\nto define pro-inflationary and disinflationary types of keywords in different\ncontexts and get their visualization with SHAP method. This analysis provides\nadditional operational information about inflationary processes at the regional\nlevel The proposed approach can be scaled for other regions. At the same time\nthe limitation of the work is the time and power costs for the initial training\nof similar models for all regions of Russia.\n"
    },
    {
        "paper_id": 2403.00775,
        "authors": "Alessandro Niro and Michael Werner",
        "title": "Detecting Anomalous Events in Object-centric Business Processes via\n  Graph Neural Networks",
        "comments": "12 pages, 2 figures, to appear in the ICPM 2023 Workshops Proceedings",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  Detecting anomalies is important for identifying inefficiencies, errors, or\nfraud in business processes. Traditional process mining approaches focus on\nanalyzing 'flattened', sequential, event logs based on a single case notion.\nHowever, many real-world process executions exhibit a graph-like structure,\nwhere events can be associated with multiple cases. Flattening event logs\nrequires selecting a single case identifier which creates a gap with the real\nevent data and artificially introduces anomalies in the event logs.\nObject-centric process mining avoids these limitations by allowing events to be\nrelated to different cases. This study proposes a novel framework for anomaly\ndetection in business processes that exploits graph neural networks and the\nenhanced information offered by object-centric process mining. We first\nreconstruct and represent the process dependencies of the object-centric event\nlogs as attributed graphs and then employ a graph convolutional autoencoder\narchitecture to detect anomalous events. Our results show that our approach\nprovides promising performance in detecting anomalies at the activity type and\nattributes level, although it struggles to detect anomalies in the temporal\norder of events.\n"
    },
    {
        "paper_id": 2403.00777,
        "authors": "Ahmed N. Bakry, Almohammady S. Alsharkawy, Mohamed S. Farag, and Kamal\n  R. Raslan",
        "title": "Combating Financial Crimes with Unsupervised Learning Techniques:\n  Clustering and Dimensionality Reduction for Anti-Money Laundering",
        "comments": null,
        "journal-ref": null,
        "doi": "10.58675/2636-3305.1664",
        "license": "http://creativecommons.org/licenses/by-sa/4.0/",
        "abstract": "  Anti-Money Laundering (AML) is a crucial task in ensuring the integrity of\nfinancial systems. One keychallenge in AML is identifying high-risk groups\nbased on their behavior. Unsupervised learning, particularly clustering, is a\npromising solution for this task. However, the use of hundreds of features\ntodescribe behavior results in a highdimensional dataset that negatively\nimpacts clustering performance.In this paper, we investigate the effectiveness\nof combining clustering method agglomerative hierarchicalclustering with four\ndimensionality reduction techniques -Independent Component Analysis (ICA),\nandKernel Principal Component Analysis (KPCA), Singular Value Decomposition\n(SVD), Locality Preserving Projections (LPP)- to overcome the issue of\nhigh-dimensionality in AML data and improve clusteringresults. This study aims\nto provide insights into the most effective way of reducing the dimensionality\nofAML data and enhance the accuracy of clustering-based AML systems. The\nexperimental results demonstrate that KPCA outperforms other dimension\nreduction techniques when combined with agglomerativehierarchical clustering.\nThis superiority is observed in the majority of situations, as confirmed by\nthreedistinct validation indices.\n"
    },
    {
        "paper_id": 2403.00782,
        "authors": "Hanshuang Tong, Jun Li, Ning Wu, Ming Gong, Dongmei Zhang, Qi Zhang",
        "title": "Ploutos: Towards interpretable stock movement prediction with financial\n  large language model",
        "comments": "8 pages, 4 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Recent advancements in large language models (LLMs) have opened new pathways\nfor many domains. However, the full potential of LLMs in financial investments\nremains largely untapped. There are two main challenges for typical deep\nlearning-based methods for quantitative finance. First, they struggle to fuse\ntextual and numerical information flexibly for stock movement prediction.\nSecond, traditional methods lack clarity and interpretability, which impedes\ntheir application in scenarios where the justification for predictions is\nessential. To solve the above challenges, we propose Ploutos, a novel financial\nLLM framework that consists of PloutosGen and PloutosGPT. The PloutosGen\ncontains multiple primary experts that can analyze different modal data, such\nas text and numbers, and provide quantitative strategies from different\nperspectives. Then PloutosGPT combines their insights and predictions and\ngenerates interpretable rationales. To generate accurate and faithful\nrationales, the training strategy of PloutosGPT leverage rearview-mirror\nprompting mechanism to guide GPT-4 to generate rationales, and a dynamic token\nweighting mechanism to finetune LLM by increasing key tokens weight. Extensive\nexperiments show our framework outperforms the state-of-the-art methods on both\nprediction accuracy and interpretability.\n"
    },
    {
        "paper_id": 2403.00785,
        "authors": "Oluwafemi F Olaiyapo",
        "title": "Applying News and Media Sentiment Analysis for Generating Forex Trading\n  Signals",
        "comments": null,
        "journal-ref": "Review of Business and Economics Studies. 2023;11(4):84-94",
        "doi": "10.26794/2308-944X-2023-11-4-84-94",
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  The objective of this research is to examine how sentiment analysis can be\nemployed to generate trading signals for the Foreign Exchange (Forex) market.\nThe author assessed sentiment in social media posts and news articles\npertaining to the United States Dollar (USD) using a combination of methods:\nlexicon-based analysis and the Naive Bayes machine learning algorithm. The\nfindings indicate that sentiment analysis proves valuable in forecasting market\nmovements and devising trading signals. Notably, its effectiveness is\nconsistent across different market conditions. The author concludes that by\nanalyzing sentiment expressed in news and social media, traders can glean\ninsights into prevailing market sentiments towards the USD and other pertinent\ncountries, thereby aiding trading decision-making. This study underscores the\nimportance of weaving sentiment analysis into trading strategies as a pivotal\ntool for predicting market dynamics.\n"
    },
    {
        "paper_id": 2403.00796,
        "authors": "Narayan Tondapu",
        "title": "Enhancing Mean-Reverting Time Series Prediction with Gaussian Processes:\n  Functional and Augmented Data Structures in Financial Forecasting",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-sa/4.0/",
        "abstract": "  In this paper, we explore the application of Gaussian Processes (GPs) for\npredicting mean-reverting time series with an underlying structure, using\nrelatively unexplored functional and augmented data structures. While many\nconventional forecasting methods concentrate on the short-term dynamics of time\nseries data, GPs offer the potential to forecast not just the average\nprediction but the entire probability distribution over a future trajectory.\nThis is particularly beneficial in financial contexts, where accurate\npredictions alone may not suffice if incorrect volatility assessments lead to\ncapital losses. Moreover, in trade selection, GPs allow for the forecasting of\nmultiple Sharpe ratios adjusted for transaction costs, aiding in\ndecision-making. The functional data representation utilized in this study\nenables longer-term predictions by leveraging information from previous years,\neven as the forecast moves away from the current year's training data.\nAdditionally, the augmented representation enriches the training set by\nincorporating multiple targets for future points in time, facilitating\nlong-term predictions. Our implementation closely aligns with the methodology\noutlined in, which assessed effectiveness on commodity futures. However, our\ntesting methodology differs. Instead of real data, we employ simulated data\nwith similar characteristics. We construct a testing environment to evaluate\nboth data representations and models under conditions of increasing noise, fat\ntails, and inappropriate kernels-conditions commonly encountered in practice.\nBy simulating data, we can compare our forecast distribution over time against\na full simulation of the actual distribution of our test set, thereby reducing\nthe inherent uncertainty in testing time series models on real data. We enable\nfeature prediction through augmentation and employ sub-sampling to ensure the\nfeasibility of GPs.\n"
    },
    {
        "paper_id": 2403.00819,
        "authors": "Markus Bibinger, Nikolaus Hautsch and Alexander Ristig",
        "title": "Jump detection in high-frequency order prices",
        "comments": "33 pages, 6 figures, 5 tables",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We propose methods to infer jumps of a semi-martingale, which describes\nlong-term price dynamics based on discrete, noisy, high-frequency observations.\nDifferent to the classical model of additive, centered market microstructure\nnoise, we consider one-sided microstructure noise for order prices in a limit\norder book. We develop methods to estimate, locate and test for jumps using\nlocal order statistics. We provide a local test and show that we can\nconsistently estimate price jumps. The main contribution is a global test for\njumps. We establish the asymptotic properties and optimality of this test. We\nderive the asymptotic distribution of a maximum statistic under the null\nhypothesis of no jumps based on extreme value theory. We prove consistency\nunder the alternative hypothesis. The rate of convergence for local\nalternatives is determined and shown to be much faster than optimal rates for\nthe standard market microstructure noise model. This allows the identification\nof smaller jumps. In the process, we establish uniform consistency for spot\nvolatility estimation under one-sided microstructure noise. A simulation study\nsheds light on the finite-sample implementation and properties of our new\nstatistics and draws a comparison to a popular method for market microstructure\nnoise. We showcase how our new approach helps to improve jump detection in an\nempirical analysis of intra-daily limit order book data.\n"
    },
    {
        "paper_id": 2403.01012,
        "authors": "Hanchao Liu, Dena Firoozi",
        "title": "Hilbert Space-Valued LQ Mean Field Games: An Infinite-Dimensional\n  Analysis",
        "comments": "29 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper presents a comprehensive study of linear-quadratic (LQ) mean field\ngames (MFGs) in Hilbert spaces, generalizing the classic LQ MFG theory to\nscenarios involving $N$ agents with dynamics governed by infinite-dimensional\nstochastic equations. In this framework, both state and control processes of\neach agent take values in separable Hilbert spaces. All agents are coupled\nthrough the average state of the population which appears in their linear\ndynamics and quadratic cost functional. Specifically, the dynamics of each\nagent incorporates an infinite-dimensional noise, namely a $Q$-Wiener process,\nand an unbounded operator. The diffusion coefficient of each agent is\nstochastic involving the state, control, and average state processes. We first\nstudy the well-posedness of a system of $N$ coupled semilinear\ninfinite-dimensional stochastic evolution equations establishing the foundation\nof MFGs in Hilbert spaces. We then specialize to $N$-player LQ games described\nabove and study the asymptotic behaviour as the number of agents, $N$,\napproaches infinity. We develop an infinite-dimensional variant of the Nash\nCertainty Equivalence principle and characterize a unique Nash equilibrium for\nthe limiting MFG. Finally, we study the connections between the $N$-player game\nand the limiting MFG, demonstrating that the empirical average state converges\nto the mean field and that the resulting limiting best-response strategies form\nan $\\epsilon$-Nash equilibrium for the $N$-player game in Hilbert spaces.\n"
    },
    {
        "paper_id": 2403.01088,
        "authors": "Hayden Brown",
        "title": "Justifying the Volatility of S&P 500 Daily Returns",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Over the past 60 years, there has been a gradual increase in the volatility\nof daily returns for the S&P 500 Index. Hypothetically, suppose that market\nforces determine daily volatility such that a daily leveraged S&P 500 fund\ncannot outperform a standard S&P 500 fund in the long run. Then this\nhypothetical volatility happens to support the increase in volatility seen in\nthe S&P 500 index. On this basis, it appears that the classic argument of the\nmarket portfolio being unbeatable in the long run is determining the volatility\nof S&P 500 daily returns. Moreover, it follows that the long-term volatility of\nthe daily returns for the S&P 500 Index should continue to increase until\npassing a particular threshold. If, on the other hand, this hypothesis about\nmarket forces increasing volatility is invalid, then there is room for daily\nleveraged S&P 500 funds to outperform their unleveraged counterparts in the\nlong run.\n"
    },
    {
        "paper_id": 2403.0136,
        "authors": "Shutter Zor",
        "title": "\"Digitwashing\": The Gap between Words and Deeds in Digital\n  Transformation and Stock Price Crash Risk",
        "comments": "34 pages, 4 figures, 8 tables",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The contrast between companies' \"fleshy\" promises and the \"skeletal\"\nperformance in digital transformation may lead to a higher risk of stock price\ncrash. This paper selects a sample of Shanghai and Shenzhen A-share listed\ncompanies from 2010 to 2021, empirically analyses the specific impact of the\ngap between words and deeds in digital transformation (GDT) on the stock price\ncrash risk, and explores the possible causes of GDT. We found that GDT\nsignificantly increases the stock price crash risk, and this finding is still\nvalid after a series of robustness tests. In a further study, a deeper\nexamination of the causes of GDT reveals that firms' perceptions of economic\npolicy uncertainty significantly increase GDT, and the effect is more\npronounced in the sample of loss-making firms. At the same time, the results of\nthe heterogeneity test suggest that investors are more tolerant of state-owned\nenterprises when they are in the GDT situation. Taken together, we provide a\nconcrete bridge between the two measures of digital transformation - digital\ntext frequency and digital technology share - and offer new insights to enhance\ncapital market stability.\n"
    },
    {
        "paper_id": 2403.01361,
        "authors": "Joon Suk Huh, Ellen Vitercik, Kirthevasan Kandasamy",
        "title": "Bandit Profit-maximization for Targeted Marketing",
        "comments": "The Twenty-Fifth ACM Conference on Economics and Computation (EC'24)",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  We study a sequential profit-maximization problem, optimizing for both price\nand ancillary variables like marketing expenditures. Specifically, we aim to\nmaximize profit over an arbitrary sequence of multiple demand curves, each\ndependent on a distinct ancillary variable, but sharing the same price. A\nprototypical example is targeted marketing, where a firm (seller) wishes to\nsell a product over multiple markets. The firm may invest different marketing\nexpenditures for different markets to optimize customer acquisition, but must\nmaintain the same price across all markets. Moreover, markets may have\nheterogeneous demand curves, each responding to prices and marketing\nexpenditures differently. The firm's objective is to maximize its gross profit,\nthe total revenue minus marketing costs.\n  Our results are near-optimal algorithms for this class of problems in an\nadversarial bandit setting, where demand curves are arbitrary non-adaptive\nsequences, and the firm observes only noisy evaluations of chosen points on the\ndemand curves. For $n$ demand curves (markets), we prove a regret upper bound\nof $\\tilde{O}(nT^{3/4})$ and a lower bound of $\\Omega((nT)^{3/4})$ for\nmonotonic demand curves, and a regret bound of $\\tilde{\\Theta}(nT^{2/3})$ for\ndemands curves that are monotonic in price and concave in the ancillary\nvariables.\n"
    },
    {
        "paper_id": 2403.01468,
        "authors": "Yuliya Mishura, Kostiantyn Ralchenko, Petro Zelenko, Volodymyr\n  Zubchenko",
        "title": "Properties of the entropic risk measure EVaR in relation to selected\n  distributions",
        "comments": "16 pages, 7 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Entropic Value-at-Risk (EVaR) measure is a convenient coherent risk measure.\nDue to certain difficulties in finding its analytical representation, it was\npreviously calculated explicitly only for the normal distribution. We succeeded\nto overcome these difficulties and to calculate Entropic Value-at-Risk (EVaR)\nmeasure for Poisson, compound Poisson, Gamma, Laplace, exponential,\nchi-squared, inverse Gaussian distribution and normal inverse Gaussian\ndistribution with the help of Lambert function that is a special function,\ngenerally speaking, with two branches.\n"
    },
    {
        "paper_id": 2403.01745,
        "authors": "Han-Yu Zhu, Peng-Fei Dai, Wei-Xing Zhou",
        "title": "Uncovering the Sino-US dynamic risk spillovers effects: Evidence from\n  agricultural futures markets",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Agricultural products play a critical role in human development. With\neconomic globalization and the financialization of agricultural products\ncontinuing to advance, the interconnections between different agricultural\nfutures have become closer. We utilize a TVP-VAR-DY model combined with the\nquantile method to measure the risk spillover between 11 agricultural futures\non the futures exchanges of US and China from July 9,2014, to December 31,2022.\nThis study yielded several significant findings. Firstly, CBOT corn, soybean,\nand wheat were identified as the primary risk transmitters, with DCE corn and\nsoybean as the main risk receivers. Secondly, sudden events or increased\neconomic uncertainty can increase the overall risk spillovers. Thirdly, there\nis an aggregation of risk spillovers amongst agricultural futures based on the\ndynamic directional spillover results. Lastly, the central agricultural futures\nunder the conditional mean are CBOT corn and soybean, while CZCE hard wheat and\nlong-grained rice are the two risk spillover centers in extreme cases, as per\nthe results of the spillover network and minimum spanning tree. Based on these\nresults, decision-makers are advised to safeguard against the price risk of\nagricultural futures under sudden economic events, and investors can utilize\nthe results to construct a superior investment portfolio by taking different\nagricultural product futures as risk-leading indicators according to various\nsituations.\n"
    },
    {
        "paper_id": 2403.0177,
        "authors": "Voraprapa Nakavachara, Tanapong Potipiti, Thanee Chaiwat",
        "title": "Experimenting with Generative AI: Does ChatGPT Really Increase\n  Everyone's Productivity?",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Generative AI technologies such as ChatGPT, Gemini, and MidJourney have made\nremarkable progress in recent years. Recent literature has documented ChatGPT's\npositive impact on productivity in areas where it has strong expertise,\nattributable to extensive training datasets, such as the English language and\nPython/SQL programming. However, there is still limited literature regarding\nChatGPT's performance in areas where its capabilities could still be further\nenhanced. This paper aims to fill this gap. We conducted an experiment in which\neconomics students were asked to perform writing analysis tasks in a\nnon-English language (specifically, Thai) and math & data analysis tasks using\na less frequently used programming package (specifically, Stata). The findings\nsuggest that, on average, participants performed better using ChatGPT in terms\nof scores and time taken to complete the tasks. However, a detailed examination\nreveals that 34% of participants saw no improvement in writing analysis tasks,\nand 42% did not improve in math & data analysis tasks when employing ChatGPT.\nFurther investigation indicated that higher-ability students, as proxied by\ntheir econometrics grades, were the ones who performed worse in writing\nanalysis tasks when using ChatGPT. We also found evidence that students with\nbetter digital skills performed better with ChatGPT. This research provides\ninsights on the impact of generative AI. Thus, stakeholders can make informed\ndecisions to implement appropriate policy frameworks or redesign educational\nsystems. It also highlights the critical role of human skills in addressing and\ncomplementing the limitations of technology.\n"
    },
    {
        "paper_id": 2403.01964,
        "authors": "David Kreitmeir and Paul A. Raschky",
        "title": "The Heterogeneous Productivity Effects of Generative AI",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  We analyse the individual productivity effects of Italy's ban on ChatGPT, a\ngenerative pretrained transformer chatbot. We compile data on the daily coding\noutput quantity and quality of over 36,000 GitHub users in Italy and other\nEuropean countries and combine these data with the sudden announcement of the\nban in a difference-in-differences framework. Among the affected users in\nItaly, we find a short-term increase in output quantity and quality for less\nexperienced users and a decrease in productivity on more routine tasks for\nexperienced users.\n"
    },
    {
        "paper_id": 2403.025,
        "authors": "Yilun Wang, Shengjie Guo",
        "title": "RVRAE: A Dynamic Factor Model Based on Variational Recurrent Autoencoder\n  for Stock Returns Prediction",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In recent years, the dynamic factor model has emerged as a dominant tool in\neconomics and finance, particularly for investment strategies. This model\noffers improved handling of complex, nonlinear, and noisy market conditions\ncompared to traditional static factor models. The advancement of machine\nlearning, especially in dealing with nonlinear data, has further enhanced asset\npricing methodologies. This paper introduces a groundbreaking dynamic factor\nmodel named RVRAE. This model is a probabilistic approach that addresses the\ntemporal dependencies and noise in market data. RVRAE ingeniously combines the\nprinciples of dynamic factor modeling with the variational recurrent\nautoencoder (VRAE) from deep learning. A key feature of RVRAE is its use of a\nprior-posterior learning method. This method fine-tunes the model's learning\nprocess by seeking an optimal posterior factor model informed by future data.\nNotably, RVRAE is adept at risk modeling in volatile stock markets, estimating\nvariances from latent space distributions while also predicting returns. Our\nempirical tests with real stock market data underscore RVRAE's superior\nperformance compared to various established baseline methods.\n"
    },
    {
        "paper_id": 2403.02523,
        "authors": "Pierre Brugiere and Gabriel Turinici",
        "title": "Transformer for Times Series: an Application to the S&P500",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  The transformer models have been extensively used with good results in a wide\narea of machine learning applications including Large Language Models and image\ngeneration. Here, we inquire on the applicability of this approach to financial\ntime series. We first describe the dataset construction for two prototypical\nsituations: a mean reverting synthetic Ornstein-Uhlenbeck process on one hand\nand real S&P500 data on the other hand. Then, we present in detail the proposed\nTransformer architecture and finally we discuss some encouraging results. For\nthe synthetic data we predict rather accurately the next move, and for the\nS&P500 we get some interesting results related to quadratic variation and\nvolatility prediction.\n"
    },
    {
        "paper_id": 2403.0256,
        "authors": "Rizwanul Karim",
        "title": "Impact of COVID-19 on Exchange rate volatility of Bangladesh: Evidence\n  through GARCH model",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This study uses the GARCH (1,1) model to examine the impact of COVID-19 cases\n(log value) on the volatility of the Exchange rate return of Bangladeshi taka\n(BDT) over the US dollar (USD), Japanese Yen (JPY), and Swedish Krona (SEK).\nThe result shows that an increase in the number of COVID-19-affected cases in\nBangladesh has a significant and positive impact on the volatility of exchange\nrates BDT/USD, BDT/JPY, and BDT/SEK.\n"
    },
    {
        "paper_id": 2403.02572,
        "authors": "Felix Lokin, Fenghui Yu",
        "title": "Fill Probabilities in a Limit Order Book with State-Dependent Stochastic\n  Order Flows",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  This paper focuses on computing the fill probabilities for limit orders\npositioned at various price levels within the limit order book, which play a\ncrucial role in optimizing executions. We adopt a generic stochastic model to\ncapture the dynamics of the order book as a series of queueing systems. This\ngeneric model is state-dependent and also incorporates stylized factors. We\nsubsequently derive semi-analytical expressions to compute the relevant\nprobabilities within the context of state-dependent stochastic order flows.\nThese probabilities cover various scenarios, including the probability of a\nchange in the mid-price, the fill probabilities of orders posted at the best\nquotes, and those posted at a price level deeper than the best quotes in the\nbook, before the opposite best quote moves. These expressions can be further\ngeneralized to accommodate orders posted even deeper in the order book,\nalthough the associated probabilities are typically very small in such cases.\nLastly, we conduct extensive numerical experiments using real order book data\nfrom the foreign exchange spot market. Our findings suggest that the model is\ntractable and possesses the capability to effectively capture the dynamics of\nthe limit order book. Moreover, the derived formulas and numerical methods\ndemonstrate reasonably good accuracy in estimating the fill probabilities.\n"
    },
    {
        "paper_id": 2403.02726,
        "authors": "Mi Zhou, Vibhanshu Abhishek, Timothy Derdenger, Jaymo Kim, Kannan\n  Srinivasan",
        "title": "Bias in Generative AI",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This study analyzed images generated by three popular generative artificial\nintelligence (AI) tools - Midjourney, Stable Diffusion, and DALLE 2 -\nrepresenting various occupations to investigate potential bias in AI\ngenerators. Our analysis revealed two overarching areas of concern in these AI\ngenerators, including (1) systematic gender and racial biases, and (2) subtle\nbiases in facial expressions and appearances. Firstly, we found that all three\nAI generators exhibited bias against women and African Americans. Moreover, we\nfound that the evident gender and racial biases uncovered in our analysis were\neven more pronounced than the status quo when compared to labor force\nstatistics or Google images, intensifying the harmful biases we are actively\nstriving to rectify in our society. Secondly, our study uncovered more nuanced\nprejudices in the portrayal of emotions and appearances. For example, women\nwere depicted as younger with more smiles and happiness, while men were\ndepicted as older with more neutral expressions and anger, posing a risk that\ngenerative AI models may unintentionally depict women as more submissive and\nless competent than men. Such nuanced biases, by their less overt nature, might\nbe more problematic as they can permeate perceptions unconsciously and may be\nmore difficult to rectify. Although the extent of bias varied depending on the\nmodel, the direction of bias remained consistent in both commercial and\nopen-source AI generators. As these tools become commonplace, our study\nhighlights the urgency to identify and mitigate various biases in generative\nAI, reinforcing the commitment to ensuring that AI technologies benefit all of\nhumanity for a more inclusive future.\n"
    },
    {
        "paper_id": 2403.02832,
        "authors": "Christian Bayer, Chiheb Ben Hammouda, Antonis Papapantoleon, Michael\n  Samet and Ra\\'ul Tempone",
        "title": "Quasi-Monte Carlo for Efficient Fourier Pricing of Multi-Asset Options",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Efficiently pricing multi-asset options poses a significant challenge in\nquantitative finance. The Monte Carlo (MC) method remains the prevalent choice\nfor pricing engines; however, its slow convergence rate impedes its practical\napplication. Fourier methods leverage the knowledge of the characteristic\nfunction to accurately and rapidly value options with up to two assets.\nNevertheless, they face hurdles in the high-dimensional settings due to the\ntensor product (TP) structure of commonly employed quadrature techniques. This\nwork advocates using the randomized quasi-MC (RQMC) quadrature to improve the\nscalability of Fourier methods with high dimensions. The RQMC technique\nbenefits from the smoothness of the integrand and alleviates the curse of\ndimensionality while providing practical error estimates. Nonetheless, the\napplicability of RQMC on the unbounded domain, $\\mathbb{R}^d$, requires a\ndomain transformation to $[0,1]^d$, which may result in singularities of the\ntransformed integrand at the corners of the hypercube, and deteriorate the rate\nof convergence of RQMC. To circumvent this difficulty, we design an efficient\ndomain transformation procedure based on the derived boundary growth conditions\nof the integrand. This transformation preserves the sufficient regularity of\nthe integrand and hence improves the rate of convergence of RQMC. To validate\nthis analysis, we demonstrate the efficiency of employing RQMC with an\nappropriate transformation to evaluate options in the Fourier space for various\npricing models, payoffs, and dimensions. Finally, we highlight the\ncomputational advantage of applying RQMC over MC or TP in the Fourier domain,\nand over MC in the physical domain for options with up to 15 assets.\n"
    },
    {
        "paper_id": 2403.03367,
        "authors": "Austin Adams and Ciamac C. Moallemi and Sara Reynolds and Dan Robinson",
        "title": "am-AMM: An Auction-Managed Automated Market Maker",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Automated market makers (AMMs) have emerged as the dominant market mechanism\nfor trading on decentralized exchanges implemented on blockchains. This paper\npresents a single mechanism that targets two important unsolved problems for\nAMMs: reducing losses to informed orderflow, and maximizing revenue from\nuninformed orderflow. The ``auction-managed AMM'' works by running a\ncensorship-resistant onchain auction for the right to temporarily act as ``pool\nmanager'' for a constant-product AMM. The pool manager sets the swap fee rate\non the pool, and also receives the accrued fees from swaps. The pool manager\ncan exclusively capture some arbitrage by trading against the pool in response\nto small price movements, and also can set swap fees incorporating price\nsensitivity of retail orderflow and adapting to changing market conditions,\nwith the benefits from both ultimately accruing to liquidity providers.\nLiquidity providers can enter and exit the pool freely in response to changing\nrent, though they must pay a small fee on withdrawal. We prove that under\ncertain assumptions, this AMM should have higher liquidity in equilibrium than\nany standard, fixed-fee AMM.\n"
    },
    {
        "paper_id": 2403.0341,
        "authors": "Novan Fauzi Al Giffary, Feri Sulianta",
        "title": "Prediction Of Cryptocurrency Prices Using LSTM, SVM And Polynomial\n  Regression",
        "comments": "Asian Journal of Engineering, Social and Health Volume 3, No. 2\n  February 2024 (308-319)",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  The rapid development of information technology, especially the Internet, has\nfacilitated users with a quick and easy way to seek information. With these\nconvenience offered by internet services, many individuals who initially\ninvested in gold and precious metals are now shifting into digital investments\nin form of cryptocurrencies. However, investments in crypto coins are filled\nwith uncertainties and fluctuation in daily basis. This risk posed as\nsignificant challenges for coin investors that could result in substantial\ninvestment losses. The uncertainty of the value of these crypto coins is a\ncritical issue in the field of coin investment. Forecasting, is one of the\nmethods used to predict the future value of these crypto coins. By utilizing\nthe models of Long Short Term Memory, Support Vector Machine, and Polynomial\nRegression algorithm for forecasting, a performance comparison is conducted to\ndetermine which algorithm model is most suitable for predicting crypto currency\nprices. The mean square error is employed as a benchmark for the comparison. By\napplying those three constructed algorithm models, the Support Vector Machine\nuses a linear kernel to produce the smallest mean square error compared to the\nLong Short Term Memory and Polynomial Regression algorithm models, with a mean\nsquare error value of 0.02. Keywords: Cryptocurrency, Forecasting, Long Short\nTerm Memory, Mean Square Error, Polynomial Regression, Support Vector Machine\n"
    },
    {
        "paper_id": 2403.03597,
        "authors": "W. Benedikt Schmal",
        "title": "The 'Must Stock' Challenge in Academic Publishing: Pricing Implications\n  of Transformative Agreements",
        "comments": "35 pages, 3 figures, unreviewed preprint",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  The high relevance of top-notch academic journals turns them into 'must\nstock' products that assign its often commercial owners with extraordinary\nmarket power. Intended to tackle this, university consortia around the globe\nnegotiate so-called 'transformative agreements' with many publishing houses. It\nshall pave the way towards standard open-access publishing. While several\ncontract designs exist, the 'publish-and-read' (PAR) scheme is the one that\ncomes closest to the ideal of an entirely open access environment: Publishers\nare paid a fixed case-by-case rate for each publication, which includes a fee\nfor their extensive libraries. In turn, all subscription payments are waived. I\ntheoretically derive that this contract design benefits the included publishers\nregardless of whether the number of publications in these publishers' journals\ngrows or declines. Consequently, widespread PAR contracts are likely to raise\nentry barriers for new (open-access) competitors even further. Intending to\nlower costs for the universities, their libraries, and, ultimately, the\ntaxpayers, this PAR fee contract design of transformative agreements might\ncause the opposite.\n"
    },
    {
        "paper_id": 2403.03606,
        "authors": "Mohammad Ali Labbaf Khaniki, Mohammad Manthouri",
        "title": "Enhancing Price Prediction in Cryptocurrency Using Transformer Neural\n  Network and Technical Indicators",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This study presents an innovative approach for predicting cryptocurrency time\nseries, specifically focusing on Bitcoin, Ethereum, and Litecoin. The\nmethodology integrates the use of technical indicators, a Performer neural\nnetwork, and BiLSTM (Bidirectional Long Short-Term Memory) to capture temporal\ndynamics and extract significant features from raw cryptocurrency data. The\napplication of technical indicators, such facilitates the extraction of\nintricate patterns, momentum, volatility, and trends. The Performer neural\nnetwork, employing Fast Attention Via positive Orthogonal Random features\n(FAVOR+), has demonstrated superior computational efficiency and scalability\ncompared to the traditional Multi-head attention mechanism in Transformer\nmodels. Additionally, the integration of BiLSTM in the feedforward network\nenhances the model's capacity to capture temporal dynamics in the data,\nprocessing it in both forward and backward directions. This is particularly\nadvantageous for time series data where past and future data points can\ninfluence the current state. The proposed method has been applied to the hourly\nand daily timeframes of the major cryptocurrencies and its performance has been\nbenchmarked against other methods documented in the literature. The results\nunderscore the potential of the proposed method to outperform existing models,\nmarking a significant progression in the field of cryptocurrency price\nprediction.\n"
    },
    {
        "paper_id": 2403.0361,
        "authors": "Timo Mueller-Tribbensee, Klaus M. Miller, Bernd Skiera",
        "title": "Paying for Privacy: Pay-or-Tracking Walls",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Prestigious news publishers, and more recently, Meta, have begun to request\nthat users pay for privacy. Specifically, users receive a notification banner,\nreferred to as a pay-or-tracking wall, that requires them to (i) pay money to\navoid being tracked or (ii) consent to being tracked. These walls have invited\nconcerns that privacy might become a luxury. However, little is known about\npay-or-tracking walls, which prevents a meaningful discussion about their\nappropriateness. This paper conducts several empirical studies and finds that\ntop EU publishers use pay-or-tracking walls. Their implementations involve\nvarious approaches, including bundling the pay option with advertising-free\naccess or additional content. The price for not being tracked exceeds the\nadvertising revenue that publishers generate from a user who consents to being\ntracked. Notably, publishers' traffic does not decline when implementing a\npay-or-tracking wall and most users consent to being tracked; only a few users\npay. In short, pay-or-tracking walls seem to provide the means for expanding\nthe practice of tracking. Publishers profit from pay-or-tracking walls and may\nobserve a revenue increase of 16.4% due to tracking more users than under a\ncookie consent banner.\n"
    },
    {
        "paper_id": 2403.03612,
        "authors": "Kinshuk Jerath, Klaus M. Miller",
        "title": "Consumers' Perceived Privacy Violations in Online Advertising",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  In response to privacy concerns about collecting and using personal data, the\nonline advertising industry has been developing privacy-enhancing technologies\n(PETs), e.g., under Google's Privacy Sandbox initiative. In this research, we\nuse the dual-privacy framework, which postulates that consumers have intrinsic\nand instrumental preferences for privacy, to understand consumers' perceived\nprivacy violations (PPVs) for current and proposed online advertising\npractices. The key idea is that different practices differ in whether\nindividual data leaves the consumer's machine or not and in how they track and\ntarget consumers; these affect, respectively, the intrinsic and instrumental\ncomponents of privacy preferences differently, leading to different PPVs for\ndifferent practices. We conducted online studies focused on consumers in the\nUnited States to elicit PPVs for various advertising practices. Our findings\nconfirm the intuition that tracking and targeting consumers under the industry\nstatus quo of behavioral targeting leads to high PPV. New technologies or\nproposals that ensure that data are kept on the consumer's machine lower PPV\nrelative to behavioral targeting but, importantly, this decrease is small.\nFurthermore, group-level targeting does not differ significantly from\nindividual-level targeting in reducing PPV. Under contextual targeting, where\nthere is no tracking, PPV is significantly reduced. Interestingly, with respect\nto PPV, consumers are indifferent between seeing untargeted ads and no ads when\nthey are not being tracked. We find that consumer perceptions of privacy\nviolations under different tracking and targeting practices may differ from\nwhat technical definitions suggest. Therefore, rather than relying solely on\ntechnical perspectives, a consumer-centric approach to privacy is needed, based\non, for instance, the dual-privacy framework.\n"
    },
    {
        "paper_id": 2403.03649,
        "authors": "Enzo Brox and Riccardo Di Francesco",
        "title": "The Cost of Coming Out",
        "comments": "Updating new version of the paper with new results",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  The fear of social stigma leads many individuals worldwide to hesitate in\ndisclosing their sexual orientation. Since concealing identity is costly, it is\ncrucial to understand the extent of anti-LGB sentiments and reactions to coming\nout. This paper uses an innovative data source from a popular online game\ntogether with a natural experiment to overcome existing data and endogeneity\nissues. We exploit exogenous variation in the identity of a character to\nidentify the effects of coming out on players' revealed preferences for that\ncharacter across diverse regions globally. Our findings reveal a substantial\nand persistent negative impact of coming out.\n"
    },
    {
        "paper_id": 2403.03785,
        "authors": "Rambod Rahmani, Marco Parola, and Mario G.C.A. Cimino",
        "title": "A machine learning workflow to address credit default prediction",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  Due to the recent increase in interest in Financial Technology (FinTech),\napplications like credit default prediction (CDP) are gaining significant\nindustrial and academic attention. In this regard, CDP plays a crucial role in\nassessing the creditworthiness of individuals and businesses, enabling lenders\nto make informed decisions regarding loan approvals and risk management. In\nthis paper, we propose a workflow-based approach to improve CDP, which refers\nto the task of assessing the probability that a borrower will default on his or\nher credit obligations. The workflow consists of multiple steps, each designed\nto leverage the strengths of different techniques featured in machine learning\npipelines and, thus best solve the CDP task. We employ a comprehensive and\nsystematic approach starting with data preprocessing using Weight of Evidence\nencoding, a technique that ensures in a single-shot data scaling by removing\noutliers, handling missing values, and making data uniform for models working\nwith different data types. Next, we train several families of learning models,\nintroducing ensemble techniques to build more robust models and hyperparameter\noptimization via multi-objective genetic algorithms to consider both predictive\naccuracy and financial aspects. Our research aims at contributing to the\nFinTech industry in providing a tool to move toward more accurate and reliable\ncredit risk assessment, benefiting both lenders and borrowers.\n"
    },
    {
        "paper_id": 2403.03915,
        "authors": "Xin Yue Ren and Dena Firoozi",
        "title": "Risk-Sensitive Mean Field Games with Common Noise: A Theoretical Study\n  with Applications to Interbank Markets",
        "comments": "47 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper, we address linear-quadratic-Gaussian (LQG) risk-sensitive mean\nfield games (MFGs) with common noise. In this framework agents are exposed to a\ncommon noise and aim to minimize an exponential cost functional that reflects\ntheir risk sensitivity. We leverage the convex analysis method to derive the\noptimal strategies of agents in the limit as the number of agents goes to\ninfinity. These strategies yield a Nash equilibrium for the limiting model. The\nmodel is then applied to interbank markets, focusing on optimizing lending and\nborrowing activities to assess systemic and individual bank risks when reserves\ndrop below a critical threshold. We employ Fokker-Planck equations and the\nfirst hitting time method to formulate the overall probability of a bank or\nmarket default. We observe that the risk-averse behavior of agents reduces the\nprobability of individual defaults and systemic risk, enhancing the resilience\nof the financial system. Adopting a similar approach based on stochastic\nFokker-Planck equations, we further expand our analysis to investigate the\nconditional probabilities of individual default under specific trajectories of\nthe common market shock.\n"
    },
    {
        "paper_id": 2403.04104,
        "authors": "Bo Li",
        "title": "Testing Business Cycle Theories: Evidence from the Great Recession",
        "comments": "111 pages, 11 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Empirical business cycle studies using cross-country data usually cannot\nachieve causal relationships while within-country studies mostly focus on the\nbust period. We provide the first causal investigation into the boom period of\nthe 1999-2010 U.S. cross-metropolitan business cycle. Using a novel research\ndesign, we show that credit expansion in private-label mortgages causes a\ndifferentially stronger boom (2000-2006) and bust (2007-2010) cycle in the\nhouse-related industries in the high net-export-growth areas. Most importantly,\nour unique research design enables us to perform the most comprehensive tests\non theories (hypotheses) regarding the business cycle. We show that the\nfollowing theories (hypotheses) cannot explain the cause of the 1999-2010 U.S.\nbusiness cycle: the speculative euphoria hypothesis, the real business cycle\ntheory, the collateral-driven credit cycle theory, the business uncertainty\ntheory, and the extrapolative expectation theory.\n"
    },
    {
        "paper_id": 2403.04674,
        "authors": "David Itkin and Martin Larsson",
        "title": "Calibrated rank volatility stabilized models for large equity markets",
        "comments": "35 pages, 10 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In the framework of stochastic portfolio theory we introduce rank volatility\nstabilized models for large equity markets over long time horizons. These\nmodels are rank-based extensions of the volatility stabilized models introduced\nby Fernholz & Karatzas in 2005. On the theoretical side we establish global\nexistence of the model and ergodicity of the induced ranked market weights. We\nalso derive explicit expressions for growth-optimal portfolios and show the\nexistence of relative arbitrage with respect to the market portfolio. On the\nempirical side we calibrate the model to sixteen years of CRSP US equity data\nmatching (i) rank-based volatilities, (ii) stock turnover as measured by market\nweight collisions, (iii) the average market rate of return and (iv) the capital\ndistribution curve. Assessment of model fit and error analysis is conducted\nboth in and out of sample. To the best of our knowledge this is the first model\nexhibiting relative arbitrage that has statistically been shown to have a good\nquantitative fit with the empirical features (i)-(iv). We additionally simulate\ntrajectories of the calibrated model and compare them to historical\ntrajectories, both in and out of sample.\n"
    },
    {
        "paper_id": 2403.05222,
        "authors": "Alfred Galichon and Simon Weber",
        "title": "Matching under Imperfectly Transferable Utility",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  In this paper, we examine matching models with imperfectly transferable\nutility (ITU). We provide motivating examples, discuss the theoretical\nfoundations of ITU matching models and present methods for estimating them. We\nalso explore connected topics and provide an overview of the related\nliterature. This paper has been submitted as a draft chapter for the Handbook\nof the Economics of Matching, edited by Che, Chiappori and Salani\\'e.\n"
    },
    {
        "paper_id": 2403.05541,
        "authors": "Jun Xu",
        "title": "AI in ESG for Financial Institutions: An Industrial Survey",
        "comments": "31 pages, 14 tables, 3 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The burgeoning integration of Artificial Intelligence (AI) into\nEnvironmental, Social, and Governance (ESG) initiatives within the financial\nsector represents a paradigm shift towards more sus-tainable and equitable\nfinancial practices. This paper surveys the industrial landscape to delineate\nthe necessity and impact of AI in bolstering ESG frameworks. With the advent of\nstringent regulatory requirements and heightened stakeholder awareness,\nfinancial institutions (FIs) are increasingly compelled to adopt ESG criteria.\nAI emerges as a pivotal tool in navigating the complex in-terplay of financial\nactivities and sustainability goals. Our survey categorizes AI applications\nacross three main pillars of ESG, illustrating how AI enhances analytical\ncapabilities, risk assessment, customer engagement, reporting accuracy and\nmore. Further, we delve into the critical con-siderations surrounding the use\nof data and the development of models, underscoring the importance of data\nquality, privacy, and model robustness. The paper also addresses the imperative\nof responsible and sustainable AI, emphasizing the ethical dimensions of AI\ndeployment in ESG-related banking processes. Conclusively, our findings suggest\nthat while AI offers transformative potential for ESG in banking, it also poses\nsignificant challenges that necessitate careful consideration. The final part\nof the paper synthesizes the survey's insights, proposing a forward-looking\nstance on the adoption of AI in ESG practices. We conclude with recommendations\nwith a reference architecture for future research and development, advocating\nfor a balanced approach that leverages AI's strengths while mitigating its\nrisks within the ESG domain.\n"
    },
    {
        "paper_id": 2403.05671,
        "authors": "Shabnam Salehi, Mojtaba Ardestani",
        "title": "Investigating Changes of Water Quality in Reservoirs based on Flood and\n  Inflow Fluctuations",
        "comments": "18 pages, 14 figuers",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Water temperature and dissolved oxygen are essential indicators of water\nquality and ecosystem sustainability. Lately, heavy rainfalls are happening\nfrequently and forcefully affecting the thermal structure and mixing layers in\ndepth by sharply increasing the volume of inflow entitled flash flood. It can\noccur by sudden intense precipitation and develop within minutes or hours.\nBecause of heavy debris load and speedy water, this phenomenon has remarkable\neffects on water quality. A higher flow during floods may worsens water quality\nat lakes and reservoirs that are thermally stratified (with separate density\nlayers) and decrease dissolved oxygen content. However, it is unclear how well\nthese parameters represent the response of lakes to changes in volume\ndischarge. To address this question, researchers simulate the thermal structure\nin two stratified reservoirs, considering the Rajae reservoir as a\nrepresentative reservoir in the north of Iran and Minab reservoir in the south.\nIn this study, the model realistically represented variations of dissolved\noxygen and temperature of dams Lake response to flash floods. The model\nperformance was evaluated using observed data from stations on the dams lake.\nIn this case, the inflow charge considered in a 10-day flash flood from April\n6th to April 16th during the yearly normal flow. The complete mixture in a part\nof the thermal structure has been proved in Rajaee reservoir. The nonpermanent\nimpact of the massive inflow of storm runoff caused an increase in\noxygen-consuming, leading to a severe decrease in dissolved oxygen on\nepilimnion and metalimnion. The situation in Minab reservoir was relatively\ndifferent from Rajae reservoir. The inflow changes not only cause mixture but\nalso help expanding stratification.\n"
    },
    {
        "paper_id": 2403.05743,
        "authors": "Xinyi Wang, Qing Zhao, Lang Tong",
        "title": "Forecasting Electricity Market Signals via Generative AI",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  This paper presents a generative artificial intelligence approach to\nprobabilistic forecasting of electricity market signals, such as real-time\nlocational marginal prices and area control error signals. Inspired by the\nWiener-Kallianpur innovation representation of nonparametric time series, we\npropose a weak innovation autoencoder architecture and a novel deep learning\nalgorithm that extracts the canonical independent and identically distributed\ninnovation sequence of the time series, from which samples of future time\nseries are generated. The validity of the proposed approach is established by\nproving that, under ideal training conditions, the generated samples have the\nsame conditional probability distribution as that of the ground truth. Three\napplications involving highly dynamic and volatile time series in real-time\nmarket operations are considered: (i) locational marginal price forecasting for\nself-scheduled resources such as battery storage participants, (ii)\ninterregional price spread forecasting for virtual bidders in interchange\nmarkets, and (iii) area control error forecasting for frequency regulations.\nNumerical studies based on market data from multiple independent system\noperators demonstrate the superior performance of the proposed generative\nforecaster over leading classical and modern machine learning techniques under\nboth probabilistic and point forecasting metrics.\n"
    },
    {
        "paper_id": 2403.0583,
        "authors": "Gergely Horvath, Mofei Jia",
        "title": "The impact of social status on the formation of collaborative ties and\n  effort provision: An experimental study",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  We study whether competition for social status induces higher effort\nprovision and efficiency when individuals collaborate with their network\nneighbors. We consider a laboratory experiment in which individuals choose a\ncostly collaborative effort and their network neighbors. They benefit from\ntheir neighbors' effort and effort choices of direct neighbors are strategic\ncomplements. We introduce two types of social status in a 2x2 factorial design:\n1) individuals receive monetary benefits for incoming links representing\npopularity; 2) they receive feedback on their relative payoff ranking within\nthe group. We find that link benefits induce higher effort provision and\nstrengthen the collaborative ties relative to the Baseline treatment without\nsocial status. In contrast, the ranking information induces lower effort as\nindividuals start competing for higher ranking. Overall, we find that social\nstatus has no significant impact on the number of links in the network and the\nefficiency of collaboration in the group.\n"
    },
    {
        "paper_id": 2403.05913,
        "authors": "Gergely Horvath",
        "title": "Network formation and efficiency in linear-quadratic games: An\n  experimental study",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  We experimentally study effort provision and network formation in the\nlinear-quadratic game characterized by positive externality and complementarity\nof effort choices among network neighbors. We compare experimental outcomes to\nthe equilibrium and efficient allocations and study the impact of group size\nand linking costs. We find that individuals overprovide effort relative to the\nequilibrium level on the network they form. However, their payoffs are lower\nthan the equilibrium payoffs because they create fewer links than it is optimal\nwhich limits the beneficial spillovers of effort provision. Reducing the\nlinking costs does not significantly increase the connectedness of the network\nand the welfare loss is higher in larger groups. Individuals connect to the\nhighest effort providers in the group and ignore links to relative low effort\nproviders, even if those links would be beneficial to form. This effect\nexplains the lack of links in the network.\n"
    },
    {
        "paper_id": 2403.06035,
        "authors": "Masoud Taherinia, Mehrdad Matin, Jamal Valipour, Kavian Abdolahi,\n  Peyman Shouryabi, Mohammad Mahdi Barzegar",
        "title": "Capital Structure Adjustment Speed and Expected Returns: Examination of\n  Information Asymmetry as a Moderating Role",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Shareholders' expectations of stock returns and fluctuations are constantly\nchanging due to restrictions in financial status and undesirable capital\nstructure, which constrain managers to limit the changes in price trends in\norder to cover the risk instigated and infused by the unfavorable situation.\nThe present research examines the moderating impact of information asymmetry on\nthe relationship between capital structure adjustment and expected returns. The\ndata from 120 companies approved in the Tehran Stock Exchange were extracted,\nand a hybrid data regression model was used to test the research hypotheses.\nFindings indicate that the capital structure adjustment speed correlates with\nthe expected returns. Moreover, the information asymmetry positively affects\nthe relationship between capital structure adjustment speed and expected\nreturns.\n"
    },
    {
        "paper_id": 2403.0615,
        "authors": "Zhang Xu, Mingsheng Zhang, Wei Zhao",
        "title": "Algorithmic Collusion and Price Discrimination: The Over-Usage of Data",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  As firms' pricing strategies increasingly rely on algorithms, two concerns\nhave received much attention: algorithmic tacit collusion and price\ndiscrimination. This paper investigates the interaction between these two\nissues through simulations. In each period, a new buyer arrives with\nindependently and identically distributed willingness to pay (WTP), and each\nfirm, observing private signals about WTP, adopts Q-learning algorithms to set\nprices. We document two novel mechanisms that lead to collusive outcomes. Under\nasymmetric information, the algorithm with information advantage adopts a\nBait-and-Restrained-Exploit strategy, surrendering profits on some signals by\nsetting higher prices, while exploiting limited profits on the remaining\nsignals by setting much lower prices. Under a symmetric information structure,\ncompetition on some signals facilitates convergence to supra-competitive prices\non the remaining signals. Algorithms tend to collude more on signals with\nhigher expected WTP. Both uncertainty and the lack of correlated signals\nexacerbate the degree of collusion, thereby reducing both consumer surplus and\nsocial welfare. A key implication is that the over-usage of data, both\npayoff-relevant and non-relevant, by AIs in competitive contexts will reduce\nthe degree of collusion and consequently lead to a decline in industry profits.\n"
    },
    {
        "paper_id": 2403.06188,
        "authors": "M\\\"ucahit Ayg\\\"un, Fabio Bellini, Roger J. A. Laeven",
        "title": "On Geometrically Convex Risk Measures",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Geometrically convex functions constitute an interesting class of functions\nobtained by replacing the arithmetic mean with the geometric mean in the\ndefinition of convexity. As recently suggested, geometric convexity may be a\nsensible property for financial risk measures ([7,13,4]).\n  We introduce a notion of GG-convex conjugate, parallel to the classical\nnotion of convex conjugate introduced by Fenchel, and we discuss its\nproperties. We show how GG-convex conjugation can be axiomatized in the spirit\nof the notion of general duality transforms introduced in [2,3].\n  We then move to the study of GG-convex risk measures, which are defined as\nGG-convex functionals defined on suitable spaces of random variables. We derive\na general dual representation that extends analogous expressions presented in\n[4] under the additional assumptions of monotonicity and positive homogeneity.\nAs a prominent example, we study the family of Orlicz risk measures. Finally,\nwe introduce multiplicative versions of the convex and of the increasing convex\norder and discuss related consistency properties of law-invariant GG-convex\nrisk measures.\n"
    },
    {
        "paper_id": 2403.06253,
        "authors": "Rishabh Gupta, Ewa A. Drzazga-Szcz\\c{e}\\'sniak, Sabre Kais, Dominik\n  Szcz\\c{e}\\'sniak",
        "title": "Entropy corrected geometric Brownian motion",
        "comments": "6 pages, 5 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The geometric Brownian motion (GBM) is widely employed for modeling\nstochastic processes, yet its solutions are characterized by the log-normal\ndistribution. This comprises predictive capabilities of GBM mainly in terms of\nforecasting applications. Here, entropy corrections to GBM are proposed to go\nbeyond log-normality restrictions and better account for intricacies of real\nsystems. It is shown that GBM solutions can be effectively refined by arguing\nthat entropy is reduced when deterministic content of considered data\nincreases. Notable improvements over conventional GBM are observed for several\ncases of non-log-normal distributions, ranging from a dice roll experiment to\nreal world data.\n"
    },
    {
        "paper_id": 2403.06303,
        "authors": "Marie-Claude Vachon and Anne Mackay",
        "title": "A Unifying Approach for the Pricing of Debt Securities",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  We propose a unifying framework for the pricing of debt securities under\ngeneral time-inhomogeneous short-rate diffusion processes. The pricing of\nbonds, bond options, callable/putable bonds, and convertible bonds (CBs) are\ncovered. Using continuous-time Markov chain (CTMC) approximation, we obtain\nclosed-form matrix expressions to approximate the price of bonds and bond\noptions under general one-dimensional short-rate processes. A simple and\nefficient algorithm is also developed to price callable/putable debts. The\navailability of a closed-form expression for the price of zero-coupon bonds\nallows for the perfect fit of the approximated model to the current market term\nstructure of interest rates, regardless of the complexity of the underlying\ndiffusion process selected. We further consider the pricing of CBs under\ngeneral bi-dimensional time-inhomogeneous diffusion processes to model equity\nand short-rate dynamics. Credit risk is also incorporated into the model using\nthe approach of Tsiveriotis and Fernandes (1998). Based on a two-layer CTMC\nmethod, an efficient algorithm is developed to approximate the price of\nconvertible bonds. When conversion is only allowed at maturity, a closed-form\nmatrix expression is obtained. Numerical experiments show the accuracy and\nefficiency of the method across a wide range of model parameters and short-rate\nmodels.\n"
    },
    {
        "paper_id": 2403.06368,
        "authors": "Jiayi Wen, Zixi Ye, Xuan Zhang",
        "title": "A New Testing Method for Justification Bias Using High-Frequency Data of\n  Health and Employment",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Justification bias, wherein retirees may report poorer health to rationalize\ntheir retirement, poses a major concern to the widely-used measure of\nself-assessed health in retirement studies. This paper introduces a novel\nmethod for testing the presence of this bias in the spirit of regression\ndiscontinuity. The underlying idea is that any sudden shift in self-assessed\nhealth immediately following retirement is more likely attributable to the\nbias. Our strategy is facilitated by a unique high-frequency data that offers\nmonthly, in contrast to the typical biennial, information on employment,\nself-assessed health, and objective health conditions. Across a wider\npost-retirement time frame, we observe a decline in self-assessed health,\npotentially stemming from both justification bias and changes in actual health.\nHowever, this adverse effect diminishes with shorter intervals, indicating no\nevidence of such bias. Our method also validates a widely-used indirect testing\napproach.\n"
    },
    {
        "paper_id": 2403.06482,
        "authors": "Daixin Wang, Zhiqiang Zhang, Yeyu Zhao, Kai Huang, Yulin Kang, Jun\n  Zhou",
        "title": "Financial Default Prediction via Motif-preserving Graph Neural Network\n  with Curriculum Learning",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  User financial default prediction plays a critical role in credit risk\nforecasting and management. It aims at predicting the probability that the user\nwill fail to make the repayments in the future. Previous methods mainly extract\na set of user individual features regarding his own profiles and behaviors and\nbuild a binary-classification model to make default predictions. However, these\nmethods cannot get satisfied results, especially for users with limited\ninformation. Although recent efforts suggest that default prediction can be\nimproved by social relations, they fail to capture the higher-order topology\nstructure at the level of small subgraph patterns. In this paper, we fill in\nthis gap by proposing a motif-preserving Graph Neural Network with curriculum\nlearning (MotifGNN) to jointly learn the lower-order structures from the\noriginal graph and higherorder structures from multi-view motif-based graphs\nfor financial default prediction. Specifically, to solve the problem of weak\nconnectivity in motif-based graphs, we design the motif-based gating mechanism.\nIt utilizes the information learned from the original graph with good\nconnectivity to strengthen the learning of the higher-order structure. And\nconsidering that the motif patterns of different samples are highly unbalanced,\nwe propose a curriculum learning mechanism on the whole learning process to\nmore focus on the samples with uncommon motif distributions. Extensive\nexperiments on one public dataset and two industrial datasets all demonstrate\nthe effectiveness of our proposed method.\n"
    },
    {
        "paper_id": 2403.06779,
        "authors": "Junyi Ye, Bhaskar Goswami, Jingyi Gu, Ajim Uddin, Guiling Wang",
        "title": "From Factor Models to Deep Learning: Machine Learning in Reshaping\n  Empirical Asset Pricing",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  This paper comprehensively reviews the application of machine learning (ML)\nand AI in finance, specifically in the context of asset pricing. It starts by\nsummarizing the traditional asset pricing models and examining their\nlimitations in capturing the complexities of financial markets. It explores how\n1) ML models, including supervised, unsupervised, semi-supervised, and\nreinforcement learning, provide versatile frameworks to address these\ncomplexities, and 2) the incorporation of advanced ML algorithms into\ntraditional financial models enhances return prediction and portfolio\noptimization. These methods can adapt to changing market dynamics by modeling\nstructural changes and incorporating heterogeneous data sources, such as text\nand images. In addition, this paper explores challenges in applying ML in asset\npricing, addressing the growing demand for explainability in decision-making\nand mitigating overfitting in complex models. This paper aims to provide\ninsights into novel methodologies showcasing the potential of ML to reshape the\nfuture of quantitative finance.\n"
    },
    {
        "paper_id": 2403.07019,
        "authors": "Md. Galib Ishraq Emran, Rhidi Barma, Akram Hussain Khan, and Mrinmoy\n  Roy",
        "title": "Reasons behind the Water Crisis and its Potential Health Outcomes",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Globally, the water crisis has become a significant problem that affects\ndeveloping and industrialized nations. Water shortage can harm public health by\nincreasing the chance of contracting water-borne diseases, dehydration, and\nmalnutrition. This study aims to examine the causes of the water problem and\nits likely effects on human health. The study scrutinizes the reasons behind\nthe water crisis, including population increase, climate change, and\ninefficient water management techniques. The results of a lack of water on\nhuman health, such as the spread of infectious diseases, a higher risk of\nstarvation and dehydration, and psychological stress, are also concealed in the\nstudy. The research further suggests several ways to deal with the water\nsituation and lessen its potential outcomes on human health. These remedies\ninclude enhanced sanitation and hygiene procedures, water management, and\nconservation techniques like rainwater gathering and wastewater recycling.\n"
    },
    {
        "paper_id": 2403.07166,
        "authors": "Doron Sayag, Avichai Snir, and Daniel Levy",
        "title": "Small Price Changes, Sales Volume, and Menu Cost",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  The finding of small price changes in many retail price datasets is often\nviewed as a puzzle. We show that a possible explanation for the presence of\nsmall price changes is related to sales volume, an observation that has been\noverlooked in the existing literature. Analyzing a large retail scanner price\ndataset that contains information on both prices and sales volume, we find that\nsmall price changes are more frequent when products sales volume is high. This\nfinding holds across product categories, within product categories, and for\nindividual products. It is also robust to various sensitivity analyses such as\nmeasurement errors, the definition of small price changes, the inclusion of\nmeasures of price synchronization, the size of producers, the time horizon used\nto compute the average sales volume, the revenues, the competition, shoppers\ncharacteristics, etc.\n"
    },
    {
        "paper_id": 2403.0718,
        "authors": "Yuxiang Sun, Jingyi Li, Mengdie Lu, Zongying Guo",
        "title": "Study of the Impact of the Big Data Era on Accounting and Auditing",
        "comments": "4 pages",
        "journal-ref": "Frontiers in Business, Economics and Management Vol. 13 No. 3\n  (2024) PUBLISHED: 05-03-2024",
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Big data revolutionizes accounting and auditing, offering deep insights but\nalso introducing challenges like data privacy and security. With data from IoT,\nsocial media, and transactions, traditional practices are evolving.\nProfessionals must adapt to these changes, utilizing AI and machine learning\nfor efficient data analysis and anomaly detection. Key to overcoming these\nchallenges are enhanced analytics tools, continuous learning, and industry\ncollaboration. By addressing these areas, the accounting and auditing fields\ncan harness big data's potential while ensuring accuracy, transparency, and\nintegrity in financial reporting. Keywords: Big Data, Accounting, Audit, Data\nPrivacy, AI, Machine Learning, Transparency.\n"
    },
    {
        "paper_id": 2403.0753,
        "authors": "Suganthi Pazhanivel Koushika, Anbalagan Krishnaveni, Sellaperumal\n  Pazhanivelan, Alagirisamy Bharani, Venugopal Arunkumar, Perumal Devaki and\n  Narayanan Muthukrishnan",
        "title": "Carbon Economics of Different Agricultural Practices for Farming Soil",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
        "abstract": "  The loss of soil organic carbon (SOC) poses a severe danger to agricultural\nsustainability around the World. This review examines various farming practices\nand their impact on soil organic carbon storage. After a careful review of the\nliterature, most of the research indicated that different farming practices,\nsuch as organic farming, cover crops, conservation tillage, and agroforestry,\nplay vital roles in increasing the SOC content of the soil sustainably. Root\nexudation from cover crops increases microbial activity and helps break down\ncomplex organic compounds into organic carbon. Conservation tillage enhances\nthe soil structure and maintains carbon storage without disturbing the soil.\nAgroforestry systems boost organic carbon input and fasten nutrient cycling\nbecause the trees and crops have symbiotic relationships. Intercropping and\ncrop rotations have a role in changing the composition of plant residues and\npromoting carbon storage. There were many understanding on the complex\ninteractions between soil organic carbon dynamics and agricultural practices.\nBased on the study, the paper reveals, the role of different agricultural\npractices like Carbon storage through cover crops, crop rotation, mulching\nConservation tillage, conventional tillage, zero tillage and organic amendments\nin organic carbon storage in the soil for maximum crop yield to improve the\neconomic condition of the cultivators.\n"
    },
    {
        "paper_id": 2403.07617,
        "authors": "Avichai Snir, Daniel Levy, Dudi Levy, and Haipeng Allan Chen",
        "title": "Price Gouging or Market Forces? Fairness Perceptions of Price Hikes in\n  the Pandemic",
        "comments": null,
        "journal-ref": null,
        "doi": "10.13140/RG.2.2.26574.06727",
        "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
        "abstract": "  We report the results of surveys we conducted in the US and Israel in 2020, a\ntime when many prices increased following the spread of the pandemic. To assess\nrespondents perceptions of price increases, we focus on goods whose prices have\nincreased during the pandemic, including some essential goods. Consistent with\nthe principle of dual entitlement, we find that respondents perceive price\nincreases as more acceptable if they are due to cost shocks than if they are\ndue to demand shocks. However, we also find large differences across the two\npopulations, as well as across goods.\n"
    },
    {
        "paper_id": 2403.07998,
        "authors": "Khizar Qureshi and Tauhid Zaman",
        "title": "Pairs Trading Using a Novel Graphical Matching Approach",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Pairs trading, a strategy that capitalizes on price movements of asset pairs\ndriven by similar factors, has gained significant popularity among traders.\nCommon practice involves selecting highly cointegrated pairs to form a\nportfolio, which often leads to the inclusion of multiple pairs sharing common\nassets. This approach, while intuitive, inadvertently elevates portfolio\nvariance and diminishes risk-adjusted returns by concentrating on a small\nnumber of highly cointegrated assets. Our study introduces an innovative pair\nselection method employing graphical matchings designed to tackle this\nchallenge. We model all assets and their cointegration levels with a weighted\ngraph, where edges signify pairs and their weights indicate the extent of\ncointegration. A portfolio of pairs is a subgraph of this graph. We construct a\nportfolio which is a maximum weighted matching of this graph to select pairs\nwhich have strong cointegration while simultaneously ensuring that there are no\nshared assets within any pair of pairs. This approach ensures each asset is\nincluded in just one pair, leading to a significantly lower variance in the\nmatching-based portfolio compared to a baseline approach that selects pairs\npurely based on cointegration. Theoretical analysis and empirical testing using\ndata from the S\\&P 500 between 2017 and 2023, affirm the efficacy of our\nmethod. Notably, our matching-based strategy showcases a marked improvement in\nrisk-adjusted performance, evidenced by a gross Sharpe ratio of 1.23, a\nsignificant enhancement over the baseline value of 0.48 and market value of\n0.59. Additionally, our approach demonstrates reduced trading costs\nattributable to lower turnover, alongside minimized single asset risk due to a\nmore diversified asset base.\n"
    },
    {
        "paper_id": 2403.08202,
        "authors": "Ziyi Xu and Xue Cheng",
        "title": "Trading Large Orders in the Presence of Multiple High-Frequency\n  Anticipatory Traders",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We investigate a market with a normal-speed informed trader (IT) who may\nemploy mixed strategy and multiple anticipatory high-frequency traders (HFTs)\nwho are under different inventory pressures, in a three-period Kyle's model.\nThe pure- and mixed-strategy equilibria are considered and the results provide\nrecommendations for IT's randomization strategy with different numbers of HFTs.\nSome surprising results about investors' profits arise: the improvement of\nanticipatory traders' speed or a more precise prediction may harm themselves\nbut help IT.\n"
    },
    {
        "paper_id": 2403.0823,
        "authors": "Minyu Shen, Weihua Gu, Michael J. Cassidy, Yongjie Lin, Wei Ni",
        "title": "A vicious cycle along busy bus corridors and how to abate it",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
        "abstract": "  We unveil that a previously-unreported vicious cycle can be created when bus\nqueues form at curbside stops along a corridor. Buses caught in this cycle\nexhibit growing variation in headways as they travel from stop to stop. Bus\n(and patron) delays accumulate in like fashion and can grow large on long, busy\ncorridors. We show that this damaging cycle can be abated in simple ways.\nPresent solutions entail holding buses at a corridor entrance and releasing\nthem as per various strategies proposed in the literature. We introduce a\nmodest variant to the simplest of these strategies. It releases buses at\nheadways that are slightly less than, or equal to, the scheduled values. It\nturns out that periodically releasing buses at slightly smaller headways can\nsubstantially reduce bus delays caused by holding so that benefits can more\nreadily outweigh costs in corridors that contain a sufficient number of serial\nbus stops. The simple variant is shown to perform about as well as, or better\nthan, other bus-holding strategies in terms of saving delays, and is more\neffective than other strategies in regularizing bus headways. We also show that\ngrouping buses from across multiple lines and holding them by group can be\neffective when patrons have the flexibility to choose buses from across all\nlines in a group. Findings come by formulating select models of bus-corridor\ndynamics and using these to simulate part of the Bus Rapid Transit corridor in\nGuangzhou, China.\n"
    },
    {
        "paper_id": 2403.08362,
        "authors": "Marcus H\\\"aggbom, Morten Karlsmark, Joakim And\\'en",
        "title": "Mean-Field Microcanonical Gradient Descent",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Microcanonical gradient descent is a sampling procedure for energy-based\nmodels allowing for efficient sampling of distributions in high dimension. It\nworks by transporting samples from a high-entropy distribution, such as\nGaussian white noise, to a low-energy region using gradient descent. We put\nthis model in the framework of normalizing flows, showing how it can often\noverfit by losing an unnecessary amount of entropy in the descent. As a remedy,\nwe propose a mean-field microcanonical gradient descent that samples several\nweakly coupled data points simultaneously, allowing for better control of the\nentropy loss while paying little in terms of likelihood fit. We study these\nmodels in the context of financial time series, illustrating the improvements\non both synthetic and real data.\n"
    },
    {
        "paper_id": 2403.08678,
        "authors": "Petri P. Karenlampi",
        "title": "Path-dependency and leverage effect on capital return in periodic growth\n  processes",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Probability theory is applied to the finances of periodic growth processes.\nThe expected value of the profit rate, on accrual basis, does not directly\ndepend on divestments, neither on the capitalization path. The expected value\nof capitalization is path dependent. Because of the path-dependent\ncapitalization, the return rate on capital is path-dependent, and the\ntime-average return rate on capital differs from the expected value of the\nreturn rate on capital for the growth cycle. In the absence of intermediate\ndivestments, the internal rate of return is path-independent, thereby differing\nfrom the expected value of the rate of return on capital. It is shown that the\narea-average of internal rate of return is not representative for the rate of\nreturn on capital within an estate. It is shown that the rotation cycle length\nmaximizing the return rate on equity is independent of market interest rate.\nCorrespondingly, from the viewpoint of wealth accumulation, the often-suggested\ndependency of suitable rotation length on discount rate appears to be a\nmodeling artifact. Leverage effect enters the microeconomics of the growth\nprocesses through a separate leverage equation, where the leverage coefficient\nmay reach positive or negative values. The leverage effect on the internal rate\nof return and the net present value are discussed. Both effects are solvable,\nresulting in incorrect estimates.\n"
    },
    {
        "paper_id": 2403.08811,
        "authors": "Jackie Grant",
        "title": "The UK Universities Superannuation Scheme valuations 2014-2023: gilt\n  yield dependence, self-sufficiency and metrics",
        "comments": "53 pages, 20 figures, 10 tables and 4 appendices (conclusions\n  updated, post-analysis note added, glossary expanded, typos fixed)",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
        "abstract": "  This review considers the Universities Superannuation Scheme (USS) valuations\nfrom 2014 to 2023. USS is a 70-80 billion GBP Defined Benefit pension scheme\nwith over 500,000 members who are employed (or have been employed) at around 70\nUK universities. Disputes over USS have led to a decade of industrial action.\nNew results are presented showing the high dependence of USS pension\ncontributions on the return from UK government bonds (the gilt yield). The two\nconditions of the USS-specific 'self-sufficiency' (SfS) definition are\nexamined. USS data are presented along with new analysis. It is shown that the\nsecond SfS condition of 'maintaining a high funding ratio' dominates USS\nmodelling to amplify gilt yield dependence, inflating the SfS liabilities\nbeyond the regulatory requirements, and leading to excessive prudence. The Red,\nAmber and Green status of USS metrics 'Actual' and 'Target' Reliance are also\nexamined. It is shown that Target Reliance tethers the cost of future pensions\nto the SfS definition and that Actual Reliance can simultaneously be Green and\nRed. Implications for regulatory intervention are considered. An aim of this\nreview is to support evidence-based decision making and consensus building.\n"
    },
    {
        "paper_id": 2403.08846,
        "authors": "Roozbeh Qorbanian, Nils L\\\"ohndorf, David Wozabal",
        "title": "Valuation of Power Purchase Agreements for Corporate Renewable Energy\n  Procurement",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Corporate renewable power purchase agreements (PPAs) are long-term contracts\nthat enable companies to source renewable energy without having to develop and\noperate their own capacities. Typically, producers and consumers agree on a\nfixed per-unit price at which power is purchased. The value of the PPA to the\nbuyer depends on the so called capture price defined as the difference between\nthis fixed price and the market value of the produced volume during the\nduration of the contract. To model the capture price, practitioners often use\neither fundamental or statistical approaches to model future market prices,\nwhich both have their inherent limitations. We propose a new approach that\nblends the logic of fundamental electricity market models with statistical\nlearning techniques. In particular, we use regularized inverse optimization in\na quadratic fundamental bottom-up model of the power market to estimate the\nmarginal costs of different technologies as a parametric function of exogenous\nfactors. We compare the out-of-sample performance in forecasting the capture\nprice using market data from three European countries and demonstrate that our\napproach outperforms established statistical learning benchmarks. We then\ndiscuss the case of a photovoltaic plant in Spain to illustrate how to use the\nmodel to value a PPA from the buyer's perspective.\n"
    },
    {
        "paper_id": 2403.08886,
        "authors": "Sebasti\\'an Leavy, Gabriela Allegretti, Elen Presotto, Marco Antonio\n  Montoya and Edson Talamini",
        "title": "Measuring the bioeconomy economically: exploring the connections between\n  concepts, methods, data, indicators and their limitations",
        "comments": "68 pages, 12 figures, 1 Table, 2 annexes, Total of 17,758 words",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Despite its relevance, measuring the contributions of the bioeconomy to\nnational economies remains an arduous task that faces limitations. Part of the\ndifficulty is associated with the lack of a clear and widely accepted concept\nof the bioeconomy and moves on to the connections between methods, data and\nindicators. The present study aims to define the concepts of bioeconomy and to\nexplore the connections between concepts, methods, data and indicators when\nmeasuring the bioeconomy economically, and the limitations involved in this\nprocess. The bioeconomy concepts were defined based on a literature review and\na content analysis of 84 documents selected through snowballing procedures to\nfind articles measuring 'how big is the bioeconomy?'. The content of the 84\ndocuments was uploaded to the QDA Miner software and coded according to the\nbioeconomy concept, the methods or models used, the data sources accessed, the\nindicators calculated, and the limitations reported by the authors. The results\nof the occurrence and co-occurrence of the codes were extracted and analyzed\nstatistically, indicating that the measurement of bioeconomy (i) need recognize\nand pursue the proposed concept of holistic bioeconomy; (ii) rarely considered\naspects of holistic bioeconomy (3.5%); (iii) is primarily based on the concept\nof biomass-based bioeconomy (BmBB) (94%); (iv) the association with the concept\nof biosphere (BsBB) appeared in 26% of the studies; (v) the biotech-based\nbioeconomy (BtBB) was the least frequent (1.2%); (vi) there is a diversity of\nmethods and models, but the most common are those traditionally used to measure\nmacroeconomic activities, especially input-output models; (vii) depending on\nthe prevailing methods, the data comes from various official statistical\ndatabases, such as national accounts and economic activity classification\nsystems;...\n"
    },
    {
        "paper_id": 2403.09045,
        "authors": "Nail Kashaev, Martin Pl\\'avala, Victor H. Aguiar",
        "title": "Entangled vs. Separable Choice",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We investigate joint probabilistic choice rules describing the behavior of\ntwo decision makers, each facing potentially distinct menus. These rules are\nseparable when they can be decomposed into individual choices correlated solely\nthrough their respective probabilistic choice rules. Despite its significant\ninterest for the study of peer effects, influence, and taste variation, a\ncomplete characterization of these rules has remained elusive (Chambers,\nMasatlioglu, and Turansick, 2021). We fully characterize separable choices\nthrough a finite system of inequalities inspired by Afriat's theorem. Our\nresults address the possibility of entangled choices, where decision makers\nbehave as if they do not communicate, yet their choices are not separable. More\ngenerally, we establish that separable joint choice restrictions can be\nfactored into individual choice restrictions if only if at least one decision\nmaker's probabilistic choice rule uniquely identifies the distribution over\ndeterministic choice rules. The no communication condition and the individual\nrestrictions are no longer sufficient in the absence of this uniqueness. Our\nresults offer robust tools for distinguishing between separable decision-making\nand behaviors beyond mere peer effects such as imitation and cheating.\n"
    },
    {
        "paper_id": 2403.09138,
        "authors": "Aprodhita Anindya Putri, Akhmad Yunani",
        "title": "Study on Standardizing Working Time: A Case of XYZ Retail Store in\n  Bandung, Indonesia",
        "comments": "9 pages, 11 tables, and 3 figures",
        "journal-ref": "International Research Journal of Economics and Management\n  Studies, Vol. 3, No. 2, pp. 86-94, 2024",
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Work time standardization helps to find and reduce wasteful movements and\ntime in the workplace, such as chatting, mobile phone use, insufficient rest,\nor unproductive tasks. This study aims to map the process of displaying\nproducts from the warehouse to the shelves and calculate and determine the\nstandard working time of employees of the Operations Division of PT XYZ Branch\nwho oversee displaying X Milk and Y Bread. The data was collected six times in\nthree weeks, including interviews and observations, and took a sample of 20\npieces on each product to carry out data analysis such as data sufficiency\ntests and control charts. Several time deviations were found in the display\nprocess of X Milk products on all observation days in different activities.\nWhereas in the process of displaying Y Bread, only the deviation of working\ntime was found on the 4th observation day, which proves that the process needs\nto have a standard working time so that the activity work time is more\ncontrolled. Therefore, the analysis is carried out with the calculation of\nperformance rating, time allowance, normal time, and standard time. The result\nof the standard time calculation for the display process of X Milk products is\n15.83 minutes and Y Bread is 9.18 minutes for each product of 20 units.\n"
    },
    {
        "paper_id": 2403.09265,
        "authors": "Johannes Kn\\\"orr, Martin Bichler, Teodora Dobos",
        "title": "Zonal vs. Nodal Pricing: An Analysis of Different Pricing Rules in the\n  German Day-Ahead Market",
        "comments": "36 pages, 7 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  The European electricity market is based on large pricing zones with a\nuniform day-ahead price. The energy transition leads to changes in supply and\ndemand and increasing redispatch costs. In an attempt to ensure efficient\nmarket clearing and congestion management, the EU Commission has mandated the\nBidding Zone Review (BZR) to reevaluate the configuration of European bidding\nzones. Based on a unique data set published in the context of the BZR for the\ntarget year 2025, we compare various pricing rules for the German power market.\nWe compare market clearing and pricing for different zonal and nodal models,\nincluding their generation costs and associated redispatch costs. In numerical\nexperiments with this dataset, the differences in the average prices in\ndifferent zones are low. Congestion arises as well, but not necessarily on the\ncross-zonal interconnectors. The total costs across different configurations\nare similar and the reduction of standard deviations in prices is also small.\nThis might be different with other load and generation scenarios, but the BZR\ndata is important as it was created to make a decision about splits of the\nexisting bidding zones. Nodal pricing rules lead to the lowest total cost. We\nalso evaluate differences of nodal pricing rules with respect to the necessary\nuplift payments, which is relevant in the context of the current discussion on\nnon-uniform pricing in the EU. While the study focuses on Germany, the analysis\nis relevant beyond and feeds into the broader discussion about pricing rules in\nnon-convex markets.\n"
    },
    {
        "paper_id": 2403.09267,
        "authors": "Antonio Briola, Silvia Bartolucci, Tomaso Aste",
        "title": "Deep Limit Order Book Forecasting",
        "comments": "43 pages, 14 figures, 12 Tables",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  We exploit cutting-edge deep learning methodologies to explore the\npredictability of high-frequency Limit Order Book mid-price changes for a\nheterogeneous set of stocks traded on the NASDAQ exchange. In so doing, we\nrelease `LOBFrame', an open-source code base to efficiently process large-scale\nLimit Order Book data and quantitatively assess state-of-the-art deep learning\nmodels' forecasting capabilities. Our results are twofold. We demonstrate that\nthe stocks' microstructural characteristics influence the efficacy of deep\nlearning methods and that their high forecasting power does not necessarily\ncorrespond to actionable trading signals. We argue that traditional machine\nlearning metrics fail to adequately assess the quality of forecasts in the\nLimit Order Book context. As an alternative, we propose an innovative\noperational framework that evaluates predictions' practicality by focusing on\nthe probability of accurately forecasting complete transactions. This work\noffers academics and practitioners an avenue to make informed and robust\ndecisions on the application of deep learning techniques, their scope and\nlimitations, effectively exploiting emergent statistical properties of the\nLimit Order Book.\n"
    },
    {
        "paper_id": 2403.09272,
        "authors": "Maximilian Stargardt (1,2), David Kress (1), Heidi Heinrichs (1),\n  J\\\"orn-Christian Meyer (3), Jochen Lin{\\ss}en (1), Grit Walther (3) and\n  Detlef Stolten (1,2) ((1) Forschungszentrum J\\\"ulich GmbH, Institute of\n  Energy and Climate Research - Techno-economic Systems Analysis (IEK-3),\n  J\\\"ulich, Germany (2) RWTH Aachen University, Chair of Fuel Cells, Faculty of\n  Mechanical Engineering, Aachen, Germany (3) RWTH Aachen University, Chair of\n  Operations Management, Schoolf of Business and Economics, Aachen, Germany)",
        "title": "Global Shipyard Capacities Limiting the Ramp-Up of Global Hydrogen\n  Transport",
        "comments": "Number of pages:26 + 4 pages Appendix; Number of figures: 10",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Decarbonizing the global energy system requires significant expansions of\nrenewable energy technologies. Given that cost-effective renewable sources are\nnot necessarily situated in proximity to the largest energy demand centers\nglobally, the maritime transportation of low-carbon energy carriers, such as\nrenewable-based hydrogen or ammonia, will be needed. However, whether existent\nshipyards possess the required capacity to provide the necessary global fleet\nhas not yet been answered. Therefore, this study estimates global tanker demand\nbased on projections for global hydrogen demand, while comparing these\nprojections with historic shipyard production. Our findings reveal a potential\nbottleneck until 2033-2039 if relying on liquefied hydrogen exclusively. This\nbottleneck could be circumvented by increasing local hydrogen production,\nutilizing pipelines, or liquefied ammonia as an energy carrier for hydrogen.\nFurthermore, the regional concentration of shipyard locations raises concerns\nabout diversification. Increasing demand for container vessels could\nsubstantially hinder the scale-up of maritime hydrogen transport.\n"
    },
    {
        "paper_id": 2403.0947,
        "authors": "Marco Letta, Pierluigi Montalbano, Adriana Paolantonio",
        "title": "Climate Immobility Traps: A Household-Level Test",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  The complex relationship between climate shocks, migration, and adaptation\nhampers a rigorous understanding of the heterogeneous mobility outcomes of farm\nhouseholds exposed to climate risk. To unpack this heterogeneity, the analysis\ncombines longitudinal multi-topic household survey data from Nigeria with a\ncausal machine learning approach, tailored to a conceptual framework bridging\neconomic migration theory and the poverty traps literature. The results show\nthat pre-shock asset levels, in situ adaptive capacity, and cumulative shock\nexposure drive not just the magnitude but also the sign of the impact of\nagriculture-relevant weather anomalies on the mobility outcomes of farming\nhouseholds. While local adaptation acts as a substitute for migration, the\nroles played by wealth constraints and repeated shock exposure suggest the\npresence of climate-induced immobility traps.\n"
    },
    {
        "paper_id": 2403.09494,
        "authors": "Austin Adams",
        "title": "Layer 2 be or Layer not 2 be: Scaling on Uniswap v3",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  This paper studies the market structure impact of cheaper and faster chains\non the Uniswap v3 Protocol. The Uniswap Protocol is the largest decentralized\napplication on Ethereum by both gas and blockspace used, and user behaviors of\nthe protocol are very sensitive to fluctuations in gas prices and market\nstructure due to the economic factors of the Protocol. We focus on the chains\nwhere Uniswap v3 has the most activity, giving us the best comparison to\nEthereum mainnet. Because of cheaper gas and lower block times, we find\nevidence that the majority of swaps get better gas-adjusted execution on these\nchains, liquidity providers are more capital efficient, and liquidity providers\nhave increased fee returns from more arbitrage. We also present evidence that\ntwo second block times may be too long for optimal liquidity provider returns,\ncompared to first come, first served. We argue that many of the current\ndrawbacks with AMMs may be due to chain dynamics and are vastly improved with\ncheaper and faster transactions\n"
    },
    {
        "paper_id": 2403.09532,
        "authors": "Ariel Neufeld, Matthew Ng Cheng En, Ying Zhang",
        "title": "Robust SGLD algorithm for solving non-convex distributionally robust\n  optimisation problems",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper we develop a Stochastic Gradient Langevin Dynamics (SGLD)\nalgorithm tailored for solving a certain class of non-convex distributionally\nrobust optimisation problems. By deriving non-asymptotic convergence bounds, we\nbuild an algorithm which for any prescribed accuracy $\\varepsilon>0$ outputs an\nestimator whose expected excess risk is at most $\\varepsilon$. As a concrete\napplication, we employ our robust SGLD algorithm to solve the (regularised)\ndistributionally robust Mean-CVaR portfolio optimisation problem using real\nfinancial data. We empirically demonstrate that the trading strategy obtained\nby our robust SGLD algorithm outperforms the trading strategy obtained when\nsolving the corresponding non-robust Mean-CVaR portfolio optimisation problem\nusing, e.g., a classical SGLD algorithm. This highlights the practical\nrelevance of incorporating model uncertainty when optimising portfolios in real\nfinancial markets.\n"
    },
    {
        "paper_id": 2403.09761,
        "authors": "Alexander Lipton",
        "title": "Hydrodynamics of Markets:Hidden Links Between Physics and Finance",
        "comments": "123 pages, 14 figures. arXiv admin note: text overlap with\n  arXiv:2309.04547",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  An intriguing link between a wide range of problems occurring in physics and\nfinancial engineering is presented. These problems include the evolution of\nsmall perturbations of linear flows in hydrodynamics, the movements of\nparticles in random fields described by the Kolmogorov and Klein-Kramers\nequations, the Ornstein-Uhlenbeck and Feller processes, and their\ngeneralizations. They are reduced to affine differential and\npseudo-differential equations and solved in a unified way by using Kelvin waves\nand developing a comprehensive math framework for calculating transition\nprobabilities and expectations. Kelvin waves are instrumental for studying the\nwell-known Black-Scholes, Heston, and Stein-Stein models and more complex\npath-dependent volatility models, as well as the pricing of Asian options,\nvolatility and variance swaps, bonds, and bond options. Kelvin waves help to\nsolve several cutting-edge problems, including hedging the impermanent loss of\nAutomated Market Makers for cryptocurrency trading. This title is also\navailable as Open Access on Cambridge Core.\n"
    },
    {
        "paper_id": 2403.09899,
        "authors": "Shawn Berry",
        "title": "Prediction of retail chain failure: examples of recent U.S. retail\n  failures",
        "comments": "1 figure, 7 tables, 13 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  Over the last several years, several well-established and prominent\nbrick-and-mortar retail chains have ceased operations, raising concerns for\nsomething that some have referred to as a retail apocalypse. While the demise\nof brick-and-mortar is far from certain, scholars have attempted to model the\nlikelihood that a retailer is about to fail using different approaches. This\npaper examines the failures of Bed Bath and Beyond, J.C. Penney, Rite Aid, and\nSears Holdings in the United States between 2013 and 2022. A model of retail\nfailure is presented that considers internal and external firm factors using\nboth annual report and macroeconomic data. The findings suggest that certain\nrevenue-based financial ratios and the annual average U.S. inflation rates are\nstatistically significant predictors of failure. Furthermore, the failure model\ndemonstrated that it can provide a nontrivial early warning signal at least the\nyear before failure. The paper concludes with a discussion and directions for\nfuture research.\n"
    },
    {
        "paper_id": 2403.10273,
        "authors": "Eduardo Abi Jaber, Eyal Neuman and Sturmius Tuschmann",
        "title": "Optimal Portfolio Choice with Cross-Impact Propagators",
        "comments": "44 pages, 5 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  We consider a class of optimal portfolio choice problems in continuous time\nwhere the agent's transactions create both transient cross-impact driven by a\nmatrix-valued Volterra propagator, as well as temporary price impact. We\nformulate this problem as the maximization of a revenue-risk functional, where\nthe agent also exploits available information on a progressively measurable\nprice predicting signal. We solve the maximization problem explicitly in terms\nof operator resolvents, by reducing the corresponding first order condition to\na coupled system of stochastic Fredholm equations of the second kind and\nderiving its solution. We then give sufficient conditions on the matrix-valued\npropagator so that the model does not permit price manipulation. We also\nprovide an implementation of the solutions to the optimal portfolio choice\nproblem and to the associated optimal execution problem. Our solutions yield\nfinancial insights on the influence of cross-impact on the optimal strategies\nand its interplay with alpha decays.\n"
    },
    {
        "paper_id": 2403.10441,
        "authors": "Guanxing Fu, Paul P. Hager, Ulrich Horst",
        "title": "A Mean-Field Game of Market Entry: Portfolio Liquidation with Trading\n  Constraints",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  We consider both $N$-player and mean-field games of optimal portfolio\nliquidation in which the players are not allowed to change the direction of\ntrading. Players with an initially short position of stocks are only allowed to\nbuy while players with an initially long position are only allowed to sell the\nstock. Under suitable conditions on the model parameters we show that the games\nare equivalent to games of timing where the players need to determine the\noptimal times of market entry and exit. We identify the equilibrium entry and\nexit times and prove that equilibrium mean-trading rates can be characterized\nin terms of the solutions to a highly non-linear higher-order integral equation\nwith endogenous terminal condition. We prove the existence of a unique solution\nto the integral equation from which we obtain the existence of a unique\nequilibrium both in the mean-field and the $N$-player game.\n"
    },
    {
        "paper_id": 2403.10482,
        "authors": "Bruno de Melo, Jamiel Sheikh",
        "title": "Can a GPT4-Powered AI Agent Be a Good Enough Performance Attribution\n  Analyst?",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
        "abstract": "  Performance attribution analysis, defined as the process of explaining the\ndrivers of the excess performance of an investment portfolio against a\nbenchmark, stands as a significant feature of portfolio management and plays a\ncrucial role in the investment decision-making process, particularly within the\nfund management industry. Rooted in a solid financial and mathematical\nframework, the importance and methodologies of this analytical technique are\nextensively documented across numerous academic research papers and books. The\nintegration of large language models (LLMs) and AI agents marks a\ngroundbreaking development in this field. These agents are designed to automate\nand enhance the performance attribution analysis by accurately calculating and\nanalyzing portfolio performances against benchmarks. In this study, we\nintroduce the application of an AI Agent for a variety of essential performance\nattribution tasks, including the analysis of performance drivers and utilizing\nLLMs as calculation engine for multi-level attribution analysis and\nquestion-answering (QA) tasks. Leveraging advanced prompt engineering\ntechniques such as Chain-of-Thought (CoT) and Plan and Solve (PS), and\nemploying a standard agent framework from LangChain, the research achieves\npromising results: it achieves accuracy rates exceeding 93% in analyzing\nperformance drivers, attains 100% in multi-level attribution calculations, and\nsurpasses 84% accuracy in QA exercises that simulate official examination\nstandards. These findings affirm the impactful role of AI agents, prompt\nengineering and evaluation in advancing portfolio management processes,\nhighlighting a significant development in the practical application and\nevaluation of Generative AI technologies within the domain.\n"
    },
    {
        "paper_id": 2403.10614,
        "authors": "Pierre-Loup Beauregard",
        "title": "Gentrification, displacement, and income trajectory of incumbents",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Gentrification is associated with rapid demographic changes within inner-city\nneighborhoods. While many fear that gentrification drives low-income people\nfrom their homes and communities, there is limited evidence of the consequences\nof these changes. I use Canadian administrative tax files to track the\nmovements of incumbent workers and their income trajectory as their\nneighborhood gentrifies. I exploit the timing at which neighborhoods gentrify\nin a matched staggered event-study framework. I find no evidence of\ndisplacement effects, even for low socioeconomic status households. In fact,\nfamilies living in gentrifying neighborhoods are more likely to stay longer. I\nsuggest that this might be related to tenant rights protection laws. When they\nendogenously decide to leave, low-income families do not relocate to worse\nneighborhoods. Finally, I find no adverse effects on their income trajectory,\nsuggesting no repercussions on their labor market outcomes.\n"
    },
    {
        "paper_id": 2403.10631,
        "authors": "Giuseppe Calafiore, Giulia Fracastoro, Anton Proskurnikov",
        "title": "Default Resilience and Worst-Case Effects in Financial Networks",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  In this paper we analyze the resilience of a network of banks to joint price\nfluctuations of the external assets in which they have shared exposures, and\nevaluate the worst-case effects of the possible default contagion. Indeed, when\nthe prices of certain external assets either decrease or increase, all banks\nexposed to them experience varying degrees of simultaneous shocks to their\nbalance sheets. These coordinated and structured shocks have the potential to\nexacerbate the likelihood of defaults. In this context, we introduce first a\nconcept of {default resilience margin}, $\\epsilon^*$, i.e., the maximum\namplitude of asset prices fluctuations that the network can tolerate without\ngenerating defaults. Such threshold value is computed by considering two\ndifferent measures of price fluctuations, one based on the maximum individual\nvariation of each asset, and the other based on the sum of all the asset's\nabsolute variations. For any price perturbation having amplitude no larger than\n$\\epsilon^*$, the network absorbs the shocks remaining default free. When the\nperturbation amplitude goes beyond $\\epsilon^*$, however, defaults may occur.\nIn this case we find the worst-case systemic loss, that is, the total unpaid\ndebt under the most severe price variation of given magnitude. Computation of\nboth the threshold level $\\epsilon^*$ and of the worst-case loss and of a\ncorresponding worst-case asset price scenario, amounts to solving suitable\nlinear programming problems.}\n"
    },
    {
        "paper_id": 2403.10636,
        "authors": "Geoff Boeing, Jaehyun Ha",
        "title": "Resilient by Design: Simulating Street Network Disruptions across Every\n  Urban Area in the World",
        "comments": null,
        "journal-ref": "Transportation Research Part A: Policy and Practice, 2024",
        "doi": "10.1016/j.tra.2024.104016",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Street networks allow people and goods to move through cities, but they are\nvulnerable to disasters like floods, earthquakes, and terrorist attacks.\nWell-planned network design can make a city more resilient and robust to such\ndisruptions, but we still know little about worldwide patterns of\nvulnerability, or worldwide empirical relationships between specific design\ncharacteristics and resilience. This study quantifies and measures the\nvulnerability of the street networks of every urban area in the world then\nmodels the relationships between vulnerability and street network design\ncharacteristics. To do so, we simulate over 2.4 billion trips across more than\n8,000 urban areas in 178 countries, while also simulating network disruption\nevents representing floods, earthquakes, and targeted attacks. We find that\ndisrupting high-centrality nodes severely impacts network function. All else\nequal, networks with higher connectivity, fewer chokepoints, or less circuity\nare less vulnerable to disruption's impacts. This study thus contributes a new\nglobal understanding of network design and vulnerability to the literature. We\nargue that these design characteristics offer high leverage points for street\nnetwork resilience and robustness that planners should emphasize when designing\nor retrofitting urban networks.\n"
    },
    {
        "paper_id": 2403.10652,
        "authors": "Cecilia Ying, Stephen Thomas",
        "title": "Improving Fairness in Credit Lending Models using Subgroup Threshold\n  Optimization",
        "comments": "Neural Information Processing Systems (NeurIPS) Workshop in Strategic\n  ML",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  In an effort to improve the accuracy of credit lending decisions, many\nfinancial intuitions are now using predictions from machine learning models.\nWhile such predictions enjoy many advantages, recent research has shown that\nthe predictions have the potential to be biased and unfair towards certain\nsubgroups of the population. To combat this, several techniques have been\nintroduced to help remove the bias and improve the overall fairness of the\npredictions. We introduce a new fairness technique, called \\textit{Subgroup\nThreshold Optimizer} (\\textit{STO}), that does not require any alternations to\nthe input training data nor does it require any changes to the underlying\nmachine learning algorithm, and thus can be used with any existing machine\nlearning pipeline. STO works by optimizing the classification thresholds for\nindividual subgroups in order to minimize the overall discrimination score\nbetween them. Our experiments on a real-world credit lending dataset show that\nSTO can reduce gender discrimination by over 90\\%.\n"
    },
    {
        "paper_id": 2403.10916,
        "authors": "Moseli Mots'oehli and Anton Nikolaev and Wawan B. IGede and John\n  Lynham and Peter J. Mous and Peter Sadowski",
        "title": "FishNet: Deep Neural Networks for Low-Cost Fish Stock Estimation",
        "comments": "IEEE COINS 2024",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Fish stock assessment often involves manual fish counting by taxonomy\nspecialists, which is both time-consuming and costly. We propose FishNet, an\nautomated computer vision system for both taxonomic classification and fish\nsize estimation from images captured with a low-cost digital camera. The system\nfirst performs object detection and segmentation using a Mask R-CNN to identify\nindividual fish from images containing multiple fish, possibly consisting of\ndifferent species. Then each fish species is classified and the length is\npredicted using separate machine learning models. To develop the model, we use\na dataset of 300,000 hand-labeled images containing 1.2M fish of 163 different\nspecies and ranging in length from 10cm to 250cm, with additional annotations\nand quality control methods used to curate high-quality training data. On\nheld-out test data sets, our system achieves a 92% intersection over union on\nthe fish segmentation task, a 89% top-1 classification accuracy on single fish\nspecies classification, and a 2.3cm mean absolute error on the fish length\nestimation task.\n"
    },
    {
        "paper_id": 2403.10982,
        "authors": "Panteleimon Kruglov, Charles Shaw",
        "title": "Financial Performance and Innovation: Evidence From USA, 1998-2023",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  This study explores the relationship between R&D intensity, as a measure of\ninnovation, and financial performance among S&P 500 companies over 100 quarters\nfrom 1998 to 2023, including multiple crisis periods. It challenges the\nconventional wisdom that larger companies are more prone to innovate, using a\ncomprehensive dataset across various industries. The analysis reveals diverse\nassociations between innovation and key financial indicators such as firm size,\nassets, EBITDA, and tangibility. Our findings underscore the importance of\ninnovation in enhancing firm competitiveness and market positioning,\nhighlighting the effectiveness of countercyclical innovation policies. This\nresearch contributes to the debate on the role of R&D investments in driving\nfirm value, offering new insights for both academic and policy discussions.\n"
    },
    {
        "paper_id": 2403.1101,
        "authors": "Klaus Altendorfer, Wolfgang Seiringer, Thomas Felberbauer, Balwin\n  Bokor, Fabian Brockmann",
        "title": "How Periodic Forecast Updates Influence MRP Planning Parameters: A\n  Simulation Study",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  In many supply chains, the current efforts at digitalization have led to\nimproved information exchanges between manufacturers and their customers.\nSpecifically, demand forecasts are often provided by the customers and\nregularly updated as the related customer information improves. In this paper,\nwe investigate the influence of forecast updates on the production planning\nmethod of Material Requirements Planning (MRP). A simulation study was carried\nout to assess how updates in information affect the setting of planning\nparameters in a rolling horizon MRP planned production system. An intuitive\nresult is that information updates lead to disturbances in the production\norders for the MRP standard, and, therefore, an extension for MRP to mitigate\nthese effects is developed. A large numerical simulation experiment shows that\nthe MRP safety stock exploitation heuristic, that has been developed, leads to\nsignificantly improved results as far as inventory and backorder costs are\nconcerned. An interesting result is that the fixed-order-quantity lotsizing\npolicy performs - in most instances - better than the fixed-order-period\nlotsizing policy, when periodic forecast updates occur. In addition, the\nsimulation study shows that underestimating demand is marginally more costly\nthan overestimating it, based on the comparative analysis of all instances.\nFurthermore, the results indicate that the MRP safety stock exploitation\nheuristic can mitigate the negative effects of biased forecasts.\n"
    },
    {
        "paper_id": 2403.11028,
        "authors": "David B. McMillon",
        "title": "What Makes Systemic Discrimination, \"Systemic?\" Exposing the Amplifiers\n  of Inequity",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Drawing on work spanning economics, public health, education, sociology, and\nlaw, I formalize theoretically what makes systemic discrimination \"systemic.\"\nInjustices do not occur in isolation, but within a complex system of\ninterdependent factors; and their effects may amplify as a consequence. I\ndevelop a taxonomy of these amplification mechanisms, connecting them to\nwell-understood concepts in economics that are precise, testable and\npolicy-oriented. This framework reveals that these amplification mechanisms can\neither be directly disrupted, or exploited to amplify the effects of\nequity-focused interventions instead. In other words, it shows how to use the\nmachinery of systemic discrimination against itself. Real-world examples\ndiscussed include but are not limited to reparations for slavery and Jim Crow,\nvouchers or place-based neighborhood interventions, police shootings,\naffirmative action, and Covid-19.\n"
    },
    {
        "paper_id": 2403.11622,
        "authors": "Michele Azzone, Emilio Barucci and Davide Stocco",
        "title": "Asset management with an ESG mandate",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We investigate the portfolio frontier and risk premia in equilibrium when an\ninstitutional investor aims to minimize the tracking error variance and to\nattain an ESG score higher than the benchmark's one (ESG mandate). Provided\nthat a negative ESG premium for stocks is priced by the market, we show that an\nESG mandate can reduce the mean-variance inefficiency of the portfolio frontier\nwhen the asset manager targets a limited over-performance with respect to the\nbenchmark. Instead, for a high over-performance target, an ESG mandate leads to\na higher variance. The mean-variance improvement is due to the fact that the\nESG mandate induces the asset manager to over-invest in assets with a high\nmean-standard deviation ratio. In equilibrium, with asset managers and\nmean-variance investors, a negative ESG premium arises if the ESG mandate is\nbinding for asset managers. A result that is borne out by the data.\n"
    },
    {
        "paper_id": 2403.1168,
        "authors": "Viktoras Kulionis, Andreas Froemelt, Stephan Pfister",
        "title": "Multiscale Orientation Values for Biodiversity, Climate and Water: A\n  Scientific Input for Science- Based Targets",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  In this study, we explore a range of options and outcomes associated with\nusing different allocation approaches to operationalise the Planetary\nBoundaries (PB) framework at the country, sector, and city scales. We\ndemonstrate: (i) how to translate the PB framework into various sub-global\nscales (countries, cities, industries); and (ii) how to take global/local\naspects (e.g., water use at the watershed level) into account. Finally, we\napply the proposed methodology to derive country, city, and sector-specific\nbudgets consistent with the PB concept for Switzerland. We then benchmark the\ntranslated PBs for climate, biodiversity, and freshwater use against actual\nenvironmental pressures in Switzerland from both production- and\nconsumption-based perspectives. This effectively enables us to provide a\ncomprehensive assessment of whether Switzerland is living within its safe\noperating space.\n"
    },
    {
        "paper_id": 2403.11738,
        "authors": "Alexandre Pannier and Cristopher Salvi",
        "title": "A path-dependent PDE solver based on signature kernels",
        "comments": "35 pages, 1 figure",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We develop a provably convergent kernel-based solver for path-dependent PDEs\n(PPDEs). Our numerical scheme leverages signature kernels, a recently\nintroduced class of kernels on path-space. Specifically, we solve an optimal\nrecovery problem by approximating the solution of a PPDE with an element of\nminimal norm in the signature reproducing kernel Hilbert space (RKHS)\nconstrained to satisfy the PPDE at a finite collection of collocation paths. In\nthe linear case, we show that the optimisation has a unique closed-form\nsolution expressed in terms of signature kernel evaluations at the collocation\npaths. We prove consistency of the proposed scheme, guaranteeing convergence to\nthe PPDE solution as the number of collocation points increases. Finally,\nseveral numerical examples are presented, in particular in the context of\noption pricing under rough volatility. Our numerical scheme constitutes a valid\nalternative to the ubiquitous Monte Carlo methods.\n"
    },
    {
        "paper_id": 2403.11824,
        "authors": "Laurence Carassus and Massinissa Ferhoune",
        "title": "Nonconcave Robust Utility Maximization under Projective Determinacy",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We study a robust utility maximization problem in a general discrete-time\nfrictionless market. The investor is assumed to have a random, nonconcave and\nnondecreasing utility function, which may or may not be finite on the whole\nreal-line. She also faces model ambiguity on her beliefs about the market,\nwhich is modeled through a set of priors. We prove, using only primal methods,\nthe existence of an optimal investment strategy when the utility function is\nalso upper-semicontinuous. For that, we introduce the new notion of\nprojectively measurable functions. We show basic properties of these functions\nas stability under sums, differences, products, suprema, infima and\ncompositions but also assuming the set-theoretical axiom of Projective\nDeterminacy (PD) stability under integration and existence of\n$\\epsilon$-optimal selectors. We consider projectively measurable random\nutility function and price process and assume that the graphs of the sets of\nlocal priors are projective sets. Our other assumptions are stated on a\nprior-by-prior basis and correspond to generally accepted assumptions in the\nliterature on markets without ambiguity.\n"
    },
    {
        "paper_id": 2403.11897,
        "authors": "Ofelia Bonesini, Antoine Jacquier, Aitor Muguruza",
        "title": "Risk premium and rough volatility",
        "comments": "17 pages, 6 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  One the one hand, rough volatility has been shown to provide a consistent\nframework to capture the properties of stock price dynamics both under the\nhistorical measure and for pricing purposes. On the other hand, market price of\nvolatility risk is a well-studied object in Financial Economics, and empirical\nestimates show it to be stochastic rather than deterministic. Starting from a\nrough volatility model under the historical measure, we take up this challenge\nand provide an analysis of the impact of such a non-deterministic risk for\npricing purposes.\n"
    },
    {
        "paper_id": 2403.12078,
        "authors": "Hiroki Masuda and Lorenzo Mercuri and Yuma Uehara",
        "title": "Student t-L\\'evy regression model in YUIMA",
        "comments": "arXiv admin note: substantial text overlap with arXiv:2306.16790",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  The aim of this paper is to discuss an estimation and a simulation method in\nthe \\textsf{R} package YUIMA for a linear regression model driven by a\nStudent-$t$ L\\'evy process with constant scale and arbitrary degrees of\nfreedom. This process finds applications in several fields, for example\nfinance, physic, biology, etc. The model presents two main issues. The first is\nrelated to the simulation of a sample path at high-frequency level. Indeed,\nonly the $t$-L\\'evy increments defined on an unitary time interval are\nStudent-$t$ distributed. In YUIMA, we solve this problem by means of the\ninverse Fourier transform for simulating the increments of a Student-$t$\nL\\'{e}vy defined on a interval with any length. A second problem is due to the\nfact that joint estimation of trend, scale, and degrees of freedom does not\nseem to have been investigated as yet. In YUIMA, we develop a two-step\nestimation procedure that efficiently deals with this issue. Numerical examples\nare given in order to explain methods and classes used in the YUIMA package.\n"
    },
    {
        "paper_id": 2403.12107,
        "authors": "Anton Korinek and Donghyun Suh",
        "title": "Scenarios for the Transition to AGI",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We analyze how output and wages behave under different scenarios for\ntechnological progress that may culminate in Artificial General Intelligence\n(AGI), defined as the ability of AI systems to perform all tasks that humans\ncan perform. We assume that human work can be decomposed into atomistic tasks\nthat differ in their complexity. Advances in technology make ever more complex\ntasks amenable to automation. The effects on wages depend on a race between\nautomation and capital accumulation. If the distribution of task complexity\nexhibits a sufficiently thick infinite tail, then there is always enough work\nfor humans, and wages may rise forever. By contrast, if the complexity of tasks\nthat humans can perform is bounded and full automation is reached, then wages\ncollapse. But declines may occur even before if large-scale automation outpaces\ncapital accumulation and makes labor too abundant. Automating productivity\ngrowth may lead to broad-based gains in the returns to all factors. By\ncontrast, bottlenecks to growth from irreproducible scarce factors may\nexacerbate the decline in wages.\n"
    },
    {
        "paper_id": 2403.12108,
        "authors": "Eli Ben-Michael, D. James Greiner, Melody Huang, Kosuke Imai, Zhichao\n  Jiang, Sooahn Shin",
        "title": "Does AI help humans make better decisions? A methodological framework\n  for experimental evaluation",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  The use of Artificial Intelligence (AI) based on data-driven algorithms has\nbecome ubiquitous in today's society. Yet, in many cases and especially when\nstakes are high, humans still make final decisions. The critical question,\ntherefore, is whether AI helps humans make better decisions as compared to a\nhuman alone or AI an alone. We introduce a new methodological framework that\ncan be used to answer experimentally this question with no additional\nassumptions. We measure a decision maker's ability to make correct decisions\nusing standard classification metrics based on the baseline potential outcome.\nWe consider a single-blinded experimental design, in which the provision of\nAI-generated recommendations is randomized across cases with a human making\nfinal decisions. Under this experimental design, we show how to compare the\nperformance of three alternative decision-making systems--human-alone,\nhuman-with-AI, and AI-alone. We apply the proposed methodology to the data from\nour own randomized controlled trial of a pretrial risk assessment instrument.\nWe find that AI recommendations do not improve the classification accuracy of a\njudge's decision to impose cash bail. Our analysis also shows that AI-alone\ndecisions generally perform worse than human decisions with or without AI\nassistance. Finally, AI recommendations tend to impose cash bail on non-white\narrestees more often than necessary when compared to white arrestees.\n"
    },
    {
        "paper_id": 2403.12161,
        "authors": "Arijit Das, Tanmoy Nandi, Prasanta Saha, Suman Das, Saronyo Mukherjee,\n  Sudip Kumar Naskar, Diganta Saha",
        "title": "Effect of Leaders Voice on Financial Market: An Empirical Deep Learning\n  Expedition on NASDAQ, NSE, and Beyond",
        "comments": "20 pages original research",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
        "abstract": "  Financial market like the price of stock, share, gold, oil, mutual funds are\naffected by the news and posts on social media. In this work deep learning\nbased models are proposed to predict the trend of financial market based on NLP\nanalysis of the twitter handles of leaders of different fields. There are many\nmodels available to predict financial market based on only the historical data\nof the financial component but combining historical data with news and posts of\nthe social media like Twitter is the main objective of the present work.\nSubstantial improvement is shown in the result. The main features of the\npresent work are: a) proposing completely generalized algorithm which is able\nto generate models for any twitter handle and any financial component, b)\npredicting the time window for a tweets effect on a stock price c) analyzing\nthe effect of multiple twitter handles for predicting the trend. A detailed\nsurvey is done to find out the latest work in recent years in the similar\nfield, find the research gap, and collect the required data for analysis and\nprediction. State-of-the-art algorithm is proposed and complete implementation\nwith environment is given. An insightful trend of the result improvement\nconsidering the NLP analysis of twitter data on financial market components is\nshown. The Indian and USA financial markets are explored in the present work\nwhere as other markets can be taken in future. The socio-economic impact of the\npresent work is discussed in conclusion.\n"
    },
    {
        "paper_id": 2403.1218,
        "authors": "Boming Ning, Kiseop Lee",
        "title": "Advanced Statistical Arbitrage with Reinforcement Learning",
        "comments": "14 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Statistical arbitrage is a prevalent trading strategy which takes advantage\nof mean reverse property of spread of paired stocks. Studies on this strategy\noften rely heavily on model assumption. In this study, we introduce an\ninnovative model-free and reinforcement learning based framework for\nstatistical arbitrage. For the construction of mean reversion spreads, we\nestablish an empirical reversion time metric and optimize asset coefficients by\nminimizing this empirical mean reversion time. In the trading phase, we employ\na reinforcement learning framework to identify the optimal mean reversion\nstrategy. Diverging from traditional mean reversion strategies that primarily\nfocus on price deviations from a long-term mean, our methodology creatively\nconstructs the state space to encapsulate the recent trends in price movements.\nAdditionally, the reward function is carefully tailored to reflect the unique\ncharacteristics of mean reversion trading.\n"
    },
    {
        "paper_id": 2403.12285,
        "authors": "Thanos Konstantinidis, Giorgos Iacovides, Mingxue Xu, Tony G.\n  Constantinides, Danilo Mandic",
        "title": "FinLlama: Financial Sentiment Classification for Algorithmic Trading\n  Applications",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  There are multiple sources of financial news online which influence market\nmovements and trader's decisions. This highlights the need for accurate\nsentiment analysis, in addition to having appropriate algorithmic trading\ntechniques, to arrive at better informed trading decisions. Standard lexicon\nbased sentiment approaches have demonstrated their power in aiding financial\ndecisions. However, they are known to suffer from issues related to context\nsensitivity and word ordering. Large Language Models (LLMs) can also be used in\nthis context, but they are not finance-specific and tend to require significant\ncomputational resources. To facilitate a finance specific LLM framework, we\nintroduce a novel approach based on the Llama 2 7B foundational model, in order\nto benefit from its generative nature and comprehensive language manipulation.\nThis is achieved by fine-tuning the Llama2 7B model on a small portion of\nsupervised financial sentiment analysis data, so as to jointly handle the\ncomplexities of financial lexicon and context, and further equipping it with a\nneural network based decision mechanism. Such a generator-classifier scheme,\nreferred to as FinLlama, is trained not only to classify the sentiment valence\nbut also quantify its strength, thus offering traders a nuanced insight into\nfinancial news articles. Complementing this, the implementation of\nparameter-efficient fine-tuning through LoRA optimises trainable parameters,\nthus minimising computational and memory requirements, without sacrificing\naccuracy. Simulation results demonstrate the ability of the proposed FinLlama\nto provide a framework for enhanced portfolio management decisions and\nincreased market returns. These results underpin the ability of FinLlama to\nconstruct high-return portfolios which exhibit enhanced resilience, even during\nvolatile periods and unpredictable market events.\n"
    },
    {
        "paper_id": 2403.12647,
        "authors": "Shige Peng, Shuzhen Yang, Wenqing Zhang",
        "title": "Uncertainty in the financial market and application to forecastabnormal\n  financial fluctuations",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The integration and innovation of finance and technology have gradually\ntransformed the financial system into a complex one. Analyses of the causesd of\nabnormal fluctuations in the financial market to extract early warning\nindicators revealed that most early warning systems are qualitative and causal.\nHowever, these models cannot be used to forecast the risk of the financial\nmarket benchmark. Therefore, from a quantitative analysis perspective, we focus\non the mean and volatility uncertainties of the stock index (benchmark) and\nthen construct three early warning indicators: mean uncertainty, volatility\nuncertainty, and ALM-G-value at risk. Based on the novel warning indicators, we\nestablish a new abnormal fluctuations warning model, which will provide a\nshort-term warning for the country, society, and individuals to reflect in\nadvance.\n"
    },
    {
        "paper_id": 2403.12653,
        "authors": "Mikkel Bennedsen and Kim Christensen and Peter Christensen",
        "title": "Composite likelihood estimation of stationary Gaussian processes with a\n  view toward stochastic volatility",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  We develop a framework for composite likelihood inference of parametric\ncontinuous-time stationary Gaussian processes. We derive the asymptotic theory\nof the associated maximum composite likelihood estimator. We implement our\napproach on a pair of models that has been proposed to describe the random\nlog-spot variance of financial asset returns. A simulation study shows that it\ndelivers good performance in these settings and improves upon a\nmethod-of-moments estimation. In an application, we inspect the dynamic of an\nintraday measure of spot variance computed with high-frequency data from the\ncryptocurrency market. The empirical evidence supports a mechanism, where the\nshort- and long-term correlation structure of stochastic volatility are\ndecoupled in order to capture its properties at different time scales.\n"
    },
    {
        "paper_id": 2403.12694,
        "authors": "Julius D\\\"uker and Alexander Rieber",
        "title": "Performance, Knowledge Acquisition and Satisfaction in Self-selected\n  Groups: Evidence from a Classroom Field Experiment",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-sa/4.0/",
        "abstract": "  We investigate how to efficiently set up work groups to boost group\nproductivity, individual satisfaction, and learning. Therefore, we conduct a\nnatural field experiment in a compulsory undergraduate course and study\ndifferences between self-selected and randomly assigned groups. We find that\nself-selected groups perform significantly worse on group assignments. Yet,\nstudents in self-selected groups learn more and are more satisfied than those\nin randomly assigned groups. The effect of allowing students to pick group\nmembers dominates the effect of different group compositions in self-selected\ngroups: When controlling for the skill, gender, and home region composition of\ngroups, the differences between self-selected and randomly formed groups\npersist almost unaltered. The distribution of GitHub commits per group reveals\nthat the better average performance of randomly assigned groups is mainly\ndriven by highly skilled individuals distributed over more groups due to the\nassignment mechanism. Moreover, these highly skilled individuals contribute\nmore to the group in randomly formed groups. We argue that this mechanism\nexplains why self-selected groups perform worse on the projects but acquire\nmore knowledge than randomly formed groups. These findings are relevant for\nsetting up workgroups in academic, business, and governmental organizations\nwhen tasks are not constrained to the skill set of specific individuals.\n"
    },
    {
        "paper_id": 2403.13138,
        "authors": "Christopher Chambers, Alan Miller, Ruodu Wang, Qinyu Wu",
        "title": "Max-stability under first-order stochastic dominance",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Max-stability is the property that taking a maximum between two inputs\nresults in a maximum between two outputs. We investigate max-stability with\nrespect to first-order stochastic dominance, the most fundamental notion of\nstochastic dominance in decision theory. Under two additional standard axioms\nof monotonicity and lower semicontinuity, we establish a representation theorem\nfor functionals satisfying max-stability, which turns out to be represented by\nthe supremum of a bivariate function. Our characterized functionals encompass\nspecial classes of functionals in the literature of risk measures, such as\nbenchmark-loss Value at Risk (VaR) and $\\Lambda$-quantile.\n"
    },
    {
        "paper_id": 2403.13192,
        "authors": "Dennis Lartey Quayesam, Anani Lotsi, Felix Okoe Mettle",
        "title": "Modeling stock price dynamics on the Ghana Stock Exchange: A Geometric\n  Brownian Motion approach",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Modeling financial data often relies on assumptions that may prove\ninsufficient or unrealistic in practice. The Geometric Brownian Motion (GBM)\nmodel is frequently employed to represent stock price processes. This study\ninvestigates whether the behavior of weekly and monthly returns of selected\nequities listed on the Ghana Stock Exchange conforms to the GBM model.\nParameters of the GBM model were estimated for five equities, and forecasts\nwere generated for three months. Evaluation of estimation accuracy was\nconducted using mean square error (MSE). Results indicate that the expected\nprices from the modeled equities closely align with actual stock prices\nobserved on the Exchange. Furthermore, while some deviations were observed, the\nactual prices consistently fell within the estimated confidence intervals.\n"
    },
    {
        "paper_id": 2403.13361,
        "authors": "Mohamed Elshazli A. Zidan, Anouar Ben Mabrouk, Nidhal Ben Abdallah and\n  Tawfeeq M. Alanazi",
        "title": "Multifractal wavelet dynamic mode decomposition modeling for marketing\n  time series",
        "comments": "57 pages, 25 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/publicdomain/zero/1.0/",
        "abstract": "  Marketing is the way we ensure our sales are the best in the market, our\nprices the most accessible, and our clients satisfied, thus ensuring our brand\nhas the widest distribution. This requires sophisticated and advanced\nunderstanding of the whole related network. Indeed, marketing data may exist in\ndifferent forms such as qualitative and quantitative data. However, in the\nliterature, it is easily noted that large bibliographies may be collected about\nqualitative studies, while only a few studies adopt a quantitative point of\nview. This is a major drawback that results in marketing science still focusing\non design, although the market is strongly dependent on quantities such as\nmoney and time. Indeed, marketing data may form time series such as brand sales\nin specified periods, brand-related prices over specified periods, market\nshares, etc. The purpose of the present work is to investigate some marketing\nmodels based on time series for various brands. This paper aims to combine the\ndynamic mode decomposition and wavelet decomposition to study marketing series\ndue to both prices, and volume sales in order to explore the effect of the time\nscale on the persistence of brand sales in the market and on the forecasting of\nsuch persistence, according to the characteristics of the brand and the related\nmarket competition or competitors. Our study is based on a sample of Saudi\nbrands during the period 22 November 2017 to 30 December 2021.\n"
    },
    {
        "paper_id": 2403.13388,
        "authors": "Guohui Guan and Lin He and Zongxia Liang and Litian Zhang",
        "title": "Optimal VPPI strategy under Omega ratio with stochastic benchmark",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper studies a variable proportion portfolio insurance (VPPI) strategy.\nThe objective is to determine the risk multiplier by maximizing the extended\nOmega ratio of the investor's cushion, using a binary stochastic benchmark.\nWhen the stock index declines, investors aim to maintain the minimum guarantee.\nConversely, when the stock index rises, investors seek to track some excess\nreturns. The optimization problem involves the combination of a non-concave\nobjective function with a stochastic benchmark, which is effectively solved\nbased on the stochastic version of concavification technique. We derive\nsemi-analytical solutions for the optimal risk multiplier, and the value\nfunctions are categorized into three distinct cases. Intriguingly, the\nclassification criteria are determined by the relationship between the optimal\nrisky multiplier in Zieling et al. (2014 and the value of 1. Simulation results\nconfirm the effectiveness of the VPPI strategy when applied to real market data\ncalibrations.\n"
    },
    {
        "paper_id": 2403.13429,
        "authors": "Kaushalya Kularatnam, Tania Stathaki",
        "title": "Detecting and Triaging Spoofing using Temporal Convolutional Networks",
        "comments": null,
        "journal-ref": "AAAI 2024 Workshop on AI in Finance for Social Impact",
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-sa/4.0/",
        "abstract": "  As algorithmic trading and electronic markets continue to transform the\nlandscape of financial markets, detecting and deterring rogue agents to\nmaintain a fair and efficient marketplace is crucial. The explosion of large\ndatasets and the continually changing tricks of the trade make it difficult to\nadapt to new market conditions and detect bad actors. To that end, we propose a\nframework that can be adapted easily to various problems in the space of\ndetecting market manipulation. Our approach entails initially employing a\nlabelling algorithm which we use to create a training set to learn a weakly\nsupervised model to identify potentially suspicious sequences of order book\nstates. The main goal here is to learn a representation of the order book that\ncan be used to easily compare future events. Subsequently, we posit the\nincorporation of expert assessment to scrutinize specific flagged order book\nstates. In the event of an expert's unavailability, recourse is taken to the\napplication of a more complex algorithm on the identified suspicious order book\nstates. We then conduct a similarity search between any new representation of\nthe order book against the expert labelled representations to rank the results\nof the weak learner. We show some preliminary results that are promising to\nexplore further in this direction\n"
    },
    {
        "paper_id": 2403.13625,
        "authors": "Francesco Zola, Lander Segurola, Erin King, Martin Mullins, Raul\n  Orduna",
        "title": "Enhancing Law Enforcement Training: A Gamified Approach to Detecting\n  Terrorism Financing",
        "comments": null,
        "journal-ref": "International Journal of Police Science & Management, Sage, 0(0),\n  [2024]",
        "doi": "10.1177/14613557241237174",
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  Tools for fighting cyber-criminal activities using new technologies are\npromoted and deployed every day. However, too often, they are unnecessarily\ncomplex and hard to use, requiring deep domain and technical knowledge. These\ncharacteristics often limit the engagement of law enforcement and end-users in\nthese technologies that, despite their potential, remain misunderstood. For\nthis reason, in this study, we describe our experience in combining learning\nand training methods and the potential benefits of gamification to enhance\ntechnology transfer and increase adult learning. In fact, in this case,\nparticipants are experienced practitioners in professions/industries that are\nexposed to terrorism financing (such as Law Enforcement Officers, Financial\nInvestigation Officers, private investigators, etc.) We define training\nactivities on different levels for increasing the exchange of information about\nnew trends and criminal modus operandi among and within law enforcement\nagencies, intensifying cross-border cooperation and supporting efforts to\ncombat and prevent terrorism funding activities. On the other hand, a game\n(hackathon) is designed to address realistic challenges related to the dark\nnet, crypto assets, new payment systems and dark web marketplaces that could be\nused for terrorist activities. The entire methodology was evaluated using\nquizzes, contest results, and engagement metrics. In particular, training\nevents show about 60% of participants complete the 11-week training course,\nwhile the Hackathon results, gathered in two pilot studies (Madrid and The\nHague), show increasing expertise among the participants (progression in the\nachieved points on average). At the same time, more than 70% of participants\npositively evaluate the use of the gamification approach, and more than 85% of\nthem consider the implemented Use Cases suitable for their investigations.\n"
    },
    {
        "paper_id": 2403.13791,
        "authors": "Tahir Choulli and Martin Schweizer",
        "title": "New Stochastic Fubini Theorems",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/publicdomain/zero/1.0/",
        "abstract": "  The classic stochastic Fubini theorem says that if one stochastically\nintegrates with respect to a semimartingale $S$ an $\\eta(dz)$-mixture of\n$z$-parametrized integrands $\\psi^z$, the result is just the $\\eta(dz)$-mixture\nof the individual $z$-parametrized stochastic integrals $\\int\\psi^z{d}S.$ But\nif one wants to use such a result for the study of Volterra semimartingales of\nthe form $ X_t =\\int_0^t \\Psi_{t,s}dS_s, t \\geq0,$ the classic assumption that\none has a fixed measure $\\eta$ is too restrictive; the mixture over the\nintegrands needs to be taken instead with respect to a stochastic kernel on the\nparameter space. To handle that situation and prove a corresponding new\nstochastic Fubini theorem, we introduce a new notion of measure-valued\nstochastic integration with respect to a general multidimensional\nsemimartingale. As an application, we show how this allows to handle a class of\nquite general stochastic Volterra semimartingales.\n"
    },
    {
        "paper_id": 2403.14063,
        "authors": "Divyanshu Daiya, Monika Yadav, Harshit Singh Rao",
        "title": "DiffSTOCK: Probabilistic relational Stock Market Predictions using\n  Diffusion Models",
        "comments": "Accepted for presentation to the 2024 IEEE International Conference\n  on Acoustics, Speech, and Signal Processing (ICASSP 2024), Seoul, Korea",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  In this work, we propose an approach to generalize denoising diffusion\nprobabilistic models for stock market predictions and portfolio management.\nPresent works have demonstrated the efficacy of modeling interstock relations\nfor market time-series forecasting and utilized Graph-based learning models for\nvalue prediction and portfolio management. Though convincing, these\ndeterministic approaches still fall short of handling uncertainties i.e., due\nto the low signal-to-noise ratio of the financial data, it is quite challenging\nto learn effective deterministic models. Since the probabilistic methods have\nshown to effectively emulate higher uncertainties for time-series predictions.\nTo this end, we showcase effective utilisation of Denoising Diffusion\nProbabilistic Models (DDPM), to develop an architecture for providing better\nmarket predictions conditioned on the historical financial indicators and\ninter-stock relations. Additionally, we also provide a novel deterministic\narchitecture MaTCHS which uses Masked Relational Transformer(MRT) to exploit\ninter-stock relations along with historical stock features. We demonstrate that\nour model achieves SOTA performance for movement predication and Portfolio\nmanagement.\n"
    },
    {
        "paper_id": 2403.14231,
        "authors": "S\\'ebastien Bossu, St\\'ephane Cr\\'epey (LPSM, UPCit\\'e), Hoang-Dung\n  Nguyen (LPSM, UPCit\\'e)",
        "title": "Spanning Multi-Asset Payoffs With ReLUs",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We propose a distributional formulation of the spanning problem of a\nmulti-asset payoff by vanilla basket options. This problem is shown to have a\nunique solution if and only if the payoff function is even and absolutely\nhomogeneous, and we establish a Fourier-based formula to calculate the\nsolution. Financial payoffs are typically piecewise linear, resulting in a\nsolution that may be derived explicitly, yet may also be hard to numerically\nexploit. One-hidden-layer feedforward neural networks instead provide a natural\nand efficient numerical alternative for discrete spanning. We test this\napproach for a selection of archetypal payoffs and obtain better hedging\nresults with vanilla basket options compared to industry-favored approaches\nbased on single-asset vanilla hedges.\n"
    },
    {
        "paper_id": 2403.14483,
        "authors": "Shaojie Li, Xinqi Dong, Danqing Ma, Bo Dang, Hengyi Zang, and Yulu\n  Gong",
        "title": "Utilizing the LightGBM Algorithm for Operator User Credit Assessment\n  Research",
        "comments": null,
        "journal-ref": "ACE (2024) Vol. 75: 36-47",
        "doi": "10.54254/2755-2721/75/20240503",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Mobile Internet user credit assessment is an important way for communication\noperators to establish decisions and formulate measures, and it is also a\nguarantee for operators to obtain expected benefits. However, credit evaluation\nmethods have long been monopolized by financial industries such as banks and\ncredit. As supporters and providers of platform network technology and network\nresources, communication operators are also builders and maintainers of\ncommunication networks. Internet data improves the user's credit evaluation\nstrategy. This paper uses the massive data provided by communication operators\nto carry out research on the operator's user credit evaluation model based on\nthe fusion LightGBM algorithm. First, for the massive data related to user\nevaluation provided by operators, key features are extracted by data\npreprocessing and feature engineering methods, and a multi-dimensional feature\nset with statistical significance is constructed; then, linear regression,\ndecision tree, LightGBM, and other machine learning algorithms build multiple\nbasic models to find the best basic model; finally, integrates Averaging,\nVoting, Blending, Stacking and other integrated algorithms to refine multiple\nfusion models, and finally establish the most suitable fusion model for\noperator user evaluation.\n"
    },
    {
        "paper_id": 2403.14695,
        "authors": "Denis Levchenko, Efstratios Rappos, Shabnam Ataee, Biagio Nigro,\n  Stephan Robert",
        "title": "Chain-structured neural architecture search for financial time series\n  forecasting",
        "comments": "17 pages, 3 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  We compare three popular neural architecture search strategies on\nchain-structured search spaces: Bayesian optimization, the hyperband method,\nand reinforcement learning in the context of financial time series forecasting.\n"
    },
    {
        "paper_id": 2403.14724,
        "authors": "Tucker Balch, Vamsi K. Potluru, Deepak Paramanand, Manuela Veloso",
        "title": "Six Levels of Privacy: A Framework for Financial Synthetic Data",
        "comments": "Six privacy levels framework; excerpted from \"Synthetic Data\n  Applications in Finance'' (arxiv:2401.00081) article",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Synthetic Data is increasingly important in financial applications. In\naddition to the benefits it provides, such as improved financial modeling and\nbetter testing procedures, it poses privacy risks as well. Such data may arise\nfrom client information, business information, or other proprietary sources\nthat must be protected. Even though the process by which Synthetic Data is\ngenerated serves to obscure the original data to some degree, the extent to\nwhich privacy is preserved is hard to assess. Accordingly, we introduce a\nhierarchy of ``levels'' of privacy that are useful for categorizing Synthetic\nData generation methods and the progressively improved protections they offer.\nWhile the six levels were devised in the context of financial applications,\nthey may also be appropriate for other industries as well. Our paper includes:\nA brief overview of Financial Synthetic Data, how it can be used, how its value\ncan be assessed, privacy risks, and privacy attacks. We close with details of\nthe ``Six Levels'' that include defenses against those attacks.\n"
    },
    {
        "paper_id": 2403.14841,
        "authors": "T. van der Zwaard, L.A. Grzelak, C.W. Oosterlee",
        "title": "On the Hull-White model with volatility smile for Valuation Adjustments",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Affine Diffusion dynamics are frequently used for Valuation Adjustments (xVA)\ncalculations due to their analytic tractability. However, these models cannot\ncapture the market-implied skew and smile, which are relevant when computing\nxVA metrics. Hence, additional degrees of freedom are required to capture these\nmarket features. In this paper, we address this through an SDE with\nstate-dependent coefficients. The SDE is consistent with the convex combination\nof a finite number of different AD dynamics. We combine Hull-White one-factor\nmodels where one model parameter is varied. We use the Randomized AD (RAnD)\ntechnique to parameterize the combination of dynamics. We refer to our SDE with\nstate-dependent coefficients and the RAnD parametrization of the original\nmodels as the rHW model. The rHW model allows for efficient semi-analytic\ncalibration to European swaptions through the analytic tractability of the\nHull-White dynamics. We use a regression-based Monte-Carlo simulation to\ncalculate exposures. In this setting, we demonstrate the significant effect of\nskew and smile on exposures and xVAs of linear and early-exercise interest rate\nderivatives.\n"
    },
    {
        "paper_id": 2403.14862,
        "authors": "Haihao Lu, Luyang Zhang",
        "title": "The Power of Linear Programming in Sponsored Listings Ranking: Evidence\n  from Field Experiments",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Sponsored listing is one of the major revenue sources for many prominent\nonline marketplaces, such as Amazon, Walmart, and Alibaba. When consumers visit\na marketplace's webpage for a specific item, in addition to that item, the\nmarketplace might also display a ranked listing of sponsored items from various\nthird-party sellers. These sellers are charged an advertisement fee if a user\npurchases any of the sponsored items from this listing. Determining how to rank\nthese sponsored items for each incoming visit is a crucial challenge for online\nmarketplaces, a problem known as sponsored listings ranking (SLR). The major\ndifficulty of SLR lies in balancing the trade-off between maximizing the\noverall revenue and recommending high-quality and relevant ranked listings.\nWhile a more relevant ranking may result in more purchases and consumer\nengagement, the marketplace also needs to take account of the potential revenue\nwhen making ranking decisions. Due to the latency requirement and historical\nreasons, many online marketplaces use score-based ranking algorithms for SLR\noptimization. Alternatively, recent research also discusses obtaining the\nranking by solving linear programming (LP). In this paper, we collaborate with\na leading online global marketplace and conduct a series of field experiments\nto compare the performance of the score-based ranking algorithms and the\nLP-based algorithms. The field experiment lasted for $19$ days, which included\n$329.3$ million visits in total. We observed that the LP-based approach\nimproved all major metrics by $1.80\\%$ of revenue, $1.55\\%$ of purchase, and\n$1.39\\%$ of the gross merchandise value (GMV), compared to an extremely-tuned\nscore-based algorithm that was previously used in production by the\nmarketplace.\n"
    },
    {
        "paper_id": 2403.14868,
        "authors": "Michael Kalkbrener and Natalie Packham",
        "title": "A Markov approach to credit rating migration conditional on economic\n  states",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  We develop a model for credit rating migration that accounts for the impact\nof economic state fluctuations on default probabilities. The joint process for\nthe economic state and the rating is modelled as a time-homogeneous Markov\nchain. While the rating process itself possesses the Markov property only under\nrestrictive conditions, methods from Markov theory can be used to derive the\nrating process' asymptotic behaviour. We use the mathematical framework to\nformalise and analyse different rating philosophies, such as point-in-time\n(PIT) and through-the-cycle (TTC) ratings. Furthermore, we introduce stochastic\norders on the bivariate process' transition matrix to establish a consistent\nnotion of \"better\" and \"worse\" ratings. Finally, the construction of PIT and\nTTC ratings is illustrated on a Merton-type firm-value process.\n"
    },
    {
        "paper_id": 2403.15062,
        "authors": "Masanori Hirano",
        "title": "Construction of a Japanese Financial Benchmark for Large Language Models",
        "comments": "9 pages, Joint Workshop of the 7th Financial Technology and Natural\n  Language Processing (FinNLP), the 5th Knowledge Discovery from Unstructured\n  Data in Financial Services (KDF), and The 4th Workshop on Economics and\n  Natural Language Processing (ECONLP) In conjunction with LREC-COLING-2024",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  With the recent development of large language models (LLMs), models that\nfocus on certain domains and languages have been discussed for their necessity.\nThere is also a growing need for benchmarks to evaluate the performance of\ncurrent LLMs in each domain. Therefore, in this study, we constructed a\nbenchmark comprising multiple tasks specific to the Japanese and financial\ndomains and performed benchmark measurements on some models. Consequently, we\nconfirmed that GPT-4 is currently outstanding, and that the constructed\nbenchmarks function effectively. According to our analysis, our benchmark can\ndifferentiate benchmark scores among models in all performance ranges by\ncombining tasks with different difficulties.\n"
    },
    {
        "paper_id": 2403.15074,
        "authors": "Arindam Misra",
        "title": "A Taxmans guide to taxation of crypto assets",
        "comments": "105 pages, 59 figures and 4 Tables",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  The Financial system has witnessed rapid technological changes. The rise of\nBitcoin and other crypto assets based on Distributed Ledger Technology mark a\nfundamental change in the way people transact and transmit value over a\ndecentralized network, spread across geographies. This has created regulatory\nand tax policy blind spots, as governments and tax administrations take time to\nunderstand and provide policy responses to this innovative, revolutionary, and\nfast-paced technology. Due to the breakneck speed of innovation in blockchain\ntechnology and advent of Decentralized Finance, Decentralized Autonomous\nOrganizations and the Metaverse, it is unlikely that the policy interventions\nand guidance by regulatory authorities or tax administrations would be ahead or\nin sync with the pace of innovation. This paper tries to explain the principles\non which crypto assets function, their underlying technology and relates them\nto the tax issues and taxable events which arise within this ecosystem. It also\nprovides instances of tax and regulatory policy responses already in effect in\nvarious jurisdictions, including the recent changes in reporting standards by\nthe FATF and the OECD. This paper tries to explain the rationale behind\nexisting laws and policies and the challenges in their implementation. It also\nattempts to present a ballpark estimate of tax potential of this asset class\nand suggests creation of global public digital infrastructure that can address\nissues related to pseudonymity and extra-territoriality. The paper analyses\nboth direct and indirect taxation issues related to crypto assets and discusses\nmore recent aspects like proof-of-stake and maximal extractable value in\ngreater detail.\n"
    },
    {
        "paper_id": 2403.15163,
        "authors": "Nick James and Max Menzies",
        "title": "Nonlinear shifts and dislocations in financial market structure and\n  composition",
        "comments": "Accepted manuscript. Minor edits since v1. Equal contribution",
        "journal-ref": "Chaos 34, 073116 (2024)",
        "doi": "10.1063/5.0209904",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper develops new mathematical techniques to identify temporal shifts\namong a collection of US equities partitioned into a new and more detailed set\nof market sectors. Although conceptually related, our three analyses reveal\ndistinct insights about financial markets, with meaningful implications for\ninvestment managers. First, we explore a variety of methods to identify\nnonlinear shifts in market sector structure and describe the mathematical\nconnection between the measure used and the captured phenomena. Second, we\nstudy network structure with respect to our new market sectors and identify\nmeaningfully connected sector-to-sector mappings. Finally, we conduct a series\nof sampling experiments over different sample spaces and contrast the\ndistribution of Sharpe ratios produced by long-only, long-short and short-only\ninvestment portfolios. In addition, we examine the sector composition of the\ntop-performing portfolios for each of these portfolio styles. In practice, the\nmethods proposed in this paper could be used to identify regime shifts,\noptimally structured portfolios, and better communities of equities.\n"
    },
    {
        "paper_id": 2403.152,
        "authors": "Enzo Brox and Michael Lechner",
        "title": "Teamwork and Spillover Effects in Performance Evaluations",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  This article shows how coworker performance affects individual performance\nevaluation in a teamwork setting at the workplace. We use high-quality data on\nfootball matches to measure an important component of individual performance,\nshooting performance, isolated from collaborative effects. Employing causal\nmachine learning methods, we address the assortative matching of workers and\nestimate both average and heterogeneous effects. There is substantial evidence\nfor spillover effects in performance evaluations. Coworker shooting\nperformance, meaningfully impacts both, manager decisions and third-party\nexpert evaluations of individual performance. Our results underscore the\nsignificant role coworkers play in shaping career advancements and highlight a\ncomplementary channel, to productivity gains and learning effects, how\ncoworkers impact career advancement. We characterize the groups of workers that\nare most and least affected by spillover effects and show that spillover\neffects are reference point dependent. While positive deviations from a\nreference point create positive spillover effects, negative deviations are not\nharmful for coworkers.\n"
    },
    {
        "paper_id": 2403.15243,
        "authors": "Florian Krach, Josef Teichmann, Hanna Wutte",
        "title": "Robust Utility Optimization via a GAN Approach",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  Robust utility optimization enables an investor to deal with market\nuncertainty in a structured way, with the goal of maximizing the worst-case\noutcome. In this work, we propose a generative adversarial network (GAN)\napproach to (approximately) solve robust utility optimization problems in\ngeneral and realistic settings. In particular, we model both the investor and\nthe market by neural networks (NN) and train them in a mini-max zero-sum game.\nThis approach is applicable for any continuous utility function and in\nrealistic market settings with trading costs, where only observable information\nof the market can be used. A large empirical study shows the versatile\nusability of our method. Whenever an optimal reference strategy is available,\nour method performs on par with it and in the (many) settings without known\noptimal strategy, our method outperforms all other reference strategies.\nMoreover, we can conclude from our study that the trained path-dependent\nstrategies do not outperform Markovian ones. Lastly, we uncover that our\ngenerative approach for learning optimal, (non-) robust investments under\ntrading costs generates universally applicable alternatives to well known\nasymptotic strategies of idealized settings.\n"
    },
    {
        "paper_id": 2403.15262,
        "authors": "Shun Yiu, Rob Seamans, Manav Raj, Ted Liu",
        "title": "Strategic Responses to Technological Change: Evidence from ChatGPT and\n  Upwork",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  AI technologies have the potential to affect labor market outcomes by both\nincreasing worker productivity and reducing the demand for certain skills or\ntasks. Such changes may have important implications for the ways in which\nworkers seek jobs and position themselves. In this project, we examine how\nfreelancers change their strategic behavior on an online work platform\nfollowing the launch of ChatGPT in December 2022. We present evidence\nsuggesting that, in response to technological change, freelancers increase the\nconcentration of job applications across specializations and differentiate\nthemselves from both their past behavior and their peers. We show that such\nchanges are shaped by the extent to which a freelancer experiences growth in\ndemand or supply in the domains they specialize in. We document heterogeneity\nin this effect across freelancer characteristics and document how strategic\nrepositioning can help mitigate the negative effects of technological change on\nfreelancer performance on the platform.\n"
    },
    {
        "paper_id": 2403.15281,
        "authors": "Jiafu An, Difang Huang, Chen Lin and Mingzhu Tai",
        "title": "Measuring Gender and Racial Biases in Large Language Models",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  In traditional decision making processes, social biases of human decision\nmakers can lead to unequal economic outcomes for underrepresented social\ngroups, such as women, racial or ethnic minorities. Recently, the increasing\npopularity of Large language model based artificial intelligence suggests a\npotential transition from human to AI based decision making. How would this\nimpact the distributional outcomes across social groups? Here we investigate\nthe gender and racial biases of OpenAIs GPT, a widely used LLM, in a high\nstakes decision making setting, specifically assessing entry level job\ncandidates from diverse social groups. Instructing GPT to score approximately\n361000 resumes with randomized social identities, we find that the LLM awards\nhigher assessment scores for female candidates with similar work experience,\neducation, and skills, while lower scores for black male candidates with\ncomparable qualifications. These biases may result in a 1 or 2 percentage point\ndifference in hiring probabilities for otherwise similar candidates at a\ncertain threshold and are consistent across various job positions and\nsubsamples. Meanwhile, we also find stronger pro female and weaker anti black\nmale patterns in democratic states. Our results demonstrate that this LLM based\nAI system has the potential to mitigate the gender bias, but it may not\nnecessarily cure the racial bias. Further research is needed to comprehend the\nroot causes of these outcomes and develop strategies to minimize the remaining\nbiases in AI systems. As AI based decision making tools are increasingly\nemployed across diverse domains, our findings underscore the necessity of\nunderstanding and addressing the potential unequal outcomes to ensure equitable\noutcomes across social groups.\n"
    },
    {
        "paper_id": 2403.15293,
        "authors": "Valerio Capraro",
        "title": "Human behaviour through a LENS: How Linguistic content triggers Emotions\n  and Norms and determines Strategy choices",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Over the last two decades, a growing body of experimental research has\nprovided evidence that linguistic frames influence human behaviour in economic\ngames, beyond the economic consequences of the available actions. This article\nproposes a novel framework that transcends the traditional confines of\noutcome-based preference models. According to the LENS model, the Linguistic\ndescription of the decision problem triggers Emotional responses and suggests\npotential Norms of behaviour, which then interact to shape an individual's\nStrategic choice. The article reviews experimental evidence that supports each\npath of the LENS model. Furthermore, it identifies and discusses several\ncritical research questions that arise from this model, pointing towards\navenues for future inquiry.\n"
    },
    {
        "paper_id": 2403.1581,
        "authors": "Aman Saggu, Lennart Ante, Ender Demir",
        "title": "Anticipatory Gains and Event-Driven Losses in Blockchain-Based Fan\n  Tokens: Evidence from the FIFA World Cup",
        "comments": "31 pages, 5 tables, 1 figure",
        "journal-ref": "Research in International Business and Finance, Volume 70, Part A,\n  102333 (2024)",
        "doi": "10.1016/j.ribaf.2024.102333",
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  National football teams increasingly issue tradeable blockchain-based fan\ntokens to strategically enhance fan engagement. This study investigates the\nimpact of 2022 World Cup matches on the dynamic performance of each team's fan\ntoken. The event study uncovers fan token returns surged six months before the\nWorld Cup, driven by positive anticipation effects. However, intraday analysis\nreveals a reversal of fan token returns consistently declining and trading\nvolumes rising as matches unfold. To explain findings, we uncover asymmetries\nwhereby defeats in high-stake matches caused a plunge in fan token returns,\ncompared to low-stake matches, intensifying in magnitude for knockout matches.\nContrarily, victories enhance trading volumes, reflecting increased market\nactivity without a corresponding positive effect on returns. We align findings\nwith the classic market adage \"buy the rumor, sell the news,\" unveiling\ncognitive biases and nuances in investor sentiment, cautioning the dichotomy of\npre-event optimism and subsequent performance declines.\n"
    },
    {
        "paper_id": 2403.15923,
        "authors": "Yaacov Kopeliovich and Michael Pokojovy",
        "title": "On Merton's Optimal Portfolio Problem under Sporadic Bankruptcy",
        "comments": "12 pages, 1 figure",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-sa/4.0/",
        "abstract": "  Consider a stock market following a geometric Brownian motion and a riskless\nasset continuously compounded at a constant rate. Assuming the stock can go\nbankrupt, i.e., lose all of its value, at some exogenous random time\n(independent of the stock price) modeled as the first arrival time of a\nhomogeneous Poisson process, we study the Merton's optimal portfolio problem\nconsisting of maximizing the expected logarithmic utility of the total wealth\nat a preselected finite maturity time. First, we present a heuristic derivation\nbased on a new type of Hamilton-Jacobi-Bellman equation. Then, we formally\nreduce the problem to a classical controlled Markovian diffusion with a new\ntype of terminal and running costs. A new version of Merton's ratio is\nrigorously derived using Bellman's dynamic programming principle and validated\nwith a suitable type of verification theorem. A real-world example comparing\nthe latter ratio to the classical Merton's ratio is given.\n"
    },
    {
        "paper_id": 2403.15925,
        "authors": "David Xiao",
        "title": "Hedge Fund Index Rules and Construction",
        "comments": "23 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  A Hedge Fund Index is very useful for tracking the performance of hedge fund\ninvestments, especially the timing of fund redemption. This paper presents a\nmethodology for constructing a hedge fund index that is more like a\nquantitative fund of fund, rather than a weighted sum of a number of early\nreplicable market indices, which are re-balanced periodically. The constructed\nindex allows hedge funds to directly hedge their exposures to index-linked\nproducts. That is important given that hedge funds are an asset class with\nreduced transparency, and the returns are traditionally difficult to replicate\nusing liquid instruments.\n"
    },
    {
        "paper_id": 2403.1598,
        "authors": "Martin Larsson and Shukun Long",
        "title": "Markovian projections for It\\^o semimartingales with jumps",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Given a general It\\^o semimartingale, its Markovian projection is an It\\^o\nprocess, with Markovian differential characteristics, that matches the\none-dimensional marginal laws of the original process. We construct Markovian\nprojections for It\\^o semimartingales with jumps, whose flows of\none-dimensional marginal laws are solutions to non-local\nFokker--Planck--Kolmogorov equations (FPKEs). As an application, we show how\nMarkovian projections appear in building calibrated diffusion/jump models with\nboth local and stochastic features.\n"
    },
    {
        "paper_id": 2403.16006,
        "authors": "Boyi Li, Weixuan Xia",
        "title": "Crypto Inverse-Power Options and Fractional Stochastic Volatility",
        "comments": "42 pages, 2 tables, 5 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Recent empirical evidence has highlighted the crucial role of jumps in both\nprice and volatility within the cryptocurrency market. In this paper, we\nintroduce an analytical model framework featuring fractional stochastic\nvolatility, accommodating price--volatility co-jumps and volatility short-term\ndependency concurrently. We particularly focus on inverse options, including\nthe emerging Quanto inverse options and their power-type generalizations, aimed\nat mitigating cryptocurrency exchange rate risk and adjusting inherent risk\nexposure. Characteristic function-based pricing--hedging formulas are derived\nfor these inverse options. The general model framework is then applied to\nasymmetric Laplace jump-diffusions and Gaussian-mixed tempered stable-type\nprocesses, employing three types of fractional kernels, for an extensive\nempirical analysis involving model calibration on two independent Bitcoin\noptions data sets, during and after the COVID-19 pandemic. Key insights from\nour theoretical analysis and empirical findings include: (1) the superior\nperformance of fractional stochastic-volatility models compared to various\nbenchmark models, including those incorporating jumps and stochastic\nvolatility, (2) the practical necessity of jumps in both price and volatility,\nalong with their co-jumps and rough volatility, in the cryptocurrency market,\n(3) stability of calibrated parameter values in line with stylized facts, and\n(4) the suggestion that a piecewise kernel offers much higher computational\nefficiency relative to the commonly used Riemann--Liouville kernel in\nconstructing fractional models, yet maintaining the same accuracy level, thanks\nto its potential for obtaining explicit model characteristic functions.\n"
    },
    {
        "paper_id": 2403.16228,
        "authors": "Bahman Angoshtari, Shida Duan",
        "title": "Rank-Dependent Predictable Forward Performance Processes",
        "comments": "43 pages, 3 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Predictable forward performance processes (PFPPs) are stochastic optimal\ncontrol frameworks for an agent who controls a randomly evolving system but can\nonly prescribe the system dynamics for a short period ahead. This is a common\nscenario in which a controlling agent frequently re-calibrates her model. We\nintroduce a new class of PFPPs based on rank-dependent utility, generalizing\nexisting models that are based on expected utility theory (EUT). We establish\nexistence of rank-dependent PFPPs under a conditionally complete market and\nexogenous probability distortion functions which are updated periodically. We\nshow that their construction reduces to solving an integral equation that\ngeneralizes the integral equation obtained under EUT in previous studies. We\nthen propose a new approach for solving the integral equation via theory of\nVolterra equations. We illustrate our result in the special case of\nconditionally complete Black-Scholes model.\n"
    },
    {
        "paper_id": 2403.16296,
        "authors": "Elham Daadmehr",
        "title": "Workplace sustainability or financial resilience? Composite-financial\n  resilience index",
        "comments": null,
        "journal-ref": "Risk Manag 26, 7 (2024)",
        "doi": "10.1057/s41283-023-00139-9",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Due to the variety of corporate risks in turmoil markets and the consequent\nfinancial distress especially in COVID-19 time, this paper investigates\ncorporate resilience and compares different types of resilience that can be\npotential sources of heterogeneity in firms' implied rate of return.\nSpecifically, the novelty is not only to quantify firms' financial resilience\nbut also to compare it with workplace resilience which matters more in the\nCOVID-19 era. The study prepares several pieces of evidence of the necessity\nand insufficiency of these two main types of resilience by comparing earnings\nexpectations and implied discount rates of high- and low-resilience firms.\nParticularly, results present evidence of the possible amplification of\nworkplace resilience by the financial status of firms in the COVID-19 era. The\npaper proposes a novel composite-financial resilience index as a potential\nmeasure for disaster risk that significantly and persistently reveals\nlow-resilience characteristics of firms and resilience-heterogeneity in implied\ndiscount rates.\n"
    },
    {
        "paper_id": 2403.16452,
        "authors": "Didarul Islam, Mohammad Abdullah Al Faisal",
        "title": "Determinants of Uruguay's Real Effective Exchange Rate: A\n  Mundell-Fleming Model Approach",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This study examines the factors influencing the short-term real effective\nexchange rate (REER) in Uruguay by applying an extended Mundell-Fleming model.\nAnalyzing the impact of the US lending rate (USLR), money supply (M2),\ninflation (CPI), and the world interest rate (WIR), the paper uses a linear\nregression model with Newey-West standard errors. Key findings reveal that an\nincrease in the USLR, CPI, and M2 is associated with a depreciation of the\nREER. In contrast, WIR shows no significant impact. These findings are\nconsistent with the theoretical expectations of the Mundell-Fleming model\nregarding open economies under floating exchange rates. Therefore, authorities\nshould tighten monetary policy, control inflation, adjust fiscal strategies,\nand boost exports in response to Peso depreciation.\n"
    },
    {
        "paper_id": 2403.16525,
        "authors": "Eva L\\\"utkebohmert, Julian Sester",
        "title": "Measuring Name Concentrations through Deep Learning",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We propose a new deep learning approach for the quantification of name\nconcentration risk in loan portfolios. Our approach is tailored for small\nportfolios and allows for both an actuarial as well as a mark-to-market\ndefinition of loss. The training of our neural network relies on Monte Carlo\nsimulations with importance sampling which we explicitly formulate for the\nCreditRisk${+}$ and the ratings-based CreditMetrics model. Numerical results\nbased on simulated as well as real data demonstrate the accuracy of our new\napproach and its superior performance compared to existing analytical methods\nfor assessing name concentration risk in small and concentrated portfolios.\n"
    },
    {
        "paper_id": 2403.16934,
        "authors": "Gerald Schweiger, Adrian Barnett, Peter van den Besselaar, Lutz\n  Bornmann, Andreas De Block, John P.A. Ioannidis, Ulf Sandstr\\\"om, Stijn Conix",
        "title": "The Costs of Competition in Distributing Scarce Research Funds",
        "comments": "29 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Research funding systems are not isolated systems - they are embedded in a\nlarger scientific system with an enormous influence on the system. This paper\naims to analyze the allocation of competitive research funding from different\nperspectives: How reliable are decision processes for funding? What are the\neconomic costs of competitive funding? How does competition for funds affect\ndoing risky research? How do competitive funding environments affect scientists\nthemselves, and which ethical issues must be considered? We attempt to identify\ngaps in our knowledge of research funding systems; we propose recommendations\nfor policymakers and funding agencies, including empirical experiments of\ndecision processes and the collection of data on these processes. With our\nrecommendations we hope to contribute to developing improved ways of organizing\nresearch funding.\n"
    },
    {
        "paper_id": 2403.1698,
        "authors": "Jeff Strnad",
        "title": "Economic DAO Governance: A Contestable Control Approach",
        "comments": "91 pages, 2 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this article, we propose a new form of DAO governance that uses a\nsequential auction mechanism to overcome entrenched control issues that have\nemerged for DAOs by creating a regime of temporary contestable control. The\nmechanism avoids potential public choice problems inherent in voting approaches\nbut at the same time provides a vehicle that can enhance and secure value that\ninheres to DAO voting and other DAO non-market governance procedures. It is\nrobust to empty voting and is code feasible. It facilitates the ability of DAOs\nto meet their normative and operational goals in the face of diverse regulatory\napproaches. Designed to shift control to the party with the most promising\nbusiness plan, at the same time it distributes surplus in a way that tends to\npromote investment by other parties.\n"
    },
    {
        "paper_id": 2403.17095,
        "authors": "David Ardia, Cl\\'ement Aymard, Tolga Cenesizoglu",
        "title": "Revisiting Boehmer et al. (2021): Recent Period, Alternative Method,\n  Different Conclusions",
        "comments": null,
        "journal-ref": null,
        "doi": "10.2139/ssrn.4703056",
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  We reassess Boehmer et al. (2021, BJZZ)'s seminal work on the predictive\npower of retail order imbalance (ROI) for future stock returns. First, we\nreplicate their 2010-2015 analysis in the more recent 2016-2021 period. We find\nthat the ROI's predictive power weakens significantly. Specifically, past ROI\ncan no longer predict weekly returns on large-cap stocks, and the long-short\nstrategy based on past ROI is no longer profitable. Second, we analyze the\neffect of using the alternative quote midpoint (QMP) method to identify and\nsign retail trades on their main conclusions. While the results based on the\nQMP method align with BJZZ's findings in 2010-2015, the two methods provide\ndifferent conclusions in 2016-2021. Our study shows that BJZZ's original\nfindings are sensitive to the sample period and the approach to identify ROIs.\n"
    },
    {
        "paper_id": 2403.17112,
        "authors": "Nabeel Asharaf and Richard S.J. Tol",
        "title": "The Impact of Pradhan Mantri Ujjwala Yojana on Indian Households",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This study critically evaluates the impact of the Pradhan Mantri Ujjwala\nYojana (PMUY) on LPG accessibility among poor households in India. Using\nPropensity Score Matching and Difference-in-Differences estimators and the\nNational Family Health Survey (NFHS) dataset, the Average Treatment Effect on\nthe interdedly Treated is a modest 2.1 percentage point increase in LPG\nconsumption due to PMUY, with a parallel decrease in firewood consumption.\nRegional analysis reveals differential impacts, with significant progress in\nthe North, West, and South but less pronounced effects in the East and North\nEast. The study also underscores variance across social groups, with Schedule\nCaste households showing the most substantial benefits, while Scheduled Tribes\nhouseholds are hardly affected. Despite the PMUY's initial success in\nfacilitating LPG access, sustaining its usage remains challenging. Policy\nshould emphasise targeted interventions, income support, and address regional\nand community-specific disparities for the sustained usage of LPG.\n"
    },
    {
        "paper_id": 2403.17127,
        "authors": "David Ardia, S\\'ebastien Laurent, Rosnel Sessinou",
        "title": "High-Dimensional Mean-Variance Spanning Tests",
        "comments": null,
        "journal-ref": null,
        "doi": "10.2139/ssrn.4703023",
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  We introduce a new framework for the mean-variance spanning (MVS) hypothesis\ntesting. The procedure can be applied to any test-asset dimension and only\nrequires stationary asset returns and the number of benchmark assets to be\nsmaller than the number of time periods. It involves individually testing\nmoment conditions using a robust Student-t statistic based on the batch-mean\nmethod and combining the p-values using the Cauchy combination test.\nSimulations demonstrate the superior performance of the test compared to\nstate-of-the-art approaches. For the empirical application, we look at the\nproblem of domestic versus international diversification in equities. We find\nthat the advantages of diversification are influenced by economic conditions\nand exhibit cross-country variation. We also highlight that the rejection of\nthe MVS hypothesis originates from the potential to reduce variance within the\ndomestic global minimum-variance portfolio.\n"
    },
    {
        "paper_id": 2403.17162,
        "authors": "Tubagus Aryandi Gunawan, Lilianna Gittoes, Cecelia Isaac, Chris Greig,\n  Eric Larson",
        "title": "Design Insights for Industrial CO2 Capture, Transport, and Storage\n  Systems",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  We present design methods and insights for CO2 capture, transport, and\nstorage systems for clusters of industrial facilities, with a case-study focus\non the state of Louisiana. Our analytical framework includes: (1) evaluating\nthe scale and concentration of capturable CO2 emissions at individual\nfacilities for the purpose of estimating the cost of CO2 capture retrofits, (2)\na screening method to identify potential CO2 storage sites and estimate their\nstorage capacities, injectivities, and costs; and (3) an approach for\ncost-minimized design of pipeline infrastructure connecting CO2 capture plants\nwith storage sites that considers land use patterns, existing rights-of-way,\ndemographics, and a variety of social and environmental justice factors. In\napplying our framework to Louisiana, we estimate up to 50 million tCO2/y of\nindustrial emissions (out of today's total emissions of 130 MtCO2/y) can be\ncaptured at under 100 USD/tCO2, and up to 100 MtCO2/y at under 120 USD/tCO2. We\nidentified 98 potential storage sites with estimated aggregate total\ninjectivity between 330 and 730 MtCO2/yr and storage costs ranging from 8 to 17\nUSD/tCO2. We find dramatic reductions in the aggregate pipeline length and CO2\ntransport cost per tonne when groups of capture plants share pipeline\ninfrastructure rather than build dedicated single-user pipelines. Smaller\nfacilities (emitting less than 1 MtCO2/y), which account for a quarter of\nLouisiana's industrial emissions, see the largest transport cost benefits from\nsharing of infrastructure. Pipeline routes designed to avoid disadvantaged\ncommunities (social and environmental justice) so as not to reinforce\nhistorical practices of disenfranchisement involve only modestly higher\npipeline lengths and costs.\n"
    },
    {
        "paper_id": 2403.17187,
        "authors": "W. Brent Lindquist, Svetlozar T. Rachev",
        "title": "Alternatives to classical option pricing",
        "comments": "25 pages, 3 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  We develop two alternate approaches to arbitrage-free, market-complete,\noption pricing. The first approach requires no riskless asset. We develop the\ngeneral framework for this approach and illustrate it with two specific\nexamples. The second approach does use a riskless asset. However, by ensuring\nequality between real-world and risk-neutral price-change probabilities, the\nsecond approach enables the computation of risk-neutral option prices utilizing\nexpectations under the natural world probability P. This produces the same\noption prices as the classical approach in which prices are computed under the\nrisk neutral measure Q. The second approach and the two specific examples of\nthe first approach require the introduction of new, marketable asset types,\nspecifically perpetual derivatives of a stock, and a stock whose cumulative\nreturn (rather than price) is deflated.\n"
    },
    {
        "paper_id": 2403.17206,
        "authors": "Mario Bossler, Ying Liang, Thorsten Schank",
        "title": "The Devil is in the Details: Heterogeneous Effects of the German Minimum\n  Wage on Working Hours and Minijobs",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  In 2015, Germany introduced a national minimum wage. While the literature\nagrees on at most limited negative effects on the overall employment level, we\ngo into detail and analyze the impact on the working hours dimension and on the\nsubset of minijobs. Using data from the German Structure of Earnings Survey in\n2010, 2014, and 2018, we find empirical evidence that the minimum wage\nsignificantly reduces inequality in hourly and monthly wages. While various\ntheoretical mechanisms suggest a reduction in working hours, these remain\nunchanged on average. However, minijobbers experience a notable reduction in\nworking hours which can be linked to the specific institutional framework.\nRegarding employment, the results show no effects for regular jobs, but there\nis a noteworthy decline in minijobs, driven by transitions to regular\nemployment and non-employment. The transitions in non-employment imply a wage\nelasticity of employment of -0.1 for minijobs. Our findings highlight that the\ninstitutional setting leads to heterogeneous effects of the minimum wage.\n"
    },
    {
        "paper_id": 2403.17546,
        "authors": "S. Di Luozzo, A. Fronzetti Colladon, M. M. Schiraldi",
        "title": "Decoding excellence: Mapping the demand for psychological traits of\n  operations and supply chain professionals through text mining",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The current study proposes an innovative methodology for the profiling of\npsychological traits of Operations Management (OM) and Supply Chain Management\n(SCM) professionals. We use innovative methods and tools of text mining and\nsocial network analysis to map the demand for relevant skills from a set of job\ndescriptions, with a focus on psychological characteristics. The proposed\napproach aims to evaluate the market demand for specific traits by combining\nrelevant psychological constructs, text mining techniques, and an innovative\nmeasure, namely, the Semantic Brand Score. We apply the proposed methodology to\na dataset of job descriptions for OM and SCM professionals, with the objective\nof providing a mapping of their relevant required skills, including\npsychological characteristics. In addition, the analysis is then detailed by\nconsidering the region of the organization that issues the job description, its\norganizational size, and the seniority level of the open position in order to\nunderstand their nuances. Finally, topic modeling is used to examine key\ncomponents and their relative significance in job descriptions. By employing a\nnovel methodology and considering contextual factors, we provide an innovative\nunderstanding of the attitudinal traits that differentiate professionals. This\nresearch contributes to talent management, recruitment practices, and\nprofessional development initiatives, since it provides new figures and\nperspectives to improve the effectiveness and success of Operations Management\nand Supply Chain Management professionals.\n"
    },
    {
        "paper_id": 2403.17798,
        "authors": "Matthew P Hamilton, Caroline Gao, Jonathan Karnon, Luis\n  Salvador-Carulla, Sue M Cotton and Cathrine Mihalopoulos",
        "title": "Ethical considerations when planning, implementing and releasing health\n  economic model software: a new proposal",
        "comments": "14 pages, 3 tables. This paper results from a split of\n  arXiv:2310.14138v1 into two papers, the other of which is arXiv:2310.14138v2.",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Most health economic analyses are undertaken with the aid of computers.\nHowever, the research ethics of implementing health economic models as software\n(or computational health economic models (CHEMs)) are poorly understood. We\npropose that developers and funders of CHEMs should adhere to research ethics\nprinciples and pursue the goals of: (i) socially acceptable user requirements\nand design specifications; (ii) fit for purpose implementations; and (iii)\nsocially beneficial post-release use. We further propose that a transparent\n(T), reusable (R) and updatable (U) CHEM is suggestive of a project team that\nhas largely met these goals. We propose six criteria for assessing TRU CHEMs:\n(T1) software files are publicly available; (T2) developer contributions and\njudgments on appropriate use are easily identified; (R1) programming practices\nfacilitate independent reuse of model components; (R2) licenses permit reuse\nand derivative works; (U1) maintenance infrastructure is in place; and (U2)\nreleases are systematically retested and deprecated. Few existing CHEMs would\nmeet all TRU criteria. Addressing these limitations will require the\ndevelopment of new and updated good practice guidelines and investments by\ngovernments and other research funders in enabling infrastructure and human\ncapital.\n"
    },
    {
        "paper_id": 2403.18126,
        "authors": "Victor Le Coz and Jean-Philippe Bouchaud",
        "title": "Revisiting Elastic String Models of Forward Interest Rates",
        "comments": "21 pages, 12 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Twenty five years ago, several authors proposed to describe the forward\ninterest rate curve (FRC) as an elastic string along which idiosyncratic shocks\npropagate, accounting for the peculiar structure of the return correlation\nacross different maturities. In this paper, we revisit the specific \"stiff''\nelastic string field theory of Baaquie and Bouchaud (2004) in a way that makes\nits micro-foundation more transparent. Our model can be interpreted as\ncapturing the effect of market forces that set the rates of nearby tenors in a\nself-referential fashion. The model is parsimonious and accurately reproduces\nthe whole correlation structure of the FRC over the time period 1994-2023, with\nan error around 1% and with only one adjustable parameter, the value of which\nbeing very stable across the last three decades. The dependence of correlation\non time resolution (also called the Epps effect) is also faithfully reproduced\nwithin the model and leads to a cross-tenor information propagation time on the\norder of 30 minutes. Finally, we confirm that the perceived time in interest\nrate markets is a strongly sub-linear function of real time, as surmised by\nBaaquie and Bouchaud (2004). In fact, our results are fully compatible with\nhyperbolic discounting, in line with the recent behavioral Finance literature\n(Farmer and Geanakoplos, 2009).\n"
    },
    {
        "paper_id": 2403.18177,
        "authors": "Cheuk Yin Lee, Shen-Ning Tung, Tai-Ho Wang",
        "title": "Growth rate of liquidity provider's wealth in G3Ms",
        "comments": "27 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Geometric mean market makers (G3Ms), such as Uniswap and Balancer, represent\na widely used class of automated market makers (AMMs). These G3Ms are\ncharacterized by the following rule: the reserves of the AMM must maintain the\nsame (weighted) geometric mean before and after each trade. This paper\ninvestigates the effects of trading fees on liquidity providers' (LP)\nprofitability in a G3M, as well as the adverse selection faced by LPs due to\narbitrage activities involving a reference market. Our work expands the model\ndescribed in previous studies for G3Ms, integrating transaction fees and\ncontinuous-time arbitrage into the analysis. Within this context, we analyze\nG3M dynamics, characterized by stochastic storage processes, and calculate the\ngrowth rate of LP wealth. In particular, our results align with and extend the\nresults concerning the constant product market maker, commonly referred to as\nUniswap v2.\n"
    },
    {
        "paper_id": 2403.18521,
        "authors": "Victor Galaz and Megan Meacham",
        "title": "Redirecting Flows -- Navigating the Future of the Amazon",
        "comments": "38 pages, 11 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  The Amazon Basin and the Latin America and Caribbean (LAC) region stands at a\ncritical juncture, grappling with pressing environmental challenges while\nholding immense potential for transformative change through innovative\nsolutions. This report illuminates the diverse landscape of social-ecological\nissues, technological advancements, community-led initiatives, and strategic\nactions that could help foster biosphere-based sustainability and resilience\nacross the region.\n"
    },
    {
        "paper_id": 2403.18528,
        "authors": "Xiangyu Cui, Jianjun Gao, Lingjie Kong",
        "title": "Limited Attention Allocation in a Stochastic Linear Quadratic System\n  with Multiplicative Noise",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  This study addresses limited attention allocation in a stochastic linear\nquadratic system with multiplicative noise. Our approach enables strategic\nresource allocation to enhance noise estimation and improve control decisions.\nWe provide analytical optimal control and propose a numerical method for\noptimal attention allocation. Additionally, we apply our ffndings to dynamic\nmean-variance portfolio selection, showing effective resource allocation across\ntime periods and factors, providing valuable insights for investors.\n"
    },
    {
        "paper_id": 2403.18737,
        "authors": "Matthew Willetts and Christian Harrington",
        "title": "Optimal Rebalancing in Dynamic AMMs",
        "comments": "16 pages including appendix",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Dynamic AMM pools, as found in Temporal Function Market Making, rebalance\ntheir holdings to a new desired ratio (e.g. moving from being 50-50 between two\nassets to being 90-10 in favour of one of them) by introducing an arbitrage\nopportunity that disappears when their holdings are in line with their target.\nStructuring this arbitrage opportunity reduces to the problem of choosing the\nsequence of portfolio weights the pool exposes to the market via its trading\nfunction. Linear interpolation from start weights to end weights has been used\nto reduce the cost paid by pools to arbitrageurs to rebalance. Here we obtain\nthe $\\textit{optimal}$ interpolation in the limit of small weight changes\n(which has the downside of requiring a call to a transcendental function) and\nthen obtain a cheap-to-compute approximation to that optimal approach that\ngives almost the same performance improvement. We then demonstrate this method\non a range of market backtests, including simulating pool performance when\ntrading fees are present, finding that the new approximately-optimal method of\nchanging weights gives robust increases in pool performance. For a BTC-ETH-DAI\npool from July 2022 to June 2023, the increases of pool P\\&L from\napproximately-optimal weight changes is $\\sim25\\%$ for a range of different\nstrategies and trading fees.\n"
    },
    {
        "paper_id": 2403.18822,
        "authors": "Nisarg Patel, Harmit Shah, Kishan Mewada",
        "title": "Enhancing Financial Data Visualization for Investment Decision-Making",
        "comments": "5 pages, 10 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Navigating the intricate landscape of financial markets requires adept\nforecasting of stock price movements. This paper delves into the potential of\nLong Short-Term Memory (LSTM) networks for predicting stock dynamics, with a\nfocus on discerning nuanced rise and fall patterns. Leveraging a dataset from\nthe New York Stock Exchange (NYSE), the study incorporates multiple features to\nenhance LSTM's capacity in capturing complex patterns. Visualization of key\nattributes, such as opening, closing, low, and high prices, aids in unraveling\nsubtle distinctions crucial for comprehensive market understanding. The\nmeticulously crafted LSTM input structure, inspired by established guidelines,\nincorporates both price and volume attributes over a 25-day time step, enabling\nthe model to capture temporal intricacies. A comprehensive methodology,\nincluding hyperparameter tuning with Grid Search, Early Stopping, and Callback\nmechanisms, leads to a remarkable 53% improvement in predictive accuracy. The\nstudy concludes with insights into model robustness, contributions to financial\nforecasting literature, and a roadmap for real-time stock market prediction.\nThe amalgamation of LSTM networks, strategic hyperparameter tuning, and\ninformed feature selection presents a potent framework for advancing the\naccuracy of stock price predictions, contributing substantively to financial\ntime series forecasting discourse.\n"
    },
    {
        "paper_id": 2403.18823,
        "authors": "Kapil Panda",
        "title": "Artificial Intelligence-based Analysis of Change in Public Finance\n  between US and International Markets",
        "comments": "5 pages, 2 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Public finances are one of the fundamental mechanisms of economic governance\nthat refer to the financial activities and decisions made by government\nentities to fund public services, projects, and operations through assets. In\ntoday's globalized landscape, even subtle shifts in one nation's public debt\nlandscape can have significant impacts on that of international finances,\nnecessitating a nuanced understanding of the correlations between international\nand national markets to help investors make informed investment decisions.\nTherefore, by leveraging the capabilities of artificial intelligence, this\nstudy utilizes neural networks to depict the correlations between US and\nInternational Public Finances and predict the changes in international public\nfinances based on the changes in US public finances. With the neural network\nmodel achieving a commendable Mean Squared Error (MSE) value of 2.79, it is\nable to affirm a discernible correlation and also plot the effect of US market\nvolatility on international markets. To further test the accuracy and\nsignificance of the model, an economic analysis was conducted that aimed to\ncorrelate the changes seen by the results of the model with historical stock\nmarket changes. This model demonstrates significant potential for investors to\npredict changes in international public finances based on signals from US\nmarkets, marking a significant stride in comprehending the intricacies of\nglobal public finances and the role of artificial intelligence in decoding its\nmultifaceted patterns for practical forecasting.\n"
    },
    {
        "paper_id": 2403.18831,
        "authors": "Armand Mihai Cismaru",
        "title": "DeepTraderX: Challenging Conventional Trading Strategies with Deep\n  Learning in Multi-Threaded Market Simulations",
        "comments": "11 pages, 9 png figures, uses apalike.sty and SCITEPRESS.sty, to be\n  published in the proceedings of ICAART 2024",
        "journal-ref": "In Proceedings of the 16th International Conference on Agents and\n  Artificial Intelligence - Volume 3, ISBN 978-989-758-680-4, ISSN 2184-433X,\n  pages 412-421 (2024)",
        "doi": "10.5220/0000183700003636",
        "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
        "abstract": "  In this paper, we introduce DeepTraderX (DTX), a simple Deep Learning-based\ntrader, and present results that demonstrate its performance in a\nmulti-threaded market simulation. In a total of about 500 simulated market\ndays, DTX has learned solely by watching the prices that other strategies\nproduce. By doing this, it has successfully created a mapping from market data\nto quotes, either bid or ask orders, to place for an asset. Trained on\nhistorical Level-2 market data, i.e., the Limit Order Book (LOB) for specific\ntradable assets, DTX processes the market state $S$ at each timestep $T$ to\ndetermine a price $P$ for market orders. The market data used in both training\nand testing was generated from unique market schedules based on real historic\nstock market data. DTX was tested extensively against the best strategies in\nthe literature, with its results validated by statistical analysis. Our\nfindings underscore DTX's capability to rival, and in many instances, surpass,\nthe performance of public-domain traders, including those that outclass human\ntraders, emphasising the efficiency of simple models, as this is required to\nsucceed in intricate multi-threaded simulations. This highlights the potential\nof leveraging \"black-box\" Deep Learning systems to create more efficient\nfinancial markets.\n"
    },
    {
        "paper_id": 2403.18837,
        "authors": "Yasuko Kawahata",
        "title": "Repetitive Dilemma Games in Distribution Information Using Interplay of\n  Droop Quota: Meek's Method in Impact of Maximum Compensation and Minimum Cost\n  Routes in Information Role of Marginal Contribution in Two-Sided Matching\n  Markets",
        "comments": "Wallace's Law, Droop Quota, Meek's Method, Marginal Contribution,\n  Two-Sided Matching Market, Repetitive Dilemma Game, Maximum Compensation\n  Problem, Minimum Cost Pathways, Fake News, Fact-Checking, Information Market\n  Equilibrium",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  This paper is a preliminary report of the research plan and a digest of the\nresults and discussions. On research note explores the complex dynamics of fake\nnews dissemination and fact-checking costs within the framework of information\nmarkets and analyzes the equilibrium between supply and demand using the\nconcepts of droop quotas, Meek's method, and marginal contributions. By\nadopting a two-sided matching market perspective, we delve into scenarios in\nwhich markets are stable under the influence of fake news perceived as truth\nand those in which credibility prevails. Through the application of iterated\ndilemma game theory, we investigate the strategic choices of news providers\naffected by the costs associated with spreading fake news and fact-checking\nefforts. We further examine the maximum reward problem and strategies to\nminimize the cost path for spreading fake news, and consider a nuanced\nunderstanding of market segmentation into \"cheap\" and \"premium\" segments based\non the nature of the information being spread. Our analysis uses mathematical\nmodels and computational processes to identify stable equilibrium points that\nensure market stability in the face of deceptive information practices and\nprovide insight into effective strategies to enhance the informational health\nof the market. Through this comprehensive approach, this paper aims for a more\ntruthful and reliable perspective from which to observe information markets.\nThis paper is partially an attempt to utilize \"Generative AI\" and was written\nwith educational intent. There are currently no plans for it to become a\npeer-reviewed paper.\n"
    },
    {
        "paper_id": 2403.18839,
        "authors": "Jai Pal",
        "title": "Long Short-Term Memory Pattern Recognition in Currency Trading",
        "comments": "10 Pages, 8 Figures, 4 Listings",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  This study delves into the analysis of financial markets through the lens of\nWyckoff Phases, a framework devised by Richard D. Wyckoff in the early 20th\ncentury. Focusing on the accumulation pattern within the Wyckoff framework, the\nresearch explores the phases of trading range and secondary test, elucidating\ntheir significance in understanding market dynamics and identifying potential\ntrading opportunities. By dissecting the intricacies of these phases, the study\nsheds light on the creation of liquidity through market structure, offering\ninsights into how traders can leverage this knowledge to anticipate price\nmovements and make informed decisions. The effective detection and analysis of\nWyckoff patterns necessitate robust computational models capable of processing\ncomplex market data, with spatial data best analyzed using Convolutional Neural\nNetworks (CNNs) and temporal data through Long Short-Term Memory (LSTM) models.\nThe creation of training data involves the generation of swing points,\nrepresenting significant market movements, and filler points, introducing noise\nand enhancing model generalization. Activation functions, such as the sigmoid\nfunction, play a crucial role in determining the output behavior of neural\nnetwork models. The results of the study demonstrate the remarkable efficacy of\ndeep learning models in detecting Wyckoff patterns within financial data,\nunderscoring their potential for enhancing pattern recognition and analysis in\nfinancial markets. In conclusion, the study highlights the transformative\npotential of AI-driven approaches in financial analysis and trading strategies,\nwith the integration of AI technologies shaping the future of trading and\ninvestment practices.\n"
    },
    {
        "paper_id": 2403.19051,
        "authors": "Shahin Heidari, Shannon Hashemi, Mohammad-Soroush Khorsand, Alireza\n  Daneshfar, Seyedalireza Jazayerifar",
        "title": "Towards Standardized Regulations for Block Chain Smart Contracts:\n  Insights from Delphi and SWARA Analysis",
        "comments": null,
        "journal-ref": "Amity Business School, Amity University Madhya Pradesh, Vol. XI,\n  Issue II, July-December, (2023), pp: (1-15)",
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  The rise of digital currency and the public ledger Block Chain has led to the\ndevelopment of a new type of electronic contract known as \"smart contracts.\"\nFor these contracts to be considered valid, they must adhere to traditional\ncontract rules and be concluded without any impediments. Once written,\nencrypted, and signed, smart contracts are recorded in the Block Chain Ledger,\nproviding transparent and secure record-keeping. Smart contracts offer several\nbenefits, including their ability to execute automatically without requiring\nhuman intervention, their provision of public visibility of contract provisions\non the Block Chain, their avoidance of financial crimes like Money Laundering,\nand their prevention of contract abuses. However, disputes arising from smart\ncontracts still require human intervention, presenting unique challenges in\nenforcing these contracts, such as evidentiary issues, enforceability of\nwaivers of defenses, and jurisdictional and choice-of-law considerations. Due\nto the novel nature of smart contracts, there are currently no standardized\nregulations that apply to them. Countries that have approved them have turned\nto customary law to legitimize their use. The Delphi method was used to\nidentify critical success factors for applying blockchain transactions in a\nmanufacturing company. Stepwise Weight Assessment Ratio Analysis (SWARA) was\nthen utilized to determine the most influential factors. The proposed\nmethodology was implemented, and results show that the most influential factors\nfor the successful application of blockchain transactions as smart contracts in\na manufacturing company are: turnover, the counter argument, vision, components\nfor building, and system outcome quality. Conversely, connections with\ngovernment entities and subcontractors, and the guarantee of quality have the\nleast influence on successful implementation.\n"
    },
    {
        "paper_id": 2403.19077,
        "authors": "Vijay Mohan and Peyman Khezr",
        "title": "Blockchains, MEV and the knapsack problem: a primer",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  In this paper, we take a close look at a problem labeled maximal extractable\nvalue (MEV), which arises in a blockchain due to the ability of a block\nproducer to manipulate the order of transactions within a block. Indeed,\nblockchains such as Ethereum have spent considerable resources addressing this\nissue and have redesigned the block production process to account for MEV. This\npaper provides an overview of the MEV problem and tracks how Ethereum has\nadapted to its presence. A vital aspect of the block building exercise is that\nit is a variant of the knapsack problem. Consequently, this paper highlights\nthe role of designing auctions to fill a knapsack--or knapsack auctions--in\nalleviating the MEV problem. Overall, this paper presents a survey of the main\nissues and an accessible primer for researchers and students wishing to explore\nthe economics of block building and MEV further.\n"
    },
    {
        "paper_id": 2403.19502,
        "authors": "Stijn De Backer, Luis E. C. Rocha, Jan Ryckebusch, Koen Schoors",
        "title": "On the potential of quantum walks for modeling financial return\n  distributions",
        "comments": "23 pages, 8 figures, 2 tables",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Accurate modeling of the temporal evolution of asset prices is crucial for\nunderstanding financial markets. We explore the potential of discrete-time\nquantum walks to model the evolution of asset prices. Return distributions\nobtained from a model based on the quantum walk algorithm are compared with\nthose obtained from classical methodologies. We focus on specific limitations\nof the classical models, and illustrate that the quantum walk model possesses\ngreat flexibility in overcoming these. This includes the potential to generate\nasymmetric return distributions with complex market tendencies and higher\nprobabilities for extreme events than in some of the classical models.\nFurthermore, the temporal evolution in the quantum walk possesses the potential\nto provide asset price dynamics.\n"
    },
    {
        "paper_id": 2403.19563,
        "authors": "Dmitry Arkhangelsky, Kazuharu Yanagimoto, Tom Zohar",
        "title": "Flexible Analysis of Individual Heterogeneity in Event Studies:\n  Application to the Child Penalty",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We provide a practical toolkit for analyzing effect heterogeneity in event\nstudies. We develop an estimation algorithm and adapt existing econometric\nresults to provide its theoretical justification. We apply these tools to Dutch\nadministrative data to study individual heterogeneity in the child-penalty (CP)\ncontext in three ways. First, we document significant heterogeneity in the\nindividual-level CP trajectories, emphasizing the importance of going beyond\nthe average CP. Second, we use individual-level estimates to examine the impact\nof childcare supply expansion policies. Our approach uncovers nonlinear\ntreatment effects, challenging the conventional policy evaluation methods\nconstrained to less flexible specifications. Third, we use the individual-level\nestimates as a regressor on the right-hand side to study the intergenerational\nelasticity of the CP between mothers and daughters. After adjusting for the\nmeasurement error bias, we find the elasticity of 24\\%. Our methodological\nframework contributes to empirical practice by offering a flexible approach\ntailored to specific research questions and contexts. We provide an open-source\npackage ('unitdid') to facilitate widespread adoption.\n"
    },
    {
        "paper_id": 2403.19735,
        "authors": "Taejin Park",
        "title": "Enhancing Anomaly Detection in Financial Markets with an LLM-based\n  Multi-Agent Framework",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper introduces a Large Language Model (LLM)-based multi-agent\nframework designed to enhance anomaly detection within financial market data,\ntackling the longstanding challenge of manually verifying system-generated\nanomaly alerts. The framework harnesses a collaborative network of AI agents,\neach specialised in distinct functions including data conversion, expert\nanalysis via web research, institutional knowledge utilization or\ncross-checking and report consolidation and management roles. By coordinating\nthese agents towards a common objective, the framework provides a comprehensive\nand automated approach for validating and interpreting financial data\nanomalies. I analyse the S&P 500 index to demonstrate the framework's\nproficiency in enhancing the efficiency, accuracy and reduction of human\nintervention in financial market monitoring. The integration of AI's autonomous\nfunctionalities with established analytical methods not only underscores the\nframework's effectiveness in anomaly detection but also signals its broader\napplicability in supporting financial market monitoring.\n"
    },
    {
        "paper_id": 2403.19781,
        "authors": "Zhiyuan Yao, Zheng Li, Matthew Thomas, Ionut Florescu",
        "title": "Reinforcement Learning in Agent-Based Market Simulation: Unveiling\n  Realistic Stylized Facts and Behavior",
        "comments": "Accpeted in IJCNN 2024",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
        "abstract": "  Investors and regulators can greatly benefit from a realistic market\nsimulator that enables them to anticipate the consequences of their decisions\nin real markets. However, traditional rule-based market simulators often fall\nshort in accurately capturing the dynamic behavior of market participants,\nparticularly in response to external market impact events or changes in the\nbehavior of other participants. In this study, we explore an agent-based\nsimulation framework employing reinforcement learning (RL) agents. We present\nthe implementation details of these RL agents and demonstrate that the\nsimulated market exhibits realistic stylized facts observed in real-world\nmarkets. Furthermore, we investigate the behavior of RL agents when confronted\nwith external market impacts, such as a flash crash. Our findings shed light on\nthe effectiveness and adaptability of RL-based agents within the simulation,\noffering insights into their response to significant market events.\n"
    },
    {
        "paper_id": 2403.19915,
        "authors": "Ardyn Nordstrom, Morgan Nordstrom, Matthew D. Webb",
        "title": "Using Images as Covariates: Measuring Curb Appeal with Deep Learning",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper details an innovative methodology to integrate image data into\ntraditional econometric models. Motivated by forecasting sales prices for\nresidential real estate, we harness the power of deep learning to add\n\"information\" contained in images as covariates. Specifically, images of homes\nwere categorized and encoded using an ensemble of image classifiers (ResNet-50,\nVGG16, MobileNet, and Inception V3). Unique features presented within each\nimage were further encoded through panoptic segmentation. Forecasts from a\nneural network trained on the encoded data results in improved out-of-sample\npredictive power. We also combine these image-based forecasts with standard\nhedonic real estate property and location characteristics, resulting in a\nunified dataset. We show that image-based forecasts increase the accuracy of\nhedonic forecasts when encoded features are regarded as additional covariates.\nWe also attempt to \"explain\" which covariates the image-based forecasts are\nmost highly correlated with. The study exemplifies the benefits of\ninterdisciplinary methodologies, merging machine learning and econometrics to\nharness untapped data sources for more accurate forecasting.\n"
    },
    {
        "paper_id": 2403.20039,
        "authors": "Didarul Islam, Mohammad Abdullah Al Faisal",
        "title": "Variability in Aggregate Personal Income Across Industrial Sectors\n  During COVID-19 Shock: A Time-Series Exploration",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This study explored the variability in Aggregate Personal Income (PI) across\n13 major industrial sectors in the US during the COVID-19 pandemic. Utilizing\ntime-series data from 2010 Q1 to 2019 Q4, we employed Autoregressive Integrated\nMoving Average (ARIMA) models to establish baseline trends in Personal Income\n(PI) before the pandemic. We then extended these models to forecast PI values\nfor the subsequent 14 quarters, from 2020 Q1 to 2023 Q2, as if the pandemic had\nnever happened. This forecasted data was compared with the actual PI data\ncollected during the pandemic to quantify its impacts. This approach allowed\nfor the assessment of both immediate and extended effects of COVID-19 on\nsector-specific PI. Our study highlighted the resilience of PI in sectors like\nUtilities, Retail, Finance, Real Estate, and Healthcare, with Farming showing\nan early recovery in PI, despite significant initial setbacks. In contrast, PI\nin Accommodation and Food Services experienced delayed recovery, contributing\nsignificantly to the overall impact variance alongside Farming (53.26\\% and\n33.26\\% respectively). Finance and Utilities demonstrated positive deviations,\nsuggesting a lesser impact or potential benefit in early pandemic stages.\nMeanwhile, sectoral PI in Manufacturing, Wholesale and Education showed\nmoderate recovery, whereas Construction and Government lagged in resilience.\nThe aggregate economic impact, initially negative at -0.027 in 2020 Q1,\ndrastically worsened to -1.42 in Q2, but improved by Q4, reflecting a broader\ntrend of adaptation and resilience across all the sectors during the pandemic.\n"
    },
    {
        "paper_id": 2403.20171,
        "authors": "Yuyu Chen, Paul Embrechts, Ruodu Wang",
        "title": "Risk exchange under infinite-mean Pareto models",
        "comments": "arXiv admin note: substantial text overlap with arXiv:2208.08471",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We study the optimal decisions of agents who aim to minimize their risks by\nallocating their positions over extremely heavy-tailed (i.e., infinite-mean)\nand possibly dependent losses. The loss distributions of our focus are\nsuper-Pareto distributions which include the class of extremely heavy-tailed\nPareto distributions. For a portfolio of super-Pareto losses,\nnon-diversification is preferred by decision makers equipped with well-defined\nand monotone risk measures. The phenomenon that diversification is not\nbeneficial in the presence of super-Pareto losses is further illustrated by an\nequilibrium analysis in a risk exchange market. First, agents with super-Pareto\nlosses will not share risks in a market equilibrium. Second, transferring\nlosses from agents bearing super-Pareto losses to external parties without any\nlosses may arrive at an equilibrium which benefits every party involved. The\nempirical studies show that extremely heavy tails exist in real datasets.\n"
    },
    {
        "paper_id": 2403.2031,
        "authors": "Soheila Khajoui, Saeid Dehyadegari, Sayyed Abdolmajid Jalaee",
        "title": "Predicting the impact of e-commerce indices on international trade in\n  Iran and other selected members of the Organization for Economic Co-operation\n  and Development (OECD) by using the artificial intelligence and P-VAR model",
        "comments": "10 pages, 3 figures, 3 tables",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This study aims at predicting the impact of e-commerce indicators on\ninternational trade of the selected OECD countries and Iran, by using the\nartificial intelligence approach and P-VAR. According to the nature of export,\nimport, GDP, and ICT functions, and the characteristics of nonlinearity, this\nanalysis is performed by using the MPL neural network. The export, import, GDP,\nand ICT findings were examined with 99 percent accuracy. Using the P-VAR model\nin the Eviews software, the initial database and predicted data were applied to\nestimate the impact of e-commerce on international trade. The findings from\nanalyzing the data show that there is a bilateral correlation between\ne-commerce which means that ICT and international trade affect each other and\nthe Goodness of fit of the studied model is confirmed.\n"
    },
    {
        "paper_id": 2404.00012,
        "authors": "Baptiste Lefort, Eric Benhamou, Jean-Jacques Ohana, David Saltiel,\n  Beatrice Guez, Thomas Jacquot",
        "title": "Stress index strategy enhanced with financial news sentiment analysis\n  for the equity markets",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  This paper introduces a new risk-on risk-off strategy for the stock market,\nwhich combines a financial stress indicator with a sentiment analysis done by\nChatGPT reading and interpreting Bloomberg daily market summaries. Forecasts of\nmarket stress derived from volatility and credit spreads are enhanced when\ncombined with the financial news sentiment derived from GPT-4. As a result, the\nstrategy shows improved performance, evidenced by higher Sharpe ratio and\nreduced maximum drawdowns. The improved performance is consistent across the\nNASDAQ, the S&P 500 and the six major equity markets, indicating that the\nmethod generalises across equities markets.\n"
    },
    {
        "paper_id": 2404.00013,
        "authors": "Debarati Chakraborty and Ravi Ranjan",
        "title": "Missing Data Imputation With Granular Semantics and AI-driven Pipeline\n  for Bankruptcy Prediction",
        "comments": "15 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This work focuses on designing a pipeline for the prediction of bankruptcy.\nThe presence of missing values, high dimensional data, and highly\nclass-imbalance databases are the major challenges in the said task. A new\nmethod for missing data imputation with granular semantics has been introduced\nhere. The merits of granular computing have been explored here to define this\nmethod. The missing values have been predicted using the feature semantics and\nreliable observations in a low-dimensional space, in the granular space. The\ngranules are formed around every missing entry, considering a few of the highly\ncorrelated features and most reliable closest observations to preserve the\nrelevance and reliability, the context, of the database against the missing\nentries. An intergranular prediction is then carried out for the imputation\nwithin those contextual granules. That is, the contextual granules enable a\nsmall relevant fraction of the huge database to be used for imputation and\novercome the need to access the entire database repetitively for each missing\nvalue. This method is then implemented and tested for the prediction of\nbankruptcy with the Polish Bankruptcy dataset. It provides an efficient\nsolution for big and high-dimensional datasets even with large imputation\nrates. Then an AI-driven pipeline for bankruptcy prediction has been designed\nusing the proposed granular semantic-based data filling method followed by the\nsolutions to the issues like high dimensional dataset and high class-imbalance\nin the dataset. The rest of the pipeline consists of feature selection with the\nrandom forest for reducing dimensionality, data balancing with SMOTE, and\nprediction with six different popular classifiers including deep NN. All\nmethods defined here have been experimentally verified with suitable\ncomparative studies and proven to be effective on all the data sets captured\nover the five years.\n"
    },
    {
        "paper_id": 2404.00015,
        "authors": "Javier Mancilla, Andr\\'e Sequeira, Tomas Tagliani, Francisco Llaneza,\n  Claudio Beiza",
        "title": "Empowering Credit Scoring Systems with Quantum-Enhanced Machine Learning",
        "comments": "Preprint",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Quantum Kernels are projected to provide early-stage usefulness for quantum\nmachine learning. However, highly sophisticated classical models are hard to\nsurpass without losing interpretability, particularly when vast datasets can be\nexploited. Nonetheless, classical models struggle once data is scarce and\nskewed. Quantum feature spaces are projected to find better links between data\nfeatures and the target class to be predicted even in such challenging\nscenarios and most importantly, enhanced generalization capabilities. In this\nwork, we propose a novel approach called Systemic Quantum Score (SQS) and\nprovide preliminary results indicating potential advantage over purely\nclassical models in a production grade use case for the Finance sector. SQS\nshows in our specific study an increased capacity to extract patterns out of\nfewer data points as well as improved performance over data-hungry algorithms\nsuch as XGBoost, providing advantage in a competitive market as it is the\nFinTech and Neobank regime.\n"
    },
    {
        "paper_id": 2404.00028,
        "authors": "Peng Liu",
        "title": "Antinetwork among China A-shares",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The correlation-based financial networks, constructed with the correlation\nrelationships among the time series of fluctuations of daily logarithmic prices\nof stocks, are intensively studied. However, these studies ignore the\nimportance of negative correlations. This paper is the first time to consider\nthe negative and positive correlations separately, and accordingly to construct\nweighted temporal antinetwork and network among stocks listed in the Shanghai\nand Shenzhen stock exchanges. For (anti)networks during the first 24 years of\nthe 21st century, the node's degree and strength, the assortativity\ncoefficient, the average local clustering coefficient, and the average shortest\npath length are analyzed systematically. This paper unveils some essential\ndifferences in these topological measurements between antinetwork and network.\nThe findings of the differences between antinetwork and network have an\nimportant role in understanding the dynamics of a financial complex system. The\nobservation of antinetwork is of great importance in optimizing investment\nportfolios and risk management. More importantly, this paper proposes a new\ndirection for studying complex systems, namely the correlation-based\nantinetwork.\n"
    },
    {
        "paper_id": 2404.00034,
        "authors": "Junliang Luo, Stefan Kitzler, Pietro Saggese",
        "title": "Investigating Similarities Across Decentralized Financial (DeFi)\n  Services",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  We explore the adoption of graph representation learning (GRL) algorithms to\ninvestigate similarities across services offered by Decentralized Finance\n(DeFi) protocols. Following existing literature, we use Ethereum transaction\ndata to identify the DeFi building blocks. These are sets of protocol-specific\nsmart contracts that are utilized in combination within single transactions and\nencapsulate the logic to conduct specific financial services such as swapping\nor lending cryptoassets. We propose a method to categorize these blocks into\nclusters based on their smart contract attributes and the graph structure of\ntheir smart contract calls. We employ GRL to create embedding vectors from\nbuilding blocks and agglomerative models for clustering them. To evaluate\nwhether they are effectively grouped in clusters of similar functionalities, we\nassociate them with eight financial functionality categories and use this\ninformation as the target label. We find that in the best-case scenario purity\nreaches .888. We use additional information to associate the building blocks\nwith protocol-specific target labels, obtaining comparable purity (.864) but\nhigher V-Measure (.571); we discuss plausible explanations for this difference.\nIn summary, this method helps categorize existing financial products offered by\nDeFi protocols, and can effectively automatize the detection of similar DeFi\nservices, especially within protocols.\n"
    },
    {
        "paper_id": 2404.0006,
        "authors": "Yejin Kim, Youngbin Lee, Minyoung Choe, Sungju Oh, Yongjae Lee",
        "title": "Temporal Graph Networks for Graph Anomaly Detection in Financial\n  Networks",
        "comments": "Presented at the AAAI 2024 Workshop on AI in Finance for Social\n  Impact (https://sites.google.com/view/aifin-aaai2024)",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  This paper explores the utilization of Temporal Graph Networks (TGN) for\nfinancial anomaly detection, a pressing need in the era of fintech and\ndigitized financial transactions. We present a comprehensive framework that\nleverages TGN, capable of capturing dynamic changes in edges within financial\nnetworks, for fraud detection. Our study compares TGN's performance against\nstatic Graph Neural Network (GNN) baselines, as well as cutting-edge hypergraph\nneural network baselines using DGraph dataset for a realistic financial\ncontext. Our results demonstrate that TGN significantly outperforms other\nmodels in terms of AUC metrics. This superior performance underlines TGN's\npotential as an effective tool for detecting financial fraud, showcasing its\nability to adapt to the dynamic and complex nature of modern financial systems.\nWe also experimented with various graph embedding modules within the TGN\nframework and compared the effectiveness of each module. In conclusion, we\ndemonstrated that, even with variations within TGN, it is possible to achieve\ngood performance in the anomaly detection task.\n"
    },
    {
        "paper_id": 2404.00183,
        "authors": "Andrew Balthrop and Hyunseok Jung",
        "title": "Shared Hardships Strengthen Bonds: Negative Shocks, Embeddedness and\n  Employee Retention",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Unexpected events -- \"shocks\" -- are the motive force in explaining changes\nin embeddedness and retention within the unfolding model of labor turnover.\nSubstantial research effort has examined strategies for insulating valued\nemployees from adverse shocks. However, this paper provides empirical evidence\nthat unambiguously negative shocks can increase employee retention when\nunderlying firm and employee incentives with respect to these shocks are\naligned. Using survival analysis on a unique data set of 466,236 communication\nrecords and 45,873 employment spells from 21 trucking companies, we show how\nequipment-related shocks tend to increase the duration of employment. Equipment\nshocks also generate paradoxically positive sentiments that demonstrate an\nincrease in employees' affective commitment to the firm. Our results highlight\nthe important moderating role aligned incentives have in how shocks ultimately\ntranslate into retention. Shared hardships strengthen bonds in employment as in\nother areas.\n"
    },
    {
        "paper_id": 2404.00187,
        "authors": "Bahar Arslan, Vanni Noferini, Spyridon Vrontos",
        "title": "Portfolio management using graph centralities: Review and comparison",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  We investigate an application of network centrality measures to portfolio\noptimization, by generalizing the method in [Pozzi, Di Matteo and Aste,\n\\emph{Spread of risks across financial markets: better to invest in the\nperipheries}, Scientific Reports 3:1665, 2013], that however had significant\nlimitations with respect to the state of the art in network theory. In this\npaper, we systematically compare many possible variants of the originally\nproposed method on S\\&P 500 stocks. We use daily data from twenty-seven years\nas training set and their following year as test set. We thus select the best\nnetwork-based methods according to different viewpoints including for instance\nthe highest Sharpe Ratio and the highest expected return. We give emphasis in\nnew centrality measures and we also conduct a thorough analysis, which reveals\nsignificantly stronger results compared to those with more traditional methods.\nAccording to our analysis, this graph-theoretical approach to investment can be\nused successfully by investors with different investment profiles leading to\nhigh risk-adjusted returns.\n"
    },
    {
        "paper_id": 2404.00424,
        "authors": "Zhaofeng Zhang, Banghao Chen, Shengxin Zhu, Nicolas Langren\\'e",
        "title": "From attention to profit: quantitative trading strategy based on\n  transformer",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  In traditional quantitative trading practice, navigating the complicated and\ndynamic financial market presents a persistent challenge. Former machine\nlearning approaches have struggled to fully capture various market variables,\noften ignore long-term information and fail to catch up with essential signals\nthat may lead the profit. This paper introduces an enhanced transformer\narchitecture and designs a novel factor based on the model. By transfer\nlearning from sentiment analysis, the proposed model not only exploits its\noriginal inherent advantages in capturing long-range dependencies and modelling\ncomplex data relationships but is also able to solve tasks with numerical\ninputs and accurately forecast future returns over a period. This work collects\nmore than 5,000,000 rolling data of 4,601 stocks in the Chinese capital market\nfrom 2010 to 2019. The results of this study demonstrated the model's superior\nperformance in predicting stock trends compared with other 100 factor-based\nquantitative strategies with lower turnover rates and a more robust half-life\nperiod. Notably, the model's innovative use transformer to establish factors,\nin conjunction with market sentiment information, has been shown to enhance the\naccuracy of trading signals significantly, thereby offering promising\nimplications for the future of quantitative trading strategies.\n"
    },
    {
        "paper_id": 2404.00806,
        "authors": "Sara Fish, Yannai A. Gonczarowski, Ran I. Shorrer",
        "title": "Algorithmic Collusion by Large Language Models",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The rise of algorithmic pricing raises concerns of algorithmic collusion. We\nconduct experiments with algorithmic pricing agents based on Large Language\nModels (LLMs), and specifically GPT-4. We find that (1) LLM-based agents are\nadept at pricing tasks, (2) LLM-based pricing agents autonomously collude in\noligopoly settings to the detriment of consumers, and (3) variation in\nseemingly innocuous phrases in LLM instructions (\"prompts\") may increase\ncollusion. These results extend to auction settings. Our findings underscore\nthe need for antitrust regulation regarding algorithmic pricing, and uncover\nregulatory challenges unique to LLM-based pricing agents.\n"
    },
    {
        "paper_id": 2404.00825,
        "authors": "Nolan Alexander, William Scherer",
        "title": "Using Machine Learning to Forecast Market Direction with Efficient\n  Frontier Coefficients",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  We propose a novel method to improve estimation of asset returns for\nportfolio optimization. This approach first performs a monthly directional\nmarket forecast using an online decision tree. The decision tree is trained on\na novel set of features engineered from portfolio theory: the efficient\nfrontier functional coefficients. Efficient frontiers can be decomposed to\ntheir functional form, a square-root second-order polynomial, and the\ncoefficients of this function captures the information of all the constituents\nthat compose the market in the current time period. To make these forecasts\nactionable, these directional forecasts are integrated to a portfolio\noptimization framework using expected returns conditional on the market\nforecast as an estimate for the return vector. This conditional expectation is\ncalculated using the inverse Mills ratio, and the Capital Asset Pricing Model\nis used to translate the market forecast to individual asset forecasts. This\nnovel method outperforms baseline portfolios, as well as other feature sets\nincluding technical indicators and the Fama-French factors. To empirically\nvalidate the proposed model, we employ a set of market sector ETFs.\n"
    },
    {
        "paper_id": 2404.01333,
        "authors": "Tatsuru Kikuchi",
        "title": "Methods of Stochastic Field Theory in Non-Equilibrium Systems --\n  Spontaneous Symmetry Breaking of Ergodicity",
        "comments": "42 pages, including a review article",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Recently, a couple of investigations related to symmetry breaking phenomena,\n'spontaneous stochasticity' and 'ergodicity breaking' have led to significant\nimpacts in a variety of fields related to the stochastic processes such as\neconomics and finance. We investigate on the origins and effects of those\noriginal symmetries in the action from the mathematical and the effective field\ntheory points of view. It is naturally expected that whenever the system\nrespects any symmetry, it would be spontaneously broken once the system falls\ninto a vacuum state which minimizes an effective action of the dynamical\nsystem.\n"
    },
    {
        "paper_id": 2404.01337,
        "authors": "Silvia Garc\\'ia-M\\'endez, Francisco de Arriba-P\\'erez, Ana\n  Barros-Vila, Francisco J. Gonz\\'alez-Casta\\~no",
        "title": "Detection of Temporality at Discourse Level on Financial News by\n  Combining Natural Language Processing and Machine Learning",
        "comments": null,
        "journal-ref": null,
        "doi": "10.1016/j.eswa.2022.116648",
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  Finance-related news such as Bloomberg News, CNN Business and Forbes are\nvaluable sources of real data for market screening systems. In news, an expert\nshares opinions beyond plain technical analyses that include context such as\npolitical, sociological and cultural factors. In the same text, the expert\noften discusses the performance of different assets. Some key statements are\nmere descriptions of past events while others are predictions. Therefore,\nunderstanding the temporality of the key statements in a text is essential to\nseparate context information from valuable predictions. We propose a novel\nsystem to detect the temporality of finance-related news at discourse level\nthat combines Natural Language Processing and Machine Learning techniques, and\nexploits sophisticated features such as syntactic and semantic dependencies.\nMore specifically, we seek to extract the dominant tenses of the main\nstatements, which may be either explicit or implicit. We have tested our system\non a labelled dataset of finance-related news annotated by researchers with\nknowledge in the field. Experimental results reveal a high detection precision\ncompared to an alternative rule-based baseline approach. Ultimately, this\nresearch contributes to the state-of-the-art of market screening by identifying\npredictive knowledge for financial decision making.\n"
    },
    {
        "paper_id": 2404.01338,
        "authors": "Silvia Garc\\'ia-M\\'endez, Francisco de Arriba-P\\'erez, Ana\n  Barros-Vila, Francisco J. Gonz\\'alez-Casta\\~no, Enrique Costa-Montenegro",
        "title": "Automatic detection of relevant information, predictions and forecasts\n  in financial news through topic modelling with Latent Dirichlet Allocation",
        "comments": null,
        "journal-ref": null,
        "doi": "10.1007/s10489-023-04452-4",
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Financial news items are unstructured sources of information that can be\nmined to extract knowledge for market screening applications. Manual extraction\nof relevant information from the continuous stream of finance-related news is\ncumbersome and beyond the skills of many investors, who, at most, can follow a\nfew sources and authors. Accordingly, we focus on the analysis of financial\nnews to identify relevant text and, within that text, forecasts and\npredictions. We propose a novel Natural Language Processing (NLP) system to\nassist investors in the detection of relevant financial events in unstructured\ntextual sources by considering both relevance and temporality at the discursive\nlevel. Firstly, we segment the text to group together closely related text.\nSecondly, we apply co-reference resolution to discover internal dependencies\nwithin segments. Finally, we perform relevant topic modelling with Latent\nDirichlet Allocation (LDA) to separate relevant from less relevant text and\nthen analyse the relevant text using a Machine Learning-oriented temporal\napproach to identify predictions and speculative statements. We created an\nexperimental data set composed of 2,158 financial news items that were manually\nlabelled by NLP researchers to evaluate our solution. The ROUGE-L values for\nthe identification of relevant text and predictions/forecasts were 0.662 and\n0.982, respectively. To our knowledge, this is the first work to jointly\nconsider relevance and temporality at the discursive level. It contributes to\nthe transfer of human associative discourse capabilities to expert systems\nthrough the combination of multi-paragraph topic segmentation and co-reference\nresolution to separate author expression patterns, topic modelling with LDA to\ndetect relevant text, and discursive temporality analysis to identify forecasts\nand predictions within this text.\n"
    },
    {
        "paper_id": 2404.01451,
        "authors": "Katalin Varga and Tibor Szendrei",
        "title": "Non-stationary Financial Risk Factors and Macroeconomic Vulnerability\n  for the UK",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Tracking the build-up of financial vulnerabilities is a key component of\nfinancial stability policy. Due to the complexity of the financial system, this\ntask is daunting, and there have been several proposals on how to manage this\ngoal. One way to do this is by the creation of indices that act as a signal for\nthe policy maker. While factor modelling in finance and economics has a rich\nhistory, most of the applications tend to focus on stationary factors.\nNevertheless, financial stress (and in particular tail events) can exhibit a\nhigh degree of inertia. This paper advocates moving away from the stationary\nparadigm and instead proposes non-stationary factor models as measures of\nfinancial stress. Key advantage of a non-stationary factor model is that while\nsome popular measures of financial stress describe the variance-covariance\nstructure of the financial stress indicators, the new index can capture the\ntails of the distribution. To showcase this, we use the obtained factors as\nvariables in a growth-at-risk exercise. This paper offers an overview of how to\nconstruct non-stationary dynamic factors of financial stress using the UK\nfinancial market as an example.\n"
    },
    {
        "paper_id": 2404.01522,
        "authors": "David Garc\\'ia-Lorite, Raul Merino",
        "title": "Watanabe's expansion: A Solution for the convexity conundrum",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper, we present a new method for pricing CMS derivatives. We use\nMallaivin's calculus to establish a model-free connection between the price of\na CMS derivative and a quadratic payoff. Then, we apply Watanabe's expansions\nto quadratic payoffs case under local and stochastic local volatility. Our\napproximations are generic. To evaluate their accuracy, we will compare the\napproximations numerically under the normal SABR model against the market\nstandards: Hagan's approximation, and a Monte Carlo simulation.\n"
    },
    {
        "paper_id": 2404.01624,
        "authors": "Qishuo Cheng",
        "title": "Intelligent Optimization of Mine Environmental Damage Assessment and\n  Repair Strategies Based on Deep Learning",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In recent decades, financial quantification has emerged and matured rapidly.\nFor financial institutions such as funds, investment institutions are\nincreasingly dissatisfied with the situation of passively constructing\ninvestment portfolios with average market returns, and are paying more and more\nattention to active quantitative strategy investment portfolios. This requires\nthe introduction of active stock investment fund management models. Currently,\nin my country's stock fund investment market, there are many active\nquantitative investment strategies, and the algorithms used vary widely, such\nas SVM, random forest, RNN recurrent memory network, etc. This article focuses\non this trend, using the emerging LSTM-GRU gate-controlled long short-term\nmemory network model in the field of financial stock investment as a basis to\nbuild a set of active investment stock strategies, and combining it with SVM,\nwhich has been widely used in the field of quantitative stock investment.\nComparing models such as RNN, theoretically speaking, compared to SVM that\nsimply relies on kernel functions for high-order mapping and classification of\ndata, neural network algorithms such as RNN and LSTM-GRU have better principles\nand are more suitable for processing financial stock data. Then, through\nmultiple By comparison, it was finally found that the LSTM- GRU gate-controlled\nlong short-term memory network has a better accuracy. By selecting the LSTM-GRU\nalgorithm to construct a trading strategy based on the Shanghai and Shenzhen\n300 Index constituent stocks, the parameters were adjusted and the neural layer\nconnection was adjusted. Finally, It has significantly outperformed the\nbenchmark index CSI 300 over the long term. The conclusion of this article is\nthat the research results can provide certain quantitative strategy references\nfor financial institutions to construct active stock investment portfolios.\n"
    },
    {
        "paper_id": 2404.01782,
        "authors": "Abdul Haris, Muhammad Munawir Syarif, Hamed Narolla, Rachmat Hidayat",
        "title": "Multicriteria Analysis Model in Sustainable Corn Farming Area Planning",
        "comments": "There are 12 pages, 9 fiqures and 36 references",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This study aims to develop a framework for multicriteria analysis to evaluate\nalternatives for sustainable corn agricultural area planning, considering the\nintegration of ecological, economic, and social aspects as pillars of\nsustainability. The research method uses qualitative and quantitative\napproaches to integrate ecological, economic, and social aspects in the\nmulticriteria analysis. The analysis involves land evaluation, subcriteria\nidentification, and data integration using Multidimensional Scaling and\nAnalytical Hierarchy Process methods to prioritize developing sustainable corn\nagricultural areas. Based on the results of the RAP-Corn analysis, it indicates\nthat the ecological dimension depicts less sustainability. The AHP results\nyield weight distribution and highly relevant scores that describe tangible\npreferences. Priority directions are grouped as strategic steps toward\nachieving the goals of sustainable corn agricultural area planning.\n"
    },
    {
        "paper_id": 2404.01866,
        "authors": "Bartosz Bieganowski, Robert Slepaczuk",
        "title": "Supervised Autoencoder MLP for Financial Time Series Forecasting",
        "comments": "29 pages, 28 figures, 17 tables",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
        "abstract": "  This paper investigates the enhancement of financial time series forecasting\nwith the use of neural networks through supervised autoencoders, aiming to\nimprove investment strategy performance. It specifically examines the impact of\nnoise augmentation and triple barrier labeling on risk-adjusted returns, using\nthe Sharpe and Information Ratios. The study focuses on the S&P 500 index,\nEUR/USD, and BTC/USD as the traded assets from January 1, 2010, to April 30,\n2022. Findings indicate that supervised autoencoders, with balanced noise\naugmentation and bottleneck size, significantly boost strategy effectiveness.\nHowever, excessive noise and large bottleneck sizes can impair performance,\nhighlighting the importance of precise parameter tuning. This paper also\npresents a derivation of a novel optimization metric that can be used with\ntriple barrier labeling. The results of this study have substantial policy\nimplications, suggesting that financial institutions and regulators could\nleverage techniques presented to enhance market stability and investor\nprotection, while also encouraging more informed and strategic investment\napproaches in various financial sectors.\n"
    },
    {
        "paper_id": 2404.02053,
        "authors": "Enmin Zhu, Jerome Yen",
        "title": "BERTopic-Driven Stock Market Predictions: Unraveling Sentiment Insights",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper explores the intersection of Natural Language Processing (NLP) and\nfinancial analysis, focusing on the impact of sentiment analysis in stock price\nprediction. We employ BERTopic, an advanced NLP technique, to analyze the\nsentiment of topics derived from stock market comments. Our methodology\nintegrates this sentiment analysis with various deep learning models, renowned\nfor their effectiveness in time series and stock prediction tasks. Through\ncomprehensive experiments, we demonstrate that incorporating topic sentiment\nnotably enhances the performance of these models. The results indicate that\ntopics in stock market comments provide implicit, valuable insights into stock\nmarket volatility and price trends. This study contributes to the field by\nshowcasing the potential of NLP in enriching financial analysis and opens up\navenues for further research into real-time sentiment analysis and the\nexploration of emotional and contextual aspects of market sentiment. The\nintegration of advanced NLP techniques like BERTopic with traditional financial\nanalysis methods marks a step forward in developing more sophisticated tools\nfor understanding and predicting market behaviors.\n"
    },
    {
        "paper_id": 2404.02175,
        "authors": "Javier Marin",
        "title": "Social Dynamics of Consumer Response: A Unified Framework Integrating\n  Statistical Physics and Marketing Dynamics",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Comprehending how consumers react to advertising inputs is essential for\nmarketers aiming to optimize advertising strategies and improve campaign\neffectiveness. This study examines the complex nature of consumer behaviour by\napplying theoretical frameworks derived from physics and social psychology. We\npresent an innovative equation that captures the relation between spending on\nadvertising and consumer response, using concepts such as symmetries, scaling\nlaws, and phase transitions. By validating our equation against well-known\nmodels such as the Michaelis-Menten and Hill equations, we prove its\neffectiveness in accurately representing the complexity of consumer response\ndynamics. The analysis emphasizes the importance of key model parameters, such\nas marketing effectiveness, response sensitivity, and behavioural sensitivity,\nin influencing consumer behaviour. The work explores the practical implications\nfor advertisers and marketers, as well as discussing the limitations and future\nresearch directions. In summary, this study provides a thorough framework for\ncomprehending and forecasting consumer reactions to advertising, which has\nimplications for optimizing advertising strategies and allocating resources.\n"
    },
    {
        "paper_id": 2404.0227,
        "authors": "Arkadiusz Lipiecki, Bartosz Uniejewski and Rafa{\\l} Weron",
        "title": "Postprocessing of point predictions for probabilistic forecasting of\n  electricity prices: Diversity matters",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Operational decisions relying on predictive distributions of electricity\nprices can result in significantly higher profits compared to those based\nsolely on point forecasts. However, the majority of models developed in both\nacademic and industrial settings provide only point predictions. To address\nthis, we examine three postprocessing methods for converting point forecasts\ninto probabilistic ones: Quantile Regression Averaging, Conformal Prediction,\nand the recently introduced Isotonic Distributional Regression. We find that\nwhile IDR demonstrates the most varied performance, combining its predictive\ndistributions with those of the other two methods results in an improvement of\nca. 7.5% compared to a benchmark model with normally distributed errors, over a\n4.5-year test period in the German power market spanning the COVID pandemic and\nthe war in Ukraine. Remarkably, the performance of this combination is at par\nwith state-of-the-art Distributional Deep Neural Networks.\n"
    },
    {
        "paper_id": 2404.02273,
        "authors": "Gregory M. Dickinson",
        "title": "Beyond Social Media Analogues",
        "comments": null,
        "journal-ref": "99 NYU L. Rev. Online 109 (2024)",
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  The steady flow of social-media cases toward the Supreme Court shows a nation\nreworking its fundamental relationship with technology. The cases raise a host\nof questions ranging from difficult to impossible: how to nurture a vibrant\npublic square when a few tech giants dominate the flow of information, how\nsocial media can be at the same time free from conformist groupthink and also\nprotected against harmful disinformation campaigns, and how government and\nindustry can cooperate on such problems without devolving toward censorship.\n  To such profound questions, this Essay offers a comparatively modest\ncontribution -- what not to do. Always the lawyer's instinct is toward analogy,\nconsidering what has come before and how it reveals what should come next.\nAlmost invariably, that is the right choice. The law's cautious evolution\nprotects society from disruptive change. But almost is not always, and, with\nsocial media, disruptive change is already upon us. Using social-media laws\nfrom Texas and Florida as a case study, this Essay shows how social-media's\ndistinct features render it poorly suited to analysis by analogy and argues\nthat courts should instead shift their attention toward crafting legal\ndoctrines targeted to address social media's unique ills.\n"
    },
    {
        "paper_id": 2404.02343,
        "authors": "Evangelia Dragazi, Shuaiqiang Liu, Antonis Papapantoleon",
        "title": "Improved model-free bounds for multi-asset options using option-implied\n  information and deep learning",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  We consider the computation of model-free bounds for multi-asset options in a\nsetting that combines dependence uncertainty with additional information on the\ndependence structure. More specifically, we consider the setting where the\nmarginal distributions are known and partial information, in the form of known\nprices for multi-asset options, is also available in the market. We provide a\nfundamental theorem of asset pricing in this setting, as well as a superhedging\nduality that allows to transform the maximization problem over probability\nmeasures in a more tractable minimization problem over trading strategies. The\nlatter is solved using a penalization approach combined with a deep learning\napproximation using artificial neural networks. The numerical method is fast\nand the computational time scales linearly with respect to the number of traded\nassets. We finally examine the significance of various pieces of additional\ninformation. Empirical evidence suggests that \"relevant\" information, i.e.\nprices of derivatives with the same payoff structure as the target payoff, are\nmore useful that other information, and should be prioritized in view of the\ntrade-off between accuracy and computational efficiency.\n"
    },
    {
        "paper_id": 2404.02497,
        "authors": "Lei Bill Wang, Om Prakash Bedant, Haoran Wang, Zhenbang Jiao",
        "title": "From Friendship Networks to Classroom Dynamics: Leveraging Neural\n  Networks, Instrumental Variable and Genetic Algorithms for Optimal\n  Educational Outcomes",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  This study uses data from the China Educational Panel Survey (CEPS) to design\na classroom assignment policy that maximizes peer effects. Our approach\ncomprises three steps: firstly, we develop a friendship formation discrete\nchoice model and estimate it with an interpretable neural network architecture,\nPeerNN, generating an adjacency-probability matrix $\\Omega$ that reflects\nfriendship formation probabilities. Secondly, we incorporate $\\Omega$ into a\nlinear-in-means model to estimate peer effects. The peer effect parameter,\n$\\beta$, has a different interpretation from the conventional linear-in-means\nmodel and opens up a strategic scope of mean-maximizing classroom assignment\npolicy. By exploiting the conditional random classroom assignment in many\nChinese middle schools, we construct a valid instrument to address the\nendogeneity issue induced by $\\Omega$ and consistently estimate $\\beta$.\nLastly, utilizing the estimates of $\\Omega$ and $\\beta$, we employ a genetic\nalgorithm (GA) to search for the mean-maximizing class assignment policy.\nThough the result is much more efficient (i.e. more positive average peer\neffect) than random classroom assignment (i.e. the current practice in most\nChinese middle schools), GA policy is highly inequitable: a small number of\nstudents are predicted to experience severely negative peer effects. To balance\nstudents' academic performance with educational equity, we propose a fairness\nmetric and penalize classroom assignment that generates large variances in peer\neffects. The modified method is called algorithmically fair genetic algorithm\n(AFGA). AFGA policy is less efficient but much more equitable. We allow\nuser-defined parameters for AFGA such that the school principals can adjust the\ntrade-off between efficiency and equity according to their preferences.\n"
    },
    {
        "paper_id": 2404.02498,
        "authors": "Sang Hu and Zihan Zhou",
        "title": "From Time-inconsistency to Time-consistency for Optimal Stopping\n  Problems",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  For optimal stopping problems with time-inconsistent preference, we measure\nthe inherent level of time-inconsistency by taking the time needed to turn the\nnaive strategies into the sophisticated ones. In particular, when in a repeated\nexperiment the naive agent can observe her actual sequence of actions which are\ninconsistent with what she has planned at the initial time, she then chooses\nher immediate action based on the observations on her later actual behavior.\nThe procedure is repeated until her actual sequence of actions are consistent\nwith her plan at any time. We show that for the preference value of cumulative\nprospect theory, in which the time-inconsistency is due to the probability\ndistortion, the higher the degree of probability distortion, the more severe\nthe level of time-inconsistency, and the more time required to turn the naive\nstrategies into the sophisticated ones.\n"
    },
    {
        "paper_id": 2404.02582,
        "authors": "Francesco Catalano and Laura Nasello and Daniel Guterding",
        "title": "Quantum computing approach to realistic ESG-friendly stock portfolios",
        "comments": "12 pages, 6 figures",
        "journal-ref": "Risks 12, 66 (2024)",
        "doi": "10.3390/risks12040066",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Finding an optimal balance between risk and returns in investment portfolios\nis a central challenge in quantitative finance, often addressed through\nMarkowitz portfolio theory (MPT). While traditional portfolio optimization is\ncarried out in a continuous fashion, as if stocks could be bought in fractional\nincrements, practical implementations often resort to approximations, as\nfractional stocks are typically not tradeable. While these approximations are\neffective for large investment budgets, they deteriorate as budgets decrease.\nTo alleviate this issue, a discrete Markowitz portfolio theory (DMPT) with\nfinite budgets and integer stock weights can be formulated, but results in a\nnon-polynomial (NP)-hard problem. Recent progress in quantum processing units\n(QPUs), including quantum annealers, makes solving DMPT problems feasible. Our\nstudy explores portfolio optimization on quantum annealers, establishing a\nmapping between continuous and discrete Markowitz portfolio theories. We find\nthat correctly normalized discrete portfolios converge to continuous solutions\nas budgets increase. Our DMPT implementation provides efficient frontier\nsolutions, outperforming traditional rounding methods, even for moderate\nbudgets. Responding to the demand for environmentally and socially responsible\ninvestments, we enhance our discrete portfolio optimization with ESG\n(environmental, social, governance) ratings for EURO STOXX 50 index stocks. We\nintroduce a utility function incorporating ESG ratings to balance risk, return,\nand ESG-friendliness, and discuss implications for ESG-aware investors.\n"
    },
    {
        "paper_id": 2404.02595,
        "authors": "Nouhaila Innan, Alberto Marchisio, Muhammad Shafique, and Mohamed\n  Bennai",
        "title": "QFNN-FFD: Quantum Federated Neural Network for Financial Fraud Detection",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  This study introduces the Quantum Federated Neural Network for Financial\nFraud Detection (QFNN-FFD), a cutting-edge framework merging Quantum Machine\nLearning (QML) and quantum computing with Federated Learning (FL) for financial\nfraud detection. Using quantum technologies' computational power and the robust\ndata privacy protections offered by FL, QFNN-FFD emerges as a secure and\nefficient method for identifying fraudulent transactions within the financial\nsector. Implementing a dual-phase training model across distributed clients\nenhances data integrity and enables superior performance metrics, achieving\nprecision rates consistently above 95%. Additionally, QFNN-FFD demonstrates\nexceptional resilience by maintaining an impressive 80% accuracy, highlighting\nits robustness and readiness for real-world applications. This combination of\nhigh performance, security, and robustness against noise positions QFNN-FFD as\na transformative advancement in financial technology solutions and establishes\nit as a new benchmark for privacy-focused fraud detection systems. This\nframework facilitates the broader adoption of secure, quantum-enhanced\nfinancial services and inspires future innovations that could use QML to tackle\ncomplex challenges in other areas requiring high confidentiality and accuracy.\n"
    },
    {
        "paper_id": 2404.02612,
        "authors": "Anastasia Cosma",
        "title": "Exploring Sustainable Clothing Consumption in Middle-Income Countries: A\n  case study of Romanian consumers",
        "comments": "arXiv admin note: This version has been removed by arXiv\n  administrators as the submitter did not have the right to agree to the\n  license at the time of submission",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  The overconsumption of consumers under today's increasingly scarce natural\nresources has overwhelmed the textile industry in middle-income countries, such\nas Romania. It is becoming more and more essential to encourage sustainable\nclothing consumption behaviors, such as purchasing recyclable clothes.\nNotwithstanding there is a limited number of studies trying to understand the\nintrinsic factors that motivate consumers' purchase intention toward\nsustainable clothes in middle-income countries. Moreover, the effect of\nconsumers' environmental knowledge on determining their purchase intention of\nsustainable clothes remains understudied. Consequently, the purpose of this\npaper is to make a significant contribution to the sustainable consumption\nliterature by providing a consolidated framework that explores the behavioral\nfactors inclining Romanian consumers' purchase intention towards sustainable\nclothes. The foundation of this study combines consumers' social value\norientation and the theory of planned behavior. the partial least square path\nmodelling procedure was used to analyze the data of 1,018 Romanian respondents.\nThe findings of this study show that altruistic value orientation, subjective\nnorms, and sustainable attitudes have a positive effect on Romanian consumers'\npurchase intention of sustainable clothing. Thus, these insights provide\nessential practical implications of advocating for the consumption of\nsustainable clothes along with useful guidelines for practitioners in the\ntextile industry among middle-income countries, especially in Romania, to\nreduce overconsumption.\n"
    },
    {
        "paper_id": 2404.02687,
        "authors": "Ezzat Elokda, Heinrich Nax, Saverio Bolognani, Florian D\\\"orfler",
        "title": "Karma: An Experimental Study",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  A system of non-tradable credits that flow between individuals like karma,\nhence proposed under that name, is a mechanism for repeated resource allocation\nthat comes with attractive efficiency and fairness properties, in theory. In\nthis study, we test karma in an online experiment in which human subjects\nrepeatedly compete for a resource with time-varying and stochastic individual\npreferences or urgency to acquire the resource. We confirm that karma has\nsignificant and sustained welfare benefits even in a population with no prior\ntraining. We identify mechanism usage in contexts with sporadic high urgency,\nmore so than with frequent moderate urgency, and implemented as an easy\n(binary) karma bidding scheme as particularly effective for welfare\nimprovements: relatively larger aggregate efficiency gains are realized that\nare (almost) Pareto superior. These findings provide guidance for further\ntesting and for future implementation plans of such mechanisms in the real\nworld.\n"
    },
    {
        "paper_id": 2404.02803,
        "authors": "Thomas Renault, David Restrepo Amariles, Aurore Troussel",
        "title": "Collaboratively adding context to social media posts reduces the sharing\n  of false news",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  We build a novel database of around 285,000 notes from the Twitter Community\nNotes program to analyze the causal influence of appending contextual\ninformation to potentially misleading posts on their dissemination. Employing a\ndifference in difference design, our findings reveal that adding context below\na tweet reduces the number of retweets by almost half. A significant, albeit\nsmaller, effect is observed when focusing on the number of replies or quotes.\nCommunity Notes also increase by 80% the probability that a tweet is deleted by\nits creator. The post-treatment impact is substantial, but the overall effect\non tweet virality is contingent upon the timing of the contextual information's\npublication. Our research concludes that, although crowdsourced fact-checking\nis effective, its current speed may not be adequate to substantially reduce the\ndissemination of misleading information on social media.\n"
    },
    {
        "paper_id": 2404.02858,
        "authors": "G. Apicella, A. Molent, M. Gaudenzi",
        "title": "The Life Care Annuity: enhancing product features and refining pricing\n  methods",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The state-of-the-art proposes Life Care Annuities, that have been recently\ndesigned as variable annuity contracts with Long-Term Care payouts and\nGuaranteed Lifelong Withdrawal Benefits. In this paper, we propose more general\nfeatures for these insurance products and refine their pricing methods. We name\nour proposed product ``GLWB-LTC''. In particular, as to the product features,\nwe allow dynamic withdrawal strategies, including the surrender option.\nFurthermore, we consider stochastic interest rates, described by a\nCox-Ingersoll-Ross process. As to the numerical methods, we solve the\nstochastic control problem involved by the selection of the optimal withdrawal\nstrategy through a robust tree method, which outperforms the Monte Carlo\napproach. We name this method ``Tree-LTC'', and we use it to estimate the fair\nprice of the product, as some relevant parameters vary, such as, for instance,\nthe entry age of the policyholder. Furthermore, our numerical results show how\nthe optimal withdrawal strategy varies over time with the health status of the\npolicyholder. Our findings stress the important advantage of flexible\nwithdrawal strategies in relation to insurance policies offering protection\nfrom health risks. Indeed, the policyholder is given more choice about how much\nto save for protection from the possible disability states at future times.\n"
    },
    {
        "paper_id": 2404.03508,
        "authors": "Rodolfo G. Campos, Benedikt Heid, Jacopo Timini",
        "title": "The economic consequences of geopolitical fragmentation: Evidence from\n  the Cold War",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The Cold War was the defining episode of geopolitical fragmentation in the\ntwentieth century. Trade between East and West across the Iron Curtain (a\nsymbolical and physical barrier dividing Europe into two distinct areas) was\nrestricted, but the severity of these restrictions varied over time. We\nquantify the trade and welfare effects of the Iron Curtain and show how the\ndifficulty of trading across the Iron Curtain fluctuated throughout the Cold\nWar. Using a novel dataset on trade between the two economic blocs and a\nquantitative trade model, we find that while the Iron Curtain at its height\nrepresented a tariff equivalent of 48% in 1951, trade between East and West\ngradually became easier until the fall of the Berlin Wall in 1989. Despite the\neasing of trade restrictions, we estimate that the Iron Curtain roughly halved\nEast-West trade flows and caused substantial welfare losses in the Eastern bloc\ncountries that persisted until the end of the Cold War. Conversely, the Iron\nCurtain led to an increase in intra-bloc trade, especially in the Eastern bloc,\nwhich outpaced the integration of Western Europe in the run-up to the formation\nof the European Union.\n"
    },
    {
        "paper_id": 2404.03581,
        "authors": "Alexander Erlei, Mattheus Brenig, Nils Engelbrecht",
        "title": "Consumer Behavior under Benevolent Price Discrimination",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Extensive research shows that consumers are generally averse to price\ndiscrimination. However, instruments of differential pricing can benefit\nconsumer surplus and alleviate inequity through targeted price discounts. This\npaper examines how these outcome considerations influence consumer reactions to\nprice discrimination. Six studies with 3951 participants show that a large\nshare of consumers is willing to costly switch away from a store that\nintroduces a discount for low-income consumers. This happens irrespective of\nwhether income differences are due to luck or merit. While the\nprice-discriminating store does attract some new high-income consumers, it\ncannot compensate the loss of existing consumers. Allowing for altruistic\npreferences by simulating a market mechanism increases costly support for price\ndiscounts, but does not alleviate consumer aversions. Finally, we provide\nevidence that warm glow drives costly support for price discounts.\n"
    },
    {
        "paper_id": 2404.03783,
        "authors": "Muqiao Huang and Ruodu Wang",
        "title": "Coherent risk measures and uniform integrability",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We establish a profound connection between coherent risk measures, a\nprominent object in quantitative finance, and uniform integrability, a\nfundamental concept in probability theory. Instead of working with absolute\nvalues of random variables, which is convenient in studying integrability, we\nwork directly with random loses and gains, which have clear financial\ninterpretation. We introduce a technical tool called the folding score of\ndistortion risk measures. The analysis of the folding score allows us to\nconvert some conditions on absolute values to those on gains and losses. As our\nmain results, we obtain three sets of equivalent conditions for uniform\nintegrability. In particular, a set is uniformly integrable if and only if one\ncan find a coherent distortion risk measure that is bounded on the set, but not\nfinite on $L^1$.\n"
    },
    {
        "paper_id": 2404.03792,
        "authors": "Domonkos F. Vamossy",
        "title": "Social Media Emotions and Market Behavior",
        "comments": "arXiv admin note: text overlap with arXiv:2112.03868",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  I explore the relationship between investor emotions expressed on social\nmedia and asset prices. The field has seen a proliferation of models aimed at\nextracting firm-level sentiment from social media data, though the behavior of\nthese models often remains uncertain. Against this backdrop, my study employs\nEmTract, an open-source emotion model, to test whether the emotional responses\nidentified on social media platforms align with expectations derived from\ncontrolled laboratory settings. This step is crucial in validating the\nreliability of digital platforms in reflecting genuine investor sentiment. My\nfindings reveal that firm-specific investor emotions behave similarly to lab\nexperiments and can forecast daily asset price movements. These impacts are\nlarger when liquidity is lower or short interest is higher. My findings on the\npersistent influence of sadness on subsequent returns, along with the\ninsignificance of the one-dimensional valence metric, underscores the\nimportance of dissecting emotional states. This approach allows for a deeper\nand more accurate understanding of the intricate ways in which investor\nsentiments drive market movements.\n"
    },
    {
        "paper_id": 2404.03794,
        "authors": "Scott Delhommer and Domonkos F. Vamossy",
        "title": "Effect of State and Local Sexual Orientation Anti-Discrimination Laws on\n  Labor Market Differentials",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper presents the first quasi-experimental research examining the\neffect of both local and state anti-discrimination laws on sexual orientation\non the labor supply and wages of lesbian, gay, and bisexual (LGB) workers. To\ndo so, we use the American Community Survey data on household composition to\ninfer sexual orientation and combine this with a unique panel dataset on local\nanti-discrimination laws. Using variation in law implementation across\nlocalities over time and between same-sex and different-sex couples, we find\nthat anti-discrimination laws significantly reduce gaps in labor force\nparticipation rate, employment, and the wage gap for gay men relative to\nstraight men. These laws also significantly reduce the labor force\nparticipation rate, employment, and wage premium for lesbian women relative to\nstraight women. One explanation for the reduced labor supply and wage premium\nis that lesbian couples begin to have more children in response to the laws.\nFinally, we present evidence that state anti-discrimination laws significantly\nand persistently increased support for same-sex marriage. This research shows\nthat anti-discrimination laws can be an effective policy tool for reducing\nlabor market inequalities across sexual orientation and improving sentiment\ntoward LGB Americans.\n"
    },
    {
        "paper_id": 2404.03968,
        "authors": "Bartosz Uniejewski",
        "title": "Regularization for electricity price forecasting",
        "comments": "Forthcoming in the Operations Research and Decisions",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The most commonly used form of regularization typically involves defining the\npenalty function as a L1 or L2 norm. However, numerous alternative approaches\nremain untested in practical applications. In this study, we apply ten\ndifferent penalty functions to predict electricity prices and evaluate their\nperformance under two different model structures and in two distinct\nelectricity markets. The study reveals that LQ and elastic net consistently\nproduce more accurate forecasts compared to other regularization types. In\nparticular, they were the only types of penalty functions that consistently\nproduced more accurate forecasts than the most commonly used LASSO.\nFurthermore, the results suggest that cross-validation outperforms Bayesian\ninformation criteria for parameter optimization, and performs as well as models\nwith ex-post parameter selection.\n"
    },
    {
        "paper_id": 2404.03976,
        "authors": "Abe Alexander and Lars Fritz",
        "title": "A theoretical framework for dynamical fee choice in AMMs",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  In the ever evolving landscape of decentralized finance automated market\nmakers (AMMs) play a key role: they provide a market place for trading assets\nin a decentralized manner. For so-called bluechip pairs, arbitrage activity\nprovides a major part of the revenue generation of AMMs but also a major source\nof loss due to the so-called informed orderflow. Finding ways to minimize those\nlosses while still keeping uninformed trading activity alive is a major problem\nin the field. In this paper we will investigate the mechanics of said arbitrage\nand try to understand how AMMs can maximize the revenue creation or in other\nwords minimize the losses. To that end, we model the dynamics of arbitrage\nactivity for a concrete implementation of a pool and study its sensitivity to\nthe choice of fee aiming to maximize the value retention. We manage to map the\nensuing dynamics to that of a random walk with a specific reward scheme that\nprovides a convenient starting point for further studies.\n"
    },
    {
        "paper_id": 2404.03989,
        "authors": "Ali Dogdu and Murad Kayacan",
        "title": "Instruments And Effects Of Monetary And Fiscal Policy: The Relationship\n  Between Inflation, Vat, And Deposit Interest Rate",
        "comments": "in Turkish language",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this study, we aimed to examine the effect of VAT revenues and Deposit\nInterest Rates on Inflation in Turkey between 1985-2022. Within the framework\nof econometric analysis of the obtained data, the analysis was carried out\nusing ADF unit root test, Johansen Co-Integration Test, Error Terms and VECM\n(Vector Error Correction Model) models. According to the analysis results, it\nwas understood that the data were stationary at the I(I) level, it was\ndetermined that there was a cointegrated relationship between them in the long\nterm, and by estimating the error term, causality findings were determined\nwithin the framework of VECM analysis. According to the causality results of\nthe Wald Test; causality is found from Deposit Interest Rate to VAT and\nInflation, and from Inflation to VAT and Deposit Interest Rate (bidirectional),\nwhile causality is also found from VAT to Inflation and Deposit Interest Rates.\n"
    },
    {
        "paper_id": 2404.04077,
        "authors": "Alessandro Annarelli, Tiziana Catarci, Laura Palagi",
        "title": "The forgotten pillar of sustainability: development of the S-assessment\n  tool to evaluate Organizational Social Sustainability",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Pursuing sustainable development has become a global imperative, underscored\nadopting of the 2030 Agenda for Sustainable Development and its 17 Sustainable\nDevelopment Goals (SDG). At the heart of this agenda lies the recognition of\nsocial sustainability as a pivotal component, emphasizing the need for\ninclusive societies where every individual can thrive. Despite its\nsignificance, social sustainability remains a \"forgotten pillar,\" often\novershadowed by environmental concerns. In response, this paper presents the\ndevelopment and validation of the S-Assessment Tool for Social Sustainability,\na comprehensive questionnaire designed to evaluate organizations' performance\nacross critical dimensions such as health and wellness, gender equality, decent\nwork, and economic growth, reducing inequalities, and responsible production\nand consumption. The questionnaire was constructed on the critical dimensions\nidentified through a systematic and narrative hybrid approach to the analysis\nof peer-reviewed literature. The framework has been structured around the\nvalues of the SDGs. It aims to empower organizations to better understand and\naddress their social impact, fostering positive change and contributing to the\ncollective effort towards a more equitable and sustainable future. Through\ncollaborative partnerships and rigorous methodology, this research underscores\nthe importance of integrating social sustainability into organizational\npractices and decision-making processes, ultimately advancing the broader\nagenda of sustainable development.\n"
    },
    {
        "paper_id": 2404.04097,
        "authors": "David Winkelmann, Charlotte K\\\"ohler",
        "title": "Subscription-Based Inventory Planning for E-Grocery Retailing",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The growing e-grocery sector faces challenges in becoming profitable due to\nheightened customer expectations and logistical complexities. This paper\naddresses the impact of uncertainty in customer demand on inventory planning\nfor online grocery retailers. Given the perishable nature of grocery products\nand intense market competition, retailers must ensure product availability\nwhile minimising overstocking costs. We propose introducing subscription offers\nas a solution to mitigate these inventory challenges. Unlike existing\nliterature focusing on uniform subscription models that may harm profitability,\nour approach considers the synergy between implementing product subscriptions\nand cost savings from improved inventory planning. We present a three-step\nprocedure enabling retailers to understand uncertainty costs, quantify the\nvalue of gathering additional planning information, and implement\nprofitability-enhancing subscription offers. This holistic approach ensures the\ndevelopment of sustainable subscription models in the e-grocery domain.\n"
    },
    {
        "paper_id": 2404.04105,
        "authors": "Michael Pedersen",
        "title": "Judgment in macroeconomic output growth predictions: Efficiency,\n  accuracy and persistence",
        "comments": "25 pages, 3 figures, 8 tables",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The present study applies observations of individual predictions of the first\nthree releases of the US output growth rate to evaluate how the applied\njudgment affects prediction efficiency and accuracy as well as if judgment is\npersistent. While the first two issues have been assessed in other studies,\nthere is little evidence on the formation of judgment in macroeconomic\nprojections. Most of the forecasters produce unbiased predictions, but\nemploying the median Bloomberg projection as baseline, it turns out that\njudgment generally does not improve accuracy. There seems to be persistence in\nthe judgment applied by forecasters in the sense that the sign of the\nadjustment in the first release prediction carries over to the projections of\nthe two following revisions. One possible explanation is that forecasters use\nsome kind of anchor-and-adjustment heuristic.\n"
    },
    {
        "paper_id": 2404.04276,
        "authors": "Eldar Knar",
        "title": "Recursive index for assessing value added of individual scientific\n  publications",
        "comments": "38 pages, 10 firures, 4 table",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  An aggregated recursive K-index is proposed as a new scientometric indicator\nof added value and scientific research output of individual publications. This\nindex can be used instead of or in addition to the H-index (J.E. Hirsch. An\nindex to quantify an individual's scientific research output,\narXiv:physics/0508025). In particular, it is proposed to switch from a pure\nstrategy for assessing the quality and effectiveness of R&D using the H-index\n(Hirsch index) to a mixed strategy (in the context of publication activity as a\ncombination of cooperative and noncooperative games) using the K-index on\nsubnational and H-index on international or differentiated levels. In the\ncontext of a hybrid strategy of the scientist's payoff functions. This\ntransition is correct and in demand for a number of national scientific systems\nwith limited financial, material, infrastructural and linguistic (in terms of\nthe English language) potential. Scientific systems with highly developed\nindigenous (autochthonous) characteristics are also needed in some scientific\nareas.\n"
    },
    {
        "paper_id": 2404.04282,
        "authors": "Diego Vallarino",
        "title": "Analyzing Economic Convergence Across the Americas: A Survival Analysis\n  Approach to GDP per Capita Trajectories",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  By integrating survival analysis, machine learning algorithms, and economic\ninterpretation, this research examines the temporal dynamics associated with\nattaining a 5 percent rise in purchasing power parity-adjusted GDP per capita\nover a period of 120 months (2013-2022). A comparative investigation reveals\nthat DeepSurv is proficient at capturing non-linear interactions, although\nstandard models exhibit comparable performance under certain circumstances. The\nweight matrix evaluates the economic ramifications of vulnerabilities, risks,\nand capacities. In order to meet the GDPpc objective, the findings emphasize\nthe need of a balanced approach to risk-taking, strategic vulnerability\nreduction, and investment in governmental capacities and social cohesiveness.\nPolicy guidelines promote individualized approaches that take into account the\ncomplex dynamics at play while making decisions.\n"
    },
    {
        "paper_id": 2404.04335,
        "authors": "Boyao Wu, Difang Huang, Muzi Chen",
        "title": "Estimating Contagion Mechanism in Global Equity Market with Time-Zone\n  Effect",
        "comments": null,
        "journal-ref": "Financial Management, 2023, 52: 543-572",
        "doi": "10.1111/fima.12430",
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  This paper proposes a time-zone vector autoregression (VAR) model to\ninvestigate comovements in the global financial market. Analyzing daily data\nfrom 36 national equity markets, we explore the subprime and European debt\ncrises using static analysis and the COVID-19 crisis through a rolling window\nmethod. Our study of comovements using VAR coefficients reveals a resonance\neffect in the global system. Findings on densities and assortativities suggest\nthe existence of the transmission mechanism in all periods and abnormal\nstructural changes during the crises. Strength analysis uncovers the\ninformation transmission mechanism across continents over normal and turmoil\nperiods and emphasizes specific stock markets' unique roles. We examine dynamic\ncontinent strengths to demonstrate the contagion mechanism in the global equity\nmarket over an extended period. Incorporating the time-zone effect\nsignificantly enhances the VAR model's interpretability. Signed networks\nprovide more information on global equity markets and better identifies\ncritical contagion patterns than unsigned networks.\n"
    },
    {
        "paper_id": 2404.04506,
        "authors": "Simonetta Vezzoso",
        "title": "Super Apps and the Digital Markets Act",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  The Digital Markets Act (DMA) aims to ensure contestability and fairness in\ndigital markets, particularly focusing on regulating Big Tech companies. The\npaper explores the DMA's capacity to address both current and future challenges\nin digital market contestability and fairness, spotlighting the trend towards\nplatform integration and the potential rise of \"super-apps\" akin to WeChat and\nKakaoTalk. Specifically, it investigates WhatsApp, owned by Meta, as a\ngatekeeper that might expand its service offerings, integrating additional\nfunctionalities like AI and metaverse technologies. The paper discusses whether\nthe DMA's obligations, such as mandated interoperability and data portability,\ncan mitigate the emergent risks to market fairness and contestability from such\nintegrations. Despite recognizing that the DMA has the potential to address\nmany issues arising from platform integration, it suggests the necessity for\nadaptability and a complementary relationship with traditional antitrust law to\nensure sustained contestability and fairness in evolving digital markets.\n"
    },
    {
        "paper_id": 2404.04543,
        "authors": "Jason P Davis and Jian Bai Li",
        "title": "Early Adoption of Generative AI by Global Business Leaders: Insights\n  from an INSEAD Alumni Survey",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  How are new technologies like generative AI quickly adopted and used by\nexecutive and managerial leaders to create value in organizations? A survey of\nINSEAD's global alumni base revealed several intriguing insights into\nperceptions and engagements with generative AI across a broad spectrum of\ndemographics, industries, and geographies. Notably, there's a prevailing\noptimism about the role of generative AI in enhancing productivity and\ninnovation, as evidenced by the 90% of respondents being excited about its\ntime-saving and efficiency benefits. Analysis revealed different attitudes\nabout adoption and use across demographic variables. Younger respondents are\nsignificantly more excited about generative AI and more likely to be using it\nat work and in personal life than older participants. Those in Europe have a\nsomewhat more distant view of generative AI than those in North America in\nAsia, in that they see the gains more likely to be captured by organizations\nthan individuals, and are less likely to be using it in professional and\npersonal contexts than those in North America and Asia. This may also be\nrelated to the fact that those in Europe are more likely to be working in\nFinancial Services and less likely to be working in Information Technology\nindustries than those in North America and Asia. Despite this, those in Europe\nare more likely to see AGI happening faster than those in North America,\nalthough this may reflect less interaction with generative AI in personal and\nprofessional contexts. These findings collectively underscore the complex and\nmultifaceted perceptions of generative AI's role in society, pointing to both\nits promising potential and the challenges it presents.\n"
    },
    {
        "paper_id": 2404.04707,
        "authors": "Carolina Biliotti, Massimo Riccaboni, Luca Verginer",
        "title": "Gender Bias in Emerging New Research Topics: The Impact of COVID-19 on\n  Women in Science",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We investigate the impact of new research opportunities on the long-standing\nunder-representation of women in medical and academic leadership by assessing\nthe impact of the emergence of COVID-19 as a new research topic in the life\nsciences on women's authorship. After collecting publication data from 2019 and\n2020 on biomedical publications, where the position of first and last author is\nmost important for future career development, we use the major Medical Subject\nHeading (MeSH) terms to identify the main research area of each publication and\nmeasure the relation of each paper to COVID-19. Using a\nDifference-in-Difference approach, we find that although the general female\nauthorship trend is upwards, papers in areas related to COVID-19 are less\nlikely to have a woman as first or last author compared to research areas not\nrelated to COVID-19. Conversely, new publication opportunities in the COVID-19\nresearch field increase the proportion of women in middle, less-relevant,\nauthor positions. Stay-at-home mandates, journal importance, and access to new\nfunds do not fully explain the drop in women's outcomes. The decline in female\nfirst authorship is related to the increase of teams in which both lead authors\nhave no prior experience in the COVID-related research field. In addition,\npre-existing publishing teams show reduced bias in female key authorship with\nrespect to new teams specifically formed for COVID-related research. This\nsuggests that opportunistic teams, transitioning into research areas with\nemerging interests, possess greater flexibility in choosing the primary and\nfinal authors, potentially reducing uncertainties associated with engaging in\nproductions divergent from their past scientific experiences by excluding women\nscientists from key authorship positions.\n"
    },
    {
        "paper_id": 2404.04709,
        "authors": "Daniel Freund, S\\'ebastien Martin, Jiayu Kamessi Zhao",
        "title": "Two-Sided Flexibility in Platforms",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Flexibility is a cornerstone of operations management, crucial to hedge\nstochasticity in product demands, service requirements, and resource\nallocation. In two-sided platforms, flexibility is also two-sided and can be\nviewed as the compatibility of agents on one side with agents on the other\nside. Platform actions often influence the flexibility on either the demand or\nthe supply side. But how should flexibility be jointly allocated across\ndifferent sides? Whereas the literature has traditionally focused on only one\nside at a time, our work initiates the study of two-sided flexibility in\nmatching platforms. We propose a parsimonious matching model in random graphs\nand identify the flexibility allocation that optimizes the expected size of a\nmaximum matching. Our findings reveal that flexibility allocation is a\nfirst-order issue: for a given flexibility budget, the resulting matching size\ncan vary greatly depending on how the budget is allocated. Moreover, even in\nthe simple and symmetric settings we study, the quest for the optimal\nallocation is complicated. In particular, easy and costly mistakes can be made\nif the flexibility decisions on the demand and supply side are optimized\nindependently (e.g., by two different teams in the company), rather than\njointly. To guide the search for optimal flexibility allocation, we uncover two\neffects, flexibility cannibalization, and flexibility abundance, that govern\nwhen the optimal design places the flexibility budget only on one side or\nequally on both sides. In doing so we identify the study of two-sided\nflexibility as a significant aspect of platform efficiency.\n"
    },
    {
        "paper_id": 2404.0471,
        "authors": "Pawanesh Yadav, Charu Sharma, Niteesh Sahni",
        "title": "Explaining Indian Stock Market through Geometry of Scale free Networks",
        "comments": "44 pages, 11 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper presents an analysis of the Indian stock market using a method\nbased on embedding the network in a hyperbolic space using Machine learning\ntechniques. We claim novelty on four counts. First, it is demonstrated that the\nhyperbolic clusters resemble the topological network communities more closely\nthan the Euclidean clusters. Second, we are able to clearly distinguish between\nperiods of market stability and volatility through a statistical analysis of\nhyperbolic distance and hyperbolic shortest path distance corresponding to the\nembedded network. Third, we demonstrate that using the modularity of the\nembedded network significant market changes can be spotted early. Lastly, the\ncoalescent embedding is able to segregate the certain market sectors thereby\nunderscoring its natural clustering ability.\n"
    },
    {
        "paper_id": 2404.04962,
        "authors": "Alessio Brini, Jimmie Lenz",
        "title": "A Comparison of Cryptocurrency Volatility-benchmarking New and Mature\n  Asset Classes",
        "comments": "63 pages, 12 figures. Accepted on the \"Financial Innovation\" journal\n  (DOI: 10.1186/s40854-024-00646-y). Link is not active yet. Forthcoming in\n  June 2024",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  The paper analyzes the cryptocurrency ecosystem at both the aggregate and\nindividual levels to understand the factors that impact future volatility. The\nstudy uses high-frequency panel data from 2020 to 2022 to examine the\nrelationship between several market volatility drivers, such as daily leverage,\nsigned volatility and jumps. Several known autoregressive model specifications\nare estimated over different market regimes, and results are compared to equity\ndata as a reference benchmark of a more mature asset class. The panel\nestimations show that the positive market returns at the high-frequency level\nincrease price volatility, contrary to what is expected from the classical\nfinancial literature. We attributed this effect to the price dynamics over the\nlast year of the dataset (2022) by repeating the estimation on different time\nspans. Moreover, the positive signed volatility and negative daily leverage\npositively impact the cryptocurrencies' future volatility, unlike what emerges\nfrom the same study on a cross-section of stocks. This result signals a\nstructural difference in a nascent cryptocurrency market that has to mature\nyet. Further individual-level analysis confirms the findings of the panel\nanalysis and highlights that these effects are statistically significant and\ncommonly shared among many components in the selected universe.\n"
    },
    {
        "paper_id": 2404.04989,
        "authors": "Jinchi Dong and Richard S. J. Tol and Fangzhi Wang",
        "title": "Towards a representative social cost of carbon",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The majority of estimates of the social cost of carbon use preference\nparameters calibrated to data for North America and Europe. We here use\nrepresentative data for attitudes to time and risk across the world. The social\ncost of carbon is substantially higher in the global north than in the south.\nThe difference is more pronounced if we count people rather than countries.\n"
    },
    {
        "paper_id": 2404.05101,
        "authors": "Dat Mai",
        "title": "StockGPT: A GenAI Model for Stock Prediction and Trading",
        "comments": "19 pages, 3 figures, 7 tables",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  This paper introduces StockGPT, an autoregressive ``number'' model trained\nand tested on 70 million daily U.S. stock returns over nearly 100 years.\nTreating each return series as a sequence of tokens, StockGPT automatically\nlearns the hidden patterns predictive of future returns via its attention\nmechanism. On a held-out test sample from 2001 to 2023, a daily rebalanced\nlong-short portfolio formed from StockGPT predictions earns an annual return of\n119% with a Sharpe ratio of 6.5. The StockGPT-based portfolio completely spans\nmomentum and long-/short-term reversals, eliminating the need for manually\ncrafted price-based strategies, and also encompasses most leading stock market\nfactors. This highlights the immense promise of generative AI in surpassing\nhuman in making complex financial investment decisions.\n"
    },
    {
        "paper_id": 2404.05214,
        "authors": "Nizar Riane and Claire David",
        "title": "Generalized measure Black-Scholes equation: Towards option self-similar\n  pricing",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  In this work, we give a generalized formulation of the Black-Scholes model.\nThe novelty resides in considering the Black-Scholes model to be valid on\n'average', but such that the pointwise option price dynamics depends on a\nmeasure representing the investors' 'uncertainty'. We make use of the theory of\nnon-symmetric Dirichlet forms and the abstract theory of partial differential\nequations to establish well posedness of the problem. A detailed numerical\nanalysis is given in the case of self-similar measures.\n"
    },
    {
        "paper_id": 2404.0523,
        "authors": "Ariel Neufeld, Julian Sester",
        "title": "Non-concave distributionally robust stochastic control in a discrete\n  time finite horizon setting",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this article we present a general framework for non-concave\ndistributionally robust stochastic control problems in a discrete time finite\nhorizon setting. Our framework allows to consider a variety of different\npath-dependent ambiguity sets of probability measures comprising, as a natural\nexample, the ambiguity set defined via Wasserstein-balls around path-dependent\nreference measures, as well as parametric classes of probability distributions.\nWe establish a dynamic programming principle which allows to derive both\noptimal control and worst-case measure by solving recursively a sequence of\none-step optimization problems. As a concrete application, we study the robust\nhedging problem of a financial derivative under an asymmetric (and non-convex)\nloss function accounting for different preferences of sell- and buy side when\nit comes to the hedging of financial derivatives. As our entirely data-driven\nambiguity set of probability measures, we consider Wasserstein-balls around the\nempirical measure derived from real financial data. We demonstrate that during\nadverse scenarios such as a financial crisis, our robust approach outperforms\ntypical model-based hedging strategies such as the classical Delta-hedging\nstrategy as well as the hedging strategy obtained in the non-robust setting\nwith respect to the empirical measure and therefore overcomes the problem of\nmodel misspecification in such critical periods.\n"
    },
    {
        "paper_id": 2404.05372,
        "authors": "Andrea Pinto and Antonio Scala",
        "title": "The PEAL Method: a mathematical framework to streamline securitization\n  structuring",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  Securitization is a financial process where the cash flows of\nincome-generating assets are sold to institutional investors as securities,\nliquidating illiquid assets. This practice presents persistent challenges due\nto the absence of a comprehensive mathematical framework for structuring\nasset-backed securities. While existing literature provides technical analysis\nof credit risk modeling, there remains a need for a definitive framework\ndetailing the allocation of the inbound cash flows to the outbound positions.\nTo fill this gap, we introduce the PEAL Method: a 10-step mathematical\nframework to streamline the securitization structuring across all time periods.\n  The PEAL Method offers a rigorous and versatile approach, allowing\npractitioners to structure various types of securitizations, including those\nwith complex vertical positions. By employing standardized equations, it\nfacilitates the delineation of payment priorities and enhances risk\ncharacterization for both the asset and the liability sides throughout the\nsecuritization life cycle.\n  In addition to its technical contributions, the PEAL Method aims to elevate\nindustry standards by addressing longstanding challenges in securitization. By\nproviding detailed information to investors and enabling transparent risk\nprofile comparisons, it promotes market transparency and enables stronger\nregulatory oversight.\n  In summary, the PEAL Method represents a significant advancement in\nsecuritization literature, offering a standardized framework for precision and\nefficiency in structuring transactions. Its adoption has the potential to drive\ninnovation and enhance risk management practices in the securitization market.\n"
    },
    {
        "paper_id": 2404.05803,
        "authors": "Robin Fritsch and Andrea Canidio",
        "title": "Measuring Arbitrage Losses and Profitability of AMM Liquidity",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper presents the results of a comprehensive empirical study of losses\nto arbitrageurs (following the formalization of loss-versus-rebalancing by\n[Milionis et al., 2022]) incurred by liquidity providers on automated market\nmakers (AMMs). We show that those losses exceed the fees earned by liquidity\nproviders across many of the largest AMM liquidity pools (on Uniswap).\nRemarkably, we also find that the Uniswap v2 pools are more profitable for\npassive LPs than their Uniswap v3 counterparts. We also investigate how\narbitrage losses change with block times. As expected, arbitrage losses\ndecrease when block production is faster. However, the rate of the decline\nvaries significantly across different trading pairs. For instance, when\ncomparing 100ms block times to Ethereum's current 12-second block times, the\ndecrease in losses to arbitrageurs ranges between 20% to 70%, depending on the\nspecific trading pair.\n"
    },
    {
        "paper_id": 2404.06472,
        "authors": "Pelin Ozgul, Marie-Christine Fregin, Michael Stops, Simon Janssen,\n  Mark Levels",
        "title": "High-skilled Human Workers in Non-Routine Jobs are Susceptible to AI\n  Automation but Wage Benefits Differ between Occupations",
        "comments": "main text 18 pages, 4 figures, 4 tables, robustness checks included",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Artificial Intelligence (AI) will change human work by taking over specific\njob tasks, but there is a debate which tasks are susceptible to automation, and\nwhether AI will augment or replace workers and affect wages. By combining data\non job tasks with a measure of AI susceptibility, we show that more highly\nskilled workers are more susceptible to AI automation, and that analytical\nnon-routine tasks are at risk to be impacted by AI. Moreover, we observe that\nwage growth premiums for the lowest and the highest required skill level appear\nunrelated to AI susceptibility and that workers in occupations with many\nroutine tasks saw higher wage growth if their work was more strongly\nsusceptible to AI. Our findings imply that AI has the potential to affect human\nworkers differently than canonical economic theories about the impact of\ntechnology on work these theories predict.\n"
    },
    {
        "paper_id": 2404.06489,
        "authors": "Teodora Dobos, Martin Bichler, Johannes Kn\\\"orr",
        "title": "Finding Stable Price Zones in European Electricity Markets: Aiming to\n  Square the Circle?",
        "comments": "36 pages, 12 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  The European day-ahead electricity market is split into multiple bidding\nzones with a uniform price. The increase in renewables leads to a growing\nnumber of interventions in the generation of energy sources and increasing\nredispatch costs. To ensure efficient congestion management, the EU Commission\nmandated a Bidding Zone Review (BZR) to reevaluate the configuration of\nEuropean bidding zones. An integral part of this process was a locational\nmarginal pricing study. Based on this study, alternative bidding zone\nconfigurations were proposed. These bidding zones shall be stable and robust\nover time. For Germany, four configurations were suggested. We analyzed the\nproposed configurations considering different clustering algorithms and periods\nbased on the publicly released data set. We found that the configurations do\nnot reduce the price standard deviations within zones much, and the average\nprices across zones are similar. Other configurations identified based on\nclustering the prices lead to lower price variance but they are not\ngeographically coherent. Independent of the clustering features and algorithms\nused, the resulting clusters are not stable over time. While the costs of a\nbidding zone split in Germany are high, the effect on prices would be low based\non an analysis of the new BZR data.\n"
    },
    {
        "paper_id": 2404.07132,
        "authors": "Jason R. Bailey, W. Brent Lindquist, and Svetlozar T. Rachev",
        "title": "Hedonic Models Incorporating ESG Factors for Time Series of Average\n  Annual Home Prices",
        "comments": "17 pages, 7 figures, 11 tables",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Using data from 2000 through 2022, we analyze the predictive capability of\nthe annual numbers of new home constructions and four available environmental,\nsocial, and governance factors on the average annual price of homes sold in\neight major U.S. cities. We contrast the predictive capability of a P-spline\ngeneralized additive model (GAM) against a strictly linear version of the\ncommonly used generalized linear model (GLM). As the data for the annual price\nand predictor variables constitute non-stationary time series, to avoid\nspurious correlations in the analysis we transform each time series\nappropriately to produce stationary series for use in the GAM and GLM models.\nWhile arithmetic returns or first differences are adequate transformations for\nthe predictor variables, for the average price response variable we utilize the\nseries of innovations obtained from AR(q)-ARCH(1) fits. Based on the GAM\nresults, we find that the influence of ESG factors varies markedly by city,\nreflecting geographic diversity. Notably, the presence of air conditioning\nemerges as a strong factor. Despite limitations on the length of available time\nseries, this study represents a pivotal step toward integrating ESG\nconsiderations into predictive real estate models.\n"
    },
    {
        "paper_id": 2404.07179,
        "authors": "Giambattista Albora, Matteo Straccamore and Andrea Zaccaria",
        "title": "Machine learning-based similarity measure to forecast M&A from patent\n  data",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-sa/4.0/",
        "abstract": "  Defining and finalizing Mergers and Acquisitions (M&A) requires complex human\nskills, which makes it very hard to automatically find the best partner or\npredict which firms will make a deal. In this work, we propose the MASS\nalgorithm, a specifically designed measure of similarity between companies and\nwe apply it to patenting activity data to forecast M&A deals. MASS is based on\nan extreme simplification of tree-based machine learning algorithms and\nnaturally incorporates intuitive criteria for deals; as such, it is fully\ninterpretable and explainable. By applying MASS to the Zephyr and Crunchbase\ndatasets, we show that it outperforms LightGCN, a \"black box\" graph\nconvolutional network algorithm. When similar companies have disjoint patenting\nactivities, on the contrary, LightGCN turns out to be the most effective\nalgorithm. This study provides a simple and powerful tool to model and predict\nM&A deals, offering valuable insights to managers and practitioners for\ninformed decision-making.\n"
    },
    {
        "paper_id": 2404.07221,
        "authors": "Spurthi Setty, Harsh Thakkar, Alyssa Lee, Eden Chung, Natan Vidra",
        "title": "Improving Retrieval for RAG based Question Answering Models on Financial\n  Documents",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  The effectiveness of Large Language Models (LLMs) in generating accurate\nresponses relies heavily on the quality of input provided, particularly when\nemploying Retrieval Augmented Generation (RAG) techniques. RAG enhances LLMs by\nsourcing the most relevant text chunk(s) to base queries upon. Despite the\nsignificant advancements in LLMs' response quality in recent years, users may\nstill encounter inaccuracies or irrelevant answers; these issues often stem\nfrom suboptimal text chunk retrieval by RAG rather than the inherent\ncapabilities of LLMs. To augment the efficacy of LLMs, it is crucial to refine\nthe RAG process. This paper explores the existing constraints of RAG pipelines\nand introduces methodologies for enhancing text retrieval. It delves into\nstrategies such as sophisticated chunking techniques, query expansion, the\nincorporation of metadata annotations, the application of re-ranking\nalgorithms, and the fine-tuning of embedding algorithms. Implementing these\napproaches can substantially improve the retrieval quality, thereby elevating\nthe overall performance and reliability of LLMs in processing and responding to\nqueries.\n"
    },
    {
        "paper_id": 2404.07222,
        "authors": "Qi Deng, Zhong-guo Zhou",
        "title": "Liquidity Jump, Liquidity Diffusion, and Treatment on Wash Trading of\n  Crypto Assets",
        "comments": "37 pages all inclusive",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  We propose that the liquidity of an asset includes two components: liquidity\njump and liquidity diffusion. We show that liquidity diffusion has a higher\ncorrelation with crypto wash trading than liquidity jump and demonstrate that\ntreatment on wash trading significantly reduces the level of liquidity\ndiffusion, but only marginally reduces that of liquidity jump. We confirm that\nthe autoregressive models are highly effective in modeling the\nliquidity-adjusted return with and without the treatment on wash trading. We\nargue that treatment on wash trading is unnecessary in modeling established\ncrypto assets that trade in unregulated but mainstream exchanges.\n"
    },
    {
        "paper_id": 2404.07223,
        "authors": "Youngbin Lee, Yejin Kim, Yongjae Lee",
        "title": "Stock Recommendations for Individual Investors: A Temporal Graph Network\n  Approach with Diversification-Enhancing Contrastive Learning",
        "comments": "Presented at the ICAIF 2023 Workshop on Machine Learning for Investor\n  Modelling and Recommender Systems\n  (https://sites.google.com/view/ml-for-investor-recsys)",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  In complex financial markets, recommender systems can play a crucial role in\nempowering individuals to make informed decisions. Existing studies\npredominantly focus on price prediction, but even the most sophisticated models\ncannot accurately predict stock prices. Also, many studies show that most\nindividual investors do not follow established investment theories because they\nhave their own preferences. Hence, the tricky point in stock recommendation is\nthat recommendations should give good investment performance but also should\nnot ignore individual preferences. To develop effective stock recommender\nsystems, it is essential to consider three key aspects: 1) individual\npreferences, 2) portfolio diversification, and 3) temporal aspect of both stock\nfeatures and individual preferences. In response, we develop the portfolio\ntemporal graph network recommender PfoTGNRec, which can handle time-varying\ncollaborative signals and incorporates diversification-enhancing contrastive\nlearning. As a result, our model demonstrated superior performance compared to\nvarious baselines, including cutting-edge dynamic embedding models and existing\nstock recommendation models, in a sense that our model exhibited good\ninvestment performance while maintaining competitive in capturing individual\npreferences. The source code and data are available at\nhttps://anonymous.4open.science/r/IJCAI2024-12F4.\n"
    },
    {
        "paper_id": 2404.07224,
        "authors": "Francisco de Arriba-P\\'erez, Silvia Garc\\'ia-M\\'endez, Jos\\'e A.\n  Regueiro-Janeiro, Francisco J. Gonz\\'alez-Casta\\~no",
        "title": "Detection of financial opportunities in micro-blogging data with a\n  stacked classification system",
        "comments": null,
        "journal-ref": null,
        "doi": "10.1109/ACCESS.2020.3041084",
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Micro-blogging sources such as the Twitter social network provide valuable\nreal-time data for market prediction models. Investors' opinions in this\nnetwork follow the fluctuations of the stock markets and often include educated\nspeculations on market opportunities that may have impact on the actions of\nother investors. In view of this, we propose a novel system to detect positive\npredictions in tweets, a type of financial emotions which we term\n\"opportunities\" that are akin to \"anticipation\" in Plutchik's theory.\nSpecifically, we seek a high detection precision to present a financial\noperator a substantial amount of such tweets while differentiating them from\nthe rest of financial emotions in our system. We achieve it with a three-layer\nstacked Machine Learning classification system with sophisticated features that\nresult from applying Natural Language Processing techniques to extract valuable\nlinguistic information. Experimental results on a dataset that has been\nmanually annotated with financial emotion and ticker occurrence tags\ndemonstrate that our system yields satisfactory and competitive performance in\nfinancial opportunity detection, with precision values up to 83%. This\npromising outcome endorses the usability of our system to support investors'\ndecision making.\n"
    },
    {
        "paper_id": 2404.07225,
        "authors": "Anoop Kumar, Suresh Dodda, Navin Kamuni, Rajeev Kumar Arora",
        "title": "Unveiling the Impact of Macroeconomic Policies: A Double Machine\n  Learning Approach to Analyzing Interest Rate Effects on Financial Markets",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  This study examines the effects of macroeconomic policies on financial\nmarkets using a novel approach that combines Machine Learning (ML) techniques\nand causal inference. It focuses on the effect of interest rate changes made by\nthe US Federal Reserve System (FRS) on the returns of fixed income and equity\nfunds between January 1986 and December 2021. The analysis makes a distinction\nbetween actively and passively managed funds, hypothesizing that the latter are\nless susceptible to changes in interest rates. The study contrasts gradient\nboosting and linear regression models using the Double Machine Learning (DML)\nframework, which supports a variety of statistical learning techniques. Results\nindicate that gradient boosting is a useful tool for predicting fund returns;\nfor example, a 1% increase in interest rates causes an actively managed fund's\nreturn to decrease by -11.97%. This understanding of the relationship between\ninterest rates and fund performance provides opportunities for additional\nresearch and insightful, data-driven advice for fund managers and investors\n"
    },
    {
        "paper_id": 2404.07298,
        "authors": "Dayu Yang",
        "title": "Predicting Mergers and Acquisitions: Temporal Dynamic Industry Networks",
        "comments": "Data Processing Code:\n  https://github.com/dayuyang1999/Merger_Acquisition_Data Modeling Code:\n  https://github.com/dayuyang1999/Merger_Acquisition_Prediction",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
        "abstract": "  M&A activities are pivotal for market consolidation, enabling firms to\naugment market power through strategic complementarities. Existing research\noften overlooks the peer effect, the mutual influence of M&A behaviors among\nfirms, and fails to capture complex interdependencies within industry networks.\nCommon approaches suffer from reliance on ad-hoc feature engineering, data\ntruncation leading to significant information loss, reduced predictive\naccuracy, and challenges in real-world application. Additionally, the rarity of\nM&A events necessitates data rebalancing in conventional models, introducing\nbias and undermining prediction reliability. We propose an innovative M&A\npredictive model utilizing the Temporal Dynamic Industry Network (TDIN),\nleveraging temporal point processes and deep learning to adeptly capture\nindustry-wide M&A dynamics. This model facilitates accurate, detailed\ndeal-level predictions without arbitrary data manipulation or rebalancing,\ndemonstrated through superior evaluation results from M&A cases between January\n1997 and December 2020. Our approach marks a significant improvement over\ntraditional models by providing detailed insights into M&A activities and\nstrategic recommendations for specific firms.\n"
    },
    {
        "paper_id": 2404.07331,
        "authors": "Victor Cardenas",
        "title": "Financial climate risk: a review of recent advances and key challenges",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  The document provides an overview of financial climate risks. It delves into\nhow climate change impacts the global financial system, distinguishing between\nphysical risks (such as extreme weather events) and transition risks (stemming\nfrom policy changes and economic transitions towards low carbon technologies).\nThe paper underlines the complexity of accurately defining financial climate\nrisk, citing the integration of climate science with financial risk analysis as\na significant challenge. The paper highlights the pivotal role of microfinance\ninstitutions (MFIs) in addressing financial climate risk, especially for\npopulations vulnerable to climate change. The document emphasizes the\nimportance of updating risk management practices within MFIs to explicitly\ninclude climate risk assessments and suggests leveraging technology to improve\nthese practices.\n"
    },
    {
        "paper_id": 2404.07396,
        "authors": "Van Pham and Scott Cunningham",
        "title": "Can Base ChatGPT be Used for Forecasting without Additional\n  Optimization?",
        "comments": "77 pages, added falsification exercises in section `Post\n  Scriptum:...' with new figures; new title 61 pages, 26 figures; corrected\n  typos",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  This study investigates whether OpenAI's ChatGPT-3.5 and ChatGPT-4 can\nforecast future events. To evaluate the accuracy of the predictions, we take\nadvantage of the fact that the training data at the time of our experiments\n(mid 2023) stopped at September 2021, and ask about events that happened in\n2022. We employed two prompting strategies: direct prediction and what we call\nfuture narratives which ask ChatGPT to tell fictional stories set in the future\nwith characters retelling events that happened in the past, but after ChatGPT's\ntraining data had been collected. We prompted ChatGPT to engage in\nstorytelling, particularly within economic contexts. After analyzing 100\ntrials, we find that future narrative prompts significantly enhanced\nChatGPT-4's forecasting accuracy. This was especially evident in its\npredictions of major Academy Award winners as well as economic trends, the\nlatter inferred from scenarios where the model impersonated public figures like\nthe Federal Reserve Chair, Jerome Powell. As a falsification exercise, we\nrepeated our experiments in May 2024 at which time the models included more\nrecent training data. ChatGPT-4's accuracy significantly improved when the\ntraining window included the events being prompted for, achieving 100% accuracy\nin many instances. The poorer accuracy for events outside of the training\nwindow suggests that in the 2023 prediction experiments, ChatGPT-4 was forming\npredictions based solely on its training data. Narrative prompting also\nconsistently outperformed direct prompting. These findings indicate that\nnarrative prompts leverage the models' capacity for hallucinatory narrative\nconstruction, facilitating more effective data synthesis and extrapolation than\nstraightforward predictions. Our research reveals new aspects of LLMs'\npredictive capabilities and suggests potential future applications in\nanalytical contexts.\n"
    },
    {
        "paper_id": 2404.07452,
        "authors": "Yupeng Cao, Zhi Chen, Qingyun Pei, Fabrizio Dimino, Lorenzo Ausiello,\n  Prashant Kumar, K.P. Subbalakshmi, Papa Momar Ndiaye",
        "title": "RiskLabs: Predicting Financial Risk Using Large Language Model Based on\n  Multi-Sources Data",
        "comments": "24 pages, 7 figures, 5 tables, 1 algorithm",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The integration of Artificial Intelligence (AI) techniques, particularly\nlarge language models (LLMs), in finance has garnered increasing academic\nattention. Despite progress, existing studies predominantly focus on tasks like\nfinancial text summarization, question-answering (Q$\\&$A), and stock movement\nprediction (binary classification), with a notable gap in the application of\nLLMs for financial risk prediction. Addressing this gap, in this paper, we\nintroduce \\textbf{RiskLabs}, a novel framework that leverages LLMs to analyze\nand predict financial risks. RiskLabs uniquely combines different types of\nfinancial data, including textual and vocal information from Earnings\nConference Calls (ECCs), market-related time series data, and contextual news\ndata surrounding ECC release dates. Our approach involves a multi-stage\nprocess: initially extracting and analyzing ECC data using LLMs, followed by\ngathering and processing time-series data before the ECC dates to model and\nunderstand risk over different timeframes. Using multimodal fusion techniques,\nRiskLabs amalgamates these varied data features for comprehensive multi-task\nfinancial risk prediction. Empirical experiment results demonstrate RiskLab's\neffectiveness in forecasting both volatility and variance in financial markets.\nThrough comparative experiments, we demonstrate how different data sources\ncontribute to financial risk assessment and discuss the critical role of LLMs\nin this context. Our findings not only contribute to the AI in finance\napplication but also open new avenues for applying LLMs in financial risk\nassessment.\n"
    },
    {
        "paper_id": 2404.07574,
        "authors": "Reza Hafezi, David A. Wood, Firouzeh Rosa Taghikhah",
        "title": "International environmental treaties: An honest or a misguided effort",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/publicdomain/zero/1.0/",
        "abstract": "  Climate change and environmental concerns represent a global crisis\naccompanied by significant economic challenges. Regular international\nconferences held to address these issues, such as in the UK (2021) and Egypt\n(2022), spark debate about the effectiveness and practicality of international\ncommitments. This study examines international treaties from a different\nperspective, emphasizing the need to understand the power dynamics and\nstakeholder interests that delay logical actions to mitigate anthropogenic\ncontributions to climate change and their impacts. Environmental and social\nconcerns tend to increase within nations as their economies develop, where they\nfight to keep acceptable standards of living while reducing emissions volume.\nSo, nations play disproportionate roles in global decision-making based on the\nsize of their economies. Addressing climate change requires a paradigm shift to\nemphasize acknowledging and adhering to global commitments through civil\npressure, rather than relying on traditional yet biased systems of\ninternational political diplomacy. Here, climate-friendly actions are evaluated\nand ideas to promote such activities are proposed. We introduce a \"transition\nregime\" as a solution to this metastasis challenge which gradually infects all\nnations.\n"
    },
    {
        "paper_id": 2404.0763,
        "authors": "Jinzhi Lu, Pingyang Gao",
        "title": "Short, Disclose, and Distort",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  We investigate the voluntary disclosure decision of activist speculators\nunder uncertainty about information endowment (Dye 1985). In our model, a\nspeculator first uncovers initial evidence about the target firm and then seeks\nadditional information to help interpret the initial evidence. The speculator\ntakes a position in the firm's stock, then voluntarily discloses some or all of\ntheir findings, and finally closes their position after the disclosure. We\npresent three main findings. First, the speculator will always disclose the\ninitial evidence, even though the market is uncertain about whether the\nspeculator possesses such evidence. Second, the speculator's disclosure\nstrategy of the additional information increases stock price volatility: they\ndisclose extreme news and withhold moderate news. Third, this distortion in\ndisclosure enables the speculator to engage in market manipulation, whereby\nthey take a short (long) position despite having good (bad) news.\n  Keywords: activist speculators, short and distort, voluntary disclosure,\ncomplex information, market manipulation\n  JEL Classifications: D82, D83, G14, M41\n"
    },
    {
        "paper_id": 2404.07651,
        "authors": "Rozane Bezerra dde Siqueira, Jose Ricardo Bezerra Nogueira, Carlos\n  Feitosa Luna",
        "title": "Impacto Distributivo Potencial de Reformas na Tributacao Indireta no\n  Brasil: Simulacoes Baseadas na PEC 45/2019",
        "comments": "in Portuguese language",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  This paper analyzes the redistributive impacts of indirect taxation reforms\nin Brazil inspired by PEC 45/2019, particularly in the version that led to EC\n132/2023. Comparisons are made between the current system and the simulated\nreforms, considering the distribution of the tax burden among families in\ndifferent income classes, as well as the impact on poverty and inequality\nindicators. The simulations are conducted based on the combination of effective\ntax rates that apply to goods and services consumed by households, using\nnationally representative microdata from family budgets.\n"
    },
    {
        "paper_id": 2404.07658,
        "authors": "Ludovic Gouden\\`ege, Andrea Molent, Xiao Wei, Antonino Zanette",
        "title": "Enhancing Valuation of Variable Annuities in L\\'evy Models with\n  Stochastic Interest Rate",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  This paper extends the valuation and optimal surrender framework for variable\nannuities with guaranteed minimum benefits in a L\\'evy equity market\nenvironment by incorporating a stochastic interest rate described by the\nHull-White model. This approach frames a more dynamic and realistic financial\nsetting compared to previous literature. We exploit a robust valuation\nmechanism employing a hybrid numerical method that merges tree methods for\ninterest rate modeling with finite difference techniques for the underlying\nasset price. This method is particularly effective for addressing the\ncomplexities of variable annuities, where periodic fees and mortality risks are\nsignificant factors. Our findings reveal the influence of stochastic interest\nrates on the strategic decision-making process concerning the surrender of\nthese financial instruments. Through comprehensive numerical experiments, and\nby comparing our results with those obtained through the Longstaff-Schwartz\nMonte Carlo method, we illustrate how our refined model can guide insurers in\ndesigning contracts that equitably balance the interests of both parties. This\nis particularly relevant in discouraging premature surrenders while adapting to\nthe realistic fluctuations of financial markets. Lastly, a comparative statics\nanalysis with varying interest rate parameters underscores the impact of\ninterest rates on the cost of the optimal surrender strategy, emphasizing the\nimportance of accurately modeling stochastic interest rates.\n"
    },
    {
        "paper_id": 2404.07678,
        "authors": "Maria Fay, Frederik F. Fl\\\"other",
        "title": "On the role of ethics and sustainability in business innovation",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-sa/4.0/",
        "abstract": "  For organizations to survive and flourish in the long term, innovation and\nnovelty must be continually introduced, which is particularly true in today's\nrapidly changing world. This raises a variety of ethical and sustainability\nconsiderations that seldom receive the attention they deserve. Existing\ninnovation adoption frameworks often focus on technological, organizational,\nenvironmental, and social factors impacting adoption. In this chapter, we\nexplore the ethical and sustainability angles, particularly as they relate to\nemerging technologies, artificial intelligence (AI) being a prominent example.\nWe consider how to facilitate the development and cultivation of innovation\ncultures in organizations, including budding startups as well as established\nenterprises, through approaches such as systems thinking.\n"
    },
    {
        "paper_id": 2404.0775,
        "authors": "Ingrid Haegele",
        "title": "The Broken Rung: Gender and the Leadership Gap",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Addressing female underrepresentation in leadership positions has become a\nkey policy objective. However, little is known about the extent to which\nleadership appeals differently to women. Collecting new data from a large firm,\nI document that women are substantially less likely to apply for early-career\npromotions. Realized application patterns and large-scale surveys reveal the\nrole of an understudied feature of promotions -- having to assume\nresponsibility over a team -- which is less appealing to women. This gender\ndifference is not accounted for by standard explanations, such as success\nlikelihood or confidence, but is rather a product of common design features of\nleadership positions.\n"
    },
    {
        "paper_id": 2404.07935,
        "authors": "Jos\\'e Moran and Massimo Riccaboni",
        "title": "Compositional Growth Models",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We review models of compositional growth, which were introduced to explain\nthe growth statistics of various quantities ranging from firm sizes to GDP. In\nthese models, entities are decomposed into units that grow independently. Thus,\nthe growth rate of the entity is the addition of the growth rates of the\ncomposing units, with possibly heterogeneous weights. We review such models and\nshow that they can be understood through a unifying theoretical framework,\nexplaining the resulting growth rate distributions using mixtures of Gaussians.\n"
    },
    {
        "paper_id": 2404.07969,
        "authors": "Chufeng Li and Jianyong Chen",
        "title": "An End-to-End Structure with Novel Position Mechanism and Improved EMD\n  for Stock Forecasting",
        "comments": "ICONIP 2023",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  As a branch of time series forecasting, stock movement forecasting is one of\nthe challenging problems for investors and researchers. Since Transformer was\nintroduced to analyze financial data, many researchers have dedicated\nthemselves to forecasting stock movement using Transformer or attention\nmechanisms. However, existing research mostly focuses on individual stock\ninformation but ignores stock market information and high noise in stock data.\nIn this paper, we propose a novel method using the attention mechanism in which\nboth stock market information and individual stock information are considered.\nMeanwhile, we propose a novel EMD-based algorithm for reducing short-term noise\nin stock data. Two randomly selected exchange-traded funds (ETFs) spanning over\nten years from US stock markets are used to demonstrate the superior\nperformance of the proposed attention-based method. The experimental analysis\ndemonstrates that the proposed attention-based method significantly outperforms\nother state-of-the-art baselines. Code is available at\nhttps://github.com/DurandalLee/ACEFormer.\n"
    },
    {
        "paper_id": 2404.08129,
        "authors": "Nicola Borri, Denis Chetverikov, Yukun Liu, and Aleh Tsyvinski",
        "title": "One Factor to Bind the Cross-Section of Returns",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  We propose a new non-linear single-factor asset pricing model\n$r_{it}=h(f_{t}\\lambda_{i})+\\epsilon_{it}$. Despite its parsimony, this model\nrepresents exactly any non-linear model with an arbitrary number of factors and\nloadings -- a consequence of the Kolmogorov-Arnold representation theorem. It\nfeatures only one pricing component $h(f_{t}\\lambda_{I})$, comprising a\nnonparametric link function of the time-dependent factor and factor loading\nthat we jointly estimate with sieve-based estimators. Using 171 assets across\nmajor classes, our model delivers superior cross-sectional performance with a\nlow-dimensional approximation of the link function. Most known finance and\nmacro factors become insignificant controlling for our single-factor.\n"
    },
    {
        "paper_id": 2404.08136,
        "authors": "Eric Luxenberg, Stephen Boyd",
        "title": "Exponentially Weighted Moving Models",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  An exponentially weighted moving model (EWMM) for a vector time series fits a\nnew data model each time period, based on an exponentially fading loss function\non past observed data. The well known and widely used exponentially weighted\nmoving average (EWMA) is a special case that estimates the mean using a square\nloss function. For quadratic loss functions EWMMs can be fit using a simple\nrecursion that updates the parameters of a quadratic function. For other loss\nfunctions, the entire past history must be stored, and the fitting problem\ngrows in size as time increases. We propose a general method for computing an\napproximation of EWMM, which requires storing only a window of a fixed number\nof past samples, and uses an additional quadratic term to approximate the loss\nassociated with the data before the window. This approximate EWMM relies on\nconvex optimization, and solves problems that do not grow with time. We compare\nthe estimates produced by our approximation with the estimates from the exact\nEWMM method.\n"
    },
    {
        "paper_id": 2404.08456,
        "authors": "Lorenc Kapllani and Long Teng",
        "title": "A backward differential deep learning-based algorithm for solving\n  high-dimensional nonlinear backward stochastic differential equations",
        "comments": "40 pages, 5 figures, 5 tables",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this work, we propose a novel backward differential deep learning-based\nalgorithm for solving high-dimensional nonlinear backward stochastic\ndifferential equations (BSDEs), where the deep neural network (DNN) models are\ntrained not only on the inputs and labels but also the differentials of the\ncorresponding labels. This is motivated by the fact that differential deep\nlearning can provide an efficient approximation of the labels and their\nderivatives with respect to inputs. The BSDEs are reformulated as differential\ndeep learning problems by using Malliavin calculus. The Malliavin derivatives\nof solution to a BSDE satisfy themselves another BSDE, resulting thus in a\nsystem of BSDEs. Such formulation requires the estimation of the solution, its\ngradient, and the Hessian matrix, represented by the triple of processes\n$\\left(Y, Z, \\Gamma\\right).$ All the integrals within this system are\ndiscretized by using the Euler-Maruyama method. Subsequently, DNNs are employed\nto approximate the triple of these unknown processes. The DNN parameters are\nbackwardly optimized at each time step by minimizing a differential learning\ntype loss function, which is defined as a weighted sum of the dynamics of the\ndiscretized BSDE system, with the first term providing the dynamics of the\nprocess $Y$ and the other the process $Z$. An error analysis is carried out to\nshow the convergence of the proposed algorithm. Various numerical experiments\nup to $50$ dimensions are provided to demonstrate the high efficiency. Both\ntheoretically and numerically, it is demonstrated that our proposed scheme is\nmore efficient compared to other contemporary deep learning-based\nmethodologies, especially in the computation of the process $\\Gamma$.\n"
    },
    {
        "paper_id": 2404.08475,
        "authors": "Hirbod Assa and Peng Liu",
        "title": "Factor risk measures",
        "comments": "31 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper introduces and studies factor risk measures. While risk measures\nonly rely on the distribution of a loss random variable, in many cases risk\nneeds to be measured relative to some major factors. In this paper, we\nintroduce a double-argument mapping as a risk measure to assess the risk\nrelative to a vector of factors, called factor risk measure. The factor risk\nmeasure only depends on the joint distribution of the risk and the factors. A\nset of natural axioms are discussed, and particularly distortion, quantile,\nlinear and coherent factor risk measures are introduced and characterized.\nMoreover, we introduce a large set of concrete factor risk measures and many of\nthem are new to the literature, which are interpreted in the context of\nregulatory capital requirement. Finally, the distortion factor risk measures\nare applied in the risk-sharing problem and some numerical examples are\npresented to show the difference between the Value-at-Risk and the quantile\nfactor risk measures.\n"
    },
    {
        "paper_id": 2404.08492,
        "authors": "Siting Lu",
        "title": "Strategic Interactions between Large Language Models-based Agents in\n  Beauty Contests",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
        "abstract": "  The growing adoption of large language models (LLMs) presents substantial\npotential for deeper understanding of human behaviours within game theory\nframeworks through simulations. Leveraging on the diverse pool of LLM types and\naddressing the gap in research on competitive games, this paper examines the\nstrategic interactions among multiple types of LLM-based agents in a classical\ngame of beauty contest. Drawing parallels to experiments involving human\nsubjects, LLM-based agents are assessed similarly in terms of strategic levels.\nThey demonstrate varying depth of reasoning that falls within a range of\nlevel-0 and 1, and show convergence in actions in repeated settings.\nFurthermore, I also explore how variations in group composition of agent types\ninfluence strategic behaviours, where I found higher proportion of\nfixed-strategy opponents enhances convergence for LLM-based agents, and having\na mixed environment with agents of differing relative strategic levels\naccelerates convergence for all agents. There could also be higher average\npayoffs for the more intelligent agents, albeit at the expense of the less\nintelligent agents. These results not only provide insights into outcomes for\nsimulated agents under specified scenarios, it also offer valuable implications\nfor understanding strategic interactions between algorithms.\n"
    },
    {
        "paper_id": 2404.0862,
        "authors": "Shihao Wei, Christopher Boudreaux, Zhongfeng Su, Zhan Wu",
        "title": "Natural disasters and social entrepreneurship: An attention-based view",
        "comments": "44 pages, 4 figures",
        "journal-ref": "Small Business Economics 2024",
        "doi": "10.1007/s11187-023-00822-x",
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Drawing on the attention based view, this study explores the joint effects of\nnatural disaster intensity at the country level with personal attributes in\nterms of gender, human capital, and fear of failure on the likelihood to enter\nsocial entrepreneurship. Using data on 107,386 observations across 30\ncountries, we find that natural disaster intensity has a positive effect on\nindividuals likelihood to engage in social entrepreneurship. In addition, the\neffect of natural disaster intensity is greater for males, individuals lacking\nhuman capital, and those who fear failure. Our study helps elaborate on the\nantecedents of social entrepreneurship and extends the consequences of natural\ndisasters to entrepreneurship at the individual level.\n"
    },
    {
        "paper_id": 2404.08665,
        "authors": "Silvia Garc\\'ia-M\\'endez, Francisco de Arriba-P\\'erez, Ana\n  Barros-Vila, Francisco J. Gonz\\'alez-Casta\\~no",
        "title": "Targeted aspect-based emotion analysis to detect opportunities and\n  precaution in financial Twitter messages",
        "comments": null,
        "journal-ref": null,
        "doi": "10.1016/j.eswa.2023.119611",
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  Microblogging platforms, of which Twitter is a representative example, are\nvaluable information sources for market screening and financial models. In\nthem, users voluntarily provide relevant information, including educated\nknowledge on investments, reacting to the state of the stock markets in\nreal-time and, often, influencing this state. We are interested in the user\nforecasts in financial, social media messages expressing opportunities and\nprecautions about assets. We propose a novel Targeted Aspect-Based Emotion\nAnalysis (TABEA) system that can individually discern the financial emotions\n(positive and negative forecasts) on the different stock market assets in the\nsame tweet (instead of making an overall guess about that whole tweet). It is\nbased on Natural Language Processing (NLP) techniques and Machine Learning\nstreaming algorithms. The system comprises a constituency parsing module for\nparsing the tweets and splitting them into simpler declarative clauses; an\noffline data processing module to engineer textual, numerical and categorical\nfeatures and analyse and select them based on their relevance; and a stream\nclassification module to continuously process tweets on-the-fly. Experimental\nresults on a labelled data set endorse our solution. It achieves over 90%\nprecision for the target emotions, financial opportunity, and precaution on\nTwitter. To the best of our knowledge, no prior work in the literature has\naddressed this problem despite its practical interest in decision-making, and\nwe are not aware of any previous NLP nor online Machine Learning approaches to\nTABEA.\n"
    },
    {
        "paper_id": 2404.08712,
        "authors": "Thiago C. Silva, Paulo V. B. Wilhelm and Diego R. Amancio",
        "title": "Machine learning and economic forecasting: the role of international\n  trade networks",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This study examines the effects of de-globalization trends on international\ntrade networks and their role in improving forecasts for economic growth. Using\nsection-level trade data from nearly 200 countries from 2010 to 2022, we\nidentify significant shifts in the network topology driven by rising trade\npolicy uncertainty. Our analysis highlights key global players through\ncentrality rankings, with the United States, China, and Germany maintaining\nconsistent dominance. Using a horse race of supervised regressors, we find that\nnetwork topology descriptors evaluated from section-specific trade networks\nsubstantially enhance the quality of a country's GDP growth forecast. We also\nfind that non-linear models, such as Random Forest, XGBoost, and LightGBM,\noutperform traditional linear models used in the economics literature. Using\nSHAP values to interpret these non-linear model's predictions, we find that\nabout half of most important features originate from the network descriptors,\nunderscoring their vital role in refining forecasts. Moreover, this study\nemphasizes the significance of recent economic performance, population growth,\nand the primary sector's influence in shaping economic growth predictions,\noffering novel insights into the intricacies of economic growth forecasting.\n"
    },
    {
        "paper_id": 2404.08757,
        "authors": "Michail Anthropelos and Scott Robertson",
        "title": "Strategic Informed Trading and the Value of Private Information",
        "comments": "56 pages, 3 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We consider a market of risky financial assets where the participants are an\ninformed trader, a mass of uniformed traders and noisy liquidity providers. We\nprove the existence of a market-clearing equilibrium when the insider\ninternalizes her power to impact prices. In the price-impact equilibrium the\ninsider strategically reveals a noisier (compared to when the insider takes\nprices as given) signal, and prices are less reactive to the publicly available\ninformation. In contrast to the related literature, we show that in the\nprice-impact equilibrium, the insider's ex-ante welfare monotonically increases\nin the signal precision. This clarifies when a trader with market power is\nmotivated to both obtain and refine her private information. Furthermore, even\nthough the uniformed traders act as price-takers, the effect of price impact is\nex-ante welfare improving for them. By contrast, internalization of price\nimpact may reduce insider ex-ante welfare. This happens provided the insider is\nsufficiently risk averse and the uninformed traders are sufficiently risk\ntolerant.\n"
    },
    {
        "paper_id": 2404.08903,
        "authors": "Anna Knezevic",
        "title": "Enhancing path-integral approximation for non-linear diffusion with\n  neural network",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Enhancing the existing solution for pricing of fixed income instruments\nwithin Black-Karasinski model structure, with neural network at various\nparameterisation points to demonstrate that the method is able to achieve\nsuperior outcomes for multiple calibrations across extended projection\nhorizons.\n"
    },
    {
        "paper_id": 2404.08906,
        "authors": "Lennart Ante, Aman Saggu, Benjamin Schellinger, Friedrich Wazinksi",
        "title": "Voting Participation and Engagement in Blockchain-Based Fan Tokens",
        "comments": "45 pages, 8 figures, 4 tables, 1 appendix figure, 1 appendix table",
        "journal-ref": "Electronic Markets 34 (26) 2024",
        "doi": "10.1007/s12525-024-00709-z",
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  This paper investigates the potential of blockchain-based fan tokens, a class\nof crypto asset that grants holders access to voting on club decisions and\nother perks, as a mechanism for stimulating democratized decision-making and\nfan engagement in the sports and esports sectors. By utilizing an extensive\ndataset of 3,576 fan token polls, we reveal that fan tokens engage an average\nof 4,003 participants per poll, representing around 50% of token holders,\nunderscoring their relative effectiveness in boosting fan engagement. The\nanalyses identify significant determinants of fan token poll participation,\nincluding levels of voter (dis-)agreement, poll type, sports sectors,\ndemographics, and club-level factors. This study provides valuable stakeholder\ninsights into the current state of adoption and voting trends for fan token\npolls. It also suggests strategies for increasing fan engagement, thereby\noptimizing the utility of fan tokens in sports. Moreover, we highlight the\nbroader applicability of fan token principles to any community, brand, or\norganization focused on customer engagement, suggesting a wider potential for\nthis digital innovation.\n"
    },
    {
        "paper_id": 2404.08908,
        "authors": "Jiaoying Pei",
        "title": "Reference Model Based Learning in Expectation Formation: Experimental\n  Evidence",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  How do people form expectations about future prices in financial markets? One\nof the dominant learning rules that explains the forecasting behavior is the\nAdaptive Expectation Rule (ADA), which suggests that people adjust their\npredictions by adapting to the most recent prediction error at a constant\nweight. However, this rule also implies that they will continually learn and\nadapt until the prediction error is zero, which contradicts recent experimental\nevidence showing that people usually stop learning long before reaching zero\nprediction error. A more recent learning rule, Reference Model Based Learning\n(RMBL), extends and generalizes ADA, hypothesizing that: i) People apply ADA\nbut dynamically adjust the adaptive coefficient with regards to the\nauto-correlation of the prediction error in the most recent two periods; ii)\nMeanwhile, they also utilize a satisficing rule so that people would only\nadjust their adaptive coefficient when the prediction error is higher than\ntheir anticipation. This paper utilizes a rich set of experimental data with\nobservations of 41,490 predictions from 801 subjects from the\nLearning-to-Forecast Experiments (LtFEs), i.e., the experiment that has been\nused to study expectation formation. Our results concludes that RMBL fits\nbetter than ADA in all the experiments.\n"
    },
    {
        "paper_id": 2404.08935,
        "authors": "Zhenglong Li, Vincent Tam",
        "title": "Developing An Attention-Based Ensemble Learning Framework for Financial\n  Portfolio Optimisation",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
        "abstract": "  In recent years, deep or reinforcement learning approaches have been applied\nto optimise investment portfolios through learning the spatial and temporal\ninformation under the dynamic financial market. Yet in most cases, the existing\napproaches may produce biased trading signals based on the conventional price\ndata due to a lot of market noises, which possibly fails to balance the\ninvestment returns and risks. Accordingly, a multi-agent and self-adaptive\nportfolio optimisation framework integrated with attention mechanisms and time\nseries, namely the MASAAT, is proposed in this work in which multiple trading\nagents are created to observe and analyse the price series and directional\nchange data that recognises the significant changes of asset prices at\ndifferent levels of granularity for enhancing the signal-to-noise ratio of\nprice series. Afterwards, by reconstructing the tokens of financial data in a\nsequence, the attention-based cross-sectional analysis module and temporal\nanalysis module of each agent can effectively capture the correlations between\nassets and the dependencies between time points. Besides, a portfolio generator\nis integrated into the proposed framework to fuse the spatial-temporal\ninformation and then summarise the portfolios suggested by all trading agents\nto produce a newly ensemble portfolio for reducing biased trading actions and\nbalancing the overall returns and risks. The experimental results clearly\ndemonstrate that the MASAAT framework achieves impressive enhancement when\ncompared with many well-known portfolio optimsation approaches on three\nchallenging data sets of DJIA, S&P 500 and CSI 300. More importantly, our\nproposal has potential strengths in many possible applications for future\nstudy.\n"
    },
    {
        "paper_id": 2404.08991,
        "authors": "Evangelos Katsamakas",
        "title": "Business models for the simulation hypothesis",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  The simulation hypothesis suggests that we live in a computer simulation.\nThat notion has attracted significant scholarly and popular interest. This\narticle explores the simulation hypothesis from a business perspective. Due to\nthe lack of a name for a universe consistent with the simulation hypothesis, we\npropose the term simuverse. We argue that if we live in a simulation, there\nmust be a business justification. Therefore, we ask: If we live in a simuverse,\nwhat is its business model? We identify and explore business model scenarios,\nsuch as simuverse as a project, service, or platform. We also explore business\nmodel pathways and risk management issues. The article contributes to the\nsimulation hypothesis literature and is the first to provide a business model\nperspective on the simulation hypothesis. The article discusses theoretical and\npractical implications and identifies opportunities for future research related\nto sustainability, digital transformation, and Artificial Intelligence (AI).\n"
    },
    {
        "paper_id": 2404.0909,
        "authors": "Erhan Bayraktar, Asaf Cohen, April Nellis",
        "title": "DEX Specs: A Mean Field Approach to DeFi Currency Exchanges",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  We investigate the behavior of liquidity providers (LPs) by modeling a\ndecentralized cryptocurrency exchange (DEX) based on Uniswap v3. LPs with\nheterogeneous characteristics choose optimal liquidity positions subject to\nuncertainty regarding the size of exogenous incoming transactions and the\nprices of assets in the wider market. They engage in a game among themselves,\nand the resulting liquidity distribution determines the exchange rate dynamics\nand potential arbitrage opportunities of the pool. We calibrate the\ndistribution of LP characteristics based on Uniswap data and the equilibrium\nstrategy resulting from this mean-field game produces pool exchange rate\ndynamics and liquidity evolution consistent with observed pool behavior. We\nsubsequently introduce Maximal Extractable Value (MEV) bots who perform\nJust-In-Time (JIT) liquidity attacks, and develop a Stackelberg game between\nLPs and bots. This addition results in more accurate simulated pool exchange\nrate dynamics and stronger predictive power regarding the evolution of the pool\nliquidity distribution.\n"
    },
    {
        "paper_id": 2404.0918,
        "authors": "Rodolfo G. Campos and Iliana Reggio and Jacopo Timini",
        "title": "ge_gravity2: a command for solving universal gravity models",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We describe an algorithm for computing counterfactual trade flows, prices,\noutput, and welfare in a large class of general equilibrium trade models. We\nintroduce a command called ge_gravity2 that allows users to perform these\ncomputations in Stata. This command extends the existing ge_gravity command by\nallowing users to compute the general equilibrium effects of changes in trade\npolicy in positive supply elasticity models. It can be used to solve any model\nthat falls into the class of universal gravity models as defined by Allen,\nArkolakis, and Takahashi [Universal Gravity, Journal of Political Economy,\n128(2), 2020, 393-433].\n"
    },
    {
        "paper_id": 2404.09297,
        "authors": "Pedro Gonzalez-Fernandez",
        "title": "Belief Bias Identification",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  This paper proposes a unified theoretical model to identify and test a\ncomprehensive set of probabilistic updating biases within a single framework.\nThe model achieves separate identification by focusing on the updating of\nbelief distributions, rather than classic point-belief measurements. Testing\nthe model in a laboratory experiment reveals significant heterogeneity at the\nindividual level: All tested biases are present, and each participant exhibits\nat least one identifiable bias. Notably, motivated-belief biases (optimism and\npessimism) and sequence-related biases (gambler's fallacy and hot hand fallacy)\nare identified as key drivers of biased inference. Moreover, at the population\nlevel, base rate neglect emerges as a persistent influence. This study\ncontributes to the belief-updating literature by providing a methodological\ntoolkit for researchers examining links between different conflicting biases,\nor exploring connections between updating biases and other behavioural\nphenomena.\n"
    },
    {
        "paper_id": 2404.09462,
        "authors": "Masanori Hirano",
        "title": "Experimental Analysis of Deep Hedging Using Artificial Market\n  Simulations for Underlying Asset Simulators",
        "comments": "9 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Derivative hedging and pricing are important and continuously studied topics\nin financial markets. Recently, deep hedging has been proposed as a promising\napproach that uses deep learning to approximate the optimal hedging strategy\nand can handle incomplete markets. However, deep hedging usually requires\nunderlying asset simulations, and it is challenging to select the best model\nfor such simulations. This study proposes a new approach using artificial\nmarket simulations for underlying asset simulations in deep hedging. Artificial\nmarket simulations can replicate the stylized facts of financial markets, and\nthey seem to be a promising approach for deep hedging. We investigate the\neffectiveness of the proposed approach by comparing its results with those of\nthe traditional approach, which uses mathematical finance models such as\nBrownian motion and Heston models for underlying asset simulations. The results\nshow that the proposed approach can achieve almost the same level of\nperformance as the traditional approach without mathematical finance models.\nFinally, we also reveal that the proposed approach has some limitations in\nterms of performance under certain conditions.\n"
    },
    {
        "paper_id": 2404.09553,
        "authors": "lunwu Liu and Shi Liu",
        "title": "Has Anti-corruption Efforts lowered Enterprises Innovation Efficiency?\n  -An Empirical Analysis from China",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This study adopts the fixed effects panel model and provincial panel data on\nanticorruption and the innovation efficiency of high-level technology and new\ntechnology enterprises in China from 2005 to 2014, to estimate the effects of\nthe anticorruption movement on the innovation efficiency of enterprises at\ndifferent corruption levels. The empirical results show that anticorruption is\npositively correlated with the innovation efficiency of enterprises; however,\nthe correlation is differentiated according to different corruption levels and\nbusiness natures. At a high level of corruption, anticorruption has positive\nimpacts on enterprises' innovation; at a low level of corruption, it negatively\naffects innovation efficiency. However, anticorruption has negative effects on\nthe innovation efficiency of state-owned enterprises at both high and low\ncorruption levels; for nonstate-owned enterprises, its effects are positive at\na high corruption level and negative at a low corruption level. The effects\nremain the same across different regions.\n"
    },
    {
        "paper_id": 2404.09629,
        "authors": "Thitithep Sitthiyot, Kanyarat Holasut",
        "title": "Quantifying fair income distribution in Thailand",
        "comments": "20 pages, 6 figures, 3 tables",
        "journal-ref": "PLoS ONE (2024)",
        "doi": "10.1371/journal.pone.0301693",
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Given a vast concern about high income inequality in Thailand as opposed to\nempirical findings around the world showing people's preference for fair income\ninequality over unfair income equality, it is therefore important to examine\nwhether inequality in income distribution in Thailand over the past three\ndecades is fair, and what fair inequality in income distribution in Thailand\nshould be. To quantitatively measure fair income distribution, this study\nemploys the fairness benchmarks that are derived from the distributions of\nathletes' salaries in professional sports which satisfy the concepts of\ndistributive justice and procedural justice, the no-envy principle of fair\nallocation, and the general consensus or the international norm criterion of a\nmeaningful benchmark. By using the data on quintile income shares and the\nincome Gini index of Thailand from the National Social and Economic Development\nCouncil, this study finds that, throughout the period from 1988 to 2021, the\nThai income earners in the bottom 20%, the second 20%, and the top 20% receive\nincome shares more than the fair shares whereas those in the third 20% and the\nfourth 20% receive income shares less than the fair shares. Provided that there\nare infinite combinations of quintile income shares that can have the same\nvalue of income Gini index but only one of them is regarded as fair, this study\ndemonstrates the use of fairness benchmarks as a practical guideline for\ndesigning policies with an aim to achieve fair income distribution in Thailand.\nMoreover, a comparative analysis is conducted by employing the method for\nestimating optimal (fair) income distribution representing feasible income\nequality in order to provide an alternative recommendation on what optimal\n(fair) income distribution characterizing feasible income equality in Thailand\nshould be.\n"
    },
    {
        "paper_id": 2404.09646,
        "authors": "Battulga Gankhuu",
        "title": "Derivatives of Risk Measures",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  This paper provides the first and second order derivatives of any risk\nmeasures, including VaR and ES for continuous and discrete portfolio loss\nrandom variable variables. Also, we give asymptotic results of the first and\nsecond order conditional moments for heavy-tailed portfolio loss random\nvariable.\n"
    },
    {
        "paper_id": 2404.10006,
        "authors": "Xian Gong, Paul X. McCarthy, Marian-Andrei Rizoiu, Paolo Boldi",
        "title": "Harmony in the Australian Domain Space",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  In this paper we use for the first time a systematic approach in the study of\nharmonic centrality at a Web domain level, and gather a number of significant\nnew findings about the Australian web. In particular, we explore the\nrelationship between economic diversity at the firm level and the structure of\nthe Web within the Australian domain space, using harmonic centrality as the\nmain structural feature. The distribution of harmonic centrality values is\nanalyzed over time, and we find that the distributions exhibit a consistent\npattern across the different years. The observed distribution is well captured\nby a partition of the domain space into six clusters; the temporal movement of\ndomain names across these six positions yields insights into the Australian\nDomain Space and exhibits correlations with other non-structural\ncharacteristics. From a more global perspective, we find a significant\ncorrelation between the median harmonic centrality of all domains in each OECD\ncountry and one measure of global trust, the WJP Rule of Law Index. Further\ninvestigation demonstrates that 35 countries in OECD share similar harmonic\ncentrality distributions. The observed homogeneity in distribution presents a\ncompelling avenue for exploration, potentially unveiling critical corporate,\nregional, or national insights.\n"
    },
    {
        "paper_id": 2404.10088,
        "authors": "Nikitas Stamatopoulos, B. David Clader, Stefan Woerner, William J.\n  Zeng",
        "title": "Quantum Risk Analysis of Financial Derivatives",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  We introduce two quantum algorithms to compute the Value at Risk (VaR) and\nConditional Value at Risk (CVaR) of financial derivatives using quantum\ncomputers: the first by applying existing ideas from quantum risk analysis to\nderivative pricing, and the second based on a novel approach using Quantum\nSignal Processing (QSP). Previous work in the literature has shown that quantum\nadvantage is possible in the context of individual derivative pricing and that\nadvantage can be leveraged in a straightforward manner in the estimation of the\nVaR and CVaR. The algorithms we introduce in this work aim to provide an\nadditional advantage by encoding the derivative price over multiple market\nscenarios in superposition and computing the desired values by applying\nappropriate transformations to the quantum system. We perform complexity and\nerror analysis of both algorithms, and show that while the two algorithms have\nthe same asymptotic scaling the QSP-based approach requires significantly fewer\nquantum resources for the same target accuracy. Additionally, by numerically\nsimulating both quantum and classical VaR algorithms, we demonstrate that the\nquantum algorithm can extract additional advantage from a quantum computer\ncompared to individual derivative pricing. Specifically, we show that under\ncertain conditions VaR estimation can lower the latest published estimates of\nthe logical clock rate required for quantum advantage in derivative pricing by\nup to $\\sim 30$x. In light of these results, we are encouraged that our\nformulation of derivative pricing in the QSP framework may be further leveraged\nfor quantum advantage in other relevant financial applications, and that\nquantum computers could be harnessed more efficiently by considering problems\nin the financial sector at a higher level.\n"
    },
    {
        "paper_id": 2404.10554,
        "authors": "Ivan S. Maksymov",
        "title": "Quantum Mechanics of Human Perception, Behaviour and Decision-Making: A\n  Do-It-Yourself Model Kit for Modelling Optical Illusions and Opinion\n  Formation in Social Networks",
        "comments": "Book chapter. The computational codes and supplementary videos are\n  available at https://github.com/IvanMaksymov/BehavDataSciCodes",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  On the surface, behavioural science and physics seem to be two disparate\nfields of research. However, a closer examination of problems solved by them\nreveals that they are uniquely related to one another. Exemplified by the\ntheories of quantum mind, cognition and decision-making, this unique\nrelationship serves as the topic of this chapter. Surveying the current\nacademic journal papers and scholarly monographs, we present an alternative\nvision of the role of quantum mechanics in the modern studies of human\nperception, behaviour and decision-making. To that end, we mostly aim to answer\nthe 'how' question, deliberately avoiding complex mathematical concepts but\ndeveloping a technically simple computational code that the readers can modify\nto design their own quantum-inspired models. We also present several practical\nexamples of the application of the computation code and outline several\nplausible scenarios, where quantum models based on the proposed do-it-yourself\nmodel kit can help understand the differences between the behaviour of\nindividuals and social groups.\n"
    },
    {
        "paper_id": 2404.10555,
        "authors": "Masanori Hirano, Kentaro Imajo",
        "title": "Construction of Domain-specified Japanese Large Language Model for\n  Finance through Continual Pre-training",
        "comments": "7 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Large language models (LLMs) are now widely used in various fields, including\nfinance. However, Japanese financial-specific LLMs have not been proposed yet.\nHence, this study aims to construct a Japanese financial-specific LLM through\ncontinual pre-training. Before tuning, we constructed Japanese\nfinancial-focused datasets for continual pre-training. As a base model, we\nemployed a Japanese LLM that achieved state-of-the-art performance on Japanese\nfinancial benchmarks among the 10-billion-class parameter models. After\ncontinual pre-training using the datasets and the base model, the tuned model\nperformed better than the original model on the Japanese financial benchmarks.\nMoreover, the outputs comparison results reveal that the tuned model's outputs\ntend to be better than the original model's outputs in terms of the quality and\nlength of the answers. These findings indicate that domain-specific continual\npre-training is also effective for LLMs. The tuned model is publicly available\non Hugging Face.\n"
    },
    {
        "paper_id": 2404.109,
        "authors": "Mario Ghossoub and Giulio Principi and Ruodu Wang",
        "title": "Allocation Mechanisms in Decentralized Exchange Markets with Frictions",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The classical theory of efficient allocations of an aggregate endowment in a\npure-exchange economy has hitherto primarily focused on the Pareto-efficiency\nof allocations, under the implicit assumption that transfers between agents are\nfrictionless, and hence costless to the economy. In this paper, we argue that\ncertain transfers cause frictions that result in costs to the economy. We show\nthat these frictional costs are tantamount to a form of subadditivity of the\ncost of transferring endowments between agents. We suggest an axiomatic study\nof allocation mechanisms, that is, the mechanisms that transform feasible\nallocations into other feasible allocations, in the presence of such transfer\ncosts. Among other results, we provide an axiomatic characterization of those\nallocation mechanisms that admit representations as robust (worst-case) linear\nallocation mechanisms, as well as those mechanisms that admit representations\nas worst-case conditional expectations. We call the latter Robust Conditional\nMean Allocation mechanisms, and we relate our results to the literature on\n(decentralized) risk sharing within a pool of agents.\n"
    },
    {
        "paper_id": 2404.11063,
        "authors": "Muhamad Bhayuta Yudhi Putera, Melia Famiola",
        "title": "Attitudinal Loyalty Manifestation in Banking CSR: Cross-Buying Behavior\n  and Customer Advocacy",
        "comments": "13 pages, 2 figures, 1 table, 1 appendix, published with\n  International Research Journal of Economics and Management Studies (IRJEMS)",
        "journal-ref": null,
        "doi": "10.56472/25835238/IRJEMS-V3I3P131",
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  This study in the banking industry examines the influence of attitudinal\nloyalty on customer advocacy and cross buying behavior, alongside the\nmoderating roles of Quality of Life and Corporate Social Responsibility support\nin the CSR fit and loyalty relationship. Employing Structural Equation\nModeling, it reveals that higher attitudinal loyalty significantly boosts\ncustomer advocacy and propensity for cross buying. The findings highlight the\nimportance of nurturing customer loyalty through valuable and relevant\nofferings, as CSR fit alone does not define the loyalty of the banking\ncustomer. Banks are advised to target customers with a high Quality of Life and\nengage with those who support CSR initiatives aligning with the banks\nobjectives, to enhance loyalty and deepen customer relationships.\n"
    },
    {
        "paper_id": 2404.1108,
        "authors": "Alicia Vidler",
        "title": "Recommender Systems in Financial Trading: Using machine-based conviction\n  analysis in an explainable AI investment framework",
        "comments": "10 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  Traditionally, assets are selected for inclusion in a portfolio (long or\nshort) by human analysts. Teams of human portfolio managers (PMs) seek to weigh\nand balance these securities using optimisation methods and other portfolio\nconstruction processes. Often, human PMs consider human analyst recommendations\nagainst the backdrop of the analyst's recommendation track record and the\napplicability of the analyst to the recommendation they provide. Many firms\nregularly ask analysts to provide a \"conviction\" level on their\nrecommendations. In the eyes of PMs, understanding a human analyst's track\nrecord has typically come down to basic spread sheet tabulation or, at best, a\n\"virtual portfolio\" paper trading book to keep track of results of\nrecommendations.\n  Analysts' conviction around their recommendations and their \"paper trading\"\ntrack record are two crucial workflow components between analysts and portfolio\nconstruction. Many human PMs may not even appreciate that they factor these\ndata points into their decision-making logic. This chapter explores how\nArtificial Intelligence (AI) can be used to replicate these two steps and\nbridge the gap between AI data analytics and AI-based portfolio construction\nmethods. This field of AI is referred to as Recommender Systems (RS). This\nchapter will further explore what metadata that RS systems functionally supply\nto downstream systems and their features.\n"
    },
    {
        "paper_id": 2404.11257,
        "authors": "Francisco G\\'omez Casanova, \\'Alvaro Leitao, Fernando de Lope\n  Contreras and Carlos V\\'azquez",
        "title": "Deep Joint Learning valuation of Bermudan Swaptions",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  This paper addresses the problem of pricing involved financial derivatives by\nmeans of advanced of deep learning techniques. More precisely, we smartly\ncombine several sophisticated neural network-based concepts like differential\nmachine learning, Monte Carlo simulation-like training samples and joint\nlearning to come up with an efficient numerical solution. The application of\nthe latter development represents a novelty in the context of computational\nfinance. We also propose a novel design of interdependent neural networks to\nprice early-exercise products, in this case, Bermudan swaptions. The\nimprovements in efficiency and accuracy provided by the here proposed approach\nis widely illustrated throughout a range of numerical experiments. Moreover,\nthis novel methodology can be extended to the pricing of other financial\nderivatives.\n"
    },
    {
        "paper_id": 2404.11276,
        "authors": "Haotian Chen, Xinjie Shen, Zeqi Ye, Wenjun Feng, Haoxue Wang, Xiao\n  Yang, Xu Yang, Weiqing Liu, Jiang Bian",
        "title": "Towards Data-Centric Automatic R&D",
        "comments": "17 pages, 3 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The progress of humanity is driven by those successful discoveries\naccompanied by countless failed experiments. Researchers often seek the\npotential research directions by reading and then verifying them through\nexperiments. The process imposes a significant burden on researchers. In the\npast decade, the data-driven black-box deep learning method has demonstrated\nits effectiveness in a wide range of real-world scenarios, which exacerbates\nthe experimental burden of researchers and thus renders the potential\nsuccessful discoveries veiled. Therefore, automating such a research and\ndevelopment (R&D) process is an urgent need. In this paper, we serve as the\nfirst effort to formalize the goal by proposing a Real-world Data-centric\nautomatic R&D Benchmark, namely RD2Bench. RD2Bench benchmarks all the\noperations in data-centric automatic R&D (D-CARD) as a whole to navigate future\nwork toward our goal directly. We focus on evaluating the interaction and\nsynergistic effects of various model capabilities and aiding in selecting\nwell-performing trustworthy models. Although RD2Bench is very challenging to\nthe state-of-the-art (SOTA) large language model (LLM) named GPT-4, indicating\nample research opportunities and more research efforts, LLMs possess promising\npotential to bring more significant development to D-CARD: They are able to\nimplement some simple methods without adopting any additional techniques. We\nappeal to future work to take developing techniques for tackling automatic R&D\ninto consideration, thus bringing the opportunities of the potential\nrevolutionary upgrade to human productivity.\n"
    },
    {
        "paper_id": 2404.11334,
        "authors": "Matthias Raddant and Fariba Karimi",
        "title": "The dynamics of diversity on corporate boards",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Diversity in leadership positions and corporate boards is an important aspect\nof equality. It is important because it is the key to better decision-making\nand innovation, and above all, it paves the way for future generations to\nparticipate and shape our society. Many studies emphasize the importance of the\nvisibility of role models and the effect that connectivity has on the success\nof minorities in leadership. However, the connectivity of firms, the dynamics\nof the adoption of minorities, and the long-term effects have not been well\nunderstood. Here, we present a model that shows how these effects work together\nin a dynamic model that is calibrated with empirical data of firm and board\nnetworks. We show that homophily -- the appointment of minorities is influenced\nby the presence of minorities in a board and its neighboring entities -- is an\nimportant effect shaping the trajectory towards equality. We further show how\nperception biases and feedback related to the centrality of minority members\ninfluence the dynamic. We find that reaching equality can be sped up or slowed\ndown depending on the distribution of minorities in central firms. These\ninsights bear significant implications for policy-making geared towards\nfostering equality and diversity within corporate boards.\n"
    },
    {
        "paper_id": 2404.11482,
        "authors": "Claudia Ceci and Alessandra Cretarola",
        "title": "BSDE-based stochastic control for optimal reinsurance in a dynamic\n  contagion model",
        "comments": "40 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We investigate the optimal reinsurance problem in the risk model with jump\nclustering features introduced in [7]. This modeling framework is inspired by\nthe concept initially proposed in [15], combining Hawkes and Cox processes with\nshot noise intensity models. Specifically, these processes describe\nself-exciting and externally excited jumps in the claim arrival intensity,\nrespectively. The insurer aims to maximize the expected exponential utility of\nterminal wealth for general reinsurance contracts and reinsurance premiums. We\ndiscuss two different methodologies: the classical stochastic control approach\nbased on the Hamilton-Jacobi-Bellman (HJB) equation and a backward stochastic\ndifferential equation (BSDE) approach. In a Markovian setting, differently from\nthe classical HJB-approach, the BSDE method enables us to solve the problem\nwithout imposing any requirements for regularity on the associated value\nfunction. We provide a Verification Theorem in terms of a suitable BSDE driven\nby a two-dimensional marked point process and we prove an existence result\nrelaying on the theory developed in [27] for stochastic Lipschitz generators.\nAfter discussing the optimal strategy for general reinsurance contracts and\nreinsurance premiums, we provide more explicit results in some relevant cases.\nFinally, we provide comparison results that highlight the heightened risk\nstemming from the self-exciting component in contrast to the externally-excited\ncounterpart and discuss the monotonicity property of the value function.\n"
    },
    {
        "paper_id": 2404.11526,
        "authors": "Jacob Fein-Ashley",
        "title": "A Comparison of Traditional and Deep Learning Methods for Parameter\n  Estimation of the Ornstein-Uhlenbeck Process",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  We consider the Ornstein-Uhlenbeck (OU) process, a stochastic process widely\nused in finance, physics, and biology. Parameter estimation of the OU process\nis a challenging problem. Thus, we review traditional tracking methods and\ncompare them with novel applications of deep learning to estimate the\nparameters of the OU process. We use a multi-layer perceptron to estimate the\nparameters of the OU process and compare its performance with traditional\nparameter estimation methods, such as the Kalman filter and maximum likelihood\nestimation. We find that the multi-layer perceptron can accurately estimate the\nparameters of the OU process given a large dataset of observed trajectories\nand, on average, outperforms traditional parameter estimation methods.\n"
    },
    {
        "paper_id": 2404.11705,
        "authors": "Tushar Gahlaut and Gourav Dwivedi",
        "title": "A Comprehensive Study for Multi-Criteria Comparison of EV, ICEV, and HEV",
        "comments": "25 pages, 3 figures, 3 tables",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/publicdomain/zero/1.0/",
        "abstract": "  Electric vehicles (EVs) are increasingly becoming popular as a viable means\nof transportation for the future. The use of EVs may help to provide better\nclimatic conditions in urban areas with a pocket friendly cost for\ntransportation to the consumers throughout its life. EVs enact as a boon to the\nsociety by providing zero tailpipe emissions, better comfort, low lifecycle\ncost and higher connectivity. The article aims to provide scientific\ninformation through the literature across various aspects of EVs in their\nlifetime and thus, assist the scholarly community and various organisations to\nunderstand the impact of EVs. In this study we have gathered information from\nthe articles published in SCOPUS database and through grey literature with the\nfocus of information post 2009. After identification of various factors while\npurchasing the vehicle, the total cost of ownership (TCO) for vehicles is\ncalculated and their average TCO for each segment is considered for the study.\nFollowing that, we investigate the ranking of EVs, vehicles powered by internal\ncombustion engines (ICEVs), and hybrid electric vehicles (HEVs) in a variety of\nprice segments by employing a combination of two different multi-criteria\ndecision making (MCDM) techniques. Initially, best-worst method (BWM) is used\nto determine the weights of each of the identified criterion, which is\nthereafter, used in conjunction with the technique for order preference by\nsimilarity to ideal solution (TOPSIS) to compute the rank of the available\nalternatives using BWM. The ranking obtained clearly indicates that EVs should\nbe first purchase choice of the consumers, followed by HEVs and ICEVs\nrespectively. Thus, the results help us conclude that EVs enact as a\nsustainable means of transport for the future.\n"
    },
    {
        "paper_id": 2404.11722,
        "authors": "Yifan He, Abootaleb Shirvani, Barret Shao, Svetlozar Rachev and Frank\n  Fabozzi",
        "title": "Beyond the Bid-Ask: Strategic Insights into Spread Prediction and the\n  Global Mid-Price Phenomenon",
        "comments": "54 pages, 45 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  This study introduces novel concepts in the analysis of limit order books\n(LOBs) with a focus on unveiling strategic insights into spread prediction and\nunderstanding the global mid-price (GMP) phenomenon. We define and analyze the\ntotal market order book bid--ask spread (TMOBBAS) and GMP, showcasing their\nsignificance in providing a deeper understanding of market dynamics beyond\ntraditional LOB models. Employing high-frequency data, we comprehensively\nexamine these concepts through various methodological lenses, including tail\nbehavior analysis, dynamics of log-returns, and risk--return performance\nevaluation. Our findings reveal the intricate behavior of TMOBBAS and GMP under\ndifferent market conditions, offering new perspectives on the liquidity,\nvolatility, and efficiency of markets. This paper not only contributes to the\nacademic discourse on financial markets but also presents practical\nimplications for traders, risk managers, and policymakers seeking to navigate\nthe complexities of modern financial systems.\n"
    },
    {
        "paper_id": 2404.11745,
        "authors": "Yichen Luo, Yebo Feng, Jiahua Xu, Paolo Tasca",
        "title": "Piercing the Veil of TVL: DeFi Reappraised",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Total value locked (TVL) is widely used to measure the size and popularity of\ndecentralized finance (DeFi). However, the prevalent TVL calculation framework\nsuffers from a \"double counting\" issue that results in an inflated metric. We\nfind existing methodologies addressing double counting either inconsistent or\nflawed. To solve this, we formalize the TVL framework and propose a new\nframework, total value redeemable (TVR), to accurately assess the true value\nwithin DeFi. TVL formalization reveals how DeFi's complex network spreads\nfinancial contagion via derivative tokens, amplifying liquidations and\nstablecoin depegging in market downturns. We construct the DeFi multiplier to\nquantify the double counting, which mirrors the money multiplier in traditional\nfinance (TradFi). Our measurement finds substantial double counting in DeFi.\nDuring the peak of DeFi activity on December 2, 2021, the difference between\nTVL and TVR was \\$139.87 billion, with a TVL-to-TVR ratio of about 2. We\nfurther show that TVR is a more stable metric than TVL, especially during\nmarket downturns. A 25% decline in the price of Ether (ETH) leads to a \\$1\nbillion greater non-linear decrease in TVL compared to TVR. We also observe a\nsubstitution effect between TradFi and DeFi.\n"
    },
    {
        "paper_id": 2404.11794,
        "authors": "Benjamin S. Manning, Kehang Zhu, John J. Horton",
        "title": "Automated Social Science: Language Models as Scientist and Subjects",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  We present an approach for automatically generating and testing, in silico,\nsocial scientific hypotheses. This automation is made possible by recent\nadvances in large language models (LLM), but the key feature of the approach is\nthe use of structural causal models. Structural causal models provide a\nlanguage to state hypotheses, a blueprint for constructing LLM-based agents, an\nexperimental design, and a plan for data analysis. The fitted structural causal\nmodel becomes an object available for prediction or the planning of follow-on\nexperiments. We demonstrate the approach with several scenarios: a negotiation,\na bail hearing, a job interview, and an auction. In each case, causal\nrelationships are both proposed and tested by the system, finding evidence for\nsome and not others. We provide evidence that the insights from these\nsimulations of social interactions are not available to the LLM purely through\ndirect elicitation. When given its proposed structural causal model for each\nscenario, the LLM is good at predicting the signs of estimated effects, but it\ncannot reliably predict the magnitudes of those estimates. In the auction\nexperiment, the in silico simulation results closely match the predictions of\nauction theory, but elicited predictions of the clearing prices from the LLM\nare inaccurate. However, the LLM's predictions are dramatically improved if the\nmodel can condition on the fitted structural causal model. In short, the LLM\nknows more than it can (immediately) tell.\n"
    },
    {
        "paper_id": 2404.12001,
        "authors": "Peng Yifeng",
        "title": "Internet sentiment exacerbates intraday overtrading, evidence from\n  A-Share market",
        "comments": "27 pages, 5 tables",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  Market fluctuations caused by overtrading are important components of\nsystemic market risk. This study examines the effect of investor sentiment on\nintraday overtrading activities in the Chinese A-share market. Employing\nhigh-frequency sentiment indices inferred from social media posts on the\nEastmoney forum Guba, the research focuses on constituents of the CSI 300 and\nCSI 500 indices over a period from 01/01/2018, to 12/30/2022. The empirical\nanalysis indicates that investor sentiment exerts a significantly positive\nimpact on intraday overtrading, with the influence being more pronounced among\ninstitutional investors relative to individual traders. Moreover,\nsentiment-driven overtrading is found to be more prevalent during bull markets\nas opposed to bear markets. Additionally, the effect of sentiment on\novertrading is observed to be more pronounced among individual investors in\nlarge-cap stocks compared to small- and mid-cap stocks.\n"
    },
    {
        "paper_id": 2404.12193,
        "authors": "Sergio A. De Raco and Viktoriya Semeshenko",
        "title": "Portrait comparison of binary and weighted Skill Relatedness Networks",
        "comments": "15 p\\'aginas, 9 figuras",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper we compare Skill-Relatedness Networks (SRNs) for selected\ncountries, that is to say statistically significant inter-industrial\ninteractions representing latent skills exchanges derived from observed labor\nflows, a kind of industry spaces. Using data from Argentina (ARG), Germany\n(DEU) and Sweden (SWE), we compare their SRNs utilizing an\ninformation-theoretic method that permits to compare networks of \"non-aligned\"\nnodes, which is the case of interest. For each SRN we extract its portrait, a\nfingerprint of structural measures of the distributions of their shortest\npaths, and calculate their pairwise divergences. This allows us also to\ncontrast differences in structural (binary) connectivity with differences in\nthe information provided by the (weighted) skill relatedness indicator (SR). We\nfind that, in the case of ARG, structural connectivity is very different from\ntheir counterpart in DEU and SWE, but through the glass of SR the distances\nanalyzed are all substantially smaller and more alike. These results qualify\nthe role of the SR indicator as revealing some hidden dimension different from\nconnectivity alone, providing empirical support to the suggestion that industry\nspaces may differ across countries.\n"
    },
    {
        "paper_id": 2404.12214,
        "authors": "Edmund Crawley and Alexandros Theloudis",
        "title": "Income Shocks and their Transmission into Consumption",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  This article reviews the economics literature of, primarily, the last 20\nyears, that studies the link between income shocks and consumption fluctuations\nat the household level. We identify three broad approaches through which\nresearchers estimate the consumption response to income shocks: 1.) structural\nmethods in which a fully or partially specified model helps identify the\nconsumption response to income shocks from the data; 2.) natural experiments in\nwhich the consumption response of one group who receives an income shock is\ncompared to another group who does not; 3.) elicitation surveys in which\nconsumers are asked how they expect to react to various hypothetical events.\n"
    },
    {
        "paper_id": 2404.12477,
        "authors": "Wadim Strielkowski, Oxana Mukhoryanova, Oxana Kuznetsova, Yury Syrov",
        "title": "Sustainable regional economic development and land use: a case of Russia",
        "comments": "11 pages, 4 figures, 2 tables",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  This paper analyzes sustainable regional economic development and land use\nemploying a case study of Russia. The economics of land management in Russia\nwhich is shaped by both historical legacies and contemporary policies\nrepresents an interesting conundrum. Following the dissolution of the Soviet\nUnion, Russia embarked on a thorny and complex path towards the economic\nreforms and transformation characterized, among all, by the privatization and\ndecentralization of land ownership. This transition was aimed at improving\nagricultural productivity and fostering sustainable regional economic\ndevelopment but also led to new challenges such as uneven distribution of land\nresources, unclear property rights, and underinvestment in rural\ninfrastructure. However, managing all of that effectively poses significant\nchallenges and opportunities. With the help of the comprehensive bibliographic\nnetwork analysis, this study sheds some light on the current state of\nsustainable regional economic development and land use management in Russia.\nIts results and outcomes might be helpful for the researchers and stakeholders\nalike in devising effective strategies aimed at maximizing resources for\nsustainable land use, particularly within their respective regional economies.\n"
    },
    {
        "paper_id": 2404.12598,
        "authors": "Yanwei Jia",
        "title": "Continuous-time Risk-sensitive Reinforcement Learning via Quadratic\n  Variation Penalty",
        "comments": "49 pages, 2 figures, 1 table",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  This paper studies continuous-time risk-sensitive reinforcement learning (RL)\nunder the entropy-regularized, exploratory diffusion process formulation with\nthe exponential-form objective. The risk-sensitive objective arises either as\nthe agent's risk attitude or as a distributionally robust approach against the\nmodel uncertainty. Owing to the martingale perspective in Jia and Zhou (2023)\nthe risk-sensitive RL problem is shown to be equivalent to ensuring the\nmartingale property of a process involving both the value function and the\nq-function, augmented by an additional penalty term: the quadratic variation of\nthe value process, capturing the variability of the value-to-go along the\ntrajectory. This characterization allows for the straightforward adaptation of\nexisting RL algorithms developed for non-risk-sensitive scenarios to\nincorporate risk sensitivity by adding the realized variance of the value\nprocess. Additionally, I highlight that the conventional policy gradient\nrepresentation is inadequate for risk-sensitive problems due to the nonlinear\nnature of quadratic variation; however, q-learning offers a solution and\nextends to infinite horizon settings. Finally, I prove the convergence of the\nproposed algorithm for Merton's investment problem and quantify the impact of\ntemperature parameter on the behavior of the learning procedure. I also conduct\nsimulation experiments to demonstrate how risk-sensitive RL improves the\nfinite-sample performance in the linear-quadratic control problem.\n"
    },
    {
        "paper_id": 2404.12974,
        "authors": "Felix Frischmuth, Mattis Berghoff, Martin Braun, and Philipp Haertel",
        "title": "Quantifying seasonal hydrogen storage demands under cost and market\n  uptake uncertainties in energy system transformation pathways",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Climate neutrality paradigms put electricity systems at the core of a clean\nenergy supply. At the same time, indirect electrification, with a potential\nuptake of hydrogen or derived fuel economy, plays a crucial role in\ndecarbonising the energy supply and industrial processes. Besides energy\nmarkets coordinating the transition, climate and energy policy targets require\nfundamental changes and expansions in the energy transmission, import,\ndistribution, and storage infrastructures. While existing studies identify\nrelevant demands for hydrogen, critical decisions involve imports versus\ndomestic fuel production and investments in new or repurposing existing\npipeline and storage infrastructure. Linking the pan-European energy system\nplanning model SCOPE SD with the multiperiod European gas market model IMAGINE,\nthe case study analysis and its transformation pathway results indicate\nextensive network development of hydrogen infrastructure, including expansion\nbeyond refurbished methane infrastructure. However, the ranges of future\nhydrogen storage costs and market uptake restrictions expose and quantify the\nuncertainty of its role in Europes transformation. The study finds that rapidly\nplanning the construction of hydrogen storage and pipeline infrastructure is\ncrucial to achieving the required capacity by 2050.\n"
    },
    {
        "paper_id": 2404.12988,
        "authors": "Christelle Zozoungbo",
        "title": "The Role of Gender, Birth Order, and Ability in Intra-household\n  Educational Inequality: Evidence from Benin",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper explores how gender, birth order, and innate ability affect\neducational disparities among children within households. Using both a\nreduced-form approach and a structural model of household educational resource\nallocation, it identifies the contributions of gender and birth order to\nintra-household educational inequality. In Benin, significant disparities are\nfound in households with non-educated heads and mixed-gender children, with 70%\nof the inequality attributed to gender and birth order. In contrast, households\nled by college-educated heads show 24% gender effects and 9% birth order\neffects. Additionally, college-educated parents exhibit less overall\neducational inequality among their children. Policy counterfactuals assess the\nimpact of (1) education vouchers, (2) compulsory education, and (3) targeted\neducational cost reduction for non-educated parents. All three policies reduced\nthe effects of gender and birth order on inequality. Compulsory education\nreduced overall average inequality, while targeted educational cost reduction\ncompletely eliminated gender and birth order effects. This research underscores\nthe complex factors driving intra-household educational inequalities and\nsuggests effective policy measures.\n"
    },
    {
        "paper_id": 2404.13163,
        "authors": "Alireza Javadian Sabet and Sarah H. Bana and Renzhe Yu and Morgan R.\n  Frank",
        "title": "A national longitudinal dataset of skills taught in U.S. higher\n  education curricula",
        "comments": "44 pages, 21 figures, 10 tables",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Higher education plays a critical role in driving an innovative economy by\nequipping students with knowledge and skills demanded by the workforce. While\nresearchers and practitioners have developed data systems to track detailed\noccupational skills, such as those established by the U.S. Department of Labor\n(DOL), much less effort has been made to document skill development in higher\neducation at a similar granularity. Here, we fill this gap by presenting a\nlongitudinal dataset of skills inferred from over three million course syllabi\ntaught at nearly three thousand U.S. higher education institutions. To\nconstruct this dataset, we apply natural language processing to extract from\ncourse descriptions detailed workplace activities (DWAs) used by the DOL to\ndescribe occupations. We then aggregate these DWAs to create skill profiles for\ninstitutions and academic majors. Our dataset offers a large-scale\nrepresentation of college-educated workers and their role in the economy. To\nshowcase the utility of this dataset, we use it to 1) compare the similarity of\nskills taught and skills in the workforce according to the US Bureau of Labor\nStatistics, 2) estimate gender differences in acquired skills based on\nenrollment data, 3) depict temporal trends in the skills taught in social\nscience curricula, and 4) connect college majors' skill distinctiveness to\nsalary differences of graduates. Overall, this dataset can enable new research\non the source of skills in the context of workforce development and provide\nactionable insights for shaping the future of higher education to meet evolving\nlabor demands especially in the face of new technologies.\n"
    },
    {
        "paper_id": 2404.13178,
        "authors": "Andres Gomez-Lievano and Michail Fragkias",
        "title": "The benefits and costs of agglomeration: insights from economics and\n  complexity",
        "comments": "Compendium of Urban Complexity. 31 pages, 4 figures, 1 table",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  There are many benefits and costs that come from people and firms clustering\ntogether in space. Agglomeration economies, in particular, are the\nmanifestation of centripetal forces that make larger cities disproportionately\nmore wealthy than smaller cities, pulling together individuals and firms in\nclose physical proximity. Measuring agglomeration economies, however, is not\neasy, and the identification of its causes is still debated. Such association\nof productivity with size can arise from interactions that are facilitated by\ncities (\"positive externalities\"), but also from more productive individuals\nmoving in and sorting into large cities (\"self-sorting\"). Under certain\ncircumstances, even pure randomness can generate increasing returns to scale.\nIn this chapter, we discuss some of the empirical observations, models,\nmeasurement challenges, and open question associated with the phenomenon of\nagglomeration economies. Furthermore, we discuss the implications of urban\ncomplexity theory, and in particular urban scaling, for the literature in\nagglomeration economies.\n"
    },
    {
        "paper_id": 2404.13189,
        "authors": "Gonzalo Garcia-Atance Fatjo",
        "title": "Use of two Public Distributed Ledgers to track the money of an economy",
        "comments": "10 pages, 1 figure,",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-sa/4.0/",
        "abstract": "  A tool to improve the effectiveness and the efficiency of public spending is\nproposed here. In the 19th century banknotes had a serial number. However, in\nmodern days the use of digital transactions that do not use physical currency\nhas opened the possibility to digitally track almost each cent of the economy.\nIn this article a serial number or tracking number for each cent, pence or any\nother monetary unit of the economy is proposed. Then, almost all cents can be\ntracked by recording the transactions in a public distributed ledger, rather\nthan recording the amount of the transaction, the information recorded in the\nblock of the transaction is the actual serial number or tracking number for\neach cent that changes ownership. In order to keep the privacy of the\ntransaction, only generic identification of private companies and individuals\nare recorded along with generic information about the concept of transaction,\nthe region and the date/time. A secondary public distributed ledger whose\nblocks are identified by a hash reference that is recorded in the bank\nstatement available to the payer and the payee allows for checking the accuracy\nof the first public distributed ledger by comparing the transactions made in\none day, one region and one type of concept. However, the transactions made or\nreceived by the government are recorded with a much higher level of detail in\nthe first ledger and a higher level of disclosure in the second ledger. The\nresult is a tool that is able to accurately track public spending, to keep\nprivacy of individuals and companies and to make statistical analysis and\nexperiments or real tests in the economy of a country. This tool has the\npotential to assist public policymakers in demonstrating the societal benefits\nresulting from their policies, thereby enabling more informed decision-making\nfor future policy endeavours.\n"
    },
    {
        "paper_id": 2404.13211,
        "authors": "Rajat Verma and Eunhan Ka and Satish V. Ukkusuri",
        "title": "Long-term forecasts of statewide travel demand patterns using\n  large-scale mobile phone GPS data: A case study of Indiana",
        "comments": "This paper constitutes a chapter of project SPR-4608 (DOI:\n  10.5703/128828431768) sponsored by the Joint Transportation Research Program\n  and the Indiana Department of Transportation. It has not been published\n  elsewhere",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  The growth in availability of large-scale GPS mobility data from mobile\ndevices has the potential to aid traditional travel demand models (TDMs) such\nas the four-step planning model, but those processing methods are not commonly\nused in practice. In this study, we show the application of trip generation and\ntrip distribution modeling using GPS data from smartphones in the state of\nIndiana. This involves extracting trip segments from the data and inferring the\nphone users' home locations, adjusting for data representativeness, and using a\ndata-driven travel time-based cost function for the trip distribution model.\nThe trip generation and interchange patterns in the state are modeled for 2025,\n2035, and 2045. Employment sectors like industry and retail are observed to\ninfluence trip making behavior more than other sectors. The travel growth is\npredicted to be mostly concentrated in the suburban regions, with a small\ndecline in the urban cores. Further, although the majority of the growth in\ntrip flows over the years is expected to come from the corridors between the\nmajor urban centers of the state, relative interzonal trip flow growth will\nlikely be uniformly spread throughout the state. We also validate our results\nwith the forecasts of two travel demand models, finding a difference of 5-15%\nin overall trip counts. Our GPS data-based demand model will contribute towards\naugmenting the conventional statewide travel demand model developed by the\nstate and regional planning agencies.\n"
    },
    {
        "paper_id": 2404.13291,
        "authors": "Xue Dong He, Chen Yang, Yutian Zhou",
        "title": "Liquidity Pool Design on Automated Market Makers",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Automated market makers are a popular type of decentralized exchange in which\nusers trade assets with each other directly and automatically through a\nliquidity pool and a fixed pricing function. The liquidity provider contributes\nto the liquidity pool by supplying assets to the pool and in return they earn\ntransaction fees from traders who trade through the pool. We propose a model of\noptimal liquidity provision in which the risk-averse liquidity provider decides\nthe investment proportion of wealth she would like to supply to the pool, trade\nin a centralized market, and consume in multiple periods. We derive the\nliquidity provider's optimal strategy by dynamic programming and numerically\nfind the optimal liquidity pool that maximizes the liquidity provider's\nutility. Our findings indicate that the exchange rate volatility on the\ncentralized market exerts a positive effect on the optimal transaction fee.\nMoreover, the optimal constant mean pricing formula is found to be related to\nthe relative performance of the underlying assets on the centralized market.\n"
    },
    {
        "paper_id": 2404.13371,
        "authors": "Chung-Han Hsieh and Yi-Shan Wong",
        "title": "On Risk-Sensitive Decision Making Under Uncertainty",
        "comments": "submitted for possible publication",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  This paper studies a risk-sensitive decision-making problem under\nuncertainty. It considers a decision-making process that unfolds over a fixed\nnumber of stages, in which a decision-maker chooses among multiple\nalternatives, some of which are deterministic and others are stochastic. The\ndecision-maker's cumulative value is updated at each stage, reflecting the\noutcomes of the chosen alternatives. After formulating this as a stochastic\ncontrol problem, we delineate the necessary optimality conditions for it. Two\nillustrative examples from optimal betting and inventory management are\nprovided to support our theory.\n"
    },
    {
        "paper_id": 2404.13637,
        "authors": "Mengshuo Zhao, Narayanaswamy Balakrishnan, Chuancun Yin",
        "title": "Extremal cases of distortion risk measures with partial information",
        "comments": "36 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper considers the best- and worst-case of a general class of\ndistortion risk measures when only partial information regarding the underlying\ndistributions is available. Specifically, explicit sharp lower and upper bounds\nfor a general class of distortion risk measures are derived based on the first\ntwo moments along with some shape information, such as symmetry/unimodality\nproperty of the underlying distributions. The proposed approach provides a\nunified framework for extremal problems of distortion risk measures.\n"
    },
    {
        "paper_id": 2404.13754,
        "authors": "Bastien Baldacci, Philippe Bergault, Olivier Gu\\'eant",
        "title": "Dispensing with optimal control: a new approach for the pricing and\n  management of share buyback contracts",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  This paper introduces a novel methodology for the pricing and management of\nshare buyback contracts, overcoming the limitations of traditional optimal\ncontrol methods, which frequently encounter difficulties with high-dimensional\nstate spaces and the intricacies of selecting appropriate risk penalty or risk\naversion parameter. Our methodology applies optimized heuristic strategies to\nmaximize the contract's value. The computation of this value utilizes classical\nmethods typically used for pricing path-dependent options. Additionally, our\napproach naturally leads to the formulation of a $\\Delta$-hedging strategy and\ndisentangles therefore the repurchase strategy from the hedging of the payoff.\n"
    },
    {
        "paper_id": 2404.13768,
        "authors": "Yulin Liu and Luyao Zhang",
        "title": "The Economics of Blockchain Governance: Evaluate Liquid Democracy on the\n  Internet Computer",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Decentralized Autonomous Organizations (DAOs), utilizing blockchain\ntechnology to enable collective governance, are a promising innovation. This\nresearch addresses the ongoing query in blockchain governance: How can DAOs\noptimize human cooperation? Focusing on the Network Nervous System (NNS), a\ncomprehensive on-chain governance framework underpinned by the Internet\nComputer Protocol (ICP) and liquid democracy principles, we employ theoretical\nabstraction and simulations to evaluate its potential impact on cooperation and\neconomic growth within DAOs. Our findings emphasize the significance of the\nNNS's staking mechanism, particularly the reward multiplier, in aligning\nindividual short-term interests with the DAO's long-term prosperity. This study\ncontributes to the understanding and effective design of blockchain-based\ngovernance systems.\n"
    },
    {
        "paper_id": 2404.13818,
        "authors": "Jiayue Zhang, Ken Seng Tan, Tony S. Wirjanto, Lysa Porth",
        "title": "Joint Liability Model with Adaptation to Climate Change",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper extends the application of ESG score assessment methodologies from\nlarge corporations to individual farmers' production, within the context of\nclimate change. Our proposal involves the integration of crucial agricultural\nsustainability variables into conventional personal credit evaluation\nframeworks, culminating in the formulation of a holistic sustainable credit\nrating referred to as the Environmental, Social, Economics (ESE) score. This\nESE score is integrated into theoretical joint liability models, to gain\nvaluable insights into optimal group sizes and individual-ESE score\nrelationships. Additionally, we adopt a mean-variance utility function for\nfarmers to effectively capture the risk associated with anticipated profits.\nThrough a set of simulation exercises, the paper investigates the implications\nof incorporating ESE scores into credit evaluation systems, offering a nuanced\ncomprehension of the repercussions under various climatic conditions.\n"
    },
    {
        "paper_id": 2404.13869,
        "authors": "Gordon Getty, Nikita Tkachenko",
        "title": "A separate way to measure rate of return",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Net profit is sometimes found from data for net operating surplus. We propose\na way to find it from data for consumption, pay and market-value capital, and\nconcomitantly to reveal the factor shares in consumption.\n"
    },
    {
        "paper_id": 2404.13964,
        "authors": "Jiachen T. Wang, Zhun Deng, Hiroaki Chiba-Okabe, Boaz Barak, Weijie J.\n  Su",
        "title": "An Economic Solution to Copyright Challenges of Generative AI",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/publicdomain/zero/1.0/",
        "abstract": "  Generative artificial intelligence (AI) systems are trained on large data\ncorpora to generate new pieces of text, images, videos, and other media. There\nis growing concern that such systems may infringe on the copyright interests of\ntraining data contributors. To address the copyright challenges of generative\nAI, we propose a framework that compensates copyright owners proportionally to\ntheir contributions to the creation of AI-generated content. The metric for\ncontributions is quantitatively determined by leveraging the probabilistic\nnature of modern generative AI models and using techniques from cooperative\ngame theory in economics. This framework enables a platform where AI developers\nbenefit from access to high-quality training data, thus improving model\nperformance. Meanwhile, copyright owners receive fair compensation, driving the\ncontinued provision of relevant data for generative model training. Experiments\ndemonstrate that our framework successfully identifies the most relevant data\nsources used in artwork generation, ensuring a fair and interpretable\ndistribution of revenues among copyright owners.\n"
    },
    {
        "paper_id": 2404.13986,
        "authors": "Daichi Hiraki, Siddhartha Chib, Yasuhiro Omori",
        "title": "Stochastic Volatility in Mean: Efficient Analysis by a Generalized\n  Mixture Sampler",
        "comments": "36 pages, 11 figures, 9 tables",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper we consider the simulation-based Bayesian analysis of\nstochastic volatility in mean (SVM) models. Extending the highly efficient\nMarkov chain Monte Carlo mixture sampler for the SV model proposed in Kim et\nal. (1998) and Omori et al. (2007), we develop an accurate approximation of the\nnon-central chi-squared distribution as a mixture of thirty normal\ndistributions. Under this mixture representation, we sample the parameters and\nlatent volatilities in one block. We also detail a correction of the small\napproximation error by using additional Metropolis-Hastings steps. The proposed\nmethod is extended to the SVM model with leverage. The methodology and models\nare applied to excess holding yields in empirical studies, and the SVM model\nwith leverage is shown to outperform competing volatility models based on\nmarginal likelihoods.\n"
    },
    {
        "paper_id": 2404.14041,
        "authors": "O. Bertolami",
        "title": "Natural Capital as a Stock Option",
        "comments": "13 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  The unfolding climate crisis is a physical manifestation of the damage that\nmarket economy, driven by the high intensity consumption of fossil fuels, has\ninflicted on the Earth System and on the stability conditions that were\nestablished by a complex conjugation of natural factors during the Holoecene.\nThe magnitude of the human activities and its predatory nature is such that it\nis no longer possible to consider the Earth System and the services it provides\nfor the habitability of the planet, the so-called natural capital, as an\neconomical externality. Thus one is left with two main choices in what concerns\nthe sustaintability of the planet's habitability: radical economic degrowth or\nhighly efficient solutions to internalise the maintenance and the restoration\nof ecosystems and the services of the Earth System. It is proposed that an\ninteresting strategy for the latter is to consider the natural capital as a\nstock option.\n"
    },
    {
        "paper_id": 2404.14115,
        "authors": "Tom Ewen",
        "title": "Pricing of European Calls with the Quantum Fourier Transform",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  The accurate valuation of financial derivatives plays a pivotal role in the\nfinance industry. Although closed formulas for pricing are available for\ncertain models and option types, exemplified by the European Call and Put\noptions in the Black-Scholes Model, the use of either more complex models or\nmore sophisticated options precludes the existence of such formulas, thereby\nrequiring alternative approaches. The Monte Carlo simulation, an alternative\napproach effective in nearly all scenarios, has already been challenged by\nquantum computing techniques that leverage Amplitude Estimation. Despite its\ntheoretical promise, this approach currently faces limitations due to the\nconstraints of hardware in the Noisy Intermediate-Scale Quantum (NISQ) era.\n  In this study, we introduce and analyze a quantum algorithm for pricing\nEuropean call options across a broad spectrum of asset models. This method\ntransforms a classical approach, which utilizes the Fast Fourier Transform\n(FFT), into a quantum algorithm, leveraging the efficiency of the Quantum\nFourier Transform (QFT). Furthermore, we compare this novel algorithm with\nexisting quantum algorithms for option pricing.\n"
    },
    {
        "paper_id": 2404.14136,
        "authors": "Tobias Fissler, Fangda Liu, Ruodu Wang, Linxiao Wei",
        "title": "Elicitability and identifiability of tail risk measures",
        "comments": "31 pages; typo in equation (5.1) fixed in version 2",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Tail risk measures are fully determined by the distribution of the underlying\nloss beyond its quantile at a certain level, with Value-at-Risk and Expected\nShortfall being prime examples. They are induced by law-based risk measures,\ncalled their generators, evaluated on the tail distribution. This paper\nestablishes joint identifiability and elicitability results of tail risk\nmeasures together with the corresponding quantile, provided that their\ngenerators are identifiable and elicitable, respectively. As an example, we\nestablish the joint identifiability and elicitability of the tail expectile\ntogether with the quantile. The corresponding consistent scores constitute a\nnovel class of weighted scores, nesting the known class of scores of Fissler\nand Ziegel for the Expected Shortfall together with the quantile. For\nstatistical purposes, our results pave the way to easier model fitting for tail\nrisk measures via regression and the generalized method of moments, but also\nmodel comparison and model validation in terms of established backtesting\nprocedures.\n"
    },
    {
        "paper_id": 2404.14137,
        "authors": "Abdulnasser Hatemi-J",
        "title": "An Asymmetric Capital Asset Pricing Model",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  Providing a measure of market risk is an important issue for investors and\nfinancial institutions. However, the existing models for this purpose are per\ndefinition symmetric. The current paper introduces an asymmetric capital asset\npricing model for measurement of the market risk. It explicitly accounts for\nthe fact that falling prices determine the risk for a long position in the\nrisky asset and the rising prices govern the risk for a short position. Thus, a\nposition dependent market risk measure that is provided accords better with\nreality. The empirical application reveals that Apple stock is more volatile\nthan the market only for the short seller. Surprisingly, the investor that has\na long position in this stock is facing a lower volatility than the market.\nThis property is not captured by the standard asset pricing model, which has\nimportant implications for the expected returns and hedging designs.\n"
    },
    {
        "paper_id": 2404.14141,
        "authors": "Christoph Riedl, Tom Grad, Christopher Lettl",
        "title": "Competition and Collaboration in Crowdsourcing Communities: What happens\n  when peers evaluate each other?",
        "comments": "Currently in press",
        "journal-ref": "Organization Science, 2024",
        "doi": "10.1287/orsc.2021.15163",
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Crowdsourcing has evolved as an organizational approach to distributed\nproblem solving and innovation. As contests are embedded in online communities\nand evaluation rights are assigned to the crowd, community members face a\ntension: they find themselves exposed to both competitive motives to win the\ncontest prize and collaborative participation motives in the community. The\ncompetitive motive suggests they may evaluate rivals strategically according to\ntheir self-interest, the collaborative motive suggests they may evaluate their\npeers truthfully according to mutual interest. Using field data from Threadless\non 38 million peer evaluations of more than 150,000 submissions across 75,000\nindividuals over 10 years and two natural experiments to rule out alternative\nexplanations, we answer the question of how community members resolve this\ntension. We show that as their skill level increases, they become increasingly\ncompetitive and shift from using self-promotion to sabotaging their closest\ncompetitors. However, we also find signs of collaborative behavior when\nhigh-skilled members show leniency toward those community members who do not\ndirectly threaten their chance of winning. We explain how the individual-level\nuse of strategic evaluations translates into important organizational-level\noutcomes by affecting the community structure through individuals' long-term\nparticipation. While low-skill targets of sabotage are less likely to\nparticipate in future contests, high-skill targets are more likely. This\nsuggests a feedback loop between competitive evaluation behavior and future\nparticipation. These findings have important implications for the literature on\ncrowdsourcing design, and the evolution and sustainability of crowdsourcing\ncommunities.\n"
    },
    {
        "paper_id": 2404.14252,
        "authors": "Tommaso Gastaldi",
        "title": "On a fundamental statistical edge principle",
        "comments": "For companion simulation material, real-life case studies, and source\n  code, see https://www.datatime.eu/public/arXiv_paper/",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper establishes that conditioning the probability of execution of new\norders on the self-generated historical trading information (HTI) of a trading\nstrategy is a necessary condition for a statistical trading edge. It is shown,\nin particular, that, given any trading strategy S that does not use its own\nHTI, it is always possible to construct a new strategy S* that yields a\nsystematically increasing improvement over S in terms of profit and loss (PnL)\nby using the self-generated HTI. This holds true under rather general\nconditions that are frequently met in practice, and it is proven through a\ndecision mechanism specifically designed to formally prove this idea.\nSimulations and real-world trading evidence are included for validation and\nillustration, respectively.\n"
    },
    {
        "paper_id": 2404.14302,
        "authors": "Andreas Haufler, Hayato Kato",
        "title": "A Global Minimum Tax for Large Firms Only: Implications for Tax\n  Competition",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  The Global Minimum Tax (GMT) is applied only to firms above a certain size\nthreshold. We set up a simple model of tax competition and profit shifting by\nheterogeneous multinational firms to evaluate the effects of this partial\ncoverage of the GMT. A non-haven and a haven country are bound by the GMT rate\nfor large multinationals, but can set tax rates for firms below the threshold\nnon-cooperatively. We show that the introduction of the GMT with a moderate tax\nrate increases tax revenues in both the non-haven and the haven countries.\nGradual increases in the GMT rate, however, trigger a sudden change in the tax\ncompetition equilibrium from a uniform to a split corporate tax rate, at which\ntax revenues in the non-haven country decline. In contrast, gradual increases\nin the coverage of the GMT never harm the non-haven country. We also discuss\nthe quantitative effects of introducing a $15\\%$ GMT rate in a calibrated\nversion of our model.\n"
    },
    {
        "paper_id": 2404.14337,
        "authors": "Agathe Sadeghi and Zachary Feinstein",
        "title": "Statistical Validation of Contagion Centrality in Financial Networks",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper, we introduce a novel centrality measure to evaluate shock\npropagation on financial networks capturing a notion of contagion and systemic\nrisk contributions. In comparison to many popular centrality metrics (e.g.,\neigenvector centrality) which provide only a relative centrality between nodes,\nour proposed measure is in an absolute scale permitting comparisons of\ncontagion risk over time. In addition, we provide a statistical validation\nmethod when the network is estimated from data, as is done in practice. This\nstatistical test allows us to reliably assess the computed centrality values.\nWe validate our methodology on simulated data and conduct empirical case\nstudies using financial data. We find that our proposed centrality measure\nincreases significantly during times of financial distress and is able to\nprovide insights in to the (market implied) risk-levels of different firms and\nsectors.\n"
    },
    {
        "paper_id": 2404.15023,
        "authors": "Liyuan Lin, Ruodu Wang, Ruixun Zhang, Chaoyi Zhao",
        "title": "The checkerboard copula and dependence concepts",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We study the problem of choosing the copula when the marginal distributions\nof a random vector are not all continuous. Inspired by three motivating\nexamples including simulation from copulas, stress scenarios, and co-risk\nmeasures, we propose to use the checkerboard copula, that is, intuitively, the\nunique copula with a distribution that is as uniform as possible within regions\nof flexibility. We show that the checkerboard copula has the largest Shannon\nentropy, which means that it carries the least information among all possible\ncopulas for a given random vector. Furthermore, the checkerboard copula\npreserves the dependence information of the original random vector, leading to\ntwo applications in the context of diversification penalty and impact\nportfolios.\n"
    },
    {
        "paper_id": 2404.15079,
        "authors": "Federico Cannerozzi, Giorgio Ferrari",
        "title": "Cooperation, Correlation and Competition in Ergodic $N$-player Games and\n  Mean-field Games of Singular Controls: A Case Study",
        "comments": "33 pages, 3 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We consider ergodic symmetric $N$-player and mean-field games of singular\ncontrol in both cooperative and competitive settings. The state process\ndynamics of a representative player follow geometric Brownian motion,\ncontrolled additively through a nondecreasing process. Agents aim to maximize a\nlong-time average reward functional with instantaneous profit of power type.\nThe game shows strategic complementarities, in that the marginal profit\nfunction is increasing with respect to the dynamic average of the states of the\nother players, when $N<\\infty$, or with respect to the stationary mean of the\nplayers' distribution, in the mean-field case. In the mean-field formulation,\nwe explicitly construct the solution to the mean-field control problem\nassociated with central planner optimization, as well as Nash and coarse\ncorrelated equilibria (with singular and regular recommendations). Among our\nfindings, we show that coarse correlated equilibria may exist even when Nash\nequilibria do not. Additionally, we show that a coarse correlated equilibrium\nwith a regular (absolutely continuous) recommendation can outperform a Nash\nequilibrium where the equilibrium policy is of reflecting type (thus singularly\ncontinuous). Furthermore, we prove that the constructed mean-field control and\nmean-field equilibria can approximate the cooperative and competitive\nequilibria, respectively, in the corresponding game with $N$ players when $N$\nis sufficiently large. To the best of our knowledge, this paper is the first to\ncharacterize coarse correlated equilibria, construct the explicit solution to\nan ergodic mean-field control problem, and provide approximation results for\nthe related $N$-player game in the context of singular control games.\n"
    },
    {
        "paper_id": 2404.15226,
        "authors": "Jos\\'e Moran and Angelo Secchi and Jean-Philippe Bouchaud",
        "title": "Revisiting Granular Models of Firm Growth",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We revisit granular models that represent the size of a firm as the sum of\nthe sizes of multiple constituents or sub-units. Originally developed to\naddress the unexpectedly slow reduction in volatility as firm size increases,\nthese models also explain the shape of the distribution of firm growth rates.\n  We introduce new theoretical insights regarding the relationship between firm\nsize and growth rate statistics within this framework, directly linking the\ngrowth statistics of a firm to how diversified it is. The non-intuitive nature\nof our results arises from the fat-tailed distributions of the size and the\nnumber of sub-units, which suggest the categorization of firms into three\ndistinct diversification types: well-diversified firms with sizes evenly\ndistributed across many sub-units, firms with many sub-units but concentrated\nsize in just a few, and poorly diversified firms consisting of only a small\nnumber of sub-units.\n  Inspired by our theoretical findings, we identify new empirical patterns in\nfirm growth. Our findings show that growth volatility, when adjusted by average\nsize-conditioned volatility, has a size-independent distribution, but with a\ntail that is much too thin to be in agreement with the predictions of granular\nmodels. Furthermore, the predicted Gaussian distribution of growth rates, even\nwhen rescaled for firm-specific volatility, remains fat-tailed across all\nsizes. Such discrepancies not only challenge the granularity hypothesis but\nalso underscore the need for deeper exploration into the mechanisms driving\nfirm growth.\n"
    },
    {
        "paper_id": 2404.15391,
        "authors": "Luke Snow, Vikram Krishnamurthy",
        "title": "Adaptive Mechanism Design using Multi-Agent Revealed Preferences",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  This paper constructs an algorithmic framework for adaptively achieving the\nmechanism design objective, finding a mechanism inducing socially optimal Nash\nequilibria, without knowledge of the utility functions of the agents. We\nconsider a probing scheme where the designer can iteratively enact mechanisms\nand observe Nash equilibria responses. We first derive necessary and sufficient\nconditions, taking the form of linear program feasibility, for the existence of\nutility functions under which the empirical Nash equilibria responses are\nsocially optimal. Then, we utilize this to construct a loss function with\nrespect to the mechanism, and show that its global minimization occurs at\nmechanisms under which Nash equilibria system responses are also socially\noptimal. We develop a simulated annealing-based gradient algorithm, and prove\nthat it converges in probability to this set of global minima, thus achieving\nadaptive mechanism design.\n"
    },
    {
        "paper_id": 2404.15478,
        "authors": "Alexander Barzykin, Philippe Bergault, Olivier Gu\\'eant",
        "title": "Algorithmic Market Making in Spot Precious Metals",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  The primary challenge of market making in spot precious metals is navigating\nthe liquidity that is mainly provided by futures contracts. The Exchange for\nPhysical (EFP) spread, which is the price difference between futures and spot,\nplays a pivotal role and exhibits multiple modes of relaxation corresponding to\nthe diverse trading horizons of market participants. In this paper, we\nintroduce a novel framework utilizing a nested Ornstein-Uhlenbeck process to\nmodel the EFP spread. We demonstrate the suitability of the framework for\nmaximizing the expected P\\&L of a market maker while minimizing inventory risk\nacross both spot and futures. Using a computationally efficient technique to\napproximate the solution of the Hamilton-Jacobi-Bellman equation associated\nwith the corresponding stochastic optimal control problem, our methodology\nfacilitates strategy optimization on demand in near real-time, paving the way\nfor advanced algorithmic market making that capitalizes on the co-integration\nproperties intrinsic to the precious metals sector.\n"
    },
    {
        "paper_id": 2404.15489,
        "authors": "Matthew Willetts and Christian Harrington",
        "title": "Multiblock MEV opportunities & protections in dynamic AMMs",
        "comments": "7 pages plus appendix",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Maximal Extractable Value (MEV) in Constant Function Market Making is fairly\nwell understood. Does having dynamic weights, as found in liquidity boostrap\npools (LBPs), Temporal-function market makers (TFMMs), and Replicating market\nmakers (RMMs), introduce new attack vectors? In this paper we explore how\ninter-block weight changes can be analogous to trades, and can potentially lead\nto a multi-block MEV attack. New inter-block protections required to guard\nagainst this new attack vector are analysed. We also carry our a raft of\nnumerical simulations, more than 450 million potential attack scenarios,\nshowing both successful attacks and successful defense.\n"
    },
    {
        "paper_id": 2404.15495,
        "authors": "Marcin W\\k{a}torek and Pawe{\\l} Szyd{\\l}o and Jaros{\\l}aw Kwapie\\'n\n  and Stanis{\\l}aw Dro\\.zd\\.z",
        "title": "Correlations versus noise in the NFT market",
        "comments": null,
        "journal-ref": "Chaos 34, 073112 (2024)",
        "doi": "10.1063/5.0214399",
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  The non-fungible token (NFT) market emerges as a recent trading innovation\nleveraging blockchain technology, mirroring the dynamics of the cryptocurrency\nmarket. The current study is based on the capitalization changes and\ntransaction volumes across a large number of token collections on the Ethereum\nplatform. In order to deepen the understanding of the market dynamics, the\ncollection-collection dependencies are examined by using the multivariate\nformalism of detrended correlation coefficient and correlation matrix. It\nappears that correlation strength is lower here than that observed in\npreviously studied markets. Consequently, the eigenvalue spectra of the\ncorrelation matrix more closely follow the Marchenko-Pastur distribution,\nstill, some departures indicating the existence of correlations remain. The\ncomparison of results obtained from the correlation matrix built from the\nPearson coefficients and, independently, from the detrended cross-correlation\ncoefficients suggests that the global correlations in the NFT market arise from\nhigher frequency fluctuations. Corresponding minimal spanning trees (MSTs) for\ncapitalization variability exhibit a scale-free character while, for the number\nof transactions, they are somewhat more decentralized.\n"
    },
    {
        "paper_id": 2404.16169,
        "authors": "Minwu Kim",
        "title": "Interpretable Machine Learning Models for Predicting the Next Targets of\n  Activist Funds",
        "comments": "15 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  This work develops a predictive model to identify potential targets of\nactivist investment funds, which strategically acquire significant corporate\nstakes to drive operational and strategic improvements and enhance shareholder\nvalue. Predicting these targets is crucial for companies to mitigate\nintervention risks, for activists to select optimal targets, and for investors\nto capitalize on associated stock price gains. Our analysis utilizes data from\nthe Russell 3000 index from 2016 to 2022. We tested 123 variations of models\nusing different data imputation, oversampling, and machine learning methods,\nachieving a top AUC-ROC of 0.782. This demonstrates the model's effectiveness\nin identifying likely targets of activist funds. We applied the Shapley value\nmethod to determine the most influential factors in a company's susceptibility\nto activist investment. This interpretative approach provides clear insights\ninto the driving forces behind activist targeting. Our model offers\nstakeholders a strategic tool for proactive corporate governance and investment\nstrategy, enhancing understanding of the dynamics of activist investing.\n"
    },
    {
        "paper_id": 2404.16295,
        "authors": "Liexin Cheng, Xue Cheng, Xianhua Peng",
        "title": "Joint calibration to SPX and VIX Derivative Markets with Composite\n  Change of Time Models",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The Chicago Board Options Exchange Volatility Index (VIX) is calculated from\nSPX options and derivatives of VIX are also traded in market, which leads to\nthe so-called \"consistent modeling\" problem. This paper proposes a time-changed\nL\\'evy model for log price with a composite change of time structure to capture\nboth features of the implied SPX volatility and the implied volatility of\nvolatility. Consistent modeling is achieved naturally via flexible choices of\njumps and leverage effects, as well as the composition of time changes. Many\ncelebrated models are covered as special cases. From this model, we derive an\nexplicit form of the characteristic function for the asset price (SPX) and the\npricing formula for European options as well as VIX options. The empirical\nresults indicate great competence of the proposed model in the problem of joint\ncalibration of the SPX/VIX Markets.\n"
    },
    {
        "paper_id": 2404.16449,
        "authors": "Beier Liu and Haiyun Zhu",
        "title": "Analysis of market efficiency in main stock markets: using Karman-Filter\n  as an approach",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  In this study, we utilize the Kalman-Filter analysis to assess market\nefficiency in major stock markets. The Kalman-Filter operates in two stages,\nassuming that the data contains a consistent trendline representing the true\nmarket value prior to being affected by noise. Unlike traditional methods, it\ncan forecast stock price movements effectively. Our findings reveal significant\nportfolio returns in emerging markets such as Korea, Vietnam, and Malaysia, as\nwell as positive returns in developed markets like the UK, Europe, Japan, and\nHong Kong. This suggests that the Kalman-Filter-based price reversal indicator\nyields promising results across various market types.\n"
    },
    {
        "paper_id": 2404.16467,
        "authors": "Cecilia Aubrun, Rudy Morel, Michael Benzaquen, and Jean-Philippe\n  Bouchaud",
        "title": "Riding Wavelets: A Method to Discover New Classes of Price Jumps",
        "comments": "12 pages and 11 pages of appendices, 26 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Cascades of events and extreme occurrences have garnered significant\nattention across diverse domains such as financial markets, seismology, and\nsocial physics. Such events can stem either from the internal dynamics inherent\nto the system (endogenous), or from external shocks (exogenous). The\npossibility of separating these two classes of events has critical implications\nfor professionals in those fields. We introduce an unsupervised framework\nleveraging a representation of jump time-series based on wavelet coefficients\nand apply it to stock price jumps. In line with previous work, we recover the\nfact that the time-asymmetry of volatility is a major feature. Mean-reversion\nand trend are found to be two additional key features, allowing us to identify\nnew classes of jumps. Furthermore, thanks to our wavelet-based representation,\nwe investigate the reflexive properties of co-jumps, which occur when multiple\nstocks experience price jumps within the same minute. We argue that a\nsignificant fraction of co-jumps results from an endogenous contagion\nmechanism.\n"
    },
    {
        "paper_id": 2404.16777,
        "authors": "Cristiano Arbex Valle and John E Beasley",
        "title": "Subset SSD for enhanced indexation with sector constraints",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  In this paper we apply second order stochastic dominance (SSD) to the problem\nof enhanced indexation with asset subset (sector) constraints. The problem we\nconsider is how to construct a portfolio that is designed to outperform a given\nmarket index whilst having regard to the proportion of the portfolio invested\nin distinct market sectors. In our approach, subset SSD, the portfolio\nassociated with each sector is treated in a SSD manner. In other words in\nsubset SSD we actively try to find sector portfolios that SSD dominate their\nrespective sector indices. However the proportion of the overall portfolio\ninvested in each sector is not pre-specified, rather it is decided via\noptimisation. Computational results are given for our approach as applied to\nthe S\\&P~500 over the period $29^{\\text{th}}$ August 2018 to $29^{\\text{th}}$\nDecember 2023. This period, over 5 years, includes the Covid pandemic, which\nhad a significant effect on stock prices. Our results indicate that the scaled\nversion of our subset SSD approach significantly outperforms the S\\&P~500 over\nthe period considered. Our approach also outperforms the standard SSD based\napproach to the problem.\n"
    },
    {
        "paper_id": 2404.17008,
        "authors": "Arno Botha, Tanja Verster, Roelinde Bester",
        "title": "The TruEnd-procedure: Treating trailing zero-valued balances in credit\n  data",
        "comments": "21 pages, 7255 words, 10 Figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  A novel procedure is presented for finding the true but latent endpoints\nwithin the repayment histories of individual loans. The monthly observations\nbeyond these true endpoints are false, largely due to operational failures that\ndelay account closure, thereby corrupting some loans in the dataset with\n`false' observations. Detecting these false observations is difficult at scale\nsince each affected loan history might have a different sequence of zero (or\nvery small) month-end balances that persist towards the end. Identifying these\ntrails of diminutive balances would require an exact definition of a \"small\nbalance\", which can be found using our so-called TruEnd-procedure. We\ndemonstrate this procedure and isolate the ideal small-balance definition using\nresidential mortgages from a large South African bank. Evidently, corrupted\nloans are remarkably prevalent and have excess histories that are surprisingly\nlong, which ruin the timing of certain risk events and compromise any\nsubsequent time-to-event model such as survival analysis. Excess histories can\nbe discarded using the ideal small-balance definition, which demonstrably\nimproves the accuracy of both the predicted timing and severity of risk events,\nwithout materially impacting the monetary value of the portfolio. The resulting\nestimates of credit losses are lower and less biased, which augurs well for\nraising accurate credit impairments under the IFRS 9 accounting standard. Our\nwork therefore addresses a pernicious data error, which highlights the pivotal\nrole of data preparation in producing credible forecasts of credit risk.\n"
    },
    {
        "paper_id": 2404.17227,
        "authors": "Xintong Wu, Wanling Deng, Yuotng Quan, Luyao Zhang",
        "title": "Trust Dynamics and Market Behavior in Cryptocurrency: A Comparative\n  Study of Centralized and Decentralized Exchanges",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In the evolving landscape of digital finance, the transition from centralized\nto decentralized trust mechanisms, primarily driven by blockchain technology,\nplays a critical role in shaping the cryptocurrency ecosystem. This paradigm\nshift raises questions about the traditional reliance on centralized trust and\nintroduces a novel, decentralized trust framework built upon distributed\nnetworks. Our research delves into the consequences of this shift, particularly\nfocusing on how incidents influence trust within cryptocurrency markets,\nthereby affecting trade behaviors in centralized (CEXs) and decentralized\nexchanges (DEXs). We conduct a comprehensive analysis of various events,\nassessing their effects on market dynamics, including token valuation and\ntrading volumes in both CEXs and DEXs. Our findings highlight the pivotal role\nof trust in directing user preferences and the fluidity of trust transfer\nbetween centralized and decentralized platforms. Despite certain anomalies, the\nresults largely align with our initial hypotheses, revealing the intricate\nnature of user trust in cryptocurrency markets. This study contributes\nsignificantly to interdisciplinary research, bridging distributed systems,\nbehavioral finance, and Decentralized Finance (DeFi). It offers valuable\ninsights for the distributed computing community, particularly in understanding\nand applying distributed trust mechanisms in digital economies, paving the way\nfor future research that could further explore the socio-economic dimensions\nand leverage blockchain data in this dynamic domain.\n"
    },
    {
        "paper_id": 2404.17369,
        "authors": "Steven Reece, Emma O'Donnell, Felicia Liu, Joanna Wolstenholme, Frida\n  Arriaga, Giacomo Ascenzi and Richard Pywell",
        "title": "Assessing the Potential of AI for Spatially Sensitive Nature-Related\n  Financial Risks",
        "comments": "67 pages, 10 figures, UKRI (NERC) Integrated Finance and Biodiversity\n  for a Nature Positive Future Programme",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  There is growing recognition among financial institutions, financial\nregulators and policy makers of the importance of addressing nature-related\nrisks and opportunities. Evaluating and assessing nature-related risks for\nfinancial institutions is challenging due to the large volume of heterogeneous\ndata available on nature and the complexity of investment value chains and the\nvarious components' relationship to nature. The dual problem of scaling data\nanalytics and analysing complex systems can be addressed using Artificial\nIntelligence (AI). We address issues such as plugging existing data gaps with\ndiscovered data, data estimation under uncertainty, time series analysis and\n(near) real-time updates. This report presents potential AI solutions for\nmodels of two distinct use cases, the Brazil Beef Supply Use Case and the Water\nUtility Use Case. Our two use cases cover a broad perspective within\nsustainable finance. The Brazilian cattle farming use case is an example of\ngreening finance - integrating nature-related considerations into mainstream\nfinancial decision-making to transition investments away from sectors with poor\nhistorical track records and unsustainable operations. The deployment of\nnature-based solutions in the UK water utility use case is an example of\nfinancing green - driving investment to nature-positive outcomes. The two use\ncases also cover different sectors, geographies, financial assets and AI\nmodelling techniques, providing an overview on how AI could be applied to\ndifferent challenges relating to nature's integration into finance. This report\nis primarily aimed at financial institutions but is also of interest to ESG\ndata providers, TNFD, systems modellers, and, of course, AI practitioners.\n"
    },
    {
        "paper_id": 2404.17412,
        "authors": "Tianbao Zhou, Zhixin Liu, Yingying Xu",
        "title": "Characterizing Public Debt Cycles: Don't Ignore the Impact of Financial\n  Cycles",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Based on the quarterly data from 26 advanced economies (AEs) and 18 emerging\nmarket economies (EMs) over the past two decades, this paper estimates the\nshort- and medium-term impacts of financial cycles on the duration and\namplitude of public debt cycles. The results indicate that public debt\nexpansions are larger than their contractions in duration and amplitude,\naligning with the \"deficit bias hypothesis\" and being more pronounced in EMs\nthan in AEs. The impacts of various financial cycles are different.\nSpecifically, credit cycles in EMs significantly impact the duration and\namplitude of public debt cycles. Notably, short- and medium-term credit booms\nin EMs shorten the duration of public debt contractions and reduce the\namplitude. Fast credit growth in AEs prolongs the duration of public debt\nexpansions and increases the amplitude. However, credit cycles in AEs show no\nsignificant impact. For house price cycles, the overall impact is stronger in\nEMs than in AEs, differing between short- and medium-term cycles. Finally, the\nimpact of equity price cycles is significant in the short term, but not in the\nmedium term. Equity price busts are more likely to prolong the expansion of\npublic debt in EMs while increasing the amplitude of public debt contractions\nin AEs. Uncovering the impacts of multiple financial cycles on public debt\ncycles provides implications for better debt policies under different financial\nconditions.\n"
    },
    {
        "paper_id": 2404.17497,
        "authors": "Esther Gal-Or, Muhammad Zia Hydari, Rahul Telang",
        "title": "Merchants of Vulnerabilities: How Bug Bounty Programs Benefit Software\n  Vendors",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Software vulnerabilities enable exploitation by malicious hackers,\ncompromising systems and data security. This paper examines bug bounty programs\n(BBPs) that incentivize ethical hackers to discover and responsibly disclose\nvulnerabilities to software vendors. Using game-theoretic models, we capture\nthe strategic interactions between software vendors, ethical hackers, and\nmalicious hackers. First, our analysis shows that software vendors can increase\nexpected profits by participating in BBPs, explaining their growing adoption\nand the success of BBP platforms. Second, we find that vendors with BBPs will\nrelease software earlier, albeit with more potential vulnerabilities, as BBPs\nenable coordinated vulnerability disclosure and mitigation. Third, the optimal\nnumber of ethical hackers to invite to a BBP depends solely on the expected\nnumber of malicious hackers seeking exploitation. This optimal number of\nethical hackers is lower than but increases with the expected malicious hacker\ncount. Finally, higher bounties incentivize ethical hackers to exert more\neffort, thereby increasing the probability that they will discover severe\nvulnerabilities first while reducing the success probability of malicious\nhackers. These findings highlight BBPs' potential benefits for vendors beyond\nprofitability. Earlier software releases are enabled by managing risks through\ncoordinated disclosure. As cybersecurity threats evolve, BBP adoption will\nlikely gain momentum, providing vendors with a valuable tool for enhancing\nsecurity posture and stakeholder trust. Moreover, BBPs envelop vulnerability\nidentification and disclosure into new market relationships and transactions,\nimpacting software vendors' incentives regarding product security choices like\nrelease timing.\n"
    },
    {
        "paper_id": 2404.17551,
        "authors": "Salome O. Ighomereho, Ifeoma E. Ezeabasili",
        "title": "The Role of Marketing in Public Policy Decision Making: The Case of Fuel\n  Subsidy Removal in Nigeria",
        "comments": "9 Pages",
        "journal-ref": "IOSR Journal of Humanities and Social Science, 6(5): pp 7-15\n  (2013)",
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Public policy decision making has become more complex and complicated in\nrecent times. Some authors have attributed this to the fact that public policy\ndecision makers now have more variables to consider in every decision more than\never before. Others have argued that the rate of civilization, globalization\nand information technology has made the public to be more enlightened and\nabreast with the activities of government and so can oppose government\ndecisions if they are unfavourable. This tends to increase government need for\nmore and better information in order to satisfy the public. Consequently, this\npaper examined the issue of fuel subsidy removal in Nigeria, the impact of the\npolicy on the public as well as the country and the role marketing principles\nwould have played if the Nigerian government had taken some time to investigate\nwhat should be done, how it should be done and when it should be done. It also\nproposed a roadmap for future policies that have direct implications for the\ngeneral public.\n"
    },
    {
        "paper_id": 2404.177,
        "authors": "Juan Melo",
        "title": "Decentralized Finance and Local Public Goods: A Bayesian Maximum Entropy\n  Model of School District Spending in the U.S",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper investigates the distribution of public school expenditures across\nU.S. school districts using a bayesian maximum entropy model. Covering the\nperiod 2000-2016, I explore how inter-jurisdictional competition and household\nchoice influence spending patterns within the public education sector,\nproviding a novel empirical treatment of the Tiebout hypothesis within a\nstatistical equilibrium framework. The analysis reveals that these expenditures\nare characterized by sharply peaked and positively skewed distributions,\nsuggesting significant socioeconomic stratification. Employing Bayesian\ninference and Markov Chain Monte Carlo (MCMC) sampling, I fit these patterns\ninto a statistical equilibrium model to elucidate the roles of competition, as\nwell as household mobility and arbitrage in shaping the distribution of\neducational spending. The analysis reveals how the scale parameters associated\nwith competition and household choice critically shape the equilibrium\noutcomes. The model and analysis offer a statistical basis for shaping policy\nmeasures intended to affect distributional outcomes in scenarios characterized\nby the decentralized provision of local public goods.\n"
    },
    {
        "paper_id": 2404.17713,
        "authors": "Weihong Qi",
        "title": "Revisiting the Resource Curse in the Age of Energy Transition: Cobalt\n  Reserves and Conflict in Africa",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This study reevaluates the traditional understanding of the \"political\nresource curse\" by examining the unique impact of energy transition metals,\nspecifically cobalt, on local-level conflicts in Africa. Contrary to previous\nstudies that primarily focus on high-value minerals and their political\noutcomes resulted from substantial economic revenues, this study investigates\ncobalt's influence on local conflict. Despite its strategic importance,\ncobalt's limited commercial value presents a unique yet critical case for\nanalysis. Different with the prevailing view that links mineral reserves with\nincreased conflict, this research finds that regions rich in cobalt experience\na reduction in conflict. This decrease is attributed to enhanced government\nsecurity measures, which are implemented independently of the economic benefits\nderived from cobalt as a commodity. The study utilizes a combination of\ngeoreferenced data and a difference-in-difference design to analyze the causal\nrelationship between cobalt deposits and regional conflict. The findings\nsuggest that the presence of cobalt deposits leads to enhanced security\ninterventions by governments, effectively reducing the likelihood of\nnon-governmental actors taking control of these territories. This pattern\noffers a new perspective on the role of energy transition metals in shaping\nconflict and governance, highlighting the need to reassess theoretical\nframeworks related to the political implications of natural resources with the\nongoing energy revolution.\n"
    },
    {
        "paper_id": 2404.17915,
        "authors": "Kolos Csaba \\'Agoston and Veronika Varga",
        "title": "Bertrand oligopoly in insurance markets with Value at Risk Constraints",
        "comments": "47 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  Since 2016 the operation of insurance companies in the European Union is\nregulated by the Solvency II directive. According to the EU directive the\ncapital requirement should be calculated as a 99.5\\% of Value at Risk. In this\nstudy, we examine the impact of this capital requirement constraint on\nequilibrium premiums and capitals. We discuss the case of the oligopoly\ninsurance market using Bertrand's model, assuming profit maximizing insurance\ncompanies facing Value at Risk constraints. First we analyze companies'\ndecision on premium level. The companies strategic behavior can result positive\nas well as negative expected profit for companies. The desired situation where\ncompetition eliminate positive profit and lead the market to zero-profit state\nis rare. Later we examine ex post and ax ante capital adjustments. Capital\nadjustment does not rule out market anomalies, although somehow changes them.\nPossibility of capital adjustment can lead the market to a situation where all\nof the companies suffer loss. Allowing capital adjustment results monopolistic\npremium level or market failure with positive probabilities.\n"
    },
    {
        "paper_id": 2404.18009,
        "authors": "Hanqiao Zhang",
        "title": "Exit Spillovers of Foreign-invested Enterprises in Shenzhen's\n  Electronics Manufacturing Industry",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Neighborhood characteristics have been broadly studied with different firm\nbehaviors, e.g. birth, entry, expansion, and survival, except for firm exit.\nUsing a novel dataset of foreign-invested enterprises operating in Shenzhen's\nelectronics manufacturing industry from 2017 to 2021, I investigate the\nspillover effects of firm exits on other firms in the vicinity, from both the\nindustry group and the industry class level. Significant neighborhood effects\nare identified for the industry group level, but not the industry class level.\n"
    },
    {
        "paper_id": 2404.18017,
        "authors": "Prabhu Prasad Panda, Maysam Khodayari Gharanchaei, Xilin Chen, Haoshu\n  Lyu",
        "title": "Application of Deep Learning for Factor Timing in Asset Management",
        "comments": null,
        "journal-ref": "Journal of Strategic and International Studies, June 2024",
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The paper examines the performance of regression models (OLS linear\nregression, Ridge regression, Random Forest, and Fully-connected Neural\nNetwork) on the prediction of CMA (Conservative Minus Aggressive) factor\npremium and the performance of factor timing investment with them.\nOut-of-sample R-squared shows that more flexible models have better performance\nin explaining the variance in factor premium of the unseen period, and the back\ntesting affirms that the factor timing based on more flexible models tends to\nover perform the ones with linear models. However, for flexible models like\nneural networks, the optimal weights based on their prediction tend to be\nunstable, which can lead to high transaction costs and market impacts. We\nverify that tilting down the rebalance frequency according to the historical\noptimal rebalancing scheme can help reduce the transaction costs.\n"
    },
    {
        "paper_id": 2404.18029,
        "authors": "Bingzhen Geng, Yang Liu, Yimiao Zhao",
        "title": "Value-at-Risk- and Expectile-based Systemic Risk Measures and\n  Second-order Asymptotics: With Applications to Diversification",
        "comments": "Keywords: Asymptotic approximation; Systemic risk; Expectile;\n  Sarmanov distribution; Second-order regular variation; Diversification\n  benefit",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The systemic risk measure plays a crucial role in analyzing individual losses\nconditioned on extreme system-wide disasters. In this paper, we provide a\nunified asymptotic treatment for systemic risk measures. First, we classify\nthem into two families of Value-at-Risk- (VaR-) and expectile-based systemic\nrisk measures. While VaR has been extensively studied, in the latter family, we\npropose two new systemic risk measures named the Individual Conditional\nExpectile (ICE) and the Systemic Individual Conditional Expectile (SICE), as\nalternatives to Marginal Expected Shortfall (MES) and Systemic Expected\nShortfall (SES). Second, to characterize general mutually dependent and\nheavy-tailed risks, we adopt a modeling framework where the system, represented\nby a vector of random loss variables, follows a multivariate Sarmanov\ndistribution with a common marginal exhibiting second-order regular variation.\nThird, we provide second-order asymptotic results for both families of systemic\nrisk measures. This analytical framework offers a more accurate estimate\ncompared to traditional first-order asymptotics. Through numerical and\nanalytical examples, we demonstrate the superiority of second-order asymptotics\nin accurately assessing systemic risk. Further, we conduct a comprehensive\ncomparison between VaR-based and expectile-based systemic risk measures.\nExpectile-based measures output higher risk evaluation than VaR-based ones,\nemphasizing the former's potential advantages in reporting extreme events and\ntail risk. As a financial application, we use the asymptotic treatment to\ndiscuss the diversification benefits associated with systemic risk measures.\nThe expectile-based diversification benefits consistently deduce an\nunderestimation and suggest a conservative approximation, while the VaR-based\ndiversification benefits consistently deduce an overestimation and suggest\nbehaving optimistically.\n"
    },
    {
        "paper_id": 2404.18148,
        "authors": "Andreas Finke and Thomas Hensel",
        "title": "Decentralized Peer Review in Open Science: A Mechanism Proposal",
        "comments": "14 pages, 1 figure",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  Peer review is a laborious, yet essential, part of academic publishing with\ncrucial impact on the scientific endeavor. The current lack of incentives and\ntransparency harms the credibility of this process. Researchers are neither\nrewarded for superior nor penalized for bad reviews. Additionally, confidential\nreports cause a loss of insights and make the review process vulnerable to\nscientific misconduct. We propose a community-owned and -governed system that\n1) remunerates reviewers for their efforts, 2) publishes the (anonymized)\nreports for scrutiny by the community, 3) tracks reputation of reviewers and 4)\nprovides digital certificates. Automated by transparent smart-contract\nblockchain technology, the system aims to increase quality and speed of peer\nreview while lowering the chance and impact of erroneous judgements.\n"
    },
    {
        "paper_id": 2404.18183,
        "authors": "Shuochen Bi, Wenqing Bao",
        "title": "Innovative Application of Artificial Intelligence Technology in Bank\n  Credit Risk Management",
        "comments": "6 pages, 1 figure, 2 tables",
        "journal-ref": "International Journal of Global Economics and Management ISSN:\n  3005-9690 (Print), ISSN: 3005-8090 (Online) | Volume 2, Number 3, Year 2024",
        "doi": "10.62051/IJGEM.v2n3.08",
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  With the rapid growth of technology, especially the widespread application of\nartificial intelligence (AI) technology, the risk management level of\ncommercial banks is constantly reaching new heights. In the current wave of\ndigitalization, AI has become a key driving force for the strategic\ntransformation of financial institutions, especially the banking industry. For\ncommercial banks, the stability and safety of asset quality are crucial, which\ndirectly relates to the long-term stable growth of the bank. Among them, credit\nrisk management is particularly core because it involves the flow of a large\namount of funds and the accuracy of credit decisions. Therefore, establishing a\nscientific and effective credit risk decision-making mechanism is of great\nstrategic significance for commercial banks. In this context, the innovative\napplication of AI technology has brought revolutionary changes to bank credit\nrisk management. Through deep learning and big data analysis, AI can accurately\nevaluate the credit status of borrowers, timely identify potential risks, and\nprovide banks with more accurate and comprehensive credit decision support. At\nthe same time, AI can also achieve realtime monitoring and early warning,\nhelping banks intervene before risks occur and reduce losses.\n"
    },
    {
        "paper_id": 2404.18184,
        "authors": "Shuochen Bi, Wenqing Bao, Jue Xiao, Jiangshan Wang, Tingting Deng",
        "title": "Application and practice of AI technology in quantitative investment",
        "comments": "9 pages,2 figures",
        "journal-ref": "Information Systems and Economics (2024) Clausius Scientific\n  Press, Canada , ISSN 2523-6407 Vol. 5 Num. 2",
        "doi": "10.23977/infse.2024.050217",
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  With the continuous development of artificial intelligence technology, using\nmachine learning technology to predict market trends may no longer be out of\nreach. In recent years, artificial intelligence has become a research hotspot\nin the academic circle,and it has been widely used in image recognition,\nnatural language processing and other fields, and also has a huge impact on the\nfield of quantitative investment. As an investment method to obtain stable\nreturns through data analysis, model construction and program trading,\nquantitative investment is deeply loved by financial institutions and\ninvestors. At the same time, as an important application field of quantitative\ninvestment, the quantitative investment strategy based on artificial\nintelligence technology arises at the historic moment.How to apply artificial\nintelligence to quantitative investment, so as to better achieve profit and\nrisk control, has also become the focus and difficulty of the research. From a\nglobal perspective, inflation in the US and the Federal Reserve are the\nconcerns of investors, which to some extent affects the direction of global\nassets, including the Chinese stock market. This paper studies the application\nof AI technology, quantitative investment, and AI technology in quantitative\ninvestment, aiming to provide investors with auxiliary decision-making, reduce\nthe difficulty of investment analysis, and help them to obtain higher returns.\n"
    },
    {
        "paper_id": 2404.182,
        "authors": "Xue Cheng, Meng Wang, Ziyi Xu",
        "title": "Mean Field Game of High-Frequency Anticipatory Trading",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The interactions between a large population of high-frequency traders (HFTs)\nand a large trader (LT) who executes a certain amount of assets at discrete\ntime points are studied. HFTs are faster in the sense that they trade\ncontinuously and predict the transactions of LT. A jump process is applied to\nmodel the transition of HFTs' attitudes towards inventories and the equilibrium\nis solved through the mean field game approach. When the crowd of HFTs is\naverse to running (ending) inventories, they first take then supply liquidity\nat each transaction of LT (throughout the whole execution period).\nInventory-averse HFTs lower LT's costs if the market temporary impact is\nrelatively large to the permanent one. What's more, the repeated liquidity\nconsuming-supplying behavior of HFTs makes LT's optimal strategy close to\nuniform trading.\n"
    },
    {
        "paper_id": 2404.18445,
        "authors": "Christian Peukert, Florian Abeillon, J\\'er\\'emie Haese, Franziska\n  Kaiser, Alexander Staub",
        "title": "Strategic Behavior and AI Training Data",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Human-created works represent critical data inputs to artificial intelligence\n(AI). Strategic behavior can play a major role for AI training datasets, be it\nin limiting access to existing works or in deciding which types of new works to\ncreate or whether to create new works at all. We examine creators' behavioral\nchange when their works become training data for AI. Specifically, we focus on\ncontributors on Unsplash, a popular stock image platform with about 6 million\nhigh-quality photos and illustrations. In the summer of 2020, Unsplash launched\nan AI research program by releasing a dataset of 25,000 images for commercial\nuse. We study contributors' reactions, comparing contributors whose works were\nincluded in this dataset to contributors whose works were not included. Our\nresults suggest that treated contributors left the platform at a\nhigher-than-usual rate and substantially slowed down the rate of new uploads.\nProfessional and more successful photographers react stronger than amateurs and\nless successful photographers. We also show that affected users changed the\nvariety and novelty of contributions to the platform, with long-run\nimplications for the stock of works potentially available for AI training.\nTaken together, our findings highlight the trade-off between interests of\nrightsholders and promoting innovation at the technological frontier. We\ndiscuss implications for copyright and AI policy.\n"
    },
    {
        "paper_id": 2404.18467,
        "authors": "Yuyu Chen, Taizhong Hu, Ruodu Wang, and Zhenfeng Zou",
        "title": "Dominance between combinations of infinite-mean Pareto random variables",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We study stochastic dominance between portfolios of independent and\nidentically distributed (iid) extremely heavy-tailed (i.e., infinite-mean)\nPareto random variables. With the notion of majorization order, we show that a\nmore diversified portfolio of iid extremely heavy-tailed Pareto random\nvariables is larger in the sense of first-order stochastic dominance. This\nresult is further generalized for Pareto random variables caused by triggering\nevents, random variables with tails being Pareto, bounded Pareto random\nvariables, and positively dependent Pareto random variables. These results\nprovide an important implication in investment: Diversification of extremely\nheavy-tailed Pareto profits uniformly increases investors' profitability,\nleading to a diversification benefit. Remarkably, different from the\nfinite-mean setting, such a diversification benefit does not depend on the\ndecision maker's risk aversion.\n"
    },
    {
        "paper_id": 2404.1847,
        "authors": "Yupeng Cao, Zhi Chen, Qingyun Pei, Prashant Kumar, K.P. Subbalakshmi,\n  Papa Momar Ndiaye",
        "title": "ECC Analyzer: Extract Trading Signal from Earnings Conference Calls\n  using Large Language Model for Stock Performance Prediction",
        "comments": "15 pages, 3 figures, 5 tables",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In the realm of financial analytics, leveraging unstructured data, such as\nearnings conference calls (ECCs), to forecast stock performance is a critical\nchallenge that has attracted both academics and investors. While previous\nstudies have used deep learning-based models to obtain a general view of ECCs,\nthey often fail to capture detailed, complex information. Our study introduces\na novel framework: \\textbf{ECC Analyzer}, combining Large Language Models\n(LLMs) and multi-modal techniques to extract richer, more predictive insights.\nThe model begins by summarizing the transcript's structure and analyzing the\nspeakers' mode and confidence level by detecting variations in tone and pitch\nfor audio. This analysis helps investors form an overview perception of the\nECCs. Moreover, this model uses the Retrieval-Augmented Generation (RAG) based\nmethods to meticulously extract the focuses that have a significant impact on\nstock performance from an expert's perspective, providing a more targeted\nanalysis. The model goes a step further by enriching these extracted focuses\nwith additional layers of analysis, such as sentiment and audio segment\nfeatures. By integrating these insights, the ECC Analyzer performs multi-task\npredictions of stock performance, including volatility, value-at-risk (VaR),\nand return for different intervals. The results show that our model outperforms\ntraditional analytic benchmarks, confirming the effectiveness of using advanced\nLLM techniques in financial analytics.\n"
    },
    {
        "paper_id": 2404.18499,
        "authors": "W. Benedikt Schmal",
        "title": "Quantitative Tools for Time Series Analysis in Natural Language\n  Processing: A Practitioners Guide",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  Natural language processing tools have become frequently used in social\nsciences such as economics, political science, and sociology. Many publications\napply topic modeling to elicit latent topics in text corpora and their\ndevelopment over time. Here, most publications rely on visual inspections and\ndraw inference on changes, structural breaks, and developments over time. We\nsuggest using univariate time series econometrics to introduce more\nquantitative rigor that can strengthen the analyses. In particular, we discuss\nthe econometric topics of non-stationarity as well as structural breaks. This\npaper serves as a comprehensive practitioners guide to provide researchers in\nthe social and life sciences as well as the humanities with concise advice on\nhow to implement econometric time series methods to thoroughly investigate\ntopic prevalences over time. We provide coding advice for the statistical\nsoftware R throughout the paper. The application of the discussed tools to a\nsample dataset completes the analysis.\n"
    },
    {
        "paper_id": 2404.18709,
        "authors": "Bernardo J. Zubillaga, Mateus F. B. Granha, Andr\\'e L. M. Vilela, Chao\n  Wang, Kenric P. Nelson, H. Eugene Stanley",
        "title": "Three-state Opinion Dynamics for Financial Markets on Complex Networks",
        "comments": "15 pages, 14 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This work investigates the effects of complex networks on the collective\nbehavior of a three-state opinion formation model in economic systems. Our\nmodel considers two distinct types of investors in financial markets: noise\ntraders and fundamentalists. Financial states evolve via probabilistic dynamics\nthat include economic strategies with local and global influences. The local\nmajoritarian opinion drives noise traders' market behavior, while the market\nindex influences the financial decisions of fundamentalist agents. We introduce\na level of market anxiety $q$ present in the decision-making process that\ninfluences financial action. In our investigation, nodes of a complex network\nrepresent market agents, whereas the links represent their financial\ninteractions. We investigate the stochastic dynamics of the model on three\ndistinct network topologies, including scale-free networks, small-world\nnetworks and Erd{\\\"o}s-R\\'enyi random graphs. Our model mirrors various traits\nobserved in real-world financial return series, such as heavy-tailed return\ndistributions, volatility clustering, and short-term memory correlation of\nreturns. The histograms of returns are fitted by coupled Gaussian\ndistributions, quantitatively revealing transitions from a leptokurtic to a\nmesokurtic regime under specific economic heterogeneity. We show that the\nmarket dynamics depend mainly on the average agent connectivity, anxiety level,\nand market composition rather than on specific features of network topology.\n"
    },
    {
        "paper_id": 2404.18761,
        "authors": "Aur\\'elien Alfonsi and Ahmed Kebaier and J\\'er\\^ome Lelong",
        "title": "A pure dual approach for hedging Bermudan options",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper develops a new dual approach to compute the hedging portfolio of a\nBermudan option and its initial value. It gives a \"purely dual\" algorithm\nfollowing the spirit of Rogers (2010) in the sense that it only relies on the\ndual pricing formula. The key is to rewrite the dual formula as an excess\nreward representation and to combine it with a strict convexification\ntechnique. The hedging strategy is then obtained by using a Monte Carlo method,\nsolving backward a sequence of least square problems. We show convergence\nresults for our algorithm and test it on many different Bermudan options.\nBeyond giving directly the hedging portfolio, the strength of the algorithm is\nto assess both the relevance of including financial instruments in the hedging\nportfolio and the effect of the rebalancing frequency.\n"
    },
    {
        "paper_id": 2404.18822,
        "authors": "Anas Abdelhakmi, Andrew Lim",
        "title": "A Multi-Period Black-Litterman Model",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The Black-Litterman model is a framework for incorporating forward-looking\nexpert views in a portfolio optimization problem. Existing work focuses almost\nexclusively on single-period problems and assumes that the horizon of expert\nforecasts matches that of the investor. We consider a multi-period\ngeneralization where the horizon of expert views may differ from that of a\ndynamically-trading investor. By exploiting an underlying graphical structure\nrelating the asset prices and views, we derive the conditional distribution of\nasset returns when the price process is geometric Brownian motion. We also show\nthat it can be written in terms of a multi-dimensional Brownian bridge. The new\nprice process is an affine factor model with the conditional log-price process\nplaying the role of a vector of factors. We derive an explicit expression for\nthe optimal dynamic investment policy and analyze the hedging demand associated\nwith the new covariate. More generally, the paper shows that Bayesian graphical\nmodels are a natural framework for incorporating complex information structures\nin the Black-Litterman model.\n"
    },
    {
        "paper_id": 2404.18979,
        "authors": "Nils Breitmar, Matthew C. Harding, Hanqiao Zhang",
        "title": "Analysis of Proximity Informed User Behavior in a Global Online Social\n  Network",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Despite the earlier claim of \"Death of Distance\", recent studies revealed\nthat geographical proximity still greatly influences link formation in online\nsocial networks. However, it is unclear how physical distances are intertwined\nwith users' online behaviors in a virtual world. We study the role of spatial\ndependence on a global online social network with a dyadic Logit model. Results\nshow country-specific patterns for distance effect on probabilities to build\nconnections. Effects are stronger when the possibility for two people to meet\nin person exists. Relative to weak ties, dependence on proximity is looser for\nstrong social ties.\n"
    },
    {
        "paper_id": 2404.1898,
        "authors": "Hanqiao Zhang, Joy D. Xiuyao Yang",
        "title": "Networks And Productivity -- A Study In Economic Scholars During\n  COVID-19",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The COVID-19 pandemic has disrupted traditional academic collaboration\npatterns, offering a unique opportunity to analyze the influence of peer\neffects and collaboration dynamics on research productivity. Using a novel\nnetwork dataset, this paper investigates the role of peer effects on the\nproductivity of economists, as measured by their publication count, during both\npre-pandemic and pandemic periods. The results indicate that peer effects were\nsignificant in the pre-pandemic period but not during the pandemic.\nAdditionally, the study sheds light on gender and race differences. These\nfindings enhance our understanding of how research collaboration impacts\nknowledge production and provide insights that may inform policies aimed at\npromoting collaboration and boosting research productivity within the academic\ncommunity.\n"
    },
    {
        "paper_id": 2404.19109,
        "authors": "Claudio Bellei, Muhua Xu, Ross Phillips, Tom Robinson, Mark Weber, Tim\n  Kaler, Charles E. Leiserson, Arvind, Jie Chen",
        "title": "The Shape of Money Laundering: Subgraph Representation Learning on the\n  Blockchain with the Elliptic2 Dataset",
        "comments": "KDD MLF Workshop 2024. Dataset can be accessed at\n  http://elliptic.co/elliptic2. Code can be accessed at\n  https://github.com/MITIBMxGraph/Elliptic2",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Subgraph representation learning is a technique for analyzing local\nstructures (or shapes) within complex networks. Enabled by recent developments\nin scalable Graph Neural Networks (GNNs), this approach encodes relational\ninformation at a subgroup level (multiple connected nodes) rather than at a\nnode level of abstraction. We posit that certain domain applications, such as\nanti-money laundering (AML), are inherently subgraph problems and mainstream\ngraph techniques have been operating at a suboptimal level of abstraction. This\nis due in part to the scarcity of annotated datasets of real-world size and\ncomplexity, as well as the lack of software tools for managing subgraph GNN\nworkflows at scale. To enable work in fundamental algorithms as well as domain\napplications in AML and beyond, we introduce Elliptic2, a large graph dataset\ncontaining 122K labeled subgraphs of Bitcoin clusters within a background graph\nconsisting of 49M node clusters and 196M edge transactions. The dataset\nprovides subgraphs known to be linked to illicit activity for learning the set\nof \"shapes\" that money laundering exhibits in cryptocurrency and accurately\nclassifying new criminal activity. Along with the dataset we share our graph\ntechniques, software tooling, promising early experimental results, and new\ndomain insights already gleaned from this approach. Taken together, we find\nimmediate practical value in this approach and the potential for a new standard\nin anti-money laundering and forensic analytics in cryptocurrencies and other\nfinancial networks.\n"
    },
    {
        "paper_id": 2404.1929,
        "authors": "Svetlana Boyarchenko and Sergei Levendorski\\u{i}",
        "title": "Efficient inverse $Z$-transform and Wiener-Hopf factorization",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We suggest new closely related methods for numerical inversion of\n$Z$-transform and Wiener-Hopf factorization of functions on the unit circle,\nbased on sinh-deformations of the contours of integration, corresponding\nchanges of variables and the simplified trapezoid rule. As applications, we\nconsider evaluation of high moments of probability distributions and\nconstruction of causal filters. Programs in Matlab running on a Mac with\nmoderate characteristics achieves the precision E-14 in several dozen of\nmicroseconds and E-11 in several milliseconds, respectively.\n"
    },
    {
        "paper_id": 2404.19324,
        "authors": "Hulusi Mehmet Tanrikulu, Hakan Pabuccu",
        "title": "The Effect of Data Types' on the Performance of Machine Learning\n  Algorithms for Financial Prediction",
        "comments": "33 Pages, 5 Figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Forecasting cryptocurrencies as a financial issue is crucial as it provides\ninvestors with possible financial benefits. A small improvement in forecasting\nperformance can lead to increased profitability; therefore, obtaining a\nrealistic forecast is very important for investors. Successful forecasting\nprovides traders with effective buy-or-hold strategies, allowing them to make\nmore profits. The most important thing in this process is to produce accurate\nforecasts suitable for real-life applications. Bitcoin, frequently mentioned\nrecently due to its volatility and chaotic behavior, has begun to pay great\nattention and has become an investment tool, especially during and after the\nCOVID-19 pandemic. This study provided a comprehensive methodology, including\nconstructing continuous and trend data using one and seven years periods of\ndata as inputs and applying machine learning (ML) algorithms to forecast\nBitcoin price movement. A binarization procedure was applied using continuous\ndata to construct the trend data representing each input feature trend.\nFollowing the related literature, the input features are determined as\ntechnical indicators, google trends, and the number of tweets. Random forest\n(RF), K-Nearest neighbor (KNN), Extreme Gradient Boosting (XGBoost-XGB),\nSupport vector machine (SVM) Naive Bayes (NB), Artificial Neural Networks\n(ANN), and Long-Short-Term Memory (LSTM) networks were applied on the selected\nfeatures for prediction purposes. This work investigates two main research\nquestions: i. How does the sample size affect the prediction performance of ML\nalgorithms? ii. How does the data type affect the prediction performance of ML\nalgorithms? Accuracy and area under the ROC curve (AUC) values were used to\ncompare the model performance. A t-test was performed to test the statistical\nsignificance of the prediction results.\n"
    },
    {
        "paper_id": 2404.19555,
        "authors": "Sabrina Leo, Andrea Delle Foglie, Luca Barbaro, Edoardo Marangone, Ida\n  Claudia Panetta, and Claudio Di Ciccio",
        "title": "Transforming Credit Guarantee Schemes with Distributed Ledger Technology",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Credit Guarantee Schemes (CGSs) are crucial in mitigating SMEs' financial\nconstraints. However, they are renownedly affected by critical shortcomings,\nsuch as a lack of financial sustainability and operational efficiency.\nDistributed Ledger Technologies (DLTs) have shown significant revolutionary\ninfluence in several sectors, including finance and banking, thanks to the full\noperational traceability they bring alongside verifiable computation.\nNevertheless, the potential synergy between DLTs and CGSs has not been\nthoroughly investigated yet. This paper proposes a comprehensive framework to\nutilise DLTs, particularly blockchain technologies, in CGS processes to improve\noperational efficiency and effectiveness. To this end, we compare key\narchitectural characteristics considering access level, governance structure,\nand consensus method, to examine their fit with CGS processes. We believe this\nstudy can guide policymakers and stakeholders, thereby stimulating further\ninnovation in this promising field.\n"
    },
    {
        "paper_id": 2404.1959,
        "authors": "Alexander Moog",
        "title": "Internal migration after a uniform minimum wage introduction",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  Internal migration is an essential aspect to study labor mobility. I exploit\nthe German statutory minimum wage introduction in 2015 to estimate its push and\npull effects on internal migration using a 2% sample of administrative data. In\na conditional fixed effects Poisson difference-in-differences framework with a\ncontinuous treatment, I find that the minimum wage introduction leads to an\nincrease in the out-migration of low-skilled workers with migrant background by\n25% with an increasing tendency over time from districts where a high share of\nworkers are subject to the minimum wage (high-bite districts). In contrast the\nmigration decision of native-born low-skilled workers is not affected by the\npolicy. However, both native-born low-skilled workers and those with a migrant\nbackground do relocate across establishments, leaving high-bite districts as\ntheir workplace. In addition, I find an increase for unemployed individuals\nwith a migrant background in out-migrating from high-bite districts. These\nresults emphasize the importance of considering the effects on geographical\nlabor mobility when implementing and analyzing policies that affect the\ndeterminants of internal migration.\n"
    },
    {
        "paper_id": 2404.19699,
        "authors": "Janik Ole Wecks, Johannes Voshaar, Benedikt Jost Plate, Jochen\n  Zimmermann",
        "title": "Generative AI Usage and Academic Performance",
        "comments": "This version: May 2024",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This study evaluates the impact of students' usage of generative artificial\nintelligence (GenAI) tools such as ChatGPT on their academic performance. We\nanalyze student essays using GenAI detection systems to identify GenAI users\namong the cohort. Employing multivariate regression analysis, we find that\nstudents using GenAI tools score on average 6.71 (out of 100) points lower than\nnon-users. While GenAI tools may offer benefits for learning and engagement,\nthe way students actually use it correlates with diminished academic outcomes.\nExploring the underlying mechanism, additional analyses show that the effect is\nparticularly detrimental to students with high learning potential, suggesting\nan effect whereby GenAI tool usage hinders learning. Our findings provide\nimportant empirical evidence for the ongoing debate on the integration of GenAI\nin higher education and underscores the necessity for educators, institutions,\nand policymakers to carefully consider its implications for student\nperformance.\n"
    },
    {
        "paper_id": 2405.00046,
        "authors": "Ghassan Dibeh, Omar El Deeb",
        "title": "Synchronization in a market model with time delays",
        "comments": "12 pages, 4 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  We examine a system of N=2 coupled non-linear delay-differential equations\nrepresenting financial market dynamics. In such time delay systems, coupled\noscillations have been derived. We linearize the system for small time delays\nand study its collective dynamics. Using analytical and numerical solutions, we\nobtain the bifurcation diagrams and analyze the corresponding regions of\namplitude death, phase locking, limit cycles and market synchronization in\nterms of the system frequency-like parameters and time delays. We further\nnumerically explore higher order systems with N>2, and demonstrate that limit\ncycles can be maintained for coupled N-asset models with appropriate\nparameterization.\n"
    },
    {
        "paper_id": 2405.00047,
        "authors": "Maksym Lazirko",
        "title": "The Quantum Dynamics of Cost Accounting: Investigating WIP via the\n  Time-Independent Schrodinger Equation",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-sa/4.0/",
        "abstract": "  The intersection of quantum theory and accounting presents a novel and\nintriguing frontier in exploring financial valuation and accounting practices.\nThis paper applies quantum theory to cost accounting, specifically Work in\nProgress (WIP) valuation. WIP is conceptualized as materials in a quantum\nsuperposition state whose financial value remains uncertain until observed or\nmeasured. This work comprehensively reviews the seminal works that explored the\noverlap between quantum theory and accounting. The primary contribution of this\nwork is a more nuanced understanding of the uncertainties involved, which\nemerges by applying quantum phenomena to model the complexities and\nuncertainties inherent in managerial accounting. In contrast, previous works\nfocus more on financial accounting or general accountancy.\n"
    },
    {
        "paper_id": 2405.00051,
        "authors": "Abhijit Chakraborty and Yuichi Ikeda",
        "title": "Arbitrage impact on the relationship between XRP price and correlation\n  tensor spectra of transaction networks",
        "comments": "12 pages, 8 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  The increasing use of cryptoassets for international remittances has proven\nto be faster and more cost-effective, particularly for migrants without access\nto traditional banking. However, the inherent volatility of cryptoasset prices,\nindependent of blockchain-based remittance mechanisms, introduces potential\nrisks during periods of high volatility. This study investigates the intricate\ndynamics between XRP price fluctuations across diverse crypto exchanges and the\ncorrelation of the largest singular values of the correlation tensor of XRP\ntransaction networks. Particularly, we show the impact of arbitrage\nopportunities across different crypto exchanges on the relationship between XRP\nprice and correlation tensor spectra of transaction networks. Distinct periods,\nnon-bubble and bubble, showcase different characteristics in XRP price\nfluctuations. Establishing a connection between XRP price and transaction\nnetworks, we compute correlation tensors and singular values, emphasizing the\nsignificance of the largest singular value. Comparisons with reshuffled and\nGaussian random correlation tensors validate the uniqueness of the empirical\ntensor. A set of simulated weekly XRP prices, resembling arbitrage\nopportunities across various crypto exchanges, further confirms the robustness\nof our findings. It reveals a pronounced anti-correlation during bubble periods\nand a non-significant correlation during non-bubble periods with the largest\nsingular value, irrespective of price fluctuations across different crypto\nexchanges.\n"
    },
    {
        "paper_id": 2405.00234,
        "authors": "Fabio I. Martinenghi, Xian Zhang, Luk Rombauts, Georgina M. Chambers",
        "title": "Conceiving Naturally After IVF: the effect of assisted reproduction on\n  obstetric interventions and child health at birth",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
        "abstract": "  A growing share of the world's population is being born via assisted\nreproductive technology (ART), including in-vitro fertilisation (IVF). However,\ntwo concerns persist. First, ART pregnancies correlate with predictors of poor\noutcomes at birth--and it is unclear whether this relationship is causal.\nSecond, the emotional and financial costs associated with ART-use might\nexacerbate defensive medical behaviour, where physicians intervene more than\nnecessary to reduce the risk of adverse medical outcomes and litigation. We\naddress the challenge of identifying the pure effect of ART-use on both\nmaternal and infant outcomes at birth by leveraging exogenous variation in the\nsuccess of ART cycles. We compare the obstetric outcomes for ART-conceived\nbirths with those of spontaneously-conceived births after a failed ART\ntreatment. Moreover, we flexibly adjust for key confounders using double\nmachine learning. We do this using clinical registry ART data and\nadministrative maternal and infant data from New South Wales (NSW) between\n2009-2017. We find that ART slightly decreases the risk of obstetric\ninterventions, lowering the risk of a caesarean section and increasing the rate\nof spontaneous labour (+3.5 p.p.). Moreover, we find that ART has a\nstatistically and clinically insignificant effect on infant health outcomes.\n  Keywords: Fertility, Assisted reproduction, IVF, Caesarean Section,\nObstetric, Infertility. JEL classification: I10, I12, I19.\n"
    },
    {
        "paper_id": 2405.00235,
        "authors": "Abdoulaye Ndiaye",
        "title": "Blockchain Price vs. Quantity Controls",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/publicdomain/zero/1.0/",
        "abstract": "  This paper studies the optimal transaction fee mechanisms for blockchains,\nfocusing on the distinction between price-based ($\\mathcal{P}$) and\nquantity-based ($\\mathcal{Q}$) controls. By analyzing factors such as demand\nuncertainty, validator costs, cryptocurrency price fluctuations, price\nelasticity of demand, and levels of decentralization, we establish criteria\nthat determine the selection of transaction fee mechanisms. We present a model\nframed around a Nash bargaining game, exploring how blockchain designers and\nvalidators negotiate fee structures to balance network welfare with\nprofitability. Our findings suggest that the choice between $\\mathcal{P}$ and\n$\\mathcal{Q}$ mechanisms depends critically on the blockchain's specific\ntechnical and economic features. The study concludes that no single mechanism\nsuits all contexts and highlights the potential for hybrid approaches that\nadaptively combine features of both $\\mathcal{P}$ and $\\mathcal{Q}$ to meet\nvarying demands and market conditions.\n"
    },
    {
        "paper_id": 2405.00247,
        "authors": "Susan Athey and Emil Palikot",
        "title": "The value of non-traditional credentials in the labor market",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This study investigates the labor market value of credentials obtained from\nMassive Open Online Courses (MOOCs) and shared on business networking\nplatforms. We conducted a randomized experiment involving more than 800,000\nlearners, primarily from developing countries and without college degrees, who\ncompleted technology or business-related courses on the Coursera platform\nbetween September 2022 and March 2023. The intervention targeted learners who\nhad recently completed their courses, encouraging them to share their\ncredentials and simplifying the sharing process. One year after the\nintervention, we collected data from LinkedIn profiles of approximately 40,000\nexperimental subjects. We find that the intervention leads to an increase of 17\npercentage points for credential sharing. Further, learners in the treatment\ngroup were 6\\% more likely to report new employment within a year, with an 8\\%\nincrease in jobs related to their certificates. This effect was more pronounced\namong LinkedIn users with lower baseline employability. Across the entire\nsample, the treated group received a higher number of certificate views,\nindicating an increased interest in their profiles. These results suggest that\nfacilitating credential sharing and reminding learners of the value of skill\nsignaling can yield significant gains. When the experiment is viewed as an\nencouragement design for credential sharing, we can estimate the local average\ntreatment effect (LATE) of credential sharing (that is, the impact of\ncredential sharing on the workers induced to share by the intervention) for the\noutcome of getting a job. The LATE estimates are imprecise but large in\nmagnitude; they suggest that credential sharing more than doubles the baseline\nprobability of getting a new job in scope for the credential.\n"
    },
    {
        "paper_id": 2405.00357,
        "authors": "Daniel Bartl and Stephan Eckstein",
        "title": "Optimal nonparametric estimation of the expected shortfall risk",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We address the problem of estimating the expected shortfall risk of a\nfinancial loss using a finite number of i.i.d. data. It is well known that the\nclassical plug-in estimator suffers from poor statistical performance when\nfaced with (heavy-tailed) distributions that are commonly used in financial\ncontexts. Further, it lacks robustness, as the modification of even a single\ndata point can cause a significant distortion. We propose a novel procedure for\nthe estimation of the expected shortfall and prove that it recovers the best\npossible statistical properties (dictated by the central limit theorem) under\nminimal assumptions and for all finite numbers of data. Further, this estimator\nis adversarially robust: even if a (small) proportion of the data is\nmaliciously modified, the procedure continuous to optimally estimate the true\nexpected shortfall risk. We demonstrate that our estimator outperforms the\nclassical plug-in estimator through a variety of numerical experiments across a\nrange of standard loss distributions.\n"
    },
    {
        "paper_id": 2405.00473,
        "authors": "Ayub Ahmadi, Mahdieh Tahmasebi",
        "title": "Pricing and delta computation in jump-diffusion models with stochastic\n  intensity by Malliavin calculus",
        "comments": "5 fingures. 1 table",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  In this paper, the pricing of financial derivatives and the calculation of\ntheir delta Greek are investigated as the underlying asset is a jump-diffusion\nprocess in which the stochastic intensity component follows the CIR process.\nUtilizing Malliavin derivatives for pricing financial derivatives and\nchallenging to find the Malliavin weight for accurately calculating delta will\nbe established in such models. Due to the dependence of asset price on the\ninformation of the intensity process, conditional expectation attribute to show\nboundedness of moments of Malliavin weights and the underlying asset is\nessential. Our approach is validated through numerical experiments,\nhighlighting its effectiveness and potential for risk management and hedging\nstrategies in markets characterized by jump and stochastic intensity dynamics.\n"
    },
    {
        "paper_id": 2405.00522,
        "authors": "Yihang Fu, Mingyu Zhou, Luyao Zhang",
        "title": "DAM: A Universal Dual Attention Mechanism for Multimodal Timeseries\n  Cryptocurrency Trend Forecasting",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In the distributed systems landscape, Blockchain has catalyzed the rise of\ncryptocurrencies, merging enhanced security and decentralization with\nsignificant investment opportunities. Despite their potential, current research\non cryptocurrency trend forecasting often falls short by simplistically merging\nsentiment data without fully considering the nuanced interplay between\nfinancial market dynamics and external sentiment influences. This paper\npresents a novel Dual Attention Mechanism (DAM) for forecasting cryptocurrency\ntrends using multimodal time-series data. Our approach, which integrates\ncritical cryptocurrency metrics with sentiment data from news and social media\nanalyzed through CryptoBERT, addresses the inherent volatility and prediction\nchallenges in cryptocurrency markets. By combining elements of distributed\nsystems, natural language processing, and financial forecasting, our method\noutperforms conventional models like LSTM and Transformer by up to 20\\% in\nprediction accuracy. This advancement deepens the understanding of distributed\nsystems and has practical implications in financial markets, benefiting\nstakeholders in cryptocurrency and blockchain technologies. Moreover, our\nenhanced forecasting approach can significantly support decentralized science\n(DeSci) by facilitating strategic planning and the efficient adoption of\nblockchain technologies, improving operational efficiency and financial risk\nmanagement in the rapidly evolving digital asset domain, thus ensuring optimal\nresource allocation.\n"
    },
    {
        "paper_id": 2405.00537,
        "authors": "Brad Bachu and Xin Wan and Ciamac C. Moallemi",
        "title": "Quantifying Price Improvement in Order Flow Auctions",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This work introduces a framework for evaluating onchain order flow auctions\n(OFAs), emphasizing the metric of price improvement. Utilizing a set of\nopen-source tools, our methodology systematically attributes price improvements\nto specific modifiable inputs of the system such as routing efficiency, gas\noptimization, and priority fee settings. When applied to leading Ethereum-based\ntrading interfaces such as 1Inch and Uniswap, the results reveal that\nauction-enhanced interfaces can provide statistically significant improvements\nin trading outcomes, averaging 4-5 basis points in our sample. We further\nidentify the sources of such price improvements to be added liquidity for large\nswaps. This research lays a foundation for future innovations in blockchain\nbased trading platforms.\n"
    },
    {
        "paper_id": 2405.0054,
        "authors": "Hannah Schuster, Axel Polleres, Amin Anjomshoaa, Johannes Wachs",
        "title": "Heat, Health, and Habitats: Analyzing the Intersecting Risks of Climate\n  and Demographic Shifts in Austrian Districts",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  The impact of hot weather on health outcomes of a population is mediated by a\nvariety of factors, including its age profile and local green infrastructure.\nThe combination of warming due to climate change and demographic aging suggests\nthat heat-related health outcomes will deteriorate in the coming decades. Here,\nwe measure the relationship between weekly all-cause mortality and heat days in\nAustrian districts using a panel dataset covering $2015-2022$. An additional\nday reaching $30$ degrees is associated with a $2.4\\%$ increase in mortality\nper $1000$ inhabitants during summer. This association is roughly doubled in\ndistricts with a two standard deviation above average share of the population\nover $65$. Using forecasts of hot days (RCP) and demographics in $2050$, we\nobserve that districts will have elderly populations and hot days $2-5$\nstandard deviations above the current mean in just $25$ years. This predicts a\ndrastic increase in heat-related mortality. At the same time, district green\nscores, measured using $10\\times 10$ meter resolution satellite images of\nresidential areas, significantly moderate the relationship between heat and\nmortality. Thus, although local policies likely cannot reverse warming or\ndemographic trends, they can take measures to mediate the health consequences\nof these growing risks, which are highly heterogeneous across regions, even in\nAustria.\n"
    },
    {
        "paper_id": 2405.00566,
        "authors": "Huan-Yi Su, Ke Wu, Yu-Hao Huang, Wu-Jun Li",
        "title": "NumLLM: Numeric-Sensitive Large Language Model for Chinese Finance",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Recently, many works have proposed various financial large language models\n(FinLLMs) by pre-training from scratch or fine-tuning open-sourced LLMs on\nfinancial corpora. However, existing FinLLMs exhibit unsatisfactory performance\nin understanding financial text when numeric variables are involved in\nquestions. In this paper, we propose a novel LLM, called numeric-sensitive\nlarge language model (NumLLM), for Chinese finance. We first construct a\nfinancial corpus from financial textbooks which is essential for improving\nnumeric capability of LLMs during fine-tuning. After that, we train two\nindividual low-rank adaptation (LoRA) modules by fine-tuning on our constructed\nfinancial corpus. One module is for adapting general-purpose LLMs to financial\ndomain, and the other module is for enhancing the ability of NumLLM to\nunderstand financial text with numeric variables. Lastly, we merge the two LoRA\nmodules into the foundation model to obtain NumLLM for inference. Experiments\non financial question-answering benchmark show that NumLLM can boost the\nperformance of the foundation model and can achieve the best overall\nperformance compared to all baselines, on both numeric and non-numeric\nquestions.\n"
    },
    {
        "paper_id": 2405.00576,
        "authors": "Jian He, Asma Khedher, and Peter Spreij",
        "title": "Calibration of the rating transition model for high and low default\n  portfolios",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper we develop Maximum likelihood (ML) based algorithms to\ncalibrate the model parameters in credit rating transition models. Since the\ncredit rating transition models are not Gaussian linear models, the celebrated\nKalman filter is not suitable to compute the likelihood of observed migrations.\nTherefore, we develop a Laplace approximation of the likelihood function and as\na result the Kalman filter can be used in the end to compute the likelihood\nfunction. This approach is applied to so-called high-default portfolios, in\nwhich the number of migrations (defaults) is large enough to obtain high\naccuracy of the Laplace approximation. By contrast, low-default portfolios have\na limited number of observed migrations (defaults). Therefore, in order to\ncalibrate low-default portfolios, we develop a ML algorithm using a particle\nfilter (PF) and Gaussian process regression. Experiments show that both\nalgorithms are efficient and produce accurate approximations of the likelihood\nfunction and the ML estimates of the model parameters.\n"
    },
    {
        "paper_id": 2405.00606,
        "authors": "Lars Holden",
        "title": "Some properties of Euler capital allocation",
        "comments": "12 pages, 3 figures, 4 tables, 15 references",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  The paper discusses capital allocation using the Euler formula and focuses on\nthe risk measures Value-at-Risk (VaR) and Expected shortfall (ES). Some new\nresults connected to this capital allocation is known. Two examples illustrate\nthat capital allocation with VaR is not monotonous which may be surprising\nsince VaR is monotonous. A third example illustrates why the same risk measure\nshould be used in capital allocation as in the evaluation of the total\nportfolio. We show how simulation may be used in order to estimate the expected\nReturn on risk adjusted capital in the commitment period of an asset. Finally,\nwe show how Markov chain Monte Carlo may be used in the estimation of the\ncapital allocation.\n"
    },
    {
        "paper_id": 2405.00697,
        "authors": "Xiaowei Chen, Hong Li, Yufan Lu, Rui Zhou",
        "title": "Pricing Catastrophe Bonds -- A Probabilistic Machine Learning Approach",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper proposes a probabilistic machine learning method to price\ncatastrophe (CAT) bonds in the primary market. The proposed method combines\nmachine-learning-based predictive models with Conformal Prediction, an\ninnovative algorithm that generates distribution-free probabilistic forecasts\nfor CAT bond prices. Using primary market CAT bond transaction records between\nJanuary 1999 and March 2021, the proposed method is found to be more robust and\nyields more accurate predictions of the bond spreads than traditional\nregression-based methods. Furthermore, the proposed method generates more\ninformative prediction intervals than linear regression and identifies\nimportant nonlinear relationships between various risk factors and bond\nspreads, suggesting that linear regressions could misestimate the bond spreads.\nOverall, this paper demonstrates the potential of machine learning methods in\nimproving the pricing of CAT bonds.\n"
    },
    {
        "paper_id": 2405.00701,
        "authors": "Rihito Sakurai, Haruto Takahashi and Koichi Miyamoto",
        "title": "Learning parameter dependence for Fourier-based option pricing with\n  tensor trains",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  A long-standing issue in mathematical finance is the speed-up of pricing\noptions, especially multi-asset options. A recent study has proposed to use\ntensor train learning algorithms to speed up Fourier transform (FT)-based\noption pricing, utilizing the ability of tensor networks to compress\nhigh-dimensional tensors. Another usage of the tensor network is to compress\nfunctions, including their parameter dependence. In this study, we propose a\npricing method, where, by a tensor learning algorithm, we build tensor trains\nthat approximate functions appearing in FT-based option pricing with their\nparameter dependence and efficiently calculate the option price for the varying\ninput parameters. As a benchmark test, we run the proposed method to price a\nmulti-asset option for the various values of volatilities and present asset\nprices. We show that, in the tested cases involving up to 11 assets, the\nproposed method is comparable to or outperforms Monte Carlo simulation with\n$10^5$ paths in terms of computational complexity, keeping the comparable\naccuracy.\n"
    },
    {
        "paper_id": 2405.00895,
        "authors": "Sean Lewis-Faupel, Nicholas Tenev",
        "title": "Racial and Ethnic Disparities in Mortgage Lending: New Evidence from\n  Expanded HMDA Data",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper investigates gaps in access to and the cost of housing credit by\nrace and ethnicity using the near universe of U.S. mortgage applications. Our\ndata contain borrower creditworthiness variables that have historically been\nabsent from industry-wide application data and that are likely to affect\napplication approval and loan pricing. We find large unconditional disparities\nin approval and pricing between racial and ethnic groups. After conditioning on\nkey elements of observable borrower creditworthiness, these disparities are\nsmaller but remain economically meaningful. Sensitivity analysis indicates that\nomitted factors as predictive of approval/pricing and race/ethnicity as credit\nscore can explain some of the pricing disparities but cannot explain the\napproval disparities. Taken together, our results suggest that credit score,\nincome, and down payment requirements significantly contribute to disparities\nin mortgage access and affordability but that other systemic barriers are also\nresponsible for a large share of disparate outcomes in the mortgage market.\n"
    },
    {
        "paper_id": 2405.01078,
        "authors": "Yi Jiang, Shohei Shimizu",
        "title": "Does Financial Literacy Impact Investment Participation and Retirement\n  Planning in Japan?",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  By employing causal discovery method, the Fast Causal Inference (FCI) model\nto analyze data from the 2022 \"Financial Literacy Survey,\" we explore the\ncausal relationships between financial literacy and financial activities,\nspecifically investment participation and retirement planning. Our findings\nindicate that increasing financial literacy may not directly boost engagement\nin financial investments or retirement planning in Japan, which underscores the\nnecessity for alternative strategies to motivate financial activities among\nJapanese households. This research offers valuable insights for policymakers\nfocused on improving financial well-being by advancing the use of causal\ndiscovery algorithms in understanding financial behaviors.\n"
    },
    {
        "paper_id": 2405.01137,
        "authors": "Maral Jamalova",
        "title": "Modelling user behavior towards smartphones and wearable technologies: A\n  bibliometric study and brief literature review",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  The study uses bibliometric as well as content analysis to determine the\ncurrent situation regarding the application of technology adoption models\n(i.e., the Technology Acceptance Model, Unified Theory of Acceptance and Use of\nTechnology, and Innovation Diffusion Theory) to the smartphone market that also\nincludes smart wearables. Hereby the author would like to determine the\nconnection between smartphone usage and adoption models and enrich literature\nby defining state-of-the-art tendencies and approaches. To achieve the goal,\nthe author applied a two-stage approach: in the first stage, 213 articles were\nanalyzed using Citation and Bibliographic coupling tools in VOSviewer (1.6.20).\nThe papers were selected from the Scopus database and the search of the papers\nwas conducted in the fields of Economics, Business, and Computer technologies.\nIn the second stage, the author conducted a brief literature review of the most\ninfluential papers. The results illustrate the situation regarding the\nimplementation of different models in the case of smartphone adoption. Content\nanalyses of the most influential papers were applied to explain and enrich the\nresults of bibliometric analyses as well as determine research gaps and future\nresearch development.\n"
    },
    {
        "paper_id": 2405.01233,
        "authors": "Pedro Duarte Gomes",
        "title": "Mathematics of Differential Machine Learning in Derivative Pricing and\n  Hedging",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  This article introduces the groundbreaking concept of the financial\ndifferential machine learning algorithm through a rigorous mathematical\nframework. Diverging from existing literature on financial machine learning,\nthe work highlights the profound implications of theoretical assumptions within\nfinancial models on the construction of machine learning algorithms.\n  This endeavour is particularly timely as the finance landscape witnesses a\nsurge in interest towards data-driven models for the valuation and hedging of\nderivative products. Notably, the predictive capabilities of neural networks\nhave garnered substantial attention in both academic research and practical\nfinancial applications.\n  The approach offers a unified theoretical foundation that facilitates\ncomprehensive comparisons, both at a theoretical level and in experimental\noutcomes. Importantly, this theoretical grounding lends substantial weight to\nthe experimental results, affirming the differential machine learning method's\noptimality within the prevailing context.\n  By anchoring the insights in rigorous mathematics, the article bridges the\ngap between abstract financial concepts and practical algorithmic\nimplementations.\n"
    },
    {
        "paper_id": 2405.01479,
        "authors": "Eric Ghysels and Jack Morgan",
        "title": "On Quantum Ambiguity and Potential Exponential Computational Speed-Ups\n  to Solving Dynamic Asset Pricing Models",
        "comments": "51 pages, 4 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We formulate quantum computing solutions to a large class of dynamic\nnonlinear asset pricing models using algorithms, in theory exponentially more\nefficient than classical ones, which leverage the quantum properties of\nsuperposition and entanglement. The equilibrium asset pricing solution is a\nquantum state. We introduce quantum decision-theoretic foundations of ambiguity\nand model/parameter uncertainty to deal with model selection.\n"
    },
    {
        "paper_id": 2405.01598,
        "authors": "Emily Tallman and Mike West",
        "title": "Predictive Decision Synthesis for Portfolios: Betting on Better Models",
        "comments": "25 pages, 10 figures, 3 tables",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  We discuss and develop Bayesian dynamic modelling and predictive decision\nsynthesis for portfolio analysis. The context involves model uncertainty with a\nset of candidate models for financial time series with main foci in sequential\nlearning, forecasting, and recursive decisions for portfolio reinvestments. The\nfoundational perspective of Bayesian predictive decision synthesis (BPDS)\ndefines novel, operational analysis and resulting predictive and decision\noutcomes. A detailed case study of BPDS in financial forecasting of\ninternational exchange rate time series and portfolio rebalancing, with\nresulting BPDS-based decision outcomes compared to traditional Bayesian\nanalysis, exemplifies and highlights the practical advances achievable under\nthe expanded, subjective Bayesian approach that BPDS defines.\n"
    },
    {
        "paper_id": 2405.01604,
        "authors": "Ashish Anil Pawar, Vishnureddy Prashant Muskawar, and Ritesh Tiku",
        "title": "Portfolio Management using Deep Reinforcement Learning",
        "comments": "7 pages, 9 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Algorithmic trading or Financial robots have been conquering the stock\nmarkets with their ability to fathom complex statistical trading strategies.\nBut with the recent development of deep learning technologies, these strategies\nare becoming impotent. The DQN and A2C models have previously outperformed\neminent humans in game-playing and robotics. In our work, we propose a\nreinforced portfolio manager offering assistance in the allocation of weights\nto assets. The environment proffers the manager the freedom to go long and even\nshort on the assets. The weight allocation advisements are restricted to the\nchoice of portfolio assets and tested empirically to knock benchmark indices.\nThe manager performs financial transactions in a postulated liquid market\nwithout any transaction charges. This work provides the conclusion that the\nproposed portfolio manager with actions centered on weight allocations can\nsurpass the risk-adjusted returns of conventional portfolio managers.\n"
    },
    {
        "paper_id": 2405.01798,
        "authors": "Ayse D. Lokmanoglu, Carol K. Winkler, Kareem El Damanhoury, Virginia\n  Massignan, Esteban Villa-Turek, Keyu Alexander Chen",
        "title": "The Economy and Public Diplomacy: An Analysis of RT's Economic Content\n  and Context on Facebook",
        "comments": "14 pages, 6 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  With globalization's rise, economic interdependence's impacts have become a\nprominent factor affecting personal lives, as well as national and\ninternational dynamics. This study examines RT's public diplomacy efforts on\nits non-Russian Facebook accounts over the past five years to identify the\nprominence of economic topics across language accounts. Computational analysis,\nincluding word embeddings and statistical methods, investigates how offline\neconomic indicators, like currency values and oil prices, correspond to RT's\nonline economic content changes. The results demonstrate that RT uses message\nreinforcement associated economic topics as an audience targeting strategy and\ndifferentiates their use with changing currency and oil values.\n"
    },
    {
        "paper_id": 2405.01881,
        "authors": "Xue Wen Tan, Stanley Kok",
        "title": "Explainable Risk Classification in Financial Reports",
        "comments": "ICIS 2023 Proceedings. 3.\n  https://aisel.aisnet.org/icis2023/blockchain/blockchain/3",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Every publicly traded company in the US is required to file an annual 10-K\nfinancial report, which contains a wealth of information about the company. In\nthis paper, we propose an explainable deep-learning model, called FinBERT-XRC,\nthat takes a 10-K report as input, and automatically assesses the post-event\nreturn volatility risk of its associated company. In contrast to previous\nsystems, our proposed model simultaneously offers explanations of its\nclassification decision at three different levels: the word, sentence, and\ncorpus levels. By doing so, our model provides a comprehensive interpretation\nof its prediction to end users. This is particularly important in financial\ndomains, where the transparency and accountability of algorithmic predictions\nplay a vital role in their application to decision-making processes. Aside from\nits novel interpretability, our model surpasses the state of the art in\npredictive accuracy in experiments on a large real-world dataset of 10-K\nreports spanning six years.\n"
    },
    {
        "paper_id": 2405.01892,
        "authors": "Tian Tian, Ricky Cooper, Jiahao Deng, and Qingquan Zhang",
        "title": "Transforming Investment Strategies and Strategic Decision-Making:\n  Unveiling a Novel Methodology for Enhanced Performance and Risk Management in\n  Financial Markets",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper introduces a novel methodology for index return forecasting,\nblending highly correlated stock prices, advanced deep learning techniques, and\nintricate factor integration. Departing from conventional cap-weighted\napproaches, our innovative framework promises to reimagine traditional\nmethodologies, offering heightened diversification, amplified performance\ncapture, and nuanced market depiction. At its core lies the intricate\nidentification of highly correlated company clusters, fueling predictive\naccuracy and robustness. By harnessing these interconnected constellations, we\nunlock a profound comprehension of market dynamics, bestowing both investment\nentities and individual enterprises with invaluable performance insights.\nMoreover, our methodology integrates pivotal factors such as indexes and ETFs,\nseamlessly woven with Hierarchical Risk Parity (HRP) portfolio optimization, to\nelevate performance and fortify risk management. This comprehensive\namalgamation refines risk diversification, fortifying portfolio resilience\nagainst turbulent market forces. The implications reverberate resoundingly.\nInvestment entities stand poised to calibrate against competitors with surgical\nprecision, tactically sidestepping industry-specific pitfalls, and sculpting\nbespoke investment strategies to capitalize on market fluctuations.\nConcurrently, individual enterprises find empowerment in aligning strategic\nendeavors with market trajectories, discerning key competitors, and navigating\nvolatility with steadfast resilience. In essence, this research marks a pivotal\nmoment in economic discourse, unveiling novel methodologies poised to redefine\ndecision-making paradigms and elevate performance benchmarks for both\ninvestment entities and individual enterprises navigating the intricate\ntapestry of financial realms.\n"
    },
    {
        "paper_id": 2405.02012,
        "authors": "Sullivan Hu\\'e, Christophe Hurlin, Yang Lu",
        "title": "Backtesting Expected Shortfall: Accounting for both duration and\n  severity with bivariate orthogonal polynomials",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  We propose an original two-part, duration-severity approach for backtesting\nExpected Shortfall (ES). While Probability Integral Transform (PIT) based ES\nbacktests have gained popularity, they have yet to allow for separate testing\nof the frequency and severity of Value-at-Risk (VaR) violations. This is a\ncrucial aspect, as ES measures the average loss in the event of such\nviolations. To overcome this limitation, we introduce a backtesting framework\nthat relies on the sequence of inter-violation durations and the sequence of\nseverities in case of violations. By leveraging the theory of (bivariate)\northogonal polynomials, we derive orthogonal moment conditions satisfied by\nthese two sequences. Our approach includes a straightforward, model-free Wald\ntest, which encompasses various unconditional and conditional coverage\nbacktests for both VaR and ES. This test aids in identifying any mis-specified\ncomponents of the internal model used by banks to forecast ES. Moreover, it can\nbe extended to analyze other systemic risk measures such as Marginal Expected\nShortfall. Simulation experiments indicate that our test exhibits good finite\nsample properties for realistic sample sizes. Through application to two stock\nindices, we demonstrate how our methodology provides insights into the reasons\nfor rejections in testing ES validity.\n"
    },
    {
        "paper_id": 2405.02015,
        "authors": "Wolfgang Seiringer, Balwin Bokor, Klaus Altendorfer",
        "title": "Evaluating Production Planning and Control Systems in Different\n  Environments: A Comparative Simulation Study",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Selecting the appropriate production planning and control systems (PPCS)\npresents a significant challenge for many companies, as their performance, i.e.\noverall costs, depends on the production system environment. Key environmental\ncharacteristics include the system's structure, i.e. flow shop, hybrid shop, or\njob shop, and the planned shop load. Besides selecting a suitable PPCS, its\nparameterization significantly influences the performance. This publication\ninvestigates the performance and the optimal parametrization of Material\nRequirement Planning (MRP), Reorder Point System (RPS) and Constant Work In\nProgress (ConWIP) at different stochastic multi-item multi-stage production\nsystem environments by conduction a comprehensive full factorial simulation\nstudy. The results indicate that MRP and ConWIP generally outperform RPS in all\nobserved environments. Moreover, when comparing MRP with ConWIP, the\nperformance clearly varies depending on the specific production system\nenvironment.\n"
    },
    {
        "paper_id": 2405.02115,
        "authors": "Tiziano De Angelis, Alessandro Milazzo and Gabriele Stabile",
        "title": "On variable annuities with surrender charges",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper we provide a theoretical analysis of Variable Annuities with a\nfocus on the holder's right to an early termination of the contract. We obtain\na rigorous pricing formula and the optimal exercise boundary for the surrender\noption. We also illustrate our theoretical results with extensive numerical\nexperiments. The pricing problem is formulated as an optimal stopping problem\nwith a time-dependent payoff which is discontinuous at the maturity of the\ncontract and non-smooth. This structure leads to non-monotonic optimal stopping\nboundaries which we prove nevertheless to be continuous and regular in the\nsense of diffusions for the stopping set. The lack of monotonicity of the\nboundary makes it impossible to use classical methods from optimal stopping.\nAlso more recent results about Lipschitz continuous boundaries are not\napplicable in our setup. Thus, we contribute a new methodology for non-monotone\nstopping boundaries.\n"
    },
    {
        "paper_id": 2405.02161,
        "authors": "Simone Brusatin, Tommaso Padoan, Andrea Coletta, Domenico Delli Gatti,\n  Aldo Glielmo",
        "title": "Simulating the economic impact of rationality through reinforcement\n  learning and agent-based modelling",
        "comments": "8 pages, 4 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Agent-based models (ABMs) are simulation models used in economics to overcome\nsome of the limitations of traditional frameworks based on general equilibrium\nassumptions. However, agents within an ABM follow predetermined, not fully\nrational, behavioural rules which can be cumbersome to design and difficult to\njustify. Here we leverage multi-agent reinforcement learning (RL) to expand the\ncapabilities of ABMs with the introduction of fully rational agents that learn\ntheir policy by interacting with the environment and maximising a reward\nfunction. Specifically, we propose a 'Rational macro ABM' (R-MABM) framework by\nextending a paradigmatic macro ABM from the economic literature. We show that\ngradually substituting ABM firms in the model with RL agents, trained to\nmaximise profits, allows for a thorough study of the impact of rationality on\nthe economy. We find that RL agents spontaneously learn three distinct\nstrategies for maximising profits, with the optimal strategy depending on the\nlevel of market competition and rationality. We also find that RL agents with\nindependent policies, and without the ability to communicate with each other,\nspontaneously learn to segregate into different strategic groups, thus\nincreasing market power and overall profits. Finally, we find that a higher\ndegree of rationality in the economy always improves the macroeconomic\nenvironment as measured by total output, depending on the specific rational\npolicy, this can come at the cost of higher instability. Our R-MABM framework\nis general, it allows for stable multi-agent learning, and represents a\nprincipled and robust direction to extend existing economic simulators.\n"
    },
    {
        "paper_id": 2405.0217,
        "authors": "Eduardo Abi Jaber, Shaun (Xiaoyuan) Li, Xuyang Lin",
        "title": "Fourier-Laplace transforms in polynomial Ornstein-Uhlenbeck volatility\n  models",
        "comments": "41 pages, 8 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/publicdomain/zero/1.0/",
        "abstract": "  We consider the Fourier-Laplace transforms of a broad class of polynomial\nOrnstein-Uhlenbeck (OU) volatility models, including the well-known\nStein-Stein, Sch\\\"obel-Zhu, one-factor Bergomi, and the recently introduced\nQuintic OU models motivated by the SPX-VIX joint calibration problem. We show\nthe connection between the joint Fourier-Laplace functional of the log-price\nand the integrated variance, and the solution of an infinite dimensional\nRiccati equation. Next, under some non-vanishing conditions of the\nFourier-Laplace transforms, we establish an existence result for such Riccati\nequation and we provide a discretized approximation of the joint characteristic\nfunctional that is exponentially entire. On the practical side, we develop a\nnumerical scheme to solve the stiff infinite dimensional Riccati equations and\ndemonstrate the efficiency and accuracy of the scheme for pricing SPX options\nand volatility swaps using Fourier and Laplace inversions, with specific\nexamples of the Quintic OU and the one-factor Bergomi models and their\ncalibration to real market data.\n"
    },
    {
        "paper_id": 2405.02302,
        "authors": "Ravi Kashyap",
        "title": "The Democratization of Wealth Management: Hedged Mutual Fund Blockchain\n  Protocol",
        "comments": null,
        "journal-ref": "Research in International Business and Finance, Volume 71, August\n  2024, 102487",
        "doi": "10.1016/j.ribaf.2024.102487",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We develop several innovations to bring the best practices of traditional\ninvestment funds to the blockchain landscape. Specifically, we illustrate how:\n1) fund prices can be updated regularly like mutual funds; 2) performance fees\ncan be charged like hedge funds; 3) mutually hedged blockchain investment funds\ncan operate with investor protection schemes, such as high water marks; and 4)\nmeasures to offset trading related slippage costs when redemptions happen.\nUsing our concepts - and blockchain technology - traditional funds can\ncalculate performance fees in a simplified manner and alleviate several\noperational issues. Blockchain can solve many problems for traditional finance,\nwhile tried and tested wealth management techniques can benefit\ndecentralization, speeding its adoption. We provide detailed steps - including\nmathematical formulations and instructive pointers - to implement these ideas\nand discuss how our designs overcome several blockchain bottlenecks, making\nsmart contracts smarter. We provide numerical illustrations of several\nscenarios related to our mechanisms.\n"
    },
    {
        "paper_id": 2405.02445,
        "authors": "Balwin Bokor, Wolfgang Seiringer, Klaus Altendorfer, Thomas\n  Felberbauer",
        "title": "Energy Price and Workload Related Dispatching Rule: Balancing Energy and\n  Production Logistics Costs",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  In response to the escalating need for sustainable manufacturing practices\namid fluctuating energy prices, this study introduces a novel dispatching rule\nthat integrates energy price and workload considerations with Material\nRequirement Planning (MRP) to optimize production logistics and energy costs.\nThe dispatching rule effectively adjusts machine operational states, i.e. turn\nthe machine on or off, based on current energy prices and workload. By\ndeveloping a stochastic multi-item multi-stage job shop simulation model, this\nresearch evaluates the performance of the dispatching rule through a\ncomprehensive full-factorial simulation. Findings indicate a significant\nenhancement in shop floor decision-making through reduced costs. Moreover, the\nanalysis of the Pareto front reveals trade-offs between minimizing energy and\nproduction logistics costs, aiding decision-makers in selecting optimal\nconfigurations.\n"
    },
    {
        "paper_id": 2405.02547,
        "authors": "Sid Bhatia, Samuel Gedal, Himaya Jeyakumar Grace Lee, Ravinder Chopra,\n  Daniel Roman, Shrijani Chakroborty",
        "title": "Crypto Market Analysis & Real-Estate Business Protocol Proposal |\n  Application of Ethereum Blockchain",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  This paper examines the dynamics of the cryptocurrency market and proposes a\nnovel blockchain-based protocol for real estate transactions. Our analysis\nincludes a detailed review of price trends, volatility, and correlations within\nthe cryptocurrency market, focusing on major assets like Bitcoin, Ethereum, and\nTether. We provide a critical assessment of the impact of significant market\nevents, such as the FTX bankruptcy, highlighting the vulnerabilities and\nresilience of the crypto market. The study also explores the potential of\nblockchain technology to innovate real estate transactions by enabling the\nsecure and transparent handling of property deeds without traditional\nintermediaries. We introduce a blockchain protocol that reduces transaction\ncosts, enhances security, and increases transparency, making real estate\ntransactions more accessible and efficient. Our proposal aims to leverage the\ninherent benefits of blockchain to address real-world challenges in real estate\ntransactions, providing a scalable and secure platform for property sales in a\nglobal market.\n"
    },
    {
        "paper_id": 2405.0257,
        "authors": "Jiefei Yang, Guanglian Li",
        "title": "Gradient-enhanced sparse Hermite polynomial expansions for pricing and\n  hedging high-dimensional American options",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  We propose an efficient and easy-to-implement gradient-enhanced least squares\nMonte Carlo method for computing price and Greeks (i.e., derivatives of the\nprice function) of high-dimensional American options. It employs the sparse\nHermite polynomial expansion as a surrogate model for the continuation value\nfunction, and essentially exploits the fast evaluation of gradients. The\nexpansion coefficients are computed by solving a linear least squares problem\nthat is enhanced by gradient information of simulated paths. We analyze the\nconvergence of the proposed method, and establish an error estimate in terms of\nthe best approximation error in the weighted $H^1$ space, the statistical error\nof solving discrete least squares problems, and the time step size. We present\ncomprehensive numerical experiments to illustrate the performance of the\nproposed method. The results show that it outperforms the state-of-the-art\nleast squares Monte Carlo method with more accurate price, Greeks, and optimal\nexercise strategies in high dimensions but with nearly identical computational\ncost, and it can deliver comparable results with recent neural network-based\nmethods up to dimension 100.\n"
    },
    {
        "paper_id": 2405.02575,
        "authors": "Tingguo Zheng, Hongyin Zhang and Shiqi Ye",
        "title": "Monetary Policies on Green Financial Markets: Evidence from a\n  Multi-Moment Connectedness Network",
        "comments": null,
        "journal-ref": null,
        "doi": "10.1016/j.eneco.2024.107739",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper introduces a novel multi-moment connectedness network approach for\nanalyzing the interconnectedness of green financial market. Focusing on the\nimpact of monetary policy shocks, our study reveals that connectedness within\nthe green bond and equity markets varies with different moments (returns,\nvolatility, skewness, and kurtosis) and changes significantly around Federal\nOpen Market Committee (FOMC) events. Static analysis shows a decrease in\nconnectedness with higher moments, while dynamic analysis highlights increased\nsensitivity to event-driven shocks. We find that both tight and loose monetary\npolicy shocks initially elevate connectedness within the first six months.\nHowever, the effects of tight shocks gradually fade, whereas loose shocks may\nreduce connectedness after one year. These results offer insight to\npolicymakers in regulating sustainable economies and investment managers in\nstrategizing asset allocation and risk management, especially in\nenvironmentally focused markets. Our study contributes to understanding the\ncomplex dynamics of the green financial market in response to monetary\npolicies, helping in decision-making for sustainable economic development and\nfinancial stability.\n"
    },
    {
        "paper_id": 2405.02849,
        "authors": "Alicia Vidler and Toby Walsh",
        "title": "Modelling Opaque Bilateral Market Dynamics in Financial Trading:\n  Insights from a Multi-Agent Simulation Study",
        "comments": "13 pages, 3 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  Exploring complex adaptive financial trading environments through multi-agent\nbased simulation methods presents an innovative approach within the realm of\nquantitative finance. Despite the dominance of multi-agent reinforcement\nlearning approaches in financial markets with observable data, there exists a\nset of systematically significant financial markets that pose challenges due to\ntheir partial or obscured data availability. We, therefore, devise a\nmulti-agent simulation approach employing small-scale meta-heuristic methods.\nThis approach aims to represent the opaque bilateral market for Australian\ngovernment bond trading, capturing the bilateral nature of bank-to-bank\ntrading, also referred to as \"over-the-counter\" (OTC) trading, and commonly\noccurring between \"market makers\". The uniqueness of the bilateral market,\ncharacterized by negotiated transactions and a limited number of agents, yields\nvaluable insights for agent-based modelling and quantitative finance. The\ninherent rigidity of this market structure, which is at odds with the global\nproliferation of multilateral platforms and the decentralization of finance,\nunderscores the unique insights offered by our agent-based model. We explore\nthe implications of market rigidity on market structure and consider the\nelement of stability, in market design. This extends the ongoing discourse on\ncomplex financial trading environments, providing an enhanced understanding of\ntheir dynamics and implications.\n"
    },
    {
        "paper_id": 2405.02865,
        "authors": "Alicia Vidler, Toby Walsh",
        "title": "Non cooperative Liquidity Games and their application to bond market\n  trading",
        "comments": "9 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  We present a new type of game, the Liquidity Game. We draw inspiration from\nthe UK government bond market and apply game theoretic approaches to its\nanalysis. In Liquidity Games, market participants (agents) use non-cooperative\ngames where the players' utility is directly defined by the liquidity of the\ngame itself, offering a paradigm shift in our understanding of market dynamics.\nEach player's utility is intricately linked to the liquidity generated within\nthe game, making the utility endogenous and dynamic. Players are not just\npassive recipients of utility based on external factors but active participants\nwhose strategies and actions collectively shape and are shaped by the liquidity\nof the market. This reflexivity introduces a level of complexity and realism\npreviously unattainable in conventional models.\n  We apply Liquidity Game theoretic approaches to a simple UK bond market\ninteraction and present results for market design and strategic behavior of\nparticipants. We tackle one of the largest issues within this mechanism, namely\nwhat strategy should market makers utilize when uncertain about the type of\nmarket maker they are interacting with, and what structure might regulators\nwish to see.\n"
    },
    {
        "paper_id": 2405.02919,
        "authors": "Agni Rakshit, Gautam Bandyopadhyay, Tanujit Chakraborty",
        "title": "Hedge Error Analysis In Black Scholes Option Pricing Model: An\n  Asymptotic Approach Towards Finite Difference",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  The Black-Scholes option pricing model remains a cornerstone in financial\nmathematics, yet its application is often challenged by the need for accurate\nhedging strategies, especially in dynamic market environments. This paper\npresents a rigorous analysis of hedge errors within the Black-Scholes\nframework, focusing on the efficacy of finite difference techniques in\ncalculating option sensitivities. Employing an asymptotic approach, we\ninvestigate the behavior of hedge errors under various market conditions,\nemphasizing the implications for risk management and portfolio optimization.\nThrough theoretical analysis and numerical simulations, we demonstrate the\neffectiveness of our proposed method in reducing hedge errors and enhancing the\nrobustness of option pricing models. Our findings provide valuable insights\ninto improving the accuracy of hedging strategies and advancing the\nunderstanding of option pricing in financial markets.\n"
    },
    {
        "paper_id": 2405.03402,
        "authors": "Etienne Theising",
        "title": "Distributional Reference Class Forecasting of Corporate Sales Growth\n  With Multiple Reference Variables",
        "comments": "52 pages, 11 figures, 19 tables",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper introduces an approach to reference class selection in\ndistributional forecasting with an application to corporate sales growth rates\nusing several co-variates as reference variables, that are implicit predictors.\nThe method can be used to detect expert or model-based forecasts exposed to\n(behavioral) bias or to forecast distributions with reference classes. These\nare sets of similar entities, here firms, and rank based algorithms for their\nselection are proposed, including an optional preprocessing data dimension\nreduction via principal components analysis. Forecasts are optimal if they\nmatch the underlying distribution as closely as possible. Probability integral\ntransform values rank the forecast capability of different reference variable\nsets and algorithms in a backtest on a data set of 21,808 US firms over the\ntime period 1950 - 2019. In particular, algorithms on dimension reduced\nvariables perform well using contemporaneous balance sheet and financial market\nparameters along with past sales growth rates and past operating margins\nchanges. Comparisions of actual analysts' estimates to distributional forecasts\nand of historic distributional forecasts to realized sales growth illustrate\nthe practical use of the method.\n"
    },
    {
        "paper_id": 2405.03451,
        "authors": "d'Artis Kancs",
        "title": "Uncertainty of Supply Chains: Risk and Ambiguity",
        "comments": "arXiv admin note: text overlap with arXiv:2212.11108",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-sa/4.0/",
        "abstract": "  Motivated by the recently experienced systemic shocks (the COVID-19 pandemic\nand the full-fledged Russia's war of aggression against Ukraine) - that have\ncreated new forms of uncertainties to our supplies - this paper explores the\nsupply chain robustness under risk aversion and ambiguity aversion. We aim to\nunderstand the potential consequences of deeply uncertain systemic events on\nthe supply chain resilience and how does the information precision affect\nindividual agents' choices and the chain-level preparedness to aggregate\nshocks. Augmenting a parsimonious supply chain model with uncertainty, we\nanalyse the relationship between the upstream sourcing decisions and the supply\nchain survival probability. Both risk-averse and ambiguity-averse\nindividually-optimising agents' upstream sourcing paths are efficient but can\nbecome vulnerable to aggregate shocks. In contrast, a chain-level coordination\nof downstream firm sourcing decisions can qualitatively improve the robustness\nof the entire supply chain compared to the individual decision-making baseline.\nSuch a robust decision making ensures that in the presence of an aggregate\nshock - independently of its realisation - part of upstream suppliers will\nsurvive and the final goods' supply will be ensured even under the most\ndemanding circumstances. Our results also indicate that an input source\ndiversification extracts a cost in foregone efficiency.\n"
    },
    {
        "paper_id": 2405.03453,
        "authors": "Yu Li and Antony Ware",
        "title": "A weighted multilevel Monte Carlo method",
        "comments": "19 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-sa/4.0/",
        "abstract": "  The Multilevel Monte Carlo (MLMC) method has been applied successfully in a\nwide range of settings since its first introduction by Giles (2008). When using\nonly two levels, the method can be viewed as a kind of control-variate approach\nto reduce variance, as earlier proposed by Kebaier (2005). We introduce a\ngeneralization of the MLMC formulation by extending this control variate\napproach to any number of levels and deriving a recursive formula for computing\nthe weights associated with the control variates and the optimal numbers of\nsamples at the various levels.\n  We also show how the generalisation can also be applied to the\n\\emph{multi-index} MLMC method of Haji-Ali, Nobile, Tempone (2015), at the cost\nof solving a $(2^d-1)$-dimensional minimisation problem at each node when $d$\nindex dimensions are used.\n  The comparative performance of the weighted MLMC method is illustrated in a\nrange of numerical settings. While the addition of weights does not change the\n\\emph{asymptotic} complexity of the method, the results show that significant\nefficiency improvements over the standard MLMC formulation are possible,\nparticularly when the coarse level approximations are poorly correlated.\n"
    },
    {
        "paper_id": 2405.03496,
        "authors": "Philippe Bergault, Louis Bertucci, David Bouba, Olivier Gu\\'eant,\n  Julien Guilbert",
        "title": "Price-Aware Automated Market Makers: Models Beyond Brownian Prices and\n  Static Liquidity",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  In this paper, we introduce a suite of models for price-aware automated\nmarket making platforms willing to optimize their quotes. These models\nincorporate advanced price dynamics, including stochastic volatility, jumps,\nand microstructural price models based on Hawkes processes. Additionally, we\naddress the variability in demand from liquidity takers through models that\nemploy either Hawkes or Markov-modulated Poisson processes. Each model is\nanalyzed with particular emphasis placed on the complexity of the numerical\nmethods required to compute optimal quotes.\n"
    },
    {
        "paper_id": 2405.03624,
        "authors": "Lukasz Szpruch, Tanut Treetanthiploet, Yufei Zhang",
        "title": "$\\epsilon$-Policy Gradient for Online Pricing",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Combining model-based and model-free reinforcement learning approaches, this\npaper proposes and analyzes an $\\epsilon$-policy gradient algorithm for the\nonline pricing learning task. The algorithm extends $\\epsilon$-greedy algorithm\nby replacing greedy exploitation with gradient descent step and facilitates\nlearning via model inference. We optimize the regret of the proposed algorithm\nby quantifying the exploration cost in terms of the exploration probability\n$\\epsilon$ and the exploitation cost in terms of the gradient descent\noptimization and gradient estimation errors. The algorithm achieves an expected\nregret of order $\\mathcal{O}(\\sqrt{T})$ (up to a logarithmic factor) over $T$\ntrials.\n"
    },
    {
        "paper_id": 2405.03701,
        "authors": "Kevin Xin, Lizhi Xin",
        "title": "QxEAI: Quantum-like evolutionary algorithm for automated probabilistic\n  forecasting",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
        "abstract": "  Forecasting, to estimate future events, is crucial for business and\ndecision-making. This paper proposes QxEAI, a methodology that produces a\nprobabilistic forecast that utilizes a quantum-like evolutionary algorithm\nbased on training a quantum-like logic decision tree and a classical value tree\non a small number of related time series. We demonstrate how the application of\nour quantum-like evolutionary algorithm to forecasting can overcome the\nchallenges faced by classical and other machine learning approaches. By using\nthree real-world datasets (Dow Jones Index, retail sales, gas consumption), we\nshow how our methodology produces accurate forecasts while requiring little to\nnone manual work.\n"
    },
    {
        "paper_id": 2405.03893,
        "authors": "Avichai Snir, Dudi Levy, Dian Wang, Haipeng Allan Chen, and Daniel\n  Levy",
        "title": "Large Effects of Small Cues: Priming Selfish Economic Decisions",
        "comments": "57 pages, contains the body of the paper and the appendix",
        "journal-ref": null,
        "doi": "10.13140/RG.2.2.32166.74566",
        "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
        "abstract": "  Many experimental studies report that economics students tend to act more\nselfishly than students of other disciplines, a finding that received\nwidespread public and professional attention. Two main explanations that the\nexisting literature offers for the differences found in the behavior between\neconomists and noneconomists are the selection effect, and the indoctrination\neffect. We offer an alternative, novel explanation. We argue that these\ndifferences can be explained by differences in the interpretation of the\ncontext. We test this hypothesis by conducting two social dilemma experiments\nin the US and Israel with participants from both economics and non-economics\nmajors. In the experiments, participants face a tradeoff between profit\nmaximization, that is the market norm and workers welfare, that is the social\nnorm. We use priming to manipulate the cues that the participants receive\nbefore they make their decision. We find that when participants receive cues\nsignaling that the decision has an economic context, both economics and\nnon-economics students tend to maximize profits. When the participants receive\ncues emphasizing social norms, on the other hand, both economics and\nnon-economics students are less likely to maximize profits. We conclude that\nsome of the differences found between the decisions of economics and\nnon-economics students can be explained by contextual cues.\n"
    },
    {
        "paper_id": 2405.04352,
        "authors": "David Van Dijcke, Florian Gunsilius, Austin Wright",
        "title": "Return to Office and the Tenure Distribution",
        "comments": "6 figures, 3 tables, 18 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  With the official end of the COVID-19 pandemic, debates about the return to\noffice have taken center stage among companies and employees. Despite their\nubiquity, the economic implications of return to office policies are not fully\nunderstood. Using 260 million resumes matched to company data, we analyze the\ncausal effects of such policies on employees' tenure and seniority levels at\nthree of the largest US tech companies: Microsoft, SpaceX, and Apple. Our\nestimation procedure is nonparametric and captures the full heterogeneity of\ntenure and seniority of employees in a distributional synthetic controls\nframework. We estimate a reduction in counterfactual tenure that increases for\nemployees with longer tenure. Similarly, we document a leftward shift in the\nseniority distribution towards positions below the senior level. These shifts\nappear to be driven by employees leaving to larger firms that are direct\ncompetitors. Our results suggest that return to office policies can lead to an\noutflow of senior employees, posing a potential threat to the productivity,\ninnovation, and competitiveness of the wider firm.\n"
    },
    {
        "paper_id": 2405.04539,
        "authors": "Aryan Bhambu, Arabin Kumar Dey",
        "title": "Some variation of COBRA in sequential learning setup",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-sa/4.0/",
        "abstract": "  This research paper introduces innovative approaches for multivariate time\nseries forecasting based on different variations of the combined regression\nstrategy. We use specific data preprocessing techniques which makes a radical\nchange in the behaviour of prediction. We compare the performance of the model\nbased on two types of hyper-parameter tuning Bayesian optimisation (BO) and\nUsual Grid search. Our proposed methodologies outperform all state-of-the-art\ncomparative models. We illustrate the methodologies through eight time series\ndatasets from three categories: cryptocurrency, stock index, and short-term\nload forecasting.\n"
    },
    {
        "paper_id": 2405.04692,
        "authors": "Tian Tian, Liu Ze hui, Huang Zichen, Yubing Tang",
        "title": "Enhancing Organizational Performance: Harnessing AI and NLP for User\n  Feedback Analysis in Product Development",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper explores the application of AI and NLP techniques for user\nfeedback analysis in the context of heavy machine crane products. By leveraging\nAI and NLP, organizations can gain insights into customer perceptions, improve\nproduct development, enhance satisfaction and loyalty, inform decision-making,\nand gain a competitive advantage. The paper highlights the impact of user\nfeedback analysis on organizational performance and emphasizes the reasons for\nusing AI and NLP, including scalability, objectivity, improved accuracy,\nincreased insights, and time savings. The methodology involves data collection,\ncleaning, text and rating analysis, interpretation, and feedback\nimplementation. Results include sentiment analysis, word cloud visualizations,\nand radar charts comparing product attributes. These findings provide valuable\ninformation for understanding customer sentiment, identifying improvement\nareas, and making data-driven decisions to enhance the customer experience. In\nconclusion, promising AI and NLP techniques in user feedback analysis offer\norganizations a powerful tool to understand customers, improve product\ndevelopment, increase satisfaction, and drive business success\n"
    },
    {
        "paper_id": 2405.04693,
        "authors": "Stephen H. Lihn",
        "title": "Generalization of the Alpha-Stable Distribution with the Degree of\n  Freedom",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-sa/4.0/",
        "abstract": "  A Wright function based framework is proposed to combine and extend several\ndistribution families. The $\\alpha$-stable distribution is generalized by\nadding the degree of freedom parameter. The PDF of this two-sided super\ndistribution family subsumes those of the original $\\alpha$-stable, Student's t\ndistributions, as well as the exponential power distribution and the modified\nBessel function of the second kind. Its CDF leads to a fractional extension of\nthe Gauss hypergeometric function. The degree of freedom makes possible for\nvalid variance, skewness, and kurtosis, just like Student's t. The original\n$\\alpha$-stable distribution is viewed as having one degree of freedom, that\nexplains why it lacks most of the moments. A skew-Gaussian kernel is derived\nfrom the characteristic function of the $\\alpha$-stable law, which maximally\npreserves the law in the new framework. To facilitate such framework, the\nstable count distribution is generalized as the fractional extension of the\ngeneralized gamma distribution. It provides rich subordination capabilities,\none of which is the fractional $\\chi$ distribution that supplies the needed\n'degree of freedom' parameter. Hence, the \"new\" $\\alpha$-stable distribution is\na \"ratio distribution\" of the skew-Gaussian kernel and the fractional $\\chi$\ndistribution. Mathematically, it is a new form of higher transcendental\nfunction under the Wright function family. Last, the new univariate symmetric\ndistribution is extended to the multivariate elliptical distribution\nsuccessfully.\n"
    },
    {
        "paper_id": 2405.04972,
        "authors": "Felix Haag, Carlo Stingl, Katrin Zerfass, Konstantin Hopf, Thorsten\n  Staake",
        "title": "Overcoming Anchoring Bias: The Potential of AI and XAI-based Decision\n  Support",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  Information systems (IS) are frequently designed to leverage the negative\neffect of anchoring bias to influence individuals' decision-making (e.g., by\nmanipulating purchase decisions). Recent advances in Artificial Intelligence\n(AI) and the explanations of its decisions through explainable AI (XAI) have\nopened new opportunities for mitigating biased decisions. So far, the potential\nof these technological advances to overcome anchoring bias remains widely\nunclear. To this end, we conducted two online experiments with a total of N=390\nparticipants in the context of purchase decisions to examine the impact of AI\nand XAI-based decision support on anchoring bias. Our results show that AI\nalone and its combination with XAI help to mitigate the negative effect of\nanchoring bias. Ultimately, our findings have implications for the design of AI\nand XAI-based decision support and IS to overcome cognitive biases.\n"
    },
    {
        "paper_id": 2405.05101,
        "authors": "Orcan Ogetbil, Bernhard Hientzsch",
        "title": "Inflation Models with Correlation and Skew",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We formulate a forward inflation index model with multi-factor volatility\nstructure featuring a parametric form that allows calibration to correlations\nbetween indices of different tenors observed in the market. Assuming the\nnominal interest rate follows a single factor Gaussian short rate model, we\npresent analytical prices for zero-coupon and year-on-year swaps, caps, and\nfloors. The same method applies to any interest rate model for which one can\ncompute the zero-coupon bond prices and measure shifts. We extend the\nmulti-factor model with leverage functions to capture the entire market\nvolatility skew with a single process. The time-consuming calibration step of\nthis model can be avoided in the simplified model that we further propose. We\ndemonstrate the leveraged and the simplified models with market data.\n"
    },
    {
        "paper_id": 2405.05192,
        "authors": "Ariel Neufeld, Philipp Schmocker, Sizhou Wu",
        "title": "Full error analysis of the random deep splitting method for nonlinear\n  parabolic PDEs and PIDEs with infinite activity",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper, we present a randomized extension of the deep splitting\nalgorithm introduced in [Beck, Becker, Cheridito, Jentzen, and Neufeld (2021)]\nusing random neural networks suitable to approximately solve both\nhigh-dimensional nonlinear parabolic PDEs and PIDEs with jumps having\n(possibly) infinite activity. We provide a full error analysis of our so-called\nrandom deep splitting method. In particular, we prove that our random deep\nsplitting method converges to the (unique viscosity) solution of the nonlinear\nPDE or PIDE under consideration. Moreover, we empirically analyze our random\ndeep splitting method by considering several numerical examples including both\nnonlinear PDEs and nonlinear PIDEs relevant in the context of pricing of\nfinancial derivatives under default risk. In particular, we empirically\ndemonstrate in all examples that our random deep splitting method can\napproximately solve nonlinear PDEs and PIDEs in 10'000 dimensions within\nseconds.\n"
    },
    {
        "paper_id": 2405.05449,
        "authors": "Gang Hu and Ming Gu",
        "title": "Markowitz Meets Bellman: Knowledge-distilled Reinforcement Learning for\n  Portfolio Management",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Investment portfolios, central to finance, balance potential returns and\nrisks. This paper introduces a hybrid approach combining Markowitz's portfolio\ntheory with reinforcement learning, utilizing knowledge distillation for\ntraining agents. In particular, our proposed method, called KDD (Knowledge\nDistillation DDPG), consist of two training stages: supervised and\nreinforcement learning stages. The trained agents optimize portfolio assembly.\nA comparative analysis against standard financial models and AI frameworks,\nusing metrics like returns, the Sharpe ratio, and nine evaluation indices,\nreveals our model's superiority. It notably achieves the highest yield and\nSharpe ratio of 2.03, ensuring top profitability with the lowest risk in\ncomparable return scenarios.\n"
    },
    {
        "paper_id": 2405.05578,
        "authors": "Miaomiao Shen, Linxuan Yu, Jing Xu, Zihao Sang, Ruijia Li, Xiang Yuan",
        "title": "Shaping the Future of Urban Mobility: Insights into Autonomous Vehicle\n  Acceptance in Shanghai Through TAM and Perceived Risk Analysis",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Autonomous vehicles (AVs) have begun experimental commercialization\ninitiatives in places such as Shanghai, China, and it is a valuable research\nquestion whether people's willingness to use AVs has changed from the prior.\nThis study explores Shanghai residents' attitudes towards AVs by applying the\nTechnology Acceptance Model (TAM), the Perceived Risk (BAR) model, and\nintroducing perceived externalities as a new psychological variable. Through a\nsurvey in Shanghai, where AVs are operational, and structural equation\nmodeling, it was found that perceived usefulness and ease of use positively\ninfluence willingness to use AVs, with perceived usefulness being the most\nsignificant factor. Perceived externalities have a positive impact, while\nperceived risk negatively affects willingness to use. Interestingly, ease of\nuse increases perceived risk, but this is mitigated by the benefits perceived\nin usefulness. This research, differing significantly from previous studies,\naims to guide government policy and industry strategies to enhance design,\nmarketing, and popularization.\n"
    },
    {
        "paper_id": 2405.05634,
        "authors": "Salam Rabindrajit Luwang, Anish Rai, Md.Nurujjaman, Om Prakash,\n  Chittaranjan Hens",
        "title": "High-Frequency Stock Market Order Transitions during the US-China Trade\n  War 2018: A Discrete-Time Markov Chain Analysis",
        "comments": null,
        "journal-ref": null,
        "doi": "10.1063/5.0176892",
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Statistical analysis of high-frequency stock market order transaction data is\nconducted to understand order transition dynamics. We employ a first-order\ntime-homogeneous discrete-time Markov chain model to the sequence of orders of\nstocks belonging to six different sectors during the USA-China trade war of\n2018. The Markov property of the order sequence is validated by the Chi-square\ntest. We estimate the transition probability matrix of the sequence using\nmaximum likelihood estimation. From the heat-map of these matrices, we found\nthe presence of active participation by different types of traders during high\nvolatility days. On such days, these traders place limit orders primarily with\nthe intention of deleting the majority of them to influence the market. These\nfindings are supported by high stationary distribution and low mean recurrence\nvalues of add and delete orders. Further, we found similar spectral gap and\nentropy rate values, which indicates that similar trading strategies are\nemployed on both high and low volatility days during the trade war. Among all\nthe sectors considered in this study, we observe that there is a recurring\npattern of full execution orders in Finance & Banking sector. This shows that\nthe banking stocks are resilient during the trade war. Hence, this study may be\nuseful in understanding stock market order dynamics and devise trading\nstrategies accordingly on high and low volatility days during extreme\nmacroeconomic events.\n"
    },
    {
        "paper_id": 2405.05642,
        "authors": "Kundan Mukhia, Anish Rai, SR Luwang, Md Nurujjaman, Sushovan Majhi,\n  Chittaranjan Hens",
        "title": "Complex network analysis of cryptocurrency market during crashes",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  This paper identifies the cryptocurrency market crashes and analyses its\ndynamics using the complex network. We identify three distinct crashes during\n2017-20, and the analysis is carried out by dividing the time series into\npre-crash, crash, and post-crash periods. Partial correlation based complex\nnetwork analysis is carried out to study the crashes. Degree density\n($\\rho_D$), average path length ($\\bar{l}$), and average clustering coefficient\n($\\overline{cc}$) are estimated from these networks. We find that both $\\rho_D$\nand $\\overline{cc}$ are smallest during the pre-crash period, and spike during\nthe crash suggesting the network is dense during a crash. Although $\\rho_D$ and\n$\\overline{cc}$ decrease in the post-crash period, they remain higher than\npre-crash levels for the 2017-18 and 2018-19 crashes suggesting a market\nattempt to return to normalcy. We get $\\bar{l}$ is minimal during the crash\nperiod, suggesting a rapid flow of information. A dense network and rapid\ninformation flow suggest that during a crash uninformed synchronized panic\nsell-off happens. However, during the 2019-20 crash, the values of $\\rho_D$,\n$\\overline{cc}$, and $\\bar{l}$ did not vary significantly, indicating minimal\nchange in dynamics compared to other crashes. The findings of this study may\nguide investors in making decisions during market crashes.\n"
    },
    {
        "paper_id": 2405.0578,
        "authors": "Daniel de Souza Santos and Tiago Alessandro Espinola Ferreira",
        "title": "Neural Network Learning of Black-Scholes Equation for Option Pricing",
        "comments": "15 pages and 8 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  One of the most discussed problems in the financial world is stock option\npricing. The Black-Scholes Equation is a Parabolic Partial Differential\nEquation which provides an option pricing model. The present work proposes an\napproach based on Neural Networks to solve the Black-Scholes Equations.\nReal-world data from the stock options market were used as the initial boundary\nto solve the Black-Scholes Equation. In particular, times series of call\noptions prices of Brazilian companies Petrobras and Vale were employed. The\nresults indicate that the network can learn to solve the Black-Sholes Equation\nfor a specific real-world stock options time series. The experimental results\nshowed that the Neural network option pricing based on the Black-Sholes\nEquation solution can reach an option pricing forecasting more accurate than\nthe traditional Black-Sholes analytical solutions. The experimental results\nmaking it possible to use this methodology to make short-term call option price\nforecasts in options markets.\n"
    },
    {
        "paper_id": 2405.05891,
        "authors": "David Aristei, Manuela Gallo, Raoul Minetti",
        "title": "Financial knowledge and borrower discouragement",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  This study provides first empirical evidence on the impact of entrepreneurs'\nfinancial knowledge on borrower discouragement. Using novel survey data on\nItalian micro-enterprises, we find that less financially knowledgeable\nentrepreneurs are more likely to be discouraged from applying for new\nfinancing, due to higher application costs and expected rejection. Our main\nresults are robust to several sensitivity checks, including accounting for\npotential endogeneity. Furthermore, we show that the observed self-rationing\nmechanism is rather inefficient, suggesting that financial knowledge might play\na key role in reducing credit market imperfections.\n"
    },
    {
        "paper_id": 2405.06193,
        "authors": "Anton Kurniawan and Yos Sunitiyoso",
        "title": "New Business Model for Sustainable Retail Company Using Design Thinking\n  Concept",
        "comments": "10 pages, 8 figures, Published with International Research Journal of\n  Economics and Management Studies (IRJEMS)",
        "journal-ref": null,
        "doi": "10.56472/25835238/IRJEMS-V3I4P137",
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  The waste problem is still becoming a big concern in Indonesia. Waste,\nespecially plastic waste comes from single-use packaging of daily necessities\nsuch as personal care and home care. PT. Siklus Refil Indonesia or Siklus, a\nretail company, comes to offer a sustainable solution of buying daily\nnecessities by refill method. Since April 2020, Siklus has operated in the\nGreater Jakarta area and already impacted 20,000 customers. However, Siklus\nmust change its new business model due to regulation from the Food and Drug\nSupervisory Agency (BPOM) that warned the company not to sell personal care who\ncome in direct contact with skin. The warning impacted the decreasing\ncustomers, sales, and profit of Siklus. This research has the purpose of\ndetermining the new business model of Siklus using the design thinking concept.\nBy this concept, this research empathizes with customers, defines customer\nneeds, and ideates a business model. This research continues to decide the new\nbusiness model by creating a matrix of stepwise selection. Then this research\ndoes a prototype business model and tests the new business model. After doing\nthe process, Return from Home is selected as the new business model for Siklus.\n"
    },
    {
        "paper_id": 2405.06235,
        "authors": "Zongxia Liang, Yi Xia, Bin Zou",
        "title": "A Two-layer Stochastic Game Approach to Reinsurance Contracting and\n  Competition",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We introduce a two-layer stochastic game model to study reinsurance\ncontracting and competition in a market with one insurer and two competing\nreinsurers.\n  The insurer negotiates with both reinsurers simultaneously for proportional\nreinsurance contracts that are priced using the variance premium principle; the\nreinsurance contracting between the insurer and each reinsurer is modeled as a\nStackelberg game.\n  The two reinsurers compete for business from the insurer and optimize the\nso-called relative performance, instead of their own surplus; the competition\ngame between the two reinsurers is settled by a non-cooperative Nash game. We\nobtain a sufficient and necessary condition, related to the competition degrees\nof the two reinsurers, for the existence of an equilibrium. We show that the\nequilibrium, if exists, is unique, and the equilibrium strategy of each player\nis constant, fully characterized in semi-closed form.\n  Additionally, we obtain interesting sensitivity results for the equilibrium\nstrategies through both an analytical and numerical study.\n"
    },
    {
        "paper_id": 2405.06476,
        "authors": "Alberto Baccini and Cristina Re",
        "title": "Is the panel fair? Evaluating panel compositions through network\n  analysis. The case of research assessments in Italy",
        "comments": "45 pages, 8 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In research evaluation, the fair representation of panels is usually defined\nin terms of observable characteristics of scholars such as gender or\naffiliations. An an empirical strategy is proposed for exploring hidden\nconnections between panellists such that, despite the respect of formal\nrequirements, the panel could be considered alike as unfair with respect to the\nrepresentation of diversity of research approaches and methodologies. The case\nstudy regards the three panels selected to evaluate research in economics,\nstatistics and business during the Italian research assessment exercises. The\nfirst two panels were appointed directly by the governmental agency responsible\nfor the evaluation, while the third was randomly selected. Hence the third\npanel can be considered as a control for evaluating about the fairness of the\nothers. The fair representation is explored by comparing the networks of\npanellists based on their co-authorship relations, the networks based on\njournals in which they published and the networks based on their affiliated\ninstitutions (universities, research centres and newspapers). The results show\nthat the members of the first two panels had connections much higher than the\nmembers of the control group. Hence the composition of the first two panels\nshould be considered as unfair, as the results of the research assessments.\n"
    },
    {
        "paper_id": 2405.0657,
        "authors": "Gen Norman Thomas, Siti Mutiara Ramadhanti Nur, Lely Indriaty",
        "title": "The Impact of Financial Literacy, Social Capital, and Financial\n  Technology on Financial Inclusion of Indonesian Students",
        "comments": "8, 1, IRJEMS International Research Journal of Economics and\n  Management Studies",
        "journal-ref": "Gen Norman Thomas, Siti Mutiara Ramadhanti Nur, Lely Indriaty.\n  \"The Impact of Financial Literacy, Social Capital, and Financial Technology\n  on Financial Inclusion of Indonesian Students\" IRJEMS, Vol. 3, No. 4, pp.\n  308-315, 2024",
        "doi": "10.56472/25835238/IRJEMS-V3I4P140",
        "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
        "abstract": "  This study aims to analyze the impact of financial literacy, social capital\nand financial technology on financial inclusion. The research method used a\nquantitative research method, in which questionnaires were distributed to 100\nactive students in the economics faculty at 7 private colleges in Tangerang,\nIndonesia. Based on the results of data processing using SPSS version 23, it\nresults that financial literacy, social capital and financial technology\npartially have a positive and significant influence on financial inclusion. The\nresults of this study provide input that financial literacy needs to be\nincreased because it is not yet equivalent to financial inclusion, and reducing\nthe gap between financial literacy and financial inclusion is only 2.74%.\nAnother benefit of this research is to give an understanding to students that\nstudents should be independent actors or users of financial technology products\nand that students should become pioneers in delivering financial knowledge,\nfinancial behavior and financial attitudes to the wider community.\n"
    },
    {
        "paper_id": 2405.06764,
        "authors": "Emmanuel Lepinette and Duc Thinh Vu",
        "title": "Coherent Risk Measure on $L^0$: NA Condition, Pricing and Dual\n  Representation",
        "comments": null,
        "journal-ref": "IJTAF (2021)",
        "doi": "10.1142/S0219024921500370",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The NA condition is one of the pillars supporting the classical theory of\nfinancial mathematics. We revisit this condition for financial market models\nwhere a dynamic risk-measure defined on $L^0$ is fixed to characterize the\nfamily of acceptable wealths that play the role of non negative financial\npositions. We provide in this setting a new version of the fundamental theorem\nof asset pricing and we deduce a dual characterization of the super-hedging\nprices (called risk-hedging prices) of a European option. Moreover, we show\nthat the set of all risk-hedging prices is closed under NA. At last, we provide\na dual representation of the risk-measure on $L^0$ under some conditions.\n"
    },
    {
        "paper_id": 2405.06774,
        "authors": "Reilly Pickard, Finn Wredenhagen, Julio DeJesus, Mario Schlener, Yuri\n  Lawryshyn",
        "title": "Hedging American Put Options with Deep Reinforcement Learning",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
        "abstract": "  This article leverages deep reinforcement learning (DRL) to hedge American\nput options, utilizing the deep deterministic policy gradient (DDPG) method.\nThe agents are first trained and tested with Geometric Brownian Motion (GBM)\nasset paths and demonstrate superior performance over traditional strategies\nlike the Black-Scholes (BS) Delta, particularly in the presence of transaction\ncosts. To assess the real-world applicability of DRL hedging, a second round of\nexperiments uses a market calibrated stochastic volatility model to train DRL\nagents. Specifically, 80 put options across 8 symbols are collected, stochastic\nvolatility model coefficients are calibrated for each symbol, and a DRL agent\nis trained for each of the 80 options by simulating paths of the respective\ncalibrated model. Not only do DRL agents outperform the BS Delta method when\ntesting is conducted using the same calibrated stochastic volatility model data\nfrom training, but DRL agents achieves better results when hedging the true\nasset path that occurred between the option sale date and the maturity. As\nsuch, not only does this study present the first DRL agents tailored for\nAmerican put option hedging, but results on both simulated and empirical market\ntesting data also suggest the optimality of DRL agents over the BS Delta method\nin real-world scenarios. Finally, note that this study employs a model-agnostic\nChebyshev interpolation method to provide DRL agents with option prices at each\ntime step when a stochastic volatility model is used, thereby providing a\ngeneral framework for an easy extension to more complex underlying asset\nprocesses.\n"
    },
    {
        "paper_id": 2405.06808,
        "authors": "Zhiyu Cao, Zachary Feinstein",
        "title": "Large Language Model in Financial Regulatory Interpretation",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  This study explores the innovative use of Large Language Models (LLMs) as\nanalytical tools for interpreting complex financial regulations. The primary\nobjective is to design effective prompts that guide LLMs in distilling verbose\nand intricate regulatory texts, such as the Basel III capital requirement\nregulations, into a concise mathematical framework that can be subsequently\ntranslated into actionable code. This novel approach aims to streamline the\nimplementation of regulatory mandates within the financial reporting and risk\nmanagement systems of global banking institutions. A case study was conducted\nto assess the performance of various LLMs, demonstrating that GPT-4 outperforms\nother models in processing and collecting necessary information, as well as\nexecuting mathematical calculations. The case study utilized numerical\nsimulations with asset holdings -- including fixed income, equities, currency\npairs, and commodities -- to demonstrate how LLMs can effectively implement the\nBasel III capital adequacy requirements.\n  Keywords: Large Language Models, Prompt Engineering, LLMs in Finance, Basel\nIII, Minimum Capital Requirements, LLM Ethics\n"
    },
    {
        "paper_id": 2405.06854,
        "authors": "C. Escudero, F. Lara, M. Sama",
        "title": "Optimal Trade Characterizations in Multi-Asset Crypto-Financial Markets",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This work focuses on the mathematical study of constant function market\nmakers. We rigorously establish the conditions for optimal trading under the\nassumption of a quasilinear, but not necessarily convex (or concave), trade\nfunction. This generalizes previous results that used convexity, and also\nguarantees the robustness against arbitrage of so-designed automatic market\nmakers. The theoretical results are illustrated by families of examples given\nby generalized means, and also by numerical simulations in certain concrete\ncases. These simulations along with the mathematical analysis suggest that the\nquasilinear-trade-function based automatic market makers might replicate the\nfunctioning of those based on convex functions, in particular regarding their\nresilience to arbitrage.\n"
    },
    {
        "paper_id": 2405.07071,
        "authors": "S\\'andor Juh\\'asz, Zolt\\'an Elekes, Vir\\'ag Ily\\'es, Frank Neffke",
        "title": "Colocation of skill related suppliers -- Revisiting coagglomeration\n  using firm-to-firm network data",
        "comments": "Supplementary information included",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Strong local clusters help firms compete on global markets. One explanation\nfor this is that firms benefit from locating close to their suppliers and\ncustomers. However, the emergence of global supply chains shows that physical\nproximity is not necessarily a prerequisite to successfully manage\ncustomer-supplier relations anymore. This raises the question when firms need\nto colocate in value chains and when they can coordinate over longer distances.\nWe hypothesize that one important aspect is the extent to which supply chain\npartners exchange not just goods but also know-how. To test this, we build on\nan expanding literature that studies the drivers of industrial coagglomeration\nto analyze when supply chain connections lead firms to colocation. We exploit\ndetailed micro-data for the Hungarian economy between 2015 and 2017, linking\nfirm registries, employer-employee matched data and firm-to-firm transaction\ndata from value-added tax records. This allows us to observe colocation, labor\nflows and value chain connections at the level of firms, as well as construct\naggregated coagglomeration patterns, skill relatedness and input-output\nconnections between pairs of industries. We show that supply chains are more\nlikely to support coagglomeration when the industries involved are also skill\nrelated. That is, input-output and labor market channels reinforce each other,\nbut supplier connections only matter for colocation when industries have\nsimilar labor requirements, suggesting that they employ similar types of\nknow-how. We corroborate this finding by analyzing the interactions between\nfirms, showing that supplier relations are more geographically constrained\nbetween companies that operate in skill related industries.\n"
    },
    {
        "paper_id": 2405.07103,
        "authors": "Jack Birner, Marco Mazzoli, Eleonora Priori, Pietro Terna",
        "title": "Breaking open the black box of the production function: an agent-based\n  model accounting for time in production processes",
        "comments": "31 pages, 25 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
        "abstract": "  Traditional notions of production function do not consider the time\ndimension, appearing thus timeless and instantaneous. We propose an agent-based\nmodel accounting for the whole production side of the economy to unfold the\nproduction process from its very beginning, when firms receive production\norders, to the delivery of the products to the market. In the model we analyze\nwith a high-degree of details how heterogeneous firms, having labor and capital\nas productive factors, behave along all the realization processes of their\noutputs. The main focus covers: i) the heterogeneous duration of firms'\nproduction processes, ii) the adaptive strategies they implement to adjust\ntheir choices, and iii) the possible failures which may occur due to the\nduration of the production. Our agent-based model is a controlled experiment:\nwe use a virtual central planner mechanism, which acts as the demand side of\nthe economy, to observe which firm individual behaviors and aggregate\nmacroeconomic outcomes emerge as a reply to its different behaviors in a\nceteris paribus environment. Our applied goal, then, is to discuss the role of\nindustrial policy by modeling production processes in detail.\n"
    },
    {
        "paper_id": 2405.07184,
        "authors": "Masamitsu Ohnishi and Makoto Shimoshimizu",
        "title": "Trade execution games in a Markovian environment",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper examines a trade execution game for two large traders in a\ngeneralized price impact model. We incorporate a stochastic and sequentially\ndependent factor that exogenously affects the market price into financial\nmarkets. Our model accounts for how strategic and environmental uncertainties\naffect the large traders' execution strategies. We formulate an expected\nutility maximization problem for two large traders as a Markov game model.\nApplying the backward induction method of dynamic programming, we provide an\nexplicit closed-form execution strategy at a Markov perfect equilibrium. Our\ntheoretical results reveal that the execution strategy generally lies in a\ndynamic and non-randomized class; it becomes deterministic if the Markovian\nenvironment is also deterministic. In addition, our simulation-based numerical\nexperiments suggest that the execution strategy captures various features\nobserved in financial markets.\n"
    },
    {
        "paper_id": 2405.0724,
        "authors": "Helena Laneuville and Vitor Possebom",
        "title": "Fight like a Woman: Domestic Violence and Female Judges in Brazil",
        "comments": "We included an analysis about the effect of the trial judge's gender\n  on the appeal probability, the reversal probability and the recidivism\n  probability",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  We investigate the impact of judges' gender on the outcome of domestic\nviolence cases. Using data from S\\~ao Paulo, Brazil, between 2010 and 2019, we\ncompare conviction rates by judge's gender and find that a domestic violence\ncase assigned to a female judge is 31\\% (10 p.p.) more likely to result in\nconviction than a case assigned to a male judge with similar career\ncharacteristics. To show that this decision gap rises due to different gender\nperspectives about domestic violence and not because female judges are stricter\nthan their male counterparts in all rulings, we compare it against the gender\nconviction-rate gap in similar types of crime. We find that the gender\nconviction-rate gap for domestic violence cases is significantly larger than\nthe same gap for other misdemeanor cases (3 p.p. larger) and for other physical\nassault cases (8 p.p. larger). Furthermore, we find evidence that at least two\nchannels explain this gender conviction-rate gap for domestic violence cases:\ngender-based differences in evidence interpretation and gender-based sentencing\ncriteria. Lastly, we find that this gender conviction rate has no significant\nimpact on the probability of appeals, ruling reversals or recidivism.\n"
    },
    {
        "paper_id": 2405.07292,
        "authors": "Rajveer Jat and Daanish Padha",
        "title": "Kernel Three Pass Regression Filter",
        "comments": "We have identified some errors and are currently improving them. So\n  we do not want people to read erroneous paper",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
        "abstract": "  We forecast a single time series using a high-dimensional set of predictors.\nWhen these predictors share common underlying dynamics, an approximate latent\nfactor model provides a powerful characterization of their co-movements\nBai(2003). These latent factors succinctly summarize the data and can also be\nused for prediction, alleviating the curse of dimensionality in\nhigh-dimensional prediction exercises, see Stock & Watson (2002a). However,\nforecasting using these latent factors suffers from two potential drawbacks.\nFirst, not all pervasive factors among the set of predictors may be relevant,\nand using all of them can lead to inefficient forecasts. The second shortcoming\nis the assumption of linear dependence of predictors on the underlying factors.\nThe first issue can be addressed by using some form of supervision, which leads\nto the omission of irrelevant information. One example is the three-pass\nregression filter proposed by Kelly & Pruitt (2015). We extend their framework\nto cases where the form of dependence might be nonlinear by developing a new\nestimator, which we refer to as the Kernel Three-Pass Regression Filter\n(K3PRF). This alleviates the aforementioned second shortcoming. The estimator\nis computationally efficient and performs well empirically. The short-term\nperformance matches or exceeds that of established models, while the long-term\nperformance shows significant improvement.\n"
    },
    {
        "paper_id": 2405.07308,
        "authors": "Yanqiao Deng and Minda Ma",
        "title": "China's plug-in hybrid electric vehicles transition: an operational\n  carbon perspective",
        "comments": "43 pages, 7 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Assessing the energy and emissions of representative plug-in hybrid electric\nvehicle (PHEV) model operations is crucial for accelerating carbon neutrality\ntransitions in China's passenger car sector. This study makes the first attempt\nto create a bottom-up model to measure the real-world energy use and carbon\ndioxide (CO2) emissions of China's top twenty selling PHEV model operations\nacross different geographical regions during 2020-2022. The results indicate\nthat (1) the actual electricity intensity for the best-selling PEHV models\n(20.2-38.2 kilowatt-hour [kWh]/100 kilometers [km]) was 30-40% higher than the\nNew European Driving Cycle (NEDC) values, and the actual gasoline intensity\n(4.7 to 23.5 liters [L]/100 km) was 3-6 times greater than the NEDC values. (2)\nThe overall energy consumption of the best-selling models exhibited variations\namong various geographical regions, and the total gasoline equivalent was twice\nas high in southern China (1283 mega-liters, 2020-2022) than in northern China\nand the Yangtze River Middle Reach. (3) The top-selling models emitted 4.9\nmega-tons (Mt) of CO2 nationwide from 2020-2022, 1.9 Mt from electricity and 3\nMt from gasoline. In northern China, carbon emissions per vehicle were more\nthan 1.2 times greater than those in other regions. Furthermore, targeted\npolicy implications for expediting the carbon-neutral transition within the\npassenger vehicles are proposed. Overall, this study reviews and compares\nnational and regional benchmark data and performance data for PHEV operations.\nIts objective is to bolster national decarbonization initiatives, ensuring low\nemissions and expediting the transportation sector's transition toward a\nnet-zero era.\n"
    },
    {
        "paper_id": 2405.07323,
        "authors": "Segun Taofeek Aroyehun, Almog Simchon, Fabio Carrella, Jana Lasser,\n  Stephan Lewandowsky, David Garcia",
        "title": "Computational analysis of US Congressional speeches reveals a shift from\n  evidence to intuition",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Pursuit of honest and truthful decision-making is crucial for governance and\naccountability in democracies. However, people sometimes take different\nperspectives of what it means to be honest and how to pursue truthfulness. Here\nwe explore a continuum of perspectives from evidence-based reasoning, rooted in\nascertainable facts and data, at one end, to intuitive decisions that are\ndriven by feelings and subjective interpretations, at the other. We analyze the\nlinguistic traces of those contrasting perspectives in Congressional speeches\nfrom 1879 to 2022. We find that evidence-based language has continued to\ndecline since the mid-1970s, together with a decline in legislative\nproductivity. The decline was accompanied by increasing partisan polarization\nin Congress and rising income inequality in society. Results highlight the\nimportance of evidence-based language in political decision-making.\n"
    },
    {
        "paper_id": 2405.07431,
        "authors": "Vansh Murad Kalia",
        "title": "Packing Peanuts: The Role Synthetic Data Can Play in Enhancing\n  Conventional Economic Prediction Models",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Packing peanuts, as defined by Wikipedia, is a common loose-fill packaging\nand cushioning material that helps prevent damage to fragile items. In this\npaper, I propose that synthetic data, akin to packing peanuts, can serve as a\nvaluable asset for economic prediction models, enhancing their performance and\nrobustness when integrated with real data. This hybrid approach proves\nparticularly beneficial in scenarios where data is either missing or limited in\navailability. Through the utilization of Affinity credit card spending and\nWomply small business datasets, this study demonstrates the substantial\nperformance improvements achieved by employing a hybrid data approach,\nsurpassing the capabilities of traditional economic modeling techniques.\n"
    },
    {
        "paper_id": 2405.07549,
        "authors": "Tong Pu, Yifei Zhang, Yiying Zhang",
        "title": "On Joint Marginal Expected Shortfall and Associated Contribution Risk\n  Measures",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Systemic risk is the risk that a company- or industry-level risk could\ntrigger a huge collapse of another or even the whole institution. Various\nsystemic risk measures have been proposed in the literature to quantify the\ndomino and (relative) spillover effects induced by systemic risks such as the\nwell-known CoVaR, CoES, MES and CoD risk measures, and associated contribution\nmeasures. This paper proposes another new type of systemic risk measure, called\nthe joint marginal expected shortfall (JMES), to measure whether the MES of one\nentity's risk-taking adds to another one or the overall risk conditioned on the\nevent that the entity is already in some specified distress level. We further\nintroduce two useful systemic risk contribution measures based on the\ndifference function or relative ratio function of the JMES and the conventional\nES, respectively. Some basic properties of these proposed measures are studied\nsuch as monotonicity, comonotonic additivity, non-identifiability and\nnon-elicitability. For both risk measures and two different vectors of\nbivariate risks, we establish sufficient conditions imposed on copula\nstructure, stress levels, and stochastic orders to compare these new measures.\nWe further provide some numerical examples to illustrate our main findings. A\nreal application in analyzing the risk contagion among several stock market\nindices is implemented to show the performances of our proposed measures\ncompared with other commonly used measures including CoVaR, CoES, MES, and\ntheir associated contribution measures.\n"
    },
    {
        "paper_id": 2405.07713,
        "authors": "Dorsaf Cherif and Emmanuel Lepinette",
        "title": "No-arbitrage conditions and pricing from discrete-time to\n  continuous-time strategies",
        "comments": null,
        "journal-ref": "Annals of Finance (2023)",
        "doi": "10.1007/s10436-023-00426-1",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper, a general framework is developed for continuous-time financial\nmarket models defined from simple strategies through conditional topologies\nthat avoid stochastic calculus and do not necessitate semimartingale models. We\nthen compare the usual no-arbitrage conditions of the literature, e.g. the\nusual no-arbitrage conditions NFL, NFLVR and NUPBR and the recent AIP\ncondition. With appropriate pseudo-distance topologies, we show that they hold\nin continuous time if and only if they hold in discrete time. Moreover, the\nsuper-hedging prices in continuous time coincide with the discrete-time\nsuper-hedging prices, even without any no-arbitrage condition.\n"
    },
    {
        "paper_id": 2405.07998,
        "authors": "Yazhou Niu",
        "title": "How does ClinicalTrials.gov Impact Company Innovation?",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Pharmaceutical companies may have incentives to exaggerate the therapeutic\neffects of their developed products during the clinical stage, which endangers\nthe health of patients. To increase transparency in clinical practice, the NIH\nestablished ClinicalTrials.gov in 2000, which indicates a significant impact on\nmedicine. However, little evidence shows how ClinicalTrials.gov affects medical\nenterprises innovation. By identifying the patent application activities\nthrough USPTO, Pubmed, and Compustat, we used coherent DID to prove the impact\nof ClinicalTrials.gov on innovation. We found that the emergence of\nClinicalTrials.gov reduced the number of patent applications and led to a shift\nin RD directions. This effect can also be moderated depending on firm size,\nprobably because small companies are more incentivized to manipulate data.\nHence, we suggest agencies could consider wide-ranging influences when\nformulating open science policies.\n"
    },
    {
        "paper_id": 2405.0803,
        "authors": "Maya M. Durvasula, Sabri Eyuboglu, David M. Ritzwoller",
        "title": "Medical Research as a Productivity Indicator",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  A recent literature contends that the returns to research in medicine are\ndeclining. This conclusion is premised on two empirical findings: the quantity\nof medical research is rising, while improvements in health outcomes are\nstagnant. We argue that the first finding is an artifact of conceptual and\nempirical problems in the measurement of research. We propose a method for\nannotating unstructured text based on fine-tuning a large language model. We\napply this method to construct a census of published clinical trials. We argue\nthat clinical trials function as a natural proxy for productivity-relevant\nresearch in medicine. In our data, the quantity, quality, and composition of\nclinical trials have been stable since 2010. We find that measured increases in\nthe total quantity of medical research are driven by shifts in the composition\nof scientific publications that do not correspond to real increases in the\nquantity of productivity-relevant research. We document large increases in the\ngrowth of non-trial publications, driven by publications from China and\npublications that review existing literature. This increase in quantity is\ncoincident with a decline in various measures of average publication quality.\nTaken together, we find no support for declining medical research productivity\nover this period.\n"
    },
    {
        "paper_id": 2405.08045,
        "authors": "Theodoros Zafeiriou, Dimitris Kalles",
        "title": "Comparative analysis of neural network architectures for short-term\n  FOREX forecasting",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The present document delineates the analysis, design, implementation, and\nbenchmarking of various neural network architectures within a short-term\nfrequency prediction system for the foreign exchange market (FOREX). Our aim is\nto simulate the judgment of the human expert (technical analyst) using a system\nthat responds promptly to changes in market conditions, thus enabling the\noptimization of short-term trading strategies. We designed and implemented a\nseries of LSTM neural network architectures which are taken as input the\nexchange rate values and generate the short-term market trend forecasting\nsignal and an ANN custom architecture based on technical analysis indicator\nsimulators We performed a comparative analysis of the results and came to\nuseful conclusions regarding the suitability of each architecture and the cost\nin terms of time and computational power to implement them. The ANN custom\narchitecture produces better prediction quality with higher sensitivity using\nfewer resources and spending less time than LSTM architectures. The ANN custom\narchitecture appears to be ideal for use in low-power computing systems and for\nuse cases that need fast decisions with the least possible computational cost.\n"
    },
    {
        "paper_id": 2405.08047,
        "authors": "Yizun Lin, Yangyu Zhang, Zhao-Rong Lai and Cheng Li",
        "title": "Autonomous Sparse Mean-CVaR Portfolio Optimization",
        "comments": "ICML 2024",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  The $\\ell_0$-constrained mean-CVaR model poses a significant challenge due to\nits NP-hard nature, typically tackled through combinatorial methods\ncharacterized by high computational demands. From a markedly different\nperspective, we propose an innovative autonomous sparse mean-CVaR portfolio\nmodel, capable of approximating the original $\\ell_0$-constrained mean-CVaR\nmodel with arbitrary accuracy. The core idea is to convert the $\\ell_0$\nconstraint into an indicator function and subsequently handle it through a\ntailed approximation. We then propose a proximal alternating linearized\nminimization algorithm, coupled with a nested fixed-point proximity algorithm\n(both convergent), to iteratively solve the model. Autonomy in sparsity refers\nto retaining a significant portion of assets within the selected asset pool\nduring adjustments in pool size. Consequently, our framework offers a\ntheoretically guaranteed approximation of the $\\ell_0$-constrained mean-CVaR\nmodel, improving computational efficiency while providing a robust asset\nselection scheme.\n"
    },
    {
        "paper_id": 2405.08052,
        "authors": "S M Toufiqul Huq Sowrov",
        "title": "Trade Openness, Tariffs and Economic Growth: An Empirical Study from\n  Countries of G-20",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/publicdomain/zero/1.0/",
        "abstract": "  International trade has been in the forefront of economic development and\ngrowth debates. Trade openness, its definition, scope, and impacts have also\nbeen studied numerously. Tariff has been dubbed as negative influencer of\neconomic growth as per conventional wisdom and most empirical studies. This\npaper empirically examines relationships among trade openness as trade share to\nGDP, import tariff rate and economic growth. Panel dataset of 11 G-20 member\ncountries were selected for the study. Results found a positively significant\ncorrelation between trade openness and economic growth. Tariff has negatively\nsignificant correlation with economic growth in lagged model. OLS and panel\ndata fixed-effects regression were employed to carry out the regression\nanalysis. To deal with endogeneity in trade openness variable, a 1-year lag\nregression technique was conducted. Results are robust and significant. Policy\nrecommendation suggests country specific trade opening and tariff relaxation.\n"
    },
    {
        "paper_id": 2405.08089,
        "authors": "Ali Mohammadjafari",
        "title": "Comparative Study of Bitcoin Price Prediction",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/publicdomain/zero/1.0/",
        "abstract": "  Prediction of stock prices has been a crucial and challenging task,\nespecially in the case of highly volatile digital currencies such as Bitcoin.\nThis research examineS the potential of using neural network models, namely\nLSTMs and GRUs, to forecast Bitcoin's price movements. We employ five-fold\ncross-validation to enhance generalization and utilize L2 regularization to\nreduce overfitting and noise. Our study demonstrates that the GRUs models offer\nbetter accuracy than LSTMs model for predicting Bitcoin's price. Specifically,\nthe GRU model has an MSE of 4.67, while the LSTM model has an MSE of 6.25 when\ncompared to the actual prices in the test set data. This finding indicates that\nGRU models are better equipped to process sequential data with long-term\ndependencies, a characteristic of financial time series data such as Bitcoin\nprices. In summary, our results provide valuable insights into the potential of\nneural network models for accurate Bitcoin price prediction and emphasize the\nimportance of employing appropriate regularization techniques to enhance model\nperformance.\n"
    },
    {
        "paper_id": 2405.08101,
        "authors": "G. Ibikunle, B. Moews, K. Rzayev",
        "title": "Can machine learning unlock new insights into high-frequency trading?",
        "comments": "56 pages, 4 figures, 8 tables",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  We design and train machine learning models to capture the nonlinear\ninteractions between financial market dynamics and high-frequency trading (HFT)\nactivity. In doing so, we introduce new metrics to identify liquidity-demanding\nand -supplying HFT strategies. Both types of HFT strategies increase activity\nin response to information events and decrease it when trading speed is\nrestricted, with liquidity-supplying strategies demonstrating greater\nresponsiveness. Liquidity-demanding HFT is positively linked with latency\narbitrage opportunities, whereas liquidity-supplying HFT is negatively related,\naligning with theoretical expectations. Our metrics have implications for\nunderstanding the information production process in financial markets.\n"
    },
    {
        "paper_id": 2405.08159,
        "authors": "Ariel Ortiz-Bobea, Robert G. Chambers, Yurou He and David B. Lobell",
        "title": "Large increases in public R&D investment are needed to avoid declines of\n  US agricultural productivity",
        "comments": "Main text: 19 pages, 4 figures. Supplementary material: 47 pages, 20\n  figures, 13 tables",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
        "abstract": "  Increasing agricultural productivity is a gradual process with significant\ntime lags between research and development (R&D) investment and the resulting\ngains. We estimate the response of US agricultural Total Factor Productivity\n(TFP) to both R&D investment and weather, and quantify the public R&D spending\nrequired to offset the emerging impacts of climate change. We find that\noffsetting the climate-induced productivity slowdown by 2050 alone requires a\nsustained public R&D spending growth of 5.2-7.8% per year over 2021-2050. This\namounts to an additional $208-$434B investment over this period. These are\nsubstantial requirements comparable to the public R&D spending growth that\nfollowed the two World Wars.\n"
    },
    {
        "paper_id": 2405.0816,
        "authors": "Thomas Stratmann, Markus Bjoerkheim, Christopher Koopman",
        "title": "The Causal Effect of Repealing Certificate-of-Need Laws for Ambulatory\n  Surgical Centers: Does Access to Medical Services Increase?",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  In many states, certificate-of-need (CON) laws prevent ambulatory surgical\ncenters (ASCs) from entering the market or expanding their services. This paper\nestimates the causal effects of state ASC-CON law repeal on the accessibility\nof medical services statewide, as well as for rural areas. Our findings show\nthat CON law repeals increase ASCs per capita by 44-47% statewide and 92-112%\nin rural areas. Repealing ASC-CON laws causes a continuous increase in ASCs per\ncapita, an effect which levels off ten years after repeal. Contrary to the\n'cream-skimming' hypothesis, we find no evidence that CON repeal is associated\nwith hospital closures in rural areas. Rather, some regression models show that\nrepeal is associated with fewer medical service reductions.\n"
    },
    {
        "paper_id": 2405.08168,
        "authors": "Vitor Melo, Liam Sigaud, Elijah Neilson, Markus Bjoerkheim",
        "title": "Rural Healthcare Access and Supply Constraints: A Causal Analysis",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Certificate-of-need (CON) laws require that healthcare providers receive\napproval from a state board before offering additional services in a given\ncommunity. Proponents of CON laws claim that these laws are needed to prevent\nthe oversupply of healthcare services in urban areas and to increase access in\nrural areas, which are predominantly underserved. Yet, the policy could lower\nrural access if used by incumbents to limit entry from competitors. We explore\nthe repeal of these regulations in five U.S. states to offer the first estimate\nof the causal effects of CON laws on rural and urban healthcare access. We find\nthat repealing CON laws causes a substantial increase in hospitals in both\nrural and urban areas. We also find that the repeal leads to fewer beds and\nsmaller hospitals on average, suggesting an increase in entry and competition\nin both rural and urban areas.\n"
    },
    {
        "paper_id": 2405.08602,
        "authors": "Reilly Pickard, F. Wredenhagen, and Y. Lawryshyn",
        "title": "Optimizing Deep Reinforcement Learning for American Put Option Hedging",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  This paper contributes to the existing literature on hedging American options\nwith Deep Reinforcement Learning (DRL). The study first investigates\nhyperparameter impact on hedging performance, considering learning rates,\ntraining episodes, neural network architectures, training steps, and\ntransaction cost penalty functions. Results highlight the importance of\navoiding certain combinations, such as high learning rates with a high number\nof training episodes or low learning rates with few training episodes and\nemphasize the significance of utilizing moderate values for optimal outcomes.\nAdditionally, the paper warns against excessive training steps to prevent\ninstability and demonstrates the superiority of a quadratic transaction cost\npenalty function over a linear version. This study then expands upon the work\nof Pickard et al. (2024), who utilize a Chebyshev interpolation option pricing\nmethod to train DRL agents with market calibrated stochastic volatility models.\nWhile the results of Pickard et al. (2024) showed that these DRL agents achieve\nsatisfactory performance on empirical asset paths, this study introduces a\nnovel approach where new agents at weekly intervals to newly calibrated\nstochastic volatility models. Results show DRL agents re-trained using weekly\nmarket data surpass the performance of those trained solely on the sale date.\nFurthermore, the paper demonstrates that both single-train and weekly-train DRL\nagents outperform the Black-Scholes Delta method at transaction costs of 1% and\n3%. This practical relevance suggests that practitioners can leverage readily\navailable market data to train DRL agents for effective hedging of options in\ntheir portfolios.\n"
    },
    {
        "paper_id": 2405.08822,
        "authors": "Zongxia Liang, Qi Ye",
        "title": "Despite Absolute Information Advantages, All Investors Incur Welfare\n  Loss",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  This paper delves into financial markets that incorporate a novel form of\nheterogeneity among investors, specifically in terms of their beliefs regarding\nthe reliability of signals in the business cycle economy model, which may be\nbiased. Unlike most papers in this field, we not only analyze the equilibrium\nbut also examine welfare using objective measures while investors aim to\nmaximize their utility based on subjective measures. Furthermore, we introduce\npassive investors and use their utility as a benchmark, thereby revealing the\nphenomenon of double loss sometimes. In the analysis, we examine two effects:\nthe distortion effect on total welfare and the advantage effect of information\nand highlight their key factors of influence, with a particular emphasis on the\nproportion of investors. We also demonstrate that manipulating investors'\nestimation towards the economy can be a way to improve utility and identify an\ninner connection between welfare and survival.\n"
    },
    {
        "paper_id": 2405.09023,
        "authors": "Rubing Li and Arun Sundararajan",
        "title": "The Rise of Recommerce: Ownership and Sustainability with Overlapping\n  Generations",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  The emergence of the branded recommerce channel - digitally enabled and\nbranded marketplaces that facilitate purchasing pre-owned items directly from a\nmanufacturer's e-commerce site - leads to new variants of classic IS and\neconomic questions relating to secondary markets. Such branded recommerce is\nincreasingly platform-enabled, creating opportunities for greater\nsustainability and stronger brand experience control but posing a greater risk\nof cannibalization of the sales of new items. We model the effects that the\nsales of pre-owned items have on market segmentation and product durability\nchoices for a monopolist facing heterogeneous customers, contrasting outcomes\nwhen the trade of pre-owned goods takes place through a third-party marketplace\nwith outcomes under branded recommerce. We show that the direct revenue\nbenefits of branded recommerce are not their primary source of value to the\nmonopolist, and rather, there are three indirect effects that alter profits and\nsustainability. Product durability increases, a seller finds it optimal to\nforgo marketplace fees altogether, and there are greater seller incentives to\nlower the quality uncertainty associated with pre-owned items. We establish\nthese results for a simple two-period model as well as developing a new\ninfinite horizon model with overlapping generations. Our paper sheds new\ninsight into this emerging digital channel phenomenon, underscoring the\nimportance of recommerce platforms in aligning seller profits with\nsustainability goals.\n"
    },
    {
        "paper_id": 2405.09161,
        "authors": "Jochen Wulf and Juerg Meierhofer",
        "title": "Exploring the Potential of Large Language Models for Automation in\n  Technical Customer Service",
        "comments": null,
        "journal-ref": "Proceedings of the Spring Servitization Conference (SSC2024)",
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  Purpose: The purpose of this study is to investigate the potential of Large\nLanguage Models (LLMs) in transforming technical customer service (TCS) through\nthe automation of cognitive tasks. Design/Methodology/Approach: Using a\nprototyping approach, the research assesses the feasibility of automating\ncognitive tasks in TCS with LLMs, employing real-world technical incident data\nfrom a Swiss telecommunications operator. Findings: Lower-level cognitive tasks\nsuch as translation, summarization, and content generation can be effectively\nautomated with LLMs like GPT-4, while higher-level tasks such as reasoning\nrequire more advanced technological approaches such as Retrieval-Augmented\nGeneration (RAG) or finetuning ; furthermore, the study underscores the\nsignificance of data ecosystems in enabling more complex cognitive tasks by\nfostering data sharing among various actors involved. Originality/Value: This\nstudy contributes to the emerging theory on LLM potential and technical\nfeasibility in service management, providing concrete insights for operators of\nTCS units and highlighting the need for further research to address limitations\nand validate the applicability of LLMs across different domains.\n"
    },
    {
        "paper_id": 2405.0926,
        "authors": "Roger J. A. Laeven, Emanuela Rosazza Gianin, Marco Zullino",
        "title": "Geometric BSDEs",
        "comments": "In this version, we have restructured and polished v1",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We introduce and develop the concepts of Geometric Backward Stochastic\nDifferential Equations (GBSDEs, for short) and two-driver BSDEs. We demonstrate\ntheir natural suitability for modeling continuous-time dynamic return risk\nmeasures. We characterize a broad spectrum of associated, auxiliary ordinary\nBSDEs with drivers exhibiting growth rates involving terms of the form\n$y|\\ln(y)|+|z|^2/y$. We establish the existence, regularity, uniqueness, and\nstability of solutions to this rich class of ordinary BSDEs, considering both\nbounded and unbounded coefficients and terminal conditions. We exploit these\nresults to obtain analogous results for the original two-driver BSDEs. Finally,\nwe present a GBSDE framework for representing the dynamics of (robust)\n$L^{p}$-norms and related risk measures.\n"
    },
    {
        "paper_id": 2405.09339,
        "authors": "Zongxia Liang, Qi Ye",
        "title": "Optimal information acquisition for eliminating estimation risk",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  This paper diverges from previous literature by considering the utility\nmaximization problem in the context of investors having the freedom to actively\nacquire additional information to mitigate estimation risk. We derive\nclosed-form value functions using CARA and CRRA utility functions and establish\na criterion for valuing extra information through certainty equivalence, while\nalso formulating its associated acquisition cost. By strategically employing\nvariational methods, we explore the optimal acquisition of information, taking\ninto account the trade-off between its value and cost. Our findings indicate\nthat acquiring earlier information holds greater worth in eliminating\nestimation risk and achieving higher utility. Furthermore, we observe that\ninvestors with lower risk aversion are more inclined to pursue information\nacquisition.\n"
    },
    {
        "paper_id": 2405.0936,
        "authors": "Tolulope Fadina and Thorsten Schmidt",
        "title": "The Unfairness of $\\varepsilon$-Fairness",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Fairness in decision-making processes is often quantified using probabilistic\nmetrics. However, these metrics may not fully capture the real-world\nconsequences of unfairness. In this article, we adopt a utility-based approach\nto more accurately measure the real-world impacts of decision-making process.\nIn particular, we show that if the concept of $\\varepsilon$-fairness is\nemployed, it can possibly lead to outcomes that are maximally unfair in the\nreal-world context. Additionally, we address the common issue of unavailable\ndata on false negatives by proposing a reduced setting that still captures\nessential fairness considerations. We illustrate our findings with two\nreal-world examples: college admissions and credit risk assessment. Our\nanalysis reveals that while traditional probability-based evaluations might\nsuggest fairness, a utility-based approach uncovers the necessary actions to\ntruly achieve equality. For instance, in the college admission case, we find\nthat enhancing completion rates is crucial for ensuring fairness. Summarizing,\nthis paper highlights the importance of considering the real-world context when\nevaluating fairness.\n"
    },
    {
        "paper_id": 2405.09519,
        "authors": "Joseph M. Southgate, Katrina Groth, Peter Sandborn, Shapour Azarm",
        "title": "Cost-Benefit Analysis using Modular Dynamic Fault Tree Analysis and\n  Monte Carlo Simulations for Condition-based Maintenance of Unmanned Systems",
        "comments": "18 pages, 9 figures, 4 tables",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  Recent developments in condition-based maintenance (CBM) have helped make it\na promising approach to maintenance cost avoidance in engineering systems. By\nperforming maintenance based on conditions of the component with regards to\nfailure or time, there is potential to avoid the large costs of system shutdown\nand maintenance delays. However, CBM requires a large investment cost compared\nto other available maintenance strategies. The investment cost is required for\nresearch, development, and implementation. Despite the potential to avoid\nsignificant maintenance costs, the large investment cost of CBM makes decision\nmakers hesitant to implement. This study is the first in the literature that\nattempts to address the problem of conducting a cost-benefit analysis (CBA) for\nimplementing CBM concepts for unmanned systems. This paper proposes a method\nfor conducting a CBA to determine the return on investment (ROI) of potential\nCBM strategies. The CBA seeks to compare different CBM strategies based on the\ndifferences in the various maintenance requirements associated with maintaining\na multi-component, unmanned system. The proposed method uses modular dynamic\nfault tree analysis (MDFTA) with Monte Carlo simulations (MCS) to assess the\nvarious maintenance requirements. The proposed method is demonstrated on an\nunmanned surface vessel (USV) example taken from the literature that consists\nof 5 subsystems and 71 components. Following this USV example, it is found that\nselecting different combinations of components for a CBM strategy can have a\nsignificant impact on maintenance requirements and ROI by impacting cost\navoidances and investment costs.\n"
    },
    {
        "paper_id": 2405.09747,
        "authors": "Raeid Saqur, Ken Kato, Nicholas Vinden and Frank Rudzicz",
        "title": "NIFTY Financial News Headlines Dataset",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  We introduce and make publicly available the NIFTY Financial News Headlines\ndataset, designed to facilitate and advance research in financial market\nforecasting using large language models (LLMs). This dataset comprises two\ndistinct versions tailored for different modeling approaches: (i) NIFTY-LM,\nwhich targets supervised fine-tuning (SFT) of LLMs with an auto-regressive,\ncausal language-modeling objective, and (ii) NIFTY-RL, formatted specifically\nfor alignment methods (like reinforcement learning from human feedback (RLHF))\nto align LLMs via rejection sampling and reward modeling. Each dataset version\nprovides curated, high-quality data incorporating comprehensive metadata,\nmarket indices, and deduplicated financial news headlines systematically\nfiltered and ranked to suit modern LLM frameworks. We also include experiments\ndemonstrating some applications of the dataset in tasks like stock price\nmovement and the role of LLM embeddings in information acquisition/richness.\nThe NIFTY dataset along with utilities (like truncating prompt's context length\nsystematically) are available on Hugging Face at\nhttps://huggingface.co/datasets/raeidsaqur/NIFTY.\n"
    },
    {
        "paper_id": 2405.09764,
        "authors": "Thibaut Mastrolia and Tianrui Xu",
        "title": "Clearing time randomization and transaction fees for auction market\n  design",
        "comments": "30 pages, 11 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Flaws of a continuous limit order book mechanism raise the question of\nwhether a continuous trading session and a periodic auction session would bring\nbetter efficiency. This paper wants to go further in designing a periodic\nauction when both a continuous market and a periodic auction market are\navailable to traders. In a periodic auction, we discover that a strategic\ntrader could take advantage of the accumulated information available along the\nauction duration by arriving at the latest moment before the auction closes,\nincreasing the price impact on the market. Such price impact moves the clearing\nprice away from the efficient price and may disturb the efficiency of a\nperiodic auction market. We thus propose and quantify the effect of two\nremedies to mitigate these flaws: randomizing the auction's closing time and\noptimally designing a transaction fees policy. Our results show that these\npolicies encourage a strategic trader to send their orders earlier to enhance\nthe efficiency of the auction market, illustrated by data extracted from\nAlphabet and Apple stocks.\n"
    },
    {
        "paper_id": 2405.09766,
        "authors": "Niushan Gao, Foivos Xanthos",
        "title": "A note on continuity and consistency of measures of risk and variability",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this short note, we show that every convex, order bounded above functional\non a Banach lattice is automatically norm continuous. This improves a result in\n\\cite{RS06} and applies to many deviation and variability measures. We also\nshow that an order-continuous, law-invariant functional on an Orlicz space is\nstrongly consistent everywhere, extending a result in \\cite{KSZ14}.\n"
    },
    {
        "paper_id": 2405.09929,
        "authors": "Samuel Forbes",
        "title": "The $\\kappa$-generalised Distribution for Stock Returns",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Empirical evidence shows stock returns are often heavy-tailed rather than\nnormally distributed. The $\\kappa$-generalised distribution, originated in the\ncontext of statistical physics by Kaniadakis, is characterised by the\n$\\kappa$-exponential function that is asymptotically exponential for small\nvalues and asymptotically power law for large values. This proves to be a\nuseful property and makes it a good candidate distribution for many types of\nquantities. In this paper we focus on fitting historic daily stock returns for\nthe FTSE 100 and the top 100 Nasdaq stocks. Using a Monte-Carlo goodness of fit\ntest there is evidence that the $\\kappa$-generalised distribution is a good fit\nfor a significant proportion of the 200 stock returns analysed.\n"
    },
    {
        "paper_id": 2405.09984,
        "authors": "N.S. Gonchar",
        "title": "Mode of sustainable economic development",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  To implement the previously formulated principles of sustainable economic\ndevelopment, all non-negative solutions of the linear system of equations and\ninequalities, which are satisfied by the vector of real consumption, are\ncompletely described. It is established that the vector of real consumption\nwith the minimum level of excess supply is determined by the solution of some\nquadratic programming problem. The necessary and sufficient conditions are\nestablished under which the economic system, described by the \"input-output\"\nproduction model, functions in the mode of sustainable development. A complete\ndescription of the equilibrium states for which markets are partially cleared\nin the economy model of production \"input-output\" is given, on the basis that\nall solutions of system of linear equations and inequalities are completely\ndescribed. The existence of a family of taxation vectors in the \"input-output\"\nmodel of production, under which the economic system is able to function in the\nmode of sustainable development, is proved. Restrictions were found for the\nvector of taxation in the economic system, under which the economic system is\nable to function in the mode of sustainable development. The axioms of the\naggregated description of the economy is proposed.\n"
    },
    {
        "paper_id": 2405.10231,
        "authors": "Marit Hinnosaar and Toomas Hinnosaar",
        "title": "Influencer Cartels",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Social media influencers account for a growing share of marketing worldwide.\nWe demonstrate the existence of a novel form of market failure in this\nadvertising market: influencer cartels, where groups of influencers collude to\nincrease their advertising revenue by inflating their engagement. Our\ntheoretical model shows that influencer cartels can improve consumer welfare if\nthey expand social media engagement to the target audience, or reduce welfare\nif they divert engagement to less relevant audiences. We validate the model\nempirically using novel data on influencer cartels combined with machine\nlearning tools, and derive policy implications for how to maximize consumer\nwelfare.\n"
    },
    {
        "paper_id": 2405.10338,
        "authors": "Pierre Gosselin (IF), A\\\"ileen Lotz",
        "title": "Financial Interactions and Capital Accumulation",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In a series of precedent papers, we have presented a comprehensive\nmethodology, termed Field Economics, for translating a standard economic model\ninto a statistical field-formalism framework. This formalism requires a large\nnumber of heterogeneous agents, possibly of different types. It reveals the\nemergence of collective states among these agents or type of agents while\npreserving the interactions and microeconomic features of the system at the\nindividual level. In two prior papers, we applied this formalism to analyze the\ndynamics of capital allocation and accumulation in a simple microeconomic\nframework of investors and firms.Building upon our prior work, the present\npaper refines the initial model by expanding its scope. Instead of considering\nfinancial firms investing solely in real sectors, we now suppose that financial\nagents may also invest in other financial firms. We also introduce banks in the\nsystem that act as investors with a credit multiplier. Two types of interaction\nare now considered within the financial sector: financial agents can lend\ncapital to, or choose to buy shares of, other financial firms. Capital now\nflows between financial agents and is only partly invested in real sectors,\ndepending on their relative returns. We translate this framework into our\nformalism and study the diffusion of capital and possible defaults in the\nsystem, both at the macro and micro level.At the macro level, we find that\nseveral collective states may emerge, each characterized by a distinct level of\naverage capital and investors per sector. These collective states depend on\nexternal parameters such as level of connections between investors or firms'\nproductivity.The multiplicity of possible collective states is the consequence\nof the nature of the system composed of interconnected heterogeneous agents.\nSeveral equivalent patterns of returns and portfolio allocation may emerge. The\nmultiple collective states induce the unstable nature of financial markets, and\nsome of them include defaults may emerge. At the micro level, we study the\npropagation of returns and defaults within a given collective state. Our\nfindings highlight the significant role of banks, which can either stabilize\nthe system through lending activities or propagate instability through loans to\ninvestors.\n"
    },
    {
        "paper_id": 2405.10449,
        "authors": "David Ardia and Keven Bluteau",
        "title": "Optimal Text-Based Time-Series Indices",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  We propose an approach to construct text-based time-series indices in an\noptimal way--typically, indices that maximize the contemporaneous relation or\nthe predictive performance with respect to a target variable, such as\ninflation. We illustrate our methodology with a corpus of news articles from\nthe Wall Street Journal by optimizing text-based indices focusing on tracking\nthe VIX index and inflation expectations. Our results highlight the superior\nperformance of our approach compared to existing indices.\n"
    },
    {
        "paper_id": 2405.10494,
        "authors": "Ege Erdil, Tamay Besiroglu, Anson Ho",
        "title": "Estimating Idea Production: A Methodological Survey",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Accurately modeling the production of new ideas is crucial for innovation\ntheory and endogenous growth models. This paper provides a comprehensive\nmethodological survey of strategies for estimating idea production functions.\nWe explore various methods, including naive approaches, linear regression,\nmaximum likelihood estimation, and Bayesian inference, each suited to different\ndata availability settings. Through case studies ranging from total factor\nproductivity to software R&D, we show how to apply these methodologies in\npractice. Our synthesis provides researchers with guidance on strategies for\ncharacterizing idea production functions and highlights obstacles that must be\naddressed through further empirical validation.\n"
    },
    {
        "paper_id": 2405.10498,
        "authors": "Pranjal Rawat",
        "title": "A Deep Learning Approach to Heterogeneous Consumer Aesthetics in Retail\n  Fashion",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  In some markets, the visual appearance of a product matters a lot. This paper\ninvestigates consumer transactions from a major fashion retailer, focusing on\nconsumer aesthetics. Pretrained multimodal models convert images and text\ndescriptions into high-dimensional embeddings. The value of these embeddings is\nverified both empirically and by their ability to segment the product space. A\ndiscrete choice model is used to decompose the distinct drivers of consumer\nchoice: price, visual aesthetics, descriptive details, and seasonal variations.\nConsumers are allowed to differ in their preferences over these factors, both\nthrough observed variation in demographics and allowing for unobserved types.\nEstimation and inference employ automatic differentiation and GPUs, making it\nscalable and portable. The model reveals significant differences in price\nsensitivity and aesthetic preferences across consumers. The model is validated\nby its ability to predict the relative success of new designs and purchase\npatterns.\n"
    },
    {
        "paper_id": 2405.10539,
        "authors": "Ziyi Wang, Lijia Wei, and Lian Xue",
        "title": "Overcoming Medical Overuse with AI Assistance: An Experimental\n  Investigation",
        "comments": "35 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This study evaluates the effectiveness of Artificial Intelligence (AI) in\nmitigating medical overtreatment, a significant issue characterized by\nunnecessary interventions that inflate healthcare costs and pose risks to\npatients. We conducted a lab-in-the-field experiment at a medical school,\nutilizing a novel medical prescription task, manipulating monetary incentives\nand the availability of AI assistance among medical students using a\nthree-by-two factorial design. We tested three incentive schemes: Flat\n(constant pay regardless of treatment quantity), Progressive (pay increases\nwith the number of treatments), and Regressive (penalties for overtreatment) to\nassess their influence on the adoption and effectiveness of AI assistance. Our\nfindings demonstrate that AI significantly reduced overtreatment rates by up to\n62% in the Regressive incentive conditions where (prospective) physician and\npatient interests were most aligned. Diagnostic accuracy improved by 17% to\n37%, depending on the incentive scheme. Adoption of AI advice was high, with\napproximately half of the participants modifying their decisions based on AI\ninput across all settings. For policy implications, we quantified the monetary\n(57%) and non-monetary (43%) incentives of overtreatment and highlighted AI's\npotential to mitigate non-monetary incentives and enhance social welfare. Our\nresults provide valuable insights for healthcare administrators considering AI\nintegration into healthcare systems.\n"
    },
    {
        "paper_id": 2405.10584,
        "authors": "Huiyu Li and Junhua Hu",
        "title": "A Hybrid Deep Learning Framework for Stock Price Prediction Considering\n  the Investor Sentiment of Online Forum Enhanced by Popularity",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Stock price prediction has always been a difficult task for forecasters.\nUsing cutting-edge deep learning techniques, stock price prediction based on\ninvestor sentiment extracted from online forums has become feasible. We propose\na novel hybrid deep learning framework for predicting stock prices. The\nframework leverages the XLNET model to analyze the sentiment conveyed in user\nposts on online forums, combines these sentiments with the post popularity\nfactor to compute daily group sentiments, and integrates this information with\nstock technical indicators into an improved BiLSTM-highway model for stock\nprice prediction. Through a series of comparative experiments involving four\nstocks on the Chinese stock market, it is demonstrated that the hybrid\nframework effectively predicts stock prices. This study reveals the necessity\nof analyzing investors' textual views for stock price prediction.\n"
    },
    {
        "paper_id": 2405.10654,
        "authors": "Salma Elomari-Kessab, Guillaume Maitrier, Julius Bonart and\n  Jean-Philippe Bouchaud",
        "title": "\"Microstructure Modes\" -- Disentangling the Joint Dynamics of Prices &\n  Order Flow",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Understanding the micro-dynamics of asset prices in modern electronic order\nbooks is crucial for investors and regulators. In this paper, we use an order\nby order Eurostoxx database spanning over 3 years to analyze the joint dynamics\nof prices and order flow. In order to alleviate various problems caused by\nhigh-frequency noise, we propose a double coarse-graining procedure that allows\nus to extract meaningful information at the minute time scale. We use Principal\nComponent Analysis to construct \"microstructure modes\" that describe the most\ncommon flow/return patterns and allow one to separate them into bid-ask\nsymmetric and bid-ask anti-symmetric. We define and calibrate a Vector\nAuto-Regressive (VAR) model that encodes the dynamical evolution of these\nmodes. The parameters of the VAR model are found to be extremely stable in\ntime, and lead to relatively high $R^2$ prediction scores, especially for\nsymmetric liquidity modes. The VAR model becomes marginally unstable as more\nlags are included, reflecting the long-memory nature of flows and giving some\nfurther credence to the possibility of \"endogenous liquidity crises\". Although\nvery satisfactory on several counts, we show that our VAR framework does not\naccount for the well known square-root law of price impact.\n"
    },
    {
        "paper_id": 2405.10679,
        "authors": "Theodoros Zafeiriou, Dimitris Kalles",
        "title": "Off-the-Shelf Neural Network Architectures for Forex Time Series\n  Prediction come at a Cost",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Our study focuses on comparing the performance and resource requirements\nbetween different Long Short-Term Memory (LSTM) neural network architectures\nand an ANN specialized architecture for forex market prediction. We analyze the\nexecution time of the models as well as the resources consumed, such as memory\nand computational power. Our aim is to demonstrate that the specialized\narchitecture not only achieves better results in forex market prediction but\nalso executes using fewer resources and in a shorter time frame compared to\nLSTM architectures. This comparative analysis will provide significant insights\ninto the suitability of these two types of architectures for time series\nprediction in the forex market environment.\n"
    },
    {
        "paper_id": 2405.10762,
        "authors": "Yu Cheng, Qin Yang, Liyang Wang, Ao Xiang, Jingyu Zhang",
        "title": "Research on Credit Risk Early Warning Model of Commercial Banks Based on\n  Neural Network Algorithm",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In the realm of globalized financial markets, commercial banks are confronted\nwith an escalating magnitude of credit risk, thereby imposing heightened\nrequisites upon the security of bank assets and financial stability. This study\nharnesses advanced neural network techniques, notably the Backpropagation (BP)\nneural network, to pioneer a novel model for preempting credit risk in\ncommercial banks. The discourse initially scrutinizes conventional financial\nrisk preemptive models, such as ARMA, ARCH, and Logistic regression models,\ncritically analyzing their real-world applications. Subsequently, the\nexposition elaborates on the construction process of the BP neural network\nmodel, encompassing network architecture design, activation function selection,\nparameter initialization, and objective function construction. Through\ncomparative analysis, the superiority of neural network models in preempting\ncredit risk in commercial banks is elucidated. The experimental segment selects\nspecific bank data, validating the model's predictive accuracy and\npracticality. Research findings evince that this model efficaciously enhances\nthe foresight and precision of credit risk management.\n"
    },
    {
        "paper_id": 2405.10851,
        "authors": "Hong Yuan and Minda Ma",
        "title": "Bottom-up approach to assess carbon emissions of battery electric\n  vehicle operations in China",
        "comments": "6 pages, 6 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  The transportation sector is the third-largest global energy consumer and\nemitter, making it a focal point in the transition toward the net-zero future.\nTo accelerate the decarbonization of passenger cars, this work is the first to\npropose a bottom-up charging demand model to estimate the operational\nelectricity use and associated carbon emissions of best-selling battery\nelectric vehicles (BEVs) in various climate zones in China during the 2020s.\nThe findings reveal that (1) the operational energy demand of the top-20\nselling BEV models in China, such as Tesla, Wuling Hongguang, and BYD,\nincreased from 601 to 3054 giga-watt hours (GWh) during 2020-2022, with BEVs in\nSouth China contributing more than half of the total electricity demand; (2)\nfrom 2020 to 2022, the energy and carbon intensities of the best-selling models\ndecreased from 1364 to 1095 kilowatt-hour per vehicle and from 797 to 621\nkilograms of carbon dioxide (CO2) per vehicle, respectively, with North China\nexperiencing the highest intensity decline compared to that in other regions;\nand (3) the operational energy demand of BEV stocks in China increased from\n4774 to 12,048 GWh during 2020-2022, while the carbon emissions of BEV stocks\nrose to 6.8 mega-tons of CO2 in 2022, reflecting an annual growth rate of ~50%.\nIn summary, this work delves into the examination and contrast of benchmark\ndata on a nation-regional scale, as well as performance metrics related to BEV\nchargings. The primary aim is to support nationwide efforts in decarbonization,\naiming for carbon mitigation and facilitating the swift evolution of passenger\ncars toward a carbon-neutral future.\n"
    },
    {
        "paper_id": 2405.10884,
        "authors": "Jos\\'e-Ignacio Ant\\'on, Juan Ponce and Rafael Mu\\~noz de Bustillo",
        "title": "Road to perdition? The effect of illicit drug use on labour market\n  outcomes of prime-age men in Mexico",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  This study addresses the impact of illicit drug use on the labour market\noutcomes of men in Mexico. We leverage statistical information from three waves\nof a comparable national survey and make use of Lewbel's\nheteroskedasticity-based instrumental variable strategy to deal with the\nendogeneity of drug consumption. Our results suggests that drug consumption has\nquite negative effects in the Mexican context: It reduces employment,\noccupational attainment and formality and raises unemployment of local men.\nThese effects seem larger than those estimated for high-income economies\n"
    },
    {
        "paper_id": 2405.10917,
        "authors": "Shuxin Guo, Qiang Liu",
        "title": "Is the annualized compounded return of Medallion over 35%?",
        "comments": "9 pages, 2 tables",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  It is a challenge to estimate fund performance by compounded returns.\nArguably, it is incorrect to use yearly returns directly for compounding, with\nreported annualized return of above 60% for Medallion for the 31 years up to\n2018. We propose an estimation based on fund sizes and trading profits and\nobtain a compounded return of 32.6% before fees with a 3% financing rate.\nAlternatively, we suggest using the manager's wealth as a proxy and arriving at\na compounded growth rate of 25.6% for Simons for the 33 years up to 2020. We\nconclude that the annualized compounded return of Medallion before fees is\nprobably under 35%. Our findings have implications for how to compute fund\nperformance correctly.\n"
    },
    {
        "paper_id": 2405.1092,
        "authors": "Shuxin Guo, Qiang Liu",
        "title": "Data-generating process and time-series asset pricing",
        "comments": "43 pages, 9 tables, 1 figure",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  We study the data-generating processes for factors expressed in return\ndifferences, which the literature on time-series asset pricing seems to have\noverlooked. For the factors' data-generating processes or long-short zero-cost\nportfolios, a meaningful definition of returns is impossible; further, the\ncompounded market factor (MF) significantly underestimates the return\ndifference between the market and the risk-free rate compounded separately.\nSurprisingly, if MF were treated coercively as periodic-rebalancing long-short\n(i.e., the same as size and value), Fama-French three-factor (FF3) would be\neconomically unattractive for lacking compounding and irrelevant for suffering\nfrom the small \"size of an effect.\" Otherwise, FF3 might be misspecified if MF\nwere buy-and-hold long-short. Finally, we show that OLS with net returns for\nsingle-index models leads to inflated alphas, exaggerated t-values, and\noverestimated Sharpe ratios (SR); worse, net returns may lead to pathological\nalphas and SRs. We propose defining factors (and SRs) with non-difference\ncompound returns.\n"
    },
    {
        "paper_id": 2405.11329,
        "authors": "Qiang Liu, Yuhan Jiao, Shuxin Guo",
        "title": "Risk-neutral valuation of options under arithmetic Brownian motions",
        "comments": "19 pages, 4 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  On April 22, 2020, the CME Group switched to Bachelier pricing for a group of\noil futures options. The Bachelier model, or more generally the arithmetic\nBrownian motion (ABM), is not so widely used in finance, though. This paper\nprovides the first comprehensive survey of options pricing under ABM. Using the\nrisk-neutral valuation, we derive formulas for European options for three\nunderlying types, namely an underlying that does not pay dividends, an\nunderlying that pays a continuous dividend yield, and futures. Further, we\nderive Black-Scholes-Merton-like partial differential equations, which can in\nprinciple be utilized to price American options numerically via finite\ndifference.\n"
    },
    {
        "paper_id": 2405.11392,
        "authors": "Yunfei Peng, Pengyu Wei, Wei Wei",
        "title": "Deep Penalty Methods: A Class of Deep Learning Algorithms for Solving\n  High Dimensional Optimal Stopping Problems",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We propose a deep learning algorithm for high dimensional optimal stopping\nproblems. Our method is inspired by the penalty method for solving free\nboundary PDEs. Within our approach, the penalized PDE is approximated using the\nDeep BSDE framework proposed by \\cite{weinan2017deep}, which leads us to coin\nthe term \"Deep Penalty Method (DPM)\" to refer to our algorithm. We show that\nthe error of the DPM can be bounded by the loss function and\n$O(\\frac{1}{\\lambda})+O(\\lambda h) +O(\\sqrt{h})$, where $h$ is the step size in\ntime and $\\lambda$ is the penalty parameter. This finding emphasizes the need\nfor careful consideration when selecting the penalization parameter and\nsuggests that the discretization error converges at a rate of order\n$\\frac{1}{2}$. We validate the efficacy of the DPM through numerical tests\nconducted on a high-dimensional optimal stopping model in the area of American\noption pricing. The numerical tests confirm both the accuracy and the\ncomputational efficiency of our proposed algorithm.\n"
    },
    {
        "paper_id": 2405.11431,
        "authors": "Jingyang Wu, Xinyi Zhang, Fangyixuan Huang, Haochen Zhou, Rohtiash\n  Chandra",
        "title": "Review of deep learning models for crypto price prediction:\n  implementation and evaluation",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  There has been much interest in accurate cryptocurrency price forecast models\nby investors and researchers. Deep Learning models are prominent machine\nlearning techniques that have transformed various fields and have shown\npotential for finance and economics. Although various deep learning models have\nbeen explored for cryptocurrency price forecasting, it is not clear which\nmodels are suitable due to high market volatility. In this study, we review the\nliterature about deep learning for cryptocurrency price forecasting and\nevaluate novel deep learning models for cryptocurrency stock price prediction.\nOur deep learning models include variants of long short-term memory (LSTM)\nrecurrent neural networks, variants of convolutional neural networks (CNNs),\nand the Transformer model. We evaluate univariate and multivariate approaches\nfor multi-step ahead predicting of cryptocurrencies close-price. We also carry\nout volatility analysis on the four cryptocurrencies which reveals significant\nfluctuations in their prices throughout the COVID-19 pandemic. Additionally, we\ninvestigate the prediction accuracy of two scenarios identified by different\ntraining sets for the models. First, we use the pre-COVID-19 datasets to model\ncryptocurrency close-price forecasting during the early period of COVID-19.\nSecondly, we utilise data from the COVID-19 period to predict prices for 2023\nto 2024. Our results show that the convolutional LSTM with a multivariate\napproach provides the best prediction accuracy in two major experimental\nsettings.\n  Our results also indicate that the multivariate deep learning models exhibit\nbetter performance in forecasting four different cryptocurrencies when compared\nto the univariate models.\n"
    },
    {
        "paper_id": 2405.11444,
        "authors": "Jonathan Ch\\'avez-Casillas, Jos\\'e E. Figueroa-L\\'opez, Chuyi Yu, and\n  Yi Zhang",
        "title": "Adaptive Optimal Market Making Strategies with Inventory Liquidation Cos",
        "comments": "A preprint of this paper was distributed under the title of \"Market\n  Making with Stochastic Liquidity Demand: Simultaneous Order Arrival and Price\n  Change Forecasts\". The present paper extends the results in the referred\n  preprint, which will remain as an unpublished manuscript",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  A novel high-frequency market-making approach in discrete time is proposed\nthat admits closed-form solutions. By taking advantage of demand functions that\nare linear in the quoted bid and ask spreads with random coefficients, we model\nthe variability of the partial filling of limit orders posted in a limit order\nbook (LOB). As a result, we uncover new patterns as to how the demand's\nrandomness affects the optimal placement strategy. We also allow the price\nprocess to follow general dynamics without any Brownian or martingale\nassumption as is commonly adopted in the literature. The most important feature\nof our optimal placement strategy is that it can react or adapt to the behavior\nof market orders online. Using LOB data, we train our model and reproduce the\nanticipated final profit and loss of the optimal strategy on a given testing\ndate using the actual flow of orders in the LOB. Our adaptive optimal\nstrategies outperform the non-adaptive strategy and those that quote limit\norders at a fixed distance from the midprice.\n"
    },
    {
        "paper_id": 2405.11455,
        "authors": "Jordan G. Taqi-Eddin",
        "title": "Impact Analysis of the Chesa Boudin Administration",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  Claims of soft-handed prosecutorial policies and increases in crime were\nprecipitating factors in the removal of Chesa Boudin as district attorney of\nthe city and county of San Francisco. However, little research has been\nconducted to empirically investigate the veracity of these indictments on the\nformer district attorney. Using regression discontinuity design (RDD), I find\nthat the Boudin administration led to a 36\\% and 21\\% reduction in monthly\nprosecutions and convictions respectively for all crimes. Moreover, his tenure\nincreased monthly successful case diversions by 58\\%. When only looking at\nviolent crimes during this period, the SFDA's office saw a 36\\% decrease, 7\\%\ndecrease, and 47\\% increase in monthly prosecutions, convictions, and\nsuccessful case diversions respectively. Although, the decrease in monthly\nconvictions was not statistically significant for the violent crimes subset.\nAdditionally, I did identify a potentially causal relationship between lower\nnumbers of prosecutions and higher levels of criminal activity, however, such\nfindings did not meet the standard for statistical significance. Finally, I\nconclude that using machine learning algorithms, such as neural networks and\nK-nearest neighbors, in place of ordinary least squares regression for the\nestimation of the reduced form equation possibly may decrease the size of the\nstandard errors of the parameters in the structural equation. However, future\nresearch needs to be conducted in this space to corroborate these initially\npromising findings.\n"
    },
    {
        "paper_id": 2405.11686,
        "authors": "Colin D. Grab",
        "title": "Exploiting Distributional Value Functions for Financial Market\n  Valuation, Enhanced Feature Creation and Improvement of Trading Algorithms",
        "comments": "22 pages, 13 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  While research of reinforcement learning applied to financial markets\npredominantly concentrates on finding optimal behaviours, it is worth to\nrealize that the reinforcement learning returns $G_t$ and state value functions\nthemselves are of interest and play a pivotal role in the evaluation of assets.\nInstead of focussing on the more complex task of finding optimal decision\nrules, this paper studies and applies the power of distributional state value\nfunctions in the context of financial market valuation and machine learning\nbased trading algorithms. Accurate and trustworthy estimates of the\ndistributions of $G_t$ provide a competitive edge leading to better informed\ndecisions and more optimal behaviour. Herein, ideas from predictive knowledge\nand deep reinforcement learning are combined to introduce a novel family of\nmodels called CDG-Model, resulting in a highly flexible framework and intuitive\napproach with minimal assumptions regarding underlying distributions. The\nmodels allow seamless integration of typical financial modelling pitfalls like\ntransaction costs, slippage and other possible costs or benefits into the model\ncalculation. They can be applied to any kind of trading strategy or asset\nclass. The frameworks introduced provide concrete business value through their\npotential in market valuation of single assets and portfolios, in the\ncomparison of strategies as well as in the improvement of market timing. They\ncan positively impact the performance and enhance the learning process of\nexisting or new trading algorithms. They are of interest from a scientific\npoint-of-view and open up multiple areas of future research. Initial\nimplementations and tests were performed on real market data. While the results\nare promising, applying a robust statistical framework to evaluate the models\nin general remains a challenge and further investigations are needed.\n"
    },
    {
        "paper_id": 2405.1173,
        "authors": "Jiahao Weng and Yan Xie",
        "title": "Degree of Irrationality: Sentiment and Implied Volatility Surface",
        "comments": "21 pages, 8 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this study, we constructed daily high-frequency sentiment data and used\nthe VAR method to attempt to predict the next day's implied volatility surface.\nWe utilized 630,000 text data entries from the East Money Stock Forum from 2014\nto 2023 and employed deep learning methods such as BERT and LSTM to build daily\nmarket sentiment indicators. By applying FFT and EMD methods for sentiment\ndecomposition, we found that high-frequency sentiment had a stronger\ncorrelation with at-the-money (ATM) options' implied volatility, while\nlow-frequency sentiment was more strongly correlated with deep out-of-the-money\n(DOTM) options' implied volatility. Further analysis revealed that the shape of\nthe implied volatility surface contains richer market sentiment information\nbeyond just market panic. We demonstrated that incorporating this sentiment\ninformation can improve the accuracy of implied volatility surface predictions.\n"
    },
    {
        "paper_id": 2405.12041,
        "authors": "Diego D\\'iaz, Pablo Paniagua, Cristi\\'an Larroulet",
        "title": "Earthquakes and the wealth of nations: The cases of Chile and New\n  Zealand",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  The consequences of natural disasters, such as earthquakes, are evident:\ndeath, coordination problems, destruction of infrastructure, and displacement\nof population. However, according to empirical research, the impact of a\nnatural disaster on economic activity is mixed. Natural disasters could have\nsignificant economic effects, especially in developing economies. This is\nparticularly important for highly seismic countries such as Chile and New\nZealand. This paper contributes to the literature on natural disasters and\neconomic development by analyzing the cases of two affected regions within\nthese countries in the wake of major earthquakes experienced during 2010-2011:\nMaule (Chile) and Canterbury (New Zealand). We examine the impact of natural\ndisasters on GDP per capita by applying the synthetic control method. Using the\nsynthetic approach, we assess the effects of these two earthquakes by building\ncounterfactuals to compare their recovery trajectories. We find that Chile and\nNew Zealand experienced opposite economic effects. The Canterbury region grew\n10% more in three years than its synthetic counterfactual without the\nearthquake, while the Maule region declined by 5%. We build synthetic controls\nat a regional and economic-sector level, looking at aggregated and sectoral\neffects. The difference in institutions, such as property rights and the large\namount of government spending given for reconstruction after the New Zealand\nearthquake relative to Chile's, help to explain the difference in outcomes.\n"
    },
    {
        "paper_id": 2405.12154,
        "authors": "Martin Herdegen, Nazem Khan and Cosimo Munari",
        "title": "Risk, utility and sensitivity to large losses",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Risk and utility functionals are fundamental building blocks in economics and\nfinance. In this paper we investigate under which conditions a risk or utility\nfunctional is sensitive to the accumulation of losses in the sense that any\nsufficiently large multiple of a position that exposes an agent to future\nlosses has positive risk or negative utility. We call this property sensitivity\nto large losses and provide necessary and sufficient conditions thereof that\nare easy to check for a very large class of risk and utility functionals. In\nparticular, our results do not rely on convexity and can therefore also be\napplied to most examples discussed in the recent literature, including\n(non-convex) star-shaped risk measures or S-shaped utility functions\nencountered in prospect theory. As expected, Value at Risk generally fails to\nbe sensitive to large losses. More surprisingly, this is also true of Expected\nShortfall. By contrast, expected utility functionals as well as (optimized)\ncertainty equivalents are proved to be sensitive to large losses for many\nstandard choices of concave and nonconcave utility functions, including\n$S$-shaped utility functions. We also show that Value at Risk and Expected\nShortfall become sensitive to large losses if they are either properly adjusted\nor if the property is suitably localized.\n"
    },
    {
        "paper_id": 2405.1224,
        "authors": "Daniel Aguilar, Minor Acu\\~na, and Breyner Chac\\'on",
        "title": "Modelo de la inflaci\\'on en Costa Rica",
        "comments": "in Spanish language",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  The stability of the inflation rate is a necessary condition for the proper\nfunctioning of any capitalist economy. In an economic environment with volatile\ninflation, the growth of the economy and its distribution among the agents of\nsociety is compromised. For this reason, and because in Costa Rica, as in most\ncapitalist nations, the monetary authority is in charge of maintaining price\nstability, it is necessary to have models capable of predicting the behavior of\ninflation. on in the country. Along these lines, this work aims to compare two\nof the main types of basic predictive models found in the literature:\nunivariate autoregressive models, exemplified by ARIMA models, and multivariate\nmodels formulated based on the theory. economics, like the Phillips curve.\n"
    },
    {
        "paper_id": 2405.12479,
        "authors": "W. Brent Lindquist, Svetlozar T. Rachev, Jagdish Gnawali, Frank J.\n  Fabozzi",
        "title": "Dynamic Asset Pricing in a Unified Bachelier-Black-Scholes-Merton Model",
        "comments": "38 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  We present a unified, market-complete model that integrates both the\nBachelier and Black-Scholes-Merton frameworks for asset pricing. The model\nallows for the study, within a unified framework, of asset pricing in a natural\nworld that experiences the possibility of negative security prices or riskless\nrates. In contrast to classical Black-Scholes-Merton, we show that option\npricing in the unified model displays a difference depending on whether the\nreplicating, self-financing portfolio uses riskless bonds or a single riskless\nbank account. We derive option price formulas and extend our analysis to the\nterm structure of interest rates by deriving the pricing of zero-coupon bonds,\nforward contracts, and futures contracts. We identify a necessary condition for\nthe unified model to support a perpetual derivative. Discrete binomial pricing\nunder the unified model is also developed. In every scenario analyzed, we show\nthat the unified model simplifies to the standard Black-Scholes-Merton pricing\nunder specific limits and provides pricing in the Bachelier model limit. We\nnote that the Bachelier limit within the unified model allows for positive\nriskless rates. The unified model prompts us to speculate on the possibility of\na mixed multiplicative and additive deflator model for risk-neutral option\npricing.\n"
    },
    {
        "paper_id": 2405.12565,
        "authors": "Yaxin Pang (CGS i3), Shenle Pan, Eric Ballot (CGS i3)",
        "title": "Resilience Analysis of Multi-modal Logistics Service Network Through\n  Robust Optimization with Budget-of-Uncertainty",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Supply chain resilience analysis aims to identify the critical elements in\nthe supply chain, measure its reliability, and analyze solutions for improving\nvulnerabilities. While extensive methods like stochastic approaches have been\ndominant, robust optimization-widely applied in robust planning under\nuncertainties without specific probability distributions-remains relatively\nunderexplored for this research problem. This paper employs robust optimization\nwith budget-of-uncertainty as a tool to analyze the resilience of multi-modal\nlogistics service networks under time uncertainty. We examine the interactive\neffects of three critical factors: network size, disruption scale, disruption\ndegree. The computational experiments offer valuable managerial insights for\npractitioners and researchers.\n"
    },
    {
        "paper_id": 2405.12608,
        "authors": "Mario Bossler, Lars Chittka, Thorsten Schank",
        "title": "A 22 percent increase in the German minimum wage: nothing crazy!",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  We present the first empirical evidence on the 22 percent increase in the\nGerman minimum wage, implemented in 2022, raising it from Euro 9.82 to 10.45 in\nJuly and to Euro 12 in October. Leveraging the German Earnings Survey, a large\nand novel data source comprising around 8 million employee-level observations\nreported by employers each month, we apply a\ndifference-in-difference-in-differences approach to analyze the policy's impact\non hourly wages, monthly earnings, employment, and working hours. Our findings\nreveal significant positive effects on wages, affirming the policy's intended\nbenefits for low-wage workers. Interestingly, we identify a negative effect on\nworking hours, mainly driven by minijobbers. The hours effect results in an\nimplied labor demand elasticity in terms of the employment volume of -0.17\nwhich only partially offsets the monthly wage gains. We neither observe a\nnegative effect on the individual's employment retention nor the regional\nemployment levels.\n"
    },
    {
        "paper_id": 2405.12743,
        "authors": "Shawn Berry",
        "title": "Consumer lying in online reviews: recent evidence",
        "comments": "16 pages, 9 tables",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  The persistence of lying by some consumers in their online posts of\nexperiences with businesses is problematic, and taints the global pool of\ninformation that is used for decision making by people that assume they are\ntrue accounts of experiences. This study is based on data from my dissertation\nabout fake online Google reviews of restaurants (Berry, 2024), and leverages an\ninstrument that quantifies the trust of people. The findings are based on a\nsample of n=351, and provide a general proxy for lying in online reviews, and\nsketch out the characteristics of a typical person that has the propensity to\nbe untruthful. A predictive model of posting untrue online reviews is\nconstructed. The findings have wider implications for the study and monitoring\nof deceptive behavior, including the propagation of misinformation, and a means\nof quantifying the potential for antisocial behavior as measured by the trust\nof people instrument in Berry (2024). Directions for future research and\nlimitations are also discussed.\n"
    },
    {
        "paper_id": 2405.12768,
        "authors": "Philippe van der Beck, Jean-Philippe Bouchaud, Dario Villamaina",
        "title": "Ponzi Funds",
        "comments": "30 Pages, 9 figures, 3 appendices",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Many active funds hold concentrated portfolios. Flow-driven trading in these\nsecurities causes price pressure, which pushes up the funds' existing positions\nresulting in realized returns. We decompose fund returns into a price pressure\n(self-inflated) and a fundamental component and show that when allocating\ncapital across funds, investors are unable to identify whether realized returns\nare self-inflated or fundamental. Because investors chase self-inflated fund\nreturns at a high frequency, even short-lived impact meaningfully affects fund\nflows at longer time scales. The combination of price impact and return chasing\ncauses an endogenous feedback loop and a reallocation of wealth to early fund\ninvestors, which unravels once the price pressure reverts. We find that flows\nchasing self-inflated returns predict bubbles in ETFs and their subsequent\ncrashes, and lead to a daily wealth reallocation of 500 Million from ETFs\nalone. We provide a simple regulatory reporting measure -- fund illiquidity --\nwhich captures a fund's potential for self-inflated returns.\n"
    },
    {
        "paper_id": 2405.12982,
        "authors": "Michele Azzone, Roberto Baviera and Pietro Manzoni",
        "title": "The puzzle of Carbon Allowance spread",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  A growing number of contributions in the literature have identified a puzzle\nin the European carbon allowance (EUA) market. Specifically, a persistent\ncost-of-carry spread (C-spread) over the risk-free rate has been observed. We\nare the first to explain the anomalous C-spread with the credit spread of the\ncorporates involved in the emission trading scheme. We obtain statistical\nevidence that the C-spread is cointegrated with both this credit spread and the\nrisk-free interest rate. This finding has a relevant policy implication: the\nmost effective solution to solve the market anomaly is including the EUA in the\nlist of European Central Bank eligible collateral for refinancing operations.\nThis change in the ECB monetary policy operations would greatly benefit the\ncarbon market and the EU green transition.\n"
    },
    {
        "paper_id": 2405.12988,
        "authors": "Ayush Singh, Anshu K. Jha and Amit N. Kumar",
        "title": "Prediction of Cryptocurrency Prices through a Path Dependent Monte Carlo\n  Simulation",
        "comments": "21 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  In this paper, our focus lies on the Merton's jump diffusion model, employing\njump processes characterized by the compound Poisson process. Our primary\nobjective is to forecast the drift and volatility of the model using a variety\nof methodologies. We adopt an approach that involves implementing different\ndrift, volatility, and jump terms within the model through various machine\nlearning techniques, traditional methods, and statistical methods on\nprice-volume data. Additionally, we introduce a path-dependent Monte Carlo\nsimulation to model cryptocurrency prices, taking into account the volatility\nand unexpected jumps in prices.\n"
    },
    {
        "paper_id": 2405.1299,
        "authors": "Edward Sharkey, Philip Treleaven",
        "title": "BERT vs GPT for financial engineering",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  The paper benchmarks several Transformer models [4], to show how these models\ncan judge sentiment from a news event. This signal can then be used for\ndownstream modelling and signal identification for commodity trading. We find\nthat fine-tuned BERT models outperform fine-tuned or vanilla GPT models on this\ntask. Transformer models have revolutionized the field of natural language\nprocessing (NLP) in recent years, achieving state-of-the-art results on various\ntasks such as machine translation, text summarization, question answering, and\nnatural language generation. Among the most prominent transformer models are\nBidirectional Encoder Representations from Transformers (BERT) and Generative\nPre-trained Transformer (GPT), which differ in their architectures and\nobjectives.\n  A CopBERT model training data and process overview is provided. The CopBERT\nmodel outperforms similar domain specific BERT trained models such as FinBERT.\nThe below confusion matrices show the performance on CopBERT & CopGPT\nrespectively. We see a ~10 percent increase in f1_score when compare CopBERT vs\nGPT4 and 16 percent increase vs CopGPT. Whilst GPT4 is dominant It highlights\nthe importance of considering alternatives to GPT models for financial\nengineering tasks, given risks of hallucinations, and challenges with\ninterpretability. We unsurprisingly see the larger LLMs outperform the BERT\nmodels, with predictive power. In summary BERT is partially the new XGboost,\nwhat it lacks in predictive power it provides with higher levels of\ninterpretability. Concluding that BERT models might not be the next XGboost\n[2], but represent an interesting alternative for financial engineering tasks,\nthat require a blend of interpretability and accuracy.\n"
    },
    {
        "paper_id": 2405.12991,
        "authors": "Mohammed Perves",
        "title": "Systematic Comparable Company Analysis and Computation of Cost of Equity\n  using Clustering",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Computing cost of equity for private corporations and performing comparable\ncompany analysis (comps) for both public and private corporations is an\nintegral but tedious and time-consuming task, with important applications\nspanning the finance world, from valuations to internal planning. Performing\ncomps traditionally often times include high ambiguity and subjectivity,\nleading to unreliability and inconsistency. In this paper, I will present a\nsystematic and faster approach to compute cost of equity for private\ncorporations and perform comps for both public and private corporations using\nspectral and agglomerative clustering. This leads to a reduction in the time\nrequired to perform comps by orders of magnitude and entire process being more\nconsistent and reliable.\n"
    },
    {
        "paper_id": 2405.12993,
        "authors": "Imran Ansari, Charu Sharma, Akshay Agrawal, Niteesh Sahni",
        "title": "A novel portfolio construction strategy based on the core-periphery\n  profile of stocks",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper highlights the significance of mesoscale structures, particularly\nthe core-periphery structure, in financial networks for portfolio optimization.\nWe build portfolios of stocks belonging to the periphery part of the Planar\nmaximally filtered subgraphs of the underlying network of stocks created from\nPearson correlations between pairs of stocks and compare its performance with\nsome well-known strategies of Pozzi et. al. hinging around the local indices of\ncentrality in terms of the Sharpe ratio, returns and standard deviation. Our\nfindings reveal that these portfolios consistently outperform traditional\nstrategies and further the core-periphery profile obtained is statistically\nsignificant across time periods. These empirical findings substantiate the\nefficacy of using the core-periphery profile of the stock market network for\nboth inter-day and intraday trading and provide valuable insights for investors\nseeking better returns.\n"
    },
    {
        "paper_id": 2405.13076,
        "authors": "Jinxin Xu, Kaixian Xu, Yue Wang, Qinyan Shen and Ruisi Li",
        "title": "A K-means Algorithm for Financial Market Risk Forecasting",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Financial market risk forecasting involves applying mathematical models,\nhistorical data analysis and statistical methods to estimate the impact of\nfuture market movements on investments. This process is crucial for investors\nto develop strategies, financial institutions to manage assets and regulators\nto formulate policy. In today's society, there are problems of high error rate\nand low precision in financial market risk prediction, which greatly affect the\naccuracy of financial market risk prediction. K-means algorithm in machine\nlearning is an effective risk prediction technique for financial market. This\nstudy uses K-means algorithm to develop a financial market risk prediction\nsystem, which significantly improves the accuracy and efficiency of financial\nmarket risk prediction. Ultimately, the outcomes of the experiments confirm\nthat the K-means algorithm operates with user-friendly simplicity and achieves\na 94.61% accuracy rate\n"
    },
    {
        "paper_id": 2405.13102,
        "authors": "Tommaso Cesari, Roberto Colomboni",
        "title": "Trading Volume Maximization with Online Learning",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We explore brokerage between traders in an online learning framework. At any\nround $t$, two traders meet to exchange an asset, provided the exchange is\nmutually beneficial. The broker proposes a trading price, and each trader tries\nto sell their asset or buy the asset from the other party, depending on whether\nthe price is higher or lower than their private valuations. A trade happens if\none trader is willing to sell and the other is willing to buy at the proposed\nprice. Previous work provided guidance to a broker aiming at enhancing traders'\ntotal earnings by maximizing the gain from trade, defined as the sum of the\ntraders' net utilities after each interaction. In contrast, we investigate how\nthe broker should behave to maximize the trading volume, i.e., the total number\nof trades. We model the traders' valuations as an i.i.d. process with an\nunknown distribution. If the traders' valuations are revealed after each\ninteraction (full-feedback), and the traders' valuations cumulative\ndistribution function (cdf) is continuous, we provide an algorithm achieving\nlogarithmic regret and show its optimality up to constant factors. If only\ntheir willingness to sell or buy at the proposed price is revealed after each\ninteraction ($2$-bit feedback), we provide an algorithm achieving\npoly-logarithmic regret when the traders' valuations cdf is Lipschitz and show\nthat this rate is near-optimal. We complement our results by analyzing the\nimplications of dropping the regularity assumptions on the unknown traders'\nvaluations cdf. If we drop the continuous cdf assumption, the regret rate\ndegrades to $\\Theta(\\sqrt{T})$ in the full-feedback case, where $T$ is the time\nhorizon. If we drop the Lipschitz cdf assumption, learning becomes impossible\nin the $2$-bit feedback case.\n"
    },
    {
        "paper_id": 2405.13169,
        "authors": "Michiel Kenis and Vladimir Dvorkin and Tim Schittekatte and Kenneth\n  Bruninx and Erik Delarue and Audun Botterud",
        "title": "Evaluating Offshore Electricity Market Design Considering Endogenous\n  Infrastructure Investments: Zonal or Nodal?",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Policy makers are formulating offshore energy infrastructure plans, including\nwind turbines, electrolyzers, and HVDC transmission lines. An effective market\ndesign is crucial to guide cost-efficient investments and dispatch decisions.\nThis paper jointly studies the impact of offshore market design choices on the\ninvestment in offshore electrolyzers and HVDC transmission capacity. We present\na bilevel model that incorporates investments in offshore energy\ninfrastructure, day-ahead market dispatch, and potential redispatch actions\nnear real-time to ensure transmission constraints are respected. Our findings\ndemonstrate that full nodal pricing, i.e., nodal pricing both onshore and\noffshore, outperforms the onshore zonal combined with offshore nodal pricing or\noffshore zonal layouts. While combining onshore zonal with offshore nodal\npricing can be considered as a second-best option, it generally diminishes the\nprofitability of offshore wind farms. However, if investment costs of offshore\nelectrolyzers are relatively low, they can serve as catalysts to increase the\nrevenues of the offshore wind farms. This study contributes to the\nunderstanding of market designs for highly interconnected offshore power\nsystems, offering insights into the impact of congestion pricing methodologies\non investment decisions. Besides, it is useful towards understanding the\ninteraction of offshore loads like electrolyzers with financial support\nmechanisms for offshore wind farms.\n"
    },
    {
        "paper_id": 2405.13186,
        "authors": "Ingela Alger and Jos\\'e Ignacio Rivero-Wildemauwe",
        "title": "Doing the right thing (or not) in a lemons-like situation: on the role\n  of social preferences and Kantian moral concerns",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  We conduct a laboratory experiment using framing to assess the willingness to\n``sell a lemon'', i.e., to undertake an action that benefits self but hurts the\nother (the ``buyer''). We seek to disentangle the role of other-regarding\npreferences and (Kantian) moral concerns, and to test if it matters whether the\ndecision is described in neutral terms or as a market situation. When\nevaluating an action, morally motivated individuals consider what their own\npayoff would be if -- hypothetically -- the roles were reversed and the other\nsubject chose the same action (universalization). We vary the salience of role\nuncertainty, thus varying the ease for participants to envisage the\nrole-reversal scenario.\n"
    },
    {
        "paper_id": 2405.13241,
        "authors": "Michael Gechter, Keisuke Hirano, Jean Lee, Mahreen Mahmud, Orville\n  Mondal, Jonathan Morduch, Saravana Ravindran, Abu S. Shonchoy",
        "title": "Selecting Experimental Sites for External Validity",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Policy decisions often depend on evidence generated elsewhere. We take a\nBayesian decision-theoretic approach to choosing where to experiment to\noptimize external validity. We frame external validity through a policy lens,\ndeveloping a prior specification for the joint distribution of site-level\ntreatment effects using a microeconometric structural model and allowing for\nother sources of heterogeneity. With data from South Asia, we show that,\nrelative to basing policies on experiments in optimal sites, large efficiency\nlosses result from instead using evidence from randomly-selected sites or,\nconversely, from sites with the largest expected treatment effects.\n"
    },
    {
        "paper_id": 2405.13251,
        "authors": "Daniel Aguilar and Breyner Chac\\'on",
        "title": "Valores extremos de inflaci\\'on en Costa Rica",
        "comments": "in Spanish language. arXiv admin note: text overlap with\n  arXiv:2405.12240",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  Maintaining low, non-negative and stable inflation levels is a necessary\ncondition for the stability of the economy as a whole, because the monetary\nauthorities of most industrialized countries, including the Central Bank of\nCosta Rica since 2005, they have oriented their monetary policy precisely to\nthat task. Still Thus, both in Costa Rica and internationally, most of the\nstatistical modeling of inflation has been limited to modeling their expectancy\nconditional on different covariates using linear models. This implies a lack of\nknowledge of the dynamics of the extreme values of the inflation rate and how\nthese are related with other macroeconomic variables. In Costa Rica this is of\nparticular importance since in several periods Negative quarter-on-quarter\ninflation rates have recently been experienced, which can be problematic if\nthis becomes a recurring phenomenon. Therefore, in this work we propose to\nanswer what is the relationship between the gap of GDP, inflation expectations,\nimported inflation rate, and the extreme values of the inflation rate in Costa\nRica. That is, the main objective is to determine the relationship between the\nextreme values of the the inflation rate, GDP gap, inflation expectations and\nimported inflation.\n"
    },
    {
        "paper_id": 2405.13296,
        "authors": "Markus K. Brunnermeier, Sergio Correia, Stephan Luck, Emil Verner, Tom\n  Zimmermann",
        "title": "The Debt-Inflation Channel of the German (Hyper-)Inflation",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper studies how a large increase in the price level is transmitted to\nthe real economy through firm balance sheets. Using newly digitized macro- and\nmicro-level data from the German inflation of 1919-1923, we show that inflation\nled to a large reduction in real debt burdens and bankruptcies. Firms with\nhigher nominal liabilities at the onset of inflation experienced a larger\ndecline in interest expenses, a relative increase in their equity values, and\nhigher employment during the inflation. The results are consistent with real\neffects of a debt-inflation channel that operates even when prices and wages\nare flexible.\n"
    },
    {
        "paper_id": 2405.13327,
        "authors": "Xiwang Xiang and Minda Ma",
        "title": "Monitoring the carbon emissions transition of global building end-use\n  activity",
        "comments": "7 pages, 6 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  The building sector is the largest emitter globally and as such is at the\nforefront of the net-zero emissions pathway. This study is the first to present\na bottom-up assessment framework integrated with the decomposing structural\ndecomposition method to evaluate the emission patterns and decarbonization\nprocess of global residential building operations and commercial building\noperation simultaneously over the last two decades. The results reveal that (1)\nthe average carbon intensity of global commercial building operations has\nmaintained an annual decline of 1.94% since 2000, and emission factors and\nindustrial structures were generally the key to decarbonizing commercial\nbuilding operations; (2) the operational carbon intensity of global residential\nbuildings has maintained an annual decline of 1.2% over the past two decades,\nand energy intensity and average household size have been key to this\ndecarbonization; and (3) the total decarbonization of commercial building\noperations and residential buildings worldwide was 230.28 and 338.1 mega-tons\nof carbon dioxide per yr, respectively, with a decarbonization efficiency of\n10.05% and 9.4%. Overall, this study assesses the global historical progress in\ndecarbonizing global building operations and closes the relevant gap, and it\nhelps plan the stepwise carbon neutral pathway of future global buildings by\nthe mid-century.\n"
    },
    {
        "paper_id": 2405.13341,
        "authors": "Takeshi Kato, Mohammad Rezoanul Hoque",
        "title": "Wealth inequality and utility: Effect evaluation of redistribution and\n  consumption morals using macro-econophysical coupled approach",
        "comments": "27 pages, 6 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Reducing wealth inequality and increasing utility are critical issues. This\nstudy reveals the effects of redistribution and consumption morals on wealth\ninequality and utility. To this end, we present a novel approach that couples\nthe dynamic model of capital, consumption, and utility in macroeconomics with\nthe interaction model of joint business and redistribution in econophysics.\nWith this approach, we calculate the capital (wealth), the utility based on\nconsumption, and the Gini index of these inequality using redistribution and\nconsumption thresholds as moral parameters. The results show that:\nunder-redistribution and waste exacerbate inequality; conversely,\nover-redistribution and stinginess reduce utility; and a balanced moderate\nmoral leads to achieve both reduced inequality and increased utility. These\nfindings provide renewed economic and numerical support for the moral\nimportance known from philosophy, anthropology, and religion. The revival of\nredistribution and consumption morals should promote the transformation to a\nhuman mutual-aid economy, as indicated by philosopher and anthropologist,\ninstead of the capitalist economy that has produced the current inequality. The\npractical challenge is to implement bottom-up social business, on a foothold of\nworker coops and platform cooperatives as a community against the state and the\nmarket, with moral consensus and its operation.\n"
    },
    {
        "paper_id": 2405.1339,
        "authors": "Yunzheng Lyu, Feng Bao",
        "title": "Convergence analysis of kernel learning FBSDE filter",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Kernel learning forward backward SDE filter is an iterative and adaptive\nmeshfree approach to solve the nonlinear filtering problem. It builds from\nforward backward SDE for Fokker-Planker equation, which defines evolving\ndensity for the state variable, and employs KDE to approximate density. This\nalgorithm has shown more superior performance than mainstream particle filter\nmethod, in both convergence speed and efficiency of solving high dimension\nproblems.\n  However, this method has only been shown to converge empirically. In this\npaper, we present a rigorous analysis to demonstrate its local and global\nconvergence, and provide theoretical support for its empirical results.\n"
    },
    {
        "paper_id": 2405.13422,
        "authors": "Kenan Huremovi\\'c, Federico Nutarelli, Francesco Serti, Fernando\n  Vega-Redondo",
        "title": "Learning Trade Opportunities through Production Network",
        "comments": "34 pages, 8 tables",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  Using data on the Spanish firm-level production network we show that firms\nlearn about international trade opportunities and related business know-how\nfrom their production network peers. Our identification strategy leverages the\npanel structure of the data, import origin variation, and network structure. We\nfind evidence of both upstream and downstream network effects, even after\naccounting for sectoral and geographical spillovers. Larger firms are better at\nabsorbing valuable information but worse at disseminating it. Connections with\ngeographically distant firms provide more useful information to start\nimporting.\n"
    },
    {
        "paper_id": 2405.13513,
        "authors": "Shivam Patel, Vivek Borkar",
        "title": "An Asymptotic CVaR Measure of Risk for Markov Chains",
        "comments": "9 pages, 5 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Risk sensitive decision making finds important applications in current day\nuse cases. Existing risk measures consider a single or finite collection of\nrandom variables, which do not account for the asymptotic behaviour of\nunderlying systems. Conditional Value at Risk (CVaR) is the most commonly used\nrisk measure, and has been extensively utilized for modelling rare events in\nfinite horizon scenarios. Naive extension of existing risk criteria to\nasymptotic regimes faces fundamental challenges, where basic assumptions of\nexisting risk measures fail. We present a complete simulation based approach\nfor sequentially computing Asymptotic CVaR (ACVaR), a risk measure we define on\nlimiting empirical averages of markovian rewards. Large deviations theory,\ndensity estimation, and two-time scale stochastic approximation are utilized to\ndefine a 'tilted' probability kernel on the underlying state space to\nfacilitate ACVaR simulation. Our algorithm enjoys theoretical guarantees, and\nwe numerically evaluate its performance over a variety of test cases.\n"
    },
    {
        "paper_id": 2405.13609,
        "authors": "Maximilian N\\\"agele, Jan Olle, Thomas F\\\"osel, Remmy Zen, Florian\n  Marquardt",
        "title": "Tackling Decision Processes with Non-Cumulative Objectives using\n  Reinforcement Learning",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Markov decision processes (MDPs) are used to model a wide variety of\napplications ranging from game playing over robotics to finance. Their optimal\npolicy typically maximizes the expected sum of rewards given at each step of\nthe decision process. However, a large class of problems does not fit\nstraightforwardly into this framework: Non-cumulative Markov decision processes\n(NCMDPs), where instead of the expected sum of rewards, the expected value of\nan arbitrary function of the rewards is maximized. Example functions include\nthe maximum of the rewards or their mean divided by their standard deviation.\nIn this work, we introduce a general mapping of NCMDPs to standard MDPs. This\nallows all techniques developed to find optimal policies for MDPs, such as\nreinforcement learning or dynamic programming, to be directly applied to the\nlarger class of NCMDPs. Focusing on reinforcement learning, we show\napplications in a diverse set of tasks, including classical control, portfolio\noptimization in finance, and discrete optimization problems. Given our\napproach, we can improve both final performance and training time compared to\nrelying on standard MDPs.\n"
    },
    {
        "paper_id": 2405.13753,
        "authors": "Tom S\\\"uhr, Samira Samadi, Chiara Farronato",
        "title": "A Dynamic Model of Performative Human-ML Collaboration: Theory and\n  Empirical Evidence",
        "comments": "9 Pages and appendix",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Machine learning (ML) models are increasingly used in various applications,\nfrom recommendation systems in e-commerce to diagnosis prediction in\nhealthcare. In this paper, we present a novel dynamic framework for thinking\nabout the deployment of ML models in a performative, human-ML collaborative\nsystem. In our framework, the introduction of ML recommendations changes the\ndata generating process of human decisions, which are only a proxy to the\nground truth and which are then used to train future versions of the model. We\nshow that this dynamic process in principle can converge to different stable\npoints, i.e. where the ML model and the Human+ML system have the same\nperformance. Some of these stable points are suboptimal with respect to the\nactual ground truth. We conduct an empirical user study with 1,408 participants\nto showcase this process. In the study, humans solve instances of the knapsack\nproblem with the help of machine learning predictions. This is an ideal setting\nbecause we can see how ML models learn to imitate human decisions and how this\nlearning process converges to a stable point. We find that for many levels of\nML performance, humans can improve the ML predictions to dynamically reach an\nequilibrium performance that is around 92% of the maximum knapsack value. We\nalso find that the equilibrium performance could be even higher if humans\nrationally followed the ML recommendations. Finally, we test whether monetary\nincentives can increase the quality of human decisions, but we fail to find any\npositive effect. Our results have practical implications for the deployment of\nML models in contexts where human decisions may deviate from the indisputable\nground truth.\n"
    },
    {
        "paper_id": 2405.13959,
        "authors": "Prajwal Naga, Dinesh Balivada, Sharath Chandra Nirmala, Poornoday\n  Tiruveedi",
        "title": "Decision Trees for Intuitive Intraday Trading Strategies",
        "comments": "6 pages, 5 figures, 1 table",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  This research paper aims to investigate the efficacy of decision trees in\nconstructing intraday trading strategies using existing technical indicators\nfor individual equities in the NIFTY50 index. Unlike conventional methods that\nrely on a fixed set of rules based on combinations of technical indicators\ndeveloped by a human trader through their analysis, the proposed approach\nleverages decision trees to create unique trading rules for each stock,\npotentially enhancing trading performance and saving time. By extensively\nbacktesting the strategy for each stock, a trader can determine whether to\nemploy the rules generated by the decision tree for that specific stock. While\nthis method does not guarantee success for every stock, decision treebased\nstrategies outperform the simple buy-and-hold strategy for many stocks. The\nresults highlight the proficiency of decision trees as a valuable tool for\nenhancing intraday trading performance on a stock-by-stock basis and could be\nof interest to traders seeking to improve their trading strategies.\n"
    },
    {
        "paper_id": 2405.14177,
        "authors": "Xinman Cheng, Guanxing Fu, Xiaonyu Xia",
        "title": "Long Time Behavior of Optimal Liquidation Problems",
        "comments": "31 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper, we study the long time behavior of an optimal liquidation\nproblem with semimartingale strategies and external flows. To investigate the\nlimit rigorously, we study the convergence of three BSDEs characterizing the\nvalue function and the optimal strategy, from finite horizon to infinite\nhorizon. We find that in the long time limit the player may not necessarily\nliquidate her assets at all due to the existence of external flows, even if in\nany given finite time horizon, the player is forced to liquidate all assets.\nMoreover, when the intensity of the external flow is damped, the player will\nliquidate her assets in the long run.\n"
    },
    {
        "paper_id": 2405.14262,
        "authors": "Abdul Rahman",
        "title": "Unlocking Profit Potential: Maximizing Returns with Bayesian\n  Optimization of Supertrend Indicator Parameters",
        "comments": "Submited as thesis : 13 pages, 7 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-sa/4.0/",
        "abstract": "  This paper investigates the potential of Bayesian optimization (BO) to\noptimize the atr multiplier and atr period -the parameters of the Supertrend\nindicator for maximizing trading profits across diverse stock datasets. By\nemploying BO, the thesis aims to automate the identification of optimal\nparameter settings, leading to a more data-driven and potentially more\nprofitable trading strategy compared to relying on manually chosen parameters.\nThe effectiveness of the BO-optimized Supertrend strategy will be evaluated\nthrough backtesting on a variety of stock datasets.\n"
    },
    {
        "paper_id": 2405.14418,
        "authors": "Michail Anthropelos and Constantinos Stefanakis",
        "title": "Continuous-time Equilibrium Returns in Markets with Price Impact and\n  Transaction Costs",
        "comments": "30 pages, no figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  We consider an Ito-financial market at which the risky assets' returns are\nderived endogenously through a market-clearing condition amongst heterogeneous\nrisk-averse investors with quadratic preferences and random endowments.\nInvestors act strategically by taking into account the impact that their orders\nhave on the assets' drift. A frictionless market and an one with quadratic\ntransaction costs are analysed and compared. In the former, we derive the\nunique Nash equilibrium at which investors' demand processes reveal different\nhedging needs than their true ones, resulting in a deviation of the Nash\nequilibrium from its competitive counterpart. Under price impact and\ntransaction costs, we characterize the Nash equilibrium as the (unique)\nsolution of a system of FBSDEs and derive its closed-form expression. We\nfurthermore show that under common risk aversion and absence of noise traders,\ntransaction costs do not change the equilibrium returns. On the contrary, when\nnoise traders are present, the effect of transaction costs on equilibrium\nreturns is amplified due to price impact.\n"
    },
    {
        "paper_id": 2405.14611,
        "authors": "Oliver Linton, Raghavendra Rau, Patrick Baert, Peter Bossaerts, Jon\n  Crowcroft, G.R. Evans, Paul Ewart, Nick Gay, Paul Kattuman, Stefan Scholtes,\n  Hamid Sabourian, and Richard J. Smith",
        "title": "Is the EJRA proportionate and therefore justified? A critical review of\n  the EJRA policy at Cambridge",
        "comments": "This is a revision of our earlier paper",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  This paper critically evaluates the HESA (Higher Education Statistics Agency)\nData Report for the Employer Justified Retirement Age (EJRA) Review Group at\nthe University of Cambridge (\\cite{CambridgeHESA2024}), identifying significant\nmethodological flaws and misinterpretations. Our analysis reveals issues such\nas unclear application of data filters, inconsistent variable treatment, and\nerroneous statistical conclusions. The Report suggests that the EJRA increased\njob creation rates at Cambridge, but we show Cambridge consistently had lower\njob creation rates for Established Academic Careers compared to other Russell\nGroup universities, both before and after EJRA implementation in 2011, with no\nevidence for a significant change in this deficit post implementation. This\nsuggests that EJRA is not a significant factor driving job creation rates.\nSince other universities without an EJRA exhibit higher job creation rates,\nthis suggests job creation can be sustained without such a policy. We conclude\nthat the EJRA did not achieve its intended goal of increasing opportunities for\nyoung academics and may have exacerbated existing disparities compared to other\nleading universities. We recommend EJRA be abolished at Cambridge since it does\nnot meet its aims and could be viewed as unlawful age discrimination.\\newline\n\\it{This version is a revision reflecting some of the comments made by members\nof the university EJRA review group in public discussion, see \\cite{Holmes24}.}\n"
    },
    {
        "paper_id": 2405.14617,
        "authors": "Nicolas Oderbolz, Beatrix Marosv\\\"olgyi, Matthias Hafner",
        "title": "Towards an Optimal Staking Design: Balancing Security, User Growth, and\n  Token Appreciation",
        "comments": null,
        "journal-ref": "Proceedings of the ChainScience 2024 Conference",
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  This paper examines the economic and security implications of Proof-of-Stake\n(POS) designs, providing a survey of POS design choices and their underlying\neconomic principles in prominent POS-blockchains. The paper argues that\nPOS-blockchains are essentially platforms that connect three groups of agents:\nusers, validators, and investors. To meet the needs of these groups,\nblockchains must balance trade-offs between security, user adoption, and\ninvestment into the protocol. We focus on the security aspect and identify two\ndifferent strategies: increasing the quality of validators (static security)\nvs. increasing the quantity of stakes (dynamic security). We argue that quality\ncomes at the cost of quantity, identifying a trade-off between the two\nstrategies when designing POS systems. We test our qualitative findings using\npanel analysis on collected data. The analysis indicates that enhancing the\nquality of the validator set through security measures like slashing and\nminimum staking amounts may decrease dynamic security. Further, the analysis\nreveals a strategic divergence among blockchains, highlighting the absence of a\nsingle, universally optimal staking design solution. The optimal design hinges\nupon a platform's specific objectives and its developmental stage. This\nresearch compels blockchain developers to meticulously assess the trade-offs\noutlined in this paper when developing their staking designs.\n"
    },
    {
        "paper_id": 2405.14767,
        "authors": "Hongyang Yang, Boyu Zhang, Neng Wang, Cheng Guo, Xiaoli Zhang, Likun\n  Lin, Junlin Wang, Tianyu Zhou, Mao Guan, Runjia Zhang, Christina Dan Wang",
        "title": "FinRobot: An Open-Source AI Agent Platform for Financial Applications\n  using Large Language Models",
        "comments": "FinRobot Whitepaper V1.0",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  As financial institutions and professionals increasingly incorporate Large\nLanguage Models (LLMs) into their workflows, substantial barriers, including\nproprietary data and specialized knowledge, persist between the finance sector\nand the AI community. These challenges impede the AI community's ability to\nenhance financial tasks effectively. Acknowledging financial analysis's\ncritical role, we aim to devise financial-specialized LLM-based toolchains and\ndemocratize access to them through open-source initiatives, promoting wider AI\nadoption in financial decision-making. In this paper, we introduce FinRobot, a\nnovel open-source AI agent platform supporting multiple financially specialized\nAI agents, each powered by LLM. Specifically, the platform consists of four\nmajor layers: 1) the Financial AI Agents layer that formulates Financial\nChain-of-Thought (CoT) by breaking sophisticated financial problems down into\nlogical sequences; 2) the Financial LLM Algorithms layer dynamically configures\nappropriate model application strategies for specific tasks; 3) the LLMOps and\nDataOps layer produces accurate models by applying training/fine-tuning\ntechniques and using task-relevant data; 4) the Multi-source LLM Foundation\nModels layer that integrates various LLMs and enables the above layers to\naccess them directly. Finally, FinRobot provides hands-on for both\nprofessional-grade analysts and laypersons to utilize powerful AI techniques\nfor advanced financial analysis. We open-source FinRobot at\n\\url{https://github.com/AI4Finance-Foundation/FinRobot}.\n"
    },
    {
        "paper_id": 2405.14999,
        "authors": "Daniel Winkler and Christian Hotz-Behofsits and Nils Wl\\\"omert and\n  Dominik Papies and Jura Liaukonyte",
        "title": "The Impact of Social Media on Music Demand: Evidence from a\n  Quasi-Natural Experiment",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The digital age has significantly changed how music is consumed, promoted,\nand monetized. Social media platforms like TikTok are playing a pivotal role in\nthis transformation. This shift has sparked a debate within the music industry:\nWhile some stakeholders see social media platforms like TikTok as opportunities\nto boost songs to viral status, others raise concerns about potential\ncannibalization effects, fearing that such exposure might reduce revenue from\nstreaming services like Spotify. In this paper, we evaluate the effect of a\nsong's presence - or absence - on social media on its demand on music streaming\nservices using a quasi-natural experiment: Universal Music Group's (UMG) - one\nof \"The Big 3\" record labels - decision to remove its entire content library\nfrom TikTok in February 2024. We use representative samples covering close to\n50% of the US and 94% of the German streaming markets, employing a\ndifference-in-differences approach to compare the streaming consumption of\nsongs that were removed from TikTok with those that were not. We find that\nUMG's removal of music from TikTok led to a 2-3% increase in streams on audio\nplatforms for affected songs, indicating substitution effects. However, this\naverage treatment effect masks significant heterogeneity: older songs and songs\nwith less promotional support elsewhere saw a decrease in streaming\nconsumption, suggesting that TikTok helps consumers discover or rediscover\ncontent that is not top of mind for consumers.\n"
    },
    {
        "paper_id": 2405.15461,
        "authors": "Hongshen Yang, Avinash Malik",
        "title": "Optimal market-neutral currency trading on the cryptocurrency platform",
        "comments": "24 pages, 8 figures, 11 tables",
        "journal-ref": null,
        "doi": "10.3390/ijfs12030077",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This research proposes a novel arbitrage approach in multivariate pair\ntrading, termed the Optimal Trading Technique (OTT). We present a method for\nselectively forming a \"bucket\" of fiat currencies anchored to cryptocurrency\nfor monitoring and exploiting trading opportunities simultaneously. To address\nquantitative conflicts from multiple trading signals, a novel bi-objective\nconvex optimization formulation is designed to balance investor preferences\nbetween profitability and risk tolerance. We understand that cryptocurrencies\ncarry significant financial risks. Therefore this process includes tunable\nparameters such as volatility penalties and action thresholds. In experiments\nconducted in the cryptocurrency market from 2020 to 2022, which encompassed a\nvigorous bull run followed by a bear run, the OTT achieved an annualized profit\nof 15.49%. Additionally, supplementary experiments detailed in the appendix\nextend the applicability of OTT to other major cryptocurrencies in the\npost-COVID period, validating the model's robustness and effectiveness in\nvarious market conditions. The arbitrage operation offers a new perspective on\ntrading, without requiring external shorting or holding the intermediate during\nthe arbitrage period. As a note of caution, this study acknowledges the\nhigh-risk nature of cryptocurrency investments, which can be subject to\nsignificant volatility and potential loss.\n"
    },
    {
        "paper_id": 2405.15486,
        "authors": "Qing Guo, Siyu Chen, Xiangquan Zeng",
        "title": "Digital finance, Bargaining Power and Gender Wage Gap",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The proliferation of internet technology has catalyzed the rapid development\nof digital finance, significantly impacting the optimization of resource\nallocation in China and exerting a substantial and enduring influence on the\nstructure of employment and income distribution. This research utilizes data\nsourced from the Chinese General Social Survey and the Digital Financial\nInclusion Index to scrutinize the influence of digital finance on the gender\nwage disparity in China. The findings reveal that digital finance reduces the\ngender wage gap, and this conclusion remains robust after addressing\nendogeneity problem using instrumental variable methods. Further analysis of\nthe underlying mechanisms indicates that digital finance facilitates female\nentrepreneurship by lowering financing barriers, thereby promoting employment\nopportunities for women and also empowering them to negotiate higher wages.\nSpecially, digital finance enhances women's bargaining power within domestic\nsettings, therefore exerts a positive influence on the wages of women.\nSub-sample regressions demonstrate that women from economically disadvantaged\nbackgrounds, with lower human capital, benefit more from digital finance,\nunderscoring its inclusive nature. This study provides policy evidence for\nempowering vulnerable groups to increase their wages and addressing the\npersistent issue of gender income disparity in the labor market.\n"
    },
    {
        "paper_id": 2405.15672,
        "authors": "Reetwika Basu, Eric Anderson, Chinmay Deval, Kelsey Herndon, Amanda\n  Markert, Lena Pransky, Emil Cherrington, Aparna Phalke and Alqamah Sayeed",
        "title": "Serving economic prosperity: economic impact assessments (EIA) on Earth\n  observation-based services and tools by SERVIR",
        "comments": "40 pages, 13 Tables and 4 Figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  In an era where informed decision-making is paramount for sustainable\ndevelopment and effective resource management, the role of Earth observations\n(EO) in shaping economic landscapes cannot be overstated. EO, facilitated by\nsatellites, sensors, and data analytics, is a cornerstone for evidence-based\npolicymaking, risk mitigation, and resource allocation. SERVIR is a joint\ninitiative of US Agency for International Development and NASA. This paper\npresents a comprehensive survey of relevant economic impact assessment (EIA)\nwork, summarizes SERVIRs potential interests in EIA, and identifies how and\nwhere EIA could improve how SERVIR quantifies and communicates the impact of\nits services.\n"
    },
    {
        "paper_id": 2405.15721,
        "authors": "Adam Baybutt",
        "title": "Dynamic Latent-Factor Model with High-Dimensional Asset Characteristics",
        "comments": "58 pages, 6 tables",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  We develop novel estimation procedures with supporting econometric theory for\na dynamic latent-factor model with high-dimensional asset characteristics, that\nis, the number of characteristics is on the order of the sample size. Utilizing\nthe Double Selection Lasso estimator, our procedure employs regularization to\neliminate characteristics with low signal-to-noise ratios yet maintains\nasymptotically valid inference for asset pricing tests. The crypto asset class\nis well-suited for applying this model given the limited number of tradable\nassets and years of data as well as the rich set of available asset\ncharacteristics. The empirical results present out-of-sample pricing abilities\nand risk-adjusted returns for our novel estimator as compared to benchmark\nmethods. We provide an inference procedure for measuring the risk premium of an\nobservable nontradable factor, and employ this to find that the\ninflation-mimicking portfolio in the crypto asset class has positive risk\ncompensation.\n"
    },
    {
        "paper_id": 2405.15781,
        "authors": "Claudia M. Peixoto, Diego Marcondes, Mariana P. Melo, Ana C. Maia,\n  Luis A. Correia",
        "title": "Prediction of healthcare costs on consumer direct health plan in the\n  Brazilian context",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-sa/4.0/",
        "abstract": "  The rise in healthcare costs has led to the adoption of cost-sharing devices\nin health plans. This article explores this discussion by simulating Health\nSavings Accounts (HSAs) to cover medical and hospital expenses, supported by\ncatastrophic insurance. Simulating 10 million lives, we evaluate the\nutilization of catastrophic insurance and the balances of HSAs at the end of\nworking life. To estimate annual expenditures, a Markov Chains approach -\ndistinct from the usual ones - was used based on recent past expenditures, age\nrange, and gender. The results suggest that HSAs do not create inequalities,\noffering a viable method to sustain private healthcare financing for the\nelderly.\n"
    },
    {
        "paper_id": 2405.15825,
        "authors": "Ziyu Yan",
        "title": "Multimarket Contact, Merger, and Airline Collusion",
        "comments": "25 pages, 4 figures, and 6 tables",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This thesis investigates the dynamics of multimarket contact and airline\nmergers on collusive pricing of airlines. In align with Bernheim and Whinston\n(1990) and Athey et.al.(2004), it detects collusive pricing via pairwise price\ndifference and price rigidity. The piece of work extends previous work by\nincorporating additional controls such as distinction between non-stop and\nstopover itineraries and detailed market concentration measures. The findings\nconfirm a significant relationship between multimarket contact and reduced\nprice differences, indicating collusive equilibria facilitated by frequent\ninteractions across markets. Moreover, the results highlight that airlines\nexhibit more collusive behavior when pricing non-stop flights, and are more\nlikely to attain tacit collusion when they approaches duopoly in a particular\nmarket. The study also explores the effects of airline mergers on collusion,\nemploying an event study methodology with a difference-in-difference (DID)\ndesign. It finds no direct evidence that mergers lead to increased collusion\namong unmerged carriers. However, it reveals that during and after the merger\nprocess, carrier pairs between merged and unmerged carriers are more likely to\ncollude compared to pairs of unmerged carriers.\n"
    },
    {
        "paper_id": 2405.15833,
        "authors": "Jianyuan Zhong, Zhijian Xu, Saizhuo Wang, Xiangyu Wen, Jian Guo, Qiang\n  Xu",
        "title": "DSPO: An End-to-End Framework for Direct Sorted Portfolio Construction",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
        "abstract": "  In quantitative investment, constructing characteristic-sorted portfolios is\na crucial strategy for asset allocation. Traditional methods transform raw\nstock data of varying frequencies into predictive characteristic factors for\nasset sorting, often requiring extensive manual design and misalignment between\nprediction and optimization goals. To address these challenges, we introduce\nDirect Sorted Portfolio Optimization (DSPO), an innovative end-to-end framework\nthat efficiently processes raw stock data to construct sorted portfolios\ndirectly. DSPO's neural network architecture seamlessly transitions stock data\nfrom input to output while effectively modeling the intra-dependency of\ntime-steps and inter-dependency among all tradable stocks. Additionally, we\nincorporate a novel Monotonical Logistic Regression loss, which directly\nmaximizes the likelihood of constructing optimal sorted portfolios. To the best\nof our knowledge, DSPO is the first method capable of handling market\ncross-sections with thousands of tradable stocks fully end-to-end from raw\nmulti-frequency data. Empirical results demonstrate DSPO's effectiveness,\nyielding a RankIC of 10.12\\% and an accumulated return of 121.94\\% on the New\nYork Stock Exchange in 2023-2024, and a RankIC of 9.11\\% with a return of\n108.74\\% in other markets during 2021-2022.\n"
    },
    {
        "paper_id": 2405.15862,
        "authors": "Hugo Sant'Anna",
        "title": "The Mariana Environmental Disaster and its Labor Market Effects",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  This paper examines the labor market impacts of the 2015 Mariana Dam disaster\nin Brazil. It contrasts two theoretical models: an urban spatial equilibrium\nmodel and a factor of production model, with diverging perspectives on\nenvironmental influences on labor outcomes. Utilizing rich national\nadministrative and spatial data, the study reveals that the unusual\nenvironmental alteration, with minimal human capital loss, primarily affected\noutcomes via the factor of production channel. Nevertheless, spatial\nequilibrium dynamics are discernible within certain market segments. This\nresearch contributes to the growing literature on environmental changes and its\neconomic consequences.\n"
    },
    {
        "paper_id": 2405.15929,
        "authors": "Hui Li, Jian Ni and Fangzhu Yang",
        "title": "Product Design Using Generative Adversarial Network: Incorporating\n  Consumer Preference and External Data",
        "comments": "46 pages, 26 figures, 5 tables",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The development of generative artificial intelligence (AI) enables\nlarge-scale product design automation. However, this automated process usually\ndoes not incorporate consumer preference information from the internal dataset\nof a company. Furthermore, external sources such as social media and\nuser-generated content (UGC) websites often contain rich product design and\nconsumer preference information, but such information is not utilized by\ncompanies when generating designs. We propose a semi-supervised deep generative\nframework that integrates consumer preferences and external data into the\nproduct design process, allowing companies to generate consumer-preferred\ndesigns in a cost-effective and scalable way. We train a predictor model to\nlearn consumer preferences and use predicted popularity levels as additional\ninput labels to guide the training procedure of a continuous conditional\ngenerative adversarial network (CcGAN). The CcGAN can be instructed to generate\nnew designs with a certain popularity level, enabling companies to efficiently\ncreate consumer-preferred designs and save resources by avoiding the\ndevelopment and testing of unpopular designs. The framework also incorporates\nexisting product designs and consumer preference information from external\nsources, which is particularly helpful for small or start-up companies that\nhave limited internal data and face the \"cold-start\" problem. We apply the\nproposed framework to a real business setting by helping a large self-aided\nphotography chain in China design new photo templates. We show that our\nproposed model performs well in terms of generating appealing template designs\nfor the company.\n"
    },
    {
        "paper_id": 2405.15975,
        "authors": "Haoyang Cao and Zhengqi Wu and Renyuan Xu",
        "title": "Inference of Utilities and Time Preference in Sequential Decision-Making",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  This paper introduces a novel stochastic control framework to enhance the\ncapabilities of automated investment managers, or robo-advisors, by accurately\ninferring clients' investment preferences from past activities. Our approach\nleverages a continuous-time model that incorporates utility functions and a\ngeneric discounting scheme of a time-varying rate, tailored to each client's\nrisk tolerance, valuation of daily consumption, and significant life goals. We\naddress the resulting time inconsistency issue through state augmentation and\nthe establishment of the dynamic programming principle and the verification\ntheorem. Additionally, we provide sufficient conditions for the identifiability\nof client investment preferences. To complement our theoretical developments,\nwe propose a learning algorithm based on maximum likelihood estimation within a\ndiscrete-time Markov Decision Process framework, augmented with entropy\nregularization. We prove that the log-likelihood function is locally concave,\nfacilitating the fast convergence of our proposed algorithm. Practical\neffectiveness and efficiency are showcased through two numerical examples,\nincluding Merton's problem and an investment problem with unhedgeable risks.\n  Our proposed framework not only advances financial technology by improving\npersonalized investment advice but also contributes broadly to other fields\nsuch as healthcare, economics, and artificial intelligence, where understanding\nindividual preferences is crucial.\n"
    },
    {
        "paper_id": 2405.16052,
        "authors": "Anish Rai, Buddha Nath Sharma, Salam Rabindrajit Luwang, Md.Nurujjaman\n  and Sushovan Majhi",
        "title": "Identifying Extreme Events in the Stock Market: A Topological Data\n  Analysis",
        "comments": "14 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  This paper employs Topological Data Analysis (TDA) to detect extreme events\n(EEs) in the stock market at a continental level. Previous approaches, which\nanalyzed stock indices separately, could not detect EEs for multiple time\nseries in one go. TDA provides a robust framework for such analysis and\nidentifies the EEs during the crashes for different indices. The TDA analysis\nshows that $L^1$, $L^2$ norms and Wasserstein distance ($W_D$) of the world\nleading indices rise abruptly during the crashes, surpassing a threshold of\n$\\mu+4*\\sigma$ where $\\mu$ and $\\sigma$ are the mean and the standard deviation\nof norm or $W_D$, respectively. Our study identified the stock index crashes of\nthe 2008 financial crisis and the COVID-19 pandemic across continents as EEs.\nGiven that different sectors in an index behave differently, a sector-wise\nanalysis was conducted during the COVID-19 pandemic for the Indian stock\nmarket. The sector-wise results show that after the occurrence of EE, we have\nobserved strong crashes surpassing $\\mu+2*\\sigma$ for an extended period for\nthe banking sector. While for the pharmaceutical sector, no significant spikes\nwere noted. Hence, TDA also proves successful in identifying the duration of\nshocks after the occurrence of EEs. This also indicates that the Banking sector\ncontinued to face stress and remained volatile even after the crash. This study\ngives us the applicability of TDA as a powerful analytical tool to study EEs in\nvarious fields.\n"
    },
    {
        "paper_id": 2405.16193,
        "authors": "Sammy Kemboi Chepkilot",
        "title": "Effect of Interest Payments on External Debt on Economic Growth in Kenya",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  In Kenya, interest payments on external debt have been increasing from 2010\nto 2015, while GDP growth experienced a slight decline over the same period.\nPolicymakers are concerned that the rapid increase in external debt in\ndeveloping countries such as Kenya has the potential to erode the country's\nsovereign rating, particularly if it is not supported by proportionate growth\nin the size of the economy. The purpose of this study was to investigate the\neffect of interest payments on external debt on economic growth in Kenya. The\nstudy utilized secondary data for 25 years, from 1991 to 2015, for GDP growth\nand interest payments on external debt. The results from the analysis of\nvariance statistics indicate that the model was statistically significant. This\nimplies that interest payments on external debt are good predictors of GDP\ngrowth. Regression coefficient results show that GDP growth and the logarithm\nof interest payments on external debt are negatively and significantly related.\nThe study recommends that future government plans should ensure that external\nborrowings are taken at rates not higher than the interest rate payments.\n"
    },
    {
        "paper_id": 2405.16333,
        "authors": "Yury Lebedev and Arunava Banerjee",
        "title": "Gaussian Recombining Split Tree",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  Binomial trees are widely used in the financial sector for valuing securities\nwith early exercise characteristics, such as American stock options. However,\nwhile effective in many scenarios, pricing options with CRR binomial trees are\nlimited. Major limitations are volatility estimation, constant volatility\nassumption, subjectivity in parameter choices, and impracticality of\ninstantaneous delta hedging. This paper presents a novel tree: Gaussian\nRecombining Split Tree (GRST), which is recombining and does not need\nlog-normality or normality market assumption. GRST generates a discrete\nprobability mass function of market data distribution, which approximates a\nGaussian distribution with known parameters at any chosen time interval. GRST\nMixture builds upon the GRST concept while being flexible to fit a large class\nof market distributions and when given a 1-D time series data and moments of\ndistributions at each time interval, fits a Gaussian mixture with the same\nmixture component probabilities applied at each time interval. Gaussian\nRecombining Split Tre Mixture comprises several GRST tied using Gaussian\nmixture component probabilities at the first node. Our extensive empirical\nanalysis shows that the option prices from the GRST align closely with the\nmarket.\n"
    },
    {
        "paper_id": 2405.16336,
        "authors": "Mauricio Elizalde, Stephan Sturm",
        "title": "Intertemporal Cost-efficient Consumption",
        "comments": "21 pages, 7 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We aim to provide an intertemporal, cost-efficient consumption model that\nextends the consumption optimization inspired by the Distribution Builder, a\ntool developed by Sharpe, Johnson, and Goldstein. The Distribution Builder\nenables the recovery of investors' risk preferences by allowing them to select\na desired distribution of terminal wealth within their budget constraints.\n  This approach differs from the classical portfolio optimization, which\nconsiders the agent's risk aversion modeled by utility functions that are\nchallenging to measure in practice. Our intertemporal model captures the\ndependent structure between consumption periods using copulas. This strategy is\ndemonstrated using both the Black-Scholes and CEV models.\n"
    },
    {
        "paper_id": 2405.16449,
        "authors": "Xuefeng Gao, Lingfei Li, Xun Yu Zhou",
        "title": "Reinforcement Learning for Jump-Diffusions, with Financial Applications",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We study continuous-time reinforcement learning (RL) for stochastic control\nin which system dynamics are governed by jump-diffusion processes. We formulate\nan entropy-regularized exploratory control problem with stochastic policies to\ncapture the exploration--exploitation balance essential for RL. Unlike the pure\ndiffusion case initially studied by Wang et al. (2020), the derivation of the\nexploratory dynamics under jump-diffusions calls for a careful formulation of\nthe jump part. Through a theoretical analysis, we find that one can simply use\nthe same policy evaluation and $q$-learning algorithms in Jia and Zhou (2022a,\n2023), originally developed for controlled diffusions, without needing to check\na priori whether the underlying data come from a pure diffusion or a\njump-diffusion. However, we show that the presence of jumps ought to affect\nparameterizations of actors and critics in general. We investigate as an\napplication the mean--variance portfolio selection problem with stock price\nmodelled as a jump-diffusion, and show that both RL algorithms and\nparameterizations are invariant with respect to jumps. Finally, we present a\ndetailed study on applying the general theory to option hedging.\n"
    },
    {
        "paper_id": 2405.16636,
        "authors": "Tiziano De Angelis and Damien Lamberton",
        "title": "A probabilistic approach to continuous differentiability of optimal\n  stopping boundaries",
        "comments": "41 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We obtain the first probabilistic proof of continuous differentiability of\ntime-dependent optimal boundaries in optimal stopping problems. The underlying\nstochastic dynamics is a one-dimensional, time-inhomogeneous diffusion. The\ngain function is also time-inhomogeneous and not necessarily smooth. Moreover,\nwe include state-dependent discount rate and the time-horizon can be either\nfinite or infinite. Our arguments of proof are of a local nature that allows us\nto obtain the result under more general conditions than those used in the PDE\nliterature. As a byproduct of our main result we also obtain the first\nprobabilistic proof of the link between the value function of an optimal\nstopping problem and the solution of the Stefan's problem.\n"
    },
    {
        "paper_id": 2405.16688,
        "authors": "Rem Sadykhov and Geoffrey Goodell and Philip Treleaven",
        "title": "DeTEcT: Dynamic and Probabilistic Parameters Extension",
        "comments": "23 pages, 3 tables",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper presents a theoretical extension of the DeTEcT framework proposed\nby Sadykhov et al., DeTEcT, where a formal analysis framework was introduced\nfor modelling wealth distribution in token economies. DeTEcT is a framework for\nanalysing economic activity, simulating macroeconomic scenarios, and\nalgorithmically setting policies in token economies. This paper proposes four\nways of parametrizing the framework, where dynamic vs static parametrization is\nconsidered along with the probabilistic vs non-probabilistic. Using these\nparametrization techniques, we demonstrate that by adding restrictions to the\nframework it is possible to derive the existing wealth distribution models from\nDeTEcT. In addition to exploring parametrization techniques, this paper studies\nhow money supply in DeTEcT framework can be transformed to become dynamic, and\nhow this change will affect the dynamics of wealth distribution. The motivation\nfor studying dynamic money supply is that it enables DeTEcT to be applied to\nmodelling token economies without maximum supply (i.e., Ethereum), and it adds\nconstraints to the framework in the form of symmetries.\n"
    },
    {
        "paper_id": 2405.1707,
        "authors": "Monika Zimmermann and Florian Ziel",
        "title": "Efficient mid-term forecasting of hourly electricity load using\n  generalized additive models",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Accurate mid-term (weeks to one year) hourly electricity load forecasts are\nessential for strategic decision-making in power plant operation, ensuring\nsupply security and grid stability, and energy trading. While numerous models\neffectively predict short-term (hours to a few days) hourly load, mid-term\nforecasting solutions remain scarce. In mid-term load forecasting, besides\ndaily, weekly, and annual seasonal and autoregressive effects, capturing\nweather and holiday effects, as well as socio-economic non-stationarities in\nthe data, poses significant modeling challenges. To address these challenges,\nwe propose a novel forecasting method using Generalized Additive Models (GAMs)\nbuilt from interpretable P-splines and enhanced with autoregressive\npost-processing. This model uses smoothed temperatures, Error-Trend-Seasonal\n(ETS) modeled non-stationary states, a nuanced representation of holiday\neffects with weekday variations, and seasonal information as input. The\nproposed model is evaluated on load data from 24 European countries. This\nanalysis demonstrates that the model not only has significantly enhanced\nforecasting accuracy compared to state-of-the-art methods but also offers\nvaluable insights into the influence of individual components on predicted\nload, given its full interpretability. Achieving performance akin to day-ahead\nTSO forecasts in fast computation times of a few seconds for several years of\nhourly data underscores the model's potential for practical application in the\npower system industry.\n"
    },
    {
        "paper_id": 2405.17682,
        "authors": "Victor Cardenas",
        "title": "Managing Financial Climate Risk in Banking Services: A Review of Current\n  Practices and the Challenges Ahead",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  The document discusses the financial climate risk in the context of the\nbanking industry, emphasizing the need for a comprehensive understanding of\nclimate change across different spatial and temporal scales. It highlights the\nchallenges in estimating physical and transition risks, specifically extreme\nevents and limitations of current climate models. The document also reviews\ncurrent gaps in assessing physical and transition risks, including the\ndevelopment, improvement of modeling frameworks, highlighting the need for\ndetailed databases of exposed physical assets and climatic hazard modeling. It\nalso emphasizes the importance of integrating financial climate risks into\nfinancial risk management practices, particularly in smaller banks and lending\norganizations.\n"
    },
    {
        "paper_id": 2405.17753,
        "authors": "Vladimir Dvorkin",
        "title": "Regression Equilibrium in Electricity Markets",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Renewable power producers participating in electricity markets build\nforecasting models independently, relying on their own data, model and feature\npreferences. In this paper, we argue that in renewable-dominated markets, such\nan uncoordinated approach to forecasting results in substantial opportunity\ncosts for stochastic producers and additional operating costs for the power\nsystem. As a solution, we introduce Regression Equilibrium--a welfare-optimal\nstate of electricity markets under uncertainty, where profit-seeking stochastic\nproducers do not benefit by unilaterally deviating from their equilibrium\nforecast models. While the regression equilibrium maximizes the private\nwelfare, i.e., the average profit of stochastic producers across the day-ahead\nand real-time markets, it also aligns with the socially optimal, least-cost\ndispatch solution for the system. We base the equilibrium analysis on the\ntheory of variational inequalities, providing results on the existence and\nuniqueness of regression equilibrium in energy-only markets. We also devise two\nmethods for computing the regression equilibrium: centralized optimization and\na decentralized ADMM-based algorithm that preserves the privacy of regression\ndatasets.\n"
    },
    {
        "paper_id": 2405.17762,
        "authors": "Oleg V. Pavlov and Evangelos Katsamakas",
        "title": "Tuition too high? Blame competition",
        "comments": null,
        "journal-ref": "Journal of Economic Behavior & Organization (2023), 213, 409-431",
        "doi": "10.1016/j.jebo.2023.07.030",
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  We develop a feedback theory that includes reinforcing and balancing feedback\neffects that emerge when colleges compete for reputation, applicants, and\ntuition revenue. The feedback theory is replicated in a formal duopoly model\nconsisting of two competing colleges. An independent ranking entity determines\nthe relative order of the colleges. College applicants choose between the two\ncolleges based on the rankings and the financial aid offered by the colleges.\nContrary to the conventional wisdom that competition lowers prices and benefits\nconsumers, our simulations show that competition between academic institutions\nfor resources and reputation leads to tuition escalation that negatively\naffects students and their families. Four of the five scenarios -- rankings, a\ncapital campaign, facilities improvements, and an excellence campaign --\nincrease college tuition, institutional debt, and expenditures per student;\nonly the scenario of ignoring the rankings decreases these measures. By\nreferring to the feedback structure of academic competition, the article makes\nseveral recommendations for controlling tuition inflation. This article\ncontributes to the literature on the economics of higher education and\nillustrates the value of feedback economics in developing economic theory.\n"
    },
    {
        "paper_id": 2405.1777,
        "authors": "Zhonghao Xian, Xing Yan, Cheuk Hang Leung, Qi Wu",
        "title": "Risk-Neutral Generative Networks",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  We present a functional generative approach to extract risk-neutral densities\nfrom market prices of options. Specifically, we model the log-returns on the\ntime-to-maturity continuum as a stochastic curve driven by standard normal. We\nthen use neural nets to represent the term structures of the location, the\nscale, and the higher-order moments, and impose stringent conditions on the\nlearning process to ensure the neural net-based curve representation is free of\nstatic arbitrage. This specification is structurally clear in that it separates\nthe modeling of randomness from the modeling of the term structures of the\nparameters. It is data adaptive in that we use neural nets to represent the\nshape of the stochastic curve. It is also generative in that the functional\nform of the stochastic curve, although parameterized by neural nets, is an\nexplicit and deterministic function of the standard normal. This explicitness\nallows for the efficient generation of samples to price options across strikes\nand maturities, without compromising data adaptability. We have validated the\neffectiveness of this approach by benchmarking it against a comprehensive set\nof baseline models. Experiments show that the extracted risk-neutral densities\naccommodate a diverse range of shapes. Its accuracy significantly outperforms\nthe extensive set of baseline models--including three parametric models and\nnine stochastic process models--in terms of accuracy and stability. The success\nof this approach is attributed to its capacity to offer flexible term\nstructures for risk-neutral skewness and kurtosis.\n"
    },
    {
        "paper_id": 2405.17841,
        "authors": "Xiaomin Shi, Zuo Quan Xu",
        "title": "Constrained monotone mean--variance investment-reinsurance under the\n  Cram\\'er--Lundberg model with random coefficients",
        "comments": "arXiv admin note: text overlap with arXiv:2212.14188",
        "journal-ref": null,
        "doi": "10.1016/j.sysconle.2024.105796",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper studies an optimal investment-reinsurance problem for an insurer\n(she) under the Cram\\'er--Lundberg model with monotone mean--variance (MMV)\ncriterion. At any time, the insurer can purchase reinsurance (or acquire new\nbusiness) and invest in a security market consisting of a risk-free asset and\nmultiple risky assets whose excess return rate and volatility rate are allowed\nto be random. The trading strategy is subject to a general convex cone\nconstraint, encompassing no-shorting constraint as a special case. The optimal\ninvestment-reinsurance strategy and optimal value for the MMV problem are\ndeduced by solving certain backward stochastic differential equations with\njumps. In the literature, it is known that models with MMV criterion and\nmean--variance criterion lead to the same optimal strategy and optimal value\nwhen the wealth process is continuous. Our result shows that the conclusion\nremains true even if the wealth process has compensated Poisson jumps and the\nmarket coefficients are random.\n"
    },
    {
        "paper_id": 2405.17924,
        "authors": "Ning Li, Huaikang Zhou, Kris Mikel-Hong",
        "title": "Generative AI Enhances Team Performance and Reduces Need for Traditional\n  Teams",
        "comments": "55 pages, 8 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-sa/4.0/",
        "abstract": "  Recent advancements in generative artificial intelligence (AI) have\ntransformed collaborative work processes, yet the impact on team performance\nremains underexplored. Here we examine the role of generative AI in enhancing\nor replacing traditional team dynamics using a randomized controlled experiment\nwith 435 participants across 122 teams. We show that teams augmented with\ngenerative AI significantly outperformed those relying solely on human\ncollaboration across various performance measures. Interestingly, teams with\nmultiple AIs did not exhibit further gains, indicating diminishing returns with\nincreased AI integration. Our analysis suggests that centralized AI usage by a\nfew team members is more effective than distributed engagement. Additionally,\nindividual-AI pairs matched the performance of conventional teams, suggesting a\nreduced need for traditional team structures in some contexts. However, despite\nthis capability, individual-AI pairs still fell short of the performance levels\nachieved by AI-assisted teams. These findings underscore that while generative\nAI can replace some traditional team functions, more comprehensively\nintegrating AI within team structures provides superior benefits, enhancing\noverall effectiveness beyond individual efforts.\n"
    },
    {
        "paper_id": 2405.18434,
        "authors": "Viorel Silaghi, Zobaida Alssadi, Ben Mathew, Majed Alotaibi, Ali\n  Alqarni, Marius Silaghi",
        "title": "Modeling the Feedback of AI Price Estimations on Actual Market Values",
        "comments": "On February 15, 2022 we uploaded in overleaf the first draft of this\n  paper under the name \"Public AI on house price estimations through Zillow may\n  influence a monotonic house price increase and inflation forever according to\n  simulations\", https://www.overleaf.com/read/yttcffkrhvjf\\#7120e1",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Public availability of Artificial Intelligence generated information can\nchange the markets forever, and its factoring into economical dynamics may take\neconomists by surprise, out-dating models and schools of thought. Real estate\nhyper-inflation is not a new phenomenon but its consistent and almost\nmonotonous persistence over 12 years, coinciding with prominence of public\nestimation information from Zillow, a successful Mass Real Estate Estimator\n(MREE), could not escape unobserved. What we model is a repetitive theoretical\ngame between the MREE and the home owners, where each player has secret\ninformation and expertise. If the intention is to keep housing affordable and\nmaintain old American lifestyle with broad home-ownership, new challenges are\ndefined. Simulations show that a simple restriction of MREE-style price\nestimation availability to opt-in properties may help partially reduce feedback\nloop by acting on its likely causes, as suggested by experimental simulation\nmodels. The conjecture that the MREE pressure on real estate inflation rate is\ncorrelated with the absolute MREE estimation errors, which is logically\nexplainable, is then validated in simulations.\n"
    },
    {
        "paper_id": 2405.18594,
        "authors": "Hamza Bodor and Laurent Carlier",
        "title": "A Novel Approach to Queue-Reactive Models: The Importance of Order Sizes",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/publicdomain/zero/1.0/",
        "abstract": "  In this article, we delve into the applications and extensions of the\nqueue-reactive model for the simulation of limit order books. Our approach\nemphasizes the importance of order sizes, in conjunction with their type and\narrival rate, by integrating the current state of the order book to determine,\nnot only the intensity of order arrivals and their type, but also their sizes.\nThese extensions generate simulated markets that are in line with numerous\nstylized facts of the market. Our empirical calibration, using futures on\nGerman bonds, reveals that the extended queue-reactive model significantly\nimproves the description of order flow properties and the shape of queue\ndistributions. Moreover, our findings demonstrate that the extended model\nproduces simulated markets with a volatility comparable to historical real\ndata, utilizing only endogenous information from the limit order book. This\nresearch underscores the potential of the queue-reactive model and its\nextensions in accurately simulating market dynamics and providing valuable\ninsights into the complex nature of limit order book modeling.\n"
    },
    {
        "paper_id": 2405.18728,
        "authors": "Corinne Powers",
        "title": "A Tick-by-Tick Solution for Concentrated Liquidity Provisioning",
        "comments": "14 pages, 6 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
        "abstract": "  Automated market makers with concentrated liquidity capabilities are\nprogrammable at the tick level. The maximization of earned fees, plus\ndepreciated reserves, is a convex optimization problem whose vector solution\ngives the best provision of liquidity at each tick under a given set of\nparameter estimates for swap volume and price volatility. Surprisingly, early\nresults show that concentrating liquidity around the current price is usually\nnot the best strategy.\n"
    },
    {
        "paper_id": 2405.18748,
        "authors": "John Bistlinea, Chikara Onda, Morgan Browning, Johannes Emmerling,\n  Gokul Iyer, Megan Mahajan, Jim McFarland, Haewon McJeon, Robbie Orvis,\n  Francisco Ralston Fonseca, Christopher Roney, Noah Sandoval, Luis Sarmiento,\n  John Weyant, Jared Woollacott, Mei Yuan",
        "title": "Equity Implications of Net-Zero Emissions: A Multi-Model Analysis of\n  Energy Expenditures Across Income Classes Under Economy-Wide Deep\n  Decarbonization Policies",
        "comments": null,
        "journal-ref": "2024, Energy and Climate Change, 5: 100118",
        "doi": "10.1016/j.egycc.2023.100118",
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  With companies, states, and countries targeting net-zero emissions around\nmidcentury, there are questions about how these targets alter household welfare\nand finances, including distributional effects across income groups. This paper\nexamines the distributional dimensions of technology transitions and net-zero\npolicies with a focus on welfare impacts across household incomes. The analysis\nuses a model intercomparison with a range of energy-economy models using\nharmonized policy scenarios reaching economy-wide, net-zero CO2 emissions\nacross the United States in 2050. We employ a novel linking approach that\nconnects output from detailed energy system models with survey microdata on\nenergy expenditures across income classes to provide distributional analysis of\nnet-zero policies. Although there are differences in model structure and input\nassumptions, we find broad agreement in qualitative trends in policy incidence\nand energy burdens across income groups. Models generally agree that direct\nenergy expenditures for many households will likely decline over time with\nreference and net-zero policies. However, there is variation in the extent of\nchanges relative to current levels, energy burdens relative to reference\nlevels, and electricity expenditures. Policy design, primarily how climate\npolicy revenues are used, has first-order impacts on distributional outcomes.\nNet-zero policy costs, in both absolute and relative terms, are unevenly\ndistributed across households, and relative increases in energy expenditures\nare higher for lowest-income households. However, we also find that recycled\nrevenues from climate policies have countervailing effects when rebated on a\nper-capita basis, offsetting higher energy burdens and potentially even leading\nto net progressive outcomes.\n"
    },
    {
        "paper_id": 2405.18936,
        "authors": "Zoltan Eisler and Johannes Muhle-Karbe",
        "title": "Optimizing Broker Performance Evaluation through Intraday Modeling of\n  Execution Cost",
        "comments": "15 pages, 4 figures, 2 tables",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Minimizing execution costs for large orders is a fundamental challenge in\nfinance. Firms often depend on brokers to manage their trades due to limited\ninternal resources for optimizing trading strategies. This paper presents a\nmethodology for evaluating the effectiveness of broker execution algorithms\nusing trading data. We focus on two primary cost components: a linear cost that\nquantifies short-term execution quality and a quadratic cost associated with\nthe price impact of trades. Using a model with transient price impact, we\nderive analytical formulas for estimating these costs. Furthermore, we enhance\nestimation accuracy by introducing novel methods such as weighting price\nchanges based on their expected impact content. Our results demonstrate\nsubstantial improvements in estimating both linear and impact costs, providing\na robust and efficient framework for selecting the most cost-effective brokers.\n"
    },
    {
        "paper_id": 2405.18938,
        "authors": "Antonio Briola, Silvia Bartolucci, Tomaso Aste",
        "title": "HLOB -- Information Persistence and Structure in Limit Order Books",
        "comments": "34 pages, 7 figures, 7 tables, 3 equations",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  We introduce a novel large-scale deep learning model for Limit Order Book\nmid-price changes forecasting, and we name it `HLOB'. This architecture (i)\nexploits the information encoded by an Information Filtering Network, namely\nthe Triangulated Maximally Filtered Graph, to unveil deeper and non-trivial\ndependency structures among volume levels; and (ii) guarantees deterministic\ndesign choices to handle the complexity of the underlying system by drawing\ninspiration from the groundbreaking class of Homological Convolutional Neural\nNetworks. We test our model against 9 state-of-the-art deep learning\nalternatives on 3 real-world Limit Order Book datasets, each including 15\nstocks traded on the NASDAQ exchange, and we systematically characterize the\nscenarios where HLOB outperforms state-of-the-art architectures. Our approach\nsheds new light on the spatial distribution of information in Limit Order Books\nand on its degradation over increasing prediction horizons, narrowing the gap\nbetween microstructural modeling and deep learning-based forecasting in\nhigh-frequency financial markets.\n"
    },
    {
        "paper_id": 2405.19075,
        "authors": "Baishuai Zuo, Chuancun Yin",
        "title": "Worst-cases of distortion riskmetrics and weighted entropy with partial\n  information",
        "comments": "26 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper, we discuss the worst-case of distortion riskmetrics for\ngeneral distributions when only partial information (mean and variance) is\nknown. This result is applicable to general class of distortion risk measures\nand variability measures. Furthermore, we also consider worst-case of weighted\nentropy for general distributions when only partial information is available.\nSpecifically, we provide some applications for entropies, weighted entropies\nand risk measures. The commonly used entropies include Gini functional,\ncumulative residual entropy, tail-Gini functional, cumulative Tsallis past\nentropy, extended Gini coefficient and so on. The risk measures contain some\npremium principles and shortfalls based on entropy. The shortfalls include the\nGini shortfall, extended Gini shortfall, shortfall of cumulative residual\nentropy and shortfall of cumulative residual Tsallis entropy with order\n$\\alpha$.\n"
    },
    {
        "paper_id": 2405.19104,
        "authors": "Sabrina Aufiero, Preben Forer, Pierpaolo Vivo, Fabio Caccioli, Silvia\n  Bartolucci",
        "title": "Phase transitions in debt recycling",
        "comments": "27 pages, 13 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Debt recycling is an aggressive equity extraction strategy that potentially\npermits faster repayment of a mortgage. While equity progressively builds up as\nthe mortgage is repaid monthly, mortgage holders may obtain another loan they\ncould use to invest on a risky asset. The wealth produced by a successful\ninvestment is then used to repay the mortgage faster. The strategy is riskier\nthan a standard repayment plan since fluctuations in the house market and\ninvestment's volatility may also lead to a fast default, as both the mortgage\nand the liquidity loan are secured against the same good. The general\nconditions of the mortgage holder and the outside market under which debt\nrecycling may be recommended or discouraged have not been fully investigated.\nIn this paper, to evaluate the effectiveness of traditional monthly mortgage\nrepayment versus debt recycling strategies, we build a dynamical model of debt\nrecycling and study the time evolution of equity and mortgage balance as a\nfunction of loan-to-value ratio, house market performance, and return of the\nrisky investment. We find that the model has a rich behavior as a function of\nits main parameters, showing strongly and weakly successful phases - where the\nmortgage is eventually repaid faster and slower than the standard monthly\nrepayment strategy, respectively - a default phase where the equity locked in\nthe house vanishes before the mortgage is repaid, signalling a failure of the\ndebt recycling strategy, and a permanent re-mortgaging phase - where further\ninvestment funds from the lender are continuously secured, but the mortgage is\nnever fully repaid. The strategy's effectiveness is found to be highly\nsensitive to the initial mortgage-to-equity ratio, the monthly amount of\nscheduled repayments, and the economic parameters at the outset. The analytical\nresults are corroborated with numerical simulations with excellent agreement.\n"
    },
    {
        "paper_id": 2405.19313,
        "authors": "Jian-Qiao Zhu, Haijiang Yan, Thomas L. Griffiths",
        "title": "Language Models Trained to do Arithmetic Predict Human Risky and\n  Intertemporal Choice",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  The observed similarities in the behavior of humans and Large Language Models\n(LLMs) have prompted researchers to consider the potential of using LLMs as\nmodels of human cognition. However, several significant challenges must be\naddressed before LLMs can be legitimately regarded as cognitive models. For\ninstance, LLMs are trained on far more data than humans typically encounter,\nand may have been directly trained on human data in specific cognitive tasks or\naligned with human preferences. Consequently, the origins of these behavioral\nsimilarities are not well understood. In this paper, we propose a novel way to\nenhance the utility of LLMs as cognitive models. This approach involves (i)\nleveraging computationally equivalent tasks that both an LLM and a rational\nagent need to master for solving a cognitive problem and (ii) examining the\nspecific task distributions required for an LLM to exhibit human-like\nbehaviors. We apply this approach to decision-making -- specifically risky and\nintertemporal choice -- where the key computationally equivalent task is the\narithmetic of expected value calculations. We show that an LLM pretrained on an\necologically valid arithmetic dataset, which we call Arithmetic-GPT, predicts\nhuman behavior better than many traditional cognitive models. Pretraining LLMs\non ecologically valid arithmetic datasets is sufficient to produce a strong\ncorrespondence between these models and human decision-making. Our results also\nsuggest that LLMs used as cognitive models should be carefully investigated via\nablation studies of the pretraining data.\n"
    },
    {
        "paper_id": 2405.19578,
        "authors": "Denish Omondi Otieno, Faranak Abri, Sima Siami-Namini, Akbar Siami\n  Namin",
        "title": "The Accuracy of Domain Specific and Descriptive Analysis Generated by\n  Large Language Models",
        "comments": "12 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Large language models (LLMs) have attracted considerable attention as they\nare capable of showcasing impressive capabilities generating comparable\nhigh-quality responses to human inputs. LLMs, can not only compose textual\nscripts such as emails and essays but also executable programming code.\nContrary, the automated reasoning capability of these LLMs in performing\nstatistically-driven descriptive analysis, particularly on user-specific data\nand as personal assistants to users with limited background knowledge in an\napplication domain who would like to carry out basic, as well as advanced\nstatistical and domain-specific analysis is not yet fully explored. More\nimportantly, the performance of these LLMs has not been compared and discussed\nin detail when domain-specific data analysis tasks are needed. This study,\nconsequently, explores whether LLMs can be used as generative AI-based personal\nassistants to users with minimal background knowledge in an application domain\ninfer key data insights. To demonstrate the performance of the LLMs, the study\nreports a case study through which descriptive statistical analysis, as well as\nNatural Language Processing (NLP) based investigations, are performed on a\nnumber of phishing emails with the objective of comparing the accuracy of the\nresults generated by LLMs to the ones produced by analysts. The experimental\nresults show that LangChain and the Generative Pre-trained Transformer (GPT-4)\nexcel in numerical reasoning tasks i.e., temporal statistical analysis, achieve\ncompetitive correlation with human judgments on feature engineering tasks while\nstruggle to some extent on domain specific knowledge reasoning, where\ndomain-specific knowledge is required.\n"
    },
    {
        "paper_id": 2405.20094,
        "authors": "Reza Arabpour, John Armstrong, Luca Galimberti, Anastasis Kratsios,\n  Giulia Livieri",
        "title": "Low-dimensional approximations of the conditional law of Volterra\n  processes: a non-positive curvature approach",
        "comments": "Main body: 25 Pages, Appendices 29 Pages, 14 Tables, 6 Figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Predicting the conditional evolution of Volterra processes with stochastic\nvolatility is a crucial challenge in mathematical finance. While deep neural\nnetwork models offer promise in approximating the conditional law of such\nprocesses, their effectiveness is hindered by the curse of dimensionality\ncaused by the infinite dimensionality and non-smooth nature of these problems.\nTo address this, we propose a two-step solution. Firstly, we develop a stable\ndimension reduction technique, projecting the law of a reasonably broad class\nof Volterra process onto a low-dimensional statistical manifold of non-positive\nsectional curvature. Next, we introduce a sequentially deep learning model\ntailored to the manifold's geometry, which we show can approximate the\nprojected conditional law of the Volterra process. Our model leverages an\nauxiliary hypernetwork to dynamically update its internal parameters, allowing\nit to encode non-stationary dynamics of the Volterra process, and it can be\ninterpreted as a gating mechanism in a mixture of expert models where each\nexpert is specialized at a specific point in time. Our hypernetwork further\nallows us to achieve approximation rates that would seemingly only be possible\nwith very large networks.\n"
    },
    {
        "paper_id": 2405.20399,
        "authors": "Rafael Andersson Lipcsey",
        "title": "AI Diffusion to Low-Middle Income Countries; A Blessing or a Curse?",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Rapid advancements in AI have sparked significant research into its impacts\non productivity and labor, which can be profoundly positive or negative. Often\noverlooked in this debate is understanding of how AI technologies spread across\nand within economies. Equally ignored are developing economies facing\nsubstantial labor market impacts from rapid, and a loss in competitiveness,\nfrom slow AI diffusion. This paper reviews literature on technology diffusion\nand proposes a three-way framework for understanding AI diffusion: global value\nchains, research collaboration, and inter-firm knowledge transfers. This is\nused to measure AI diffusion in sixteen low-middle-income, and four developed\neconomies, as well as to evaluate dependence on China and the USA for access to\nAI technologies. The study finds a significant gap in diffusion rates between\nthe two groups, but current trends indicate it is narrowing. China is\nidentified as a crucial future source of AI diffusion through value chains,\nwhile the USA is more influential in research and knowledge transfers. The\npaper's limitations include the omission of additional data sources and\ncountries, and the lack of investigation into the relationship between\ndiffusion and technology intensity. Nonetheless, it raises salient macro-level\nquestions about AI diffusion and suggests emphasis on redistribution mechanisms\nof AI induced economic gains, and bilateral agreements as a complement to\ninternational accords, to address diverse needs and corresponding risks faced\nby economies transitioning into an AI-dominated era. Additionally, it\nhighlights the need for research into the links between AI diffusion,\ntechnology intensity, and productivity; case studies combined with targeted\npolicy recommendations; more accurate methods for measuring AI diffusion; and a\ndeeper investigation into its labor market impacts particular to LMICs.\n"
    },
    {
        "paper_id": 2405.20522,
        "authors": "Alice Da Fonseca and Peter Lake and Ariana Barrenechea",
        "title": "Visualization of Board of Director Connections for Analysis in Socially\n  Responsible Investing",
        "comments": "16 pages, 17 figures and 1 table. Article on Finance Social Network",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  This project is a collaboration between industry and academia to delve into\nFinance Social Networks, specifically the Board of Directors of public\ncompanies. Knowing the connections between Directors and Executives in\ndifferent companies can generate powerful stories and meaningful insights on\ninvestments. A proof of concept in the form of a Data Visualization tool\nreveals its strength in investigating corporate governance and sustainability,\nas well as in the partnership between industry and academic institutions.\n"
    },
    {
        "paper_id": 2405.20564,
        "authors": "Giampaolo Bonomi",
        "title": "Divide and Diverge: Polarization Incentives",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We study polarization in a probabilistic voting model with aggregate shocks\nand a decreasing marginal utility from office rents. In equilibrium, parties\noffer different policies, despite being rent-motivated and ex-ante identical\nfrom the point of view of voters. When candidates compete on a single policy\nissue, parties' equilibrium payoffs increase in voter polarization, even when\nthe change is driven by the supporters of the opposite party becoming more\nextreme. With multiple policy issues, parties benefit if the society is split\ninto two factions and the ideological cohesion within such factions increases.\nWe connect our results to empirical evidence on polarizing political\ncommunication, party identity, and zero-sum thinking, and find that\npolarization could be reduced by intervening on the electoral rule.\n"
    },
    {
        "paper_id": 2405.20604,
        "authors": "Paul Goldsmith-Pinkham",
        "title": "Tracking the Credibility Revolution across Fields",
        "comments": "Update with minor tweaks + full cite of Currie kleven and Zwiers\n  (instead of et al.)",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  This paper updates Currie, Kleven, and Zwiers (2020) by examining the\ncredibility revolution across fields, including finance and macroeconomics,\nusing NBER working papers up to May 2024. While the growth in terms related to\nidentification and research designs have continued, finance and macroeconomics\nhave lagged behind applied micro. Difference-in-differences and regression\ndiscontinuity designs have risen since 2002, but the growth in\ndifference-in-difference has been larger, more persistent, and more ubiquitous.\nIn contrast, instrumental variables have stayed flat over this period. Finance\nand macro, particularly corporate finance, has experienced significant growth\nin mentions of experimental and quasi-experimental methods and identification\nover this time period, but a large component of the credibility revolution in\nfinance is due to difference-in-differences. Bartik and shift-share instruments\nhave grown across all fields, with the most pronounced growth in international\ntrade and investment, economic history, and labor studies. Synthetic control\nhas not seen continued growth, and has fallen since 2020.\n"
    },
    {
        "paper_id": 2405.20679,
        "authors": "Fernando Acebes, Jos\\'e Manuel Gonz\\'alez-Varona, Adolfo\n  L\\'opez-Paredes, Javier Pajares",
        "title": "Beyond probability-impact matrices in project risk management: A\n  quantitative methodology for risk prioritisation",
        "comments": "13 pages",
        "journal-ref": "Humanit Soc Sci Commun 11, 670 (2024)",
        "doi": "10.1057/s41599-024-03180-5",
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  The project managers who deal with risk management are often faced with the\ndifficult task of determining the relative importance of the various sources of\nrisk that affect the project. This prioritisation is crucial to direct\nmanagement efforts to ensure higher project profitability. Risk matrices are\nwidely recognised tools by academics and practitioners in various sectors to\nassess and rank risks according to their likelihood of occurrence and impact on\nproject objectives. However, the existing literature highlights several\nlimitations to use the risk matrix. In response to the weaknesses of its use,\nthis paper proposes a novel approach for prioritising project risks. Monte\nCarlo Simulation (MCS) is used to perform a quantitative prioritisation of\nrisks with the simulation software MCSimulRisk. Together with the definition of\nproject activities, the simulation includes the identified risks by modelling\ntheir probability and impact on cost and duration. With this novel methodology,\na quantitative assessment of the impact of each risk is provided, as measured\nby the effect that it would have on project duration and its total cost. This\nallows the differentiation of critical risks according to their impact on\nproject duration, which may differ if cost is taken as a priority objective.\nThis proposal is interesting for project managers because they will, on the one\nhand, know the absolute impact of each risk on their project duration and cost\nobjectives and, on the other hand, be able to discriminate the impacts of each\nrisk independently on the duration objective and the cost objective.\n"
    },
    {
        "paper_id": 2405.20688,
        "authors": "Fernando Acebes, David Curto, Juan de Anton, Felix Villafanez",
        "title": "Analisis cuantitativo de riesgos utilizando \"MCSimulRisk\" como\n  herramienta didactica",
        "comments": "13 pages, in Spanish language",
        "journal-ref": "DyO - 82 (Abril 2024)",
        "doi": "10.37610/dyo.v0i82.662",
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  Risk management is a fundamental discipline in project management, which\nincludes, among others, quantitative risk analysis. Throughout several years of\nteaching, we have observed difficulties in students performing Monte Carlo\nSimulation within the quantitative analysis of risks. This article aims to\npresent MCSimulRisk as a teaching tool that allows students to perform Monte\nCarlo simulation and apply it to projects of any complexity simply and\nintuitively. This tool allows for incorporating any uncertainty identified in\nthe project into the model.\n"
    },
    {
        "paper_id": 2405.20715,
        "authors": "Diabul Haque",
        "title": "Transforming Japan Real Estate",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  The Japanese real estate market, valued over 35 trillion USD, offers\nsignificant investment opportunities. Accurate rent and price forecasting could\nprovide a substantial competitive edge. This paper explores using alternative\ndata variables to predict real estate performance in 1100 Japanese\nmunicipalities. A comprehensive house price index was created, covering all\nmunicipalities from 2005 to the present, using a dataset of over 5 million\ntransactions. This core dataset was enriched with economic factors spanning\ndecades, allowing for price trajectory predictions.\n  The findings show that alternative data variables can indeed forecast real\nestate performance effectively. Investment signals based on these variables\nyielded notable returns with low volatility. For example, the net migration\nratio delivered an annualized return of 4.6% with a Sharpe ratio of 1.5.\nTaxable income growth and new dwellings ratio also performed well, with\nannualized returns of 4.1% (Sharpe ratio of 1.3) and 3.3% (Sharpe ratio of\n0.9), respectively. When combined with transformer models to predict\nrisk-adjusted returns 4 years in advance, the model achieved an R-squared score\nof 0.28, explaining nearly 30% of the variation in future municipality prices.\n  These results highlight the potential of alternative data variables in real\nestate investment. They underscore the need for further research to identify\nmore predictive factors. Nonetheless, the evidence suggests that such data can\nprovide valuable insights into real estate price drivers, enabling more\ninformed investment decisions in the Japanese market.\n"
    },
    {
        "paper_id": 2405.20822,
        "authors": "Pablo de la Vega, Guido Zack, Jimena Calvo and Emiliano Libman",
        "title": "Inflation Determinants in Argentina (2004-2022)",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
        "abstract": "  This paper analyzes the empirical relationship between the inflation rate and\nits proximate determinants in Argentina, using quarterly data over the period\n2004-2022 and an error-correction vector model approach. Unlike previous\nliterature, this paper uses a theoretical framework to motivate the inclusion\nof variables that are expected to contribute to explain inflation, thus\nreducing the risk of omitting relevant variables and formalizing key\nmechanisms. Inference is performed through Granger causality analysis, impulse\nresponse functions and forecast errors variance decomposition. The results\nsuggest that an anti-inflationary plan for Argentina should take into\nconsideration both the greater relevance of the inertial component, the\nexchange rate and the interest rate in the short-run dynamics of the price\nlevel, and the long-run relationship between prices, interest rate and activity\nlevel.\n"
    },
    {
        "paper_id": 2405.20912,
        "authors": "Andreas Hagn, Rainer Kolisch, Giacomo Dall'Olio, Stefan Weltge",
        "title": "A Branch-Price-Cut-And-Switch Approach for Optimizing Team Formation and\n  Routing for Airport Baggage Handling Tasks with Stochastic Travel Times",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  In airport operations, optimally using dedicated personnel for baggage\nhandling tasks plays a crucial role in the design of resource-efficient\nprocesses. Teams of workers with different qualifications must be formed, and\nloading or unloading tasks must be assigned to them. Each task has a time\nwindow within which it can be started and should be finished. Violating these\ntemporal restrictions incurs severe financial penalties for the operator. In\npractice, various components of this process are subject to uncertainties. We\nconsider the aforementioned problem under the assumption of stochastic travel\ntimes across the apron. We present two binary program formulations to model the\nproblem at hand and solve it with a Branch-Price-Cut-and-Switch approach, in\nwhich we dynamically switch between two master problem formulations.\nFurthermore, we use an exact separation method to identify violated rank-1\nChv\\'atal-Gomory cuts and utilize an efficient branching rule relying on task\nfinish times. We test the algorithm on instances generated based on real-world\ndata from a major European hub airport with a planning horizon of up to two\nhours, 30 flights per hour, and three available task execution modes to choose\nfrom. Our results indicate that our algorithm is able to significantly\noutperform existing solution approaches. Moreover, an explicit consideration of\nstochastic travel times allows for solutions that utilize the available\nworkforce more efficiently, while simultaneously guaranteeing a stable service\nlevel for the baggage handling operator.\n"
    },
    {
        "paper_id": 2406.00076,
        "authors": "Jose M. Gonzalez-Varona, Natalia Martin-Cruz, Fernando Acebes, Javier\n  Pajares",
        "title": "How public funding affects complexity in R&D projects. An analysis of\n  team project perceptions",
        "comments": "12 pages",
        "journal-ref": "Journal of Business Research 158, 113672 2023",
        "doi": "10.1016/j.jbusres.2023.113672",
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  In this paper, we apply a case study approach to advance current\nunderstanding of what effects public co-funding of R&D projects have on project\nteam members' perceived complexity. We chose an R&D project carried out by an\nindustrial SME in northern Spain. The chosen research strategy was a\nqualitative approach, and sixteen employees participated in the project. We\nheld in-depth semi-structured interviews at the beginning and end of the\nco-funded part of the project. NVivo data analysis software was used for\nqualitative data analysis. Results showed a substantial increase in perceived\ncomplexity. We observed that this was due to unresolved tension between the\nrequirements of the project's co-financing entity and normal SME working\nprocedures. New working procedures needed to be developed in order to comply\nwith the co-financing entity's requirements. However, overall perceived\ncomplexity significantly decreased once the co-financed part of the project was\ncompleted.\n"
    },
    {
        "paper_id": 2406.00077,
        "authors": "Fernando Acebes, David Poza, Jose M Gonzalez-Varona, Javier Pajares,\n  Adolfo Lopez-Paredes",
        "title": "On the project risk baseline: integrating aleatory uncertainty into\n  project scheduling",
        "comments": "11 pages",
        "journal-ref": "Computers & Industrial Engineering, 160(2021), 107537. 2021",
        "doi": "10.1016/j.cie.2021.107537",
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  Obtaining a viable schedule baseline that meets all project constraints is\none of the main issues for project managers. The literature on this topic\nfocuses mainly on methods to obtain schedules that meet resource restrictions\nand, more recently, financial limitations. The methods provide different viable\nschedules for the same project, and the solutions with the shortest duration\nare considered the best-known schedule for that project. However, no tools\ncurrently select which schedule best performs in project risk terms. To bridge\nthis gap, this paper aims to propose a method for selecting the project\nschedule with the highest probability of meeting the deadline of several\nalternative schedules with the same duration. To do so, we propose integrating\naleatory uncertainty into project scheduling by quantifying the risk of several\nexecution alternatives for the same project. The proposed method, tested with a\nwell-known repository for schedule benchmarking, can be applied to any project\ntype to help managers to select the project schedules from several alternatives\nwith the same duration, but the lowest risk.\n"
    },
    {
        "paper_id": 2406.00078,
        "authors": "Fernando Acebes, Javier Pajares, Jose M Gonzalez-Varona, Adolfo\n  Lopez-Paredes",
        "title": "Project Risk Management from the bottom-up: Activity Risk Index",
        "comments": "22 pages",
        "journal-ref": "Cent Eur J Oper Res 29, 1375-1396 (2021)",
        "doi": "10.1007/s10100-020-00703-8",
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  Project managers need to manage risks throughout the project lifecycle and,\nthus, need to know how changes in activity durations influence project duration\nand risk. We propose a new indicator (the Activity Risk Index, ARI) that\nmeasures the contribution of each activity to the total project risk while it\nis underway. In particular, the indicator informs us about what activities\ncontribute the most to the project's uncertainty so that project managers can\npay closer attention to the performance of these activities. The main\ndifference between our indicator and other activity sensitivity metrics in the\nliterature (e.g. cruciality, criticality, significance, or schedule sensitivity\nindices) is that our indicator is based on the Schedule Risk Baseline concept\ninstead of on cost or schedule baselines. The new metric not only provides\ninformation at the beginning of the project, but also while it is underway.\nFurthermore, the ARI is the only one to offer a normalized result: if we add\nits value for each activity, the total sum is 100%.\n"
    },
    {
        "paper_id": 2406.00113,
        "authors": "Ciamac C. Moallemi and Dan Robinson",
        "title": "Loss-Versus-Fair: Efficiency of Dutch Auctions on Blockchains",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Milionis et al.(2023) studied the rate at which automated market makers leak\nvalue to arbitrageurs when block times are discrete and follow a Poisson\nprocess, and where the risky asset price follows a geometric Brownian motion.\nWe extend their model to analyze another popular mechanism in decentralized\nfinance for onchain trading: Dutch auctions. We compute the expected losses\nthat a seller incurs to arbitrageurs and expected time-to-fill for Dutch\nauctions as a function of starting price, volatility, decay rate, and average\ninterblock time. We also extend the analysis to gradual Dutch auctions, a\nvariation on Dutch auctions for selling tokens over time at a continuous rate.\nWe use these models to explore the tradeoff between speed of execution and\nquality of execution, which could help inform practitioners in setting\nparameters for starting price and decay rate on Dutch auctions, or help\nplatform designers determine performance parameters like block times.\n"
    },
    {
        "paper_id": 2406.00435,
        "authors": "Yang Liu and Zhenyu Shen",
        "title": "Modelling Non-monotone Risk Aversion and Convex Compensation in\n  Incomplete Markets",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In hedge funds, convex compensation schemes are popular to stimulate a\nhigh-profit performance for portfolio managers. In economics, non-monotone risk\naversion is proposed to argue that individuals may not be risk-averse when the\nwealth level is low. Combining these two ingredients, we study the optimal\ncontrol strategy of the manager in incomplete markets. Generally, we propose a\nwide class of utility functions, the Piecewise Symmetric Asymptotic Hyperbolic\nAbsolute Risk Aversion (PSAHARA) utility, to model the two ingredients,\ncontaining both non-concavity and non-differentiability as some abnormalities.\nSignificantly, we derive an explicit optimal control for the family of PSAHARA\nutilities. This control is expressed into a unified four-term structure,\nfeaturing the asymptotic Merton term and the risk adjustment term. Furthermore,\nwe provide a detailed asymptotic analysis and numerical illustration of the\noptimal portfolio. We obtain the following key insights: (i) A manager with the\nPSAHARA utility becomes extremely risk-seeking when his/her wealth level tends\nto zero; (ii) The optimal investment ratio tends to the Merton constant as the\nwealth level approaches infinity and the negative Merton constant when the\nwealth falls to negative infinity, implying that such a manager takes a\nrisk-seeking investment as the wealth falls negatively low; (iii) The convex\ncompensation still induces a great risk-taking behavior in the case that the\npreference is modeled by SAHARA utility. Finally, we conduct a real-data\nanalysis of the U.S. stock market under the above model and conclude that the\nPSAHARA portfolio is very risk-seeking and leads to a high return and a high\nvolatility (two-peak Sharpe ratio).\n"
    },
    {
        "paper_id": 2406.00459,
        "authors": "Lei Fan, Justin Sirignano",
        "title": "Machine Learning Methods for Pricing Financial Derivatives",
        "comments": "27 pages, 16 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Stochastic differential equation (SDE) models are the foundation for pricing\nand hedging financial derivatives. The drift and volatility functions in SDE\nmodels are typically chosen to be algebraic functions with a small number (less\nthan 5) parameters which can be calibrated to market data. A more flexible\napproach is to use neural networks to model the drift and volatility functions,\nwhich provides more degrees-of-freedom to match observed market data. Training\nof models requires optimizing over an SDE, which is computationally\nchallenging. For European options, we develop a fast stochastic gradient\ndescent (SGD) algorithm for training the neural network-SDE model. Our SGD\nalgorithm uses two independent SDE paths to obtain an unbiased estimate of the\ndirection of steepest descent. For American options, we optimize over the\ncorresponding Kolmogorov partial differential equation (PDE). The neural\nnetwork appears as coefficient functions in the PDE. Models are trained on\nlarge datasets (many contracts), requiring either large simulations (many Monte\nCarlo samples for the stock price paths) or large numbers of PDEs (a PDE must\nbe solved for each contract). Numerical results are presented for real market\ndata including S&P 500 index options, S&P 100 index options, and single-stock\nAmerican options. The neural-network-based SDE models are compared against the\nBlack-Scholes model, the Dupire's local volatility model, and the Heston model.\nModels are evaluated in terms of how accurate they are at pricing out-of-sample\nfinancial derivatives, which is a core task in derivative pricing at financial\ninstitutions.\n"
    },
    {
        "paper_id": 2406.0061,
        "authors": "Qiqin Zhou",
        "title": "Portfolio Optimization with Robust Covariance and Conditional\n  Value-at-Risk Constraints",
        "comments": "11 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  The measure of portfolio risk is an important input of the Markowitz\nframework. In this study, we explored various methods to obtain a robust\ncovariance estimators that are less susceptible to financial data noise. We\nevaluated the performance of large-cap portfolio using various forms of Ledoit\nShrinkage Covariance and Robust Gerber Covariance matrix during the period of\n2012 to 2022. Out-of-sample performance indicates that robust covariance\nestimators can outperform the market capitalization-weighted benchmark\nportfolio, particularly during bull markets. The Gerber covariance with\nMean-Absolute-Deviation (MAD) emerged as the top performer. However, robust\nestimators do not manage tail risk well under extreme market conditions, for\nexample, Covid-19 period. When we aim to control for tail risk, we should add\nconstraint on Conditional Value-at-Risk (CVaR) to make more conservative\ndecision on risk exposure. Additionally, we incorporated unsupervised\nclustering algorithm K-means to the optimization algorithm (i.e. Nested\nClustering Optimization, NCO). It not only helps mitigate numerical instability\nof the optimization algorithm, but also contributes to lower drawdown as well.\n"
    },
    {
        "paper_id": 2406.00655,
        "authors": "Andrzej Cichocki, Sergio Cruces, Auxiliadora Sarmiento, Toshihisa\n  Tanaka",
        "title": "Generalized Exponentiated Gradient Algorithms and Their Application to\n  On-Line Portfolio Selection",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
        "abstract": "  This paper introduces a novel family of generalized exponentiated gradient\n(EG) updates derived from an Alpha-Beta divergence regularization function.\nCollectively referred to as EGAB, the proposed updates belong to the category\nof multiplicative gradient algorithms for positive data and demonstrate\nconsiderable flexibility by controlling iteration behavior and performance\nthrough three hyperparameters: $\\alpha$, $\\beta$, and the learning rate $\\eta$.\nTo enforce a unit $l_1$ norm constraint for nonnegative weight vectors within\ngeneralized EGAB algorithms, we develop two slightly distinct approaches. One\nmethod exploits scale-invariant loss functions, while the other relies on\ngradient projections onto the feasible domain. As an illustration of their\napplicability, we evaluate the proposed updates in addressing the online\nportfolio selection problem (OLPS) using gradient-based methods. Here, they not\nonly offer a unified perspective on the search directions of various OLPS\nalgorithms (including the standard exponentiated gradient and diverse\nmean-reversion strategies), but also facilitate smooth interpolation and\nextension of these updates due to the flexibility in hyperparameter selection.\nSimulation results confirm that the adaptability of these generalized gradient\nupdates can effectively enhance the performance for some portfolios,\nparticularly in scenarios involving transaction costs.\n"
    },
    {
        "paper_id": 2406.00665,
        "authors": "Sunwoo Kim, Joungho Park, and Jay H. Lee",
        "title": "Integrating solid direct air capture systems with green hydrogen\n  production: Economic synergy of sector coupling",
        "comments": "Some of the results of our previous preprint paper are flawed, and we\n  are withdrawing them to prevent the spread of incorrect knowledge",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In the global pursuit of sustainable energy solutions, mitigating carbon\ndioxide (CO2) emissions stands as a pivotal challenge. With escalating\natmospheric CO2 levels, the imperative of direct air capture (DAC) systems\nbecomes evident. Simultaneously, green hydrogen (GH) emerges as a pivotal\nmedium for renewable energy. Nevertheless, the substantial expenses associated\nwith these technologies impede widespread adoption, primarily due to\nsignificant installation costs and underutilized operational advantages when\ndeployed independently. Integration through sector coupling enhances system\nefficiency and sustainability, while shared power sources and energy storage\ndevices offer additional economic benefits. In this study, we assess the\neconomic viability of polymer electrolyte membrane electrolyzers versus\nalkaline electrolyzers within the context of sector coupling. Our findings\nindicate that combining GH production with solid DAC systems yields significant\neconomic advantages, with approximately a 10% improvement for PEM electrolyzers\nand a 20% enhancement for alkaline electrolyzers. These results highlight a\nsubstantial opportunity to improve the efficiency and economic viability of\nrenewable energy and green hydrogen initiatives, thereby facilitating the\nbroader adoption of cleaner technologies.\n"
    },
    {
        "paper_id": 2406.00669,
        "authors": "Sunwoo Kim, Joungho Park, Jay H. Lee",
        "title": "Multi-technology co-optimization approach for sustainable hydrogen and\n  electricity supply chains considering variability and demand scale",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In the pursuit of a carbon-neutral future, hydrogen emerges as a pivotal\nelement, serving as a carbon-free energy carrier and feedstock. As efforts to\ndecarbonize sectors such as heating and transportation intensify, understanding\nand navigating through the dynamics of hydrogen demand expansion becomes\ncritical. Transitioning to hydrogen economy is complicated by varying regional\nscales and types of hydrogen demand, with forecasts indicating a rise in\nvariable demand that calls for diverse production technologies. Currently,\nsteam methane reforming is prevalent, but its significant carbon emissions make\na shift to cleaner alternatives like blue and green hydrogen imperative. Each\nproduction method possesses distinct characteristics, necessitating a thorough\nexploration and co-optimization with electricity supply chains as well as\ncarbon capture, utilization, and storage systems. Our study fills existing\nresearch gaps by introducing a superstructure optimization framework that\naccommodates various demand scenarios and technologies. Through case studies in\nCalifornia, we underscore the critical role of demand profiles in shaping the\noptimal configurations and economics of supply chains and emphasize the need\nfor diversified portfolios and co-optimization to facilitate sustainable energy\ntransitions.\n"
    },
    {
        "paper_id": 2406.00998,
        "authors": "Benjamin Avanzi, Eric Dong, Patrick J. Laub, Bernard Wong",
        "title": "Distributional Refinement Network: Distributional Forecasting via Deep\n  Learning",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  A key task in actuarial modelling involves modelling the distributional\nproperties of losses. Classic (distributional) regression approaches like\nGeneralized Linear Models (GLMs; Nelder and Wedderburn, 1972) are commonly\nused, but challenges remain in developing models that can (i) allow covariates\nto flexibly impact different aspects of the conditional distribution, (ii)\nintegrate developments in machine learning and AI to maximise the predictive\npower while considering (i), and, (iii) maintain a level of interpretability in\nthe model to enhance trust in the model and its outputs, which is often\ncompromised in efforts pursuing (i) and (ii). We tackle this problem by\nproposing a Distributional Refinement Network (DRN), which combines an\ninherently interpretable baseline model (such as GLMs) with a flexible neural\nnetwork-a modified Deep Distribution Regression (DDR; Li et al., 2019) method.\nInspired by the Combined Actuarial Neural Network (CANN; Schelldorfer and\nW{\\''u}thrich, 2019), our approach flexibly refines the entire baseline\ndistribution. As a result, the DRN captures varying effects of features across\nall quantiles, improving predictive performance while maintaining adequate\ninterpretability. Using both synthetic and real-world data, we demonstrate the\nDRN's superior distributional forecasting capacity. The DRN has the potential\nto be a powerful distributional regression model in actuarial science and\nbeyond.\n"
    },
    {
        "paper_id": 2406.01168,
        "authors": "Shumiao Ouyang, Hayong Yun, Xingjian Zheng",
        "title": "How Ethical Should AI Be? How AI Alignment Shapes the Risk Preferences\n  of LLMs",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This study examines the risk preferences of Large Language Models (LLMs) and\nhow aligning them with human ethical standards affects their economic\ndecision-making. Analyzing 30 LLMs reveals a range of inherent risk profiles,\nfrom risk-averse to risk-seeking. We find that aligning LLMs with human values,\nfocusing on harmlessness, helpfulness, and honesty, shifts them towards risk\naversion. While some alignment improves investment forecast accuracy, excessive\nalignment leads to overly cautious predictions, potentially resulting in severe\nunderinvestment. Our findings highlight the need for a nuanced approach that\nbalances ethical alignment with the specific requirements of economic domains\nwhen using LLMs in finance.\n"
    },
    {
        "paper_id": 2406.01199,
        "authors": "Alexandre V. Antonov, Koushik Balasubramanian, Alexander Lipton,\n  Marcos Lopez de Prado",
        "title": "A Geometric Approach To Asset Allocation With Investor Views",
        "comments": "Abstract Page + TOC + 35 pages, 4 Figures, 4 Tables",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  In this article, a geometric approach to incorporating investor views in\nportfolio construction is presented. In particular, the proposed approach\nutilizes the notion of generalized Wasserstein barycenter (GWB) to combine the\nstatistical information about asset returns with investor views to obtain an\nupdated estimate of the asset drifts and covariance, which are then fed into a\nmean-variance optimizer as inputs. Quantitative comparisons of the proposed\ngeometric approach with the conventional Black-Litterman model (and a closely\nrelated variant) are presented. The proposed geometric approach provides\ninvestors with more flexibility in specifying their confidence in their views\nthan conventional Black-Litterman model-based approaches. The geometric\napproach also rewards the investors more for making correct decisions than\nconventional BL based approaches. We provide empirical and theoretical\njustifications for our claim.\n"
    },
    {
        "paper_id": 2406.01335,
        "authors": "Xi-Ning Zhuang, Zhao-Yun Chen, Cheng Xue, Xiao-Fan Xu, Chao Wang,\n  Huan-Yu Liu, Tai-Ping Sun, Yun-Jie Wang, Yu-Chun Wu, and Guo-Ping Guo",
        "title": "Statistics-Informed Parameterized Quantum Circuit via Maximum Entropy\n  Principle for Data Science and Finance",
        "comments": "19 pages, 5 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Quantum machine learning has demonstrated significant potential in solving\npractical problems, particularly in statistics-focused areas such as data\nscience and finance. However, challenges remain in preparing and learning\nstatistical models on a quantum processor due to issues with trainability and\ninterpretability. In this letter, we utilize the maximum entropy principle to\ndesign a statistics-informed parameterized quantum circuit (SI-PQC) for\nefficiently preparing and training of quantum computational statistical models,\nincluding arbitrary distributions and their weighted mixtures. The SI-PQC\nfeatures a static structure with trainable parameters, enabling in-depth\noptimized circuit compilation, exponential reductions in resource and time\nconsumption, and improved trainability and interpretability for learning\nquantum states and classical model parameters simultaneously. As an efficient\nsubroutine for preparing and learning in various quantum algorithms, the SI-PQC\naddresses the input bottleneck and facilitates the injection of prior\nknowledge.\n"
    },
    {
        "paper_id": 2406.01407,
        "authors": "Jochen Wulf, J\\\"urg Meierhofer",
        "title": "Utilizing Large Language Models for Automating Technical Customer\n  Support",
        "comments": "arXiv admin note: text overlap with arXiv:2405.09161",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  The use of large language models (LLMs) such as OpenAI's GPT-4 in technical\ncustomer support (TCS) has the potential to revolutionize this area. This study\nexamines automated text correction, summarization of customer inquiries and\nquestion answering using LLMs. Through prototypes and data analyses, the\npotential and challenges of integrating LLMs into the TCS will be demonstrated.\nOur results show promising approaches for improving the efficiency and quality\nof customer service through LLMs, but also emphasize the need for\nquality-assured implementation and organizational adjustments in the data\necosystem.\n"
    },
    {
        "paper_id": 2406.01614,
        "authors": "Fernando Acebes, David Poza, Jose Manuel Gonzalez-Varona, Adolfo\n  Lopez-Paredes",
        "title": "Stochastic Earned Duration Analysis for Project Schedule Management",
        "comments": "14 pages",
        "journal-ref": "Engineering 8 (February 2022), 148-161 2022",
        "doi": "10.1016/j.eng.2021.07.019",
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  Earned duration management (EDM) is a methodology for project schedule\nmanagement (PSM) that can be considered an alternative to earned value\nmanagement (EVM). EDM provides an estimation of deviations in schedule and a\nfinal project duration estimation. There is a key difference between EDM and\nEVM: In EDM, the value of activities is expressed as work periods; whereas in\nEVM, value is expressed in terms of cost. In this paper, we present how EDM can\nbe applied to monitor and control stochastic projects. To explain the\nmethodology, we use a real case study with a project that presents a high level\nof uncertainty and activities with random durations. We analyze the usability\nof this approach according to the activities network topology and compare the\nEVM and earned schedule methodology (ESM) for PSM.\n"
    },
    {
        "paper_id": 2406.01615,
        "authors": "Jose M Gonzalez-Varona, Adolfo Lopez-Paredes, David Poza, Fernando\n  Acebes",
        "title": "Building and development of an organizational competence for digital\n  transformation in SMEs",
        "comments": "10 pages",
        "journal-ref": "Journal of Industrial Engineering and Management (JIEM) 14 (1),\n  15-24 2021",
        "doi": "10.3926/jiem.3279",
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  Purpose: The new competitive environment characterized by innovation and\nconstant change is forcing a new organizational behavior. This requires a\ndigital transformation of SMEs based on collective performance determinants.\nSMEs have particular characteristics that differentiate them from large\ncompanies and a model that allows them to identify, leverage and develop their\ndigital capabilities can help them to advance in digital maturity.\n  Design/methodology/approach: An in-depth review of the existing literature on\ndigital transformation and organizational competence was carried out on Scopus\nand Web of Science to identify the digital challenges faced by SMEs, and what\ndigital capabilities they have to develop to face these challenges. In order to\nobtain the necessary information for the refinement of organizational\ncompetence for digital transformation model, six experts were interviewed;\nthree of them are academics and the other three are professionals with\nmanagement responsibilities in SMEs. We used semi-structured interviews, to\nkeep the interviews focused and facilitate cross-data analysis between experts.\nIn addition, it allowed us the possibility of analyzing new relevant aspects\nthat could arise during the interview.\n  Findings: As a result of this study we have developed a refined model of\norganizational competence for digital transformation that allows SMEs to\nidentify and develop the digital capabilities necessary to advance in the\ndigital transformation, refined with the opinions of six experts consulted. We\nwere able to observe the importance of organizational learning and\norganizational knowledge to advance the digital transformation of SMEs.\n"
    },
    {
        "paper_id": 2406.01616,
        "authors": "Jose M Gonzalez-Varona, Adolfo Lopez-Paredes, Javier Pajares, Fernando\n  Acebes, Felix Villafanez",
        "title": "Applicability of Business Intelligence Maturity Models to SMEs",
        "comments": "15 pages, in Spanish language",
        "journal-ref": "Direccion y Organizacion 71, 31-45 2020",
        "doi": "10.37610/dyo.v0i71.577",
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  Large companies are fully engaged in their digital transformation,\nspecifically in developing strategic Business Intelligence (BI) projects. They\nhave a Digital Strategy and top-level executives managing the change. BI\nprojects are also being carried out in SMEs. In this paper, we present the\nresults of an interpretative study conducted on a sample of SMEs from different\nsectors carrying out BI projects. We study whether BI maturity models are valid\nfor SMEs and their usefulness for analysing the projects and evaluating the\nresults at the end of the project.\n"
    },
    {
        "paper_id": 2406.0164,
        "authors": "Emir Fejzic, Will Usher",
        "title": "Stakeholder-driven research in the European Climate and Energy Modelling\n  Forum",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  A fast-paced policy context is characteristic of energy and climate research,\nwhich strives to develop solutions to wicked problems such as climate change.\nFunding agencies in the European Union recognize the importance of linking\nresearch and policy in climate and energy research. This calls for an increased\nunderstanding of how stakeholder engagement can effectively be used to\nco-design research questions that include stakeholders' concerns. This paper\nreviews the current literature on stakeholder engagement, from which we create\na set of criteria. These are used to critically assess recent and relevant\npapers on stakeholder engagement in climate and energy projects. We obtained\nthe papers from a scoping review of stakeholder engagement through workshops in\nEU climate and energy research. With insights from the literature and current\nEU climate and energy projects, we developed a workshop programme for\nstakeholder engagement. This programme was applied to the European Climate and\nEnergy Modelling Forum project, aiming to co-design the most pressing and\nurgent research questions according to European stakeholders. The outcomes\ninclude 82 co-designed and ranked research questions for nine specific climate\nand energy research themes. Findings from the scoping review indicate that\npapers rarely define the term 'stakeholder'. Additionally, the concepts of\nco-creation, co-design, and co-production are used interchangeably and often\nwithout definition. We propose that workshop planners use stakeholder\nidentification and selection methods from the broader stakeholder engagement\nliterature.\n"
    },
    {
        "paper_id": 2406.01714,
        "authors": "Alexander Budzier and Bent Flyvbjerg",
        "title": "The Oxford Olympics Study 2024: Are Cost and Cost Overrun at the Games\n  Coming Down?",
        "comments": "27 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-sa/4.0/",
        "abstract": "  The present paper is an update of the \"Oxford Olympics Study 2016\" (Flyvbjerg\net al. 2016). We document that the Games remain costly and continue to have\nlarge cost overruns, to a degree that threatens their viability. The IOC is\naware of the problem and has initiated reform. We assess the reforms and find:\n(a) Olympic costs are statistically significantly increasing; prior analysis\ndid not show this trend; it is a step in the wrong direction. (b) Cost overruns\nwere decreasing until 2008, but have increased since then; again a step in the\nwrong direction. (c) At present, the cost of Paris 2024 is USD 8.7 billion\n(2022 level) and cost overruns is 115% in real terms; this is not the cheap\nGames that were promised. (d) Cost overruns are the norm for the Games, past,\npresent, and future; they are the only project type that never delivered on\nbudget. We assess a new IOC policy of reducing cost by reusing existing venues\ninstead of building new ones. We find that reuse did not have the desired\neffect for Tokyo 2020 and also look ineffective for Paris 2024. Finally, we\nrecommend that the Games look to other types of megaprojects for better data,\nbetter forecasting, and how to generate the positive learning curves that are\nnecessary for bringing costs and overrun down. Only if this happens are Los\nAngeles 2028 and Brisbane 2032 likely to live up to the IOC's intentions of a\nmore affordable Games that more cities will want to host.\n"
    },
    {
        "paper_id": 2406.01722,
        "authors": "Tom\\'as Aguirre",
        "title": "On Labs and Fabs: Mapping How Alliances, Acquisitions, and Antitrust are\n  Shaping the Frontier AI Industry",
        "comments": "This is a preliminary version of the paper. Comments are welcome.\n  E-mail: t6aguirre@gmail.com",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
        "abstract": "  As frontier AI models advance, policy proposals for safe AI development are\ngaining increasing attention from researchers and policymakers. This paper\nexplores the current integration in the AI supply chain, focusing on vertical\nrelationships and strategic partnerships among AI labs, cloud providers, chip\nmanufacturers, and lithography companies. It aims to lay the groundwork for a\ndeeper understanding of the implications of various governance interventions,\nincluding antitrust measures. The study has two main contributions. First, it\nprofiles 25 leading companies in the AI supply chain, analyzing 300\nrelationships and noting 80 significant mergers and acquisitions along with 40\nantitrust cases. Second, we discuss potential market definitions and the\nintegration drivers based on the observed trends. The analysis reveals\npredominant horizontal integration through natural growth rather than\nacquisitions and notable trends of backward vertical integration in the\nsemiconductor supply chain. Strategic partnerships are also significant\ndownstream, especially between AI companies and cloud providers, with large\ntech companies often pursuing conglomerate integration by acquiring specialized\nAI startups or forming alliances with frontier AI labs. To further understand\nthe strategic partnerships in the industry, we provide three brief case studies\nfeaturing companies like OpenAI and Nvidia. We conclude by posing open research\nquestions on market dynamics and possible governance interventions, such as\nlicensing and safety audits.\n"
    },
    {
        "paper_id": 2406.01898,
        "authors": "Mahdi Ebrahimi Kahou, James Yu, Jesse Perla, Geoff Pleiss",
        "title": "How Inductive Bias in Machine Learning Aligns with Optimality in\n  Economic Dynamics",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  This paper examines the alignment of inductive biases in machine learning\n(ML) with structural models of economic dynamics. Unlike dynamical systems\nfound in physical and life sciences, economics models are often specified by\ndifferential equations with a mixture of easy-to-enforce initial conditions and\nhard-to-enforce infinite horizon boundary conditions (e.g. transversality and\nno-ponzi-scheme conditions). Traditional methods for enforcing these\nconstraints are computationally expensive and unstable. We investigate\nalgorithms where those infinite horizon constraints are ignored, simply\ntraining unregularized kernel machines and neural networks to obey the\ndifferential equations. Despite the inherent underspecification of this\napproach, our findings reveal that the inductive biases of these ML models\ninnately enforce the infinite-horizon conditions necessary for the\nwell-posedness. We theoretically demonstrate that (approximate or exact)\nmin-norm ML solutions to interpolation problems are sufficient conditions for\nthese infinite-horizon boundary conditions in a wide class of problems. We then\nprovide empirical evidence that deep learning and ridgeless kernel methods are\nnot only theoretically sound with respect to economic assumptions, but may even\ndominate classic algorithms in low to medium dimensions. More importantly,\nthese results give confidence that, despite solving seemingly ill-posed\nproblems, there are reasons to trust the plethora of black-box ML algorithms\nused by economists to solve previously intractable, high-dimensional dynamical\nsystems -- paving the way for future work on estimation of inverse problems\nwith embedded optimal control problems.\n"
    },
    {
        "paper_id": 2406.02102,
        "authors": "Pablo Alvarez-Campana, Felix Villafanez, Fernando Acebes, David Poza",
        "title": "Simulation-based approach for Multiproject Scheduling based on composite\n  priority rules",
        "comments": "12 pages",
        "journal-ref": "International Journal of Simulation Modelling (IJSIMM) 23 (1) 2024",
        "doi": "10.2507/IJSIMM23-1-667",
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  This paper presents a simulation approach to enhance the performance of\nheuristics for multi-project scheduling. Unlike other heuristics available in\nthe literature that use only one priority criterion for resource allocation,\nthis paper proposes a structured way to sequentially apply more than one\npriority criterion for this purpose. By means of simulation, different feasible\nschedules are obtained to, therefore, increase the probability of finding the\nschedule with the shortest duration. The performance of this simulation\napproach was validated with the MPSPLib library, one of the most prominent\nlibraries for resource-constrained multi-project scheduling. These results\nhighlight the proposed method as a useful option for addressing limited time\nand resources in portfolio management.\n"
    },
    {
        "paper_id": 2406.02155,
        "authors": "Masaaki Fujii, Masashi Sekine",
        "title": "Mean field equilibrium asset pricing model with habit formation",
        "comments": "33 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  This paper presents an asset pricing model in an incomplete market involving\na large number of heterogeneous agents based on the mean field game theory. In\nthe model, we incorporate habit formation in consumption preferences, which has\nbeen widely used to explain various phenomena in financial economics. In order\nto characterize the market-clearing equilibrium, we derive a quadratic-growth\nmean field backward stochastic differential equation (BSDE) and study its\nwell-posedness and asymptotic behavior in the large population limit.\nAdditionally, we introduce an exponential quadratic Gaussian reformulation of\nthe asset pricing model, in which the solution is obtained in a semi-analytic\nform.\n"
    },
    {
        "paper_id": 2406.02314,
        "authors": "Julia Pargmann and Florian Berding",
        "title": "Integrating Sustainability in Controlling and Accounting Practices: A\n  Critical Review and Implications for Competences in German Vocational\n  Business Education",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  Sustainability in accounting and controlling has traditionally been\nunderstood in terms of securing the long-term existence of companies. However,\nwith the introduction of integrated non-financial reporting, sustainability, as\nper the triple bottom line model, is increasingly being discussed as a\ncomponent of accounting and controlling. Yet, integration primarily occurs in\nseparate sustainability management and controlling departments. Moreover, the\nimplementation of sustainability efforts requires suitably qualified employees,\nwho drive the transition. The academic discourse surrounding sustainability in\nbusinesses in general, and in accounting and controlling specifically, is\ncomplex. It remains unclear to what extent sustainability has been integrated\ninto accounting and controlling, and what competencies employees need to manage\nthis transformation. These questions will be critically analyzed in this\nstructured literature review of 79 publications. The results provide insights\ninto a) how companies conceptualize sustainability, b) whether and how they\nintegrate it into their value creation processes, and c) the relevance of\naccounting and controlling for these developments. To contextualize the role of\nemployees, the competency requirements within companies will be analyzed to\nenable employees in accounting and controlling to engage effectively in\nsustainability-oriented activities. Specifically, implications for changes in\ncurricula with a focus on accounting and controlling are derived.\n"
    },
    {
        "paper_id": 2406.02319,
        "authors": "Guido Gazzani, Julien Guyon",
        "title": "Pricing and calibration in the 4-factor path-dependent volatility model",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  We consider the path-dependent volatility (PDV) model of Guyon and Lekeufack\n(2023), where the instantaneous volatility is a linear combination of a\nweighted sum of past returns and the square root of a weighted sum of past\nsquared returns. We discuss the influence of an additional parameter that\nunlocks enough volatility on the upside to reproduce the implied volatility\nsmiles of S&P 500 and VIX options. This PDV model, motivated by empirical\nstudies, comes with computational challenges, especially in relation to VIX\noptions pricing and calibration. We propose an accurate neural network\napproximation of the VIX which leverages on the Markovianity of the 4-factor\nversion of the model. The VIX is learned as a function of the Markovian factors\nand the model parameters. We use this approximation to tackle the joint\ncalibration of S&P 500 and VIX options.\n"
    },
    {
        "paper_id": 2406.02437,
        "authors": "Shidi Deng, Maximilian Schiffer, Martin Bichler",
        "title": "Algorithmic Collusion in Dynamic Pricing with Deep Reinforcement\n  Learning",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Nowadays, a significant share of the Business-to-Consumer sector is based on\nonline platforms like Amazon and Alibaba and uses Artificial Intelligence for\npricing strategies. This has sparked debate on whether pricing algorithms may\ntacitly collude to set supra-competitive prices without being explicitly\ndesigned to do so. Our study addresses these concerns by examining the risk of\ncollusion when Reinforcement Learning algorithms are used to decide on pricing\nstrategies in competitive markets. Prior research in this field focused on\nTabular Q-learning (TQL) and led to opposing views on whether learning-based\nalgorithms can lead to supra-competitive prices. Our work contributes to this\nongoing discussion by providing a more nuanced numerical study that goes beyond\nTQL by additionally capturing off- and on-policy Deep Reinforcement Learning\n(DRL) algorithms. We study multiple Bertrand oligopoly variants and show that\nalgorithmic collusion depends on the algorithm used. In our experiments, TQL\nexhibits higher collusion and price dispersion phenomena compared to DRL\nalgorithms. We show that the severity of collusion depends not only on the\nalgorithm used but also on the characteristics of the market environment. We\nfurther find that Proximal Policy Optimization appears to be less sensitive to\ncollusive outcomes compared to other state-of-the-art DRL algorithms.\n"
    },
    {
        "paper_id": 2406.02459,
        "authors": "Rachel Avram, Eric J. Koepcke, Alaa Moussawi, Melissa Nu\\~nez",
        "title": "Do Cure Violence Programs Reduce Gun Violence? Evidence from New York\n  City",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Cure Violence is a community violence intervention program that aims to\nreduce gun violence by mediating conflicts, \"treating\" high-risk individuals,\nand changing community norms. Using NYC shootings data from 2006-2023, we\nassess the efficacy of Cure Violence using both difference-in-differences and\nevent study models. We find that, on average, Cure Violence is associated with\na 14% reduction in shootings relative to the counterfactual. This association\npersists in the years after treatment, neither increasing nor decreasing much\nover time. We exploit variation in the geography and timing of Cure\nimplementation to argue against alternative explanations. Additionally, we find\nsuggestive evidence of spillover effects into nearby precincts, as well as\nincreasing returns to opening new Cure programs. Interpreted causally, our\nresults imply that around 1,300 shootings were avoided between 2012-2023 due to\nCure -- generating a net social surplus of $2.45 billion and achieving a\nbenefit-cost ratio of 6.5:1.\n"
    },
    {
        "paper_id": 2406.02492,
        "authors": "Shawn Berry",
        "title": "Distrust of social media influencers in America",
        "comments": "13 pages, 7 tables",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
        "abstract": "  The popularity of social media influencers (SMI) as a means for businesses\nand causes to engage with the public and develop followers is undeniable.\nHowever, the use of SMI have been scrutinized due to various scandals that\nreflect poorly on brands and firms. Consequently, the distrust of SMI can\ncreate the potential for damage to a brand if audiences are not receptive to\ncommunications and messaging. This study (n=351) shares insights and findings\nof the apparent distrust of SMI that were discovered during the data analysis\nphase of my dissertation (Berry, 2024a). The study examines levels of trust and\ndistrust toward SMI in the United States according to various demographic\ncharacteristics of respondents, specifically, age, gender, income level,\neducation level, and region of the United States. Chi square analysis of the\nvariables and a predictive model of trust of SMI are presented. Finally,\nrecommendations and suggestions for future research are discussed.\n"
    },
    {
        "paper_id": 2406.02588,
        "authors": "Juan De Anton, Juan J Senovilla, Jose M Gonzalez-Varona, Fernando\n  Acebes",
        "title": "Production planning in 3DPrinting factories",
        "comments": "12 pages",
        "journal-ref": "International Journal of Production Management and Engineering 8\n  (2), 75-86 2020",
        "doi": "10.4995/ijpme.2020.12944",
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  Production planning in 3D printing factories brings new challenges among\nwhich the scheduling of parts to be produced stands out. A main issue is to\nincrease the efficiency of the plant and 3D printers productivity. Planning,\nscheduling, and nesting in 3D printing are recurrent problems in the search for\nnew techniques to promote the development of this technology. In this work, we\naddress the problem for the suppliers that have to schedule their daily\nproduction. This problem is part of the LONJA3D model, a managed 3D printing\nmarket where the parts ordered by the customers are reorganized into new\nbatches so that suppliers can optimize their production capacity. In this\npaper, we propose a method derived from the design of combinatorial auctions to\nsolve the nesting problem in 3D printing. First, we propose the use of a\nheuristic to create potential manufacturing batches. Then, we compute the\nexpected return for each batch. The selected batch should generate the highest\nincome. Several experiments have been tested to validate the process. This\nmethod is a first approach to the planning problem in 3D printing and further\nresearch is proposed to improve the procedure\n"
    },
    {
        "paper_id": 2406.02589,
        "authors": "Fernando Acebes, M Pereda, David Poza, Javier Pajares, Jose M Galan",
        "title": "Stochastic Earned Value Analysis using Monte Carlo Simulation and\n  Statistical Learning Techniques",
        "comments": "20 pages",
        "journal-ref": "International Journal of Project Management 33 (7), 1597-1609 2015",
        "doi": "10.1016/j.ijproman.2015.06.012",
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  The aim of this paper is to describe a new an integrated methodology for\nproject control under uncertainty. This proposal is based on Earned Value\nMethodology and risk analysis and presents several refinements to previous\nmethodologies. More specifically, the approach uses extensive Monte Carlo\nsimulation to obtain information about the expected behavior of the project.\nThis dataset is exploited in several ways using different statistical learning\nmethodologies in a structured fashion. Initially, simulations are used to\ndetect if project deviations are a consequence of the expected variability\nusing Anomaly Detection algorithms. If the project follows this expected\nvariability, probabilities of success in cost and time and expected cost and\ntotal duration of the project can be estimated using classification and\nregression approaches.\n"
    },
    {
        "paper_id": 2406.02604,
        "authors": "Bivas Dinda",
        "title": "Gated recurrent neural network with TPE Bayesian optimization for\n  enhancing stock index prediction accuracy",
        "comments": "23 pages, 9 figures, 12 tables",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  The recent advancement of deep learning architectures, neural networks, and\nthe combination of abundant financial data and powerful computers are\ntransforming finance, leading us to develop an advanced method for predicting\nfuture stock prices. However, the accessibility of investment and trading at\neveryone's fingertips made the stock markets increasingly intricate and prone\nto volatility. The increased complexity and volatility of the stock market have\ndriven demand for more models, which would effectively capture high volatility\nand non-linear behavior of the different stock prices. This study explored\ngated recurrent neural network (GRNN) algorithms such as LSTM (long short-term\nmemory), GRU (gated recurrent unit), and hybrid models like GRU-LSTM, LSTM-GRU,\nwith Tree-structured Parzen Estimator (TPE) Bayesian optimization for\nhyperparameter optimization (TPE-GRNN). The aim is to improve the prediction\naccuracy of the next day's closing price of the NIFTY 50 index, a prominent\nIndian stock market index, using TPE-GRNN. A combination of eight influential\nfactors is carefully chosen from fundamental stock data, technical indicators,\ncrude oil price, and macroeconomic data to train the models for capturing the\nchanges in the price of the index with the factors of the broader economy.\nSingle-layer and multi-layer TPE-GRNN models have been developed. The models'\nperformance is evaluated using standard matrices like R2, MAPE, and RMSE. The\nanalysis of models' performance reveals the impact of feature selection and\nhyperparameter optimization (HPO) in enhancing stock index price prediction\naccuracy. The results show that the MAPE of our proposed TPE-LSTM method is the\nlowest (best) with respect to all the previous models for stock index price\nprediction.\n"
    },
    {
        "paper_id": 2406.02712,
        "authors": "Mario Ghossoub and Michael Boyuan Zhu",
        "title": "Efficiency in Pure-Exchange Economies with Risk-Averse Monetary\n  Utilities",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We study Pareto efficiency in a pure-exchange economy where agents'\npreferences are represented by risk-averse monetary utilities. These coincide\nwith law-invariant monetary utilities, and they can be shown to correspond to\nthe class of monotone, (quasi-)concave, Schur concave, and\ntranslation-invariant utility functionals. This covers a large class of utility\nfunctionals, including a variety of law-invariant robust utilities. We show\nthat Pareto optima exist and are comonotone, and we provide a crisp\ncharacterization thereof in the case of law-invariant positively homogeneous\nmonetary utilities. This characterization provides an easily implementable\nalgorithm that fully determines the shape of Pareto-optimal allocations. In the\nspecial case of law-invariant comonotone-additive monetary utility functionals\n(concave Yaari-Dual utilities), we provide a closed-form characterization of\nPareto optima. As an application, we examine risk-sharing markets where all\nagents evaluate risk through law-invariant coherent risk measures, a widely\npopular class of risk measures. In a numerical illustration, we characterize\nPareto-optimal risk-sharing for some special types of coherent risk measures.\n"
    },
    {
        "paper_id": 2406.02878,
        "authors": "Roongkiat Ratanabanchuen, Kanis Saengchote, Voraprapa Nakavachara,\n  Thitiphong Amonthumniyom, Pongsathon Parinyavuttichai, Polpatt Vinaibodee",
        "title": "The test of investors' behavioral bias through the price discovery\n  process in cryptoasset exchange\" Transactional-level evidence from Thailand",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
        "abstract": "  Analyzing investors' trading behavior in cryptoasset markets provides new\nevidence supporting the theory that retail investors likely exhibit behavioral\nbiases. We investigate the price discovery process between Thailand's most\nhighly liquid exchange and the global exchange. Under the no-arbitrage\nassumption, bid-offer quotes in the local exchange should quickly move to match\nthe new price levels in the global exchange, as the price process of\ncryptoassets in the local exchange does not contain new information that can\nlead the price dynamic in the global exchange. We analyze intraday bid-offer\nquotes and investors' portfolio positions and find that investors exhibit the\ndisposition effect by attempting to sell their profitable positions during\nmarket upturns. The rate of bid-offer movement is significantly slow to match a\nnew global price level only in situations when most investors in the market are\nin profit. These insights are crucial as they suggest that the risk-return\ncharacteristics of asset prices between the bull and bear market may differ,\nresulting from investors' behavioral biases.\n"
    },
    {
        "paper_id": 2406.02969,
        "authors": "Raeid Saqur, Anastasis Kratsios, Florian Krach, Yannick Limmer,\n  Jacob-Junqi Tian, John Willes, Blanka Horvath and Frank Rudzicz",
        "title": "Filtered not Mixed: Stochastic Filtering-Based Online Gating for Mixture\n  of Large Language Models",
        "comments": "29 pages, 5 Appendix sections",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We propose MoE-F -- a formalised mechanism for combining $N$ pre-trained\nexpert Large Language Models (LLMs) in online time-series prediction tasks by\nadaptively forecasting the best weighting of LLM predictions at every time\nstep. Our mechanism leverages the conditional information in each expert's\nrunning performance to forecast the best combination of LLMs for predicting the\ntime series in its next step. Diverging from static (learned) Mixture of\nExperts (MoE) methods, MoE-F employs time-adaptive stochastic filtering\ntechniques to combine experts. By framing the expert selection problem as a\nfinite state-space, continuous-time Hidden Markov model (HMM), we can leverage\nthe Wohman-Shiryaev filter. Our approach first constructs $N$ parallel filters\ncorresponding to each of the $N$ individual LLMs. Each filter proposes its best\ncombination of LLMs, given the information that they have access to.\nSubsequently, the $N$ filter outputs are aggregated to optimize a lower bound\nfor the loss of the aggregated LLMs, which can be optimized in closed-form,\nthus generating our ensemble predictor. Our contributions here are: (I) the\nMoE-F algorithm -- deployable as a plug-and-play filtering harness, (II)\ntheoretical optimality guarantees of the proposed filtering-based gating\nalgorithm, and (III) empirical evaluation and ablative results using state of\nthe art foundational and MoE LLMs on a real-world Financial Market Movement\ntask where MoE-F attains a remarkable 17% absolute and 48.5% relative F1\nmeasure improvement over the next best performing individual LLM expert.\n"
    },
    {
        "paper_id": 2406.03393,
        "authors": "Marcel Caesmann and Janis Goldzycher and Matteo Grigoletto and Lorenz\n  Gschwent",
        "title": "Censorship in Democracy",
        "comments": "25 pages, 8 figures, 5 tables",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The spread of propaganda, misinformation, and biased narratives from\nautocratic regimes, especially on social media, is a growing concern in many\ndemocracies. Can censorship be an effective tool to curb the spread of such\nslanted narratives? In this paper, we study the European Union's ban on Russian\nstate-led news outlets after the 2022 Russian invasion of Ukraine. We analyze\n775,616 tweets from 133,276 users on Twitter/X, employing a\ndifference-in-differences strategy. We show that the ban reduced pro-Russian\nslant among users who had previously directly interacted with banned outlets.\nThe impact is most pronounced among users with the highest pre-ban slant\nlevels. However, this effect was short-lived, with slant returning to its\npre-ban levels within two weeks post-enforcement. Additionally, we find a\ndetectable albeit less pronounced indirect effect on users who had not directly\ninteracted with the outlets before the ban. We provide evidence that other\nsuppliers of propaganda may have actively sought to mitigate the ban's\ninfluence by intensifying their activity, effectively counteracting the\npersistence and reach of the ban.\n"
    },
    {
        "paper_id": 2406.035,
        "authors": "David Curto, Fernando Acebes, Jose M Gonzalez-Varona, David Poza",
        "title": "Impact of aleatoric, stochastic and epistemic uncertainties on project\n  cost contingency reserves",
        "comments": "15 pages",
        "journal-ref": "International Journal of Production Economics 253, 108626 2022",
        "doi": "10.1016/j.ijpe.2022.108626",
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  In construction projects, contingency reserves have traditionally been\nestimated based on a percentage of the total project cost, which is arbitrary\nand, thus, unreliable in practical cases. Monte Carlo simulation provides a\nmore reliable estimation. However, works on this topic have focused exclusively\non the effects of aleatoric uncertainty, but ignored the impacts of other\nuncertainty types. In this paper, we present a method to quantitatively\ndetermine project cost contingency reserves based on Monte Carlo Simulation\nthat considers the impact of not only aleatoric uncertainty, but also of the\neffects of other uncertainty kinds (stochastic, epistemic) on the total project\ncost. The proposed method has been validated with a real-case construction\nproject in Spain. The obtained results demonstrate that the approach will be\nhelpful for construction Project Managers because the obtained cost contingency\nreserves are consistent with the actual uncertainty type that affects the risks\nidentified in their projects.\n"
    },
    {
        "paper_id": 2406.03614,
        "authors": "Alexander Bakumenko (1), Kate\\v{r}ina Hlav\\'a\\v{c}kov\\'a-Schindler\n  (2), Claudia Plant (2), and Nina C. Hubig (1) ((1) Clemson University, USA,\n  (2) University of Vienna, Austria)",
        "title": "Advancing Anomaly Detection: Non-Semantic Financial Data Encoding with\n  LLMs",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Detecting anomalies in general ledger data is of utmost importance to ensure\ntrustworthiness of financial records. Financial audits increasingly rely on\nmachine learning (ML) algorithms to identify irregular or potentially\nfraudulent journal entries, each characterized by a varying number of\ntransactions. In machine learning, heterogeneity in feature dimensions adds\nsignificant complexity to data analysis. In this paper, we introduce a novel\napproach to anomaly detection in financial data using Large Language Models\n(LLMs) embeddings. To encode non-semantic categorical data from real-world\nfinancial records, we tested 3 pre-trained general purpose sentence-transformer\nmodels. For the downstream classification task, we implemented and evaluated 5\noptimized ML models including Logistic Regression, Random Forest, Gradient\nBoosting Machines, Support Vector Machines, and Neural Networks. Our\nexperiments demonstrate that LLMs contribute valuable information to anomaly\ndetection as our models outperform the baselines, in selected settings even by\na large margin. The findings further underscore the effectiveness of LLMs in\nenhancing anomaly detection in financial journal entries, particularly by\ntackling feature sparsity. We discuss a promising perspective on using LLM\nembeddings for non-semantic data in the financial context and beyond.\n"
    },
    {
        "paper_id": 2406.03652,
        "authors": "Duy Khanh Lam",
        "title": "Ensembling Portfolio Strategies for Long-Term Investments: A\n  Distribution-Free Preference Framework for Decision-Making and Algorithms",
        "comments": "25 pages, 12 figures, 3 tables, working paper",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper investigates the problem of ensembling multiple strategies for\nsequential portfolios to outperform individual strategies in terms of long-term\nwealth. Due to the uncertainty of strategies' performances in the future\nmarket, which are often based on specific models and statistical assumptions,\ninvestors often mitigate risk and enhance robustness by combining multiple\nstrategies, akin to common approaches in collective learning prediction.\nHowever, the absence of a distribution-free and consistent preference framework\ncomplicates decisions of combination due to the ambiguous objective. To address\nthis gap, we introduce a novel framework for decision-making in combining\nstrategies, irrespective of market conditions, by establishing the investor's\npreference between decisions and then forming a clear objective. Through this\nframework, we propose a combinatorial strategy construction, free from\nstatistical assumptions, for any scale of component strategies, even infinite,\nsuch that it meets the determined criterion. Finally, we test the proposed\nstrategy along with its accelerated variant and some other multi-strategies.\nThe numerical experiments show results in favor of the proposed strategies,\nalbeit with small tradeoffs in their Sharpe ratios, in which their cumulative\nwealths eventually exceed those of the best component strategies while the\naccelerated strategy significantly improves performance.\n"
    },
    {
        "paper_id": 2406.03709,
        "authors": "Xiaomin Shi, Zuo Quan Xu",
        "title": "Mean-variance portfolio selection in jump-diffusion model under\n  no-shorting constraint: A viscosity solution approach",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper concerns a continuous time mean-variance (MV) portfolio selection\nproblem in a jump-diffusion financial model with no-shorting trading\nconstraint. The problem is reduced to two subproblems: solving a stochastic\nlinear-quadratic (LQ) control problem under control constraint, and finding a\nmaximal point of a real function. Based on a two-dimensional fully coupled\nordinary differential equation (ODE), we construct an explicit viscosity\nsolution to the Hamilton-Jacobi-Bellman equation of the constrained LQ problem.\nTogether with the Meyer-It\\^o formula and a verification procedure, we obtain\nthe optimal feedback controls of the constrained LQ problem and the original MV\nproblem, which corrects the flawed results in some existing literatures. In\naddition, closed-form efficient portfolio and efficient frontier are derived.\nIn the end, we present several examples where the two-dimensional ODE is\ndecoupled.\n"
    },
    {
        "paper_id": 2406.03742,
        "authors": "Mahdi Goldani",
        "title": "Evaluating Feature Selection Methods for Macro-Economic Forecasting,\n  Applied for Inflation Indicator of Iran",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  This study explores various feature selection techniques applied to\nmacro-economic forecasting, using Iran's World Bank Development Indicators.\nEmploying a comprehensive evaluation framework that includes Root Mean Square\nError (RMSE) and Mean Absolute Error (MAE) within a 10-fold cross-validation\nsetup, this research systematically analyzes and ranks different feature\nselection methodologies. The study distinctly highlights the efficiency of\nStepwise Selection, Tree-based methods, Hausdorff distance, Euclidean distance,\nand Mutual Information (MI) Score, noting their superior performance in\nreducing predictive errors. In contrast, methods like Recursive Feature\nElimination with Cross-Validation (RFECV) and Variance Thresholding showed\nrelatively lower effectiveness. The results underline the robustness of\nsimilarity-based approaches, particularly Hausdorff and Euclidean distances,\nwhich consistently performed well across various datasets, achieving an average\nrank of 9.125 out of a range of tested methods. This paper provides crucial\ninsights into the effectiveness of different feature selection methods,\noffering significant implications for enhancing the predictive accuracy of\nmodels used in economic analysis and planning. The findings advocate for the\nprioritization of stepwise and tree-based methods alongside similarity-based\ntechniques for researchers and practitioners working with complex economic\ndatasets.\n"
    },
    {
        "paper_id": 2406.03823,
        "authors": "Eiji Yamamura and Fumio Ohtake",
        "title": "Views about ChatGPT: Are human decision making and human learning\n  necessary?",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Using individual-level survey data from 2024, this study investigated how\nrespondent characteristics are associated with a subjective view of generative\nartificial intelligence (GAI). We asked 14 questions concerning respondents\nview about GAI, such as general view, faulty GAI, autonomous GEI, GAI replacing\nhumans, and importance of human learning. Regression analysis based on the\nordered logit model revealed that: (1) In some cases, the results of smartphone\nand computer usage times differed. Smartphone usage time was negatively\ncorrelated with the importance of human learning, whereas computer usage was\nnot negatively correlated. (2) Managers and ordinary businesspeople have\npositive views of GAI. However, managers do not show a positive view about GAI\nbeing responsible for human decision making. (3) Teachers generally have a\nnegative view about GAI replacing humans and no need of learning. They do not\nhave negative views about GAI producing documents unless GAI is faulty. (4)\nMedical industry workers positively view GAI if it operates following their\ndirection. However, they do not agree with the view that GAI replaces humans,\nand that human learning is unnecessary. (5) Females are less likely than men to\nhave a positive view of GAI. In summary, views about GAI vary widely by the\nindividual characteristics and condition of GAI, and by the question set.\n"
    },
    {
        "paper_id": 2406.04016,
        "authors": "Julio Backhoff-Veraguas, Gregoire Loeper, Jan Obloj",
        "title": "Geometric Martingale Benamou-Brenier transport and geometric Bass\n  martingales",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  We introduce and study geometric Bass martingales. Bass martingales were\nintroduced in \\cite{Ba83} and studied recently in a series of works, including\n\\cite{BaBeHuKa20,BaBeScTs23}, where they appear as solutions to the martingale\nversion of the Benamou-Brenier optimal transport formulation. These arithmetic,\nas well as our novel geometric, Bass martingales are continuous martingale on\n$[0,1]$ with prescribed initial and terminal distributions. An arithmetic Bass\nmartingale is the one closest to Brownian motion: its quadratic variation is as\nclose as possible to being linear in the averaged $L^2$ sense. Its geometric\ncounterpart we develop here, is the one closest to a geometric Brownian motion:\nthe quadratic variation of its logarithm is as close as possible to being\nlinear. By analogy between Bachelier and Black-Scholes models in mathematical\nfinance, the newly obtained geometric Bass martingales} have the potential to\nbe of more practical importance in a number of applications.\n  Our main contribution is to exhibit an explicit bijection between geometric\nBass martingales and their arithmetic counterparts. This allows us, in\nparticular, to translate fine properties of the latter into the new geometric\nsetting. We obtain an explicit representation for a geometric Bass martingale\nfor given initial and terminal marginals, we characterise it as a solution to\nan SDE, and we show that geometric Brownian motion is the only process which is\nboth an arithmetic and a geometric Bass martingale. Finally, we deduce a dual\nformulation for our geometric martingale Benamou-Brenier problem. Our main\nproof is probabilistic in nature and uses a suitable change of measure, but we\nalso provide PDE arguments relying on the dual formulation of the problem,\nwhich offer a rigorous proof under suitable regularity assumptions.\n"
    },
    {
        "paper_id": 2406.04074,
        "authors": "Shufan Zhang, Minda Ma, Nan Zhou, Jinyue Yan, Wei Feng, Ran Yan,\n  Kairui You, Jingjing Zhang, Jing Ke",
        "title": "Estimation of Global Building Stocks by 2070: Unlocking Renovation\n  Potential",
        "comments": "25 pages, 4 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Buildings produce one-third of carbon emissions globally, however, data\nabsence regarding global floorspace poses challenges in advancing building\ncarbon neutrality. We compile the measured building stocks for 14 major\neconomies and apply our global building stock model, GLOBUS, to evaluate future\ntrends in stock turnover. Based on a scenario not considering renovation, by\n2070 the building stock in developed economies will be ~1.4 times that of 2020\n(100 billion m2); in developing economies it is expected to be 2.2 times that\nof 2020 (313 billion m2). Based on a techno-economic potential scenario,\nhowever, stocks in developed economies will decline to approximately 0.8 times\nthe 2020 level, while stocks in developing economies will increase to nearly\ntwice the 2020 level due to their fewer buildings currently. Overall, GLOBUS\nprovides a way of calculating the global building stock, helping scientists,\nengineers, and policymakers conduct a range of investigation across various\nfuture scenarios.\n"
    },
    {
        "paper_id": 2406.0439,
        "authors": "Mahdi Goldani Soraya Asadi Tirvan",
        "title": "Sensitivity Assessing to Data Volume for forecasting: introducing\n  similarity methods as a suitable one in Feature selection methods",
        "comments": "arXiv admin note: text overlap with arXiv:2406.03742",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-sa/4.0/",
        "abstract": "  In predictive modeling, overfitting poses a significant risk, particularly\nwhen the feature count surpasses the number of observations, a common scenario\nin high-dimensional data sets. To mitigate this risk, feature selection is\nemployed to enhance model generalizability by reducing the dimensionality of\nthe data. This study focuses on evaluating the stability of feature selection\ntechniques with respect to varying data volumes, particularly employing time\nseries similarity methods. Utilizing a comprehensive dataset that includes the\nclosing, opening, high, and low prices of stocks from 100 high-income companies\nlisted in the Fortune Global 500, this research compares several feature\nselection methods including variance thresholds, edit distance, and Hausdorff\ndistance metrics. The aim is to identify methods that show minimal sensitivity\nto the quantity of data, ensuring robustness and reliability in predictions,\nwhich is crucial for financial forecasting. Results indicate that among the\ntested feature selection strategies, the variance method, edit distance, and\nHausdorff methods exhibit the least sensitivity to changes in data volume.\nThese methods therefore provide a dependable approach to reducing feature space\nwithout significantly compromising the predictive accuracy. This study not only\nhighlights the effectiveness of time series similarity methods in feature\nselection but also underlines their potential in applications involving\nfluctuating datasets, such as financial markets or dynamic economic conditions.\nThe findings advocate for their use as principal methods for robust feature\nselection in predictive analytics frameworks.\n"
    },
    {
        "paper_id": 2406.04439,
        "authors": "Mengfei Chen, Mohamed Kharbeche, Mohamed Haouari, Weihong \"Grace\" Guo",
        "title": "A simulation-optimization framework for food supply chain network design\n  to ensure food accessibility under uncertainty",
        "comments": "32 pages, 6 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  How to ensure accessibility to food and nutrition while food supply chains\nsuffer from demand and supply uncertainties caused by disruptive forces such as\nthe COVID-19 pandemic and natural disasters is an emerging and critical issue.\nUnstable access to food influences the level of nutrition that weakens the\nhealth and well-being of citizens. Therefore, a food accessibility evaluation\nindex is proposed in this work to quantify how well nutrition needs are met.\nThe proposed index is then embedded in a stochastic multi-objective\nmixed-integer optimization problem to determine the optimal supply chain design\nto maximize food accessibility and minimize cost. Considering uncertainty in\ndemand and supply, the multi-objective problem is solved in a two-phase\nsimulation-optimization framework in which Green Field Analysis is applied to\ndetermine the long-term, tactical decisions such as supply chain configuration,\nand then Monte Carlo simulation is performed iteratively to determine the\nshort-term supply chain operations by solving a stochastic programming problem.\nA case study is conducted on the beef supply chain in Qatar. Pareto efficient\nsolutions are validated in discrete event simulation to evaluate the\nperformance of the designed supply chain in various realistic scenarios and\nprovide recommendations for different decision-makers.\n"
    },
    {
        "paper_id": 2406.04969,
        "authors": "Johannes Bleher, Michael Bleher",
        "title": "An Algebraic Framework for the Modeling of Limit Order Books",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
        "abstract": "  Introducing an algebraic framework for modeling limit order books (LOBs) with\ntools from physics and stochastic processes, our proposed framework captures\nthe creation and annihilation of orders, order matching, and the time evolution\nof the LOB state. It also enables compositional settings, accommodating the\ninteraction of heterogeneous traders and different market structures. We employ\nDirac notation and generalized generating functions to describe the state space\nand dynamics of LOBs. The utility of this framework is shown through\nsimulations of simplified market scenarios, illustrating how variations in\ntrader behavior impact key market observables such as spread, return\nvolatility, and liquidity. The algebraic representation allows for exact\nsimulations using the Gillespie algorithm, providing a robust tool for\nexploring the implications of market design and policy changes on LOB dynamics.\nFuture research can expand this framework to incorporate more complex order\ntypes, adaptive event rates, and multi-asset trading environments, offering\ndeeper insights into market microstructure and trader behavior and estimation\nof key drivers for market microstructure dynamics.\n"
    },
    {
        "paper_id": 2406.05094,
        "authors": "Cristiano Salvagnin, Aldo Glielmo, Maria Elena De Giuli, Antonietta\n  Mira",
        "title": "Investigating the price determinants of the European Emission Trading\n  System: a non-parametric approach",
        "comments": "17 pages, 9 figures, 3 tables",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The European carbon market plays a pivotal role in the European Union's\nambitious target of achieving carbon neutrality by 2050. Understanding the\nintricacies of factors influencing European Union Emission Trading System (EU\nETS) market prices is paramount for effective policy making and strategy\nimplementation. We propose the use of the Information Imbalance, a recently\nintroduced non-parametric measure quantifying the degree to which a set of\nvariables is informative with respect to another one, to study the\nrelationships among macroeconomic, economic, uncertainty, and energy variables\nconcerning EU ETS prices. Our analysis shows that in Phase 3 commodity related\nvariables such as the ERIX index are the most informative to explain the\nbehaviour of the EU ETS market price. Transitioning to Phase 4, financial\nfluctuations take centre stage, with the uncertainty in the EUR/CHF exchange\nrate emerging as a crucial determinant. These results reflect the disruptive\nimpacts of the COVID-19 pandemic and the energy crisis in reshaping the\nimportance of the different variables. Beyond variable analysis, we also\npropose to leverage the Information Imbalance to address the problem of\nmixed-frequency forecasting, and we identify the weekly time scale as the most\ninformative for predicting the EU ETS price. Finally, we show how the\nInformation Imbalance can be effectively combined with Gaussian Process\nregression for efficient nowcasting and forecasting using very small sets of\nhighly informative predictors.\n"
    },
    {
        "paper_id": 2406.05172,
        "authors": "Mart\\'in Mart\\'in-Gonz\\'alez, Sara M. Gonz\\'alez-Betancor and Carmen\n  P\\'erez-Esparrells",
        "title": "Early School Leaving in Spain: a longitudinal analysis by gender",
        "comments": "35 pages, 3 tables, 8 figures",
        "journal-ref": null,
        "doi": "10.1332/17579597Y2024D000000023",
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Spain is one of the eight EU-27 countries that failed to reduce early school\nleaving (ESL) below 10% in 2020, and now faces the challenge of achieving a\nrate below 9% by 2030. The determinants of this phenomenon are usually studied\nusing cross-sectional data at the micro-level and without differentiation by\ngender. In this study, we analyse it for the first time for Spain using panel\ndata (between 2002-2020), taking into account the high regional inequalities at\nthe macroeconomic level and the masculinisation of the phenomenon. The results\nshow a positive relationship between ESL and socioeconomic variables such as\nthe adolescent fertility rate, immigration, unemployment or the weight of the\nindustrial and construction sectors in the regional economy, with significant\ngender differences that invite us to discuss educational policies.\nSurprisingly, youth unemployment has only small but significant impact on\nfemale ESL.\n"
    },
    {
        "paper_id": 2406.05283,
        "authors": "Guido Kuersteiner and Ingmar Prucha and Ying Zeng",
        "title": "Differential Test Performance and Peer Effects",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  We use variation of test scores measuring closely related skills to isolate\npeer effects. The intuition for our identification strategy is that the\ndifference in closely related scores eliminates factors common to the\nperformance in either test while retaining idiosyncratic test specific\nvariation. Common factors include unobserved teacher and group effects as well\nas test invariant ability and factors relevant for peer group formation. Peer\neffects work through idiosyncratic shocks which have the interpretation of\nindividual and test specific ability or effort. We use education production\nfunctions as well as restrictions on the information content of unobserved test\ntaking ability to formalize our approach. An important implication of our\nidentifying assumptions is that we do not need to rely on randomized group\nassignment. We show that our model restrictions are sufficient for the\nformulation of linear and quadratic moment conditions that identify the peer\neffects parameter of interest. We use Project STAR data to empirically measure\npeer effects in kindergarten through third grade classes. We find evidence of\nhighly significant peer effects with magnitudes that are at the lower end of\nthe range of estimates found in the literature.\n"
    },
    {
        "paper_id": 2406.05408,
        "authors": "Bnaya Dreyfuss and Raphael Raux",
        "title": "Human Learning about AI Performance",
        "comments": "34 pages (48 with appendix); 10 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
        "abstract": "  How do humans assess the performance of Artificial Intelligence (AI) across\ndifferent tasks? AI has been noted for its surprising ability to accomplish\nvery complex tasks while failing seemingly trivial ones. We show that humans\nengage in ``performance anthropomorphism'' when assessing AI capabilities: they\nproject onto AI the ability model that they use to assess humans. In this\nmodel, observing an agent fail an easy task is highly diagnostic of a low\nability, making them unlikely to succeed at any harder task. Conversely, a\nsuccess on a hard task makes successes on any easier task likely. We\nexperimentally show that humans project this model onto AI. Both prior beliefs\nand belief updating about AI performance on standardized math questions appear\nconsistent with the human ability model. This contrasts with actual AI\nperformance, which is uncorrelated with human difficulty in our context, and\nmakes such beliefs misspecified. Embedding our framework into an adoption\nmodel, we show that patterns of under- and over-adoption can be sustained in an\nequilibrium with anthropomorphic beliefs.\n"
    },
    {
        "paper_id": 2406.05662,
        "authors": "Ivan Guo, Shijia Jin, Kihun Nam",
        "title": "Macroscopic Market Making Games",
        "comments": "52 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  In continuation of the macroscopic market making \\`a la Avellaneda-Stoikov as\na control problem, this paper explores its stochastic game. Concerning the\nprice competition, each agent is compared with the best quote from the others.\nWe start with the linear case. While constructing the solution directly, the\nordering property and the dimension reduction in the equilibrium are revealed.\nFor the non-linear case, extending the decoupling approach, we introduce a\nmultidimensional characteristic equation to study the well-posedness of\nforward-backward stochastic differential equations. Properties of coefficients\nin the characteristic equation are obtained via non-smooth analysis. In\naddition to novel well-posedness results, the linear price impact arises and\nthe impact function can be further decomposed into two parts in some examples.\n"
    },
    {
        "paper_id": 2406.05854,
        "authors": "Francesca Mariani and Maria Cristina Recchioni and Tai-Ho Wang and\n  Roberto Giacalone",
        "title": "Can market volumes reveal traders' rationality and a new risk premium?",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  An empirical analysis, suggested by optimal Merton dynamics, reveals some\nunexpected features of asset volumes. These features are connected to traders'\nbelief and risk aversion. This paper proposes a trading strategy model in the\noptimal Merton framework that is representative of the collective behavior of\nheterogeneous rational traders. This model allows for the estimation of the\naverage risk aversion of traders acting on a specific risky asset, while\nrevealing the existence of a price of risk closely related to market price of\nrisk and volume rate. The empirical analysis, conducted on real data, confirms\nthe validity of the proposed model.\n"
    },
    {
        "paper_id": 2406.05917,
        "authors": "Renli Wu, Christopher Esposito, James Evans",
        "title": "China's Rising Leadership in Global Science",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Major shifts in the global system of science and technology are destabilizing\nthe global status order and demonstrating the capacity for emerging countries\nlike China and India to exert greater influence. In order to measure changes in\nthe global scientific system, we develop a framework to assess the hierarchical\nposition of countries in the international scientific collaboration network.\nUsing a machine-learning model to identify the leaders of 5,966,623 scientific\nteams that collaborated across international borders, we show that Chinese\nscientists substantially narrowed their leadership deficit with scientists from\nthe US, UK, and EU between 1990 and 2023 in absolute terms. Consequently, China\nand the US are on track to reach an equal number of team leaders engaged in\nbilateral collaborations between 2027 and 2028. Nevertheless, Chinese progress\nhas been considerably slower in per-collaborator terms: after adjusting for the\nnumber of non-leaders from each country, our models do not predict parity\nbetween the US and China until after 2087. These dynamics extend to 11 critical\ntechnology areas central to ongoing diplomacy between the two nations, such AI,\nSemiconductors, and Advanced Communications, and to China's scientific\nleadership with respect to the European Union and the United Kingdom. Thus,\nwhile China's elite scientists are achieving leadership in the international\nscientific community, China's scientific enterprise continues to face\ndevelopmental constraints. We conclude by reviewing several steps that Chinese\nscience is taking to overcome these constraints, by increasing its engagement\nin scientific training and research in signatory nations to the Belt and Road\nInitiative.\n"
    },
    {
        "paper_id": 2406.06235,
        "authors": "Alessandra Amendola, Vincenzo Candila, Antonio Naimoli, Giuseppe\n  Storti",
        "title": "Adaptive combinations of tail-risk forecasts",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  In order to meet the increasingly stringent global standards of banking\nmanagement and regulation, several methods have been proposed in the literature\nfor forecasting tail risk measures such as the Value-at-Risk (VaR) and Expected\nShortfall (ES). However, regardless of the approach used, there are several\nsources of uncertainty, including model specifications, data-related issues and\nthe estimation procedure, which can significantly affect the accuracy of VaR\nand ES measures. Aiming to mitigate the influence of these sources of\nuncertainty and improve the predictive performance of individual models, we\npropose novel forecast combination strategies based on the Model Confidence Set\n(MCS). In particular, consistent joint VaR and ES loss functions within the MCS\nframework are used to adaptively combine forecasts generated by a wide range of\nparametric, semi-parametric, and non-parametric models. Our results reveal that\nthe proposed combined predictors provide a suitable alternative for forecasting\nrisk measures, passing the usual backtests, entering the set of superior models\nof the MCS, and usually exhibiting lower standard deviations than other model\nspecifications.\n"
    },
    {
        "paper_id": 2406.06524,
        "authors": "Bernhard K Meister, Henry CW Price",
        "title": "Gas Fees on the Ethereum Blockchain: From Foundations to Derivatives\n  Valuations",
        "comments": "29 pages, 8 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The gas fee, paid for inclusion in the blockchain, is analyzed in two parts.\nFirst, we consider how effort in terms of resources required to process and\nstore a transaction turns into a gas limit, which, through a fee, comprised of\nthe base and priority fee in the current version of Ethereum, is converted into\nthe cost paid by the user. We hew closely to the Ethereum protocol to simplify\nthe analysis and to constrain the design choices when considering\nmultidimensional gas. Second, we assume that the gas price is given deus ex\nmachina by a fractional Ornstein-Uhlenbeck process and evaluate various\nderivatives. These contracts can, for example, mitigate gas cost volatility.\nThe ability to price and trade forwards besides the existing spot inclusion\ninto the blockchain could be beneficial.\n"
    },
    {
        "paper_id": 2406.06552,
        "authors": "Sabrina Khurshid, Mohammed Shahid Abdulla, and Gourab Ghatak",
        "title": "Optimizing Sharpe Ratio: Risk-Adjusted Decision-Making in Multi-Armed\n  Bandits",
        "comments": "Submitted to ACML Journal Track 2024",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
        "abstract": "  Sharpe Ratio (SR) is a critical parameter in characterizing financial time\nseries as it jointly considers the reward and the volatility of any\nstock/portfolio through its variance. Deriving online algorithms for optimizing\nthe SR is particularly challenging since even offline policies experience\nconstant regret with respect to the best expert Even-Dar et al (2006). Thus,\ninstead of optimizing the usual definition of SR, we optimize regularized\nsquare SR (RSSR). We consider two settings for the RSSR, Regret Minimization\n(RM) and Best Arm Identification (BAI). In this regard, we propose a novel\nmulti-armed bandit (MAB) algorithm for RM called UCB-RSSR for RSSR\nmaximization. We derive a path-dependent concentration bound for the estimate\nof the RSSR. Based on that, we derive the regret guarantees of UCB-RSSR and\nshow that it evolves as O(log n) for the two-armed bandit case played for a\nhorizon n. We also consider a fixed budget setting for well-known BAI\nalgorithms, i.e., sequential halving and successive rejects, and propose SHVV,\nSHSR, and SuRSR algorithms. We derive the upper bound for the error probability\nof all proposed BAI algorithms. We demonstrate that UCB-RSSR outperforms the\nonly other known SR optimizing bandit algorithm, U-UCB Cassel et al (2023). We\nalso establish its efficacy with respect to other benchmarks derived from the\nGRA-UCB and MVTS algorithms. We further demonstrate the performance of proposed\nBAI algorithms for multiple different setups. Our research highlights that our\nproposed algorithms will find extensive applications in risk-aware portfolio\nmanagement problems. Consequently, our research highlights that our proposed\nalgorithms will find extensive applications in risk-aware portfolio management\nproblems.\n"
    },
    {
        "paper_id": 2406.06594,
        "authors": "Chang Zong, Jian Shao, Weiming Lu, Yueting Zhuang",
        "title": "Stock Movement Prediction with Multimodal Stable Fusion via Gated\n  Cross-Attention Mechanism",
        "comments": "29 pages, 10 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  The accurate prediction of stock movements is crucial for investment\nstrategies. Stock prices are subject to the influence of various forms of\ninformation, including financial indicators, sentiment analysis, news\ndocuments, and relational structures. Predominant analytical approaches,\nhowever, tend to address only unimodal or bimodal sources, neglecting the\ncomplexity of multimodal data. Further complicating the landscape are the\nissues of data sparsity and semantic conflicts between these modalities, which\nare frequently overlooked by current models, leading to unstable performance\nand limiting practical applicability. To address these shortcomings, this study\nintroduces a novel architecture, named Multimodal Stable Fusion with Gated\nCross-Attention (MSGCA), designed to robustly integrate multimodal input for\nstock movement prediction. The MSGCA framework consists of three integral\ncomponents: (1) a trimodal encoding module, responsible for processing\nindicator sequences, dynamic documents, and a relational graph, and\nstandardizing their feature representations; (2) a cross-feature fusion module,\nwhere primary and consistent features guide the multimodal fusion of the three\nmodalities via a pair of gated cross-attention networks; and (3) a prediction\nmodule, which refines the fused features through temporal and dimensional\nreduction to execute precise movement forecasting. Empirical evaluations\ndemonstrate that the MSGCA framework exceeds current leading methods, achieving\nperformance gains of 8.1%, 6.1%, 21.7% and 31.6% on four multimodal datasets,\nrespectively, attributed to its enhanced multimodal fusion stability.\n"
    },
    {
        "paper_id": 2406.06706,
        "authors": "Qiqin Zhou",
        "title": "Application of Black-Litterman Bayesian in Statistical Arbitrage",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  \\begin{abstract} In this paper, we integrated the statistical arbitrage\nstrategy, pairs trading, into the Black-Litterman model and constructed\nefficient mean-variance portfolios. Typically, pairs trading underperforms\nunder volatile or distressed market condition because the selected asset pairs\nfail to revert to equilibrium within the investment horizon. By enhancing this\nstrategy with the Black-Litterman portfolio optimization, we achieved superior\nperformance compared to the S\\&P 500 market index under both normal and extreme\nmarket conditions. Furthermore, this research presents an innovative idea of\nincorporating traditional pairs trading strategies into the portfolio\noptimization framework in a scalable and systematic manner.\n"
    },
    {
        "paper_id": 2406.0686,
        "authors": "Chen Tong and Peter Reinhard Hansen and Ilya Archakov",
        "title": "Cluster GARCH",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  We introduce a novel multivariate GARCH model with flexible convolution-t\ndistributions that is applicable in high-dimensional systems. The model is\ncalled Cluster GARCH because it can accommodate cluster structures in the\nconditional correlation matrix and in the tail dependencies. The expressions\nfor the log-likelihood function and its derivatives are tractable, and the\nlatter facilitate a score-drive model for the dynamic correlation structure. We\napply the Cluster GARCH model to daily returns for 100 assets and find it\noutperforms existing models, both in-sample and out-of-sample. Moreover, the\nconvolution-t distribution provides a better empirical performance than the\nconventional multivariate t-distribution.\n"
    },
    {
        "paper_id": 2406.06952,
        "authors": "Keisuke Kokubun",
        "title": "Factors and moderators of ageism: An analysis using data from 55\n  countries in the World Values Survey Wave 6",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Today, as the aging of the world accelerates, it is an urgent task to clarify\nfactors that prevent ageism. In this study, using hierarchical multiple\nregression analysis of data from 40,869 people from 55 countries collected in\nthe World Values Survey Wave 6, we showed that after controlling for\ndemographic factors, stereotypes, a hungry spirit, and male chauvinism are\nrelated to ageism, and that altruism, trust within and outside the family, and\ntrust in competition moderate the relationship between the independent and\ndependent variables. Furthermore, data from Japan, which has the highest aging\nrate and aging speed in the world, showed that these moderation relationships\nare moderated.\n"
    },
    {
        "paper_id": 2406.07149,
        "authors": "Erlend Hordvei and Sebastian Emil Hummelen and Marianne Petersen and\n  Stian Backe and Pedro Crespo del Granado",
        "title": "From Policy to Practice: The Cost of Europe's Green Hydrogen Ambitions",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The European Commission's new definition of green hydrogen provides clear\nguidelines and legal certainty for producers and consumers. However, the strict\ncriteria for electrolysis production, requiring additionality, temporal\ncorrelation, and geographical correlation, could increase hydrogen costs,\naffecting its competitiveness as an energy carrier. This study examines the\nimpact of these European regulations using a stochastic capacity expansion\nmodel for the European energy market up to 2048. We analyze how these\nrequirements influence costs and investment decisions. Our results show that\ngreen hydrogen production requirements will raise system costs by 82 Euro\nbillion from 2024 to 2048, driven mainly by a rapid transition from fossil\nfuels to renewable energy. The additionality requirement, which mandates the\nuse of new renewable energy installations for electrolysis, emerges as the most\nexpensive to comply with but also the most effective in accelerating the\ntransition to renewable power, particularly before 2030.\n"
    },
    {
        "paper_id": 2406.072,
        "authors": "Daniele Maria Di Nosse and Federico Gatta",
        "title": "A Multi-step Approach for Minimizing Risk in Decentralized Exchanges",
        "comments": "The SIAG/FME Code Quest 2023 Winning Strategy",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Decentralized Exchanges are becoming even more predominant in today's\nfinance. Driven by the need to study this phenomenon from an academic\nperspective, the SIAG/FME Code Quest 2023 was announced. Specifically,\nparticipating teams were asked to implement, in Python, the basic functions of\nan Automated Market Maker and a liquidity provision strategy in an Automated\nMarket Maker to minimize the Conditional Value at Risk, a critical measure of\ninvestment risk. As the competition's winning team, we highlight our approach\nin this work. In particular, as the dependence of the final return on the\ninitial wealth distribution is highly non-linear, we cannot use standard ad-hoc\napproaches. Additionally, classical minimization techniques would require a\nsignificant computational load due to the cost of the target function. For\nthese reasons, we propose a three-step approach. In the first step, the target\nfunction is approximated by a Kernel Ridge Regression. Then, the approximating\nfunction is minimized. In the final step, the previously discovered minimum is\nutilized as the starting point for directly optimizing the desired target\nfunction. By using this procedure, we can both reduce the computational\ncomplexity and increase the accuracy of the solution. Finally, the overall\ncomputational load is further reduced thanks to an algorithmic trick concerning\nthe returns simulation and the usage of Cython.\n"
    },
    {
        "paper_id": 2406.0721,
        "authors": "Adrian Odenweller, Falko Ueckerdt",
        "title": "The green hydrogen ambition and implementation gap",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Green hydrogen is critical for decarbonising hard-to-electrify sectors, but\nfaces high costs and investment risks. Here we define and quantify the green\nhydrogen ambition and implementation gap, showing that meeting hydrogen\nexpectations will remain challenging despite surging announcements of projects\nand subsidies. Tracking 137 projects over three years, we identify a wide 2022\nimplementation gap with only 2% of global capacity announcements finished on\nschedule. In contrast, the 2030 ambition gap towards 1.5{\\deg}C scenarios is\ngradually closing as the announced project pipeline has nearly tripled to 441\nGW within three years. However, we estimate that, without carbon pricing,\nrealising all these projects would require global subsidies of \\$1.6 trillion\n(\\$1.2 - 2.6 trillion range), far exceeding announced subsidies. Given past and\nfuture implementation gaps, policymakers must prepare for prolonged green\nhydrogen scarcity. Policy support needs to secure hydrogen investments, but\nshould focus on applications where hydrogen is indispensable.\n"
    }
]