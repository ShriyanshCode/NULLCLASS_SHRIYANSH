[
    {
        "paper_id": 1701.01255,
        "authors": "V. Gontis and A. Kononovicius",
        "title": "Burst and inter-burst duration statistics as empirical test of\n  long-range memory in the financial markets",
        "comments": "10 pages, 7 figures",
        "journal-ref": null,
        "doi": "10.1016/j.physa.2017.04.163",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We address the problem of long-range memory in the financial markets. There\nare two conceptually different ways to reproduce power-law decay of\nauto-correlation function: using fractional Brownian motion as well as\nnon-linear stochastic differential equations. In this contribution we address\nthis problem by analyzing empirical return and trading activity time series\nfrom the Forex. From the empirical time series we obtain probability density\nfunctions of burst and inter-burst duration. Our analysis reveals that the\npower-law exponents of the obtained probability density functions are close to\n$3/2$, which is a characteristic feature of the one-dimensional stochastic\nprocesses. This is in a good agreement with earlier proposed model of absolute\nreturn based on the non-linear stochastic differential equations derived from\nthe agent-based herding model.\n"
    },
    {
        "paper_id": 1701.01327,
        "authors": "Antoine Jacquier and Hao Liu",
        "title": "Optimal liquidation in a Level-I limit order book for large tick stocks",
        "comments": "27 pages, 12 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We propose a framework to study the optimal liquidation strategy in a limit\norder book for large-tick stocks, with spread equal to one tick. All order book\nevents (market orders, limit orders and cancellations) occur according to\nindependent Poisson processes, with parameters depending on price move\ndirections. Our goal is to maximise the expected terminal wealth of an agent\nwho needs to liquidate her positions within a fixed time horizon. Assuming that\nthe agent trades (through sell limit order or/and sell market order) only when\nthe price moves, we model her liquidation procedure as a semi-Markov decision\nprocess, and compute the semi-Markov kernel using Laplace method in the\nlanguage of queueing theory. The optimal liquidation policy is then solved by\ndynamic programming, and illustrated numerically.\n"
    },
    {
        "paper_id": 1701.01427,
        "authors": "Victor Haghani and Richard Dewey",
        "title": "Rational Decision-Making Under Uncertainty: Observed Betting Patterns on\n  a Biased Coin",
        "comments": "8 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/publicdomain/zero/1.0/",
        "abstract": "  What would you do if you were invited to play a game where you were given\n\\$25 and allowed to place bets for 30 minutes on a coin that you were told was\nbiased to come up heads 60% of the time? This is exactly what we did, gathering\n61 young, quantitatively trained men and women to play this game. The results,\nin a nutshell, were that the majority of these 61 players did not place their\nbets very well, displaying a broad panoply of behaviorial and cognitive biases.\nAbout 30% of the subjects actually went bust, losing their full \\$25 stake. We\nalso discuss optimal betting strategies, valuation of the opportunity to play\nthe game and its similarities to investing in the stock market. The main\nimplication of our study is that people need to be better educated and trained\nin how to approach decision making under uncertainty. If these quantitatively\ntrained players, playing the simplest game we can think of involving\nuncertainty and favourable odds, did not play well, what hope is there for the\nrest of us when it comes to playing the biggest and most important game of all:\ninvesting our savings? In the words of Ed Thorp, who gave us helpful feedback\non our research: \"This is a great experiment for many reasons. It ought to\nbecome part of the basic education of anyone interested in finance or\ngambling.\"\n"
    },
    {
        "paper_id": 1701.01428,
        "authors": "Rickard Nyman, Paul Ormerod",
        "title": "Predicting Economic Recessions Using Machine Learning Algorithms",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Even at the beginning of 2008, the economic recession of 2008/09 was not\nbeing predicted. The failure to predict recessions is a persistent theme in\neconomic forecasting. The Survey of Professional Forecasters (SPF) provides\ndata on predictions made for the growth of total output, GDP, in the United\nStates for one, two, three and four quarters ahead since the end of the 1960s.\nOver a three quarters ahead horizon, the mean prediction made for GDP growth\nhas never been negative over this period. The correlation between the mean SPF\nthree quarters ahead forecast and the data is very low, and over the most\nrecent 25 years is not significantly different from zero.\n  Here, we show that the machine learning technique of random forests has the\npotential to give early warning of recessions. We use a small set of\nexplanatory variables from financial markets which would have been available to\na forecaster at the time of making the forecast. We train the algorithm over\nthe 1970Q2-1990Q1 period, and make predictions one, three and six quarters\nahead. We then re-train over 1970Q2-1990Q2 and make a further set of\npredictions, and so on. We did not attempt any optimisation of predictions,\nusing only the default input parameters to the algorithm we downloaded in the\npackage R.\n  We compare the predictions made from 1990 to the present with the actual\ndata. One quarter ahead, the algorithm is not able to improve on the SPF\npredictions. Three and six quarters ahead, the correlations between actual and\npredicted are low, but they are very significantly different from zero.\nAlthough the timing is slightly wrong, a serious downturn in the first half of\n2009 could have been predicted six quarters ahead in late 2007. The algorithm\nnever predicts a recession when one did not occur.\n  We obtain even stronger results with random forest machine learning\ntechniques in the case of the United Kingdom.\n"
    },
    {
        "paper_id": 1701.01429,
        "authors": "Javier de Frutos, Victor Gaton",
        "title": "Chebyshev Reduced Basis Function applied to Option Valuation",
        "comments": null,
        "journal-ref": "Computational Management Science. 2017",
        "doi": "10.1007/s10287-017-0287-4",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We present a numerical method for the frequent pricing of financial\nderivatives that depends on a large number of variables. The method is based on\nthe construction of a polynomial basis to interpolate the value function of the\nproblem by means of a hierarchical orthogonalization process that allows to\nreduce the number of degrees of freedom needed to have an accurate\nrepresentation of the value function. In the paper we consider, as an example,\na GARCH model that depends on eight parameters and show that a very large\nnumber of contracts for different maturities and asset and parameters values\ncan be valued in a small computational time with the proposed procedure. In\nparticular the method is applied to the problem of model calibration. The\nmethod is easily generalizable to be used with other models or problems.\n"
    },
    {
        "paper_id": 1701.01515,
        "authors": "Wenting Chen and Kai Du and Xinzi Qiu",
        "title": "Analytic properties of American option prices under a modified\n  Black-Scholes equation with spatial fractional derivatives",
        "comments": "12 pages",
        "journal-ref": null,
        "doi": "10.1016/j.physa.2017.08.068",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper investigates analytic properties of American option prices under\nthe finite moment log-stable (FMLS) model. Under this model the price of\nAmerican options is characterised by the free boundary problem of a fractional\npartial differential equation (FPDE) system. Using the technique of\napproximation we prove that the American put price under the FMLS model is\nconvex with respect the underlying price, and specify the impact of the tail\nindex on option prices.\n"
    },
    {
        "paper_id": 1701.01677,
        "authors": "Krishna Khatri",
        "title": "The Shapley Value of Digraph Games",
        "comments": "5 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper the Shapley value of digraph (directed graph) games are\nconsidered. Digraph games are transferable utility (TU) games with limited\ncooperation among players, where players are represented by nodes. A\nrestrictive relation between two adjacent players is established by a directed\nline segment. Directed path, connecting the initial player with the terminal\nplayer, form the coalition among players. A dominance relation is established\nbetween players and this relation determines whether or not a player wants to\ncooperate. To cooperate, we assume that a player joins a coalition where he/she\nis not dominated by any other players.The Shapley value is defined as the\naverage of marginal contribution vectors corresponding to all permutations that\ndo not violate the subordination of players. The Shapley value for cyclic\ndigraph games is calculated and analyzed. For a given family of characteristic\nfunctions, a quick way to calculate Shapley values is formulated.\n"
    },
    {
        "paper_id": 1701.01891,
        "authors": "Zbigniew Palmowski and Joanna Tumilewicz",
        "title": "Pricing insurance drawdown-type contracts with underlying L\\'evy assets",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper we consider some insurance policies related to drawdown and\ndrawup events of log-returns for an underlying asset modeled by a spectrally\nnegative geometric L\\'evy process. We consider four contracts, three of which\nwere introduced in Zhang et al. (2013) for a geometric Brownian motion. The\nfirst one is an insurance contract where the protection buyer pays a constant\npremium until the drawdown of fixed size of log-returns occurs. In return\nhe/she receives a certain insured amount at the drawdown epoch. The next\ninsurance contract provides protection from any specified drawdown with a\ndrawup contingency. This contract expires early if a certain fixed drawup event\noccurs prior to the fixed drawdown. The last two contracts are extensions of\nthe previous ones by an additional cancellation feature which allows the\ninvestor to terminate the contract earlier. We focus on two problems:\ncalculating the fair premium $p$ for the basic contracts and identifying the\noptimal stopping rule for the policies with the cancellation feature. To do\nthis we solve some two-sided exit problems related to drawdown and drawup of\nspectrally negative L\\'evy processes, which is of independent mathematical\ninterest. We also heavily rely on the theory of optimal stopping.\n"
    },
    {
        "paper_id": 1701.02015,
        "authors": "Leif Doering, Blanka Horvath, Josef Teichmann",
        "title": "Functional Analytic (Ir-)Regularity Properties of SABR-type Processes",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The SABR model is a benchmark stochastic volatility model in interest rate\nmarkets, which has received much attention in the past decade. Its popularity\narose from a tractable asymptotic expansion for implied volatility, derived by\nheat kernel methods. As markets moved to historically low rates, this expansion\nappeared to yield inconsistent prices. Since the model is deeply embedded in\nmarket practice, alternative pricing methods for SABR have been addressed in\nnumerous approaches in recent years. All standard option pricing methods make\ncertain regularity assumptions on the underlying model, but for SABR these are\nrarely satisfied. We examine here regularity properties of the model from this\nperspective with view to a number of (asymptotic and numerical) option pricing\nmethods. In particular, we highlight delicate degeneracies of the SABR model\n(and related processes) at the origin, which deem the currently used popular\nheat kernel methods and all related methods from (sub-) Riemannian geometry\nill-suited for SABR-type processes, when interest rates are near zero. We\ndescribe a more general semigroup framework, which permits to derive a suitable\ngeometry for SABR-type processes (in certain parameter regimes) via symmetric\nDirichlet forms. Furthermore, we derive regularity properties (Feller-\nproperties and strong continuity properties) necessary for the applicability of\npopular numerical schemes to SABR-semigroups, and identify suitable Banach- and\nHilbert spaces for these. Finally, we comment on the short time and large time\nasymptotic behaviour of SABR-type processes beyond the heat-kernel framework.\n"
    },
    {
        "paper_id": 1701.02028,
        "authors": "Christoph Wunderer",
        "title": "Asset correlation estimation for inhomogeneous exposure pools",
        "comments": "19 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  A possible data source for the estimation of asset correlations is default\ntime series. This study investigates the systematic error that is made if the\nexposure pool underlying a default time series is assumed to be homogeneous\nwhen in reality it is not. We find that the asset correlation will always be\nunderestimated if homogeneity with respect to the probability of default (PD)\nis wrongly assumed, and the error is the larger the more spread out the PD is\nwithin the exposure pool. If the exposure pool is inhomogeneous with respect to\nthe asset correlation itself then the error may be going in both directions,\nbut for most PD- and asset correlation ranges relevant in practice the asset\ncorrelation is systematically underestimated. Both effects stack up and the\nerror tends to become even larger if in addition a negative correlation between\nasset correlation and PD is assumed, which is plausible in many circumstances\nand consistent with the Basel RWA formula. It is argued that the generic\ninhomogeneity effect described is one of the reasons why asset correlations\nmeasured from default data tend to be lower than asset correlations derived\nfrom asset value data.\n"
    },
    {
        "paper_id": 1701.02167,
        "authors": "Dirk Becherer, Todor Bilarev, Peter Frentrup",
        "title": "Stability for gains from large investors' strategies in M1/J1 topologies",
        "comments": "Some important corrections",
        "journal-ref": "Bernoulli, Volume 25, Number 2 (2019), 1105-1140",
        "doi": "10.3150/17-BEJ1014",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We prove continuity of a controlled SDE solution in Skorokhod's $M_1$ and\n$J_1$ topologies and also uniformly, in probability, as a non-linear functional\nof the control strategy. The functional comes from a finance problem to model\nprice impact of a large investor in an illiquid market. We show that\n$M_1$-continuity is the key to ensure that proceeds and wealth processes from\n(self-financing) c\\`{a}dl\\`{a}g trading strategies are determined as the\ncontinuous extensions for those from continuous strategies. We demonstrate by\nexamples how continuity properties are useful to solve different stochastic\ncontrol problems on optimal liquidation and to identify asymptotically\nrealizable proceeds.\n"
    },
    {
        "paper_id": 1701.02182,
        "authors": "Jamal Bouoiyour (CATT), Refk Selmi (CATT)",
        "title": "Political elections and uncertainty -Are BRICS markets equally exposed\n  to Trump's agenda?",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  There certainly is little or no doubt that politicians, sometimes consciously\nand sometimes not, exert a significant impact on stock markets. The evolving\nvolatility over the Republican Donald Trump's surprise victory in the US\npresidential election is a perfect example when politicians, through announced\npolicies, send signals to financial markets. The present paper seeks to address\nwhether BRICS (Brazil, Russia, India, China and South Africa) stock markets\nequally vulnerable to Trump's plans. For this purpose, two methods were\nadopted. The first presents an event-study methodology based on regression\nestimation of abnormal returns. The second is based on vote intentions by\nintegrating data from social media (Twitter), search queries (Google Trends)\nand public opinion polls. Our results robustly reveal that although some\nmarkets emerged losers, others took the opposite route. China took the biggest\nhit with Brazil, while the damage was much more limited for India and South\nAfrica. These adverse responses can be explained by the Trump's\nneo-mercantilist attitude revolving around tearing up trade deals, instituting\ntariffs, and labeling China a \"currency manipulator\". However, Russia looks to\nbe benefiting due to Trump's sympathetic attitude towards Vladimir Putin and\nexpectations about the scaling down of sanctions imposed on Russia over its\nrole in the conflict in Ukraine.\n"
    },
    {
        "paper_id": 1701.02216,
        "authors": "Satoshi Nakano and Kazuhiko Nishimura",
        "title": "Structural propagation in a production network with restoring\n  substitution elasticities",
        "comments": null,
        "journal-ref": "Physica A 2018",
        "doi": "10.1016/j.physa.2018.08.110",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We model an economy-wide production network by cascading binary compounding\nfunctions, based on the sequential processing nature of the production\nactivities. As we observe a hierarchy among the intermediate processes spanning\nthe empirical input--output transactions, we utilize a stylized sequence of\nprocesses for modeling the intra-sectoral production activities. Under the\nproductivity growth that we measure jointly with the state-restoring elasticity\nparameters for each sectoral activity, the network of production completely\nreplicates the records of multi-sectoral general equilibrium prices and shares\nfor all factor inputs observed in two temporally distant states. Thereupon, we\nstudy propagation of a small exogenous productivity shock onto the structure of\nproduction networks by way of hierarchical clustering.\n"
    },
    {
        "paper_id": 1701.02245,
        "authors": "Takashi Shinzato",
        "title": "Property Safety Stock Policy for Correlated Commodities Based on\n  Probability Inequality",
        "comments": "16 pages, 3 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Deriving the optimal safety stock quantity with which to meet customer\nsatisfaction is one of the most important topics in stock management. However,\nit is difficult to control the stock management of correlated marketable\nmerchandise when using an inventory control method that was developed under the\nassumption that the demands are not correlated. For this, we propose a\ndeterministic approach that uses a probability inequality to derive a\nreasonable safety stock for the case in which we know the correlation between\nvarious commodities. Moreover, over a given lead time, the relation between the\nappropriate safety stock and the allowable stockout rate is analytically\nderived, and the potential of our proposed procedure is validated by numerical\nexperiments.\n"
    },
    {
        "paper_id": 1701.02646,
        "authors": "Yang Yu, Guangyi Liu, Wendong Zhu, Fei Wang, Bin Shu, Kai Zhang, Ram\n  Rajagopal, and Nicolas Astier",
        "title": "Economic information from Smart Meter: Nexus Between Demand Profile and\n  Electricity Retail Price Between Demand Profile and Electricity Retail Price",
        "comments": "8 pages, 5 figures, submitting to IEEE Trans on Smart Grid",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper, we demonstrate that a consumer's marginal system impact is\nonly determined by their demand profile rather than their demand level. Demand\nprofile clustering is identical to cluster consumers according to their\nmarginal impacts on system costs. A profile-based uniform-rate price is\neconomically efficient as real-time pricing. We develop a criteria system to\nevaluate the economic efficiency of an implemented retail price scheme in a\ndistribution system by comparing profile clustering and daily-average\nclustering. Our criteria system can examine the extent of a retail price\nscheme's inefficiency even without information about the distribution system's\ndaily cost structure. We analyze data from a real distribution system in China.\nIn this system, targeting each consumer's high-impact days is more efficient\nthan target high-impact consumers.\n"
    },
    {
        "paper_id": 1701.02647,
        "authors": "Wesley S. Boyce, Haim Mano, and John L. Kent",
        "title": "The Influence of Collaboration in Procurement Relationships",
        "comments": null,
        "journal-ref": null,
        "doi": "10.5121/ijmvsc.2016.7301",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Supply Chain Management often requires independent organizations to work\ntogether to achieve shared objectives. This collaboration is necessary when\ncoordinated actions benefit the group more than the uncoordinated efforts of\nindividual firms. Despite the commonly reported benefits that can be gained in\nclose relationships, recent research has indicated that collaboration attempts\nbetween purchasing firms and their suppliers have not been as widespread as\nanticipated. Using a survey of procurement professionals, this research\ninvestigates how the purchasing function utilizes collaboration in its supply\nchain relationships. Structural equation modeling is used to identify how\ninformation sharing, decision synchronization, incentive alignment,\ncollaborative communication, and trust impact collaboration, as well as how\ncollaboration impacts performance. Results from 86 survey responses indicate\nthat firms are still not fully utilizing collaborative relationships.\n"
    },
    {
        "paper_id": 1701.02649,
        "authors": "Gane Samb Lo, Cheikh Mohamed Haidara",
        "title": "Sur la d\\'ecomposabilit\\'e empirique des indicateurs de pauvret\\'e",
        "comments": "11 pages, in French",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We study the empirical decomposition of poverty indicators. This property is\nvery important and convenient in the context of the fight against poverty.\nIndeed, it makes it possible to put in place sectoral poverty reduction\npolicies on the basis of a relevant stratification laid down at the outset. The\nsimultaneous impacts of these policies, measured as reduction gains over the\npopulation as a whole, is then obtained by aggregating those obtained at each\nstratum by a relatively simple formula. It turns out that indicators as\nimportant as those of Sen and Shorrocks do not verify this property contrary to\nthe elements of the class of Foster - Greer and Thorbecke. Given the data from\nthe 1996 Senegalese Survey of Households (ESAM), we show that the lack of\ndecomposability of these indicators on the income variable for several types of\npopulation stratification is practically zero , of the order of one to two per\nthousand. This makes it possible to use the decomposition of the Sen and\nShorrocks indicators without any untoward consequences. An explanatory model of\nthese results is presented for future research.\n"
    },
    {
        "paper_id": 1701.02662,
        "authors": "M.L. Bertotti, G. Modanese",
        "title": "Mathematical models describing the effects of different tax evasion\n  behaviors",
        "comments": "To appear in J. of Economic Interaction and Coordination",
        "journal-ref": null,
        "doi": "10.1007/s11403-016-0185-9",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Microscopic models describing a whole of economic interactions in a closed\nsociety are considered. The presence of a tax system combined with a\nredistribution process is taken into account, as well as the occurrence of tax\nevasion. In particular, the existence is postulated, in relation to the level\nof evasion, of different individual taxpayer behaviors. The effects of the\nmentioned different behaviors on shape and features of the emerging income\ndistribution profile are investigated qualitatively and quantitatively.\nNumerical solutions show that the Gini inequality index of the total population\nincreases when the evasion level is higher, but does not depend significantly\non the evasion spread. For fixed spread, the relative difference between the\naverage incomes of the worst evaders and honest taxpayers increases\napproximately as a quadratic function of the evasion level.\n"
    },
    {
        "paper_id": 1701.02681,
        "authors": "T. A. McWalter, R. Rudd, J. Kienitz, E. Platen",
        "title": "Recursive Marginal Quantization of Higher-Order Schemes",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Quantization techniques have been applied in many challenging finance\napplications, including pricing claims with path dependence and early exercise\nfeatures, stochastic optimal control, filtering problems and efficient\ncalibration of large derivative books. Recursive Marginal Quantization of the\nEuler scheme has recently been proposed as an efficient numerical method for\nevaluating functionals of solutions of stochastic differential equations. This\nmethod involves recursively quantizing the conditional marginals of the\ndiscrete-time Euler approximation of the underlying process. By generalizing\nthis approach, we show that it is possible to perform recursive marginal\nquantization for two higher-order schemes: the Milstein scheme and a simplified\nweak order 2.0 scheme. As part of this generalization a simple matrix\nformulation is presented, allowing efficient implementation. We further extend\nthe applicability of recursive marginal quantization by showing how absorption\nand reflection at the zero boundary may be incorporated, when this is\nnecessary. To illustrate the improved accuracy of the higher order schemes,\nvarious computations are performed using geometric Brownian motion and its\ngeneralization, the constant elasticity of variance model. For both processes,\nwe show numerical evidence of improved weak order convergence and we compare\nthe marginal distributions implied by the three schemes to the known analytical\ndistributions. By pricing European, Bermudan and Barrier options, further\nevidence of improved accuracy of the higher order schemes is demonstrated.\n"
    },
    {
        "paper_id": 1701.02798,
        "authors": "Kazutoshi Yamazaki",
        "title": "Phase-type Approximation of the Gerber-Shiu Function",
        "comments": "16 pages. Forthcoming in the Journal of the Operations Research\n  Society of Japan, vol. 60, no. 3, 2017 (special issue of the 60th anniversary\n  of the Operations Research Society of Japan)",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The Gerber-Shiu function provides a way of measuring the risk of an insurance\ncompany. It is given by the expected value of a function that depends on the\nruin time, the deficit at ruin, and the surplus prior to ruin. Its computation\nrequires the evaluation of the overshoot/undershoot distributions of the\nsurplus process at ruin. In this paper, we use the recent developments of the\nfluctuation theory and approximate it in a closed form by fitting the\nunderlying process by phase-type Levy processes. A sequence of numerical\nresults are given.\n"
    },
    {
        "paper_id": 1701.02821,
        "authors": "Andrey Itkin",
        "title": "Modeling stochastic skew of FX options using SLV models with stochastic\n  spot/vol correlation and correlated jumps",
        "comments": "42 pages, 12 figures, 2 tables",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  It is known that the implied volatility skew of FX options demonstrates a\nstochastic behavior which is called stochastic skew. In this paper we create\nstochastic skew by assuming the spot/instantaneous variance correlation to be\nstochastic. Accordingly, we consider a class of SLV models with stochastic\ncorrelation where all drivers - the spot, instantaneous variance and their\ncorrelation are modeled by Levy processes. We assume all diffusion components\nto be fully correlated as well as all jump components. A new fully implicit\nsplitting finite-difference scheme is proposed for solving forward PIDE which\nis used when calibrating the model to market prices of the FX options with\ndifferent strikes and maturities. The scheme is unconditionally stable, of\nsecond order of approximation in time and space, and achieves a linear\ncomplexity in each spatial direction. The results of simulation obtained by\nusing this model demonstrate capacity of the presented approach in modeling\nstochastic skew.\n"
    },
    {
        "paper_id": 1701.02958,
        "authors": "Gon\\c{c}alo Sim\\~oes, Mark McDonald, Stacy Williams, Daniel Fenn,\n  Raphael Hauser",
        "title": "Robust Portfolio Optimisation with Specified Competitors",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We extend Relative Robust Portfolio Optimisation models to allow portfolios\nto optimise their distance to a set of benchmarks. Portfolio managers are also\ngiven the option of computing regret in a way which is more in line with market\npractices than other approaches suggested in the literature. In addition, they\nare given the choice of simply adding an extra constraint to their optimisation\nproblem instead of outright changing the objective function, as is commonly\nsuggested in the literature. We illustrate the benefits of this approach by\napplying it to equity portfolios in a variety of regions.\n"
    },
    {
        "paper_id": 1701.03098,
        "authors": "Shanshan Wang",
        "title": "Trading strategies for stock pairs regarding to the cross-impact cost",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We extend the framework of trading strategies of Gatheral [2010] from single\nstocks to a pair of stocks. Our trading strategy with the executions of two\nround-trip trades can be described by the trading rates of the paired stocks\nand the ratio of their trading periods. By minimizing the potential cost\narising from cross-impacts, i.e., the price change of one stock due to the\ntrades of another stock, we can find out an optimal strategy for executing a\nsequence of trades from different stocks. We further apply the model of the\nstrategy to a specific case, where we quantify the cross-impacts of traded\nvolumes and of time lag with empirical data for the computation of costs. We\nthus picture the influence of cross-impacts on the trading strategy.\n"
    },
    {
        "paper_id": 1701.03512,
        "authors": "Sai K. Popuri and Andrew M. Raim and Nagaraj K. Neerchal and Matthias\n  K. Gobbert",
        "title": "Parallelizing Computation of Expected Values in Recombinant Binomial\n  Trees",
        "comments": "19 pages and 5 figures (png/jpeg files)",
        "journal-ref": "J. Stat. Comp. & Sim. 88 (2018) 657-674",
        "doi": "10.1080/00949655.2017.1402898",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Recombinant binomial trees are binary trees where each non-leaf node has two\nchild nodes, but adjacent parents share a common child node. Such trees arise\nin finance when pricing an option. For example, valuation of a European option\ncan be carried out by evaluating the expected value of asset payoffs with\nrespect to random paths in the tree. In many variants of the option valuation\nproblem, a closed form solution cannot be obtained and computational methods\nare needed. The cost to exactly compute expected values over random paths grows\nexponentially in the depth of the tree, rendering a serial computation of one\nbranch at a time impractical. We propose a parallelization method that\ntransforms the calculation of the expected value into an \"embarrassingly\nparallel\" problem by mapping the branches of the binomial tree to the processes\nin a multiprocessor computing environment. We also propose a parallel Monte\nCarlo method which takes advantage of the mapping to achieve a reduced variance\nover the basic Monte Carlo estimator. Performance results from R and Julia\nimplementations of the parallelization method on a distributed computing\ncluster indicate that both the implementations are scalable, but Julia is\nsignificantly faster than a similarly written R code. A simulation study is\ncarried out to verify the convergence and the variance reduction behavior in\nthe proposed Monte Carlo method.\n"
    },
    {
        "paper_id": 1701.0377,
        "authors": "Dominik Hartmann, Cristian Jara-Figueroa, Miguel Guevara, Alex Simoes,\n  C\\'esar A. Hidalgo",
        "title": "The structural constraints of income inequality in Latin America",
        "comments": null,
        "journal-ref": "Integration & Trade Journal, No. 40, June 2016, p.70-85",
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Recent work has shown that a country's productive structure constrains its\nlevel of economic growth and income inequality. Here, we compare the productive\nstructure of countries in Latin America and the Caribbean (LAC) with that of\nChina and other High-Performing Asian Economies (HPAE) to expose the increasing\ngap in their productive capabilities. Moreover, we use the product space and\nthe Product Gini Index to reveal the structural constraints on income\ninequality. Our network maps reveal that HPAE have managed to diversify into\nproducts typically produced by countries with low levels of income inequality,\nwhile LAC economies have remained dependent on products related to high levels\nof income inequality. We also introduce the Xgini, a coefficient that captures\nthe constraints on income inequality imposed by the mix of products a country\nmakes. Finally, we argue that LAC countries need to emphasize a smart\ncombination of social and economic policies to overcome the structural\nconstraints for inclusive growth.\n"
    },
    {
        "paper_id": 1701.03897,
        "authors": "Michael R. Tehranchi",
        "title": "A Black--Scholes inequality: applications and generalisation",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The space of call price functions has a natural noncommutative semigroup\nstructure with an involution. A basic example is the Black--Scholes call price\nsurface, from which an interesting inequality for Black--Scholes implied\nvolatility is derived. The binary operation is compatible with the convex\norder, and therefore a one-parameter sub-semigroup gives rise to an\narbitrage-free market model. It is shown that each such one-parameter semigroup\ncorresponds to a unique log-concave probability density, providing a family of\ntractable call price surface parametrisations in the spirit of the\nGatheral--Jacquier SVI surface. An explicit example is given to illustrate the\nidea. The key observation is an isomorphism linking an initial call price curve\nto the lift zonoid of the terminal price of the underlying asset.\n"
    },
    {
        "paper_id": 1701.0396,
        "authors": "Tim Leung, Hongzhong Zhang",
        "title": "Optimal Trading with a Trailing Stop",
        "comments": "4 figures, 26 pages",
        "journal-ref": null,
        "doi": "10.1007/s00245-019-09559-0",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Trailing stop is a popular stop-loss trading strategy by which the investor\nwill sell the asset once its price experiences a pre-specified percentage\ndrawdown. In this paper, we study the problem of timing buy and then sell an\nasset subject to a trailing stop. Under a general linear diffusion framework,\nwe study an optimal double stopping problem with a random path-dependent\nmaturity. Specifically, we first derive the optimal liquidation strategy prior\nto a given trailing stop, and prove the optimality of using a sell limit order\nin conjunction with the trailing stop. Our analytic results for the liquidation\nproblem is then used to solve for the optimal strategy to acquire the asset and\nsimultaneously initiate the trailing stop. The method of solution also lends\nitself to an efficient numerical method for computing the the optimal\nacquisition and liquidation regions. For illustration, we implement an example\nand conduct a sensitivity analysis under the exponential Ornstein-Uhlenbeck\nmodel.\n"
    },
    {
        "paper_id": 1701.04134,
        "authors": "Seyed Amir Hejazi, Kenneth R. Jackson, Guojun Gan",
        "title": "A Spatial Interpolation Framework for Efficient Valuation of Large\n  Portfolios of Variable Annuities",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Variable Annuity (VA) products expose insurance companies to considerable\nrisk because of the guarantees they provide to buyers of these products.\nManaging and hedging these risks requires insurers to find the value of key\nrisk metrics for a large portfolio of VA products. In practice, many companies\nrely on nested Monte Carlo (MC) simulations to find key risk metrics. MC\nsimulations are computationally demanding, forcing insurance companies to\ninvest hundreds of thousands of dollars in computational infrastructure per\nyear. Moreover, existing academic methodologies are focused on fair valuation\nof a single VA contract, exploiting ideas in option theory and regression. In\nmost cases, the computational complexity of these methods surpasses the\ncomputational requirements of MC simulations. Therefore, academic methodologies\ncannot scale well to large portfolios of VA contracts. In this paper, we\npresent a framework for valuing such portfolios based on spatial interpolation.\nWe provide a comprehensive study of this framework and compare existing\ninterpolation schemes. Our numerical results show superior performance, in\nterms of both computational efficiency and accuracy, for these methods compared\nto nested MC simulations. We also present insights into the challenge of\nfinding an effective interpolation scheme in this framework, and suggest\nguidelines that help us build a fully automated scheme that is efficient and\naccurate.\n"
    },
    {
        "paper_id": 1701.04167,
        "authors": "Anulekha Dhara, Bikramjit Das, and Karthik Natarajan",
        "title": "Worst-Case Expected Shortfall with Univariate and Bivariate Marginals",
        "comments": "23 pages, 6 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Worst-case bounds on the expected shortfall risk given only limited\ninformation on the distribution of the random variables has been studied\nextensively in the literature. In this paper, we develop a new worst-case bound\non the expected shortfall when the univariate marginals are known exactly and\nadditional expert information is available in terms of bivariate marginals.\nSuch expert information allows for one to choose from among the many possible\nparametric families of bivariate copulas. By considering a neighborhood of\ndistance $\\rho$ around the bivariate marginals with the Kullback-Leibler\ndivergence measure, we model the trade-off between conservatism in the\nworst-case risk measure and confidence in the expert information. Our bound is\ndeveloped when the only information available on the bivariate marginals forms\na tree structure in which case it is efficiently computable using convex\noptimization. For consistent marginals, as $\\rho$ approaches $\\infty$, the\nbound reduces to the comonotonic upper bound and as $\\rho$ approaches $0$, the\nbound reduces to the worst-case bound with bivariates known exactly. We also\ndiscuss extensions to inconsistent marginals and instances where the expert\ninformation which might be captured using other parameters such as\ncorrelations.\n"
    },
    {
        "paper_id": 1701.0426,
        "authors": "Antoine Jacquier, Claude Martini, Aitor Muguruza",
        "title": "On VIX Futures in the rough Bergomi model",
        "comments": "21 pages, 37 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The rough Bergomi model introduced by Bayer, Friz and Gatheral has been\noutperforming conventional Markovian stochastic volatility models by\nreproducing implied volatility smiles in a very realistic manner, in particular\nfor short maturities. We investigate here the dynamics of the VIX and the\nforward variance curve generated by this model, and develop efficient pricing\nalgorithms for VIX futures and options. We further analyse the validity of the\nrough Bergomi model to jointly describe the VIX and the SPX, and present a\njoint calibration algorithm based on the hybrid scheme by Bennedsen, Lunde and\nPakkanen.\n"
    },
    {
        "paper_id": 1701.04431,
        "authors": "Brendan Pass",
        "title": "Interpolating between matching and hedonic pricing models",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We consider the theoretical properties of a model which encompasses\nbi-partite matching under transferable utility on the one hand, and hedonic\npricing on the other. This framework is intimately connected to tripartite\nmatching problems (known as multi-marginal optimal transport problems in the\nmathematical literature). We exploit this relationship in two ways; first, we\nshow that a known structural result from multi-marginal optimal transport can\nbe used to establish an upper bound on the dimension of the support of stable\nmatchings. Next, assuming the distribution of agents on one side of the market\nis continuous, we identify a condition on their preferences that ensures purity\nand uniqueness of the stable matching; this condition is a variant of a known\ncondition in the mathematical literature, which guarantees analogous properties\nin the multi-marginal optimal transport problem. We exhibit several examples of\nsurplus functions for which our condition is satisfied, as well as some for\nwhich it fails.\n"
    },
    {
        "paper_id": 1701.04491,
        "authors": "Tomohiro Uchiyama",
        "title": "A geometric approach to the transfer problem for a finite number of\n  traders",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We present a complete characterization of the classical transfer problem for\nan exchange economy with an arbitrary finite number of traders. Our method is\ngeometric, using an equilibrium manifold developed by Debreu, Mas-Colell, and\nBalasko. We show that for a regular equilibrium the transfer problem arises if\nand only if the index at the equilibrium is $-1$. This implies that the\ntransfer problem does not happen if the equilibrium is Walras tatonnement\nstable. Our result generalizes Balasko's analogous result for an exchange\neconomy with two traders.\n"
    },
    {
        "paper_id": 1701.04565,
        "authors": "Masahiko Egami, Rusudan Kevkhishvili",
        "title": "Time Reversal and Last Passage Time of Diffusions with Applications to\n  Credit Risk Management",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We study time reversal, last passage time, and $h$-transform of linear\ndiffusions. For general diffusions with killing, we obtain the probability\ndensity of the last passage time to an arbitrary level and analyze the\ndistribution of the time left until killing after the last passage time. With\nthese tools, we develop a new risk management framework for companies based on\nthe leverage process (the ratio of a company asset process over its debt) and\nits corresponding alarming level. We also suggest how a company can determine\nthe alarming level for the leverage process by constructing a relevant\noptimization problem.\n"
    },
    {
        "paper_id": 1701.0478,
        "authors": "Ruediger Frey, Lars Roesler, Dan Lu",
        "title": "Corporate Security Prices in Structural Credit Risk Models with\n  Incomplete Information: Extended Version",
        "comments": "5 figures. A shorter version of this paper will appear in\n  Mathematical Finance",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The paper studies derivative asset analysis in structural credit risk models\nwhere the asset value of the firm is not fully observable. It is shown that in\norder to compute the price dynamics of traded securities one needs to solve a\nstochastic filtering problem for the asset value. We transform this problem to\na filtering problem for a stopped diffusion process and we apply results from\nthe filtering literature to this problem. In this way we obtain an\nSPDE-characterization for the filter density. Moreover, we characterize the\ndefault intensity under incomplete information and we determine the price\ndynamics of traded securities. Armed with these results we study derivative\nasset analysis in our setup: we explain how the model can be applied to the\npricing of options on traded assets and we discuss dynamic hedging and model\ncalibration. The paper closes with a small simulation study.\n"
    },
    {
        "paper_id": 1701.05016,
        "authors": "Ziping Zhao and Daniel P. Palomar",
        "title": "Mean-Reverting Portfolio Design with Budget Constraint",
        "comments": "Paper submitted to IEEE Transactions on Signal Processing. Part of\n  this work will appear in the Proceedings of the 50th annual Asilomar\n  conference on signals, systems, and computers, Nov. 6-9, 2016, CA, USA",
        "journal-ref": null,
        "doi": "10.1109/TSP.2018.2799193",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper considers the mean-reverting portfolio design problem arising from\nstatistical arbitrage in the financial markets. We first propose a general\nproblem formulation aimed at finding a portfolio of underlying component assets\nby optimizing a mean-reversion criterion characterizing the mean-reversion\nstrength, taking into consideration the variance of the portfolio and an\ninvestment budget constraint. Then several specific problems are considered\nbased on the general formulation, and efficient algorithms are proposed.\nNumerical results on both synthetic and market data show that our proposed\nmean-reverting portfolio design methods can generate consistent profits and\noutperform the traditional design methods and the benchmark methods in the\nliterature.\n"
    },
    {
        "paper_id": 1701.05091,
        "authors": "Rasmus Pedersen, Olivier Wintenberger (LSTA, University of Copenhagen)",
        "title": "On the tail behavior of a class of multivariate conditionally\n  heteroskedastic processes",
        "comments": "Extremes, Springer Verlag (Germany), A Para\\^itre",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Conditions for geometric ergodicity of multivariate autoregressive\nconditional heteroskedasticity (ARCH) processes, with the so-called BEKK (Baba,\nEngle, Kraft, and Kroner) parametrization, are considered. We show for a class\nof BEKK-ARCH processes that the invariant distribution is regularly varying. In\norder to account for the possibility of different tail indices of the\nmarginals, we consider the notion of vector scaling regular variation, in the\nspirit of Perfekt (1997, Advances in Applied Probability, 29, pp. 138-164). The\ncharacterization of the tail behavior of the processes is used for deriving the\nasymptotic properties of the sample covariance matrices.\n"
    },
    {
        "paper_id": 1701.05114,
        "authors": "Ali Hosseiny",
        "title": "A geometrical imaging of the real gap between economies of China and the\n  United States",
        "comments": null,
        "journal-ref": null,
        "doi": "10.1016/j.physa.2017.02.079",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  GDP of China is about 11 trillion dollars and GDP of the United States is\nabout 18 trillion dollars. Suppose that we know for the coming years, economy\nof the US will experience a real growth rate equal to \\%3 and economy of China\nwill experience a real growth as of \\%6. Now, the question is how long does it\ntake for economy of China to catch the economy of the United States. The early\nimpression is that the desired time is the answer of the equation\n$11\\times1.06^X=18\\times1.03^X$. The correct answer however is quite different.\nGDP is not a simple number and the gap between two countries can not be\naddressed simply through their sizes. It is rather a geometrical object.\nCountries pass different paths in the space of production. The gaps between GDP\nof different countries depend on the path that each country passes through and\nlocal metric. To address distance between economies of China and of the US we\nneed to know their utility preferences and the path that China passes to reach\nthe US size. The true gap then can be found if we calculate local metric along\nthis path. It resembles impressions about measurements in the General Theory of\nRelativity. Path dependency of aggregate indexes is widely discussed in the\nIndex Number Theory. Our aim is to stick to the geometrical view presented in\nthe General Relativity to provide a visual understanding of the matter. We show\nthat different elements in the general relativity have their own counterparts\nin economics. We claim that national agencies who provide aggregate data\nresemble falling observers into a curved space time. It is while the World Bank\nor international organizations are outside observers. The vision provided here,\nleaves readers with a clear conclusion. If China keeps its growth rate, then\nthe economy of China should catch the economy of the United States sooner than\nwhat we expect.\n"
    },
    {
        "paper_id": 1701.05176,
        "authors": "Oisin Connolly",
        "title": "Dynamic Prize Linked Savings: Maximizing Savings and Managing Risk",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Prize linked savings accounts provide a return in the form of randomly chosen\naccounts receiving large cash prizes, in lieu of a guaranteed and uniform\ninterest rate. This model became legal for American national banks upon\nbipartisan passage of the American Savings Promotion Act in December 2014, and\nmany states have deregulated this option for state chartered banks and credit\nunions in recent years. Prize linked savings programs have unique appeal and\nproven societal benefits, but the product is still not available to the vast\nmajority of Americans. There is demonstrated interest in these products, but\nthe supply side may be the bottleneck, because the prevailing consensus is that\nprize linked savings primarily appeal to low income consumers. This paper\nexamines a less common, dynamic prize, model of prize linked savings and shows\nwhy it might result in a larger average account size. The paper proposes three\nmethods of managing risk under this model, and tests two of them using a Monte\nCarlo simulation. We conclude that both tested methods are effective at\nmitigating the most severe risks.\n"
    },
    {
        "paper_id": 1701.05447,
        "authors": "Amir T. Payandeh Najafabadi and Ali Panahi Bazaz",
        "title": "An Optimal Multi-layer Reinsurance Policy under Conditional Tail\n  Expectation",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  A usual reinsurance policy for insurance companies admits one or two layers\nof the payment deductions. Under optimal criterion of minimizing the\nconditional tail expectation (CTE) risk measure of the insurer's total risk,\nthis article generalized an optimal stop-loss reinsurance policy to an optimal\nmulti-layer reinsurance policy. To achieve such optimal multi-layer reinsurance\npolicy, this article starts from a given optimal stop-loss reinsurance policy\n$f(\\cdot).$ In the first step, it cuts down an interval $[0,\\infty)$ into two\nintervals $[0,M_1)$ and $[M_1,\\infty).$ By shifting the origin of Cartesian\ncoordinate system to $(M_{1},f(M_{1})),$ and showing that under the $CTE$\ncriteria $f(x)I_{[0, M_1)}(x)+(f(M_1)+f(x-M_1))I_{[M_1,\\infty)}(x)$ is, again,\nan optimal policy. This extension procedure can be repeated to obtain an\noptimal k-layer reinsurance policy. Finally, unknown parameters of the optimal\nmulti-layer reinsurance policy are estimated using some additional appropriate\ncriteria. Three simulation-based studies have been conducted to demonstrate:\n({\\bf 1}) The practical applications of our findings and ({\\bf 2}) How one may\nemploy other appropriate criteria to estimate unknown parameters of an optimal\nmulti-layer contract. The multi-layer reinsurance policy, similar to the\noriginal stop-loss reinsurance policy is optimal, in a same sense. Moreover it\nhas some other optimal criteria which the original policy does not have. Under\noptimal criterion of minimizing general translative and monotone risk measure\n$\\rho(\\cdot)$ of {\\it either} the insurer's total risk {\\it or} both the\ninsurer's and the reinsurer's total risks, this article (in its discussion)\nalso extends a given optimal reinsurance contract $f(\\cdot)$ to a multi-layer\nand continuous reinsurance policy.\n"
    },
    {
        "paper_id": 1701.0545,
        "authors": "Amir T. Payandeh-Najafabadi and Ali Panahi-Bazaz",
        "title": "An Optimal Combination of Proportional and Stop-Loss Reinsurance\n  Contracts From Insurer's and Reinsurer's Viewpoints",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  A reinsurance contract should address the conflicting interests of the\ninsurer and reinsurer. Most of existing optimal reinsurance contracts only\nconsiders the interests of one party. This article combines the proportional\nand stop-loss reinsurance contracts and introduces a new reinsurance contract\ncalled proportional-stop-loss reinsurance. Using the balanced loss function,\nunknown parameters of the proportional-stop-loss reinsurance have been\nestimated such that the expected surplus for both the insurer and reinsurer are\nmaximized. Several characteristics for the new reinsurance are provided.\n"
    },
    {
        "paper_id": 1701.05632,
        "authors": "Klaus Ackermann, Simon D Angus, Paul A Raschky",
        "title": "The Internet as Quantitative Social Science Platform: Insights from a\n  Trillion Observations",
        "comments": "40 pages, including 4 main figures, and appendix",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  With the large-scale penetration of the internet, for the first time,\nhumanity has become linked by a single, open, communications platform.\nHarnessing this fact, we report insights arising from a unified internet\nactivity and location dataset of an unparalleled scope and accuracy drawn from\nover a trillion (1.5$\\times 10^{12}$) observations of end-user internet\nconnections, with temporal resolution of just 15min over 2006-2012. We first\napply this dataset to the expansion of the internet itself over 1,647 urban\nagglomerations globally. We find that unique IP per capita counts reach\nsaturation at approximately one IP per three people, and take, on average, 16.1\nyears to achieve; eclipsing the estimated 100- and 60- year saturation times\nfor steam-power and electrification respectively. Next, we use intra-diurnal\ninternet activity features to up-scale traditional over-night sleep\nobservations, producing the first global estimate of over-night sleep duration\nin 645 cities over 7 years. We find statistically significant variation between\ncontinental, national and regional sleep durations including some evidence of\nglobal sleep duration convergence. Finally, we estimate the relationship\nbetween internet concentration and economic outcomes in 411 OECD regions and\nfind that the internet's expansion is associated with negative or positive\nproductivity gains, depending strongly on sectoral considerations. To our\nknowledge, our study is the first of its kind to use online/offline activity of\nthe entire internet to infer social science insights, demonstrating the\nunparalleled potential of the internet as a social data-science platform.\n"
    },
    {
        "paper_id": 1701.05695,
        "authors": "Jiro Akahori, Flavia Barsotti and Yuri Imamura",
        "title": "The Value of Timing Risk",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The aim of this paper is to provide a mathematical contribution on the\nsemi-static hedge of timing risk associated to positions in American-style\noptions under a multi-dimensional market model. Barrier options are considered\nin the paper and semi-static hedges are studied and discussed for a fairly\nlarge class of underlying price dynamics. Timing risk is identified with the\nuncertainty associated to the time at which the payoff payment of the barrier\noption is due. Starting from the work by Carr and Picron (1999), where the\nauthors show that the timing risk can be hedged via static positions in plain\nvanilla options, the present paper extends the static hedge formula proposed in\nCarr and Picron (1999) by giving sufficient conditions to decompose a\ngeneralized timing risk into an integral of knock-in options in a\nmulti-dimensional market model. A dedicated study of the semi-static hedge is\nthen conducted by defining the corresponding strategy based on positions in\nbarrier options. The proposed methodology allows to construct not only first\norder hedges but also higher order semi-static hedges, that can be interpreted\nas asymptotic expansions of the hedging error. The convergence of these higher\norder semi-static hedges to an exact hedge is shown. An illustration of the\nmain theoretical results is provided for i) a symmetric case, ii) a one\ndimensional case, where the first order and second order hedging errors are\nderived in analytic closed form. The materiality of the hedging benefit gain of\ngoing from order one to order two by re-iterating the timing risk hedging\nstrategy is discussed through numerical evidences by showing that order two can\nbring to more than 90% reduction of the hedging 'cost' w.r.t. order one\n(depending on the specific barrier option characteristics).\n"
    },
    {
        "paper_id": 1701.05864,
        "authors": "Nicol\\'as Hern\\'andez Santib\\'a\\~nez and Dylan Possama\\\"i and Chao\n  Zhou",
        "title": "Bank monitoring incentives under moral hazard and adverse selection",
        "comments": "48 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper, we extend the optimal securitisation model of Pag\\`es [50] and\nPossama\\\"i and Pag\\`es [51] between an investor and a bank to a setting\nallowing both moral hazard and adverse selection. Following the recent approach\nto these problems of Cvitani\\'c, Wan and Yang [14], we characterise explicitly\nand rigorously the so-called credible set of the continuation and temptation\nvalues of the bank, and obtain the value function of the investor as well as\nthe optimal contracts through a recursive system of first-order variational\ninequalities with gradient constraints. We provide a detailed discussion of the\nproperties of the optimal menu of contracts.\n"
    },
    {
        "paper_id": 1701.05967,
        "authors": "Niushan Gao, Denny H. Leung, Cosimo Munari, Foivos Xanthos",
        "title": "Fatou Property, representations, and extensions of law-invariant risk\n  measures on general Orlicz spaces",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We provide a variety of results for (quasi)convex, law-invariant functionals\ndefined on a general Orlicz space, which extend well-known results in the\nsetting of bounded random variables. First, we show that Delbaen's\nrepresentation of convex functionals with the Fatou property, which fails in a\ngeneral Orlicz space, can be always achieved under the assumption of\nlaw-invariance. Second, we identify the range of Orlicz spaces where the\ncharacterization of the Fatou property in terms of norm lower semicontinuity by\nJouini, Schachermayer and Touzi continues to hold. Third, we extend Kusuoka's\nrepresentation to a general Orlicz space. Finally, we prove a version of the\nextension result by Filipovi\\'{c} and Svindland by replacing norm lower\nsemicontinuity with the (generally non-equivalent) Fatou property. Our results\nhave natural applications to the theory of risk measures.\n"
    },
    {
        "paper_id": 1701.06001,
        "authors": "Andrei Cozma, Matthieu Mariapragassam, Christoph Reisinger",
        "title": "Calibration of a Hybrid Local-Stochastic Volatility Stochastic Rates\n  Model with a Control Variate Particle Method",
        "comments": "45 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We propose a novel and generic calibration technique for four-factor\nforeign-exchange hybrid local-stochastic volatility models with stochastic\nshort rates. We build upon the particle method introduced by Guyon and\nLabord\\`ere [Nonlinear Option Pricing, Chapter 11, Chapman and Hall, 2013] and\ncombine it with new variance reduction techniques in order to accelerate\nconvergence. We use control variates derived from a calibrated pure local\nvolatility model, a two-factor Heston-type LSV model (both with deterministic\nrates), and the stochastic (CIR) short rates. The method can be applied to a\nlarge class of hybrid LSV models and is not restricted to our particular choice\nof the diffusion. The calibration procedure is performed on real-world market\ndata for the EUR-USD currency pair and has a comparable run-time to the PDE\ncalibration of a two-factor LSV model alone.\n"
    },
    {
        "paper_id": 1701.06038,
        "authors": "Dmitry B. Rokhlin and Anatoly Usov",
        "title": "Asymptotic efficiency of the proportional compensation scheme for a\n  large number of producers",
        "comments": "17 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We consider a manager, who allocates some fixed total payment amount between\n$N$ rational agents in order to maximize the aggregate production. The profit\nof $i$-th agent is the difference between the compensation (reward) obtained\nfrom the manager and the production cost. We compare (i) the \\emph{normative}\ncompensation scheme, where the manager enforces the agents to follow an optimal\ncooperative strategy; (ii) the \\emph{linear piece rates} compensation scheme,\nwhere the manager announces an optimal reward per unit good; (iii) the\n\\emph{proportional} compensation scheme, where agent's reward is proportional\nto his contribution to the total output. Denoting the correspondent total\nproduction levels by $s^*$, $\\hat s$ and $\\overline s$ respectively, where the\nlast one is related to the unique Nash equilibrium, we examine the limits of\nthe prices of anarchy $\\mathscr A_N=s^*/\\overline s$, $\\mathscr A_N'=\\hat\ns/\\overline s$ as $N\\to\\infty$. These limits are calculated for the cases of\nidentical convex costs with power asymptotics at the origin, and for power\ncosts, corresponding to the Coob-Douglas and generalized CES production\nfunctions with decreasing returns to scale. Our results show that\nasymptotically no performance is lost in terms of $\\mathscr A'_N$, and in terms\nof $\\mathscr A_N$ the loss does not exceed $31\\%$.\n"
    },
    {
        "paper_id": 1701.06081,
        "authors": "Marian Gidea",
        "title": "Topology data analysis of critical transitions in financial networks",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We develop a topology data analysis-based method to detect early signs for\ncritical transitions in financial data. From the time-series of multiple stock\nprices, we build time-dependent correlation networks, which exhibit topological\nstructures. We compute the persistent homology associated to these structures\nin order to track the changes in topology when approaching a critical\ntransition. As a case study, we investigate a portfolio of stocks during a\nperiod prior to the US financial crisis of 2007-2008, and show the presence of\nearly signs of the critical transition.\n"
    },
    {
        "paper_id": 1701.06234,
        "authors": "Andrzej Ruszczynski and Jianing Yao",
        "title": "A Dual Method For Backward Stochastic Differential Equations with\n  Application to Risk Valuation",
        "comments": null,
        "journal-ref": "ESAIM: COCV, 2020",
        "doi": "10.1051/cocv/2020018",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We propose a numerical recipe for risk evaluation defined by a backward\nstochastic differential equation. Using dual representation of the risk\nmeasure, we convert the risk valuation to a stochastic control problem where\nthe control is a certain Radon-Nikodym derivative process. By exploring the\nmaximum principle, we show that a piecewise-constant dual control provides a\ngood approximation on a short interval. A dynamic programming algorithm extends\nthe approximation to a finite time horizon. Finally, we illustrate the\napplication of the procedure to financial risk management in conjunction with\nnested simulation and on an multidimensional portfolio valuation problem.\n"
    },
    {
        "paper_id": 1701.06299,
        "authors": "Valentina V. Tarasova, Vasily E. Tarasov",
        "title": "Economic Growth Model with Constant Pace and Dynamic Memory",
        "comments": "7 pages, PDF",
        "journal-ref": "Problems of Modern Science and Education. 2017. No.2(84). P.40-45",
        "doi": "10.20861/2304-2338-2017-84-001",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The article discusses a generalization of model of economic growth with\nconstant pace, which takes into account the effects of dynamic memory. Memory\nmeans that endogenous or exogenous variable at a given time depends not only on\ntheir value at that time, but also on their values at previous times. To\ndescribe the dynamic memory we use derivatives of non-integer orders. We obtain\nthe solutions of fractional differential equations with derivatives of\nnon-integral order, which describe the dynamics of the output caused by the\nchanges of the net investments and effects of power-law fading memory.\n"
    },
    {
        "paper_id": 1701.0641,
        "authors": "Brendan Markey-Towler",
        "title": "Economics cannot isolate itself from political theory: a mathematical\n  demonstration",
        "comments": "19 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The purpose of this paper is to provide a confession of sorts from an\neconomist to political science and philosophy. A confession of the weaknesses\nof the political position of the economist. It is intended as a guide for\npolitical scientists and philosophers to the ostensible policy criteria of\neconomics, and an illustration of an argument that demonstrates\nlogico-mathematically, therefore incontrovertibly, that any policy statement by\nan economist contains, or is, a political statement. It develops an inescapable\ncompulsion that the absolute primacy and priority of political theory and\nphilosophy in the development of policy criteria must be recognised. Economic\npolicy cannot be divorced from politics as a matter of mathematical fact, and\nrather, as Amartya Sen has done, it ought embrace political theory and\nphilosophy.\n"
    },
    {
        "paper_id": 1701.06624,
        "authors": "Amita Gajewar, Gagan Bansal",
        "title": "Revenue Forecasting for Enterprise Products",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  For any business, planning is a continuous process, and typically\nbusiness-owners focus on making both long-term planning aligned with a\nparticular strategy as well as short-term planning that accommodates the\ndynamic market situations. An ability to perform an accurate financial forecast\nis crucial for effective planning. In this paper, we focus on providing an\nintelligent and efficient solution that will help in forecasting revenue using\nmachine learning algorithms. We experiment with three different revenue\nforecasting models, and here we provide detailed insights into the methodology\nand their relative performance measured on real finance data. As a real-world\napplication of our models, we partner with Microsoft's Finance organization\n(department that reports Microsoft's finances) to provide them a guidance on\nthe projected revenue for upcoming quarters.\n"
    },
    {
        "paper_id": 1701.06625,
        "authors": "Victor Olkhov",
        "title": "Econophysics Macroeconomic Model",
        "comments": "22 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper presents macroeconomic model that is based on parallels between\nmacroeconomic multi-agent systems and multi-particle systems. We use risk\nratings of economic agents as their coordinates on economic space. Aggregates\nof economic or financial variables like Investment, Assets, Demand, Credits and\netc. of economic agents near point x define corresponding macroeconomic\nvariables as functions of time t and coordinates x on economic space. Parallels\nbetween multi-agent and multi-particle systems on economic space allow describe\ntransition from economic kinetic-like to economic hydrodynamic-like\napproximation and derive macroeconomic hydrodynamic-like equations on economic\nspace. Economic or financial transactions between economic agents determine\nevolution of macroeconomic variables This paper describes local macroeconomic\napproximation that takes into account transactions between economic agents with\ncoordinates near same point x on economic space only and describes interaction\nbetween macroeconomic variables by linear differential operators. For simple\nmodel of interaction between macroeconomic variables as Demand on Investment\nand Interest Rate we derive hydrodynamic-like equations in a closed form. For\nperturbations of these macroeconomic variables we derive macroeconomic wave\nequations. Macroeconomic waves on economic space can propagate with exponential\ngrowth of amplitude and cause irregular time fluctuations of macroeconomic\nvariables or induce economic crises.\n"
    },
    {
        "paper_id": 1701.06779,
        "authors": "Mathias Beiglboeck and Pierre Henry-Labordere and Nizar Touzi",
        "title": "Monotone Martingale Transport Plans and Skorohod Embedding",
        "comments": "SPA, to appear",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We show that the left-monotone martingale coupling is optimal for any given\nperformance function satisfying the martingale version of the Spence-Mirrlees\ncondition, without assuming additional structural conditions on the marginals.\nWe also give a new interpretation of the left monotone coupling in terms of\nSkorokhod embedding which allows us to give a short proof of uniqueness.\n"
    },
    {
        "paper_id": 1701.06975,
        "authors": "Antoaneta Serguieva",
        "title": "Multichannel Contagion vs Stabilisation in Multiple Interconnected\n  Financial Markets",
        "comments": "35 pages, 4 figures, 5 tables, 35 equations",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The theory of multilayer networks is in its early stages, and its development\nprovides vital methods for understanding complex systems. Multilayer networks,\nin their multiplex form, have been introduced within the last three years to\nanalysing the structure of financial systems, and existing studies have\nmodelled and evaluated interdependencies of different type among financial\ninstitutions. These studies, however, have considered the structure as a\nnon-interconnected multiplex - an ensemble of single layer networks comprising\nthe same nodes - rather than as an interconnected multiplex network. No\nmechanism of multichannel contagion has been modelled and empirically\nevaluated, and no multichannel stabilisation strategies for pre-emptive\ncontagion containment have been designed. This paper formulates an\ninterconnected multiplex structure, and a contagion mechanism among financial\ninstitutions due to bilateral exposures arising from institutions activity\nwithin different interconnected markets that compose the overall financial\nmarket. We introduce structural measures of absolute systemic risk and\nresilience, and relative systemic-risk indexes. Based on the contagion\nmechanism and systemic-risk quantification, this study designs minimum-cost\nstabilisation strategies that act simultaneously on different markets and their\ninterconnections, in order to effectively contain potential contagion\nprogressing through the overall structure. The stabilisation strategies subtly\naffect the emergence process of structure to adaptively build in structural\nresilience and achieve pre-emptive stabilisation at a minimum cost for each\ninstitution and at no cost for the system as a whole. We empirically evaluate\nthe new approach using large granular databases, maintained by the Prudential\nRegulatory Authority of the Bank of England. The capabilities of multichannel\nstabilisation are confirmed empirically.\n"
    },
    {
        "paper_id": 1701.07152,
        "authors": "Rub\\'en Loaiza-Maya, Michael S. Smith and Worapree Maneesoonthorn",
        "title": "Time Series Copulas for Heteroskedastic Data",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We propose parametric copulas that capture serial dependence in stationary\nheteroskedastic time series. We develop our copula for first order Markov\nseries, and extend it to higher orders and multivariate series. We derive the\ncopula of a volatility proxy, based on which we propose new measures of\nvolatility dependence, including co-movement and spillover in multivariate\nseries. In general, these depend upon the marginal distributions of the series.\nUsing exchange rate returns, we show that the resulting copula models can\ncapture their marginal distributions more accurately than univariate and\nmultivariate GARCH models, and produce more accurate value at risk forecasts.\n"
    },
    {
        "paper_id": 1701.07175,
        "authors": "Swarn Chatterjee",
        "title": "Day of the Week Effect in biotechnology stocks: An Application of the\n  GARCH processes",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This study examines the presence of the day-of-the-week effect on daily\nreturns of biotechnology stocks over a 16-year period from January 2002 to\nDecember 2015. Using daily returns from the NASDAQ Biotechnology Index (NBI),\nwe find that the stock returns were the lowest on Mondays, and compared to the\nMondays the stock returns were significantly higher on Wednesdays, Thursdays,\nand Fridays. Moreover, the results from using the asymmetric GARCH processes\nreveal that momentum and small-firm effect were positively associated with the\nmarket risk-adjusted returns of the biotechnology stocks during this period.\nThe findings of our study suggest that active portfolio managers need to\nconsider the day of the week, momentum, and small-firm effect when making\ntrading decisions for biotechnology stocks.\n"
    },
    {
        "paper_id": 1701.07218,
        "authors": "Joanna D\\k{e}bicka, Beata Zmy\\'slona",
        "title": "Premium valuation for a multiple state model containing manifold\n  premium-paid states",
        "comments": "16 pages, 2 figures, 3 tables. arXiv admin note: text overlap with\n  arXiv:1602.08696",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The aim of this contribution is to derive a general matrix formula for the\nnet period premium paid in more than one state. For this purpose we propose to\ncombine actuarial technics with the graph optimization methodology. The\nobtained result is useful for example to more advanced models of dread disease\ninsurances allowing period premiums paid by both healthy and ill person (e.g.\nnot terminally yet). As an application, we provide analysis of dread disease\ninsurances against the risk of lung cancer based on the actual data for the\nLower Silesian Voivodship in Poland.\n"
    },
    {
        "paper_id": 1701.07318,
        "authors": "Y. \\c{C}inar",
        "title": "Research and Teaching Efficiencies of Turkish Universities with\n  Heterogeneity Considerations: Application of Multi-Activity DEA and DEA by\n  Sequential Exclusion of Alternatives Methods",
        "comments": "24 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The research and teaching efficiencies of 45 Turkish state universities are\nevaluated by using Multi-Activity Data Envelopment Analysis (MA-DEA) model\ndeveloped by Beasley (1995). Universities are multi-purpose institutions,\ntherefore they face multiple production functions simultaneously associated\nwith research and teaching activities. MA-DEA allows assigning priorities and\nallocating shared resources to these activities.\n"
    },
    {
        "paper_id": 1701.07321,
        "authors": "F. Aleskerov, E. Victorova",
        "title": "An analysis of potential conflict zones in the arctic region",
        "comments": "20 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  As a result of the climate change the situation in Arctic area leads to\nseveral important consequences. On the one hand, oil and gas resources can be\nexploited much easier than before. Thus, one can already observe discussions on\ndisputed shelf zones where the deposits are located. On the other hand, oil and\ngas excavation leads to serious potential threats to fishing by changing\nnatural habitats which in turn can create serious damage to the economies of\nsome countries in the region. Another set of problems arises due to the\nextension of navigable season for Arctic Shipping Routes.\n"
    },
    {
        "paper_id": 1701.07322,
        "authors": "F. Aleskerov, I. Frumin, E. Kardanova",
        "title": "Heterogeneity of the educational system: an introduction to the problem",
        "comments": "64 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We analyze a heterogeneity of the educational system on the basis of one\nparameter: input grades of university students. We propose a mathematical model\nbased on the construction of universities interval order. We use the Hamming\ndistance to evaluate the heterogeneity of the educational system, and the\nUnified State Examination (USE) scores of Russian students to illustrate the\napplication of the model. We show that institutions taking weak students turn\nthe whole system of universities into a poorly structured nonhomogeneous\nsystem. In contrast, after deleting the weakest part, the remaining set of\nuniversities becomes a well-structured system\n"
    },
    {
        "paper_id": 1701.07323,
        "authors": "Abdelatif Kerzabi",
        "title": "Les produits Halal dans les {\\'e}conomies occidentales",
        "comments": "in French",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In last years, we hear about halal products in non- Muslim societies\nincluding European and American ones. In France, for example, sales of halal\nproducts sold in stores during the year 2010, increased 23 % and represented\n5.5 billion euros, including 1.1 billion for the fast food, and it has not\nstopped growing since. A new market is not only about Muslims. Halal is a\nreligious interpretation but its rapid development requires that we question\nhis true motives are not just religious\n"
    },
    {
        "paper_id": 1701.07333,
        "authors": "Asaf Levi, Juan Sabuco, Miguel A. F. Sanjuan",
        "title": "Supply based on demand dynamical model",
        "comments": "26 pages, 11 figures",
        "journal-ref": null,
        "doi": "10.1016/j.cnsns.2017.10.008",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We propose and analyze numerically a simple dynamical model that describes\nthe firm behaviors under uncertainty of demand forecast. Iterating this simple\nmodel and varying some parameters values we observe a wide variety of market\ndynamics such as equilibria, periodic and chaotic behaviors. Interestingly the\nmodel is also able to reproduce market collapses.\n"
    },
    {
        "paper_id": 1701.08204,
        "authors": "Gaoyue Guo",
        "title": "A stability result on optimal Skorokhod embedding",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Motivated by the model- independent pricing of derivatives calibrated to the\nreal market, we consider an optimization problem similar to the optimal\nSkorokhod embedding problem, where the embedded Brownian motion needs only to\nreproduce a finite number of prices of Vanilla options. We derive in this paper\nthe corresponding dualities and the geometric characterization of optimizers.\nThen we show a stability result, i.e. when more and more Vanilla options are\ngiven, the optimization problem converges to an optimal Skorokhod embedding\nproblem, which constitutes the basis of the numerical computation in practice.\nIn addition, by means of different metrics on the space of probability\nmeasures, a convergence rate analysis is provided under suitable conditions.\n"
    },
    {
        "paper_id": 1701.08299,
        "authors": "Viktor Witkovsky, Gejza Wimmer, Tomas Duby",
        "title": "Computing the aggregate loss distribution based on numerical inversion\n  of the compound empirical characteristic function of frequency and severity",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  A non-parametric method for evaluation of the aggregate loss distribution\n(ALD) by combining and numerically inverting the empirical characteristic\nfunctions (CFs) is presented and illustrated. This approach to evaluate ALD is\nbased on purely non-parametric considerations, i.e., based on the empirical CFs\nof frequency and severity of the claims in the actuarial risk applications.\nThis approach can be, however, naturally generalized to a more complex\nsemi-parametric modeling approach, e.g., by incorporating the generalized\nPareto distribution fit of the severity distribution heavy tails, and/or by\nconsidering the weighted mixture of the parametric CFs (used to model the\nexpert knowledge) and the empirical CFs (used to incorporate the knowledge\nbased on the historical data - internal and/or external). Here we present a\nsimple and yet efficient method and algorithms for numerical inversion of the\nCF, suitable for evaluation of the ALDs and the associated measures of interest\nimportant for applications, as, e.g., the value at risk (VaR). The presented\napproach is based on combination of the Gil-Pelaez inversion formulae for\nderiving the probability distribution (PDF and CDF) from the compound\n(empirical) CF and the trapezoidal rule used for numerical integration. The\napplicability of the suggested approach is illustrated by analysis of a well\nknow insurance dataset, the Danish fire loss data.\n"
    },
    {
        "paper_id": 1701.08399,
        "authors": "Tomasz R. Bielecki, Igor Cialenco and Marek Rutkowski",
        "title": "Arbitrage-Free Pricing Of Derivatives In Nonlinear Market Models",
        "comments": "Forthcoming in Probability, Uncertainty and Quantitative Risk",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The objective of this paper is to provide a comprehensive study no-arbitrage\npricing of financial derivatives in the presence of funding costs, the\ncounterparty credit risk and market frictions affecting the trading mechanism,\nsuch as collateralization and capital requirements. To achieve our goals, we\nextend in several respects the nonlinear pricing approach developed in El\nKaroui and Quenez (1997) and El Karoui et al. (1997), which was subsequently\ncontinued in Bielecki and Rutkowski (2015).\n"
    },
    {
        "paper_id": 1701.08545,
        "authors": "Rafael Company, Vera Egorova, Lucas J\\'odar, Fazlollah Soleymani",
        "title": "Computing stable numerical solutions for multidimensional American\n  option pricing problems: a semi-discretization approach",
        "comments": "8 figures, 5 tables",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The matter of the stability for multi-asset American option pricing problems\nis a present remaining challenge. In this paper a general transformation of\nvariables allows to remove cross derivative terms reducing the stencil of the\nproposed numerical scheme and underlying computational cost. Solution of a such\nproblem is constructed by starting with a semi-discretization approach followed\nby a full discretization using exponential time differencing and matrix\nquadrature rules. To the best of our knowledge the stability of the numerical\nsolution is treated in this paper for the first time. Analysis of the time\nvariation of the numerical solution with respect to previous time level\ntogether with the use of logarithmic norm of matrices are the basis of the\nstability result. Sufficient stability conditions on step sizes, that also\nguarantee positivity and boundedness of the solution, are found. Numerical\nexamples for two and three asset problems justify the stability conditions and\nprove its competitiveness with other relevant methods.\n"
    },
    {
        "paper_id": 1701.08567,
        "authors": "Lamb Wubin, Naixin Ren",
        "title": "Decision structure of risky choice",
        "comments": "13 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  As we know, there is a controversy about the decision making under risk\nbetween economists and psychologists. We discuss to build a unified theory of\nrisky choice, which would explain both of compensatory and non-compensatory\ntheories. For risky choice, according to cognition ability, we argue that\npeople could not build a continuous and accurate subjective probability world,\nbut several order concepts, such as small, middle and large probability. People\nmake decisions based on information, experience, imagination and other things.\nAll of these things are so huge that people have to prepare some strategies.\nThat is, people have different strategies when facing to different situations.\nThe distributions of these things have different decision structures. More\nprecisely, decision making is a process of simplifying the decision structure.\nHowever, the process of decision structure simplifying is not stuck in a rut,\nbut through different path when facing problems repeatedly. It is why\npreference reversal always happens when making decisions. The most efficient\nway to simplify the decision structure is calculating expected value or making\ndecisions based on one or two dimensions. We also argue that the deliberation\ntime at least has four parts, which are consist of substitution time, first\norder time, second order time and calculation time. Decision structure also can\nsimply explain the phenomenon of paradoxes and anomalies. JEL Codes: C10, D03,\nD81\n"
    },
    {
        "paper_id": 1701.08579,
        "authors": "Juozas Vaicenavicius",
        "title": "Asset liquidation under drift uncertainty and regime-switching\n  volatility",
        "comments": "30 pages, some minor improvements",
        "journal-ref": "Applied Mathematics & Optimization (2018)",
        "doi": "10.1007/s00245-018-9518-5",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Optimal liquidation of an asset with unknown constant drift and stochastic\nregime-switching volatility is studied. The uncertainty about the drift is\nrepresented by an arbitrary probability distribution; the stochastic volatility\nis modelled by $m$-state Markov chain. Using filtering theory, an equivalent\nreformulation of the original problem as a four-dimensional optimal stopping\nproblem is found and then analysed by constructing approximating sequences of\nthree-dimensional optimal stopping problems. An optimal liquidation strategy\nand various structural properties of the problem are determined. Analysis of\nthe two-point prior case is presented in detail, building on which, an outline\nof the extension to the general prior case is given.\n"
    },
    {
        "paper_id": 1701.08711,
        "authors": "Vinci Chow",
        "title": "Predicting Auction Price of Vehicle License Plate with Deep Recurrent\n  Neural Network",
        "comments": null,
        "journal-ref": null,
        "doi": "10.1016/j.eswa.2019.113008",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In Chinese societies, superstition is of paramount importance, and vehicle\nlicense plates with desirable numbers can fetch very high prices in auctions.\nUnlike other valuable items, license plates are not allocated an estimated\nprice before auction. I propose that the task of predicting plate prices can be\nviewed as a natural language processing (NLP) task, as the value depends on the\nmeaning of each individual character on the plate and its semantics. I\nconstruct a deep recurrent neural network (RNN) to predict the prices of\nvehicle license plates in Hong Kong, based on the characters on a plate. I\ndemonstrate the importance of having a deep network and of retraining.\nEvaluated on 13 years of historical auction prices, the deep RNN's predictions\ncan explain over 80 percent of price variations, outperforming previous models\nby a significant margin. I also demonstrate how the model can be extended to\nbecome a search engine for plates and to provide estimates of the expected\nprice distribution.\n"
    },
    {
        "paper_id": 1701.08789,
        "authors": "Akash Malhotra, Mayank Maloo",
        "title": "Understanding food inflation in India: A Machine Learning approach",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Over the past decade, the stellar growth of Indian economy has been\nchallenged by persistently high levels of inflation, particularly in food\nprices. The primary reason behind this stubborn food inflation is mismatch in\nsupply-demand, as domestic agricultural production has failed to keep up with\nrising demand owing to a number of proximate factors. The relative significance\nof these factors in determining the change in food prices have been analysed\nusing gradient boosted regression trees (BRT), a machine learning technique.\nThe results from BRT indicates all predictor variables to be fairly significant\nin explaining the change in food prices, with MSP and farm wages being\nrelatively more important than others. International food prices were found to\nhave limited relevance in explaining the variation in domestic food prices. The\nchallenge of ensuring food and nutritional security for growing Indian\npopulation with rising incomes needs to be addressed through resolute policy\nreforms.\n"
    },
    {
        "paper_id": 1701.08861,
        "authors": "Romuald Elie and Ludovic Moreau and Dylan Possama\\\"i",
        "title": "On a class of path-dependent singular stochastic control problems",
        "comments": "33 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper studies a class of non$-$Markovian singular stochastic control\nproblems, for which we provide a novel probabilistic representation. The\nsolution of such control problem is proved to identify with the solution of a\n$Z-$constrained BSDE, with dynamics associated to a non singular underlying\nforward process. Due to the non$-$Markovian environment, our main argumentation\nrelies on the use of comparison arguments for path dependent PDEs. Our\nrepresentation allows in particular to quantify the regularity of the solution\nto the singular stochastic control problem in terms of the space and time\ninitial data. Our framework also extends to the consideration of degenerate\ndiffusions, leading to the representation of the solution as the infimum of\nsolutions to $Z-$constrained BSDEs. As an application, we study the utility\nmaximisation problem with transaction costs for non$-$Markovian dynamics.\n"
    },
    {
        "paper_id": 1701.08972,
        "authors": "Takashi Kato",
        "title": "An Optimal Execution Problem in the Volume-Dependent Almgren-Chriss\n  Model",
        "comments": "22 pages, 4 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this study, we introduce an explicit trading-volume process into the\nAlmgren-Chriss model, which is a standard model for optimal execution. We\npropose a penalization method for deriving a verification theorem for an\nadaptive optimization problem. We also discuss the optimality of the\nvolume-weighted average-price strategy of a risk-neutral trader. Moreover, we\nderive a second-order asymptotic expansion of the optimal strategy and verify\nits accuracy numerically.\n"
    },
    {
        "paper_id": 1701.09043,
        "authors": "Marco Pangallo, James Sanders, Tobias Galla and Doyne Farmer",
        "title": "Towards a taxonomy of learning dynamics in 2 x 2 games",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Do boundedly rational players learn to choose equilibrium strategies as they\nplay a game repeatedly? A large literature in behavioral game theory has\nproposed and experimentally tested various learning algorithms, but a\ncomparative analysis of their equilibrium convergence properties is lacking. In\nthis paper we analyze Experience-Weighted Attraction (EWA), which generalizes\nfictitious play, best-response dynamics, reinforcement learning and also\nreplicator dynamics. Studying $2\\times 2$ games for tractability, we recover\nsome well-known results in the limiting cases in which EWA reduces to the\nlearning rules that it generalizes, but also obtain new results for other\nparameterizations. For example, we show that in coordination games EWA may only\nconverge to the Pareto-efficient equilibrium, never reaching the\nPareto-inefficient one; that in Prisoner Dilemma games it may converge to fixed\npoints of mutual cooperation; and that limit cycles or chaotic dynamics may be\nmore likely with longer or shorter memory of previous play.\n"
    },
    {
        "paper_id": 1702.00037,
        "authors": "Foad Shokrollahi",
        "title": "Fractional delta hedging strategy for pricing currency options with\n  transaction costs",
        "comments": "14 pages, 3 figures",
        "journal-ref": "Communications in Mathematical Finance, vol.6, no.1, 2017, 1-20",
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This study deals with the problem of pricing European currency options in\ndiscrete time setting, whose prices follow the fractional Black Scholes model\nwith transaction costs. Both the pricing formula and the fractional partial\ndifferential equation for European call currency options are obtained by\napplying the delta-hedging strategy. Some Greeks and the estimator of\nvolatility are also provided. The empirical studies and the simulation findings\nshow that the fractional Black Scholes with transaction costs is a satisfactory\nmodel.\n"
    },
    {
        "paper_id": 1702.00144,
        "authors": "Taisei Kaizoji and Michiko Miyano",
        "title": "Zipf's law for share price and company fundamentals",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We statistically investigate the distribution of share price and the\ndistributions of three common financial indicators using data from\napproximately 8,000 companies publicly listed worldwide for the period\n2004-2013. We find that the distribution of share price follows Zipf's law;\nthat is, it can be approximated by a power law distribution with exponent equal\nto 1. An examination of the distributions of dividends per share, cash flow per\nshare, and book value per share - three financial indicators that can be\nassumed to influence corporate value (i.e. share price) - shows that these\ndistributions can also be approximated by a power law distribution with\npower-law exponent equal to 1. We estimate a panel regression model in which\nshare price is the dependent variable and the three financial indicators are\nexplanatory variables. The two-way fixed effects model that was selected as the\nbest model has quite high power for explaining the actual data. From these\nresults, we can surmise that the reason why share price follows Zipf's law is\nthat corporate value, i.e. company fundamentals, follows Zipf's law.\n"
    },
    {
        "paper_id": 1702.00152,
        "authors": "Foad Shokrollahi",
        "title": "The valuation of European option with transaction costs by mixed\n  fractional Merton model",
        "comments": "14 pages, 3 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper deals with the problem of discrete-time option pricing by the\nmixed fractional version of Merton model with transaction costs. By a\nmean-self-financing delta hedging argument in a discrete-time setting, a\nEuropean call option pricing formula is obtained. We also investigate the\neffect of the time-step $\\delta t$ and the Hurst parameter $H$ on our pricing\noption model, which reveals that these parameters have high impact on option\npricing. The properties of this model are also explained.\n"
    },
    {
        "paper_id": 1702.00215,
        "authors": "Alessandra Cretarola and Gianna Fig\\`a-Talamanca",
        "title": "A confidence-based model for asset and derivative prices in the BitCoin\n  market",
        "comments": null,
        "journal-ref": null,
        "doi": "10.1007/s10203-019-00262-x",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We endorse the idea, suggested in recent literature, that BitCoin prices are\ninfluenced by sentiment and confidence about the underlying technology; as a\nconsequence, an excitement about the BitCoin system may propagate to BitCoin\nprices causing a Bubble effect, the presence of which is documented in several\npapers about the cryptocurrency. In this paper we develop a bivariate model in\ncontinuous time to describe the price dynamics of one BitCoin as well as the\nbehavior of a second factor affecting the price itself, which we name\nconfidence indicator. The two dynamics are possibly correlated and we also take\ninto account a delay between the confidence indicator and its delivered effect\non the BitCoin price. Statistical properties of the suggested model are\ninvestigated and its arbitrage-free property is shown. Further, based on\nrisk-neutral evaluation, a quasi-closed formula is derived for European style\nderivatives on the BitCoin. A short numerical application is finally provided.\n"
    },
    {
        "paper_id": 1702.00586,
        "authors": "Claude Godreche, Satya N. Majumdar, Gregory Schehr",
        "title": "Record statistics of a strongly correlated time series: random walks and\n  L\\'evy flights",
        "comments": "64 pages, 14 figures. Topical review, submitted for publication in J.\n  Phys. A",
        "journal-ref": "J. Phys. A: Math. Theor. 50, 333001 (2017)",
        "doi": "10.1088/1751-8121/aa71c1",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We review recent advances on the record statistics of strongly correlated\ntime series, whose entries denote the positions of a random walk or a L\\'evy\nflight on a line. After a brief survey of the theory of records for independent\nand identically distributed random variables, we focus on random walks. During\nthe last few years, it was indeed realized that random walks are a very useful\n\"laboratory\" to test the effects of correlations on the record statistics. We\nstart with the simple one-dimensional random walk with symmetric jumps (both\ncontinuous and discrete) and discuss in detail the statistics of the number of\nrecords, as well as of the ages of the records, i.e., the lapses of time\nbetween two successive record breaking events. Then we review the results that\nwere obtained for a wide variety of random walk models, including random walks\nwith a linear drift, continuous time random walks, constrained random walks\n(like the random walk bridge) and the case of multiple independent random\nwalkers. Finally, we discuss further observables related to records, like the\nrecord increments, as well as some questions raised by physical applications of\nrecord statistics, like the effects of measurement error and noise.\n"
    },
    {
        "paper_id": 1702.00982,
        "authors": "Miklos Rasonyi",
        "title": "On utility maximization without passing by the dual problem",
        "comments": "Corrections in the proof of Theorem 3.6, modified definition of Fatou\n  convergence and changes in Lemma 4.1",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We treat utility maximization from terminal wealth for an agent with utility\nfunction $U:\\mathbb{R}\\to\\mathbb{R}$ who dynamically invests in a\ncontinuous-time financial market and receives a possibly unbounded random\nendowment. We prove the existence of an optimal investment without introducing\nthe associated dual problem. We rely on a recent result of Orlicz space theory,\ndue to Delbaen and Owari which leads to a simple and transparent proof.\n  Our results apply to non-smooth utilities and even strict concavity can be\nrelaxed. We can handle certain random endowments with non-hedgeable risks,\ncomplementing earlier papers. Constraints on the terminal wealth can also be\nincorporated.\n  As examples, we treat frictionless markets with finitely many assets and\nlarge financial markets.\n"
    },
    {
        "paper_id": 1702.00994,
        "authors": "Jacob Lundgren, Yuri Shpolyanskiy",
        "title": "Approaches to Asian Option Pricing with Discrete Dividends",
        "comments": "19 pages, 9 figures, 5 tables, LaTeX; corrected detail in description\n  of hybrid numerical scheme + illustration, results unchanged",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The method and characteristics of several approaches to the pricing of\ndiscretely monitored arithmetic Asian options on stocks with discrete, absolute\ndividends are described. The contrast between method behaviors for options with\nan Asian tail and those with monitoring throughout their lifespan is\nemphasized. Rates of convergence are confirmed, but greater focus is put on\nactual performance in regions of accuracy which are realistic for use by\npractitioners. A hybrid approach combining Curran's analytical approximation\nwith a two-dimensional finite difference method is examined with respect to the\nerrors caused by the approximating assumptions. For Asian tails of equidistant\nmonitoring dates, this method performs very well, but as the scenario deviates\nfrom the method's ideal conditions, the errors in the approximation grow\nunfeasible. For general monitoring straightforward solution of the full\nthree-dimensional partial differential equation by finite differences is highly\naccurate but suffers from rapid degradation in performance as the monitoring\ninterval increases. For options with long monitoring intervals a randomized\nquasi-Monte Carlo method with control variate variance reduction stands out as\na powerful alternative.\n"
    },
    {
        "paper_id": 1702.01017,
        "authors": "Diptesh Ghosh, Anindya S. Chakrabarti",
        "title": "Emergence of Distributed Coordination in the Kolkata Paise Restaurant\n  Problem with Finite Information",
        "comments": "12 pages, 4 figures",
        "journal-ref": null,
        "doi": "10.1016/j.physa.2017.04.171",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper, we study a large-scale distributed coordination problem and\npropose efficient adaptive strategies to solve the problem. The basic problem\nis to allocate finite number of resources to individual agents such that there\nis as little congestion as possible and the fraction of unutilized resources is\nreduced as far as possible. In the absence of a central planner and global\ninformation, agents can employ adaptive strategies that uses only finite\nknowledge about the competitors. In this paper, we show that a combination of\nfinite information sets and reinforcement learning can increase the utilization\nrate of resources substantially.\n"
    },
    {
        "paper_id": 1702.01045,
        "authors": "St\\'ephane Cr\\'epey (LaMME), Shiqi Song (LaMME)",
        "title": "Invariance times",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  On a probability space $(\\Omega,\\mathcal{A},\\mathbb{Q})$ we consider two\nfiltrations $\\mathbb{F}\\subset \\mathbb{G}$ and a $\\mathbb{G}$ stopping time\n$\\theta$ such that the $\\mathbb{G}$ predictable processes coincide with\n$\\mathbb{F}$ predictable processes on $(0,\\theta]$. In this setup it is\nwell-known that, for any $\\mathbb{F}$ semimartingale $X$, the process\n$X^{\\theta-}$ ($X$ stopped \"right before $\\theta$\") is a $\\mathbb{G}$\nsemimartingale.Given a positive constant $T$, we call $\\theta$ an invariance\ntime if there exists a probability measure $\\mathbb{P}$ equivalent to\n$\\mathbb{Q}$ on $\\mathcal{F}\\_T$ such that, for any $(\\mathbb{F},\\mathbb{P})$\nlocal martingale $X$, $X^{\\theta-}$ is a $(\\mathbb{G},\\mathbb{Q})$ local\nmartingale. We characterize invariance times in terms of the\n$(\\mathbb{F},\\mathbb{Q})$ Az\\'ema supermartingale of $\\theta$ and we derive a\nmild and tractable invariance time sufficiency condition. We discuss invariance\ntimes in mathematical finance and BSDE applications.\n"
    },
    {
        "paper_id": 1702.01164,
        "authors": "Jose E. Figueroa-Lopez and K. Lee",
        "title": "Estimation of a noisy subordinated Brownian Motion via two-scales power\n  variations",
        "comments": "2 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  High frequency based estimation methods for a semiparametric pure-jump\nsubordinated Brownian motion exposed to a small additive microstructure noise\nare developed building on the two-scales realized variations approach\noriginally developed by Zhang et. al. (2005) for the estimation of the\nintegrated variance of a continuous Ito process. The proposed estimators are\nshown to be robust against the noise and, surprisingly, to attain better rates\nof convergence than their precursors, method of moment estimators, even in the\nabsence of microstructure noise. Our main results give approximate optimal\nvalues for the number K of regular sparse subsamples to be used, which is an\nimportant tune-up parameter of the method. Finally, a data-driven plug-in\nprocedure is devised to implement the proposed estimators with the optimal\nK-value. The developed estimators exhibit superior performance as illustrated\nby Monte Carlo simulations and a real high-frequency data application.\n"
    },
    {
        "paper_id": 1702.01175,
        "authors": "Takanori Adachi and Yoshihiro Ryu",
        "title": "Monetary value measures in a category of probability spaces",
        "comments": "7 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We generalize the notion of monetary value measures developed with category\ntheory in [Adachi, 2014] by extending their base category from the category\n\\c{hi} to the category of probability spaces Prob introduced in [Adachi and\nRyu, 2016].\n"
    },
    {
        "paper_id": 1702.01354,
        "authors": "Md. Mahmudul Alam, Kazi Ashraful Alam, Md. Gazi Salah Uddin",
        "title": "Market Depth and Risk Return Analysis of Dhaka Stock Exchange: An\n  Empirical Test of Market Efficiency",
        "comments": null,
        "journal-ref": "Publisher- ASA University Review, Vol. 1(1), pp. 93-101 (2007)",
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  It is customary that when security prices fully reflect all available\ninformation, the markets for those securities are said to be efficient. And if\nmarkets are inefficient, investors can use available information ignored by the\nmarket to earn abnormally high returns on their investments. In this context\nthis paper tries to find evidence supporting the reality of weak-form\nefficiency of the Dhaka Stock Exchange (DSE) by examining the issues of market\nrisk-return relationship and market depth or liquidity for DSE. The study uses\na data set of daily market index and returns for the period of 1994 to 2005 and\nweekly market capital turnover in proportion of total market capital for the\nperiod of 1994 to 2005. The paper also looks about the market risk (systemic\nrisk) and return where it is found that market rate of return of DSE is very\nlow or sometimes negative. Eventually Capital Asset Pricing Model (CAPM), which\nenvisages the relationship between risk and the expected rate of return on a\nrisky security, is found unrelated in DSE market. As proper risk-return\nrelationships of the market is seems to be deficient in DSE and the market is\nnot liquid, interest of the available investors are bring into being very\ninsignificant. All these issues are very noteworthy to the security analysts,\ninvestors and security exchange regulatory bodies in their policy making\ndecisions to progress the market condition.\n"
    },
    {
        "paper_id": 1702.01362,
        "authors": "Nina Anchugina, Matthew Ryan, Arkadii Slinko",
        "title": "Hyperbolic Discounting of the Far-Distant Future",
        "comments": "8 pages, 2 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We prove an analogue of Weitzman's (1998) famous result that an exponential\ndiscounter who is uncertain of the appropriate exponential discount rate should\ndiscount the far-distant future using the lowest (i.e., most patient) of the\npossible discount rates. Our analogous result applies to a hyperbolic\ndiscounter who is uncertain about the appropriate hyperbolic discount rate. In\nthis case, the far-distant future should be discounted using the\nprobability-weighted harmonic mean of the possible hyperbolic discount rates.\n"
    },
    {
        "paper_id": 1702.01385,
        "authors": "Masaaki Fukasawa and Mitja Stadje",
        "title": "Perfect hedging under endogenous permanent market impacts",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We model a nonlinear price curve quoted in a market as the utility\nindifference curve of a representative liquidity supplier. As the utility\nfunction we adopt a g-expectation. In contrast to the standard framework of\nfinancial engineering, a trader is no more price taker as any trade has a\npermanent market impact via an effect to the supplier's inventory. The P&L of a\ntrading strategy is written as a nonlinear stochastic integral. Under this\nmarket impact model, we introduce a completeness condition under which any\nderivative can be perfectly replicated by a dynamic trading strategy. In the\nspecial case of a Markovian setting the corresponding pricing and hedging can\nbe done by solving a semi-linear PDE.\n"
    },
    {
        "paper_id": 1702.01686,
        "authors": "Pawan Kumar",
        "title": "Demonetization and Its Impact on Employment in India",
        "comments": "5",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  On November 08, the sudden announcement to demonetization the high\ndenomination currency notes sent tremors all across the country. Given the\ntiming, and socioeconomic and political repercussions of the decision, many\ntermed it a financial emergency. Given high proportion of these notes in\ncirculation, over 86 percent, it led to most economic activities, particularly\nemployment, affected in a big way. Political parties, however, seemed divided\non the issue, i.e. those in favor of the decision feel it will help to curb the\ngalloping size of black money, fake currency, cross boarder terrorism, etc. In\nsharp contrast, the others believe it is a purely misleading, decision, based\non no or poor understanding of black economy, and hence is only politically\nmotivated in wake of the assembly elections due in a couple of states.\n"
    },
    {
        "paper_id": 1702.01706,
        "authors": "Kim Weston",
        "title": "Existence of a Radner equilibrium in a model with transaction costs",
        "comments": "To appear in Mathematics and Financial Economics",
        "journal-ref": null,
        "doi": "10.1007/s11579-018-0214-7",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We prove the existence of a Radner equilibrium in a model with proportional\ntransaction costs on an infinite time horizon and analyze the effect of\ntransaction costs on the endogenously determined interest rate. Two agents\nreceive exogenous, unspanned income and choose between consumption and\ninvesting into an annuity. After establishing the existence of a discrete-time\nequilibrium, we show that the discrete-time equilibrium converges to a\ncontinuous-time equilibrium model. The continuous-time equilibrium provides an\nexplicit formula for the equilibrium interest rate in terms of the transaction\ncost parameter. We analyze the impact of transaction costs on the equilibrium\ninterest rate and welfare levels.\n"
    },
    {
        "paper_id": 1702.01742,
        "authors": "Alex Ushveridze",
        "title": "Business Dynamics in KPI Space. Some thoughts on how business analytics\n  can benefit from using principles of classical physics",
        "comments": "24 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The biggest problem with the methods of machine learning used today in\nbusiness analytics is that they do not generalize well and often fail when\napplied to new data. One of the possible approaches to this problem is to\nenrich these methods (which are almost exclusively based on statistical\nalgorithms) with some intrinsically deterministic add-ons borrowed from\ntheoretical physics. The idea proposed in this note is to divide the set of Key\nPerformance Indicators (KPIs) characterizing an individual business into the\nfollowing two distinct groups: 1) highly volatile KPIs mostly determined by\nexternal factors and thus poorly controllable by a business, and 2) relatively\nstable KPIs identified and controlled by a business itself. It looks like,\nwhereas the dynamics of the first group can, as before, be studied using\nstatistical methods, for studying and optimizing the dynamics of the second\ngroup it is better to use deterministic principles similar to the Principle of\nLeast Action of classical mechanics. Such approach opens a whole bunch of new\ninteresting opportunities in business analytics, with numerous practical\napplications including diverse aspects of operational and strategic planning,\nchange management, ROI optimization, etc. Uncovering and utilizing dynamical\nlaws of the controllable KPIs would also allow one to use dynamical invariants\nof business as the most natural sets of risk and performance indicators, and\nfacilitate business growth by using effects of parametric resonance with\nnatural business cycles.\n"
    },
    {
        "paper_id": 1702.01819,
        "authors": "Drew Fudenberg, Kevin He",
        "title": "Learning and Type Compatibility in Signaling Games",
        "comments": null,
        "journal-ref": "Econometrica, Vol. 86, No. 4, July 2018, 1215-1255",
        "doi": "10.3982/ECTA15085",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Which equilibria will arise in signaling games depends on how the receiver\ninterprets deviations from the path of play. We develop a micro-foundation for\nthese off-path beliefs, and an associated equilibrium refinement, in a model\nwhere equilibrium arises through non-equilibrium learning by populations of\npatient and long-lived senders and receivers. In our model, young senders are\nuncertain about the prevailing distribution of play, so they rationally send\nout-of-equilibrium signals as experiments to learn about the behavior of the\npopulation of receivers. Differences in the payoff functions of the types of\nsenders generate different incentives for these experiments. Using the Gittins\nindex (Gittins, 1979), we characterize which sender types use each signal more\noften, leading to a constraint on the receiver's off-path beliefs based on\n\"type compatibility\" and hence a learning-based equilibrium selection.\n"
    },
    {
        "paper_id": 1702.01936,
        "authors": "Michel Baes, Pablo Koch-Medina, Cosimo Munari",
        "title": "Existence, uniqueness and stability of optimal portfolios of eligible\n  assets",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In a capital adequacy framework, risk measures are used to determine the\nminimal amount of capital that a financial institution has to raise and invest\nin a portfolio of pre-specified eligible assets in order to pass a given\ncapital adequacy test. From a capital efficiency perspective, it is important\nto identify the set of portfolios of eligible assets that allow to pass the\ntest by raising the least amount of capital. We study the existence and\nuniqueness of such optimal portfolios as well as their sensitivity to changes\nin the underlying capital position. This naturally leads to investigating the\ncontinuity properties of the set-valued map associating to each capital\nposition the corresponding set of optimal portfolios. We pay special attention\nto lower semicontinuity, which is the key continuity property from a financial\nperspective. This \"stability\" property is always satisfied if the test is based\non a polyhedral risk measure but it generally fails once we depart from\npolyhedrality even when the reference risk measure is convex. However, lower\nsemicontinuity can be often achieved if one if one is willing to focuses on\nportfolios that are close to being optimal. Besides capital adequacy, our\nresults have a variety of natural applications to pricing, hedging, and capital\nallocation problems.\n"
    },
    {
        "paper_id": 1702.02007,
        "authors": "Dogus Ozuyar, Sevilay Gumus Ozuyar, Oguzhan Karadeniz, Ozge Varol",
        "title": "The Installation Costs of a Satellite and Space Shuttle Launch Complex\n  as a Public Expenditure Project",
        "comments": "2nd International Annual Meeting of Sosyoekonomi Society",
        "journal-ref": "Sosyoekonomi proceedings book, ISBN:978-605-9190-61-9, 2016",
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  From the 1940's to the present, space explorations, which is a highly\nimportant topic for the world and human beings, penetrate into many areas from\nthe communication to the national security as well as from the discovery of\nexoplanets and new life forms to space mining. On the top of the countries\nwhich do researches on these fields are the developed countries and the\ndeveloping countries are only used as launch areas, in an irrelevant manner of\nthe research and development. However, developing countries can significantly\nreduce foreign dependency and security flaws as well as providing important\nreputation gain in international platforms by conducting space research and\ndevelopment activities as already done by developed countries. All the large\nscale space probes conducted by developed countries oblige Turkey to develop\nspace researches in terms of economy, security and scientific aspects. Due to\nthese reasons, the approximate costs of a launch base, which will be installed\nto conduct space researches in Turkey, and of a satellite or a spacecraft,\nwhich will be able to launch from this base and serve a variety of purposes,\nare calculated in this study. In an effort to make the mentioned calculations,\nexamples of various countries that have already established a launch base and\nalready launched from these bases are analyzed and some projections are built\nfor Turkey by calculating the estimated costs. Since these projections must be\ncarried out by the Republic of Turkey since the private sector in Turkey will\nnot be willing to invest in such activities, the possible public advantages can\nbe gained through these activities are also mentioned and evaluated.\n"
    },
    {
        "paper_id": 1702.02087,
        "authors": "Kasper Larsen, Halil Mete Soner, and Gordan \\v{Z}itkovi\\'c",
        "title": "Conditional Davis Pricing",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We study the set of marginal utility-based prices of a financial derivative\nin the case where the investor has a non-replicable random endowment. We\nprovide an example showing that even in the simplest of settings - such as\nSamuelson's geometric Brownian motion model - the interval of marginal\nutility-based prices can be a non-trivial strict subinterval of the set of all\nno-arbitrage prices. This is in stark contrast to the case with a replicable\nendowment where non- uniqueness is exceptional. We provide formulas for the end\npoints for these prices and illustrate the theory with several examples.\n"
    },
    {
        "paper_id": 1702.02254,
        "authors": "Nina Anchugina",
        "title": "One-Switch Discount Functions",
        "comments": "37 pages, 11 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Bell (1988) introduced the one-switch property for preferences over sequences\nof dated outcomes. This property concerns the effect of adding a common delay\nto two such sequences: it says that the preference ranking of the delayed\nsequences is either independent of the delay, or else there is a unique delay\nsuch that one strict ranking prevails for shorter delays and the opposite\nstrict ranking for longer delays. For preferences that have a discounted\nutility (DU) representation, Bell (1988) argues that the only discount\nfunctions consistent with the one-switch property are sums of exponentials.\nThis paper proves that discount functions of the linear times exponential form\nalso satisfy the one-switch property. We further demonstrate that preferences\nwhich have a DU representation with a linear times exponential discount\nfunction exhibit increasing impatience (Takeuchi (2011)). We also clarify an\nambiguity in the original Bell (1988) definition of the one-switch property by\ndistinguishing a weak one-switch property from the (strong) one-switch\nproperty. We show that the one-switch property and the weak one-switch property\ndefinitions are equivalent in a continuous-time version of the Anscombe and\nAumann (1963) setting.\n"
    },
    {
        "paper_id": 1702.02763,
        "authors": "Victor Olkhov",
        "title": "Econophysics of Macroeconomics: \"Action-at-a-Distance\" and Waves",
        "comments": "24 pages. arXiv admin note: text overlap with arXiv:1701.06625",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We present macroeconomic model that describes evolution of macroeconomic\nvariables and macroeconomic waves on economic space. Risk ratings of economic\nagents play role of their coordinates on economic space. Aggregation of\neconomic variables like Assets and Investment, Credits and Loans of economic\nagents at point x define corresponding macroeconomic variables as functions of\ntime t and coordinates x on economic space. Evolution of macroeconomic\nvariables is determined by economic and financial transactions between economic\nagents. Such transactions can occur between economic agents with any\ncoordinates x and y and that reflect non-local \"action-at-a-distance\" character\nof internal macroeconomic interactions. For instance, Buy-Sell transactions\nbetween points x and y on economic space define dynamics of Assets at point x\nand Investment at point y. Aggregates of transactions between economic agents\nat point x and y on economic space define economic fields as functions of two\ncoordinates. To describe dynamics of economic fields on economic space we\nderive hydrodynamic-like equations. For simple models of interactions between\neconomic fields we derive hydrodynamic-like equations in a closed form and\nobtain wave equations for their perturbations. Economic field waves propagate\non economic space and their amplitudes can grow up as exponent in time and may\ndisturb economic stability. Diversities of macroeconomic and financial waves on\neconomic space in simple models uncover importance of wave processes for\nmacroeconomic modeling and forecasting.\n"
    },
    {
        "paper_id": 1702.02777,
        "authors": "Giulia Livieri, Saad Mouti, Andrea Pallavicini and Mathieu Rosenbaum",
        "title": "Rough volatility: evidence from option prices",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  It has been recently shown that spot volatilities can be very well modeled by\nrough stochastic volatility type dynamics. In such models, the log-volatility\nfollows a fractional Brownian motion with Hurst parameter smaller than 1/2.\nThis result has been established using high frequency volatility estimations\nfrom historical price data. We revisit this finding by studying implied\nvolatility based approximations of the spot volatility. Using at-the-money\noptions on the S&P500 index with short maturity, we are able to confirm that\nvolatility is rough. The Hurst parameter found here, of order 0.3, is slightly\nlarger than that usually obtained from historical data. This is easily\nexplained from a smoothing effect due to the remaining time to maturity of the\nconsidered options.\n"
    },
    {
        "paper_id": 1702.02826,
        "authors": "Masaru Shintani and Ken Umeno",
        "title": "Super Generalized Central Limit Theorem: Limit distributions for sums of\n  non-identical random variables with power-laws",
        "comments": "4pages,1figure",
        "journal-ref": null,
        "doi": "10.7566/JPSJ.87.043003",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In nature or societies, the power-law is present ubiquitously, and then it is\nimportant to investigate the mathematical characteristics of power-laws in the\nrecent era of big data. In this paper we prove the superposition of\nnon-identical stochastic processes with power-laws converges in density to a\nunique stable distribution. This property can be used to explain the\nuniversality of stable laws such that the sums of the logarithmic return of\nnon-identical stock price fluctuations follow stable distributions.\n"
    },
    {
        "paper_id": 1702.03098,
        "authors": "Takaaki Koike, Mihoko Minami",
        "title": "Estimation of Risk Contributions with MCMC",
        "comments": "31 pages, 1 table, 7 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Determining risk contributions of unit exposures to portfolio-wide economic\ncapital is an important task in financial risk management. Computing risk\ncontributions involves difficulties caused by rare-event simulations. In this\nstudy, we address the problem of estimating risk contributions when the total\nrisk is measured by value-at-risk (VaR). Our proposed estimator of VaR\ncontributions is based on the Metropolis-Hasting (MH) algorithm, which is one\nof the most prevalent Markov chain Monte Carlo (MCMC) methods. Unlike existing\nestimators, our MH-based estimator consists of samples from conditional loss\ndistribution given a rare event of interest. This feature enhances sample\nefficiency compared with the crude Monte Carlo method. Moreover, our method has\nthe consistency and asymptotic normality, and is widely applicable to various\nrisk models having joint loss density. Our numerical experiments based on\nsimulation and real-world data demonstrate that in various risk models, even\nthose having high-dimensional (approximately 500) inhomogeneous margins, our MH\nestimator has smaller bias and mean squared error compared with existing\nestimators.\n"
    },
    {
        "paper_id": 1702.03226,
        "authors": "Bernardo Alves Furtado and Isaque Daniel Eberhardt Rocha",
        "title": "An applied spatial agent-based model of administrative boundaries using\n  SEAL",
        "comments": "11 pages. 4 figures. Accepted ABMUS 2017 as part of AAMAS 2017, S\\~ao\n  Paulo, Brazil",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper extends and adapts an existing abstract model into an empirical\nmetropolitan region in Brazil. The model - named SEAL: a Spatial Economic\nAgent-based Lab - comprehends a framework to enable public policy ex-ante\nanalysis. The aim of the model is to use official data and municipalities\nspatial boundaries to allow for policy experimentation. The current version\nconsiders three markets: housing, labor and goods. Families' members age,\nconsume, join the labor market and trade houses. A single consumption tax is\ncollected by municipalities that invest back into quality of life improvements.\nWe test whether a single metropolitan government - which is an aggregation of\nmunicipalities - would be in the best interest of its citizens. Preliminary\nresults for 20 simulation runs indicate that it may be the case. Future\ndevelopments include improving performance to enable running of higher\npercentage of the population and a number of runs that make the model more\nrobust.\n"
    },
    {
        "paper_id": 1702.03232,
        "authors": "St\\'ephane Cr\\'epey (LaMME), Shiqi Song (LaMME)",
        "title": "Invariance properties in the dynamic gaussian copula model *",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We prove that the default times (or any of their minima) in the dynamic\nGaussian copula model of Cr{\\'e}pey, Jeanblanc, and Wu (2013) are invariance\ntimes in the sense of Cr{\\'e}pey and Song (2017), with related invariance\nprobability measures different from the pricing measure. This reflects a\ndeparture from the immersion property, whereby the default intensities of the\nsurviving names and therefore the value of credit protection spike at default\ntimes. These properties are in line with the wrong-way risk feature of\ncounterparty risk embedded in credit derivatives, i.e. the adverse dependence\nbetween the default risk of a counterparty and an underlying credit derivative\nexposure.\n"
    },
    {
        "paper_id": 1702.0329,
        "authors": "Anup Rao",
        "title": "A Theory of Market Efficiency",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We introduce a mathematical theory called market connectivity that gives\nconcrete ways to both measure the efficiency of markets and find inefficiencies\nin large markets. The theory leads to new methods for testing the famous\nefficient markets hypothesis that do not suffer from the joint-hypothesis\nproblem that has plagued past work. Our theory suggests metrics that can be\nused to compare the efficiency of one market with another, to find\ninefficiencies that may be profitable to exploit, and to evaluate the impact of\npolicy and regulations on market efficiency.\n  A market's efficiency is tied to its ability to communicate information\nrelevant to market participants. Market connectivity calculates the speed and\nreliability with which this communication is carried out via trade in the\nmarket. We model the market by a network called the trade network, which can be\ncomputed by recording transactions in the market over a fixed interval of time.\nThe nodes of the network correspond to participants in the market. Every pair\nof nodes that trades in the market is connected by an edge that is weighted by\nthe rate of trade, and associated with a vector that represents the type of\nitem that is bought or sold.\n  We evaluate the ability of the market to communicate by considering how it\ndeals with shocks. A shock is a change in the beliefs of market participants\nabout the value of the products that they trade. We compute the effect of every\npotential significant shock on trade in the market. We give mathematical\ndefinitions for a few concepts that measure the ability of the market to\neffectively dissipate shocks.\n"
    },
    {
        "paper_id": 1702.03382,
        "authors": "Dan Pirjol, Lingjiong Zhu",
        "title": "Short Maturity Asian Options for the CEV Model",
        "comments": "37 pages, 4 figures",
        "journal-ref": "Prob. Eng. Inf. Sci. 33 (2019) 258-290",
        "doi": "10.1017/S0269964818000165",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We present a rigorous study of the short maturity asymptotics for Asian\noptions with continuous-time averaging, under the assumption that the\nunderlying asset follows the Constant Elasticity of Variance (CEV) model. We\npresent an analytical approximation for the Asian options prices which has the\nappropriate short maturity asymptotics, and demonstrate good numerical\nagreement of the asymptotic results with the results of Monte Carlo simulations\nand benchmark test cases for option parameters relevant in practical\napplications.\n"
    },
    {
        "paper_id": 1702.03838,
        "authors": "Iacopo Mastromatteo, Michael Benzaquen, Zoltan Eisler, Jean-Philippe\n  Bouchaud",
        "title": "Trading Lightly: Cross-Impact and Optimal Portfolio Execution",
        "comments": "7 pages, 4 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We model the impact costs of a strategy that trades a basket of correlated\ninstruments, by extending to the multivariate case the linear propagator model\npreviously used for single instruments. Our specification allows us to\ncalibrate a cost model that is free of arbitrage and price manipulation. We\nillustrate our results using a pool of US stocks and show that neglecting\ncross-impact effects leads to an incorrect estimation of the liquidity and\nsuboptimal execution strategies. We show in particular the importance of\nsynchronizing the execution of correlated contracts.\n"
    },
    {
        "paper_id": 1702.03977,
        "authors": "Yaofeng Fu, Ruokun Huang and Yiran Sheng",
        "title": "Labor Contract Law -An Economic View",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  China's new labor law -- Labor Contract Law has been put into practice for\nover one year. Since its inception, debates have been whirling around the\nnation, if not the world. In this article, we take an economic perspective to\nanalyze the possible impact of the core item -- open-ended employment contract,\nand we find that it deals poorly with adverse selection, with moral hazard\nproblems arise, which fails to meet the expectations of law-makers and other\nparties.\n"
    },
    {
        "paper_id": 1702.04053,
        "authors": "Wujiang Lou",
        "title": "Discounting with Imperfect Collateral",
        "comments": "27 pages, 2 figures, 6 tables",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Cash collateral is perfect in that it provides simultaneous counterparty\ncredit risk protection and derivatives funding. Securities are imperfect\ncollateral, because of collateral segregation or differences in CSA haircuts\nand repo haircuts. Moreover, the collateral rate term structure is not\nobservable in the repo market, for derivatives netting sets are perpetual while\nrepo tenors are typically in months. This article synthesizes these effects\ninto a derivative financing rate that replaces the risk-free discount rate. A\nbreak-even repo formulae is employed to supply non-observable collateral rates,\nenabling collateral liquidity value adjustment (LVA) to be computed. A linear\nprogramming problem of maximizing LVA under liquidity coverage ratio (LCR)\nconstraint is formulated as a core algorithm of collateral optimization.\nNumerical examples show that LVA could be sizable for long average duration,\ndeep in or out of the money swap portfolios.\n"
    },
    {
        "paper_id": 1702.04287,
        "authors": "Carsten Chong and Claudia Kl\\\"uppelberg",
        "title": "Contagion in financial systems: A Bayesian network approach",
        "comments": null,
        "journal-ref": "SIAM J. Financial Math., 9(1):28-53, 2018",
        "doi": "10.1137/17M1116659",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We develop a structural default model for interconnected financial\ninstitutions in a probabilistic framework. For all possible network structures\nwe characterize the joint default distribution of the system using Bayesian\nnetwork methodologies. Particular emphasis is given to the treatment and\nconsequences of cyclic financial linkages. We further demonstrate how Bayesian\nnetwork theory can be applied to detect contagion channels within the financial\nnetwork, to measure the systemic importance of selected entities on others, and\nto compute conditional or unconditional probabilities of default for single or\nmultiple institutions.\n"
    },
    {
        "paper_id": 1702.04289,
        "authors": "Martin Theissen, Sebastian M. Krause and Thomas Guhr",
        "title": "Regularities and Irregularities in Order Flow Data",
        "comments": "10 pages, 9 figures",
        "journal-ref": null,
        "doi": "10.1140/epjb/e2017-80087-6",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We identify and analyze statistical regularities and irregularities in the\nrecent order flow of different NASDAQ stocks, focusing on the positions where\norders are placed in the orderbook. This includes limit orders being placed\noutside of the spread, inside the spread and (effective) market orders. We find\nthat limit order placement inside the spread is strongly determined by the\ndynamics of the spread size. Most orders, however, arrive outside of the\nspread. While for some stocks order placement on or next to the quotes is\ndominating, deeper price levels are more important for other stocks. As market\norders are usually adjusted to the quote volume, the impact of market orders\ndepends on the orderbook structure, which we find to be quite diverse among the\nanalyzed stocks as a result of the way limit order placement takes place.\n"
    },
    {
        "paper_id": 1702.04388,
        "authors": "M. Assadsolimani, D. Chetalova",
        "title": "Estimating VaR in credit risk: Aggregate vs single loss distribution",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Using Monte Carlo simulation to calculate the Value at Risk (VaR) as a\npossible risk measure requires adequate techniques. One of these techniques is\nthe application of a compound distribution for the aggregates in a portfolio.\nIn this paper, we consider the aggregated loss of Gamma distributed severities\nand estimate the VaR by introducing a new approach to calculate the quantile\nfunction of the Gamma distribution at high confidence levels. We then compare\nthe VaR obtained from the aggregation process with the VaR obtained from a\nsingle loss distribution where the severities are drawn first from an\nexponential and then from a truncated exponential distribution. We observe that\nthe truncated exponential distribution as a model for the severities yields\nresults closer to those obtained from the aggregation process. The deviations\ndepend strongly on the number of obligors in the portfolio, but also on the\namount of gross loss which truncates the exponential distribution.\n"
    },
    {
        "paper_id": 1702.04443,
        "authors": "Takahiro Omi, Yoshito Hirata and Kazuyuki Aihara",
        "title": "Hawkes process model with a time-dependent background rate and its\n  application to high-frequency financial data",
        "comments": "11 pages, 7 figures",
        "journal-ref": "Phys. Rev. E 96, 012303 (2017)",
        "doi": "10.1103/PhysRevE.96.012303",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  A Hawkes process model with a time-varying background rate is developed for\nanalyzing the high-frequency financial data. In our model, the logarithm of the\nbackground rate is modeled by a linear model with a relatively large number of\nvariable-width basis functions, and the parameters are estimated by a Bayesian\nmethod. Our model can capture not only the slow time-variation, such as in the\nintraday seasonality, but also the rapid one, which follows a macroeconomic\nnews announcement. By analyzing the tick data of the Nikkei 225 mini, we find\nthat (i) our model is better fitted to the data than the Hawkes models with a\nconstant background rate or a slowly varying background rate, which have been\ncommonly used in the field of quantitative finance; (ii) the improvement in the\ngoodness-of-fit to the data by our model is significant especially for sessions\nwhere considerable fluctuation of the background rate is present; and (iii) our\nmodel is statistically consistent with the data. The branching ratio, which\nquantifies the level of the endogeneity of markets, estimated by our model is\n0.41, suggesting the relative importance of exogenous factors in the market\ndynamics. We also demonstrate that it is critically important to appropriately\nmodel the time-dependent background rate for the branching ratio estimation.\n"
    },
    {
        "paper_id": 1702.04642,
        "authors": "Dawei Cheng, Zhibin Niu, Yi Tu, Liqing Zhang",
        "title": "Prediction defaults for networked-guarantee loans",
        "comments": "6 pages,7 figures",
        "journal-ref": "2018 24th International Conference on Pattern Recognition (ICPR)",
        "doi": "10.1109/ICPR.2018.8545474",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Networked-guarantee loans may cause the systemic risk related concern of the\ngovernment and banks in China. The prediction of default of enterprise loans is\na typical extremely imbalanced prediction problem, and the networked-guarantee\nmake this problem more difficult to solve. Since the guaranteed loan is a debt\nobligation promise, if one enterprise in the guarantee network falls into a\nfinancial crisis, the debt risk may spread like a virus across the guarantee\nnetwork, even lead to a systemic financial crisis. In this paper, we propose an\nimbalanced network risk diffusion model to forecast the enterprise default risk\nin a short future. Positive weighted k-nearest neighbors (p-wkNN) algorithm is\ndeveloped for the stand-alone case -- when there is no default contagious; then\na data-driven default diffusion model is integrated to further improve the\nprediction accuracy. We perform the empirical study on a real-world three-years\nloan record from a major commercial bank. The results show that our proposed\nmethod outperforms conventional credit risk methods in terms of AUC. In\nsummary, our quantitative risk evaluation model shows promising prediction\nperformance on real-world data, which could be useful to both regulators and\nstakeholders.\n"
    },
    {
        "paper_id": 1702.04967,
        "authors": "Takanori Adachi and Michal Fabinger",
        "title": "Multi-Dimensional Pass-Through and Welfare Measures under Imperfect\n  Competition",
        "comments": "68 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper provides a comprehensive analysis of welfare measures when\noligopolistic firms face multiple policy interventions and external changes\nunder general forms of market demands, production costs, and imperfect\ncompetition. We present our results in terms of two welfare measures, namely,\nmarginal cost of public funds and incidence, in relation to multi-dimensional\npass-through. Our arguments are best understood with two-dimensional taxation\nwhere homogeneous firms face unit and ad valorem taxes. The first part of the\npaper studies this leading case. We show, e.g., that there exists a simple and\nempirically relevant set of sufficient statistics for the marginal cost of\npublic funds, namely unit tax and ad valorem pass-through and industry demand\nelasticity. We then specialize our general setting to the case of price or\nquantity competition and show how the marginal cost of public funds and the\npass-through are expressed using elasticities and curvatures of regular and\ninverse demands. Based on the results of the leading case, the second part of\nthe paper presents a generalization with the tax revenue function specified as\na general function parameterized by a vector of multi-dimensional tax\nparameters. We then argue that our results are carried over to the case of\nheterogeneous firms and other extensions.\n"
    },
    {
        "paper_id": 1702.05005,
        "authors": "Francisco Salas-Molina, Juan A. Rodr\\'iguez-Aguilar, Pablo\n  D\\'iaz-Garc\\'ia",
        "title": "PyCaMa: Python for cash management",
        "comments": "9 pages, 1 figure",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
        "abstract": "  Selecting the best policy to keep the balance between what a company holds in\ncash and what is placed in alternative investments is by no means\nstraightforward. We here introduce PyCaMa, a Python module for multiobjective\ncash management based on linear programming that allows to derive optimal\npolicies for cash management with multiple bank accounts in terms of both cost\nand risk of policies.\n"
    },
    {
        "paper_id": 1702.05036,
        "authors": "Jean-Pierre Fouque and Ning Ning",
        "title": "Uncertain Volatility Models with Stochastic Bounds",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper, we propose the uncertain volatility models with stochastic\nbounds. Like the regular uncertain volatility models, we know only that the\ntrue model lies in a family of progressively measurable and bounded processes,\nbut instead of using two deterministic bounds, the uncertain volatility\nfluctuates between two stochastic bounds generated by its inherent stochastic\nvolatility process. This brings better accuracy and is consistent with the\nobserved volatility path such as for the VIX as a proxy for instance. We apply\nthe regular perturbation analysis upon the worst case scenario price, and\nderive the first order approximation in the regime of slowly varying stochastic\nbounds. The original problem which involves solving a fully nonlinear PDE in\ndimension two for the worst case scenario price, is reduced to solving a\nnonlinear PDE in dimension one and a linear PDE with source, which gives a\ntremendous computational advantage. Numerical experiments show that this\napproximation procedure performs very well, even in the regime of moderately\nslow varying stochastic bounds.\n"
    },
    {
        "paper_id": 1702.05315,
        "authors": "Alessio Sancetta",
        "title": "Estimation for the Prediction of Point Processes with Many Covariates",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Estimation of the intensity of a point process is considered within a\nnonparametric framework. The intensity measure is unknown and depends on\ncovariates, possibly many more than the observed number of jumps. Only a single\ntrajectory of the counting process is observed. Interest lies in estimating the\nintensity conditional on the covariates. The impact of the covariates is\nmodelled by an additive model where each component can be written as a linear\ncombination of possibly unknown functions. The focus is on prediction as\nopposed to variable screening. Conditions are imposed on the coefficients of\nthis linear combination in order to control the estimation error. The rates of\nconvergence are optimal when the number of active covariates is large. As an\napplication, the intensity of the buy and sell trades of the New Zealand dollar\nfutures is estimated and a test for forecast evaluation is presented. A\nsimulation is included to provide some finite sample intuition on the model and\nasymptotic properties.\n"
    },
    {
        "paper_id": 1702.05434,
        "authors": "Mathias Pohl, Alexander Ristig, Walter Schachermayer and Ludovic\n  Tangpi",
        "title": "The amazing power of dimensional analysis: Quantifying market impact",
        "comments": "Revised version",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This note complements the inspiring work on dimensional analysis and market\nmicrostructure by Kyle and Obizhaeva [18]. Following closely these authors, our\nmain result shows by a similar argument as usually applied in physics the\nfollowing remarkable fact. If the market impact of a meta-order only depends on\nfour well-defined and financially meaningful variables, then -- up to a\nconstant -- there is only one possible form of this dependence. In particular,\nthe market impact is proportional to the square-root of the size of the\nmeta-order.\n  This theorem can be regarded as a special case of a more general result of\nKyle and Obizhaeva. These authors consider five variables which might have an\ninfluence on the size of the market impact. In this case one finds a richer\nvariety of possible functional relations which we precisely characterize. We\nalso discuss the analogies to classical arguments from physics, such as the\nperiod of a pendulum.\n"
    },
    {
        "paper_id": 1702.05649,
        "authors": "Tianran Geng and Thaleia Zariphopoulou",
        "title": "Temporal and Spatial Turnpike-Type Results Under Forward Time-Monotone\n  Performance Criteria",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We present turnpike-type results for the risk tolerance function in an\nincomplete market setting under time-monotone forward performance criteria. We\nshow that, contrary to the classical case, the temporal and spatial limits do\nnot coincide. We also show that they depend directly on the left- and right-end\nof the support of an underlying measure, which is used to construct the forward\nperformance criterion. We provide examples with discrete and continuous\nmeasures, and discuss the asymptotic behavior of the risk tolerance for each\ncase.\n"
    },
    {
        "paper_id": 1702.05809,
        "authors": "Adarsh Kulkarni, Priya Mani and Carlotta Domeniconi",
        "title": "Network-based Anomaly Detection for Insider Trading",
        "comments": "9 pages, 13 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Insider trading is one of the numerous white collar crimes that can\ncontribute to the instability of the economy. Traditionally, the detection of\nillegal insider trades has been a human-driven process. In this paper, we\ncollect the insider tradings made available by the US Securities and Exchange\nCommissions (SEC) through the EDGAR system, with the aim of initiating an\nautomated large-scale and data-driven approach to the problem of identifying\nillegal insider tradings. The goal of the study is the identification of\ninteresting patterns, which can be indicators of potential anomalies. We use\nthe collected data to construct networks that capture the relationship between\ntrading behaviors of insiders. We explore different ways of building networks\nfrom insider trading data, and argue for a need of a structure that is capable\nof capturing higher order relationships among traders. Our results suggest the\ndiscovery of interesting patterns.\n"
    },
    {
        "paper_id": 1702.05944,
        "authors": "Sachapon Tungsong, Fabio Caccioli, Tomaso Aste",
        "title": "Relation between regional uncertainty spillovers in the global banking\n  system",
        "comments": "24 pages, 4 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We report on time-varying network connectedness within three banking systems:\nNorth America, the EU, and ASEAN. The original method by Diebold and Yilmaz is\nimproved by using exponentially weighted daily returns and ridge regularization\non vector autoregression (VAR) and forecast error variance decomposition\n(FEVD). We compute the total network connectedness for each of the three\nbanking systems, which quantifies regional uncertainty. Results over rolling\nwindows of 300 days during the period between 2005 and 2015 reveal changing\nuncertainty patterns which are similar across regions, with common peaks\nassociated with identifiable exogenous events. Lead-lag relationships among\nchanges of total network connectedness of the three systems, quantified by\ntransfer entropy, reveal that uncertainties in the three regional systems are\nsignificantly causally related, with the North American system having the\nlargest influence on EU and ASEAN.\n"
    },
    {
        "paper_id": 1702.06055,
        "authors": "J. M. Chen, A. G. Hawkes, E. Scalas, M. Trinh",
        "title": "Performance of information criteria used for model selection of Hawkes\n  process models of financial data",
        "comments": "28 pages, 1 figure, submitted to a special issue of Quantitative\n  Finance. Some typos in the previous version have been now corrected",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We test three common information criteria (IC) for selecting the order of a\nHawkes process with an intensity kernel that can be expressed as a mixture of\nexponential terms. These processes find application in high-frequency financial\ndata modelling. The information criteria are Akaike's information criterion\n(AIC), the Bayesian information criterion (BIC) and the Hannan-Quinn criterion\n(HQ). Since we work with simulated data, we are able to measure the performance\nof model selection by the success rate of the IC in selecting the model that\nwas used to generate the data. In particular, we are interested in the relation\nbetween correct model selection and underlying sample size. The analysis\nincludes realistic sample sizes and parameter sets from recent literature where\nparameters were estimated using empirical financial intra-day data. We compare\nour results to theoretical predictions and similar empirical findings on the\nasymptotic distribution of model selection for consistent and inconsistent IC.\n"
    },
    {
        "paper_id": 1702.06191,
        "authors": "G. Ruiz L\\'opez and A. Fern\\'andez de Marcos",
        "title": "Evidence for criticality in financial data",
        "comments": "12 pages, 6 figures",
        "journal-ref": null,
        "doi": "10.1140/epjb/e2017-80535-3",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We provide evidence that cumulative distributions of absolute normalized\nreturns for the $100$ American companies with the highest market\ncapitalization, uncover a critical behavior for different time scales $\\Delta\nt$. Such cumulative distributions, in accordance with a variety of complex\n--and financial-- systems, can be modeled by the cumulative distribution\nfunctions of $q$-Gaussians, the distribution function that, in the context of\nnonextensive statistical mechanics, maximizes a non-Boltzmannian entropy. These\n$q$-Gaussians are characterized by two parameters, namely $(q,\\beta)$, that are\nuniquely defined by $\\Delta t$. From these dependencies, we find a monotonic\nrelationship between $q$ and $\\beta$, which can be seen as evidence of\ncriticality. We numerically determine the various exponents which characterize\nthis criticality.\n"
    },
    {
        "paper_id": 1702.06913,
        "authors": "Christian Kleiber",
        "title": "Structural Change in (Economic) Time Series",
        "comments": "12 pages, 6 figures",
        "journal-ref": null,
        "doi": "10.1007/978-3-319-64334-2_21",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Methods for detecting structural changes, or change points, in time series\ndata are widely used in many fields of science and engineering. This chapter\nsketches some basic methods for the analysis of structural changes in time\nseries data. The exposition is confined to retrospective methods for univariate\ntime series. Several recent methods for dating structural changes are compared\nusing a time series of oil prices spanning more than 60 years. The methods\nbroadly agree for the first part of the series up to the mid-1980s, for which\nchanges are associated with major historical events, but provide somewhat\ndifferent solutions thereafter, reflecting a gradual increase in oil prices\nthat is not well described by a step function. As a further illustration, 1990s\ndata on the volatility of the Hang Seng stock market index are reanalyzed.\n"
    },
    {
        "paper_id": 1702.07169,
        "authors": "Alexander M. G. Cox and Sam M. Kinsley",
        "title": "Robust Hedging of Options on a Leveraged Exchange Traded Fund",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  A leveraged exchange traded fund (LETF) is an exchange traded fund that uses\nfinancial derivatives to amplify the price changes of a basket of goods. In\nthis paper, we consider the robust hedging of European options on a LETF,\nfinding model-free bounds on the price of these options.\n  To obtain an upper bound, we establish a new optimal solution to the\nSkorokhod embedding problem (SEP) using methods introduced in\nBeiglb\\\"ock-Cox-Huesmann. This stopping time can be represented as the hitting\ntime of some region by a Brownian motion, but unlike other solutions of e.g.\nRoot, this region is not unique. Much of this paper is dedicated to\ncharacterising the choice of the embedding region that gives the required\noptimality property. Notably, this appears to be the first solution to the SEP\nwhere the solution is not uniquely characterised by its geometric structure,\nand an additional condition is needed on the stopping region to guarantee that\nit is the optimiser. An important part of determining the optimal region is\nidentifying the correct form of the dual solution, which has a financial\ninterpretation as a model-independent superhedging strategy.\n"
    },
    {
        "paper_id": 1702.07374,
        "authors": "Huai-Long Shi and Wei-Xing Zhou (ECUST)",
        "title": "Time series momentum and contrarian effects in the Chinese stock market",
        "comments": "11 Latex pages including 1 figure and 4 tables",
        "journal-ref": "Physica A 483, 309-318 (2017)",
        "doi": "10.1016/j.physa.2017.04.139",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper concentrates on the time series momentum or contrarian effects in\nthe Chinese stock market. We evaluate the performance of the time series\nmomentum strategy applied to major stock indices in mainland China and explore\nthe relation between the performance of time series momentum strategies and\nsome firm-specific characteristics. Our findings indicate that there is a time\nseries momentum effect in the short run and a contrarian effect in the long run\nin the Chinese stock market. The performances of the time series momentum and\ncontrarian strategies are highly dependent on the look-back and holding periods\nand firm-specific characteristics.\n"
    },
    {
        "paper_id": 1702.07423,
        "authors": "Li-Xin Zhong, Wen-Juan Xu, Yun-Xin He, Chen-Yang Zhong, Rong-Da Chen,\n  Tian Qiu, Yong-Dong Shi, Fei Ren",
        "title": "A generalized public goods game with coupling of individual ability and\n  project benefit",
        "comments": null,
        "journal-ref": null,
        "doi": "10.1016/j.chaos.2017.05.025",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Facing a heavy task, any single person can only make a limited contribution\nand team cooperation is needed. As one enjoys the benefit of the public goods,\nthe potential benefits of the project are not always maximized and may be\npartly wasted. By incorporating individual ability and project benefit into the\noriginal public goods game, we study the coupling effect of the four\nparameters, the upper limit of individual contribution, the upper limit of\nindividual benefit, the needed project cost and the upper limit of project\nbenefit on the evolution of cooperation. Coevolving with the individual-level\ngroup size preferences, an increase in the upper limit of individual benefit\npromotes cooperation while an increase in the upper limit of individual\ncontribution inhibits cooperation. The coupling of the upper limit of\nindividual contribution and the needed project cost determines the critical\npoint of the upper limit of project benefit, where the equilibrium frequency of\ncooperators reaches its highest level. Above the critical point, an increase in\nthe upper limit of project benefit inhibits cooperation. The evolution of\ncooperation is closely related to the preferred group-size distribution. A\nfunctional relation between the frequency of cooperators and the dominant group\nsize is found.\n"
    },
    {
        "paper_id": 1702.07556,
        "authors": "Takuji Arai and Yuto Imai",
        "title": "A closed-form representation of mean-variance hedging for additive\n  processes via Malliavin calculus",
        "comments": "21 pages, 2 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We focus on mean-variance hedging problem for models whose asset price\nfollows an exponential additive process. Some representations of mean-variance\nhedging strategies for jump type models have already been suggested, but none\nis suited to develop numerical methods of the values of strategies for any\ngiven time up to the maturity. In this paper, we aim to derive a new explicit\nclosed-form representation, which enables us to develop an efficient numerical\nmethod using the fast Fourier transforms. Note that our representation is\ndescribed in terms of Malliavin derivatives. In addition, we illustrate\nnumerical results for exponential L\\'evy models.\n"
    },
    {
        "paper_id": 1702.07786,
        "authors": "David Landriault and Bin Li and Hongzhong Zhang",
        "title": "A Unified Approach for Drawdown (Drawup) of Time-Homogeneous Markov\n  Processes",
        "comments": "24 pages, 2 figures",
        "journal-ref": null,
        "doi": "10.1017/jpr.2017.20",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Drawdown (resp. drawup) of a stochastic process, also referred as the\nreflected process at its supremum (resp. infimum), has wide applications in\nmany areas including financial risk management, actuarial mathematics and\nstatistics. In this paper, for general time-homogeneous Markov processes, we\nstudy the joint law of the first passage time of the drawdown (resp. drawup)\nprocess, its overshoot, and the maximum of the underlying process at this first\npassage time. By using short-time pathwise analysis, under some mild regularity\nconditions, the joint law of the three drawdown quantities is shown to be the\nunique solution to an integral equation which is expressed in terms of\nfundamental two-sided exit quantities of the underlying process. Explicit forms\nfor this joint law are found when the Markov process has only one-sided jumps\nor is a L\\'{e}vy process (possibly with two-sided jumps). The proposed\nmethodology provides a unified approach to study various drawdown quantities\nfor the general class of time-homogeneous Markov processes.\n"
    },
    {
        "paper_id": 1702.07936,
        "authors": "Zachary Feinstein",
        "title": "Obligations with Physical Delivery in a Multi-Layered Financial Network",
        "comments": "36 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper provides a general framework for modeling financial contagion in a\nsystem with obligations in multiple illiquid assets (e.g., currencies). In so\ndoing, we develop a multi-layered financial network that extends the single\nnetwork of Eisenberg and Noe (2001). In particular, we develop a financial\ncontagion model with fire sales that allows institutions to both buy and sell\nassets to cover their liabilities in the different assets and act as utility\nmaximizers.\n  We prove that, under standard assumptions and without market impacts,\nequilibrium portfolio holdings exist and are unique. However, with market\nimpacts, we prove that equilibrium portfolio holdings and market prices exist\nwhich clear the multi-layered financial system. In general, though, these\nclearing solutions are not unique. We extend this result by considering the\nt\\^atonnement process to find the unique attained equilibrium. The attained\nequilibrium need not be continuous with respect to the initial shock; these\npoints of discontinuity match those stresses in which a financial crisis\nbecomes a systemic crisis. We further provide mathematical formulations for\npayment rules and utility functions satisfying the necessary conditions for\nthese existence and uniqueness results.\n  We demonstrate the value of our model through illustrative numerical case\nstudies. In particular, we study a counterfactual scenario on the event that\nGreece re-instituted the drachma on a dataset from the European Banking\nAuthority.\n"
    },
    {
        "paper_id": 1702.08029,
        "authors": "Bence Toth, Zoltan Eisler, Jean-Philippe Bouchaud",
        "title": "The short-term price impact of trades is universal",
        "comments": "7 pages, 5 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We analyze a proprietary dataset of trades by a single asset manager,\ncomparing their price impact with that of the trades of the rest of the market.\nIn the context of a linear propagator model we find no significant difference\nbetween the two, suggesting that both the magnitude and time dependence of\nimpact are universal in anonymous, electronic markets. This result is important\nas optimal execution policies often rely on propagators calibrated on anonymous\ndata. We also find evidence that in the wake of a trade the order flow of other\nmarket participants first adds further copy-cat trades enhancing price impact\non very short time scales. The induced order flow then quickly inverts, thereby\ncontributing to impact decay.\n"
    },
    {
        "paper_id": 1702.08081,
        "authors": "Jiro Akahori, Xiaoming Song, and Tai-Ho Wang",
        "title": "Probability density of lognormal fractional SABR model",
        "comments": "33 pages, 3 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Instantaneous volatility of logarithmic return in the lognormal fractional\nSABR model is driven by the exponentiation of a correlated fractional Brownian\nmotion. Due to the mixed nature of driving Brownian and fractional Brownian\nmotions, probability density for such a model is less studied in the\nliterature. We show in this paper a bridge representation for the joint density\nof the lognormal fractional SABR model in a Fourier space. Evaluating the\nbridge representation along a properly chosen deterministic path yields a small\ntime asymptotic expansion to the leading order for the probability density of\nthe fractional SABR model. A direct generalization of the representation to\njoint density at multiple times leads to a heuristic derivation of the large\ndeviations principle for the joint density in small time. Approximation of\nimplied volatility is readily obtained by applying the Laplace asymptotic\nformula to the call or put prices and comparing coefficients.\n"
    },
    {
        "paper_id": 1702.08391,
        "authors": "Maria Letizia Bertotti, Amit K Chattopadhyay, Giovanni Modanese",
        "title": "Economic inequality and mobility for stochastic models with\n  multiplicative noise",
        "comments": "15 pages, 4 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this article, we discuss a dynamical stochastic model that represents the\ntime evolution of income distribution of a population, where the dynamics\ndevelop from an interplay of multiple economic exchanges in the presence of\nmultiplicative noise. The model remit stretches beyond the conventional\nframework of a Langevin-type kinetic equation in that our model dynamics is\nself-consistently constrained by dynamical conservation laws emerging from\npopulation and wealth conservation. This model is numerically solved and\nanalyzed to interpret the inequality of income as a function of relevant\ndynamical parameters like the {\\it mobility} $M$ and the {\\it total income}\n$\\mu$. In our model, inequality is quantified by the {\\it Gini index} $G$. In\nparticular, correlations between any two of the mobility index $M$ and/or the\ntotal income $\\mu$ with the Gini index $G$ are investigated and compared with\nthe analogous correlations resulting from an equivalent additive noise model.\nOur findings highlight the importance of a multiplicative noise based economic\nmodeling structure in the analysis of inequality. The model also depicts the\nnature of correlation between mobility and total income of a population from\nthe perspective of inequality measure.\n"
    },
    {
        "paper_id": 1702.08744,
        "authors": "Daniel Grigat, Fabio Caccioli",
        "title": "Reverse stress testing interbank networks",
        "comments": "19 pages, 9 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We reverse engineer dynamics of financial contagion to find the scenario of\nsmallest exogenous shock that, should it occur, would lead to a given final\nsystemic loss. This reverse stress test can be used to identify the potential\ntriggers of systemic events, and it removes the arbitrariness in the selection\nof shock scenarios in stress testing. We consider in particular the case of\ndistress propagation in an interbank market, and we study a network of 44\nEuropean banks, which we reconstruct using data collected from Bloomberg. By\nlooking at the distribution across banks of the size of smallest exogenous\nshocks we rank banks in terms of their systemic importance, and we show the\neffectiveness of a policy with capital requirements based on this ranking. We\nalso study the properties of smallest exogenous shocks as a function of the\nlargest eigenvalue $\\lambda_{\\rm max}$ of the matrix of interbank leverages,\nwhich determines the endogenous amplification of shocks. We find that the size\nof smallest exogenous shocks reduces and that the distribution across banks\nbecomes more localized as $\\lambda_{\\rm max}$ increases.\n"
    },
    {
        "paper_id": 1702.08774,
        "authors": "Yuri Biondi, Feng Zhou",
        "title": "Interbank Credit and the Money Manufacturing Process. A Systemic\n  Perspective on Financial Stability",
        "comments": null,
        "journal-ref": "Journal of Economic Interaction and Coordination (2018)",
        "doi": "10.1007/s11403-018-0230-y",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Interbank lending and borrowing occur when financial institutions seek to\nsettle and refinance their mutual positions over time and circumstances. This\ninteractive process involves money creation at the aggregate level.\nCoordination mismatch on interbank credit may trigger systemic crises. This\nhappened when, since summer 2007, interbank credit coordination did not longer\nwork smoothly across financial institutions, eventually requiring exceptional\nmonetary policies by central banks, and guarantee and bailout interventions by\ngovernments. Our article develops an interacting heterogeneous agents-based\nmodel of interbank credit coordination under minimal institutions. First, we\nexplore the link between interbank credit coordination and the money generation\nprocess. Contrary to received understanding, interbank credit has the capacity\nto make the monetary system unbound. Second, we develop simulation analysis on\nimperfect interbank credit coordination, studying impact of interbank dynamics\non financial stability and resilience at individual and aggregate levels.\nSystemically destabilizing forces prove to be related to the working of the\nbanking system over time, especially interbank coordination conditions and\ncircumstances.\n"
    },
    {
        "paper_id": 1702.08867,
        "authors": "Greig Smith and Goncalo dos Reis",
        "title": "Robust and Consistent Estimation of Generators in Credit Risk",
        "comments": "29 pages, 7 Figures, 9 tables",
        "journal-ref": null,
        "doi": "10.1080/14697688.2017.1383627",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Bond rating Transition Probability Matrices (TPMs) are built over a one-year\ntime-frame and for many practical purposes, like the assessment of risk in\nportfolios or the computation of banking Capital Requirements (e.g. the new\nIFRS 9 regulation), one needs to compute the TPM and probabilities of default\nover a smaller time interval. In the context of continuous time Markov chains\n(CTMC) several deterministic and statistical algorithms have been proposed to\nestimate the generator matrix. We focus on the Expectation-Maximization (EM)\nalgorithm by Bladt and Sorensen (2005) for a CTMC with an absorbing state for\nsuch estimation.\n  This work's contribution is threefold. Firstly, we provide directly\ncomputable closed-form expressions for quantities appearing in the EM algorithm\nand associated information matrix, allowing to easily approximate confidence\nintervals. Previously, these quantities had to be estimated numerically and\nconsiderable computational speedups have been gained. Secondly, we prove\nconvergence to a single set of parameters under very weak conditions (for the\nTPM problem). Finally, we provide a numerical benchmark of our results against\nother known algorithms, in particular, on several problems related to credit\nrisk. The EM algorithm we propose, padded with the new formulas (and error\ncriteria), outperforms other known algorithms in several metrics, in\nparticular, with much less overestimation of probabilities of default in higher\nratings than other statistical algorithms.\n"
    },
    {
        "paper_id": 1702.08901,
        "authors": "Stefan Weber",
        "title": "Solvency II, or How to Sweep the Downside Risk Under the Carpet",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Under Solvency II the computation of capital requirements is based on value\nat risk (V@R). V@R is a quantile-based risk measure and neglects extreme risks\nin the tail. V@R belongs to the family of distortion risk measures. A serious\ndeficiency of V@R is that firms can hide their total downside risk in corporate\nnetworks, unless a consolidated solvency balance sheet is required for each\neconomic scenario. In this case, they can largely reduce their total capital\nrequirements via appropriate transfer agreements within a network structure\nconsisting of sufficiently many entities and thereby circumvent capital\nregulation. We prove several versions of such a result for general distortion\nrisk measures of V@R-type, explicitly construct suitable allocations of the\nnetwork portfolio, and finally demonstrate how these findings can be extended\nbeyond distortion risk measures. We also discuss why consolidation requirements\ncannot completely eliminate this problem. Capital regulation should thus be\nbased on coherent or convex risk measures like average value at risk or\nexpectiles.\n"
    },
    {
        "paper_id": 1703.00062,
        "authors": "Tetsuya Ishikawa, Scott Robertson",
        "title": "Optimal Investment and Pricing in the Presence of Defaults",
        "comments": "2 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We consider the optimal investment problem when the traded asset may default,\ncausing a jump in its price. For an investor with constant absolute risk\naversion, we compute indifference prices for defaultable bonds, as well as a\nprice for dynamic protection against default. For the latter problem, our work\ncomplements Sircar & Zariphopoulou (2007), where it is implicitly assumed the\ninvestor is protected against default. We consider a factor model where the\nasset's instantaneous return, variance, correlation and default intensity are\ndriven by a time-homogenous diffusion X taking values in an arbitrary region E.\nWe identify the certainty equivalent with a semi-linear degenerate elliptic\npartial differential equation with quadratic growth in both function and\ngradient. Under a minimal integrability assumption on the market price of risk,\nwe show the certainty equivalent is a classical solution. In particular, our\nresults cover when X is a one-dimensional affine diffusion and when returns,\nvariances and default intensities are also affine. Numerical examples highlight\nthe relationship between the factor process and both the indifference price and\ndefault insurance. Lastly, we show the insurance protection price is not the\ndefault intensity under the dual optimal measure.\n"
    },
    {
        "paper_id": 1703.00182,
        "authors": "Daniel Kressner, Robert Luce, Francesco Statti",
        "title": "Incremental computation of block triangular matrix exponentials with\n  application to option pricing",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We study the problem of computing the matrix exponential of a block\ntriangular matrix in a peculiar way: Block column by block column, from left to\nright. The need for such an evaluation scheme arises naturally in the context\nof option pricing in polynomial diffusion models. In this setting a\ndiscretization process produces a sequence of nested block triangular matrices,\nand their exponentials are to be computed at each stage, until a dynamically\nevaluated criterion allows to stop. Our algorithm is based on scaling and\nsquaring. By carefully reusing certain intermediate quantities from one step to\nthe next, we can efficiently compute such a sequence of matrix exponentials.\n"
    },
    {
        "paper_id": 1703.00259,
        "authors": "Junbeom Lee, Chao Zhou",
        "title": "Binary Funding Impacts in Derivative Valuation",
        "comments": "30 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  We discuss the binary nature of funding impact in derivative valuation. Under\nsome conditions, funding is either a cost or a benefit, i.e., one of the\nlending/borrowing rates does not play a role in pricing derivatives. When\nderivatives are priced, considering different lending/borrowing rates leads to\nsemi-linear BSDEs and PDEs, and thus it is necessary to solve the equations\nnumerically. However, once it can be guaranteed that only one of the rates\naffects pricing, linear equations can be recovered and analytical formulae can\nbe derived. Moreover, as a byproduct, our results explain how debt value\nadjustment (DVA) and funding benefits are dissimilar. It is often believed that\nconsidering both DVA and funding benefits results in a double-counting issue\nbut it will be shown that the two components are affected by different\nmathematical structures of derivative transactions. We find that funding\nbenefit is related to the decreasing property of the payoff function, but this\nrelationship decreases as the funding choices of underlying assets are\ntransferred to repo markets.\n"
    },
    {
        "paper_id": 1703.00308,
        "authors": "Jamal Bouoiyour (CATT), Refk Selmi (CATT)",
        "title": "Are Trump and Bitcoin Good Partners?",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  During times of extreme market turmoil, it is acknowledged that there is a\ntendency towards \"flight to safety\". A strong (weak) safe haven is defined as\nan asset that has a significant positive (negative) return in periods where\nanother asset is in distress, while hedge has to be negatively correlated\n(uncorrelated) on average. The Bitcoin's surge alongside the aftermath of\nTrump's win in the 2016 U.S. presidential elections has strengthened its status\nas the modern safe haven. This paper uses a truly noise-assisted data analysis\nmethod, termed as Ensemble Empirical Mode Decomposition-based approach, to\nexamine whether Bitcoin can act as a hedge and safe haven for U.S. stock price\nindex. The results document that the Bitcoin's safe-haven property is\ntime-varying and that it has primarily been a weak safe haven in the short term\nand the long-term. We also demonstrate that precious metals lost their safe\nhaven properties over time as the correlation between gold/silver and U.S.\nstock price declines from short-to long-run horizons.\n"
    },
    {
        "paper_id": 1703.00476,
        "authors": "Andreas Hermes and Stanislaus Maier-Paape",
        "title": "Existence and Uniqueness for the Multivariate Discrete Terminal Wealth\n  Relative",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper the multivariate fractional trading ansatz of money management\nfrom Ralph Vince (Portfolio Management Formulas: Mathematical Trading Methods\nfor the Futures, Options, and Stock Markets, John Wiley & Sons, Inc., 1990) is\ndiscussed. In particular, we prove existence and uniqueness of an optimal f of\nthe respective optimization problem under reasonable assumptions on the trade\nreturn matrix. This result generalizes a similar result for the univariate\nfractional trading ansatz. Furthermore, our result guarantees that the\nmultivariate optimal f solutions can always be found numerically by steepest\nascent methods.\n"
    },
    {
        "paper_id": 1703.00485,
        "authors": "Gautier Marti, Frank Nielsen, Miko{\\l}aj Bi\\'nkowski, Philippe Donnat",
        "title": "A review of two decades of correlations, hierarchies, networks and\n  clustering in financial markets",
        "comments": null,
        "journal-ref": "Chapter in Progress in Information Geometry: Theory and\n  Applications, 245-274, 2021",
        "doi": "10.1007/978-3-030-65459-7",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We review the state of the art of clustering financial time series and the\nstudy of their correlations alongside other interaction networks. The aim of\nthis review is to gather in one place the relevant material from different\nfields, e.g. machine learning, information geometry, econophysics, statistical\nphysics, econometrics, behavioral finance. We hope it will help researchers to\nuse more effectively this alternative modeling of the financial time series.\nDecision makers and quantitative researchers may also be able to leverage its\ninsights. Finally, we also hope that this review will form the basis of an open\ntoolbox to study correlations, hierarchies, networks and clustering in\nfinancial markets.\n"
    },
    {
        "paper_id": 1703.00703,
        "authors": "Zura Kakushadze and Willie Yu",
        "title": "*K-means and Cluster Models for Cancer Signatures",
        "comments": "124 pages, 69 figures; a trivial typo corrected; to appear in\n  Biomolecular Detection and Quantification",
        "journal-ref": "Biomolecular Detection and Quantification 13 (2017) 7-31",
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We present *K-means clustering algorithm and source code by expanding\nstatistical clustering methods applied in https://ssrn.com/abstract=2802753 to\nquantitative finance. *K-means is statistically deterministic without\nspecifying initial centers, etc. We apply *K-means to extracting cancer\nsignatures from genome data without using nonnegative matrix factorization\n(NMF). *K-means' computational cost is a fraction of NMF's. Using 1,389\npublished samples for 14 cancer types, we find that 3 cancers (liver cancer,\nlung cancer and renal cell carcinoma) stand out and do not have cluster-like\nstructures. Two clusters have especially high within-cluster correlations with\n11 other cancers indicating common underlying structures. Our approach opens a\nnovel avenue for studying such structures. *K-means is universal and can be\napplied in other fields. We discuss some potential applications in quantitative\nfinance.\n"
    },
    {
        "paper_id": 1703.00918,
        "authors": "Piotr Jaworski and Marcin Pitera",
        "title": "A note on conditional covariance matrices for elliptical distributions",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this short note we provide an analytical formula for the conditional\ncovariance matrices of the elliptically distributed random vectors, when the\nconditioning is based on the values of any linear combination of the marginal\nrandom variables. We show that one could introduce the univariate invariant\ndepending solely on the conditioning set, which greatly simplifies the\ncalculations. As an application, we show that one could define uniquely defined\nquantile-based sets on which conditional covariance matrices must be equal to\neach other if only the vector is multivariate normal. The similar results are\nobtained for conditional correlation matrices of the general elliptic case.\n"
    },
    {
        "paper_id": 1703.00923,
        "authors": "Jorge Inigo",
        "title": "Pricing of Mexican Interest Rate Swaps in Presence of Multiple\n  Collateral Currencies",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The financial crisis of 2007/08 caused catastrophic consequences and brought\na bunch of changes around the world. Interest rates that were known to follow\nor behave similarly of each other diverged. Furthermore, the regulation and in\nparticular the counterparty credit risk began to to be considered and\nquantified. Consequently, pre-crisis models are no longer valid. Indeed, this\nwork sets the basis to define a valid model that considers the post-crisis\nworld assumptions for the Mexican swap market. The model used in this work was\nthe proposed by Fujii, Shimada and Takahashi in [Fujii et. al., 2010b]. This\nmodel allow us to value interest rate derivatives and future cash flows with\nthe existence of a collateral agreement (with a collateral currency). In this\ndocument we build the discounting and projection curves for MXN interest rate\nderivatives considering the collateral currencies: USD, EUR and MXN. Also, we\npresent the pricing when the derivative is uncollateralized. Finally, we show\nthe effect of the cross-currency swaps when valuing through different\ncollateral currencies.\n"
    },
    {
        "paper_id": 1703.00957,
        "authors": "Stefano De Marco, Claude Martini",
        "title": "Moment generating functions and Normalized implied volatilities:\n  unification and extension via Fukasawa's pricing formula",
        "comments": "27 pages, 4 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We extend the model-free formula of [Fukasawa 2012] for $\\mathbb\nE[\\Psi(X_T)]$, where $X_T=\\log S_T/F$ is the log-price of an asset, to\nfunctions $\\Psi$ of exponential growth. The resulting integral representation\nis written in terms of normalized implied volatilities. Just as Fukasawa's work\nprovides rigourous ground for Chriss and Morokoff's (1999) model-free formula\nfor the log-contract (related to the Variance swap implied variance), we prove\nan expression for the moment generating function $\\mathbb E[e^{p X_T}]$ on its\nanalyticity domain, that encompasses (and extends) Matytsin's formula [Matytsin\n2000] for the characteristic function $\\mathbb E[e^{i \\eta X_T}]$ and Bergomi's\nformula [Bergomi 2016] for $\\mathbb E[e^{p X_T}]$, $p \\in [0,1]$. Besides, we\n(i) show that put-call duality transforms the first normalized implied\nvolatility into the second, and (ii) analyze the invertibility of the extended\ntransformation $d(p,\\cdot) = p \\, d_1 + (1-p)d_2$ when $p$ lies outside\n$[0,1]$. As an application of (i), one can generate representations for the MGF\n(or other payoffs) by switching between one normalized implied volatility and\nthe other.\n"
    },
    {
        "paper_id": 1703.01137,
        "authors": "Felix-Benedikt Liebrich, Gregor Svindland",
        "title": "Model Spaces for Risk Measures",
        "comments": "Preprint version",
        "journal-ref": "Insurance: Mathematics and Economics, Vol. 77, pp. 150-165 (2017)",
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We show how risk measures originally defined in a model free framework in\nterms of acceptance sets and reference assets imply a meaningful underlying\nprobability structure. Hereafter we construct a maximal domain of definition of\nthe risk measure respecting the underlying ambiguity profile. We particularly\nemphasise liquidity effects and discuss the correspondence between properties\nof the risk measure and the structure of this domain as well as\nsubdifferentiability properties.\n  Keywords: Model free risk assessment, extension of risk measures, continuity\nproperties of risk measures, subgradients.\n"
    },
    {
        "paper_id": 1703.01291,
        "authors": "Hiroshi Toyoizumi",
        "title": "Swarm behavior of traders with different subjective predictions in the\n  Market",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  A combination of a priority queueing model and mean field theory shows the\nemergence of traders' swarm behavior, even when each has a subjective\nprediction of the market driven by a limit order book. Using a nonlinear Markov\nmodel, we analyze the dynamics of traders who select a favorable order price\ntaking into account the waiting cost incurred by others. We find swarm behavior\nemerges because of the delay in trader reactions to the market, and the\ndirection of the swarm is decided by the current market position and the\nintensity of zero-intelligent random behavior, rather than subjective trader\npredictions.\n"
    },
    {
        "paper_id": 1703.01292,
        "authors": "Jian Gao, Tao Zhou",
        "title": "Quantifying China's Regional Economic Complexity",
        "comments": "14 pages, 6 figures, 1 table",
        "journal-ref": "Physica A 492 (2018) 1591-1603",
        "doi": "10.1016/j.physa.2017.11.084",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  China has experienced an outstanding economic expansion during the past\ndecades, however, literature on non-monetary metrics that reveal the status of\nChina's regional economic development are still lacking. In this paper, we fill\nthis gap by quantifying the economic complexity of China's provinces through\nanalyzing 25 years' firm data. First, we estimate the regional economic\ncomplexity index (ECI), and show that the overall time evolution of provinces'\nECI is relatively stable and slow. Then, after linking ECI to the economic\ndevelopment and the income inequality, we find that the explanatory power of\nECI is positive for the former but negative for the latter. Next, we compare\ndifferent measures of economic diversity and explore their relationships with\nmonetary macroeconomic indicators. Results show that the ECI index and the\nnon-linear iteration based Fitness index are comparative, and they both have\nstronger explanatory power than other benchmark measures. Further multivariate\nregressions suggest the robustness of our results after controlling other\nsocioeconomic factors. Our work moves forward a step towards better\nunderstanding China's regional economic development and non-monetary\nmacroeconomic indicators.\n"
    },
    {
        "paper_id": 1703.01329,
        "authors": "Marco Frittelli, Marco Maggis",
        "title": "Disentangling Price, Risk and Model Risk: V&R measures",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We propose a method to assess the intrinsic risk carried by a financial\nposition $X$ when the agent faces uncertainty about the pricing rule assigning\nits present value. Our approach is inspired by a new interpretation of the\nquasiconvex duality in a Knightian setting, where a family of probability\nmeasures replaces the single reference probability and is then applied to value\nfinancial positions.\n  Diametrically, our construction of Value\\&Risk measures is based on the\nselection of a basket of claims to test the reliability of models. We compare a\nrandom payoff $X$ with a given class of derivatives written on $X$ , and use\nthese derivatives to \\textquotedblleft test\\textquotedblright\\ the pricing\nmeasures.\n  We further introduce and study a general class of Value\\&Risk measures $%\nR(p,X,\\mathbb{P})$ that describes the additional capital that is required to\nmake $X$ acceptable under a probability $\\mathbb{P}$ and given the initial\nprice $p$ paid to acquire $X$.\n"
    },
    {
        "paper_id": 1703.01369,
        "authors": "Jian Gao, Bogang Jun, Alex \"Sandy\" Pentland, Tao Zhou, Cesar A.\n  Hidalgo",
        "title": "Collective Learning in China's Regional Economic Development",
        "comments": "29 pages, 9 figures, 4 tables",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Industrial development is the process by which economies learn how to produce\nnew products and services. But how do economies learn? And who do they learn\nfrom? The literature on economic geography and economic development has\nemphasized two learning channels: inter-industry learning, which involves\nlearning from related industries; and inter-regional learning, which involves\nlearning from neighboring regions. Here we use 25 years of data describing the\nevolution of China's economy between 1990 and 2015--a period when China\nmultiplied its GDP per capita by a factor of ten--to explore how Chinese\nprovinces diversified their economies. First, we show that the probability that\na province will develop a new industry increases with the number of related\nindustries that are already present in that province, a fact that is suggestive\nof inter-industry learning. Also, we show that the probability that a province\nwill develop an industry increases with the number of neighboring provinces\nthat are developed in that industry, a fact suggestive of inter-regional\nlearning. Moreover, we find that the combination of these two channels exhibit\ndiminishing returns, meaning that the contribution of either of these learning\nchannels is redundant when the other one is present. Finally, we address\nendogeneity concerns by using the introduction of high-speed rail as an\ninstrument to isolate the effects of inter-regional learning. Our\ndifferences-in-differences (DID) analysis reveals that the introduction of high\nspeed-rail increased the industrial similarity of pairs of provinces connected\nby high-speed rail. Also, industries in provinces that were connected by rail\nincreased their productivity when they were connected by rail to other\nprovinces where that industry was already present. These findings suggest that\ninter-regional and inter-industry learning played a role in China's great\neconomic expansion.\n"
    },
    {
        "paper_id": 1703.01505,
        "authors": "Alexander Lipton",
        "title": "Blockchains and Distributed Ledgers in Retrospective and Perspective",
        "comments": "Accepted for publication in the special issue \"Digital currencies\" of\n  the Journal of Risk Finance, 27 pages, 9 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We introduce blockchains and distributed ledgers and describe their potential\napplications to money and banking. The analysis compares public and private\nledgers and outlines the suitability of various types of ledgers for different\npurposes. Furthermore, a few historical prototypes of blockchains and\ndistributed ledgers are presented, and results of their hard forking are\nillustrated. Next, some potential applications of distributed ledgers to\ntrading, clearing and settlement, payments, trade finance, etc. are outlined.\nMonetary circuits are argued to be natural applications for blockchains.\nFinally, the role of digital currencies in modern society is articulated and\nvarious forms of digital cash, such as central bank issued electronic cash,\nbank money, bitcoin and P2P money, are compared and contrasted.\n  Keywords: blockchains, distributed ledgers, digital currencies, modern\nmonetary circuit; credit creation banking; interconnected banking network.\n"
    },
    {
        "paper_id": 1703.01574,
        "authors": "Dmitry Muravey",
        "title": "Optimal investment problem with M-CEV model: closed form solution and\n  applications to the algorithmic trading",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper studies an optimal investment problem under M-CEV with power\nutility function. Using Laplace transform we obtain explicit expression for\noptimal strategy in terms of confluent hypergeometric functions. For obtained\nrepresentations we derive asymptotic and approximation formulas contains only\nelementary functions and continued fractions. These formulas allow to make\nanalysis of impact of model's parameters and effects of parameters\nmisspecification. In addition we propose some extensions of obtained results\nthat can be applicable for algorithmic strategies.\n"
    },
    {
        "paper_id": 1703.01984,
        "authors": "Danping Li, Dongchen Li, Virginia R. Young",
        "title": "Optimality of Excess-Loss Reinsurance under a Mean-Variance Criterion",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper, we study an insurer's reinsurance-investment problem under a\nmean-variance criterion. We show that excess-loss is the unique equilibrium\nreinsurance strategy under a spectrally negative L\\'{e}vy insurance model when\nthe reinsurance premium is computed according to the expected value premium\nprinciple. Furthermore, we obtain the explicit equilibrium\nreinsurance-investment strategy by solving the extended Hamilton-Jacobi-Bellman\nequation.\n"
    },
    {
        "paper_id": 1703.01989,
        "authors": "Kevin Primicerio, Damien Challet, Stanislao Gualdi",
        "title": "Wisdom of the institutional crowd",
        "comments": "11 pages, 12 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The average portfolio structure of institutional investors is shown to have\nproperties which account for transaction costs in an optimal way. This implies\nthat financial institutions unknowingly display collective rationality, or\nWisdom of the Crowd. Individual deviations from the rational benchmark are\nample, which illustrates that system-wide rationality does not need nearly\nrational individuals. Finally we discuss the importance of accounting for\nconstraints when assessing the presence of Wisdom of the Crowd.\n"
    },
    {
        "paper_id": 1703.02104,
        "authors": "Francois Lafond and Daniel Kim",
        "title": "Long-run dynamics of the U.S. patent classification system",
        "comments": null,
        "journal-ref": null,
        "doi": "10.1007/s00191-018-0603-3",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Almost by definition, radical innovations create a need to revise existing\nclassification systems. In this paper, we argue that classification system\nchanges and patent reclassification are common and reveal interesting\ninformation about technological evolution. To support our argument, we present\nthree sets of findings regarding classification volatility in the U.S. patent\nclassification system. First, we study the evolution of the number of distinct\nclasses. Reconstructed time series based on the current classification scheme\nare very different from historical data. This suggests that using the current\nclassification to analyze the past produces a distorted view of the evolution\nof the system. Second, we study the relative sizes of classes. The size\ndistribution is exponential so classes are of quite different sizes, but the\nlargest classes are not necessarily the oldest. To explain this pattern with a\nsimple stochastic growth model, we introduce the assumption that classes have a\nregular chance to be split. Third, we study reclassification. The share of\npatents that are in a different class now than they were at birth can be quite\nhigh. Reclassification mostly occurs across classes belonging to the same\n1-digit NBER category, but not always. We also document that reclassified\npatents tend to be more cited than non-reclassified ones, even after\ncontrolling for grant year and class of origin.\n"
    },
    {
        "paper_id": 1703.02105,
        "authors": "Krishna Dasaratha, Kevin He",
        "title": "Network Structure and Naive Sequential Learning",
        "comments": null,
        "journal-ref": "Theoretical Economics 15(2):415-444, 2020",
        "doi": "10.3982/TE3388",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We study a sequential-learning model featuring a network of naive agents with\nGaussian information structures. Agents apply a heuristic rule to aggregate\npredecessors' actions. They weigh these actions according the strengths of\ntheir social connections to different predecessors. We show this rule arises\nendogenously when agents wrongly believe others act solely on private\ninformation and thus neglect redundancies among observations. We provide a\nsimple linear formula expressing agents' actions in terms of network paths and\nuse this formula to characterize the set of networks where naive agents\neventually learn correctly. This characterization implies that, on all networks\nwhere later agents observe more than one neighbor, there exist\ndisproportionately influential early agents who can cause herding on incorrect\nactions. Going beyond existing social-learning results, we compute the\nprobability of such mislearning exactly. This allows us to compare likelihoods\nof incorrect herding, and hence expected welfare losses, across network\nstructures. The probability of mislearning increases when link densities are\nhigher and when networks are more integrated. In partially segregated networks,\ndivergent early signals can lead to persistent disagreement between groups.\n"
    },
    {
        "paper_id": 1703.02311,
        "authors": "S\\'ebastien Geeraert, Charles-Albert Lehalle, Barak Pearlmutter,\n  Olivier Pironneau (LJLL), Adil Reghai",
        "title": "Mini-symposium on automatic differentiation and its applications in the\n  financial industry",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Automatic differentiation is involved for long in applied mathematics as an\nalternative to finite difference to improve the accuracy of numerical\ncomputation of derivatives. Each time a numerical minimization is involved,\nautomatic differentiation can be used. In between formal derivation and\nstandard numerical schemes, this approach is based on software solutions\napplying mechanically the chain rule to obtain an exact value for the desired\nderivative. It has a cost in memory and cpu consumption. For participants of\nfinancial markets (banks, insurances, financial intermediaries, etc), computing\nderivatives is needed to obtain the sensitivity of its exposure to well-defined\npotential market moves. It is a way to understand variations of their balance\nsheets in specific cases. Since the 2008 crisis, regulation demand to compute\nthis kind of exposure to many different case, to be sure market participants\nare aware and ready to face a wide spectrum of configurations. This paper shows\nhow automatic differentiation provides a partial answer to this recent\nexplosion of computation to perform. One part of the answer is a\nstraightforward application of Adjoint Algorithmic Differentiation (AAD), but\nit is not enough. Since financial sensitivities involves specific functions and\nmix differentiation with Monte-Carlo simulations, dedicated tools and\nassociated theoretical results are needed. We give here short introductions to\ntypical cases arising when one use AAD on financial markets.\n"
    },
    {
        "paper_id": 1703.02694,
        "authors": "Samuel Drapeau, Peng Luo, Dewen Xiong",
        "title": "Characterization of Fully Coupled FBSDE in Terms of Portfolio\n  Optimization",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We provide a verification and characterization result of optimal maximal\nsub-solutions of BSDEs in terms of fully coupled forward backward stochastic\ndifferential equations. We illustrate the application thereof in utility\noptimization with random endowment under probability and discounting\nuncertainty. We show with explicit examples how to quantify the costs of\nincompleteness when using utility indifference pricing, as well as a way to\nfind optimal solutions for recursive utilities.\n"
    },
    {
        "paper_id": 1703.02715,
        "authors": "Li Guo and Lin Peng and Yubo Tao and Jun Tu",
        "title": "Joint News, Attention Spillover,and Market Returns",
        "comments": "68 pages, 3 figures, 17 tables",
        "journal-ref": null,
        "doi": "10.2139/ssrn.2927561",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We analyze over 2.6 million news articles and propose a novel measure of\naggregate joint news coverage of firms. The measure strongly and negatively\npredicts market returns, both in sample and out of sample. The relation is\ncausal, robust to existing predictors, and is especially strong when market\nuncertainty is high or when market frictions are large. Using data on EDGAR\ndownloads by unique IPs, we provide direct evidence that joint news triggers\nattention spillover across firms. Our results are consistent with the\nexplanation that joint news generates a contagion in investor attention and\ncauses marketwide overvaluations and subsequent reversals.\n"
    },
    {
        "paper_id": 1703.02777,
        "authors": "Takashi Shinzato",
        "title": "Pythagorean theorem of Sharpe ratio",
        "comments": "13 pages, 1 figure",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In the present paper, using a replica analysis, we examine the portfolio\noptimization problem handled in previous work and discuss the minimization of\ninvestment risk under constraints of budget and expected return for the case\nthat the distribution of the hyperparameters of the mean and variance of the\nreturn rate of each asset are not limited to a specific probability family.\nFindings derived using our proposed method are compared with those in previous\nwork to verify the effectiveness of our proposed method. Further, we derive a\nPythagorean theorem of the Sharpe ratio and macroscopic relations of\nopportunity loss. Using numerical experiments, the effectiveness of our\nproposed method is demonstrated for a specific situation.\n"
    },
    {
        "paper_id": 1703.02865,
        "authors": "Johannes Preiser-Kapeller",
        "title": "Networks as Proxies: a relational approach towards economic complexity\n  in the Roman period",
        "comments": "To be published in: Koen Verboven - Jeroen Poblome (eds.), Structure\n  and Performance in the Roman Economy: Complexity Economics. Finding a New\n  Approach to Ancient Proxy Data (forthcoming)",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Based on the assumption that economic complexity is characterised by the\ninteractions of economic agents (who) constantly change their actions and\nstrategies in response to the outcome they mutually create, this paper presents\nhow network models can be used a proxies for the mapping, quantification and\nanalysis of Roman economic complexity. Network analysis provides tools to\nvisualise and analyse the inherent complexity of various types of data and\ntheir combination (archaeological, geographical, textual) or even of a single\npiece of evidence. Equally, the relational approach invites to a structural and\nquantitative comparison between periods, regions and the economic systems of\npolities and empires. An increasing number of proxies of this kind may allow us\nto capture the trajectories of economic complexity beyond metaphors.\n"
    },
    {
        "paper_id": 1703.03016,
        "authors": "Javier Garcia-Bernardo, Jan Fichtner, Eelke M. Heemskerk and Frank W.\n  Takes",
        "title": "Uncovering Offshore Financial Centers: Conduits and Sinks in the Global\n  Corporate Ownership Network",
        "comments": null,
        "journal-ref": null,
        "doi": "10.1038/s41598-017-06322-9",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Multinational corporations use highly complex structures of parents and\nsubsidiaries to organize their operations and ownership. Offshore Financial\nCenters (OFCs) facilitate these structures through low taxation and lenient\nregulation, but are increasingly under scrutiny, for instance for enabling tax\navoidance. Therefore, the identification of OFC jurisdictions has become a\npoliticized and contested issue. We introduce a novel data-driven approach for\nidentifying OFCs based on the global corporate ownership network, in which over\n98 million firms (nodes) are connected through 71 million ownership relations.\nThis granular firm-level network data uniquely allows identifying both\nsink-OFCs and conduit-OFCs. Sink-OFCs attract and retain foreign capital while\nconduit-OFCs are attractive intermediate destinations in the routing of\ninternational investments and enable the transfer of capital without taxation.\nWe identify 24 sink-OFCs. In addition, a small set of five countries -- the\nNetherlands, the United Kingdom, Ireland, Singapore and Switzerland -- canalize\nthe majority of corporate offshore investment as conduit-OFCs. Each conduit\njurisdiction is specialized in a geographical area and there is significant\nspecialization based on industrial sectors. Against the idea of OFCs as exotic\nsmall islands that cannot be regulated, we show that many sink and conduit-OFCs\nare highly developed countries.\n"
    },
    {
        "paper_id": 1703.03195,
        "authors": "Joaquim Clara-Rahola, Antonio M. Puertas, Miguel Angel\n  Sanchez-Granero, Juan E. Trinidad-Segovia and F.Javier de las Nieves",
        "title": "Diffusive and arrested-like dynamics in currency exchange markets",
        "comments": "6 pages, 4 figures",
        "journal-ref": "Physical Review Letters 118, 068301 (2017)",
        "doi": "10.1103/PhysRevLett.118.068301",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This work studies the symmetry between colloidal dynamics and the dynamics of\nthe Euro--US Dollar currency exchange market (EURUSD). We consider the EURUSD\nprice in the time range between 2001 and 2015, where we find significant\nqualitative symmetry between fluctuation distributions from this market and the\nones belonging to colloidal particles in supercooled or arrested states. In\nparticular, we find that models used for arrested physical systems are suitable\nfor describing the EURUSD fluctuation distributions. Whereas the corresponding\nmean squared price displacement (MSPD) to the EURUSD is diffusive for all\nyears, when focusing in selected time frames within a day, we find a two-step\nMSPD when the New York Stock Exchange market closes, comparable to the dynamics\nin supercooled systems. This is corroborated by looking at the price\ncorrelation functions and non-Gaussian parameters, and can be described by the\ntheoretical model. We discuss the origin and implications of this analogy.\n"
    },
    {
        "paper_id": 1703.03638,
        "authors": "Saul Jacka, Seb Armstrong, Abdelkarem Berkaoui",
        "title": "On representing and hedging claims for coherent risk measures",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We provide a dual characterisation of the weak$^*$-closure of a finite sum of\ncones in $L^\\infty$ adapted to a discrete time filtration $\\mathcal{F}_t$: the\n$t^{th}$ cone in the sum contains bounded random variables that are\n$\\mathcal{F}_t$-measurable. Hence we obtain a generalisation of Delbaen's\nm-stability condition for the problem of reserving in a collection of\nnum\\'eraires $\\mathbf{V}$, called $\\mathbf{V}$-m-stability, provided these\ncones arise from acceptance sets of a dynamic coherent measure of risk. We also\nprove that $\\mathbf{V}$-m-stability is equivalent to time-consistency when\nreserving in portfolios of $\\mathbf{V}$, which is of particular interest to\ninsurers.\n"
    },
    {
        "paper_id": 1703.04385,
        "authors": "Marian Gidea and Yuri Katz",
        "title": "Topological Data Analysis of Financial Time Series: Landscapes of\n  Crashes",
        "comments": null,
        "journal-ref": null,
        "doi": "10.1016/j.physa.2017.09.028",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We explore the evolution of daily returns of four major US stock market\nindices during the technology crash of 2000, and the financial crisis of\n2007-2009. Our methodology is based on topological data analysis (TDA). We use\npersistence homology to detect and quantify topological patterns that appear in\nmultidimensional time series. Using a sliding window, we extract time-dependent\npoint cloud data sets, to which we associate a topological space. We detect\ntransient loops that appear in this space, and we measure their persistence.\nThis is encoded in real-valued functions referred to as a 'persistence\nlandscapes'. We quantify the temporal changes in persistence landscapes via\ntheir $L^p$-norms. We test this procedure on multidimensional time series\ngenerated by various non-linear and non-equilibrium models. We find that, in\nthe vicinity of financial meltdowns, the $L^p$-norms exhibit strong growth\nprior to the primary peak, which ascends during a crash. Remarkably, the\naverage spectral density at low frequencies of the time series of $L^p$-norms\nof the persistence landscapes demonstrates a strong rising trend for 250\ntrading days prior to either dotcom crash on 03/10/2000, or to the Lehman\nbankruptcy on 09/15/2008. Our study suggests that TDA provides a new type of\neconometric analysis, which goes beyond the standard statistical measures. The\nmethod can be used to detect early warning signals of imminent market crashes.\nWe believe that this approach can be used beyond the analysis of financial time\nseries presented here.\n"
    },
    {
        "paper_id": 1703.04423,
        "authors": "Nicole B\\\"auerle and Stefanie Grether",
        "title": "Extremal Behavior of Long-Term Investors with Power Utility",
        "comments": null,
        "journal-ref": "International Journal of Theoretical and Applied Finance 20 (5),\n  2017",
        "doi": "10.1142/S0219024917500297.",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We consider a Bayesian financial market with one bond and one stock where the\naim is to maximize the expected power utility from terminal wealth. The\nsolution of this problem is known, however there are some conjectures in the\nliterature about the long-term behavior of the optimal strategy. In this paper\nwe prove now that for positive coefficient in the power utility the long-term\ninvestor is very optimistic and behaves as if the best drift has been realized.\nIn case the coefficient in the power utility is negative the long-term investor\nis very pessimistic and behaves as if the worst drift has been realized.\n"
    },
    {
        "paper_id": 1703.04549,
        "authors": "M. Andrecut",
        "title": "Systemic Risk, Maximum Entropy and Interbank Contagion",
        "comments": "13 pages, 3 figures",
        "journal-ref": "Int. J. Mod. Phys. C 27, 1650148 (2016)",
        "doi": "10.1142/S0129183116501485",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We discuss the systemic risk implied by the interbank exposures reconstructed\nwith the maximum entropy method. The maximum entropy method severely\nunderestimates the risk of interbank contagion by assuming a fully connected\nnetwork, while in reality the structure of the interbank network is sparsely\nconnected. Here, we formulate an algorithm for sparse network reconstruction,\nand we show numerically that it provides a more reliable estimation of the\nsystemic risk.\n"
    },
    {
        "paper_id": 1703.05047,
        "authors": "Dietmar Pfeifer, Andreas M\\\"andle, Olena Ragulina",
        "title": "Data driven partition-of-unity copulas with applications to risk\n  management",
        "comments": "this paper has been upgraded with the paper \"New copulas based on\n  general partition-of-unity copulas and their application to risk management\n  part II\" arXiv article arXiv:1709.07682",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We present a constructive and self-contained approach to data driven general\npartition-of-unity copulas that were recently introduced in the literature. In\nparticular, we consider Bernstein-, negative binomial and Poisson copulas and\npresent a solution to the problem of fitting such copulas to highly asymmetric\ndata.\n"
    },
    {
        "paper_id": 1703.05049,
        "authors": "Omar El Euch and Mathieu Rosenbaum",
        "title": "Perfect hedging in rough Heston models",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Rough volatility models are known to reproduce the behavior of historical\nvolatility data while at the same time fitting the volatility surface\nremarkably well, with very few parameters. However, managing the risks of\nderivatives under rough volatility can be intricate since the dynamics involve\nfractional Brownian motion. We show in this paper that surprisingly enough,\nexplicit hedging strategies can be obtained in the case of rough Heston models.\nThe replicating portfolios contain the underlying asset and the forward\nvariance curve, and lead to perfect hedging (at least theoretically). From a\nprobabilistic point of view, our study enables us to disentangle the\ninfinite-dimensional Markovian structure associated to rough volatility models.\n"
    },
    {
        "paper_id": 1703.05132,
        "authors": "Christian Bayer, Peter K. Friz, Archil Gulisashvili, Blanka Horvath,\n  Benjamin Stemper",
        "title": "Short-time near-the-money skew in rough fractional volatility models",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We consider rough stochastic volatility models where the driving noise of\nvolatility has fractional scaling, in the \"rough\" regime of Hurst parameter $H\n< 1/2$. This regime recently attracted a lot of attention both from the\nstatistical and option pricing point of view. With focus on the latter, we\nsharpen the large deviation results of Forde-Zhang (2017) in a way that allows\nus to zoom-in around the money while maintaining full analytical tractability.\nMore precisely, this amounts to proving higher order moderate deviation\nestimates, only recently introduced in the option pricing context. This in turn\nallows us to push the applicability range of known at-the-money skew\napproximation formulae from CLT type log-moneyness deviations of order\n$t^{1/2}$ (recent works of Al\\`{o}s, Le\\'{o}n & Vives and Fukasawa) to the\nwider moderate deviations regime.\n"
    },
    {
        "paper_id": 1703.0524,
        "authors": "Francis Tseng, Fei Liu, Bernardo Alves Furtado",
        "title": "Humans of Simulated New York (HOSNY): an exploratory comprehensive model\n  of city life",
        "comments": "18 pages, 5 figures, submitted (in review), typos corrected",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The model presented in this paper experiments with a comprehensive simulant\nagent in order to provide an exploratory platform in which simulation modelers\nmay try alternative scenarios and participation in policy decision-making. The\nframework is built in a computationally distributed online format in which\nusers can join in and visually explore the results. Modeled activity involves\ndaily routine errands, such as shopping, visiting the doctor or engaging in the\nlabor market. Further, agents make everyday decisions based on individual\nbehavioral attributes and minimal requirements, according to social and\ncontagion networks. Fully developed firms and governments are also included in\nthe model allowing for taxes collection, production decisions, bankruptcy and\nchange in ownership. The contributions to the literature are multifold. They\ninclude (a) a comprehensive model with detailing of the agents and firms'\nactivities and processes and original use of simultaneously (b) reinforcement\nlearning for firm pricing and demand allocation; (c) social contagion for\ndisease spreading and social network for hiring opportunities; and (d) Bayesian\nnetworks for demographic-like generation of agents. All of that within a (e)\nvisually rich environment and multiple use of databases. Hence, the model\nprovides a comprehensive framework from where interactions among citizens,\nfirms and governments can be easily explored allowing for learning and\nvisualization of policies and scenarios.\n"
    },
    {
        "paper_id": 1703.05709,
        "authors": "Martin Glanzer and Georg Ch. Pflug and Alois Pichler",
        "title": "Incorporating statistical model error into the calculation of\n  acceptability prices of contingent claims",
        "comments": "27 pages, 2 figures",
        "journal-ref": null,
        "doi": "10.1007/s10107-018-1352-7",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The determination of acceptability prices of contingent claims requires the\nchoice of a stochastic model for the underlying asset price dynamics. Given\nthis model, optimal bid and ask prices can be found by stochastic optimization.\nHowever, the model for the underlying asset price process is typically based on\ndata and found by a statistical estimation procedure. We define a confidence\nset of possible estimated models by a nonparametric neighborhood of a baseline\nmodel. This neighborhood serves as ambiguity set for a multi-stage stochastic\noptimization problem under model uncertainty. We obtain distributionally robust\nsolutions of the acceptability pricing problem and derive the dual problem\nformulation. Moreover, we prove a general large deviations result for the\nnested distance, which allows to relate the bid and ask prices under model\nambiguity to the quality of the observed data.\n"
    },
    {
        "paper_id": 1703.05979,
        "authors": "Fran\\c{c}ois Lafond, Aimee Gotway Bailey, Jan David Bakker, Dylan\n  Rebois, Rubina Zadourian, Patrick McSharry, and J. Doyne Farmer",
        "title": "How well do experience curves predict technological progress? A method\n  for making distributional forecasts",
        "comments": null,
        "journal-ref": null,
        "doi": "10.1016/j.techfore.2017.11.001",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Experience curves are widely used to predict the cost benefits of increasing\nthe deployment of a technology. But how good are such forecasts? Can one\npredict their accuracy a priori? In this paper we answer these questions by\ndeveloping a method to make distributional forecasts for experience curves. We\ntest our method using a dataset with proxies for cost and experience for 51\nproducts and technologies and show that it works reasonably well. The framework\nthat we develop helps clarify why the experience curve method often gives\nsimilar results to simply assuming that costs decrease exponentially. To\nillustrate our method we make a distributional forecast for prices of solar\nphotovoltaic modules.\n"
    },
    {
        "paper_id": 1703.0602,
        "authors": "Wei Lin, Shenghong Li and Shane Chern",
        "title": "Pricing VIX Derivatives With Free Stochastic Volatility Model",
        "comments": "21 pages, 16 figures and 23 conferences",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper, we relax the power parameter of instantaneous variance and\ndevelop a new stochastic volatility plus jumps model that generalize the Heston\nmodel and 3/2 model as special cases. This model has two distinctive features.\nFirst, we do not restrict the new parameter, letting the data speak as to its\ndirection. The Generalized Methods of Moments suggests that the newly added\nparameter is to create varying volatility fluctuation in different period\ndiscovered in financial market. Moreover, upward and downward jumps are\nseparately modeled to accommodate the market data. Our model is novel and\nhighly tractable, which means that the quasi-closed-form solutions for future\nand option prices can be effectively derived. We have employed data on VIX\nfuture and corresponding option contracts to test this model to evaluate its\nability of performing pricing and capturing features of the implied volatility.\nTo sum up, the free stochastic volatility model with asymmetric jumps is able\nto adequately capture implied volatility dynamics and thus it can be seen as a\nsuperior model relative to the fixed volatility model in pricing VIX\nderivatives.\n"
    },
    {
        "paper_id": 1703.06351,
        "authors": "Nassim Nicholas Taleb",
        "title": "Election Predictions as Martingales: An Arbitrage Approach",
        "comments": null,
        "journal-ref": "Quantitative Finance 18 (1), 1-5, 2018",
        "doi": "10.1080/14697688.2017.1395230",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We consider the estimation of binary election outcomes as martingales and\npropose an arbitrage pricing when one continuously updates estimates. We argue\nthat the estimator needs to be priced as a binary option as the arbitrage\nvaluation minimizes the conventionally used Brier score for tracking the\naccuracy of probability assessors.\n  We create a dual martingale process $Y$, in $[L,H]$ from the standard\narithmetic Brownian motion, $X$ in $(-\\infty, \\infty)$ and price elections\naccordingly. The dual process $Y$ can represent the numerical votes needed for\nsuccess.\n  We show the relationship between the volatility of the estimator in relation\nto that of the underlying variable. When there is a high uncertainty about the\nfinal outcome, 1) the arbitrage value of the binary gets closer to 50\\%, 2) the\nestimate should not undergo large changes even if polls or other bases show\nsignificant variations.\n  There are arbitrage relationships between 1) the binary value, 2) the\nestimation of $Y$, 3) the volatility of the estimation of $Y$ over the\nremaining time to expiration. We note that these arbitrage relationships were\noften violated by the various forecasting groups in the U.S. presidential\nelections of 2016, as well as the notion that all intermediate assessments of\nthe success of a candidate need to be considered, not just the final one.\n"
    },
    {
        "paper_id": 1703.06603,
        "authors": "Sujay Mukhoti and Pritam Ranjan",
        "title": "A New Class of Discrete-time Stochastic Volatility Model with Correlated\n  Errors",
        "comments": "35 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In an efficient stock market, the returns and their time-dependent volatility\nare often jointly modeled by stochastic volatility models (SVMs). Over the last\nfew decades several SVMs have been proposed to adequately capture the defining\nfeatures of the relationship between the return and its volatility. Among one\nof the earliest SVM, Taylor (1982) proposed a hierarchical model, where the\ncurrent return is a function of the current latent volatility, which is further\nmodeled as an auto-regressive process. In an attempt to make the SVMs more\nappropriate for complex realistic market behavior, a leverage parameter was\nintroduced in the Taylor SVM, which however led to the violation of the\nefficient market hypothesis (EMH, a necessary mean-zero condition for the\nreturn distribution that prevents arbitrage possibilities). Subsequently, a\nhost of alternative SVMs had been developed and are currently in use. In this\npaper, we propose mean-corrections for several generalizations of Taylor SVM\nthat capture the complex market behavior as well as satisfy EMH. We also\nestablish a few theoretical results to characterize the key desirable features\nof these models, and present comparison with other popular competitors.\nFurthermore, four real-life examples (Oil price, CITI bank stock price,\nEuro-USD rate, and S&P 500 index returns) have been used to demonstrate the\nperformance of this new class of SVMs.\n"
    },
    {
        "paper_id": 1703.06739,
        "authors": "Kiyoshi Kanazawa, Takumi Sueshige, Hideki Takayasu, Misako Takayasu",
        "title": "Derivation of the Boltzmann Equation for Financial Brownian Motion:\n  Direct Observation of the Collective Motion of High-Frequency Traders",
        "comments": "5 pages, 5 figures + Appendices",
        "journal-ref": "Phys. Rev. Lett. 120, 138301 (2018)",
        "doi": "10.1103/PhysRevLett.120.138301",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  A microscopic model is established for financial Brownian motion from the\ndirect observation of the dynamics of high-frequency traders (HFTs) in a\nforeign exchange market. Furthermore, a theoretical framework parallel to\nmolecular kinetic theory is developed for the systematic description of the\nfinancial market from microscopic dynamics of HFTs. We report first on a\nmicroscopic empirical law of traders' trend-following behavior by tracking the\ntrajectories of all individuals, which quantifies the collective motion of HFTs\nbut has not been captured in conventional order-book models. We next introduce\nthe corresponding microscopic model of HFTs and present its theoretical\nsolution paralleling molecular kinetic theory: Boltzmann-like and Langevin-like\nequations are derived from the microscopic dynamics via the\nBogoliubov-Born-Green-Kirkwood-Yvon hierarchy. Our model is the first\nmicroscopic model that has been directly validated through data analysis of the\nmicroscopic dynamics, exhibiting quantitative agreements with mesoscopic and\nmacroscopic empirical results.\n"
    },
    {
        "paper_id": 1703.0684,
        "authors": "T. T. Chen, B. Zheng, Y. Li, and X. F. Jiang",
        "title": "New approaches in agent-based modeling of complex financial systems",
        "comments": "arXiv admin note: text overlap with arXiv:1504.01811",
        "journal-ref": "Front. Phys. 12(3), 128905 (2017)",
        "doi": "10.1007/s11467-017-0661-2",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Agent-based modeling is a powerful simulation technique to understand the\ncollective behavior and microscopic interaction in complex financial systems.\nRecently, the concept for determining the key parameters of the agent-based\nmodels from empirical data instead of setting them artificially was suggested.\nWe first review several agent-based models and the new approaches to determine\nthe key model parameters from historical market data. Based on the agents'\nbehaviors with heterogenous personal preferences and interactions, these models\nare successful to explain the microscopic origination of the temporal and\nspatial correlations of the financial markets. We then present a novel paradigm\ncombining the big-data analysis with the agent-based modeling. Specifically,\nfrom internet query and stock market data, we extract the information driving\nforces, and develop an agent-based model to simulate the dynamic behaviors of\nthe complex financial systems.\n"
    },
    {
        "paper_id": 1703.06969,
        "authors": "Jean-Pierre Fouque, Ruimeng Hu",
        "title": "Optimal Portfolio under Fractional Stochastic Environment",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Rough stochastic volatility models have attracted a lot of attentions\nrecently, in particular for the linear option pricing problem. In this paper,\nstarting with power utilities, we propose to use a martingale distortion\nrepresentation of the optimal value function for the nonlinear asset allocation\nproblem in a (non-Markovian) fractional stochastic environment (for all Hurst\nindex $H \\in (0,1)$). We rigorously establish a first order approximation of\nthe optimal value, where the return and volatility of the underlying asset are\nfunctions of a stationary slowly varying fractional Ornstein-Uhlenbeck process.\nWe prove that this approximation can be also generated by a fixed zeroth order\ntrading strategy providing an explicit strategy which is asymptotically optimal\nin all admissible controls. Furthermore, we extend the discussion to general\nutility functions, and obtain the asymptotic optimality of this fixed strategy\nin a specific family of admissible strategies.\n"
    },
    {
        "paper_id": 1703.07339,
        "authors": "Dariusz Zawisza",
        "title": "Stochastic control on the half-line and applications to the optimal\n  dividend/consumption problem",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We consider a stochastic control problem with the assumption that the system\nis controlled until the state process breaks the fixed barrier. Assuming some\ngeneral conditions, it is proved that the resulting Hamilton Jacobi Bellman\nequations has smooth solution. The aforementioned result is used to solve the\noptimal dividend and consumption problem. In the proof we use a fixed point\ntype argument, with an operator which is based on the stochastic representation\nfor a linear equation.\n"
    },
    {
        "paper_id": 1703.07513,
        "authors": "Leonardo dos Santos Pinheiro and Flavio Codeco COelho",
        "title": "An Agent-based Model of Contagion in Financial Networks",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This work develops an agent-based model for the study of how the leverage\nthrough the use of repurchase agreements can function as a mechanism for the\npropagation and amplification of financial shocks in a financial system. Based\non the analysis of financial intermediaries in the repo and interbank lending\nmarkets during the 2007-08 financial crisis we develop a model that can be used\nto simulate the dynamics of financial contagion.\n"
    },
    {
        "paper_id": 1703.07685,
        "authors": "Daniel Lacker and Thaleia Zariphopoulou",
        "title": "Mean field and n-agent games for optimal investment under relative\n  performance criteria",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We analyze a family of portfolio management problems under relative\nperformance criteria, for fund managers having CARA or CRRA utilities and\ntrading in a common investment horizon in log-normal markets. We construct\nexplicit constant equilibrium strategies for both the finite population games\nand the corresponding mean field games, which we show are unique in the class\nof constant equilibria. In the CARA case, competition drives agents to invest\nmore in the risky asset than they would otherwise, while in the CRRA case\ncompetitive agents may over- or under-invest, depending on their levels of risk\ntolerance.\n"
    },
    {
        "paper_id": 1703.08282,
        "authors": "Man Chung Fung and Gareth W. Peters and Pavel V. Shevchenko",
        "title": "Cohort effects in mortality modelling: a Bayesian state-space approach",
        "comments": "41 pages, 12 figures",
        "journal-ref": "Ann. actuar. sci. 13 (2019) 109-144",
        "doi": "10.1017/S1748499518000131",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Cohort effects are important factors in determining the evolution of human\nmortality for certain countries. Extensions of dynamic mortality models with\ncohort features have been proposed in the literature to account for these\nfactors under the generalised linear modelling framework. In this paper we\napproach the problem of mortality modelling with cohort factors incorporated\nthrough a novel formulation under a state-space methodology. In the process we\ndemonstrate that cohort factors can be formulated naturally under the\nstate-space framework, despite the fact that cohort factors are indexed\naccording to year-of-birth rather than year. Bayesian inference for cohort\nmodels in a state-space formulation is then developed based on an efficient\nMarkov chain Monte Carlo sampler, allowing for the quantification of parameter\nuncertainty in cohort models and resulting mortality forecasts that are used\nfor life expectancy and life table constructions. The effectiveness of our\napproach is examined through comprehensive empirical studies involving male and\nfemale populations from various countries. Our results show that cohort\npatterns are present for certain countries that we studied and the inclusion of\ncohort factors are crucial in capturing these phenomena, thus highlighting the\nbenefits of introducing cohort models in the state-space framework. Forecasting\nof cohort models is also discussed in light of the projection of cohort\nfactors.\n"
    },
    {
        "paper_id": 1703.08534,
        "authors": "Sigrid K\\\"allblad",
        "title": "A Dynamic Programming Principle for Distribution-Constrained Optimal\n  Stopping",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We consider an optimal stopping problem where a constraint is placed on the\ndistribution of the stopping time. Reformulating the problem in terms of\nso-called measure-valued martingales allows us to transform the marginal\nconstraint into an initial condition and view the problem as a stochastic\ncontrol problem; we establish the corresponding dynamic programming principle.\n"
    },
    {
        "paper_id": 1703.08715,
        "authors": "Vladimir Vovk and Glenn Shafer",
        "title": "Towards a probability-free theory of continuous martingales",
        "comments": "26 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Without probability theory, we define classes of supermartingales,\nmartingales, and semimartingales in idealized financial markets with continuous\nprice paths. This allows us to establish probability-free versions of a number\nof standard results in martingale theory, including the Dubins-Schwarz theorem,\nthe Girsanov theorem, and results concerning the It\\^o integral. We also\nestablish the existence of an equity premium and a CAPM relationship in this\nprobability-free setting.\n"
    },
    {
        "paper_id": 1703.0875,
        "authors": "Ashish R. Hota and Shreyas Sundaram",
        "title": "Game-Theoretic Vaccination Against Networked SIS Epidemics and Impacts\n  of Human Decision-Making",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We study decentralized protection strategies against\nSusceptible-Infected-Susceptible (SIS) epidemics on networks. We consider a\npopulation game framework where nodes choose whether or not to vaccinate\nthemselves, and the epidemic risk is defined as the infection probability at\nthe endemic state of the epidemic under a degree-based mean-field\napproximation. Motivated by studies in behavioral economics showing that humans\nperceive probabilities and risks in a nonlinear fashion, we specifically\nexamine the impacts of such misperceptions on the Nash equilibrium protection\nstrategies. We first establish the existence and uniqueness of a threshold\nequilibrium where nodes with degrees larger than a certain threshold vaccinate.\nWhen the vaccination cost is sufficiently high, we show that behavioral biases\ncause fewer players to vaccinate, and vice versa. We quantify this effect for a\nclass of networks with power-law degree distributions by proving tight bounds\non the ratio of equilibrium thresholds under behavioral and true perceptions of\nprobabilities. We further characterize the socially optimal vaccination policy\nand investigate the inefficiency of Nash equilibrium.\n"
    },
    {
        "paper_id": 1703.08781,
        "authors": "M. Saeedian, T. Jamali, M. Z. Kamali, H. Bayani, T. Yasseri and G.R.\n  Jafari",
        "title": "Emergence of world-stock-market network",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In the age of globalization, it is natural that the stock market of each\ncountry is not independent form the other markets. In this case, collective\nbehavior could be emerged form their dependency together. This article studies\nthe collective behavior of a set of forty influential markets in the world\neconomy with the aim of exploring a global financial structure that could be\ncalled world-stock-market network. Towards this end, we analyze the\ncross-correlation matrix of the indices of these forty markets using Random\nMatrix Theory (RMT). We find the degree of collective behavior among the\nmarkets and the share of each market in their structural formation. This\nfinding together with the results obtained from the same calculation on four\nstock markets reinforce the idea of a world financial market. Finally, we draw\nthe dendrogram of the cross-correlation matrix to make communities in this\nabstract global market visible. The dendrogram, drawn by at least thirty\npercent of correlation, shows that the world financial market comprises three\ncommunities each of which includes stock markets with geographical proximity.\n"
    },
    {
        "paper_id": 1703.08807,
        "authors": "Anuj Bhowmik and Jiling Cao",
        "title": "Ex-post core, fine core and rational expectations equilibrium\n  allocations",
        "comments": "23 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper investigates the ex-post core and its relationships to the fine\ncore and the set of rational expectations equilibrium allocations in an\noligopolistic economy with asymmetric information, in which the set of agents\nconsists of some large agents and a continuum of small agents and the space of\nstates of nature is a general probability space. We show that under appropriate\nassumptions, the ex-post core is not empty and contains the set of rational\nexpectations equilibrium allocations. We provide an example of a pure exchange\ncontinuum economy with asymmetric information and infinitely many states of\nnature, in which the ex-post core does not coincide with the set of rational\nexpectations equilibrium allocations. We also show that when our economic model\ncontains either no large agents or at least two large agents with the same\ncharacteristics, the fine core is contained in the ex-post core.\n"
    },
    {
        "paper_id": 1703.08812,
        "authors": "Ravi Kashyap",
        "title": "Microstructure under the Microscope: Tools to Survive and Thrive in The\n  Age of (Too Much) Information",
        "comments": "arXiv admin note: substantial text overlap with arXiv:1603.09060",
        "journal-ref": "Journal of Trading, (Spring 2017), Institutional Investor\n  Journals, New York, USA, Vol. 12, No. 2, pp. 5-27",
        "doi": "10.3905/jot.2017.12.2.005",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Market Microstructure is the investigation of the process and protocols that\ngovern the exchange of assets with the objective of reducing frictions that can\nimpede the transfer. In financial markets, where there is an abundance of\nrecorded information, this translates to the study of the dynamic relationships\nbetween observed variables, such as price, volume and spread, and hidden\nconstituents, such as transaction costs and volatility, that hold sway over the\nefficient functioning of the system.\n  \"My dear, here we must process as much data as we can, just to stay in\nbusiness. And if you wish to make a profit you must process at least twice as\nmuch data.\" - Red Queen to Alice in Hedge-Fund-Land.\n  In this age of (Too Much) Information, it is imperative to uncover nuggets of\nknowledge (signal) from buckets of nonsense (noise). To aid in this effort to\nextract meaning from chaos and to gain a better understanding of the\nrelationships between financial variables, we summarize the application of the\ntheoretical results from (Kashyap 2016b) to microstructure studies. The central\nconcept rests on a novel methodology based on the marriage between the\nBhattacharyya distance, a measure of similarity across distributions, and the\nJohnson Lindenstrauss Lemma, a technique for dimension reduction, providing us\nwith a simple yet powerful tool that allows comparisons between data-sets\nrepresenting any two distributions. We provide an empirical illustration using\nprices, volumes and volatilities across seven countries and three different\ncontinents. The degree to which different markets or sub groups of securities\nhave different measures of their corresponding distributions tells us the\nextent to which they are different. This can aid investors looking for\ndiversification or looking for more of the same thing.\n"
    },
    {
        "paper_id": 1703.09129,
        "authors": "Amirhossein Sobhani, Mariyan Milev",
        "title": "A Numerical Method for Pricing Discrete Double Barrier Option by\n  Legendre Multiwavelet",
        "comments": null,
        "journal-ref": "Journal of Computational and Applied Mathematics 328C (2018) pp.\n  355-364",
        "doi": "10.1016/j.cam.2017.07.033",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this Article, a fast numerical numerical algorithm for pricing discrete\ndouble barrier option is presented. According to Black-Scholes model, the price\nof option in each monitoring date can be evaluated by a recursive formula upon\nthe heat equation solution. These recursive solutions are approximated by using\nLegendre multiwavelets as orthonormal basis functions and expressed in\noperational matrix form. The most important feature of this method is that its\nCPU time is nearly invariant when monitoring dates increase. Besides, the rate\nof convergence of presented algorithm was obtained. The numerical results\nverify the validity and efficiency of the numerical method.\n"
    },
    {
        "paper_id": 1703.09386,
        "authors": "Tetsuya Takaishi and Toshiaki Watanabe",
        "title": "Analysis of Realized Volatility for Nikkei Stock Average on the Tokyo\n  Stock Exchange",
        "comments": "9 pages, 7 figures",
        "journal-ref": "Journal of Physics: Conference Series 710 (2016) 012010",
        "doi": "10.1088/1742-6596/710/1/012010",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We calculate realized volatility of the Nikkei Stock Average (Nikkei225)\nIndex on the Tokyo Stock Exchange and investigate the return dynamics. To avoid\nthe bias on the realized volatility from the non-trading hours issue we\ncalculate realized volatility separately in the two trading sessions, i.e.\nmorning and afternoon, of the Tokyo Stock Exchange and find that the\nmicrostructure noise decreases the realized volatility at small sampling\nfrequency. Using realized volatility as a proxy of the integrated volatility we\nstandardize returns in the morning and afternoon sessions and investigate the\nnormality of the standardized returns by calculating variance, kurtosis and 6th\nmoment. We find that variance, kurtosis and 6th moment are consistent with\nthose of the standard normal distribution, which indicates that the return\ndynamics of the Nikkei Stock Average are well described by a Gaussian random\nprocess with time-varying volatility.\n"
    },
    {
        "paper_id": 1703.095,
        "authors": "Peter Erdos, Mihaly Ormos, David Zibriczky",
        "title": "Non-parametric and semi-parametric asset pricing",
        "comments": "37 pages, 4 figures and 3 tables",
        "journal-ref": "Economic Modelling 28:(3) pp. 1150-1162",
        "doi": "10.1016/j.econmod.2010.12.008",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We find that the CAPM fails to explain the small firm effect even if its\nnon-parametric form is used which allows time-varying risk and non-linearity in\nthe pricing function. Furthermore, the linearity of the CAPM can be rejected,\nthus the widely used risk and performance measures, the beta and the alpha, are\nbiased and inconsistent. We deduce semi-parametric measures which are\nnon-constant under extreme market conditions in a single factor setting; on the\nother hand, they are not significantly different from the linear estimates of\nthe Fama-French three-factor model. If we extend the single factor model with\nthe Fama-French factors, the simple linear model is able to explain the US\nstock returns correctly.\n"
    },
    {
        "paper_id": 1703.09667,
        "authors": "Sergey Kamenshchikov, Ilia Drozdov",
        "title": "Biased Risk Parity with Fractal Model of Risk",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  For the past two decades investors have observed long memory and highly\ncorrelated behavior of asset classes that does not fit into the framework of\nModern Portfolio Theory. Custom correlation and standard deviation estimators\nconsider normal distribution of returns and market efficiency hypothesis. It\nforced investors to search more universal instruments of tail risk protection.\nOne of the possible solutions is a naive risk parity strategy, which avoids\nestimation of expected returns and correlations. The authors develop the idea\nfurther and propose a fractal distribution of returns as a core. This class of\ndistributions is more general as it does not imply strict limitations on risk\nevolution. The proposed model allows for modifying a rule for volatility\nestimation, thus, enhancing its explanatory power. It turns out that the latter\nimproves the performance metrics of an investment portfolio over the ten year\nperiod. The fractal model of volatility plays a significant protective role\nduring the periods of market abnormal drawdowns. Consequently, it may be useful\nfor a wide range of asset managers which incorporate innovative risk models\ninto globally allocated portfolios.\n"
    },
    {
        "paper_id": 1703.09748,
        "authors": "Niushan Gao, Denny H. Leung",
        "title": "Smallest order closed sublattices and option spanning",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Let $Y$ be a sublattice of a vector lattice $X$. We consider the problem of\nidentifying the smallest order closed sublattice of $X$ containing $Y$. It is\nknown that the analogy with topological closure fails. Let $\\overline{Y}^o$ be\nthe order closure of $Y$ consisting of all order limits of nets of elements\nfrom $Y$. Then $\\overline{Y}^o$ need not be order closed. We show that in many\ncases the smallest order closed sublattice containing $Y$ is in fact the second\norder closure $\\overline{\\overline{Y}^o}^o$. Moreover, if $X$ is a\n$\\sigma$-order complete Banach lattice, then the condition that\n$\\overline{Y}^o$ is order closed for every sublattice $Y$ characterizes order\ncontinuity of the norm of $X$. The present paper provides a general approach to\na fundamental result in financial economics concerning the spanning power of\noptions written on a financial asset.\n"
    },
    {
        "paper_id": 1703.09782,
        "authors": "Matteo Gardini, Marco Diana",
        "title": "FIEMS: Fast Italian Energy Market Simulator",
        "comments": "in Italian",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The article describes the algorithm used to define the electricity price in\nday-ahead and itraday energy markets in Italy. Details of Matlab implementation\nof one of its simplified versions, capable of producing good results in a\nextremely short time, are then provided and numerical results are discussed.\n"
    },
    {
        "paper_id": 1703.10098,
        "authors": "Tshilidzi Marwala",
        "title": "Rational Choice and Artificial Intelligence",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The theory of rational choice assumes that when people make decisions they do\nso in order to maximize their utility. In order to achieve this goal they ought\nto use all the information available and consider all the choices available to\nchoose an optimal choice. This paper investigates what happens when decisions\nare made by artificially intelligent machines in the market rather than human\nbeings. Firstly, the expectations of the future are more consistent if they are\nmade by an artificially intelligent machine and the decisions are more rational\nand thus marketplace becomes more rational.\n"
    },
    {
        "paper_id": 1703.10469,
        "authors": "Zachary Feinstein",
        "title": "Harry Potter and the Goblin Bank of Gringotts",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Gringotts Wizarding Bank is well known as the only financial institution in\nall of the Wizarding UK as documented in the works recounting the heroics of\nHarry Potter. The concentration of power and wealth in this single bank needs\nto be weighed against the financial stability of the entire Wizarding economy.\nThis study will consider the impact to financial risk of breaking up Gringotts\nWizarding Bank into five component pieces, along the lines of the\nGlass-Steagall Act in the United States. The emphasis of this work is to\ncalibrate and simulate a model of the banking and financial systems within\nWizarding UK under varying stress test scenarios simulating rumors of Lord\nVoldemort's return or the release of magical creatures into an unsuspecting\nmuggle populace. We conclude by comparing the economic fallout from financial\ncrises under the two systems: (i) Gringotts Wizarding Bank as a monopoly and\n(ii) the split-up financial system. We do this comparison on the level of\nminimal system-wide capital injections that would be needed to prevent the\nfinancial crisis from surpassing the damage caused by Lord Voldemort.\n"
    },
    {
        "paper_id": 1703.10588,
        "authors": "Marcel Nutz, Florian Stebegg, Xiaowei Tan",
        "title": "Multiperiod Martingale Transport",
        "comments": "63 pages, 5 figures, forthcoming in 'Stochastic Processes and their\n  Applications'",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Consider a multiperiod optimal transport problem where distributions\n$\\mu_{0},\\dots,\\mu_{n}$ are prescribed and a transport corresponds to a scalar\nmartingale $X$ with marginals $X_{t}\\sim\\mu_{t}$. We introduce particular\ncouplings called left-monotone transports; they are characterized equivalently\nby a no-crossing property of their support, as simultaneous optimizers for a\nclass of bivariate transport cost functions with a Spence--Mirrlees property,\nand by an order-theoretic minimality property. Left-monotone transports are\nunique if $\\mu_{0}$ is atomless, but not in general. In the one-period case\n$n=1$, these transports reduce to the Left-Curtain coupling of Beiglb\\\"ock and\nJuillet. In the multiperiod case, the bivariate marginals for dates $(0,t)$ are\nof Left-Curtain type, if and only if $\\mu_{0},\\dots,\\mu_{n}$ have a specific\norder property. The general analysis of the transport problem also gives rise\nto a strong duality result and a description of its polar sets. Finally, we\nstudy a variant where the intermediate marginals $\\mu_{1},\\dots,\\mu_{n-1}$ are\nnot prescribed.\n"
    },
    {
        "paper_id": 1703.10639,
        "authors": "Francesco Lamperti, Andrea Roventini and Amir Sani",
        "title": "Agent-Based Model Calibration using Machine Learning Surrogates",
        "comments": "32 pages, 19 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Taking agent-based models (ABM) closer to the data is an open challenge. This\npaper explicitly tackles parameter space exploration and calibration of ABMs\ncombining supervised machine-learning and intelligent sampling to build a\nsurrogate meta-model. The proposed approach provides a fast and accurate\napproximation of model behaviour, dramatically reducing computation time. In\nthat, our machine-learning surrogate facilitates large scale explorations of\nthe parameter-space, while providing a powerful filter to gain insights into\nthe complex functioning of agent-based models. The algorithm introduced in this\npaper merges model simulation and output analysis into a surrogate meta-model,\nwhich substantially ease ABM calibration. We successfully apply our approach to\nthe Brock and Hommes (1998) asset pricing model and to the \"Island\" endogenous\ngrowth model (Fagiolo and Dosi, 2003). Performance is evaluated against a\nrelatively large out-of-sample set of parameter combinations, while employing\ndifferent user-defined statistical tests for output analysis. The results\ndemonstrate the capacity of machine learning surrogates to facilitate fast and\nprecise exploration of agent-based models' behaviour over their often rugged\nparameter spaces.\n"
    },
    {
        "paper_id": 1703.10806,
        "authors": "Florian Ziel, Rick Steinert",
        "title": "Probabilistic Mid- and Long-Term Electricity Price Forecasting",
        "comments": "accepted for: Renewable & Sustainable Energy Reviews",
        "journal-ref": "Renewable and Sustainable Energy Reviews, 32.3 (2018) 251-266",
        "doi": "10.1016/j.rser.2018.05.038",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The liberalization of electricity markets and the development of renewable\nenergy sources has led to new challenges for decision makers. These challenges\nare accompanied by an increasing uncertainty about future electricity price\nmovements. The increasing amount of papers, which aim to model and predict\nelectricity prices for a short period of time provided new opportunities for\nmarket participants. However, the electricity price literature seem to be very\nscarce on the issue of medium- to long-term price forecasting, which is\nmandatory for investment and political decisions. Our paper closes this gap by\nintroducing a new approach to simulate electricity prices with hourly\nresolution for several months up to three years. Considering the uncertainty of\nfuture events we are able to provide probabilistic forecasts which are able to\ndetect probabilities for price spikes even in the long-run. As market we\ndecided to use the EPEX day-ahead electricity market for Germany and Austria.\nOur model extends the X-Model which mainly utilizes the sale and purchase curve\nfor electricity day-ahead auctions. By applying our procedure we are able to\ngive probabilities for the due to the EEG practical relevant event of six\nconsecutive hours of negative prices. We find that using the supply and demand\ncurve based model in the long-run yields realistic patterns for the time series\nof electricity prices and leads to promising results considering common error\nmeasures.\n"
    },
    {
        "paper_id": 1703.10825,
        "authors": "Gifty Malhotra, R. Srivastava, H. C. Taneja",
        "title": "Quadratic approximation of slow factor of volatility in a Multi-factor\n  Stochastic volatility Model",
        "comments": "11 pages, 1 figure",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In the present work, we propose a new multifactor stochastic volatility model\nin which slow factor of volatility is approximated by a parabolic arc. We\nretain ourselves to the perturbation technique to obtain approximate expression\nfor European option prices. We introduce the notion of modified Black-Scholes\nprice. We obtain a simplified expression for European option price which is\nperturbed around the modified Black-Scholes price and have also obtained the\nexpression of modified price in terms of Black-Scholes price.\n"
    },
    {
        "paper_id": 1703.10832,
        "authors": "Teruyoshi Kobayashi and Taro Takaguchi",
        "title": "Social dynamics of financial networks",
        "comments": "7 pages, 5 figures, SI included",
        "journal-ref": "EPJ Data Science, 2018 7:15",
        "doi": "10.1140/epjds/s13688-018-0143-y",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The global financial crisis in 2007-2009 demonstrated that systemic risk can\nspread all over the world through a complex web of financial linkages, yet we\nstill lack fundamental knowledge about the evolution of the financial web. In\nparticular, interbank credit networks shape the core of the financial system,\nin which a time-varying interconnected risk emerges from a massive number of\ntemporal transactions between banks. The current lack of understanding of the\nmechanics of interbank networks makes it difficult to evaluate and control\nsystemic risk. Here, we uncover fundamental dynamics of interbank networks by\nseeking the patterns of daily transactions between individual banks. We find\nstable interaction patterns between banks from which distinctive network-scale\ndynamics emerge. In fact, the dynamical patterns discovered at the local and\nnetwork scales share common characteristics with social communication patterns\nof humans. To explain the origin of \"social\" dynamics in interbank networks, we\nprovide a simple model that allows us to generate a sequence of synthetic daily\nnetworks characterized by the observed dynamical properties. The discovery of\ndynamical principles at the daily resolution will enhance our ability to assess\nsystemic risk and could contribute to the real-time management of financial\nstability.\n"
    },
    {
        "paper_id": 1703.10897,
        "authors": "Josue Ortega",
        "title": "Multi-unit Assignment under Dichotomous Preferences",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  I study the problem of allocating objects among agents without using money.\nAgents can receive several objects and have dichotomous preferences, meaning\nthat they either consider objects to be acceptable or not. In this setup, the\negalitarian solution is more appealing than the competitive equilibrium with\nequal incomes because it is Lorenz dominant, unique in utilities, and group\nstrategy-proof. Moreover, it can be adapted to satisfy a new fairness axiom\nthat arises naturally in this context. Both solutions are disjoint.\n"
    },
    {
        "paper_id": 1703.10981,
        "authors": "Jie Sun and Qiang Yao",
        "title": "On coherency and other properties of MAXVAR",
        "comments": "10 pages",
        "journal-ref": "Vietnam Journal of Mathematics, 46:87-94 (2018)",
        "doi": "10.1007/s10013-017-0262-y",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper is concerned with the MAXVAR risk measure on L^2 space. We present\nan elementary and direct proof of its coherency and averseness. Based on the\nobservation that the MAXVAR measure is a continuous convex combination of the\nCVaR measure, we provide an explicit formula for the risk envelope of MAXVAR.\n"
    },
    {
        "paper_id": 1704.00256,
        "authors": "Visant Ahuja",
        "title": "Non-Analytic Solution to the Fokker-Planck Equation of Fractional\n  Brownian Motion via Laplace Transforms",
        "comments": "23 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper derives the non-analytic solution to the Fokker-Planck equation of\nfractional Brownian motion using the method of Laplace transform. Sequentially,\nby considering the fundamental solution of the non-analytic solution, this\npaper obtains the transition probability density function of the random\nvariable that is described by the It\\^o's stochastic ordinary differential\nequation of fractional Brownian motion. Furthermore, this paper applies the\nderived transition probability density function to the Cox-Ingersoll-Ross model\ngoverned by the fractional Brownian motion instead of the usual Brownian\nmotion.\n"
    },
    {
        "paper_id": 1704.00383,
        "authors": "Lanh Tran",
        "title": "How Wave - Wavelet Trading Wins and \"Beats\" the Market",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The purpose of this paper is to showcase trading strategies that give\nsolutions to three difficult and intriguing problems in business finance,\neconomics and statistics. The paper discusses trading strategies for both\ncommodities and stocks but the main focus is on stock market trading at the New\nYork Stock Exchange.\n  Problem 1: Buy Low and Sell High. The buy low and sell high problem can be\nsummarized like this: suppose the price of a commodity or stock fluctuates\nindefinitely, is there any explicit strategy for a trader to \"ride the price\nwaves\" by buying low and selling high to eventually win even if price does not\nincrease?\n  Problem 2: \"Beat\" the Market. In Part 2, the trading system presented in Part\n1 is transformed into a strategy that always outperforms the market eventually.\n  Problem 3: Can a Trader Outperform a geometric Brownian Motion? The general\nbelief is that it is impossible to \"beat\" a GBM since technical analysis of\nhistorical prices is useless in predicting future prices. The last part of the\npaper shows that the answer to Problem 3 is actually a \"YES\", which is quite\nsurprising.\n  The trading strategies presented are based mainly on information obtained\nfrom the movements of waves and wavelets created by large and small\nfluctuations of market prices. They do not involve any forecasting or\nprediction of future prices. Behavioral economics also plays a role in the\ndecision making process of the Wavelet Trading program. My website\nAgateTrading.com is available to the public.\n"
    },
    {
        "paper_id": 1704.00416,
        "authors": "Rongju Zhang, Nicolas Langren\\'e, Yu Tian, Zili Zhu, Fima Klebaner,\n  Kais Hamza",
        "title": "Skewed target range strategy for multiperiod portfolio optimization\n  using a two-stage least squares Monte Carlo method",
        "comments": "25 pages, 10 figures",
        "journal-ref": "Journal of Computational Finance 23(1) 97-127 (2019)",
        "doi": "10.21314/JCF.2019.368",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper, we propose a novel investment strategy for portfolio\noptimization problems. The proposed strategy maximizes the expected portfolio\nvalue bounded within a targeted range, composed of a conservative lower target\nrepresenting a need for capital protection and a desired upper target\nrepresenting an investment goal. This strategy favorably shapes the entire\nprobability distribution of returns, as it simultaneously seeks a desired\nexpected return, cuts off downside risk and implicitly caps volatility and\nhigher moments. To illustrate the effectiveness of this investment strategy, we\nstudy a multiperiod portfolio optimization problem with transaction costs and\ndevelop a two-stage regression approach that improves the classical least\nsquares Monte Carlo (LSMC) algorithm when dealing with difficult payoffs, such\nas highly concave, abruptly changing or discontinuous functions. Our numerical\nresults show substantial improvements over the classical LSMC algorithm for\nboth the constant relative risk-aversion (CRRA) utility approach and the\nproposed skewed target range strategy (STRS). Our numerical results illustrate\nthe ability of the STRS to contain the portfolio value within the targeted\nrange. When compared with the CRRA utility approach, the STRS achieves a\nsimilar mean-variance efficient frontier while delivering a better downside\nrisk-return trade-off.\n"
    },
    {
        "paper_id": 1704.00847,
        "authors": "Charles-Albert Lehalle and Eyal Neuman",
        "title": "Incorporating Signals into Optimal Trading",
        "comments": "42 pages, 9 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Optimal trading is a recent field of research which was initiated by Almgren,\nChriss, Bertsimas and Lo in the late 90's. Its main application is slicing\nlarge trading orders, in the interest of minimizing trading costs and potential\nperturbations of price dynamics due to liquidity shocks. The initial\noptimization frameworks were based on mean-variance minimization for the\ntrading costs. In the past 15 years, finer modelling of price dynamics, more\nrealistic control variables and different cost functionals were developed. The\ninclusion of signals (i.e. short term predictors of price dynamics) in optimal\ntrading is a recent development and it is also the subject of this work.\n  We incorporate a Markovian signal in the optimal trading framework which was\ninitially proposed by Gatheral, Schied, and Slynko [21] and provide results on\nthe existence and uniqueness of an optimal trading strategy. Moreover, we\nderive an explicit singular optimal strategy for the special case of an\nOrnstein-Uhlenbeck signal and an exponentially decaying transient market\nimpact. The combination of a mean-reverting signal along with a market impact\ndecay is of special interest, since they affect the short term price variations\nin opposite directions.\n  Later, we show that in the asymptotic limit were the transient market impact\nbecomes instantaneous, the optimal strategy becomes continuous. This result is\ncompatible with the optimal trading framework which was proposed by Cartea and\nJaimungal [10].\n  In order to support our models, we analyse nine months of tick by tick data\non 13 European stocks from the NASDAQ OMX exchange. We show that orderbook\nimbalance is a predictor of the future price move and it has some\nmean-reverting properties. From this data we show that market participants,\nespecially high frequency traders, use this signal in their trading strategies.\n"
    },
    {
        "paper_id": 1704.00985,
        "authors": "Mikio Ito, Kiyotaka Maeda, and Akihiko Noda",
        "title": "Discretion versus Policy Rules in Futures Markets: A Case of the\n  Osaka-Dojima Rice Exchange, 1914-1939",
        "comments": "32 pages, 2 tables, 6 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We investigate the relationship between market efficiency of rice futures\ntransaction in Osaka and the Japanese government intervention in rice\ndistributions by directly buying and selling rice during the interwar period,\nfrom the middle 1910s to 1939, considering the context of \"discretion versus\nrules.\" We use a time-varying VAR model to compare market efficiency and the\ngovernment's actions over time. We found the two facts by featuring the\ntime-varying nature of the market efficiency. First, the intervention with\ndiscretionary power disrupted the rice market and reduced market efficiency in\nthe exchange. Second, the market efficiency improved in accordance with\nreduction in the government's discretionary power to operate the rice policy.\nWhen the government obtained the discretionary power to operate the policy\nregarding commodity market, the market efficiency often reduced. Conversely,\neven if the government implemented a large-scale intervention, the market\nefficiency improved when the government chose a systematic rule-like behavior\nfollowing the law.\n"
    },
    {
        "paper_id": 1704.01028,
        "authors": "Matthias Raddant and Dror Y. Kenett",
        "title": "Interconnectedness in the Global Financial Market",
        "comments": null,
        "journal-ref": "Journal of International Money and Finance Volume 110, February\n  2021, 102280",
        "doi": "10.1016/j.jimonfin.2020.102280",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The global financial system is highly complex, with cross-border\ninterconnections and interdependencies. In this highly interconnected\nenvironment, local financial shocks and events can be easily amplified and\nturned into global events. This paper analyzes the dependencies among nearly\n4,000 stocks from 15 countries. The returns are normalized by the estimated\nvolatility using a GARCH model and a robust regression process estimates\npairwise statistical relationships between stocks from different markets. The\nestimation results are used as a measure of statistical interconnectedness, and\nto derive network representations, both by country and by sector. The results\nshow that countries like the United States and Germany are in the core of the\nglobal stock market. The energy, materials, and financial sectors play an\nimportant role in connecting markets, and this role has increased over time for\nthe energy and materials sectors. Our results confirm the role of global\nsectoral factors in stock market dependence. Moreover, our results show that\nthe dependencies are rather volatile and that heterogeneity among stocks is a\nnon-negligible aspect of this volatility.\n"
    },
    {
        "paper_id": 1704.01174,
        "authors": "Nonthachote Chatsanga and Andrew J. Parkes",
        "title": "Two-Stage Stochastic International Portfolio Optimisation under\n  Regular-Vine-Copula-Based Scenarios",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper, we present a two-stage stochastic international portfolio\noptimisation model to find an optimal allocation for the combination of both\nassets and currency hedging positions. Our optimisation model allows a\n\"currency overlay\", or a deviation of currency exposure from asset exposure, to\nprovide flexibility in hedging against, or in speculation using, currency\nexposure. The transaction costs associated with both trading and hedging are\nalso included.\n  To model the realistic dependence structure of the multivariate return\ndistributions, a new scenario generation method, employing a regular-vine\ncopula is developed. The use of vine copulas allows a better representation of\nthe characteristics of returns, specifically, their non-normality and\nasymmetric dependencies. It hence improves the representation of the\nuncertainty underlying decisions needed for international portfolio\noptimisation problems. Efficient portfolios optimised with scenarios generated\nfrom the new vine-copula method are compared with the portfolios from a\nstandard scenario generation method. Experimental results show that the\nproposed method, using realistic non-normal uncertainty, produces portfolios\nthat give better risk-return reward than those from a standard scenario\ngeneration approach, using normal distributions. The difference in risk-return\ncompensation is largest when the portfolios are constrained to require higher\nreturns. The paper shows that it can be important to model the non-normality in\nuncertainty, and not just assume normal distributions.\n"
    },
    {
        "paper_id": 1704.01179,
        "authors": "Valerii Salov",
        "title": "The Wandering of Corn",
        "comments": "65 pages, 35 figures, 8 tables, 101 references",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Time and Sales of corn futures traded electronically on the CME Group Globex\nare studied. Theories of continuous prices turn upside down reality of\nintra-day trading. Prices and their increments are discrete and obey lattice\nprobability distributions. A function for systematic evolution of futures\ntrading volume is proposed. Dependence between sample skewness and kurtosis of\nwaiting times does not support hypothesis of Weibull distribution. Kumaraswamy\ndistribution is more suitable for waiting times. Relationships between trading\nvolume and maximum profit strategies are presented. Frequencies of absolute\nb-increments are approximated by a Hurwitz Zeta distribution. Relative\nb-increments are non-Gaussian too. Dependence between b- and a-increments\nallows to interpret the sample variances of b-increments as a stochastic\nprocess. Mean sample variance of b-increments vs. a-increments is presented.\nThe L1 distance and Log-likelihood statistics for independence between a- and\nb-increments are controversial. Corn price jumps remind of chain branching\nreactions. Bi-logarithmic plots of the empirical frequencies of extreme\nb-increments vs. ranks are presented. Corresponding distributions resemble\nsnakes forked tongues. The maximum profit strategy is discussed as a measure of\nnon-equilibrium.\n"
    },
    {
        "paper_id": 1704.01316,
        "authors": "Dr. Pawan Kumar",
        "title": "ICT and Employment in India: A Sectoral Level Analysis",
        "comments": "21",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  How technology affects growth or employment has long been debated. With a\nhiatus, the debate revived once again in the form of how Information and\nCommunications Technology, as a form of new technology, exerts on productivity\nand employment. Information and Communications Technology perceived as General\nPurpose Technology like steam engine or electricity in the past, ushered the\nworld into a new techno-economic paradigm, given its deep social, economic and\ncultural implications. For instance, within economic implication, it is hard to\nimagine an economic activity that does not it, directly or indirectly.\nEventually, Information and Communications Technology intensity, measure as the\nratio of Information and Communications Technology investment to total\ninvestment, increased phenomenally in industries across sectors.\n"
    },
    {
        "paper_id": 1704.01366,
        "authors": "Takashi Shinzato",
        "title": "Replica Analysis for Portfolio Optimization with Single-Factor Model",
        "comments": "4 pages, no figure",
        "journal-ref": null,
        "doi": "10.7566/JPSJ.86.063802",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper, we use replica analysis to investigate the influence of\ncorrelation among the return rates of assets on the solution of the portfolio\noptimization problem. We consider the behavior of the optimal solution for the\ncase where the return rate is described with a single-factor model and compare\nthe findings obtained from our proposed methods with correlated return rates\nwith those obtained with independent return rates. We then analytically assess\nthe increase in the investment risk when correlation is included. Furthermore,\nwe also compare our approach with analytical procedures for minimizing the\ninvestment risk from operations research.\n"
    },
    {
        "paper_id": 1704.01503,
        "authors": "Klaus Herrmann, Marius Hofert, Melina Mailhot",
        "title": "Multivariate Geometric Expectiles",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  A generalization of expectiles for d-dimensional multivariate distribution\nfunctions is introduced. The resulting geometric expectiles are unique\nsolutions to a convex risk minimization problem and are given by d-dimensional\nvectors. They are well behaved under common data transformations and the\ncorresponding sample version is shown to be a consistent estimator. We\nexemplify their usage as risk measures in a number of multivariate settings,\nhighlighting the influence of varying margins and dependence structures.\n"
    },
    {
        "paper_id": 1704.01608,
        "authors": "Andreas Fr\\\"ohlich, Annegret Weng",
        "title": "Parameter uncertainty for integrated risk capital calculations based on\n  normally distributed subrisks",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this contribution we consider the overall risk given as the sum of random\nsubrisks $\\mathbf{X}_j$ in the context of value-at-risk (VaR) based risk\ncalculations. If we assume that the undertaking knows the parametric\ndistribution family subrisk $\\mathbf{X}_j=\\mathbf{X}_j(\\theta_j)$, but does not\nknow the true parameter vectors $\\theta_j$, the undertaking faces parameter\nuncertainty. To assess the appropriateness of methods to model parameter\nuncertainty for risk capital calculation we consider a criterion introduced in\nthe recent literature. According to this criterion, we demonstrate that, in\ngeneral, appropriateness of a risk capital model for each subrisk does not\nimply appropriateness of the model on the aggregate level of the overall\nrisk.\\\\ For the case where the overall risk is given by the sum of normally\ndistributed subrisks we prove a theoretical result leading to an appropriate\nintegrated risk capital model taking parameter uncertainty into account. Based\non the theorem we develop a method improving the approximation of the required\nconfidence level simultaneously for both - on the level of each subrisk as well\nas for the overall risk.\n"
    },
    {
        "paper_id": 1704.0184,
        "authors": "Claudiu Tiberiu Albulescu (UPT), Dominique P\\'epin (CRIEF), Stephen\n  Miller (WGU Nevada)",
        "title": "The micro-foundations of an open economy money demand: An application to\n  the Central and Eastern European countries",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper investigates and compares currency substitution between the\ncurrencies of Central and Eastern European (CEE) countries and the euro. In\naddition, we develop a model with microeconomic foundations, which identifies\ndifference between currency substitution and money demand sensitivity to\nexchange rate variations. More precisely, we posit that currency substitution\nrelates to money demand sensitivity to the interest rate spread between the CEE\ncountries and the euro area. Moreover, we show how the exchange rate affects\nmoney demand, even absent a currency substitution effect. This model applies to\nany country where an international currency offers liquidity services to\ndomestic agents. The model generates empirical tests of long-run money demand\nusing two complementary cointegrating equations. The opportunity cost of\nholding the money and the scale variable, either household consumption or\noutput, explain the long-run money demand in CEE countries.\n"
    },
    {
        "paper_id": 1704.02036,
        "authors": "Pablo Amster and Andres P. Mogni",
        "title": "On a pricing problem for a multi-asset option with general transaction\n  costs",
        "comments": "20 pages, 13 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We consider a Black-Scholes type equation arising on a pricing model for a\nmulti-asset option with general transaction costs. The pioneering work of\nLeland is thus extended in two different ways: on the one hand, the problem is\nmulti-dimensional since it involves different underlying assets; on the other\nhand, the transaction costs are not assumed to be constant (i.e. a fixed\nproportion of the traded quantity). In this work, we generalize Leland's\ncondition and prove the existence of a viscosity solution for the corresponding\nfully nonlinear initial value problem using Perron method. Moreover, we develop\na numerical ADI scheme to find an approximated solution. We apply this method\non a specific multi-asset derivative and we obtain the option price under\ndifferent pricing scenarios.\n"
    },
    {
        "paper_id": 1704.0216,
        "authors": "Sabrina Mulinacci",
        "title": "A systemic shock model for too big to fail financial institutions",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper we study the distributional properties of a vector of lifetimes\nin which each lifetime is modeled as the first arrival time between an\nidiosyncratic shock and a common systemic shock. Despite unlike the classical\nmultidimensional Marshall-Olkin model here only a unique common shock affecting\nall the lifetimes is assumed, some dependence is allowed between each\nidiosyncratic shock arrival time and the systemic shock arrival time. The\ndependence structure of the resulting distribution is studied through the\nanalysis of its singularity and its associated copula function. Finally, the\nmodel is applied to the analysis of the systemic riskiness of those European\nbanks classified as systemically important (SIFI).\n"
    },
    {
        "paper_id": 1704.02213,
        "authors": "Timo Dimitriadis and Sebastian Bayer",
        "title": "A Joint Quantile and Expected Shortfall Regression Framework",
        "comments": "31 pages, 3 figures",
        "journal-ref": "Electron. J. Statist. 13 (2019), no. 1, 1823--1871",
        "doi": "10.1214/19-EJS1560",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We introduce a novel regression framework which simultaneously models the\nquantile and the Expected Shortfall (ES) of a response variable given a set of\ncovariates. This regression is based on a strictly consistent loss function for\nthe pair quantile and ES, which allows for M- and Z-estimation of the joint\nregression parameters. We show consistency and asymptotic normality for both\nestimators under weak regularity conditions. The underlying loss function\ndepends on two specification functions, whose choice affects the properties of\nthe resulting estimators. We find that the Z-estimator is numerically unstable\nand thus, we rely on M-estimation of the model parameters. Extensive\nsimulations verify the asymptotic properties and analyze the small sample\nbehavior of the M-estimator for different specification functions. This joint\nregression framework allows for various applications including estimating,\nforecasting, and backtesting ES, which is particularly relevant in light of the\nrecent introduction of ES into the Basel Accords.\n"
    },
    {
        "paper_id": 1704.02377,
        "authors": "Zhiyuan Liu and R. A. Serota",
        "title": "On absence of steady state in the Bouchaud-M\\'ezard network model",
        "comments": "6 pages, 4 figures",
        "journal-ref": "Physica A: Statistical Mechanics and its Applications, Volume 491,\n  1 February 2018, Pages 391-398",
        "doi": "10.1016/j.physa.2017.09.076",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In the limit of infinite number of nodes (agents), the It\\^o-reduced\nBouchaud-M\\'ezard network model of economic exchange has a time-independent\nmean and a steady-state inverse gamma distribution. We show that for a finite\nnumber of nodes the mean is actually distributed as a time-dependent lognormal\nand inverse gamma is quasi-stationary, with the time-dependent scale parameter.\n"
    },
    {
        "paper_id": 1704.02392,
        "authors": "Jennifer Jhun, Patricia Palacios, James Owen Weatherall",
        "title": "Market Crashes as Critical Phenomena? Explanation, Idealization, and\n  Universality in Econophysics",
        "comments": "32 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We study the Johansen-Ledoit-Sornette (JLS) model of financial market crashes\n(Johansen, Ledoit, and Sornette [2000] \"Crashes as Critical Points.\" Int. J.\nTheor. Appl. Finan. 3(2) 219-255). On our view, the JLS model is a curious case\nfrom the perspective of the recent philosophy of science literature, as it is\nnaturally construed as a \"minimal model\" in the sense of Batterman and Rice\n(Batterman and Rice [2014] \"Minimal Model Explanations.\" Phil. Sci. 81(3):\n349-376) that nonetheless provides a causal explanation of market crashes, in\nthe sense of Woodward's interventionist account of causation (Woodward [2003].\nMaking Things Happen. Oxford:Oxford University Press).\n"
    },
    {
        "paper_id": 1704.02505,
        "authors": "Dirk Becherer and Klebert Kentia",
        "title": "Good Deal Hedging and Valuation under Combined Uncertainty about Drift\n  and Volatility",
        "comments": "26 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We study robust notions of good-deal hedging and valuation under combined\nuncertainty about the drifts and volatilities of asset prices. Good-deal bounds\nare determined by a subset of risk-neutral pricing measures such that not only\nopportunities for arbitrage are excluded but also deals that are too good, by\nrestricting instantaneous Sharpe ratios. A non-dominated multiple priors\napproach to model uncertainty (ambiguity) leads to worst-case good-deal bounds.\nCorresponding hedging strategies arise as minimizers of a suitable coherent\nrisk measure. Good-deal bounds and hedges for measurable claims are\ncharacterized by solutions to second-order backward stochastic differential\nequations whose generators are non-convex in the volatility. These hedging\nstrategies are robust with respect to uncertainty in the sense that their\ntracking errors satisfy a supermartingale property under all a-priori valuation\nmeasures, uniformly over all priors.\n"
    },
    {
        "paper_id": 1704.02638,
        "authors": "Michael Benzaquen and Jean-Philippe Bouchaud",
        "title": "A fractional reaction-diffusion description of supply and demand",
        "comments": "7 pages, 2 figures",
        "journal-ref": null,
        "doi": "10.1140/epjb/e2017-80246-9",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We suggest that the broad distribution of time scales in financial markets\ncould be a crucial ingredient to reproduce realistic price dynamics in stylised\nAgent-Based Models. We propose a fractional reaction-diffusion model for the\ndynamics of latent liquidity in financial markets, where agents are very\nheterogeneous in terms of their characteristic frequencies. Several features of\nour model are amenable to an exact analytical treatment. We find in particular\nthat the impact is a concave function of the transacted volume (aka the\n\"square-root impact law\"), as in the normal diffusion limit. However, the\nimpact kernel decays as $t^{-\\beta}$ with $\\beta=1/2$ in the diffusive case,\nwhich is inconsistent with market efficiency. In the sub-diffusive case the\ndecay exponent $\\beta$ takes any value in $[0,1/2]$, and can be tuned to match\nthe empirical value $\\beta \\approx 1/4$. Numerical simulations confirm our\ntheoretical results. Several extensions of the model are suggested.\n"
    },
    {
        "paper_id": 1704.0311,
        "authors": "Patrick S. Hagan and Andrew Lesniewski",
        "title": "Bartlett's delta in the SABR model",
        "comments": "10 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We refine the analysis of hedging strategies for options under the SABR model\ncarried out in [2]. In particular, we provide a theoretical justification of\nthe empirical observation made in [2] that the modified delta (\"Bartlett's\ndelta\") introduced there provides a more accurate and robust hedging strategy\nthan the conventional SABR delta hedge.\n"
    },
    {
        "paper_id": 1704.03205,
        "authors": "Luigi Troiano and Elena Mejuto and Pravesh Kriplani",
        "title": "On Feature Reduction using Deep Learning for Trend Prediction in Finance",
        "comments": "6 pages, 6 figures, 5 tables",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  One of the major advantages in using Deep Learning for Finance is to embed a\nlarge collection of information into investment decisions. A way to do that is\nby means of compression, that lead us to consider a smaller feature space.\nSeveral studies are proving that non-linear feature reduction performed by Deep\nLearning tools is effective in price trend prediction. The focus has been put\nmainly on Restricted Boltzmann Machines (RBM) and on output obtained by them.\nFew attention has been payed to Auto-Encoders (AE) as an alternative means to\nperform a feature reduction. In this paper we investigate the application of\nboth RBM and AE in more general terms, attempting to outline how architectural\nand input space characteristics can affect the quality of prediction.\n"
    },
    {
        "paper_id": 1704.03244,
        "authors": "Michele Bonollo, Luca Di Persio, Luca Mammi, Immacolata Oliva",
        "title": "Estimating the Counterparty Risk Exposure by using the Brownian Motion\n  Local Time",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In recent years, the counterparty credit risk measure, namely the default\nrisk in \\emph{Over The Counter} (OTC) derivatives contracts, has received great\nattention by banking regulators, specifically within the frameworks of\n\\emph{Basel II} and \\emph{Basel III.} More explicitly, to obtain the related\nrisk figures, one has first obliged to compute intermediate output functionals\nrelated to the \\emph{Mark-to-Market} (MtM) position at a given time $t \\in [0,\nT],$ T being a positive, and finite, time horizon. The latter implies an\nenormous amount of computational effort is needed, with related highly time\nconsuming procedures to be carried out, turning out into significant costs. To\novercome latter issue, we propose a smart exploitation of the properties of the\n(local) time spent by the Brownian motion close to a given value.\n"
    },
    {
        "paper_id": 1704.03597,
        "authors": "JongRoul Woo and Christopher L. Magee",
        "title": "Exploring the relationship between technological improvement and\n  innovation diffusion: An empirical test",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Different technological domains have significantly different rates of\nperformance improvement. Prior theory indicates that such differing rates\nshould influence the relative speed of diffusion of the products embodying the\ndifferent technologies since improvement in performance during the diffusion\nprocess increases the desirability of the product diffusing. However, there has\nnot been a broad empirical attempt to examine this effect and to clarify the\nunderlying cause. Therefore, this paper reviews the theoretical basis and\nfocuses upon empirical tests of this effect across multiple products and their\nunderlying technologies. The results for 18 different diffusing products show\nthe expected relationship-faster diffusion for products based on more rapidly\nimproving technological domains- between technological improvement and\ndiffusion with strong statistical significance. The empirical examination also\ndemonstrates that technological improvement does not slow down in the latter\nparts of diffusion when penetration does slow down. This finding indicates that\ndiffusion slow down in the latter stages is due to market saturation effects\nand is not due to slowdown of performance improvement.\n"
    },
    {
        "paper_id": 1704.04354,
        "authors": "Gao-Feng Gu and Xiong Xiong and Hai-Chuan Xu and Wei Zhang and\n  Yong-Jie Zhang and Wei Chen and Wei-Xing Zhou",
        "title": "An empirical behavioural order-driven model with price limit rules",
        "comments": "19 pages, 8 figures and 7 tables",
        "journal-ref": "Financial Innovation 7 (1), 70 (2021)",
        "doi": "10.1186/s40854-021-00288-4",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We develop an empirical behavioural order-driven (EBOD) model, which consists\nof an order placement process and an order cancellation process. Price limit\nrules are introduced in the definition of relative price. The order placement\nprocess is determined by several empirical regularities: the long memory in\norder directions, the long memory in relative prices, the asymmetric\ndistribution of relative prices, and the nonlinear dependence of the average\norder size and its standard deviation on the relative price. Order cancellation\nfollows a Poisson process with the arrival rate determined from real data and\nthe cancelled order is determined according to the empirical distributions of\nrelative price level and relative position at the same price level. All these\ningredients of the model are derived based on the empirical microscopic\nregularities in the order flows of stocks on the Shenzhen Stock Exchange. The\nmodel is able to produce the main stylized facts in real markets. Computational\nexperiments uncover that asymmetric setting of price limits will cause the\nstock price diverging exponentially when the up price limit is higher than the\ndown price limit and vanishing vice versus. We also find that asymmetric price\nlimits have influences on stylized facts. Our EBOD model provides a suitable\ncomputational experiment platform for academics, market participants and policy\nmakers.\n"
    },
    {
        "paper_id": 1704.04442,
        "authors": "Aurelio F. Bariviera, Luciano Zunino, Osvaldo A. Rosso",
        "title": "Crude oil market and geopolitical events: an analysis based on\n  information-theory-based quantifiers",
        "comments": "arXiv admin note: text overlap with arXiv:1603.02874",
        "journal-ref": "Fuzzy Economic Review, 2016, 21(1),41-51",
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper analyzes the informational efficiency of oil market during the\nlast three decades, and examines changes in informational efficiency with major\ngeopolitical events, such as terrorist attacks, financial crisis and other\nimportant events. The series under study is the daily prices of West Texas\nIntermediate (WTI) in USD/BBL, commonly used as a benchmark in oil pricing. The\nanalysis is performed using information-theory-derived quantifiers, namely\npermutation entropy and permutation statistical complexity. These metrics allow\ncapturing the hidden structure in the market dynamics, and allow discriminating\ndifferent degrees of informational efficiency. We find that some geopolitical\nevents impact on the underlying dynamical structure of the market.\n"
    },
    {
        "paper_id": 1704.0445,
        "authors": "Laura Cristina Lanzarini, Augusto Villa Monte, Aurelio F. Bariviera,\n  Patricia Jimbo Santana",
        "title": "Simplifying credit scoring rules using LVQ+PSO",
        "comments": null,
        "journal-ref": "Kybernetes, Vol. 46 Iss 1 pp. 8 - 16 (2017)",
        "doi": "10.1108/K-06-2016-0158",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  One of the key elements in the banking industry rely on the appropriate\nselection of customers. In order to manage credit risk, banks dedicate special\nefforts in order to classify customers according to their risk. The usual\ndecision making process consists in gathering personal and financial\ninformation about the borrower. Processing this information can be time\nconsuming, and presents some difficulties due to the heterogeneous structure of\ndata. We offer in this paper an alternative method that is able to classify\ncustomers' profiles from numerical and nominal attributes. The key feature of\nour method, called LVQ+PSO, is the finding of a reduced set of classifying\nrules. This is possible, due to the combination of a competitive neural network\nwith an optimization technique. These rules constitute a predictive model for\ncredit risk approval. The reduced quantity of rules makes this method not only\nuseful for credit officers aiming to make quick decisions about granting a\ncredit, but also could act as borrower's self selection. Our method was applied\nto an actual database of a credit consumer financial institution in Ecuador. We\nobtain very satisfactory results. Future research lines are exposed.\n"
    },
    {
        "paper_id": 1704.04524,
        "authors": "Sebastian Herrmann, Johannes Muhle-Karbe",
        "title": "Model Uncertainty, Recalibration, and the Emergence of Delta-Vega\n  Hedging",
        "comments": "44 pages; forthcoming in 'Finance and Stochastics'",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We study option pricing and hedging with uncertainty about a Black-Scholes\nreference model which is dynamically recalibrated to the market price of a\nliquidly traded vanilla option. For dynamic trading in the underlying asset and\nthis vanilla option, delta-vega hedging is asymptotically optimal in the limit\nfor small uncertainty aversion. The corresponding indifference price\ncorrections are determined by the disparity between the vegas, gammas, vannas,\nand volgas of the non-traded and the liquidly traded options.\n"
    },
    {
        "paper_id": 1704.04979,
        "authors": "Vahid Moosavi",
        "title": "Urban Data Streams and Machine Learning: A Case of Swiss Real Estate\n  Market",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper, we show how using publicly available data streams and machine\nlearning algorithms one can develop practical data driven services with no\ninput from domain experts as a form of prior knowledge. We report the initial\nsteps toward development of a real estate portal in Switzerland. Based on\ncontinuous web crawling of publicly available real estate advertisements and\nusing building data from Open Street Map, we developed a system, where we\nroughly estimate the rental and sale price indexes of 1.7 million buildings\nacross the country. In addition to these rough estimates, we developed a web\nbased API for accurate automated valuation of rental prices of individual\nproperties and spatial sensitivity analysis of rental market. We tested several\nestablished function approximation methods against the test data to check the\nquality of the rental price estimations and based on our experiments, Random\nForest gives very reasonable results with the median absolute relative error of\n6.57 percent, which is comparable with the state of the art in the industry. We\nargue that while recently there have been successful cases of real estate\nportals, which are based on Big Data, majority of the existing solutions are\nexpensive, limited to certain users and mostly with non-transparent underlying\nsystems. As an alternative we discuss, how using the crawled data sets and\nother open data sets provided from different institutes it is easily possible\nto develop data driven services for spatial and temporal sensitivity analysis\nin the real estate market to be used for different stakeholders. We believe\nthat this kind of digital literacy can disrupt many other existing business\nconcepts across many domains.\n"
    },
    {
        "paper_id": 1704.05015,
        "authors": "Mario Coccia",
        "title": "Measurement of Economic Growth, Development and Under Development: New\n  Model and Application",
        "comments": "24",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper presents a simple model to measure the relative economic growth of\neconomic systems. The model considers S-Shaped patterns of economic growth\nthat, represented with a linear model, measure how an economic system grows in\ncomparison with another one. In particular, this model introduces an approach\nwhich indicates if the economic system has a process of economic growth,\ndevelopment or under development. The application of the model is provided for\nregions and macro regions of the Italian economic system.\n"
    },
    {
        "paper_id": 1704.05276,
        "authors": "Marco Pangallo, Torsten Heinrich, J Doyne Farmer",
        "title": "Best reply structure and equilibrium convergence in generic games",
        "comments": "Main paper + Supplemental Information",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Game theory is widely used as a behavioral model for strategic interactions\nin biology and social science. It is common practice to assume that players\nquickly converge to an equilibrium, e.g. a Nash equilibrium. This can be\nstudied in terms of best reply dynamics, in which each player myopically uses\nthe best response to her opponent's last move. Existing research shows that\nconvergence can be problematic when there are best reply cycles. Here we\ncalculate how typical this is by studying the space of all possible two-player\nnormal form games and counting the frequency of best reply cycles. The two key\nparameters are the number of moves, which defines how complicated the game is,\nand the anti-correlation of the payoffs, which determines how competitive it\nis. We find that as games get more complicated and more competitive, best reply\ncycles become dominant. The existence of best reply cycles predicts\nnon-convergence of six different learning algorithms that have support from\nhuman experiments. Our results imply that for complicated and competitive games\nequilibrium is typically an unrealistic assumption. Alternatively, if for some\nreason \"real\" games are special and do not possess cycles, we raise the\ninteresting question of why this should be so.\n"
    },
    {
        "paper_id": 1704.05308,
        "authors": "Bertram D\\\"uring, Alexander Pitkin",
        "title": "High-order compact finite difference scheme for option pricing in\n  stochastic volatility jump models",
        "comments": "21 pages, 9 figures",
        "journal-ref": "J. Comput. Appl. Math. 355 (2019), 201-217",
        "doi": "10.1016/j.cam.2019.01.043",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We derive a new high-order compact finite difference scheme for option\npricing in stochastic volatility jump models, e.g. in Bates model. In such\nmodels the option price is determined as the solution of a partial\nintegro-differential equation. The scheme is fourth order accurate in space and\nsecond order accurate in time. Numerical experiments for the European option\npricing problem are presented. We validate the stability of the scheme\nnumerically and compare its performance to standard finite difference and\nfinite element methods. The new scheme outperforms a standard discretisation\nbased on a second-order central finite difference approximation in all our\nexperiments. At the same time, it is very efficient, requiring only one initial\n$LU$-factorisation of a sparse matrix to perform the option price valuation.\nCompared to finite element approaches, it is very parsimonious in terms of\nmemory requirements and computational effort, since it achieves high-order\nconvergence without requiring additional unknowns, unlike finite element\nmethods with higher polynomial order basis functions. The new high-order\ncompact scheme can also be useful to upgrade existing implementations based on\nstandard finite differences in a straightforward manner to obtain a highly\nefficient option pricing code.\n"
    },
    {
        "paper_id": 1704.05332,
        "authors": "Mihaly Ormos and Dusan Timotity",
        "title": "The case of 'Less is more': Modelling risk-preference with Expected\n  Downside Risk",
        "comments": "34 pages, 8 figures and 2 tables",
        "journal-ref": "The B.E. Journal of Theoretical Economics (2017)",
        "doi": "10.1515/bejte-2016-0100",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper discusses an alternative explanation for the empirical findings\ncontradicting the positive relationship between risk (variance) and reward\n(expected return). We show that these contradicting results might be due to the\nfalse definition of risk-perception, which we correct by introducing Expected\nDownside Risk (EDR). The EDR parameter, similar to the Expected Shortfall or\nConditional Value-at-Risk, measures the tail risk, however, fits and better\nexplains the utility perception of investors. Our results indicate that when\nusing the EDR as risk measure, both the positive and negative relationship\nbetween expected return and risk can be derived under standard conditions (e.g.\nexpected utility theory and positive risk-aversion). Therefore, no alternative\npsychological explanation or additional boundary condition on utility theory is\nrequired to explain the phenomenon. Furthermore, we show empirically that it is\na more precise linear predictor of expected return than volatility, both for\nindividual assets and portfolios.\n"
    },
    {
        "paper_id": 1704.05499,
        "authors": "Bruna Amin Gon\\c{c}alves and Laura Carpi and Osvaldo A. Rosso and\n  Martin G. Ravetti and A.P.F Atman",
        "title": "Quantifying instabilities in Financial Markets",
        "comments": "5 pages, 3 figures",
        "journal-ref": null,
        "doi": "10.1016/j.physa.2019.03.029",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Financial global crisis has devastating impacts to economies since early XX\ncentury and continues to impose increasing collateral damages for governments,\nenterprises, and society in general. Up to now, all efforts to obtain efficient\nmethods to predict these events have been disappointing. However, the quest for\na robust estimator of the degree of the market efficiency, or even, a crisis\npredictor, is still one of the most studied subjects in the field. We present\nhere an original contribution that combines Information Theory with graph\nconcepts, to study the return rate series of 32 global trade markets.\nSpecifically, we propose a very simple quantifier that shows to be highly\ncorrelated with global financial instability periods, being also a good\nestimator of the market crisis risk and market resilience. We show that this\nestimator displays striking results when applied to countries that played\ncentral roles during the last major global market crisis. The simplicity and\neffectiveness of our quantifier allow us to anticipate its use in a wide range\nof disciplines.\n"
    },
    {
        "paper_id": 1704.05729,
        "authors": "Rahul Madhavan and Ankit Baraskar",
        "title": "A generalized Bayesian framework for the analysis of subscription based\n  businesses",
        "comments": "12 pages, 4 figures, Atidiv Research",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  We have created a framework for analyzing subscription based businesses in\nterms of a unified metric which we call SCV (single customer value). The major\nadvance in this paper is to model customer churn as an exponential decay\nvariable, which directly follows from experimental data relating to\nsubscription based businesses. This Bayesian probabilistic model was used to\ncompute an expected value for the revenue contribution of a single user. We\nobtain an exact closed-form solution for the constant churn model, and an\napproximate closed-form solution for the exponential decay model. In addition,\nwe define a general methodology for decision making processes using sensitivity\nanalysis of the model equation, which we illustrate with a real-life case study\nfor a food based subscription business.\n"
    },
    {
        "paper_id": 1704.05818,
        "authors": "Lijian Chen, Kevin E. Bassler, Joseph L. McCauley, and Gemunu H.\n  Gunaratne",
        "title": "Anomalous Scaling of Stochastic Processes and the Moses Effect",
        "comments": "13 pages, 4 figures, Supplementary Info provided by request to the\n  authors",
        "journal-ref": null,
        "doi": "10.1103/PhysRevE.95.042141",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The state of a stochastic process evolving over a time $t$ is typically\nassumed to lie on a normal distribution whose width scales like $t^{1/2}$.\nHowever, processes where the probability distribution is not normal and the\nscaling exponent differs from $\\frac{1}{2}$ are known. The search for possible\norigins of such \"anomalous\" scaling and approaches to quantify them are the\nmotivations for the work reported here. In processes with stationary\nincrements, where the stochastic process is time-independent, auto-correlations\nbetween increments and infinite variance of increments can cause anomalous\nscaling. These sources have been referred to as the $\\it{Joseph}$ $\\it{effect}$\nthe $\\it{Noah}$ $\\it{effect}$, respectively. If the increments are\nnon-stationary, then scaling of increments with $t$ can also lead to anomalous\nscaling, a mechanism we refer to as the $\\it{Moses}$ $\\it{effect}$. Scaling\nexponents quantifying the three effects are defined and related to the Hurst\nexponent that characterizes the overall scaling of the stochastic process.\nMethods of time series analysis that enable accurate independent measurement of\neach exponent are presented. Simple stochastic processes are used to illustrate\neach effect. Intraday Financial time series data is analyzed, revealing that\nits anomalous scaling is due only to the Moses effect. In the context of\nfinancial market data, we reiterate that the Joseph exponent, not the Hurst\nexponent, is the appropriate measure to test the efficient market hypothesis.\n"
    },
    {
        "paper_id": 1704.06027,
        "authors": "Clemence Alasseur and Olivier Feron",
        "title": "Structural price model for electricity coupled markets",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We propose a new structural model that can compute the electricity spot and\nforward prices in two coupled markets with limited interconnection and multiple\nfuels. We choose a structural approach in order to represent some key\ncharacteristics of electricity spot prices such as their link to fuel prices,\nconsumption level and production fleet. With this model, explicit formulas are\nalso available for forward prices and other derivatives. We give some\nillustrative results of the behaviour of spot and forward prices, and of the\nvalues of transmission rights.\n"
    },
    {
        "paper_id": 1704.06388,
        "authors": "Ralph Rudd, Thomas A. McWalter, Joerg Kienitz, Eckhard Platen",
        "title": "Fast Quantization of Stochastic Volatility Models",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Recursive Marginal Quantization (RMQ) allows fast approximation of solutions\nto stochastic differential equations in one-dimension. When applied to two\nfactor models, RMQ is inefficient due to the fact that the optimization problem\nis usually performed using stochastic methods, e.g., Lloyd's algorithm or\nCompetitive Learning Vector Quantization. In this paper, a new algorithm is\nproposed that allows RMQ to be applied to two-factor stochastic volatility\nmodels, which retains the efficiency of gradient-descent techniques. By\nmargining over potential realizations of the volatility process, a significant\ndecrease in computational effort is achieved when compared to current\nquantization methods. Additionally, techniques for modelling the correct\nzero-boundary behaviour are used to allow the new algorithm to be applied to\ncases where the previous methods would fail. The proposed technique is\nillustrated for European options on the Heston and Stein-Stein models, while a\nmore thorough application is considered in the case of the popular SABR model,\nwhere various exotic options are also priced.\n"
    },
    {
        "paper_id": 1704.06429,
        "authors": "Henri Benisty",
        "title": "Simple wealth distribution model causing inequality-induced crisis\n  without external shocks",
        "comments": "15 pages, 11 figures. Work initiated from discussion on Aristotle's\n  status revisited by Paul Jorion in the many cases where the law of supply and\n  demand fails. Accepted for publication in Physical Review E on April 19, 2017",
        "journal-ref": "Phys. Rev. E 95, 052307 (2017)",
        "doi": "10.1103/PhysRevE.95.052307",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We address the issue of the dynamics of wealth accumulation and economic\ncrisis triggered by extreme inequality, attempting to stick to most possibly\nintrinsic assumptions. Our general framework is that of pure or modified\nmultiplicative processes, basically geometric Brownian motions. In contrast\nwith the usual approach of injecting into such stochastic agent models either\nspecific, idiosyncratic internal nonlinear interaction patterns, or macroscopic\ndisruptive features, we propose a dynamic inequality model where the attainment\nof a sizable fraction of the total wealth by very few agents induces a crisis\nregime with strong intermittency, the explicit coupling between the richest and\nthe rest being a mere normalization mechanism, hence with minimal extrinsic\nassumptions. The model thus harnesses the recognized lack of ergodicity of\ngeometric Brownian motions. It also provides a statistical intuition to the\nconsequences of Thomas Piketty's recent \"$r>g$\" (return rate $>$ growth rate)\nparadigmatic analysis of very-long-term wealth trends. We suggest that the\n\"water-divide\" of wealth flow may define effective classes, making an objective\nentry point to calibrate the model. Consistently, we check that a tax mechanism\nassociated to a few percent relative bias on elementary daily transactions is\nable to slow or stop the build-up of large wealth. When extreme fluctuations\nare tamed down to a stationary regime with sizable but steadier inequalities,\nit should still offer opportunities to study the dynamics of crisis and the\ninner effective classes induced through external or internal factors.\n"
    },
    {
        "paper_id": 1704.06508,
        "authors": "R\\'emi Lemoy and Geoffrey Caruso",
        "title": "Scaling evidence of the homothetic nature of cities",
        "comments": "19 pages, 11 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper we analyse the profile of land use and population density with\nrespect to the distance to the city centre for the European city. In addition\nto providing the radial population density and soil-sealing profiles for a\nlarge set of cities, we demonstrate a remarkable constancy of the profiles\nacross city size.\n  Our analysis combines the GMES/Copernicus Urban Atlas 2006 land use database\nat 5m resolution for 300 European cities with more than 100.000 inhabitants and\nthe Geostat population grid at 1km resolution. Population is allocated\nproportionally to surface and weighted by soil sealing and density classes of\nthe Urban Atlas. We analyse the profile of each artificial land use and\npopulation with distance to the town hall.\n  In line with earlier literature, we confirm the strong monocentricity of the\nEuropean city and the negative exponential curve for population density.\nMoreover, we find that land use curves, in particular the share of housing and\nroads, scale along the two horizontal dimensions with the square root of city\npopulation, while population curves scale in three dimensions with the cubic\nroot of city population. In short, European cities of different sizes are\nhomothetic in terms of land use and population density. While earlier\nliterature documented the scaling of average densities (total surface and\npopulation) with city size, we document the scaling of the whole radial\ndistance profile with city size, thus liaising intra-urban radial analysis and\nsystems of cities. In addition to providing a new empirical view of the\nEuropean city, our scaling offers a set of practical and coherent definitions\nof a city, independent of its population, from which we can re-question urban\nscaling laws and Zipf's law for cities.\n"
    },
    {
        "paper_id": 1704.0655,
        "authors": "Vitalii Makogin, Alexander Melnikov, Yuliya Mishura",
        "title": "On mean-variance hedging under partial observations and terminal wealth\n  constraints",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In the paper, a mean-square minimization problem under terminal wealth\nconstraint with partial observations is studied. The problem is naturally\nconnected to the mean-variance hedging problem under incomplete information. A\nnew approach to solving this problem is proposed. The paper provides a solution\nwhen the underlying pricing process is a square-integrable semimartingale. The\nproposed method for the study is based on the martingale representation. In\nspecial cases, the Clark-Ocone representation can be used to obtain explicit\nsolutions. The results and the method are illustrated and supported by example\nwith two correlated geometric Brownian motions.\n"
    },
    {
        "paper_id": 1704.06572,
        "authors": "Jonathan A. Ch\\'avez-Casillas and Robert J. Elliott and Bruno\n  R\\'emillard and Anatoliy V. Swishchuk",
        "title": "A level-1 Limit Order book with time dependent arrival rates",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We propose a simple stochastic model for the dynamics of a limit order book,\nextending the recent work of Cont and de Larrard (2013), where the price\ndynamics are endogenous, resulting from market transactions. We also show that\nthe conditional diffusion limit of the price process is the so-called Brownian\nmeander.\n"
    },
    {
        "paper_id": 1704.06697,
        "authors": "S\\\"uhan Altay, Katia Colaneri and Zehra Eksi",
        "title": "Pairs Trading under Drift Uncertainty and Risk Penalization",
        "comments": "24 pages, 4 figures",
        "journal-ref": null,
        "doi": "10.1142/S0219024918500462",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this work, we study a dynamic portfolio optimization problem related to\npairs trading, which is an investment strategy that matches a long position in\none security with a short position in another security with similar\ncharacteristics. The relationship between pairs, called a spread, is modeled by\na Gaussian mean-reverting process whose drift rate is modulated by an\nunobservable continuous-time, finite-state Markov chain. Using the classical\nstochastic filtering theory, we reduce this problem with partial information to\nthe one with full information and solve it for the logarithmic utility\nfunction, where the terminal wealth is penalized by the riskiness of the\nportfolio according to the realized volatility of the wealth process. We\ncharacterize optimal dollar-neutral strategies as well as optimal value\nfunctions under full and partial information and show that the certainty\nequivalence principle holds for the optimal portfolio strategy. Finally, we\nprovide a numerical analysis for a toy example with a two-state Markov chain.\n"
    },
    {
        "paper_id": 1704.06791,
        "authors": "Opeoluwa Banwo, Fabio Caccioli, Paul Harrald, Francesca Medda",
        "title": "The effect of heterogeneity on financial contagion due to overlapping\n  portfolios",
        "comments": "22 pages, 19 figures",
        "journal-ref": "Advances in Complex Systems 19, 1650016 (2016)",
        "doi": "10.1142/S0219525916500168,",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We consider a model of financial contagion in a bipartite network of assets\nand banks recently introduced in the literature, and we study the effect of\npower law distributions of degree and balance-sheet size on the stability of\nthe system. Relative to the benchmark case of banks with homogeneous degrees\nand balance-sheet sizes, we find that if banks have a power-law degree\ndistribution the system becomes less robust with respect to the initial failure\nof a random bank, and that targeted shocks to the most specialised banks (i.e.\nbanks with low degrees) or biggest banks increases the probability of observing\na cascade of defaults. In contrast, we find that a power-law degree\ndistribution for assets increases stability with respect to random shocks, but\nnot with respect to targeted shocks. We also study how allocations of capital\nbuffers between banks affects the system's stability, and we find that\nassigning capital to banks in relation to their level of diversification\nreduces the probability of observing cascades of defaults relative to size\nbased allocations. Finally, we propose a non-capital based policy that improves\nthe resilience of the system by introducing disassortative mixing between banks\nand assets.\n"
    },
    {
        "paper_id": 1704.07152,
        "authors": "V\\'eronique Maume-Deschamps (1), Didier Rulli\\`ere (2), Khalil Said\n  ((1) ICJ (2) SAF)",
        "title": "Asymptotic multivariate expectiles",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In [16], a new family of vector-valued risk measures called multivariate\nexpectiles is introduced. In this paper, we focus on the asymptotic behavior of\nthese measures in a multivariate regular variations context. For models with\nequivalent tails, we propose an estimator of these multivariate asymptotic\nexpectiles, in the Fr{\\'e}chet attraction domain case, with asymptotic\nindependence, or in the comonotonic case.\n"
    },
    {
        "paper_id": 1704.07235,
        "authors": "Umberto Cherubini, Paolo Neri",
        "title": "Value-at-Risk Diversification of $\\alpha$-stable Risks: The\n  Tail-Dependence Puzzle",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We consider the problem of risk diversification of $\\alpha$-stable heavy\ntailed risks. We study the behaviour of the aggregated Value-at-Risk, with\nparticular reference to the impact of different tail dependence structures on\nthe limits to diversification. We confirm the large evidence of sub-additivity\nviolations, particularly for risks with low tail index values and positive\ndependence. So, reinsurance strategies are not allowed to exploit\ndiversification gains, or only a very limited amount of them. Concerning the\nimpact of tail dependence, we find the peculiar results that for high tail\ndependence levels the limits to diversification are uniformly lower for all the\nlevels of dependence, and for all levels of $\\alpha<2$. The result is confirmed\nas we move towards extreme points in the tail: in this case, we show that at\nsome point in the tail the aggregated VaR becomes additive above some level of\ndependence, but this critical dependence level is lower for copulas with lower\ntail dependence.\n"
    },
    {
        "paper_id": 1704.07321,
        "authors": "Andrei Cozma, Christoph Reisinger",
        "title": "Strong order 1/2 convergence of full truncation Euler approximations to\n  the Cox-Ingersoll-Ross process",
        "comments": "16 pages, 1 figure",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We study convergence properties of the full truncation Euler scheme for the\nCox-Ingersoll-Ross process in the regime where the boundary point zero is\ninaccessible. Under some conditions on the model parameters (precisely, when\nthe Feller ratio is greater than three), we establish the strong order 1/2\nconvergence in $L^{p}$ of the scheme to the exact solution. This is consistent\nwith the optimal rate of strong convergence for Euler approximations of\nstochastic differential equations with globally Lipschitz coefficients, despite\nthe fact that the diffusion coefficient in the Cox-Ingersoll-Ross model is not\nLipschitz.\n"
    },
    {
        "paper_id": 1704.07597,
        "authors": "Tushar Vaidya and Carlos Murguia and Georgios Piliouras",
        "title": "Learning Agents in Black-Scholes Financial Markets: Consensus Dynamics\n  and Volatility Smiles",
        "comments": "19 pages, 3 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Black-Scholes (BS) is the standard mathematical model for option pricing in\nfinancial markets. Option prices are calculated using an analytical formula\nwhose main inputs are strike (at which price to exercise) and volatility. The\nBS framework assumes that volatility remains constant across all strikes,\nhowever, in practice it varies. How do traders come to learn these parameters?\nWe introduce natural models of learning agents, in which they update their\nbeliefs about the true implied volatility based on the opinions of other\ntraders. We prove convergence of these opinion dynamics using techniques from\ncontrol theory and leader-follower models, thus providing a resolution between\ntheory and market practices. We allow for two different models, one with\nfeedback and one with an unknown leader.\n"
    },
    {
        "paper_id": 1704.08161,
        "authors": "Adam B. Barrett",
        "title": "Stability of zero-growth economics analysed with a Minskyan model",
        "comments": "23 pages, 5 figures",
        "journal-ref": "Barrett, A.B. (2018). Stability of zero-growth economics analysed\n  with a Minskyan model. Ecol. Econ. 146: 228-239",
        "doi": "10.1016/j.ecolecon.2017.10.014",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  As humanity is becoming increasingly confronted by Earth's finite biophysical\nlimits, there is increasing interest in questions about the stability and\nequitability of a zero-growth capitalist economy, most notably: if one\nmaintains a positive interest rate for loans, can a zero-growth economy be\nstable? This question has been explored on a few different macroeconomic\nmodels, and both `yes' and `no' answers have been obtained. However, economies\ncan become unstable whether or not there is ongoing underlying growth in\nproductivity with which to sustain growth in output. Here we attempt, for the\nfirst time, to assess via a model the relative stability of growth versus\nno-growth scenarios. The model employed draws from Keen's model of the Minsky\nfinancial instability hypothesis. The analysis focuses on dynamics as opposed\nto equilibrium, and scenarios of growth and no-growth of output (GDP) are\nobtained by tweaking a productivity growth input parameter. We confirm that,\nwith or without growth, there can be both stable and unstable scenarios. To\nmaintain stability, firms must not change their debt levels or target debt\nlevels too quickly. Further, according to the model, the wages share is higher\nfor zero-growth scenarios, although there are more frequent substantial drops\nin employment.\n"
    },
    {
        "paper_id": 1704.08175,
        "authors": "Olivier Scaillet, Adrien Treccani, Christopher Trevisan",
        "title": "High-Frequency Jump Analysis of the Bitcoin Market",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We use the database leak of Mt. Gox exchange to analyze the dynamics of the\nprice of bitcoin from June 2011 to November 2013. This gives us a rare\nopportunity to study an emerging retail-focused, highly speculative and\nunregulated market with trader identifiers at a tick transaction level. Jumps\nare frequent events and they cluster in time. The order flow imbalance and the\npreponderance of aggressive traders, as well as a widening of the bid-ask\nspread predict them. Jumps have short-term positive impact on market activity\nand illiquidity and see a persistent change in the price.\n"
    },
    {
        "paper_id": 1704.08234,
        "authors": "Nian Yao and Zhiming Yang",
        "title": "Optimal excess-of-loss reinsurance and investment problem for an insurer\n  with default risk under a stochastic volatility model",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper, we study an optimal excess-of-loss reinsurance and investment\nproblem for an insurer in defaultable market. The insurer can buy reinsurance\nand invest in the following securities: a bank account, a risky asset with\nstochastic volatility and a defaultable corporate bond. We discuss the optimal\ninvestment strategy into two subproblems: a pre-default case and a post-default\ncase. We show the existence of a classical solution to a pre-default case via\nsuper-sub solution techniques and give an explicit characterization of the\noptimal reinsurance and investment policies that maximize the expected CARA\nutility of the terminal wealth. We prove a verification theorem establishing\nthe uniqueness of the solution. Numerical results are presented in the case of\nthe Scott model and we discuss economic insights obtained from these results.\n"
    },
    {
        "paper_id": 1704.08488,
        "authors": "Dieter Hendricks and Stephen J. Roberts",
        "title": "Optimal client recommendation for market makers in illiquid financial\n  products",
        "comments": "12 pages, 3 figures, 1 table",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The process of liquidity provision in financial markets can result in\nprolonged exposure to illiquid instruments for market makers. In this case,\nwhere a proprietary position is not desired, pro-actively targeting the right\nclient who is likely to be interested can be an effective means to offset this\nposition, rather than relying on commensurate interest arising through natural\ndemand. In this paper, we consider the inference of a client profile for the\npurpose of corporate bond recommendation, based on typical recorded information\navailable to the market maker. Given a historical record of corporate bond\ntransactions and bond meta-data, we use a topic-modelling analogy to develop a\nprobabilistic technique for compiling a curated list of client recommendations\nfor a particular bond that needs to be traded, ranked by probability of\ninterest. We show that a model based on Latent Dirichlet Allocation offers\npromising performance to deliver relevant recommendations for sales traders.\n"
    },
    {
        "paper_id": 1704.08523,
        "authors": "Andreas Kunz and Markus Popp",
        "title": "Economic Neutral Position: How to best replicate not fully replicable\n  liabilities",
        "comments": "26 pages, 6 figures",
        "journal-ref": "Insurance Mathematics and Economics, 2012, Vol 96, 53-67",
        "doi": "10.1016/j.insmatheco.2020.10.006",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Financial undertakings often have to deal with liabilities of the form\n'non-hedgeable claim size times value of a tradeable asset', e.g. foreign\nproperty insurance claims times fx rates. Which strategy to invest in the\ntradeable asset is risk minimal? We generalize the Gram-Charlier series for the\nsum of two dependent random variable, which allows us to expand the capital\nrequirements based on value-at-risk and expected shortfall. We derive a stable\nand fairly model independent approximation of the risk minimal asset allocation\nin terms of the claim size distribution and the moments of asset return. The\nresults enable a correct and easy-to-implement modularization of capital\nrequirements into a market risk and a non-hedgeable risk component.\n"
    },
    {
        "paper_id": 1704.08612,
        "authors": "Tetsuya Takaishi",
        "title": "Dynamical Analysis of Stock Market Instability by Cross-correlation\n  Matrix",
        "comments": "5 pages, 4 figures",
        "journal-ref": "Journal of Physics: Conference Series 738 (2016) 012077",
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We study stock market instability by using cross-correlations constructed\nfrom the return time series of 366 stocks traded on the Tokyo Stock Exchange\nfrom January 5, 1998 to December 30, 2013. To investigate the dynamical\nevolution of the cross-correlations, cross-correlation matrices are calculated\nwith a rolling window of 400 days. To quantify the volatile market stages where\nthe potential risk is high, we apply the principal components analysis and\nmeasure the cumulative risk fraction (CRF), which is the system variance\nassociated with the first few principal components. From the CRF, we detected\nthree volatile market stages corresponding to the bankruptcy of Lehman\nBrothers, the 2011 Tohoku Region Pacific Coast Earthquake, and the FRB QE3\nreduction observation in the study period. We further apply the random matrix\ntheory for the risk analysis and find that the first eigenvector is more\nequally de-localized when the market is volatile.\n"
    },
    {
        "paper_id": 1705.00109,
        "authors": "Stephen Boyd, Enzo Busseti, Steven Diamond, Ronald N. Kahn, Kwangmoo\n  Koh, Peter Nystrup, Jan Speth",
        "title": "Multi-Period Trading via Convex Optimization",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We consider a basic model of multi-period trading, which can be used to\nevaluate the performance of a trading strategy. We describe a framework for\nsingle-period optimization, where the trades in each period are found by\nsolving a convex optimization problem that trades off expected return, risk,\ntransaction cost and holding cost such as the borrowing cost for shorting\nassets. We then describe a multi-period version of the trading method, where\noptimization is used to plan a sequence of trades, with only the first one\nexecuted, using estimates of future quantities that are unknown when the trades\nare chosen. The single-period method traces back to Markowitz; the multi-period\nmethods trace back to model predictive control. Our contribution is to describe\nthe single-period and multi-period methods in one simple framework, giving a\nclear description of the development and the approximations made. In this paper\nwe do not address a critical component in a trading algorithm, the predictions\nor forecasts of future quantities. The methods we describe in this paper can be\nthought of as good ways to exploit predictions, no matter how they are made. We\nhave also developed a companion open-source software library that implements\nmany of the ideas and methods described in the paper.\n"
    },
    {
        "paper_id": 1705.00212,
        "authors": "Jarno Talponen and Minna Turunen",
        "title": "Option pricing: A yet simpler approach",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We provide a lean, non-technical exposition on the pricing of path-dependent\nand European-style derivatives in the Cox-Ross-Rubinstein (CRR) pricing model.\nThe main tool used in the paper for cleaning up the reasoning is applying\nstatic hedging arguments.\n  This can be accomplished by taking various routes through some auxiliary\nconsiderations, namely Arrow-Debreu securities, digital options or backward\nrandom processes. In the last case the CRR model is extended to an infinite\nstate space which leads to an interesting new phenomenon not present in the\nclassical CRR model.\n  At the end we discuss the paradox involving the drift parameter $\\mu$ in the\nBSM model pricing. We provide sensitivity analysis and the speed of converge\nfor the asymptotically vanishing drift.\n"
    },
    {
        "paper_id": 1705.00284,
        "authors": "Daniel Hern\\'andez-Hern\\'andez, Harold A. Moreno-Franco, Jos\\'e Luis\n  P\\'erez",
        "title": "Periodic strategies in optimal execution with multiplicative price\n  impact",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this work we study the optimal execution problem with multiplicative price\nimpact in algorithm trading, when an agent holds an initial position of shares\nof a financial asset. The inter-selling-decision times are modelled by the\narrival times of a Poisson process. The criterion to be optimised consists in\nmaximising the expected net present value of gains of the agent, and it is\nproved that an optimal strategy has a barrier form, depending only on the\nnumber of shares left and the level of asset price.\n"
    },
    {
        "paper_id": 1705.00336,
        "authors": "Robert Fernholz",
        "title": "Stratonovich representation of semimartingale rank processes",
        "comments": "9 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Suppose that $X_1, \\ldots , X_n$ are continuous semimartingales that are\nreversible and have nondegenerate crossings. Then the corresponding rank\nprocesses can be represented by generalized Stratonovich integrals, and this\nrepresentation can be used to decompose the relative log-return of portfolios\ngenerated by functions of ranked market weights.\n"
    },
    {
        "paper_id": 1705.0034,
        "authors": "Jie Sun, Xinmin Yang, Qiang Yao and Min Zhang",
        "title": "Risk Minimization, Regret Minimization and Progressive Hedging\n  Algorithms",
        "comments": "21 pages, 2 figures",
        "journal-ref": "Mathematical Programming,181,509-530(2020)",
        "doi": "10.1007/s10107-020-01471-8",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper begins with a study on the dual representations of risk and regret\nmeasures and their impact on modeling multistage decision making under\nuncertainty. A relationship between risk envelopes and regret envelopes is\nestablished by using the Lagrangian duality theory. Such a relationship opens a\ndoor to a decomposition scheme, called progressive hedging, for solving\nmultistage risk minimization and regret minimization problems. In particular,\nthe classical progressive hedging algorithm is modified in order to handle a\nnew class of linkage constraints that arises from reformulations and other\napplications of risk and regret minimization problems. Numerical results are\nprovided to show the efficiency of the progressive hedging algorithms.\n"
    },
    {
        "paper_id": 1705.00535,
        "authors": "Stavros Stavroyiannis",
        "title": "A note on the Nelson Cao inequality constraints in the GJR-GARCH model:\n  Is there a leverage effect?",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The majority of stylized facts of financial time series and several\nValue-at-Risk measures are modeled via univariate or multivariate GARCH\nprocesses. It is not rare that advanced GARCH models fail to converge for\ncomputational reasons, and a usual parsimonious approach is the GJR-GARCH\nmodel. There is a disagreement in the literature and the specialized\neconometric software, on which constraints should be used for the parameters,\nintroducing indirectly the distinction between asymmetry and leverage. We show\nthat the approach used by various software packages is not consistent with the\nNelson-Cao inequality constraints. Implementing Monte Carlo simulations,\ndespite of the results being empirically correct, the estimated parameters are\nnot theoretically coherent with the Nelson-Cao constraints for ensuring\npositivity of conditional variances. On the other hand ruling out the leverage\nhypothesis, the asymmetry term in the GJR model can take negative values when\ntypical constraints like the condition for the existence of the second and\nfourth moments, are imposed.\n"
    },
    {
        "paper_id": 1705.00543,
        "authors": "Peter A. Forsyth, Yuying Li, Kenneth R. Vetzal",
        "title": "Are target date funds dinosaurs? Failure to adapt can lead to extinction",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Investors in Target Date Funds are automatically switched from high risk to\nlow risk assets as their retirements approach. Such funds have become very\npopular, but our analysis brings into question the rationale for them. Based on\nboth a model with parameters fitted to historical returns and on bootstrap\nresampling, we find that adaptive investment strategies significantly\noutperform typical Target Date Fund strategies. This suggests that the vast\nmajority of Target Date Funds are serving investors poorly.\n"
    },
    {
        "paper_id": 1705.00558,
        "authors": "Christian Bayer, Juho H\\\"app\\\"ol\\\"a, Ra\\'ul Tempone",
        "title": "Implied Stopping Rules for American Basket Options from Markovian\n  Projection",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This work addresses the problem of pricing American basket options in a\nmultivariate setting, which includes among others, the Bachelier and the\nBlack-Scholes models. In high dimensions, nonlinear partial differential\nequation methods for solving the problem become prohibitively costly due to the\ncurse of dimensionality. Instead, this work proposes to use a stopping rule\nthat depends on the dynamics of a low-dimensional Markovian projection of the\ngiven basket of assets. It is shown that the ability to approximate the\noriginal value function by a lower-dimensional approximation is a feature of\nthe dynamics of the system and is unaffected by the path-dependent nature of\nthe American basket option. Assuming that we know the density of the forward\nprocess and using the Laplace approximation, we first efficiently evaluate the\ndiffusion coefficient corresponding to the low-dimensional Markovian projection\nof the basket. Then, we approximate the optimal early-exercise boundary of the\noption by solving a Hamilton-Jacobi-Bellman partial differential equation in\nthe projected, low-dimensional space. The resulting near-optimal early-exercise\nboundary is used to produce an exercise strategy for the high-dimensional\noption, thereby providing a lower bound for the price of the American basket\noption. A corresponding upper bound is also provided. These bounds allow to\nassess the accuracy of the proposed pricing method. Indeed, our approximate\nearly-exercise strategy provides a straightforward lower bound for the American\nbasket option price. Following a duality argument due to Rogers, we derive a\ncorresponding upper bound solving only the low-dimensional optimal control\nproblem. Numerically, we show the feasibility of the method using baskets with\ndimensions up to fifty. In these examples, the resulting option price relative\nerrors are only of the order of few percent.\n"
    },
    {
        "paper_id": 1705.00672,
        "authors": "Ibrahim Ekren and Johannes Muhle-Karbe",
        "title": "Portfolio Choice with Small Temporary and Transient Price Impact",
        "comments": null,
        "journal-ref": "Mathematical Finance 29.4 (2019): 1066-1115",
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We study portfolio selection in a model with both temporary and transient\nprice impact introduced by Garleanu and Pedersen (2016). In the large-liquidity\nlimit where both frictions are small, we derive explicit formulas for the\nasymptotically optimal trading rate and the corresponding minimal leading-order\nperformance loss. We find that the losses are governed by the volatility of the\nfrictionless target strategy, like in models with only temporary price impact.\nIn contrast, the corresponding optimal portfolio not only tracks the\nfrictionless optimizer, but also exploits the displacement of the market price\nfrom its unaffected level.\n"
    },
    {
        "paper_id": 1705.00691,
        "authors": "Sergey Nadtochiy, Mykhaylo Shkolnikov",
        "title": "Particle systems with singular interaction through hitting times:\n  application in systemic risk modeling",
        "comments": "33 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We propose an interacting particle system to model the evolution of a system\nof banks with mutual exposures. In this model, a bank defaults when its\nnormalized asset value hits a lower threshold, and its default causes\ninstantaneous losses to other banks, possibly triggering a cascade of defaults.\nThe strength of this interaction is determined by the level of the so-called\nnon-core exposure. We show that, when the size of the system becomes large, the\ncumulative loss process of a bank resulting from the defaults of other banks\nexhibits discontinuities. These discontinuities are naturally interpreted as\nsystemic events, and we characterize them explicitly in terms of the level of\nnon-core exposure and the fraction of banks that are \"about to default\". The\nmain mathematical challenges of our work stem from the very singular nature of\nthe interaction between the particles, which is inherited by the limiting\nsystem. A similar particle system is analyzed in [DIRT15a] and [DIRT15b], and\nwe build on and extend their results. In particular, we characterize the\nlarge-population limit of the system and analyze the jump times, the regularity\nbetween jumps, and the local uniqueness of the limiting process.\n"
    },
    {
        "paper_id": 1705.00864,
        "authors": "Yuuki Ida and Yuri Imamura",
        "title": "Towards the Exact Simulation Using Hyperbolic Brownian Motion",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In the present paper, an expansion of the transition density of Hyperbolic\nBrownian motion with drift is given, which is potentially useful for pricing\nand hedging of options under stochastic volatility models. We work on a\ncondition on the drift which dramatically simplifies the proof.\n"
    },
    {
        "paper_id": 1705.00891,
        "authors": "Syed Ali Asad Rizvi, Stephen J. Roberts, Michael A. Osborne and Favour\n  Nyikosa",
        "title": "A Novel Approach to Forecasting Financial Volatility with Gaussian\n  Process Envelopes",
        "comments": "16 pages, 8 figures, 6 tables",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper we use Gaussian Process (GP) regression to propose a novel\napproach for predicting volatility of financial returns by forecasting the\nenvelopes of the time series. We provide a direct comparison of their\nperformance to traditional approaches such as GARCH. We compare the forecasting\npower of three approaches: GP regression on the absolute and squared returns;\nregression on the envelope of the returns and the absolute returns; and\nregression on the envelope of the negative and positive returns separately. We\nuse a maximum a posteriori estimate with a Gaussian prior to determine our\nhyperparameters. We also test the effect of hyperparameter updating at each\nforecasting step. We use our approaches to forecast out-of-sample volatility of\nfour currency pairs over a 2 year period, at half-hourly intervals. From three\nkernels, we select the kernel giving the best performance for our data. We use\ntwo published accuracy measures and four statistical loss functions to evaluate\nthe forecasting ability of GARCH vs GPs. In mean squared error the GP's perform\n20% better than a random walk model, and 50% better than GARCH for the same\ndata.\n"
    },
    {
        "paper_id": 1705.01069,
        "authors": "Peter Carr, Roger Lee, Matthew Lorig",
        "title": "Pricing Variance Swaps on Time-Changed Markov Processes",
        "comments": "19 pages, 3 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We prove that the variance swap rate (fair strike) equals the price of a\nco-terminal European-style contract when the underlying is an exponential\nMarkov process, time-changed by an arbitrary continuous stochastic clock, which\nhas arbitrary correlation with the driving Markov process, provided that the\npayoff function $G$ of the European contract satisfies an ordinary\nintegro-differential equation, which depends only on the dynamics of the Markov\nprocess, not on the clock. We present examples of Markov processes where the\nfunction $G$ that prices the variance swap can be computed explicitly. In\ngeneral, the solutions $G$ are not contained in the logarithmic family\npreviously obtained in the special case where the Markov process is a L\\'evy\nprocess.\n"
    },
    {
        "paper_id": 1705.01142,
        "authors": "Swetava Ganguli, Jared Dunnmon",
        "title": "Machine Learning for Better Models for Predicting Bond Prices",
        "comments": "Submitted for publication",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Bond prices are a reflection of extremely complex market interactions and\npolicies, making prediction of future prices difficult. This task becomes even\nmore challenging due to the dearth of relevant information, and accuracy is not\nthe only consideration--in trading situations, time is of the essence. Thus,\nmachine learning in the context of bond price predictions should be both fast\nand accurate. In this course project, we use a dataset describing the previous\n10 trades of a large number of bonds among other relevant descriptive metrics\nto predict future bond prices. Each of 762,678 bonds in the dataset is\ndescribed by a total of 61 attributes, including a ground truth trade price. We\nevaluate the performance of various supervised learning algorithms for\nregression followed by ensemble methods, with feature and model selection\nconsiderations being treated in detail. We further evaluate all methods on both\naccuracy and speed. Finally, we propose a novel hybrid time-series aided\nmachine learning method that could be applied to such datasets in future work.\n"
    },
    {
        "paper_id": 1705.01144,
        "authors": "Jaydip Sen and Tamal Datta Chaudhuri",
        "title": "A Time Series Analysis-Based Forecasting Framework for the Indian\n  Healthcare Sector",
        "comments": "23 pages, 10 figures, 8 tables. The paper is accepted for publication\n  in \"Journal of Insurance and Financial Management\" Vol 3, No 1, 2017",
        "journal-ref": "Journal of Insurance and Financial Management, Vol 3, No 1, pp. 1-\n  29, 2017",
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Designing efficient and robust algorithms for accurate prediction of stock\nmarket prices is one of the most exciting challenges in the field of time\nseries analysis and forecasting. With the exponential rate of development and\nevolution of sophisticated algorithms and with the availability of fast\ncomputing platforms, it has now become possible to effectively and efficiently\nextract, store, process and analyze high volume of stock market data with\ndiversity in its contents. Availability of complex algorithms which can execute\nvery fast on parallel architecture over the cloud has made it possible to\nachieve higher accuracy in forecasting results while reducing the time required\nfor computation. In this paper, we use the time series data of the healthcare\nsector of India for the period January 2010 till December 2016. We first\ndemonstrate a decomposition approach of the time series and then illustrate how\nthe decomposition results provide us with useful insights into the behavior and\nproperties exhibited by the time series. Further, based on the structural\nanalysis of the time series, we propose six different methods of forecasting\nfor predicting the time series index of the healthcare sector. Extensive\nresults are provided on the performance of the forecasting methods to\ndemonstrate their effectiveness.\n"
    },
    {
        "paper_id": 1705.01145,
        "authors": "Joana Estevens, Paulo Rocha, Joao Boto, Pedro Lind",
        "title": "Stochastic modelling of non-stationary financial assets",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We model non-stationary volume-price distributions with a log-normal\ndistribution and collect the time series of its two parameters. The time series\nof the two parameters are shown to be stationary and Markov-like and\nconsequently can be modelled with Langevin equations, which are derived\ndirectly from their series of values. Having the evolution equations of the\nlog-normal parameters, we reconstruct the statistics of the first moments of\nvolume-price distributions which fit well the empirical data. Finally, the\nproposed framework is general enough to study other non-stationary stochastic\nvariables in other research fields, namely biology, medicine and geology.\n"
    },
    {
        "paper_id": 1705.01302,
        "authors": "Ren\\'e A\\\"id, Matteo Basei, Huy\\^en Pham",
        "title": "A McKean-Vlasov approach to distributed electricity generation\n  development",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper analyses the interaction between centralised carbon emissive\ntechnologies and distributed intermittent non-emissive technologies. In our\nmodel, there is a representative consumer who can satisfy her electricity\ndemand by investing in distributed generation (solar panels) and by buying\npower from a centralised firm at a price the firm sets. Distributed generation\nis intermittent and induces an externality cost to the consumer. The firm\nprovides non-random electricity generation subject to a carbon tax and to\ntransmission costs. The objective of the consumer is to satisfy her demand\nwhile mini\\-mising investment costs, payments to the firm and intermittency\ncosts. The objective of the firm is to satisfy the consumer's residual demand\nwhile minimising investment costs, demand deviation costs, and maximising the\npayments from the consumer. We formulate the investment decisions as\nMcKean-Vlasov control problems with stochastic coefficients. We provide\nexplicit, price model-free solutions to the optimal decision problems faced by\neach player, the solution of the Pareto optimum, and the Stackelberg\nequilibrium where the firm is the leader. We find that, from the social\nplanner's point of view, the carbon tax or transmission costs are necessary to\njustify a positive share of distributed capacity in the long-term, whatever the\nrespective investment costs of both technologies are. The Stackelberg\nequilibrium is far from the Pareto equilibrium and leads to an over-investment\nin distributed energy and to a much higher price for centralised energy.\n"
    },
    {
        "paper_id": 1705.01348,
        "authors": "Luigi Troiano and Elena Mejuto Villa and Pravesh Kriplani",
        "title": "An Alternative Estimation of Market Volatility based on Fuzzy Transform",
        "comments": "IFSA-SCIS 2017, 6 pages, 6 figures, no table",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Realization of uncertainty of prices is captured by volatility, that is the\ntendency of prices to vary along a period of time. This is generally measured\nas standard deviation of daily returns. In this paper we propose and\ninvestigate the application of fuzzy transform and its inverse as an\nalternative measure of volatility. The measure obtained is compatible with the\ndefinition of risk measure given by Luce. A comparison with standard definition\nis performed by considering the NIFTY 50 stock market index within the period\nSept. 2000 - Feb. 2017.\n"
    },
    {
        "paper_id": 1705.01406,
        "authors": "Longfeng Zhao, Wei Li, Andrea Fenu, Boris Podobnik, Yougui Wang, H.\n  Eugene Stanley",
        "title": "The q-dependent detrended cross-correlation analysis of stock market",
        "comments": "25 pages, 16 figures",
        "journal-ref": null,
        "doi": "10.1088/1742-5468/aa9db0",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The properties of q-dependent cross-correlation matrices of stock market have\nbeen analyzed by using the random matrix theory and complex network. The\ncorrelation structures of the fluctuations at different magnitudes have unique\nproperties. The cross-correlations among small fluctuations are much stronger\nthan those among large fluctuations. The large and small fluctuations are\ndominated by different groups of stocks. We use complex network representation\nto study these q-dependent matrices and discover some new identities. By\nutilizing those q-dependent correlation-based networks, we are able to\nconstruct some portfolio by those most independent stocks which consistently\nperform the best. The optimal multifractal order for portfolio optimization is\napproximately $q=2$. These results have deepened our understanding about the\ncollective behaviors of the complex financial system.\n"
    },
    {
        "paper_id": 1705.01407,
        "authors": "Sourish Das, Rituparna Sen",
        "title": "Sparse Portfolio selection via Bayesian Multiple testing",
        "comments": "23 pages, 8 figures, 9 tables",
        "journal-ref": "2020 Sankhya B, 83(2), 585 - 617",
        "doi": "10.1007/s13571-020-00240-z",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We presented Bayesian portfolio selection strategy, via the $k$ factor asset\npricing model. If the market is information efficient, the proposed strategy\nwill mimic the market; otherwise, the strategy will outperform the market. The\nstrategy depends on the selection of a portfolio via Bayesian multiple testing\nmethodologies. We present the \"discrete-mixture prior\" model and the\n\"hierarchical Bayes model with horseshoe prior.\" We define the Oracle set and\nprove that asymptotically the Bayes rule attains the risk of Bayes Oracle up to\n$O(1)$. Our proposed Bayes Oracle test guarantees statistical power by\nproviding the upper bound of the type-II error. Simulation study indicates that\nthe proposed Bayes oracle test is suitable for the efficient market with few\nstocks inefficiently priced. However, as the model becomes dense, i.e., the\nmarket is highly inefficient, one should not use the Bayes oracle test. The\nstatistical power of the Bayes Oracle portfolio is uniformly better for the\n$k$-factor model ($k>1$) than the one factor CAPM. We present the empirical\nstudy, where we considered the 500 constituent stocks of S\\&P 500 from the New\nYork Stock Exchange (NYSE), and S\\&P 500 index as the benchmark for thirteen\nyears from the year 2006 to 2018. We showed the out-sample risk and return\nperformance of the four different portfolio selection strategies and compared\nwith the S\\&P 500 index as the benchmark market index. Empirical results\nindicate that it is possible to propose a strategy which can outperform the\nmarket.\n"
    },
    {
        "paper_id": 1705.01446,
        "authors": "Fr\\'ed\\'eric Abergel (MICS), C\\^ome Hur\\'e (LPSM (UMR\\_8001)), Huy\\^en\n  Pham (LPSM (UMR\\_8001))",
        "title": "Algorithmic trading in a microstructural limit order book model",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We propose a microstructural modeling framework for studying optimal market\nmaking policies in a FIFO (first in first out) limit order book (LOB). In this\ncontext, the limit orders, market orders, and cancel orders arrivals in the LOB\nare modeled as Cox point processes with intensities that only depend on the\nstate of the LOB. These are high-dimensional models which are realistic from a\nmicro-structure point of view and have been recently developed in the\nliterature. In this context, we consider a market maker who stands ready to buy\nand sell stock on a regular and continuous basis at a publicly quoted price,\nand identifies the strategies that maximize her P\\&L penalized by her\ninventory. We apply the theory of Markov Decision Processes and dynamic\nprogramming method to characterize analytically the solutions to our optimal\nmarket making problem. The second part of the paper deals with the numerical\naspect of the high-dimensional trading problem. We use a control randomization\nmethod combined with quantization method to compute the optimal strategies.\nSeveral computational tests are performed on simulated data to illustrate the\nefficiency of the computed optimal strategy. In particular, we simulated an\norder book with constant/ symmet-ric/ asymmetrical/ state dependent\nintensities, and compared the computed optimal strategy with naive strategies.\nSome codes are available on https://github.com/comeh.\n"
    },
    {
        "paper_id": 1705.01454,
        "authors": "Yu-Sung Tu, Wei-Torng Juang",
        "title": "The Payoff Region of a Strategic Game and Its Extreme Points",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The range of a payoff function for an $n$-player finite strategic game is\ninvestigated using a novel approach, the notion of extreme points of a\nnon-convex set. The shape of a noncooperative payoff region can be estimated\nusing extreme points and supporting hyperplanes of the cooperative payoff\nregion. A basic structural characteristic of a noncooperative payoff region is\nthat any of its subregions must be non-strictly convex if the subregion\ncontains a relative neighborhood of a point on its boundary. Besides, applying\nthe properties of extreme points of a noncooperative payoff region is a simple\nand effective way to prove some results about Pareto efficiency and social\nefficiency in game theory.\n"
    },
    {
        "paper_id": 1705.02087,
        "authors": "Christa Cuchiero, Irene Klein and Josef Teichmann",
        "title": "A fundamental theorem of asset pricing for continuous time large\n  financial markets in a two filtration setting",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We present a version of the fundamental theorem of asset pricing (FTAP) for\ncontinuous time large financial markets with two filtrations in an\n$L^p$-setting for $ 1 \\leq p < \\infty$. This extends the results of Yuri\nKabanov and Christophe Stricker \\cite{KS:06} to continuous time and to a large\nfinancial market setting, however, still preserving the simplicity of the\ndiscrete time setting. On the other hand it generalizes Stricker's\n$L^p$-version of FTAP \\cite{S:90} towards a setting with two filtrations. We do\nneither assume that price processes are semi-martigales, (and it does not\nfollow due to trading with respect to the \\emph{smaller} filtration) nor that\nprice processes have any path properties, neither any other particular property\nof the two filtrations in question, nor admissibility of portfolio wealth\nprocesses, but we rather go for a completely general (and realistic) result,\nwhere trading strategies are just predictable with respect to a smaller\nfiltration than the one generated by the price processes. Applications range\nfrom modeling trading with delayed information, trading on different time\ngrids, dealing with inaccurate price information, and randomization approaches\nto uncertainty.\n"
    },
    {
        "paper_id": 1705.02154,
        "authors": "Dave Zachariah and Paul Cockshott",
        "title": "Leontief Meets Shannon - Measuring the Complexity of the Economic System",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We develop a complexity measure for large-scale economic systems based on\nShannon's concept of entropy. By adopting Leontief's perspective of the\nproduction process as a circular flow, we formulate the process as a Markov\nchain. Then we derive a measure of economic complexity as the average number of\nbits required to encode the flow of goods and services in the production\nprocess. We illustrate this measure using data from seven national economies,\nspanning several decades.\n"
    },
    {
        "paper_id": 1705.02187,
        "authors": "Paolo Sgrignoli, Rodolfo Metulini, Zhen Zhu, Massimo Riccaboni",
        "title": "The Indirect Effects of FDI on Trade: A Network Perspective",
        "comments": "30 pages, 4 figures, 12 tables; Presented at ECCS14",
        "journal-ref": "The World Economy, Vol. 40, Issue 10, pp. 2193-2225, 2017",
        "doi": "10.1111/twec.12504",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The relationship between international trade and foreign direct investment\n(FDI) is one of the main features of globalization. In this paper we\ninvestigate the effects of FDI on trade from a network perspective, since FDI\ntakes not only direct but also indirect channels from origin to destination\ncountries because of firms' incentive to reduce tax burden, to minimize\ncoordination costs, and to break barriers to market entry. We use a unique data\nset of international corporate control as a measure of stock FDI to construct a\ncorporate control network (CCN) where the nodes are the countries and the edges\nare the corporate control relationships. Based on the CCN, the network\nmeasures, i.e., the shortest path length and the communicability, are computed\nto capture the indirect channel of FDI. Empirically we find that corporate\ncontrol has a positive effect on trade both directly and indirectly. The result\nis robust with different specifications and estimation strategies. Hence, our\npaper provides strong empirical evidence of the indirect effects of FDI on\ntrade. Moreover, we identify a number of interplaying factors such as regional\ntrade agreements and the region of Asia. We also find that the indirect effects\nare more pronounced for manufacturing sectors than for primary sectors such as\noil extraction and agriculture.\n"
    },
    {
        "paper_id": 1705.02291,
        "authors": "Oleksii Mostovyi",
        "title": "Optimal consumption of multiple goods in incomplete markets",
        "comments": "14 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We consider the problem of optimal consumption of multiple goods in\nincomplete semimartingale markets. We formulate the dual problem and identify\nconditions that allow for existence and uniqueness of the solution and give a\ncharacterization of the optimal consumption strategy in terms of the dual\noptimizer. We illustrate our results with examples in both complete and\nincomplete models. In particular, we construct closed-form solutions in some\nincomplete models.\n"
    },
    {
        "paper_id": 1705.02344,
        "authors": "Jakob Knollm\\\"uller, Torsten A. En{\\ss}lin",
        "title": "Noisy independent component analysis of auto-correlated components",
        "comments": null,
        "journal-ref": "Phys. Rev. E 96, 042114 (2017)",
        "doi": "10.1103/PhysRevE.96.042114",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We present a new method for the separation of superimposed, independent,\nauto-correlated components from noisy multi-channel measurement. The presented\nmethod simultaneously reconstructs and separates the components, taking all\nchannels into account and thereby increases the effective signal-to-noise ratio\nconsiderably, allowing separations even in the high noise regime.\nCharacteristics of the measurement instruments can be included, allowing for\napplication in complex measurement situations. Independent posterior samples\ncan be provided, permitting error estimates on all desired quantities. Using\nthe concept of information field theory, the algorithm is not restricted to any\ndimensionality of the underlying space or discretization scheme thereof.\n"
    },
    {
        "paper_id": 1705.0244,
        "authors": "Masaaki Fujii, Akihiko Takahashi",
        "title": "Anticipated Backward SDEs with Jumps and quadratic-exponential growth\n  drivers",
        "comments": "revised: forthcoming in Stochastics and Dynamics",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper, we study a class of Anticipated Backward Stochastic\nDifferential Equations (ABSDE) with jumps. The solution of the ABSDE is a\ntriple $(Y,Z,\\psi)$ where $Y$ is a semimartingale, and $(Z,\\psi)$ are the\ndiffusion and jump coefficients. We allow the driver of the ABSDE to have\nlinear growth on the uniform norm of $Y$'s future paths, as well as quadratic\nand exponential growth on the spot values of $(Z,\\psi)$, respectively. The\nexistence of the unique solution is proved for Markovian and non-Markovian\nsettings with different structural assumptions on the driver. In the former\ncase, some regularities on $(Z,\\psi)$ with respect to the forward process are\nalso obtained.\n"
    },
    {
        "paper_id": 1705.02473,
        "authors": "Youssef El-Khatib and Abdulnasser Hatemi-J",
        "title": "Computation of second order price sensitivities in depressed markets",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Risk management in financial derivative markets requires inevitably the\ncalculation of the different price sensitivities. The literature contains an\nabundant amount of research works that have studied the computation of these\nimportant values. Most of these works consider the well-known Black and Scholes\nmodel where the volatility is assumed to be constant. Moreover, to our best\nknowledge, they compute only the first order price sensitivities. Some works\nthat attempt to extend to markets affected by financial crisis appeared\nrecently. However, none of these papers deal with the calculation of the price\nsensitivities of second order. Providing second derivatives for the underlying\nprice sensitivities is an important issue in financial risk management because\nthe investor can determine whether or not each source of risk is increasing at\nan increasing rate. In this paper, we work on the computation of second order\nprices sensitivities for a market under crisis. The underlying second order\nprice sensitivities are derived explicitly. The obtained formulas are expected\nto improve on the accuracy of the hedging strategies during a financial crunch.\n"
    },
    {
        "paper_id": 1705.02559,
        "authors": "Rafael D. Sorkin",
        "title": "An equation for a time-dependent profit rate",
        "comments": "plainTeX, 18 pages, no figures. Most current version will be\n  available at http://www.perimeterinstitute.ca/personal/rsorkin/some.papers/\n  (or wherever my home-page may be, such as\n  http://www.physics.syr.edu/~sorkin/some.papers/)",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Taking as a hypothesis a form of the labour theory of value, and $without$\n$assuming$ $equilibrium$, we derive an equation that yields the profit-rate\n$\\pi$ as a function of time. For a mature economy, $\\pi(t)$ reduces to the\nproduct of two factors: ($i$) a certain $retarded$ $average$ of the sum of the\ngrowth-rates of productivity and of the size of the labour-force measured by\nhours worked, and ($ii$) the ratio of the current rate of surplus value to its\nown retarded average. We also suggest an empirical test of the equation.\n"
    },
    {
        "paper_id": 1705.02789,
        "authors": "Damir Filipovi\\'c, Martin Larsson, Francesco Statti",
        "title": "Unspanned Stochastic Volatility in the Multi-factor CIR Model",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Empirical evidence suggests that fixed income markets exhibit unspanned\nstochastic volatility (USV), that is, that one cannot fully hedge volatility\nrisk solely using a portfolio of bonds. While [1] showed that no two-factor\nCox-Ingersoll-Ross (CIR) model can exhibit USV, it has been unknown to date\nwhether CIR models with more than two factors can exhibit USV or not. We\nformally review USV and relate it to bond market incompleteness. We provide\nnecessary and sufficient conditions for a multi-factor CIR model to exhibit\nUSV. We then construct a class of three-factor CIR models that exhibit USV.\nThis answers in the affirmative the above previously open question. We also\nshow that multi-factor CIR models with diagonal drift matrix cannot exhibit\nUSV.\n"
    },
    {
        "paper_id": 1705.02933,
        "authors": "Daniel Bartl, Michael Kupper, David J. Pr\\\"omel, Ludovic Tangpi",
        "title": "Duality for pathwise superhedging in continuous time",
        "comments": null,
        "journal-ref": "Finance Stoch (2019) 23: 697-728",
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We provide a model-free pricing-hedging duality in continuous time. For a\nfrictionless market consisting of $d$ risky assets with continuous price\ntrajectories, we show that the purely analytic problem of finding the minimal\nsuperhedging price of a path dependent European option has the same value as\nthe purely probabilistic problem of finding the supremum of the expectations of\nthe option over all martingale measures. The superhedging problem is formulated\nwith simple trading strategies, the claim is the limit inferior of continuous\nfunctions, which allows for upper and lower semi-continuous claims, and\nsuperhedging is required in the pathwise sense on a $\\sigma$-compact sample\nspace of price trajectories. If the sample space is stable under stopping, the\nprobabilistic problem reduces to finding the supremum over all martingale\nmeasures with compact support. As an application of the general results we\ndeduce dualities for Vovk's outer measure and semi-static superhedging with\nfinitely many securities.\n"
    },
    {
        "paper_id": 1705.03233,
        "authors": "Adamantios Ntakaris, Martin Magris, Juho Kanniainen, Moncef Gabbouj,\n  Alexandros Iosifidis",
        "title": "Benchmark Dataset for Mid-Price Forecasting of Limit Order Book Data\n  with Machine Learning Methods",
        "comments": "Published: Journal of Forecasting",
        "journal-ref": null,
        "doi": "10.1002/for.2543",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Managing the prediction of metrics in high-frequency financial markets is a\nchallenging task. An efficient way is by monitoring the dynamics of a limit\norder book to identify the information edge. This paper describes the first\npublicly available benchmark dataset of high-frequency limit order markets for\nmid-price prediction. We extracted normalized data representations of time\nseries data for five stocks from the NASDAQ Nordic stock market for a time\nperiod of ten consecutive days, leading to a dataset of ~4,000,000 time series\nsamples in total. A day-based anchored cross-validation experimental protocol\nis also provided that can be used as a benchmark for comparing the performance\nof state-of-the-art methodologies. Performance of baseline approaches are also\nprovided to facilitate experimental comparisons. We expect that such a\nlarge-scale dataset can serve as a testbed for devising novel solutions of\nexpert systems for high-frequency limit order book data analysis.\n"
    },
    {
        "paper_id": 1705.03396,
        "authors": "Philippe Deprez, Pavel V. Shevchenko and Mario V. W\\\"uthrich",
        "title": "Machine Learning Techniques for Mortality Modeling",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Various stochastic models have been proposed to estimate mortality rates. In\nthis paper we illustrate how machine learning techniques allow us to analyze\nthe quality of such mortality models. In addition, we present how these\ntechniques can be used for differentiating the different causes of death in\nmortality modeling.\n"
    },
    {
        "paper_id": 1705.03423,
        "authors": "Rupert Way, Fran\\c{c}ois Lafond, Fabrizio Lillo, Valentyn Panchenko\n  and J. Doyne Farmer",
        "title": "Wright meets Markowitz: How standard portfolio theory changes when\n  assets are technologies following experience curves",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We consider how to optimally allocate investments in a portfolio of competing\ntechnologies using the standard mean-variance framework of portfolio theory. We\nassume that technologies follow the empirically observed relationship known as\nWright's law, also called a \"learning curve\" or \"experience curve\", which\npostulates that costs drop as cumulative production increases. This introduces\na positive feedback between cost and investment that complicates the portfolio\nproblem, leading to multiple local optima, and causing a trade-off between\nconcentrating investments in one project to spur rapid progress vs.\ndiversifying over many projects to hedge against failure. We study the\ntwo-technology case and characterize the optimal diversification in terms of\nprogress rates, variability, initial costs, initial experience, risk aversion,\ndiscount rate and total demand. The efficient frontier framework is used to\nvisualize technology portfolios and show how feedback results in nonlinear\ndistortions of the feasible set. For the two-period case, in which learning and\nuncertainty interact with discounting, we compare different scenarios and find\nthat the discount rate plays a critical role.\n"
    },
    {
        "paper_id": 1705.03458,
        "authors": "A. Hernando, D. Villuendas, M. Sulc, R. Hernando, R. Seoane, A.\n  Plastino",
        "title": "Maximum Entropy Principle underlying the dynamics of automobile sales",
        "comments": "5 pages, 5 color figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  We analyze an exhaustive data-set of new-cars monthly sales. The set refers\nto 10 years of Spanish sales of more than 6500 different car model\nconfigurations and a total of 10M sold cars, from January 2007 to January 2017.\nWe find that for those model configurations with a monthly market-share higher\nthan 0.1% the sales become scalable obeying Gibrat's law of proportional growth\nunder logistic dynamics. Remarkably, the distribution of total sales follows\nthe predictions of the Maximum Entropy Principle for systems subject to\nproportional growth in dynamical equilibrium. We also encounter that the\nassociated dynamics are non-Markovian, i.e., the system has a decaying memory\nor inertia of about 5 years. Thus, car sales are predictable within a certain\ntime-period. We show that the main characteristics of the dynamics can be\ndescribed via a construct based upon the Langevin equation. This construct\nencompasses the fundamental principles that any predictive model on car sales\nshould obey.\n"
    },
    {
        "paper_id": 1705.03647,
        "authors": "Christa Cuchiero",
        "title": "Polynomial processes in stochastic portfolio theory",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We introduce polynomial processes in the sense of [8] in the context of\nstochastic portfolio theory to model simultaneously companies' market\ncapitalizations and the corresponding market weights. These models\nsubstantially extend volatility stabilized market models considered by Robert\nFernholz and Ioannis Karatzas in [18], in particular they allow for correlation\nbetween the individual stocks. At the same time they remain remarkably\ntractable which makes them applicable in practice, especially for estimation\nand calibration to high dimensional equity index data. In the diffusion case we\ncharacterize the joint polynomial property of the market capitalizations and\nthe corresponding weights, exploiting the fact that the transformation between\nabsolute and relative quantities perfectly fits the structural properties of\npolynomial processes. Explicit parameter conditions assuring the existence of a\nlocal martingale deflator and relative arbitrages with respect to the market\nportfolio are given and the connection to non-attainment of the boundary of the\nunit simplex is discussed. We also consider extensions to models with jumps and\nthe computation of optimal relative arbitrage strategies.\n"
    },
    {
        "paper_id": 1705.03666,
        "authors": "Francisco Bernal and Gon\\c{c}alo dos Reis and Greig Smith",
        "title": "Hybrid PDE solver for data-driven problems and modern branching",
        "comments": "23 pages, 7 figures; Final SMUR version; To appear in the European\n  Journal of Applied Mathematics (EJAM)",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The numerical solution of large-scale PDEs, such as those occurring in\ndata-driven applications, unavoidably require powerful parallel computers and\ntailored parallel algorithms to make the best possible use of them. In fact,\nconsiderations about the parallelization and scalability of realistic problems\nare often critical enough to warrant acknowledgement in the modelling phase.\nThe purpose of this paper is to spread awareness of the Probabilistic Domain\nDecomposition (PDD) method, a fresh approach to the parallelization of PDEs\nwith excellent scalability properties. The idea exploits the stochastic\nrepresentation of the PDE and its approximation via Monte Carlo in combination\nwith deterministic high-performance PDE solvers. We describe the ingredients of\nPDD and its applicability in the scope of data science. In particular, we\nhighlight recent advances in stochastic representations for nonlinear PDEs\nusing branching diffusions, which have significantly broadened the scope of\nPDD.\n  We envision this work as a dictionary giving large-scale PDE practitioners\nreferences on the very latest algorithms and techniques of a non-standard, yet\nhighly parallelizable, methodology at the interface of deterministic and\nprobabilistic numerical methods. We close this work with an invitation to the\nfully nonlinear case and open research questions.\n"
    },
    {
        "paper_id": 1705.03724,
        "authors": "Miryana Grigorova, Marie-Claire Quenez (LPMA)",
        "title": "Optimal stopping and a non-zero-sum Dynkin game in discrete time with\n  risk measures induced by BSDEs",
        "comments": null,
        "journal-ref": "Stochastics: An International Journal of Probability and\n  Stochastic Processes, Taylor \\& Francis: STM, Behavioural Science and Public\n  Health Titles, 2016, 89 (1)",
        "doi": "10.1080/17442508.2016.1166505",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We first study an optimal stopping problem in which a player (an agent) uses\na discrete stopping time in order to stop optimally a payoff process whose risk\nis evaluated by a (non-linear) $g$-expectation. We then consider a non-zero-sum\ngame on discrete stopping times with two agents who aim at minimizing their\nrespective risks. The payoffs of the agents are assessed by g-expectations\n(with possibly different drivers for the different players). By using the\nresults of the first part, combined with some ideas of S. Hamad{\\`e}ne and J.\nZhang, we construct a Nash equilibrium point of this game by a recursive\nprocedure. Our results are obtained in the case of a standard Lipschitz driver\n$g$ without any additional assumption on the driver besides that ensuring the\nmonotonicity of the corresponding $g$-expectation.\n"
    },
    {
        "paper_id": 1705.03787,
        "authors": "Jin Sun, Pavel V. Shevchenko, Man Chung Fung",
        "title": "A note on the impact of management fees on the pricing of variable\n  annuity guarantees",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Variable annuities, as a class of retirement income products, allow equity\nmarket exposure for a policyholder's retirement fund with electable additional\nguarantees to limit the downside risk of the market. Management fees and\nguarantee insurance fees are charged respectively for the market exposure and\nfor the protection from the downside risk. We investigate the impact of\nmanagement fees on the pricing of variable annuity guarantees under optimal\nwithdrawal strategies. Two optimal strategies, from policyholder's and from\ninsurer's perspectives, are respectively formulated and the corresponding\npricing problems are solved using dynamic programming. Our results show that\nwhen management fees are present, the two strategies can deviate significantly\nfrom each other, leading to a substantial difference of the guarantee insurance\nfees. This provides a possible explanation of lower guarantee insurance fees\nobserved in the market. Numerical experiments are conducted to illustrate our\nresults.\n"
    },
    {
        "paper_id": 1705.03848,
        "authors": "Roberto De Luca, Marco Di Mauro, Angelo Falzarano, Adele Naddeo",
        "title": "Propensity to spending of an average consumer over a brief period",
        "comments": "5 pages, 7 figures",
        "journal-ref": "Eur. Phys. J. B (2016) 89: 184",
        "doi": "10.1140/epjb/e2016-70243-y",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Understanding consumption dynamics and its impact on the whole economy and\nwelfare within the present economic crisis is not an easy task. Indeed the\nlevel of consumer demand for different goods varies with the prices, consumer\nincomes and demographic factors. Furthermore crisis may trigger different\nbehaviors which result in distortions and amplification effects. In the present\nwork we propose a simple model to quantitatively describe the time evolution\nover a brief period of the amount of money an average consumer decides to\nspend, depending on his/her available budget. A simple hydrodynamical analog of\nthe model is discussed. Finally, perspectives of this work are briefly\noutlined.\n"
    },
    {
        "paper_id": 1705.03929,
        "authors": "Dietmar Leisen and Eckhard Platen",
        "title": "Investing for the Long Run",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper studies long term investing by an investor that maximizes either\nexpected utility from terminal wealth or from consumption. We introduce the\nconcepts of a generalized stochastic discount factor (SDF) and of the minimum\nprice to attain target payouts. The paper finds that the dynamics of the SDF\nneeds to be captured and not the entire market dynamics, which simplifies\nsignificantly practical implementations of optimal portfolio strategies. We pay\nparticular attention to the case where the SDF is equal to the inverse of the\ngrowth-optimal portfolio in the given market. Then, optimal wealth evolution is\nclosely linked to the growth optimal portfolio. In particular, our concepts\nallow us to reconcile utility optimization with the practitioner approach of\ngrowth investing. We illustrate empirically that our new framework leads to\nimproved lifetime consumption-portfolio choice and asset allocation strategies.\n"
    },
    {
        "paper_id": 1705.04537,
        "authors": "Johanna F. Ziegel, Fabian Kr\\\"uger, Alexander Jordan, Fernando\n  Fasciati",
        "title": "Murphy Diagrams: Forecast Evaluation of Expected Shortfall",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Motivated by the Basel 3 regulations, recent studies have considered joint\nforecasts of Value-at-Risk and Expected Shortfall. A large family of scoring\nfunctions can be used to evaluate forecast performance in this context.\nHowever, little intuitive or empirical guidance is currently available, which\nrenders the choice of scoring function awkward in practice. We therefore\ndevelop graphical checks (Murphy diagrams) of whether one forecast method\ndominates another under a relevant class of scoring functions, and propose an\nassociated hypothesis test. We illustrate these tools with simulation examples\nand an empirical analysis of S&P 500 and DAX returns.\n"
    },
    {
        "paper_id": 1705.0478,
        "authors": "Stavros J. Sioutis",
        "title": "Calibration and Filtering of Exponential L\\'evy Option Pricing Models",
        "comments": "49 pages, 8 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The accuracy of least squares calibration using option premiums and particle\nfiltering of price data to find model parameters is determined. Derivative\nmodels using exponential L\\'evy processes are calibrated using regularized\nweighted least squares with respect to the minimal entropy martingale measure.\nSequential importance resampling is used for the Bayesian inference problem of\ntime series parameter estimation with proposal distribution determined using\nextended Kalman filter. The algorithms converge to their respective global\noptima using a highly parallelizable statistical optimization approach using a\ngrid of initial positions. Each of these methods should produce the same\nparameters. We investigate this assertion.\n"
    },
    {
        "paper_id": 1705.05334,
        "authors": "Abeer ElBahrawy, Laura Alessandretti, Anne Kandler, Romualdo\n  Pastor-Satorras, Andrea Baronchelli",
        "title": "Evolutionary dynamics of the cryptocurrency market",
        "comments": null,
        "journal-ref": "Royal Society Open Science 4, 170623 (2017)",
        "doi": "10.1098/rsos.170623",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The cryptocurrency market surpassed the barrier of \\$100 billion market\ncapitalization in June 2017, after months of steady growth. Despite its\nincreasing relevance in the financial world, however, a comprehensive analysis\nof the whole system is still lacking, as most studies have focused exclusively\non the behaviour of one (Bitcoin) or few cryptocurrencies. Here, we consider\nthe history of the entire market and analyse the behaviour of 1,469\ncryptocurrencies introduced between April 2013 and June 2017. We reveal that,\nwhile new cryptocurrencies appear and disappear continuously and their market\ncapitalization is increasing (super-)exponentially, several statistical\nproperties of the market have been stable for years. These include the number\nof active cryptocurrencies, the market share distribution and the turnover of\ncryptocurrencies. Adopting an ecological perspective, we show that the\nso-called neutral model of evolution is able to reproduce a number of key\nempirical observations, despite its simplicity and the assumption of no\nselective advantage of one cryptocurrency over another. Our results shed light\non the properties of the cryptocurrency market and establish a first formal\nlink between ecological modelling and the study of this growing system. We\nanticipate they will spark further research in this direction.\n"
    },
    {
        "paper_id": 1705.05572,
        "authors": "Zuzana Krajcovicova, Pedro Pablo Perez-Velasco and Carlos Vazquez",
        "title": "A Novel Approach to Quantification of Model Risk for Practitioners",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Models continue to increase their already broad use across industry as well\nas their sophistication. Worldwide regulation oblige financial institutions to\nmanage and address model risk with the same severity as any other type of risk,\nwhich besides defines model risk as the potential for adverse consequences from\ndecisions based on incorrect and misused model outputs and reports. Model risk\nquantification is essential not only in meeting these requirements but for\ninstitution's basic internal operative. It is however a complex task as any\ncomprehensive quantification methodology should at least consider the data used\nfor building the model, its mathematical foundations, the IT infrastructure,\noverall performance and (most importantly) usage. Besides, the current amount\nof models and different mathematical modelling techniques is overwhelming.\n  Our proposal is to define quantification of model risk as a calculation of\nthe norm of some appropriate function that belongs to a Banach space, defined\nover a weighted Riemannian manifold endowed with the Fisher--Rao metric. The\naim of the present contribution is twofold: Introduce a sufficiently general\nand sound mathematical framework to cover the aforementioned points and\nillustrate how a practitioner may identify the relevant abstract concepts and\nput them to work.\n"
    },
    {
        "paper_id": 1705.05666,
        "authors": "Nathan Lassance and Fr\\'ed\\'eric Vrins",
        "title": "Minimum R\\'enyi Entropy Portfolios",
        "comments": "17 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Accounting for the non-normality of asset returns remains challenging in\nrobust portfolio optimization. In this article, we tackle this problem by\nassessing the risk of the portfolio through the \"amount of randomness\" conveyed\nby its returns. We achieve this by using an objective function that relies on\nthe exponential of R\\'enyi entropy, an information-theoretic criterion that\nprecisely quantifies the uncertainty embedded in a distribution, accounting for\nhigher-order moments. Compared to Shannon entropy, R\\'enyi entropy features a\nparameter that can be tuned to play around the notion of uncertainty. A\nGram-Charlier expansion shows that it controls the relative contributions of\nthe central (variance) and tail (kurtosis) parts of the distribution in the\nmeasure. We further rely on a non-parametric estimator of the exponential\nR\\'enyi entropy that extends a robust sample-spacings estimator initially\ndesigned for Shannon entropy. A portfolio selection application illustrates\nthat minimizing R\\'enyi entropy yields portfolios that outperform\nstate-of-the-art minimum variance portfolios in terms of risk-return-turnover\ntrade-off.\n"
    },
    {
        "paper_id": 1705.05882,
        "authors": "Marcel Nutz, Jos\\'e A. Scheinkman",
        "title": "Shorting in Speculative Markets",
        "comments": "Forthcoming in 'Journal of Finance'",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We propose a continuous-time model of trading with heterogeneous beliefs.\nRisk-neutral agents face quadratic costs-of-carry on positions and thus their\nmarginal valuations decrease with the size of their position, as it would be\nthe case for risk-averse agents. In the equilibrium models of heterogeneous\nbeliefs that followed Harrison-Kreps, investors are risk-neutral, short-selling\nis prohibited and agents face constant marginal costs of carrying positions.\nThe resulting resale option guarantees that the price exceeds the price of the\nasset when speculation is ruled out; the difference is identified as a bubble.\nIn our model increasing marginal costs entail that the price depends on asset\nsupply. Second, agents also value an option to delay, and this may cause the\nmarket to equilibrate below the buy-and-hold price. Third, we introduce the\npossibility of short-selling. A Hamilton-Jacobi-Bellman equation of a novel\nform quantifies precisely the influence of the costs-of-carry on the price. An\nunexpected decrease in shorting costs may lead to the collapse of a bubble;\nthis links the financial innovations that facilitated shorting of MBSs to the\nsubsequent collapse of prices.\n"
    },
    {
        "paper_id": 1705.05934,
        "authors": "Daniel Hackmann",
        "title": "Analytic techniques for option pricing under a hyperexponential L\\'{e}vy\n  model",
        "comments": "32 pages, 7 Figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We develop series expansions in powers of $q^{-1}$ and $q^{-1/2}$ of\nsolutions of the equation $\\psi(z) = q$, where $\\psi(z)$ is the Laplace\nexponent of a hyperexponential L\\'{e}vy process. As a direct consequence we\nderive analytic expressions for the prices of European call and put options and\ntheir Greeks (Theta, Delta, and Gamma) and a full asymptotic expansion of the\nshort-time Black-Scholes at-the-money implied volatility. Further we\ndemonstrate how the speed of numerical algorithms for pricing exotic options,\nwhich are based on the Laplace transform, may be increased.\n"
    },
    {
        "paper_id": 1705.05943,
        "authors": "Isaac M. Sonin and Konstantin Sonin",
        "title": "Banks as Tanks: A Continuous-Time Model of Financial Clearing",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We present a simple continuous-time model of clearing in financial networks.\nFinancial firms are represented as \"tanks\" filled with fluid (money), flowing\nin and out. Once \"pipes\" connecting \"tanks\" are open, the system reaches the\nclearing payment vector in finite time. This approach provides a simple\nrecursive solution to a classical static model of financial clearing in\nbankruptcy, and suggests a practical payment mechanism. With sufficient\nresources, a system of mutual obligations can be restructured into an\nequivalent system that has a cascade structure: there is a group of banks that\npaid off their debts, another group that owes money only to banks in the first\ngroup, and so on. Technically, we use the machinery of Markov chains to analyze\nevolution of a deterministic dynamical system.\n"
    },
    {
        "paper_id": 1705.06141,
        "authors": "Shaolin Ji, Hanqing Jin, Xiaomin Shi",
        "title": "Mean-variance portfolio selection with nonlinear wealth dynamics and\n  random coefficients",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper studies the continuous time mean-variance portfolio selection\nproblem with one kind of non-linear wealth dynamics. To deal the expectation\nconstraint, an auxiliary stochastic control problem is firstly solved by two\nnew generalized stochastic Riccati equations from which a candidate portfolio\nin feedback form is constructed, and the corresponding wealth process will\nnever cross the vertex of the parabola. In order to verify the optimality of\nthe candidate portfolio, the convex duality (requires the monotonicity of the\ncost function) is established to give another more direct expression of the\nterminal wealth level. The variance-optimal martingale measure and the link\nbetween the non-linear financial market and the classical linear market are\nalso provided. Finally, we obtain the efficient frontier in closed form. From\nour results, people are more likely to invest their money in riskless asset\ncompared with the classical linear market.\n"
    },
    {
        "paper_id": 1705.06208,
        "authors": "Stanislaw Drozdz, Andrzej Kulig, Jaroslaw Kwapien, Artur Niewiarowski,\n  Marek Stanuszek",
        "title": "Hierarchical organization of H. Eugene Stanley scientific collaboration\n  community in weighted network representation",
        "comments": null,
        "journal-ref": "Journal of Informetrics 11, 1114-1127 (2017)",
        "doi": "10.1016/j.joi.2017.09.009",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  By mapping the most advanced elements of the contemporary social\ninteractions, the world scientific collaboration network develops an extremely\ninvolved and heterogeneous organization. Selected characteristics of this\nheterogeneity are studied here and identified by focusing on the scientific\ncollaboration community of H. Eugene Stanley - one of the most prolific world\nscholars at the present time. Based on the Web of Science records as of March\n28, 2016, several variants of networks are constructed. It is found that the\nStanley #1 network - this in analogy to the Erd\\H{o}s # - develops a largely\nconsistent hierarchical organization and Stanley himself obeys rules of the\nsame hierarchy. However, this is seen exclusively in the weighted network\nrepresentation. When such a weighted network is evolving, an existing relevant\nmodel indicates that the spread of weight gets stimulation to the\nmultiplicative bursts over the neighbouring nodes, which leads to a balanced\ngrowth of interconnections among them. While not exclusive to Stanley, such a\nbehaviour is not a rule, however. Networks of other outstanding scholars\nstudied here more often develop a star-like form and the central hubs\nconstitute the outliers. This study is complemented by a spectral analysis of\nthe normalised Laplacian matrices derived from the weighted variants of the\ncorresponding networks and, among others, it points to the efficiency of such a\nprocedure for identifying the component communities and relations among them in\nthe complex weighted networks.\n"
    },
    {
        "paper_id": 1705.06533,
        "authors": "David Bauder, Taras Bodnar, Nestor Parolya and Wolfgang Schmid",
        "title": "Bayesian Inference of the Multi-Period Optimal Portfolio for an\n  Exponential Utility",
        "comments": "38 pages, 5 figures",
        "journal-ref": null,
        "doi": "10.1016/j.jmva.2019.104544",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We consider the estimation of the multi-period optimal portfolio obtained by\nmaximizing an exponential utility. Employing Jeffreys' non-informative prior\nand the conjugate informative prior, we derive stochastic representations for\nthe optimal portfolio weights at each time point of portfolio reallocation.\nThis provides a direct access not only to the posterior distribution of the\nportfolio weights but also to their point estimates together with uncertainties\nand their asymptotic distributions. Furthermore, we present the posterior\npredictive distribution for the investor's wealth at each time point of the\ninvestment period in terms of a stochastic representation for the future wealth\nrealization. This in turn makes it possible to use quantile-based risk measures\nor to calculate the probability of default. We apply the suggested Bayesian\napproach to assess the uncertainty in the multi-period optimal portfolio by\nconsidering assets from the FTSE 100 in the weeks after the British referendum\nto leave the European Union. The behaviour of the novel portfolio estimation\nmethod in a precarious market situation is illustrated by calculating the\npredictive wealth, the risk associated with the holding portfolio, and the\ndefault probability in each period.\n"
    },
    {
        "paper_id": 1705.06557,
        "authors": "Ron W. Nielsen",
        "title": "Application of Differential Equations in Projecting Growth Trajectories",
        "comments": "18 pages, 6615 words, 3 tables, 7 figures. arXiv admin note: text\n  overlap with arXiv:1510.06337",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Mathematical method based on a direct or indirect analysis of growth rates is\ndescribed. It is shown how simple assumptions and a relatively easy analysis\ncan be used to describe mathematically complicated trends and to predict\ngrowth. Only rudimentary knowledge of calculus is required. Projected\ntrajectories based on such simple initial assumptions are easier to accept and\nto understand than alternative complicated projections based on more\ncomplicated assumptions and on more intricate computational procedures.\nExamples of the growth of population and of the growth of the Gross Domestic\nProduct are used to illustrate the application of this method of forecasting.\n"
    },
    {
        "paper_id": 1705.06868,
        "authors": "Peter Mitic",
        "title": "Conduct Risk - distribution models with very thin Tails",
        "comments": "Presented at SYMCOMP 2017, Guimaraes Portugal, April 2017",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Regulatory requirements dictate that financial institutions must calculate\nrisk capital (funds that must be retained to cover future losses) at least\nannually. Procedures for doing this have been well-established for many years,\nbut recent developments in the treatment of conduct risk (the risk of loss due\nto the relationship between a financial institution and its customers) have\ncast doubt on 'standard' procedures. Regulations require that operational risk\nlosses should be aggregated by originating event. The effect is that a large\nnumber of small and medium-sized losses are aggregated into a small number of\nvery large losses, such that a risk capital calculation produces a hugely\ninflated result. To solve this problem, a novel distribution based on a\none-parameter probability density with an exponential of a fourth power is\nproposed, where the parameter is to be estimated. Symbolic computation is used\nto derive the necessary analytical expressions with which to formulate the\nproblem, and is followed by numeric calculations in R. Goodness-of-fit and\nparameter estimation are both determined by using a novel method developed\nspecifically for use with probability distribution functions. The results\ncompare favourably with an existing model that used a LogGamma Mixture density,\nfor which it was necessary to limit the frequency and severity of the losses.\nNo such limits were needed using the proposed exponential density.\n"
    },
    {
        "paper_id": 1705.06899,
        "authors": "Raymond Brummelhuis and Zhongmin Luo",
        "title": "CDS Rate Construction Methods by Machine Learning Techniques",
        "comments": "51 pages; 21 Figures; 15 Tables",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Regulators require financial institutions to estimate counterparty default\nrisks from liquid CDS quotes for the valuation and risk management of OTC\nderivatives. However, the vast majority of counterparties do not have liquid\nCDS quotes and need proxy CDS rates. Existing methods cannot account for\ncounterparty-specific default risks; we propose to construct proxy CDS rates by\nassociating to illiquid counterparty liquid CDS Proxy based on Machine Learning\nTechniques. After testing 156 classifiers from 8 most popular classifier\nfamilies, we found that some classifiers achieve highly satisfactory accuracy\nrates. Furthermore, we have rank-ordered the performances and investigated\nperformance variations amongst and within the 8 classifier families. This paper\nis, to the best of our knowledge, the first systematic study of CDS Proxy\nconstruction by Machine Learning techniques, and the first systematic\nclassifier comparison study based entirely on financial market data. Its\nfindings both confirm and contrast existing classifier performance literature.\nGiven the typically highly correlated nature of financial data, we investigated\nthe impact of correlation on classifier performance. The techniques used in\nthis paper should be of interest for financial institutions seeking a CDS Proxy\nmethod, and can serve for proxy construction for other financial variables.\nSome directions for future research are indicated.\n"
    },
    {
        "paper_id": 1705.06918,
        "authors": "Panagiotis Christodoulou, Nils Detering, Thilo Meyer-Brandis",
        "title": "Local risk-minimization with multiple assets under illiquidity with\n  applications in energy markets",
        "comments": "36 pages",
        "journal-ref": "International Journal of Theoretical and Applied Finance, Vol. 21,\n  No. 4 (2018) (44 pages)",
        "doi": "10.1142/S0219024918500280",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We propose a hedging approach for general contingent claims when liquidity is\na concern and trading is subject to transaction cost. Multiple assets with\ndifferent liquidity levels are available for hedging. Our risk criterion\ntargets a tradeoff between minimizing the risk against fluctuations in the\nstock price and incurring low liquidity costs. Following \\c{C}etin U., Jarrow\nR.A., and Protter P. (2004) we work in an arbitrage-free setting assuming a\nsupply curve for each asset. In discrete time, following the ideas in Schweizer\nM. (1998) and Lamberton D., Pham H., Schweizer M. (1998) we prove the existence\nof a locally risk-minimizing strategy under mild conditions on the price\nprocess. Under stochastic and time-dependent liquidity risk we give a\nclosed-form solution for an optimal strategy in the case of a linear supply\ncurve model. Finally we show how our hedging method can be applied in energy\nmarkets where futures with different maturities are available for trading. The\nfutures closest to their delivery period are usually the most liquid but\ndepending on the contingent claim not necessary optimal in terms of hedging. In\na simulation study we investigate this tradeoff and compare the resulting hedge\nstrategies with the classical ones.\n"
    },
    {
        "paper_id": 1705.07092,
        "authors": "Mikhail Goykhman",
        "title": "Wealth dynamics in a sentiment-driven market",
        "comments": "Stock exchange implementation is available here:\n  https://github.com/Goykhman/Stock_exchange",
        "journal-ref": null,
        "doi": "10.1016/j.physa.2017.06.023",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We study dynamics of a simulated world with stock and money, driven by the\nexternally given processes which we refer to as sentiments. The considered\nsentiments influence the buy/sell stock trading attitude, the perceived price\nuncertainty, and the trading intensity of all or a part of the market\nparticipants. We study how the wealth of market participants evolves in time in\nsuch an environment. We discuss the opposite perspective in which the\nparameters of the sentiment processes can be inferred a posteriori from the\nobserved market behavior.\n"
    },
    {
        "paper_id": 1705.07155,
        "authors": "Marco D'Errico, Tarik Roukny",
        "title": "Compressing Over-the-Counter Markets",
        "comments": "European Systemic Risk Board - Working Paper Number 44",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Over-the-counter markets are at the center of the postcrisis global reform of\nthe financial system. We show how the size and structure of such markets can\nundergo rapid and extensive changes when participants engage in portfolio\ncompression, a post-trade netting technology. Tightly-knit and concentrated\ntrading structures, as featured by many large over-the-counter markets, are\nespecially susceptible to reductions of notional and reconfigurations of\nnetwork structure resulting from compression activities. Using\ntransaction-level data on credit-default-swaps markets, we estimate reduction\nlevels consistent with the historical development observed in these markets\nsince the Global Financial Crisis. Finally, we study the effect of a mandate to\ncentrally clear over-the-counter markets. When participants engage in both\ncentral clearing and portfolio compression, we find large netting failures if\nclearinghouses proliferate. Allowing for compression across clearinghouses\nby-and-large offsets this adverse effect.\n"
    },
    {
        "paper_id": 1705.07352,
        "authors": "Tiziano De Angelis, Fabien Gensbittel, St\\'ephane Villeneuve",
        "title": "A Dynkin game on assets with incomplete information on the return",
        "comments": "37 pages, 1 figure, new Section 8 added; keywords: Zero-sum games;\n  Nash equilibrium; incomplete information; free boundaries;",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper studies a 2-players zero-sum Dynkin game arising from pricing an\noption on an asset whose rate of return is unknown to both players. Using\nfiltering techniques we first reduce the problem to a zero-sum Dynkin game on a\nbi-dimensional diffusion $(X,Y)$. Then we characterize the existence of a Nash\nequilibrium in pure strategies in which each player stops at the hitting time\nof $(X,Y)$ to a set with moving boundary. A detailed description of the\nstopping sets for the two players is provided along with global $C^1$\nregularity of the value function.\n"
    },
    {
        "paper_id": 1705.07472,
        "authors": "Sigrid K\\\"allblad, Thaleia Zariphopoulou",
        "title": "On the Black's equation for the risk tolerance function",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We analyze a nonlinear equation proposed by F. Black (1968) for the optimal\nportfolio function in a log-normal model. We cast it in terms of the risk\ntolerance function and provide, for general utility functions, existence,\nuniqueness and regularity results, and we also examine various monotonicity,\nconcavity/convexity and S-shape properties. Stronger results are derived for\nutilities whose inverse marginal belongs to a class of completely monotonic\nfunctions.\n"
    },
    {
        "paper_id": 1705.08022,
        "authors": "Yash Sharma",
        "title": "Using Macroeconomic Forecasts to Improve Mean Reverting Trading\n  Strategies",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  A large class of trading strategies focus on opportunities offered by the\nyield curve. In particular, a set of yield curve trading strategies are based\non the view that the yield curve mean-reverts. Based on these strategies'\npositive performance, a multiple pairs trading strategy on major currency pairs\nwas implemented. To improve the algorithm's performance, machine learning\nforecasts of a series of pertinent macroeconomic variables were factored in, by\noptimizing the weights of the trading signals. This resulted in a clear\nimprovement in the APR over the evaluation period, demonstrating that\nmacroeconomic indicators, not only technical indicators, should be considered\nin trading strategies.\n"
    },
    {
        "paper_id": 1705.08033,
        "authors": "Josue Ortega",
        "title": "Social Integration in Two-Sided Matching Markets",
        "comments": null,
        "journal-ref": "Journal of Mathematical Economics, 78 (2002) 119-126",
        "doi": "10.1016/j.jmateco.2018.08.003",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  When several two-sided matching markets merge into one, it is inevitable that\nsome agents will become worse off if the matching mechanism used is stable. I\nformalize this observation by defining the property of integration\nmonotonicity, which requires that every agent becomes better off after any\nnumber of matching markets merge. Integration monotonicity is also incompatible\nwith the weaker efficiency property of Pareto optimality.\n  Nevertheless, I obtain two possibility results. First, stable matching\nmechanisms never hurt more than one-half of the society after the integration\nof several matching markets occurs. Second, in random matching markets there\nare positive expected gains from integration for both sides of the market,\nwhich I quantify.\n"
    },
    {
        "paper_id": 1705.0824,
        "authors": "Shan Lu, Jichang Zhao, Huiwen Wang and Ruoen Ren",
        "title": "Herding boosts too-connected-to-fail risk in stock market of China",
        "comments": null,
        "journal-ref": null,
        "doi": "10.1016/j.physa.2018.04.020",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The crowd panic and its contagion play non-negligible roles at the time of\nthe stock crash, especially for China where inexperienced investors dominate\nthe market. However, existing models rarely consider investors in networking\nstocks and accordingly miss the exact knowledge of how panic contagion leads to\nabrupt crash. In this paper, by networking stocks of sharing common mutual\nfunds, a new methodology of investigating the market crash is presented. It is\nsurprisingly revealed that the herding, which origins in the mimic of seeking\nfor high diversity across investment strategies to lower individual risk, will\nproduce too-connected-to-fail stocks and reluctantly boosts the systemic risk\nof the entire market. Though too-connected stocks might be relatively stable\nduring the crisis, they are so influential that a small downward fluctuation\nwill cascade to trigger severe drops of massive successor stocks, implying that\ntheir falls might be unexpectedly amplified by the collective panic and result\nin the market crash. Our findings suggest that the whole picture of portfolio\nstrategy has to be carefully supervised to reshape the stock network.\n"
    },
    {
        "paper_id": 1705.08291,
        "authors": "Oleksii Mostovyi, Mihai S\\^irbu",
        "title": "Sensitivity analysis of the utility maximization problem with respect to\n  model perturbations",
        "comments": "preliminary version",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We study the sensitivity of the expected utility maximization problem in a\ncontinuous semi-martingale market with respect to small changes in the market\nprice of risk. Assuming that the preferences of a rational economic agent are\nmodeled with a general utility function, we obtain a second-order expansion of\nthe value function, a first-order approximation of the terminal wealth, and\nconstruct trading strategies that match the indirect utility function up to the\nsecond order. If a risk-tolerance wealth process exists, using it as a\nnum\\'eraire and under an appropriate change of measure, we reduce the\napproximation problem to a Kunita-Watanabe decomposition.\n"
    },
    {
        "paper_id": 1705.08301,
        "authors": "Samuel N. Cohen",
        "title": "Data and uncertainty in extreme risks - a nonlinear expectations\n  approach",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Estimation of tail quantities, such as expected shortfall or Value at Risk,\nis a difficult problem. We show how the theory of nonlinear expectations, in\nparticular the Data-robust expectation introduced in [5], can assist in the\nquantification of statistical uncertainty for these problems. However, when we\nare in a heavy-tailed context (in particular when our data are described by a\nPareto distribution, as is common in much of extreme value theory), the theory\nof [5] is insufficient, and requires an additional regularization step which we\nintroduce. By asking whether this regularization is possible, we obtain a\nqualitative requirement for reliable estimation of tail quantities and risk\nmeasures, in a Pareto setting.\n"
    },
    {
        "paper_id": 1705.08411,
        "authors": "Zailei Cheng",
        "title": "Optimal Dividends in the Dual Risk Model under a Stochastic Interest\n  Rate",
        "comments": "19 pages, 2 figures",
        "journal-ref": "International Journal of Financial Engineering, Vol. 4, No. 1\n  (2017)",
        "doi": "10.1142/S2424786317500104",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Optimal dividend strategy in dual risk model is well studied in the\nliteratures. But to the best of our knowledge, all the previous works assumes\ndeterministic interest rate. In this paper, we study the optimal dividends\nstrategy in dual risk model, under a stochastic interest rate, assuming the\ndiscounting factor follows a geometric Brownian motion or exponential L\\'evy\nprocess. We will show that closed form solutions can be obtained.\n"
    },
    {
        "paper_id": 1705.08536,
        "authors": "Masanari Asano, Irina Basieva, Andrei Khrennikov, Masanori Ohya,\n  Yoshiharu Tanaka",
        "title": "A Quantum-like Model of Selection Behavior",
        "comments": null,
        "journal-ref": "J. Math. Psychology78, 2-12 (2017)",
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper, we introduce a new model of selection behavior under risk that\ndescribes an essential cognitive process for comparing values of objects and\nmaking a selection decision. This model is constructed by the quantum-like\napproach that employs the state representation specific to quantum theory,\nwhich has the mathematical framework beyond the classical probability theory.\nWe show that our quantum approach can clearly explain the famous examples of\nanomalies for the expected utility theory, the Ellsberg paradox, the Machina\nparadox and the disparity between WTA and WTP. Further, we point out that our\nmodel mathematically specifies the characteristics of the probability weighting\nfunction and the value function, which are basic concepts in the prospect\ntheory.\n"
    },
    {
        "paper_id": 1705.08545,
        "authors": "Kateryna Kononova, Anton Dek",
        "title": "Financial Time Series Forecasting: Semantic Analysis Of Economic News",
        "comments": "in Ukranian",
        "journal-ref": "Neuro-Fuzzy Modeling Techniques in Economics y.2016 ISSN 2306-3289",
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The paper proposes a method of financial time series forecasting taking into\naccount the semantics of news. For the semantic analysis of financial news the\nsampling of negative and positive words in economic sense was formed based on\nLoughran McDonald Master Dictionary. The sampling included the words with high\nfrequency of occurrence in the news of financial markets. For single-root words\nit has been left only common part that allows covering few words for one\nrequest. Neural networks were chosen for modeling and forecasting. To automate\nthe process of extracting information from the economic news a script was\ndeveloped in the MATLAB Simulink programming environment, which is based on the\ngenerated sampling of positive and negative words. Experimental studies with\ndifferent architectures of neural networks showed a high adequacy of\nconstructed models and confirmed the feasibility of using information from news\nfeeds to predict the stock prices.\n"
    },
    {
        "paper_id": 1705.08955,
        "authors": "Mario Coccia",
        "title": "Classifications of Innovations Survey and Future Directions",
        "comments": "22",
        "journal-ref": "Working Paper Ceris del Consiglio Nazionale delle Ricerche, vol.\n  8, n. 2 (2006)",
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The purpose of this paper is to focus on similarity and/or heterogeneity of\ntaxonomies of innovation present in the economic fields to show as the economic\nliterature uses different names to indicate the same type of technical change\nand innovation, and the same name for different types of innovation. This\nambiguity of classification makes it impossible to compare the various studies;\nmoreover the numerous typologies existing in the economics of innovation,\ntechnometrics, economics of technical change, management of technology, etc.,\nhave hindered the development of knowledge in these fields. The research\npresents also new directions on the classification of innovation that try to\novercome these problems.\n"
    },
    {
        "paper_id": 1705.09418,
        "authors": "Yan-Yu Chiou, Mei-Yuan Chen, Jau-er Chen",
        "title": "Nonparametric Regression with Multiple Thresholds: Estimation and\n  Inference",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper examines nonparametric regression with an exogenous threshold\nvariable, allowing for an unknown number of thresholds. Given the number of\nthresholds and corresponding threshold values, we first establish the\nasymptotic properties of the local constant estimator for a nonparametric\nregression with multiple thresholds. However, the number of thresholds and\ncorresponding threshold values are typically unknown in practice. We then use\nour testing procedure to determine the unknown number of thresholds and derive\nthe limiting distribution of the proposed test. The Monte Carlo simulation\nresults indicate the adequacy of the modified test and accuracy of the\nsequential estimation of the threshold values. We apply our testing procedure\nto an empirical study of the 401(k) retirement savings plan with income\nthresholds.\n"
    },
    {
        "paper_id": 1705.09505,
        "authors": "Mathias Beiglboeck, Alexander Cox, Martin Huesmann",
        "title": "The geometry of multi-marginal Skorokhod Embedding",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The Skorokhod Embedding Problem (SEP) is one of the classical problems in the\nstudy of stochastic processes, with applications in many different fields (cf.~\nthe surveys \\cite{Ob04,Ho11}). Many of these applications have natural\nmulti-marginal extensions leading to the \\emph{(optimal) multi-marginal\nSkorokhod problem} (MSEP). Some of the first papers to consider this problem\nare \\cite{Ho98b, BrHoRo01b, MaYo02}. However, this turns out to be difficult\nusing existing techniques: only recently a complete solution was be obtained in\n\\cite{CoObTo15} establishing an extension of the Root construction, while other\ninstances are only partially answered or remain wide open.\n  In this paper, we extend the theory developed in \\cite{BeCoHu14} to the\nmulti-marginal setup which is comparable to the extension of the optimal\ntransport problem to the multi-marginal optimal transport problem. As for the\none-marginal case, this viewpoint turns out to be very powerful. In particular,\nwe are able to show that all classical optimal embeddings have natural\nmulti-marginal counterparts. Notably these different constructions are linked\nthrough a joint geometric structure and the classical solutions are recovered\nas particular cases.\n  Moreover, our results also have consequences for the study of the martingale\ntransport problem as well as the peacock problem.\n"
    },
    {
        "paper_id": 1705.098,
        "authors": "Guy Uziel and Ran El-Yaniv",
        "title": "Growth-Optimal Portfolio Selection under CVaR Constraints",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Online portfolio selection research has so far focused mainly on minimizing\nregret defined in terms of wealth growth. Practical financial decision making,\nhowever, is deeply concerned with both wealth and risk. We consider online\nlearning of portfolios of stocks whose prices are governed by arbitrary\n(unknown) stationary and ergodic processes, where the goal is to maximize\nwealth while keeping the conditional value at risk (CVaR) below a desired\nthreshold. We characterize the asymptomatically optimal risk-adjusted\nperformance and present an investment strategy whose portfolios are guaranteed\nto achieve the asymptotic optimal solution while fulfilling the desired risk\nconstraint. We also numerically demonstrate and validate the viability of our\nmethod on standard datasets.\n"
    },
    {
        "paper_id": 1705.09827,
        "authors": "Erhan Bayraktar and Alexander Munk",
        "title": "Mini-Flash Crashes, Model Risk, and Optimal Execution",
        "comments": "Final version. To appear in Market Microstructure and liquidity.\n  Keywords: Flash crash, model error, optimal execution",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Oft-cited causes of mini-flash crashes include human errors, endogenous\nfeedback loops, the nature of modern liquidity provision, fundamental value\nshocks, and market fragmentation. We develop a mathematical model which\ncaptures aspects of the first three explanations. Empirical features of recent\nmini-flash crashes are present in our framework. For example, there are periods\nwhen no such events will occur. If they do, even just before their onset,\nmarket participants may not know with certainty that a disruption will unfold.\nOur mini-flash crashes can materialize in both low and high trading volume\nenvironments and may be accompanied by a partial synchronization in order\nsubmission.\n  Instead of adopting a classically-inspired equilibrium approach, we borrow\nideas from the optimal execution literature. Each of our agents begins with\nbeliefs about how his own trades impact prices and how prices would move in his\nabsence. They, along with other market participants, then submit orders which\nare executed at a common venue. Naturally, this leads us to explicitly\ndistinguish between how prices actually evolve and our agents' opinions. In\nparticular, every agent's beliefs will be expressly incorrect.\n"
    },
    {
        "paper_id": 1705.09955,
        "authors": "Peter Mitic",
        "title": "Standardised Reputation Measurement",
        "comments": "8 pages, submitted to IDEAL 2017, October/November Guilin China",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Well-defined formal definitions for sentiment and opinion are extended to\nincorporate the necessary elements to provide a formal quantitative definition\nof reputation. This definition takes the form of a time-based index, in which\neach element is a function of a collection of opinions mined during a given\ntime period. The resulting formal definition is validated against informal\nnotions of reputation. Practical aspects of data procurement to support such a\nreputation index are discussed. The assumption that all mined opinions comprise\na complete set is questioned. A case is made that unexpressed positive\nsentiment exists, and can be quantified.\n"
    },
    {
        "paper_id": 1705.09965,
        "authors": "J. T. Manhire",
        "title": "The Action Principle in Market Mechanics",
        "comments": "21 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
        "abstract": "  This paper explores the possibility that asset prices, especially those\ntraded in large volume on public exchanges, might comply with specific physical\nlaws of motion and probability. The paper first examines the basic dynamics of\nasset price displacement and finds one can model this dynamic as a harmonic\noscillator at local \"slices\" of elapsed time. Based on this finding, the paper\ntheorizes that price displacements are constrained, meaning they have extreme\nvalues beyond which they cannot go when measured over a large number of\nsequential periods. By assuming price displacements are also subject to the\nprinciple of stationary action, the paper explores a method for measuring\nspecific probabilities of future price displacements based on prior historical\ndata. Testing this theory with two prevalent stock indices suggests it can make\naccurate forecasts as to constraints on extreme price movements during market\n\"crashes\" and probabilities of specific price displacements at other times.\n"
    },
    {
        "paper_id": 1705.10294,
        "authors": "Tariq Abbasi and Hans Weigand",
        "title": "The Impact of Digital Financial Services on Firm's Performance: a\n  Literature Review",
        "comments": "15 pages, 7 tables and 1 figure",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Digital Financial Services continue to expand and replace the delivery of\ntraditional banking services to the customers through innovative technologies\nto meet the growing complex needs and globalization challenges. These\ndiversified digital products help the organizations (service providers) to\nimprove their firm performance and to remain competitive in the market. It also\nassists in increasing market share to grow their profitability and improve\nfinancial position. There is a growing literature on Digital Financial Services\nand firm performance. At this point of the development, this paper systemically\nreviews the existing (within last one decade) amount of literature\ninvestigating the impact of DFS on firm performance, analyzes and identifies\nthe research gaps. We identify 39 works that have appeared in a wide range of\npeer-reviewed scientific journals. We classify the methodologies and approaches\nthat researchers have used to predict the effect of such services on the\nfinancial growth and profitability. We observe that despite rapid technological\nadvancement in DFS during the last ten years, Digital Financial Services being\nthe factor affecting firm performance did not get the reasonable attention in\nacademic literature. One of the reason is that almost all the authors limit\ntheir research to banking sector while ignoring others particularly mobile\nnetwork operators (providing branchless banking) and new non-banking entrants.\nWe also notice that newer researchers often ignore past research and\ninvestigate the same issues. This study also makes several recommendations and\nsuggest directions for future research in this still emerging field.\n"
    },
    {
        "paper_id": 1705.10454,
        "authors": "Tim Leung and Brian Ward",
        "title": "Dynamic Index Tracking and Risk Exposure Control Using Derivatives",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We develop a methodology for index tracking and risk exposure control using\nfinancial derivatives. Under a continuous-time diffusion framework for price\nevolution, we present a pathwise approach to construct dynamic portfolios of\nderivatives in order to gain exposure to an index and/or market factors that\nmay be not directly tradable. Among our results, we establish a general\ntracking condition that relates the portfolio drift to the desired exposure\ncoefficients under any given model. We also derive a slippage process that\nreveals how the portfolio return deviates from the targeted return. In our\nmulti-factor setting, the portfolio's realized slippage depends not only on the\nrealized variance of the index, but also the realized covariance among the\nindex and factors. We implement our trading strategies under a number of\nmodels, and compare the tracking strategies and performances when using\ndifferent derivatives, such as futures and options.\n"
    },
    {
        "paper_id": 1705.10974,
        "authors": "Peter Mitic",
        "title": "Trends in Banking 2017 and onwards",
        "comments": "Proceedings 29th SASE Conference, Lyon France, June-July 2017",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The changing nature of the relationship between a retail bank and its\ncustomers is examined, particularly with respect to new financial concepts,\ndebt and regulation. The traditional image of a bank is portrayed as a physical\nbuilding a classical Doric portico. This image conveys concepts of service,\nsoundness, strength, stability and security (\"five-S\"). That \"five-S\" concept\nis changing, and the evidence for changes that affect customers directly is\nconsidered. A fundamental legal problem associated with those changes is\nhighlighted: a bank is no longer solely responsible for the safeguard of\ncustomer monies. A solution to this problem is proposed: banks should be\njointly liable with perpetrators of criminal activity in the event of frauds as\nan encouragement to recognise and mitigate fraud.\n"
    },
    {
        "paper_id": 1706.00203,
        "authors": "Abhijit Chakraborty, Hazem Krichene, Hiroyasu Inoue, Yoshi Fujiwara",
        "title": "Characterization of the community structure in a large-scale production\n  network in Japan",
        "comments": "12 pages, 8 figures",
        "journal-ref": "Physica A: Statistical Mechanics and its Applications Volume 513,\n  1 January 2019, Pages 210-221",
        "doi": "10.1016/j.physa.2018.08.175",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Inter-firm organizations, which play a driving role in the economy of a\ncountry, can be represented in the form of a customer-supplier network. Such a\nnetwork exhibits a heavy-tailed degree distribution, disassortative mixing and\na prominent community structure. We analyze a large-scale data set of\ncustomer-supplier relationships containing data from one million Japanese\nfirms. Using a directed network framework, we show that the production network\nexhibits the characteristics listed above. We conduct detailed investigations\nto characterize the communities in the network. The topology within smaller\ncommunities is found to be very close to a tree-like structure but becomes\ndenser as the community size increases. A large fraction (~40%) of firms with\nrelatively small in- or out-degrees have customers or suppliers solely from\nwithin their own communities, indicating interactions of a highly local nature.\nThe interaction strengths between communities as measured by the\ninter-community link weights follow a highly heterogeneous distribution. We\nfurther present the statistically significant over-expressions of different\nprefectures and sectors within different communities.\n"
    },
    {
        "paper_id": 1706.00263,
        "authors": "Laurent Devineau, Pierre-Edouard Arrouy, Paul Bonnefoy, Alexandre\n  Boumezoued",
        "title": "Fast calibration of the Libor Market Model with Stochastic Volatility\n  and Displaced Diffusion",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper demonstrates the efficiency of using Edgeworth and Gram-Charlier\nexpansions in the calibration of the Libor Market Model with Stochastic\nVolatility and Displaced Diffusion (DD-SV-LMM). Our approach brings together\ntwo research areas; first, the results regarding the SV-LMM since the work of\nWu and Zhang (2006), especially on the moment generating function, and second\nthe approximation of density distributions based on Edgeworth or Gram-Charlier\nexpansions. By exploring the analytical tractability of moments up to fourth\norder, we are able to perform an adjustment of the reference Bachelier model\nwith normal volatilities for skewness and kurtosis, and as a by-product to\nderive a smile formula relating the volatility to the moneyness with\ninterpretable parameters. As a main conclusion, our numerical results show a\n98% reduction in computational time for the DD-SV-LMM calibration process\ncompared to the classical numerical integration method developed by Heston\n(1993).\n"
    },
    {
        "paper_id": 1706.00284,
        "authors": "Christoph Siebenbrunner",
        "title": "Clearing algorithms and network centrality",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  I show that the solution of a standard clearing model commonly used in\ncontagion analyses for financial systems can be expressed as a specific form of\na generalized Katz centrality measure under conditions that correspond to a\nsystem-wide shock. This result provides a formal explanation for earlier\nempirical results which showed that Katz-type centrality measures are closely\nrelated to contagiousness. It also allows assessing the assumptions that one is\nmaking when using such centrality measures as systemic risk indicators. I\nconclude that these assumptions should be considered too strong and that, from\na theoretical perspective, clearing models should be given preference over\ncentrality measures in systemic risk analyses.\n"
    },
    {
        "paper_id": 1706.0033,
        "authors": "Laurent Pagnier and Philippe Jacquod",
        "title": "How fast can one overcome the paradox of the energy transition? A\n  physico-economic model for the European power grid",
        "comments": "13 pages, 11 figures and 2 tables",
        "journal-ref": "Energy 157, 550-560 (2018)",
        "doi": "10.1016/j.energy.2018.05.185",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The paradox of the energy transition is that the low marginal costs of new\nrenewable energy sources (RES) drag electricity prices down and discourage\ninvestments in flexible productions that are needed to compensate for the lack\nof dispatchability of the new RES. The energy transition thus discourages the\ninvestments that are required for its own harmonious expansion. To investigate\nhow this paradox can be overcome, we argue that, under certain assumptions,\nfuture electricity prices are rather accurately modeled from the residual load\nobtained by subtracting non-flexible productions from the load. Armed with the\nresulting economic indicator, we investigate future revenues for European power\nplants with various degree of flexibility. We find that, if neither carbon\ntaxes nor fuel prices change, flexible productions would be financially\nrewarded better and sooner if the energy transition proceeds faster but at more\nor less constant total production, i.e. by reducing the production of thermal\npower plants at the same rate as the RES production increases. Less flexible\nproductions, on the other hand, would see their revenue grow more moderately.\nOur results indicate that a faster energy transition with a quicker withdrawal\nof thermal power plants would reward flexible productions faster.\n"
    },
    {
        "paper_id": 1706.00467,
        "authors": "Hynek Lavicka, Jiri Kracik",
        "title": "Fluctuation analysis of electric power loads in Europe: Correlation\n  multifractality vs. Distribution function multifractality",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We analyze the time series of the power loads of the 35 separated countries\npublicly sharing hourly data through ENTSO-E platform for more than 5 years. We\napply the Multifractal Detrended Fluctuation Analysis for the demonstration of\nthe multifractal nature, autocorrelation and the distribution function\nfundamentals. Additionally, we improved the basic method described by\nKanterhardt, et al using uniform shuffling and surrogate the datasets to prove\nthe robustness of the results with respect to the non-linear effects of the\nprocesses. All the datasets exhibit multifractality in the distribution\nfunction as well as in the autocorrelation function. The basic differences\nbetween individual states are manifested in the width of the multifractal\nspectra and in the location of the maximum. We present the hypothesis about the\nproduction portfolio and the export/import dependences.\n"
    },
    {
        "paper_id": 1706.00849,
        "authors": "Artem Hulko and Mark Whitmeyer",
        "title": "A Game of Nontransitive Dice",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We consider a two player simultaneous-move game where the two players each\nselect any permissible $n$-sided die for a fixed integer $n$. A player wins if\nthe outcome of his roll is greater than that of his opponent. Remarkably, for\n$n>3$, there is a unique Nash Equilibrium in pure strategies. The unique Nash\nEquilibrium is for each player to throw the Standard $n$-sided die, where each\nside has a different number. Our proof of uniqueness is constructive. We\nintroduce an algorithm with which, for any nonstandard die, we may generate\nanother die that beats it.\n"
    },
    {
        "paper_id": 1706.00873,
        "authors": "Jean-Pierre Fouque and Yuri F. Saporito",
        "title": "Heston Stochastic Vol-of-Vol Model for Joint Calibration of VIX and S&P\n  500 Options",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  A parsimonious generalization of the Heston model is proposed where the\nvolatility-of-volatility is assumed to be stochastic. We follow the\nperturbation technique of Fouque et al (2011, CUP) to derive a first order\napproximation of the price of options on a stock and its volatility index. This\napproximation is given by Heston's quasi-closed formula and some of its Greeks.\nIt can be very efficiently calculated since it requires to compute only Fourier\nintegrals and the solution of simple ODE systems. We exemplify the calibration\nof the model with S&P 500 and VIX data.\n"
    },
    {
        "paper_id": 1706.00948,
        "authors": "Xin-Yao Qian",
        "title": "Financial Series Prediction: Comparison Between Precision of Time Series\n  Models and Machine Learning Methods",
        "comments": "The successful result can be greatly attributed to overfitting. After\n  careful consideration, I think the conclusion is not that convincible and\n  valuable",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Precise financial series predicting has long been a difficult problem because\nof unstableness and many noises within the series. Although Traditional time\nseries models like ARIMA and GARCH have been researched and proved to be\neffective in predicting, their performances are still far from satisfying.\nMachine Learning, as an emerging research field in recent years, has brought\nabout many incredible improvements in tasks such as regressing and classifying,\nand it's also promising to exploit the methodology in financial time series\npredicting. In this paper, the predicting precision of financial time series\nbetween traditional time series models and mainstream machine learning models\nincluding some state-of-the-art ones of deep learning are compared through\nexperiment using real stock index data from history. The result shows that\nmachine learning as a modern method far surpasses traditional models in\nprecision.\n"
    },
    {
        "paper_id": 1706.01254,
        "authors": "Thibaut Mastrolia",
        "title": "Moral hazard in welfare economics: on the advantage of Planner's advices\n  to manage employees' actions",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper, we study moral hazard problems in contract theory by adding an\nexogenous Planner to manage the actions of Agents hired by a Principal. We\nprovide conditions ensuring that Pareto optima exist for the Agents using the\nscalarization method associated with the multi-objective optimization problem\nand we solve the problem of the Principal by finding optimal remunerations\ngiven to the Agents. We illustrate our study with a linear-quadratic model by\ncomparing the results obtained when we add a Planner in the\nPrincipal/multi-Agents problem with the results obtained in the classical\nsecond-best case. More particularly in this example, we give necessary and\nsufficient conditions ensuring that Pareto optima are Nash equilibria and we\nprove that the Principal takes the benefit of the action of the Planner in some\ncases\n"
    },
    {
        "paper_id": 1706.01437,
        "authors": "Obryan Poyser",
        "title": "Exploring the determinants of Bitcoin's price: an application of\n  Bayesian Structural Time Series",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Currently, there is no consensus on the real properties of Bitcoin. The\ndiscussion comprises its use as a speculative or safe haven assets, while other\nauthors argue that the augmented attractiveness could end accomplishing money's\nfunctions that economic theory demands. This paper explores the association\nbetween Bitcoin's market price and a set of internal and external factors using\nBayesian Structural Time Series Approach. I aim to contribute to the discussion\nby differentiating among several attractiveness sources and employing a method\nthat provides a more flexible analytic framework that decompose each of the\ncomponents of the time series, apply variable selection, include information on\nprevious studies, and dynamically examine the behavior of the explanatory\nvariables, all in a transparent and tractable setting. The results show that\nthe Bitcoin price is negatively associated with a neutral investor's sentiment,\ngold's price and Yuan to USD exchange rate, while positively related to stock\nmarket index, USD to Euro exchange rate and variated signs among the different\ncountries' search trends. Hence, I find that Bitcoin has mixed properties since\nstill seems to act as a speculative, safe haven and a potential a capital\nflights instrument.\n"
    },
    {
        "paper_id": 1706.01534,
        "authors": "Foad Shokrollahi and Tommi Sottinen",
        "title": "Hedging in fractional Black-Scholes model with transaction costs",
        "comments": null,
        "journal-ref": "Statistics & Probability Letters, Volume 130, November 2017",
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We consider conditional-mean hedging in a fractional Black-Scholes pricing\nmodel in the presence of proportional transaction costs. We develop an explicit\nformula for the conditional-mean hedging portfolio in terms of the recently\ndiscovered explicit conditional law of the fractional Brownian motion.\n"
    },
    {
        "paper_id": 1706.01562,
        "authors": "Belkacem Berdjane",
        "title": "Pricing Asian options for NIG and VG Levy markets",
        "comments": "in French",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this work, we study the value of an Asian option in the case of\nexponential Levy markets. More specifically, we are interested in the NIG\n(normal inverse Gaussian) the VG (variance gamma) models. The exponential Levy\nmodels produce incomplete markets. There are therefore an infinite number of\nequivalent martingale measures. We are interested in two methods of\nconstructing of the risk-neutral measures. The first is based on the Esscher\ntransform, and the other consists of bringing a risk-neutral correction on the\ndynamics of the trajectories. It turns out, according to the numerical results\nobtained, that the two methods generally produce the same prices.\n"
    },
    {
        "paper_id": 1706.01666,
        "authors": "Laurent Pagnier and Philippe Jacquod",
        "title": "A predictive pan-European economic and production dispatch model for the\n  energy transition in the electricity sector",
        "comments": "6 pages, 8 figures",
        "journal-ref": "2017 IEEE Manchester PowerTech",
        "doi": "10.1109/PTC.2017.7980982",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The energy transition is well underway in most European countries. It has a\ngrowing impact on electric power systems as it dramatically modifies the way\nelectricity is produced. To ensure a safe and smooth transition towards a\npan-European electricity production dominated by renewable sources, it is of\nparamount importance to anticipate how production dispatches will evolve, to\nunderstand how increased fluctuations in power generations can be absorbed at\nthe pan-European level and to evaluate where the resulting changes in power\nflows will require significant grid upgrades. To address these issues, we\nconstruct an aggregated model of the pan-European transmission network which we\ncouple to an optimized, few-parameter dispatch algorithm to obtain time- and\ngeographically-resolved production profiles. We demonstrate the validity of our\ndispatch algorithm by reproducing historical production time series for all\npower productions in fifteen different European countries. Having calibrated\nour model in this way, we investigate future production profiles at later\nstages of the energy transition - determined by planned future production\ncapacities - and the resulting interregional power flows. We find that large\npower fluctuations from increasing penetrations of renewable sources can be\nabsorbed at the pan-European level via significantly increased electricity\nexchanges between different countries. We identify where these increased\nexchanges will require additional power transfer capacities. We finally\nintroduce a physically-based economic indicator which allows to predict future\nfinancial conditions in the electricity market. We anticipate new economic\nopportunities for dam hydroelectricity and pumped-storage plants.\n"
    },
    {
        "paper_id": 1706.01748,
        "authors": "Victor Olkhov",
        "title": "Econophysics of Macro-Finance: Local Multi-fluid Models and Surface-like\n  Waves of Financial Variables",
        "comments": "21 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper models macro financial variables alike to financial fluids with\nlocal interactions and describes surface-like waves of Investment and Profits.\nWe regard macro-finance as ensemble of economic agents and use their risk\nratings as coordinates on economic space. Aggregations of agent's financial\nvariables with risk coordinates x on economic space define macro financial\nvariables as function of x. We describe evolution and interactions between\nmacro financial variables alike to financial fluids by hydrodynamic-like\nequations. Minimum and maximum risk grades define most secure and most risky\nagents respectively. That determines borders of macro-finance domain that is\nfilled by economic agents. Perturbations of agent's risk coordinates near risk\nborders of macro domain cause disturbances of macro financial variables like\nInvestment and Profits. Such disturbances can generate waves that propagate\nalong risk borders. These waves may exponentially amplify perturbations inside\nof macro domain and impact financial sustainability. We study simple model\nInvestment and Profits and describe linear approximation of steady state\ndistributions of Investment and Profits on macro-finance domain that fulfill\ndreams of Investors: \"more risks-more Profits\". We describe Investment and\nProfits waves on risk border of economic space alike to surface waves in\nfluids. We present simple examples that specify waves as possible origin of\ntime fluctuations of macro financial variables. Description of possible steady\nstate distributions of macro financial variables and financial risk waves on\neconomic space could help for better policy-making and managing sustainable\nmacro-finance.\n"
    },
    {
        "paper_id": 1706.01813,
        "authors": "Max Reppen and Jean-Charles Rochet and H. Mete Soner",
        "title": "Optimal dividend policies with random profitability",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We study an optimal dividend problem under a bankruptcy constraint. Firms\nface a trade-off between potential bankruptcy and extraction of profits. In\ncontrast to previous works, general cash flow drifts, including\nOrnstein--Uhlenbeck and CIR processes, are considered. We provide rigorous\nproofs of continuity of the value function, whence dynamic programming, as well\nas comparison between the sub- and supersolutions of the\nHamilton--Jacobi--Bellman equation, and we provide an efficient and convergent\nnumerical scheme for finding the solution. The value function is given by a\nnonlinear PDE with a gradient constraint from below in one dimension. We find\nthat the optimal strategy is both a barrier and a band strategy and that it\nincludes voluntary liquidation in parts of the state space. Finally, we present\nand numerically study extensions of the model, including equity issuance and\ncredit lines.\n"
    },
    {
        "paper_id": 1706.01833,
        "authors": "Yaxiong Zeng, Diego Klabjan",
        "title": "Online Adaptive Machine Learning Based Algorithm for Implied Volatility\n  Surface Modeling",
        "comments": "34 Pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this work, we design a machine learning based method, online adaptive\nprimal support vector regression (SVR), to model the implied volatility surface\n(IVS). The algorithm proposed is the first derivation and implementation of an\nonline primal kernel SVR. It features enhancements that allow efficient online\nadaptive learning by embedding the idea of local fitness and budget maintenance\nto dynamically update support vectors upon pattern drifts. For algorithm\nacceleration, we implement its most computationally intensive parts in a Field\nProgrammable Gate Arrays hardware, where a 132x speedup over CPU is achieved\nduring online prediction. Using intraday tick data from the E-mini S&P 500\noptions market, we show that the Gaussian kernel outperforms the linear kernel\nin regulating the size of support vectors, and that our empirical IVS algorithm\nbeats two competing online methods with regards to model complexity and\nregression errors (the mean absolute percentage error of our algorithm is up to\n13%). Best results are obtained at the center of the IVS grid due to its larger\nnumber of adjacent support vectors than the edges of the grid. Sensitivity\nanalysis is also presented to demonstrate how hyper parameters affect the error\nrates and model complexity.\n"
    },
    {
        "paper_id": 1706.01934,
        "authors": "Cl\\'emence Alasseur, Ivar Ekeland, Romuald Elie, Nicol\\'as Hern\\'andez\n  Santib\\'a\\~nez and Dylan Possama\\\"i",
        "title": "An adverse selection approach to power pricing",
        "comments": "39 pages, 9 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We study the optimal design of electricity contracts among a population of\nconsumers with different needs. This question is tackled within the framework\nof Principal-Agent problems in presence of adverse selection. The particular\nfeatures of electricity induce an unusual structure on the production cost,\nwith no decreasing return to scale. We are nevertheless able to provide an\nexplicit solution for the problem at hand. The optimal contracts are either\nlinear or polynomial with respect to the consumption. Whenever the outside\noptions offered by competitors are not uniform among the different type of\nconsumers, we exhibit situations where the electricity provider should contract\nwith consumers with either low or high appetite for electricity.\n"
    },
    {
        "paper_id": 1706.0209,
        "authors": "Martin Baumers, Luca Beltrametti, Angelo Gasparre, and Richard Hague",
        "title": "Informing Additive Manufacturing technology adoption: total cost and the\n  impact of capacity utilisation",
        "comments": "pre-submission manuscript for green open access",
        "journal-ref": null,
        "doi": "10.1080/00207543.2017.1334978",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Informing Additive Manufacturing (AM) technology adoption decisions, this\npaper investigates the relationship between build volume capacity utilisation\nand efficient technology operation in an inter-process comparison of the costs\nof manufacturing a complex component used in the packaging industry.\nConfronting the reported costs of a conventional machining and welding pathway\nwith an estimator of the costs incurred through an AM route utilising Direct\nMetal Laser Sintering (DMLS), we weave together four aspects: optimised\ncapacity utilisation, ancillary process steps, the effect of build failure, and\ndesign adaptation. Recognising that AM users can fill unused machine capacity\nwith other, potentially unrelated, geometries, we posit a characteristic of\n'fungible' build capacity. This aspect is integrated in the cost estimation\nframework through computational build volume packing, drawing on a basket of\nsample geometries. We show that the unit cost in mixed builds at full capacity\nis lower than in builds limited to a single type of geometry; in our study this\nresults in a mean unit cost overstatement of 157%. The estimated manufacturing\ncosts savings from AM adoption range from 36% to 46%. Additionally, we indicate\nthat operating cost savings resulting from design adaptation are likely to far\noutweigh the manufacturing cost advantage.\n"
    },
    {
        "paper_id": 1706.02168,
        "authors": "Diederik Aerts, Suzette Geriente, Catarina Moreira, Sandro Sozzo",
        "title": "Testing Ambiguity and Machina Preferences Within a Quantum-theoretic\n  Framework for Decision-making",
        "comments": "19 pages, 1 figure, standard LaTex. arXiv admin note: substantial\n  text overlap with arXiv:1612.08583, arXiv:1510.09058",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The Machina thought experiments pose to major non-expected utility models\nchallenges that are similar to those posed by the Ellsberg thought experiments\nto subjective expected utility theory (SEUT). We test human choices in the\n`Ellsberg three-color example', confirming typical ambiguity aversion patterns,\nand the `Machina 50/51 and reflection examples', partially confirming the\npreferences hypothesized by Machina. Then, we show that a quantum-theoretic\nframework for decision-making under uncertainty recently elaborated by some of\nus allows faithful modeling of all data on the Ellsberg and Machina paradox\nsituations. In the quantum-theoretic framework subjective probabilities are\nrepresented by quantum probabilities, while quantum state transformations\nenable representations of ambiguity aversion and subjective attitudes toward\nit.\n"
    },
    {
        "paper_id": 1706.02227,
        "authors": "Tomasz R. Bielecki and Tao Chen and Igor Cialenco and Areski Cousin\n  and Monique Jeanblanc",
        "title": "Adaptive Robust Control Under Model Uncertainty",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper we propose a new methodology for solving an uncertain\nstochastic Markovian control problem in discrete time. We call the proposed\nmethodology the adaptive robust control. We demonstrate that the uncertain\ncontrol problem under consideration can be solved in terms of associated\nadaptive robust Bellman equation. The success of our approach is to the great\nextend owed to the recursive methodology for construction of relevant\nconfidence regions. We illustrate our methodology by considering an optimal\nportfolio allocation problem, and we compare results obtained using the\nadaptive robust control method with some other existing methods.\n"
    },
    {
        "paper_id": 1706.02408,
        "authors": "Louis-Pierre Arguin, Nien-Lin Liu, and Tai-Ho Wang",
        "title": "Most-likely-path in Asian option pricing under local volatility models",
        "comments": "31 pages, 2 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This article addresses the problem of approximating the price of options on\ndiscrete and continuous arithmetic average of the underlying, i.e. discretely\nand continuously monitored Asian options, in local volatility models. A\npath-integral-type expression for option prices is obtained using a Brownian\nbridge representation for the transition density between consecutive sampling\ntimes and a Laplace asymptotic formula. In the limit where the sampling time\nwindow approaches zero, the option price is found to be approximated by a\nconstrained variational problem on paths in time-price space. We refer to the\noptimizing path as the most-likely path (MLP). Approximation for the implied\nnormal volatility follows accordingly. The small-time asymptotics and the\nexistence of the MLP are also recovered rigorously using large deviation\ntheory.\n"
    },
    {
        "paper_id": 1706.02795,
        "authors": "Thai T. Pham and Yuanyuan Shen",
        "title": "A Deep Causal Inference Approach to Measuring the Effects of Forming\n  Group Loans in Online Non-profit Microfinance Platform",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Kiva is an online non-profit crowdsouring microfinance platform that raises\nfunds for the poor in the third world. The borrowers on Kiva are small business\nowners and individuals in urgent need of money. To raise funds as fast as\npossible, they have the option to form groups and post loan requests in the\nname of their groups. While it is generally believed that group loans pose less\nrisk for investors than individual loans do, we study whether this is the case\nin a philanthropic online marketplace. In particular, we measure the effect of\ngroup loans on funding time while controlling for the loan sizes and other\nfactors. Because loan descriptions (in the form of texts) play an important\nrole in lenders' decision process on Kiva, we make use of this information\nthrough deep learning in natural language processing. In this aspect, this is\nthe first paper that uses one of the most advanced deep learning techniques to\ndeal with unstructured data in a way that can take advantage of its superior\nprediction power to answer causal questions. We find that on average, forming\ngroup loans speeds up the funding time by about 3.3 days.\n"
    },
    {
        "paper_id": 1706.02936,
        "authors": "Thibaut Mastrolia (CMAP), Zhenjie Ren (CEREMADE)",
        "title": "Principal-Agent Problem with Common Agency without Communication",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper, we consider a problem of contract theory in which several\nPrincipals hire a common Agent and we study the model in the continuous time\nsetting. We show that optimal contracts should satisfy some equilibrium\nconditions and we reduce the optimisation problem of the Principals to a system\nof coupled Hamilton-Jacobi-Bellman (HJB) equations. We provide conditions\nensuring that for risk-neutral Principals, the system of coupled HJB equations\nadmits a solution. Further, we apply our study in a more specific\nlinear-quadratic model where two interacting Principals hire one common Agent.\nIn this continuous time model, we extend the result of Bernheim and Whinston\n(1986) in which the authors compare the optimal effort of the Agent in a\nnon-cooperative Principals model and that in the aggregate model, by showing\nthat these two optimisations coincide only in the first best case. We also\nstudy the sensibility of the optimal effort and the optimal remunerations with\nrespect to appetence parameters and the correlation between the projects.\n"
    },
    {
        "paper_id": 1706.02985,
        "authors": "Haizhen Wang, Ratthachat Chatpatanasiri, Pairote Sattayatham",
        "title": "Stock Trading Using PE ratio: A Dynamic Bayesian Network Modeling on\n  Behavioral Finance and Fundamental Investment",
        "comments": "34 pages, 7 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  On a daily investment decision in a security market, the price earnings (PE)\nratio is one of the most widely applied methods being used as a firm valuation\ntool by investment experts. Unfortunately, recent academic developments in\nfinancial econometrics and machine learning rarely look at this tool. In\npractice, fundamental PE ratios are often estimated only by subjective expert\nopinions. The purpose of this research is to formalize a process of fundamental\nPE estimation by employing advanced dynamic Bayesian network (DBN) methodology.\nThe estimated PE ratio from our model can be used either as a information\nsupport for an expert to make investment decisions, or as an automatic trading\nsystem illustrated in experiments. Forward-backward inference and EM parameter\nestimation algorithms are derived with respect to the proposed DBN structure.\nUnlike existing works in literatures, the economic interpretation of our DBN\nmodel is well-justified by behavioral finance evidences of volatility. A simple\nbut practical trading strategy is invented based on the result of Bayesian\ninference. Extensive experiments show that our trading strategy equipped with\nthe inferenced PE ratios consistently outperforms standard investment\nbenchmarks.\n"
    },
    {
        "paper_id": 1706.03139,
        "authors": "Jean-Pierre Fouque, Ruimeng Hu",
        "title": "Optimal Portfolio under Fast Mean-reverting Fractional Stochastic\n  Environment",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Empirical studies indicate the existence of long range dependence in the\nvolatility of the underlying asset. This feature can be captured by modeling\nits return and volatility using functions of a stationary fractional\nOrnstein--Uhlenbeck (fOU) process with Hurst index $H \\in (\\frac{1}{2}, 1)$. In\nthis paper, we analyze the nonlinear optimal portfolio allocation problem under\nthis model and in the regime where the fOU process is fast mean-reverting. We\nfirst consider the case of power utility, and rigorously give first order\napproximations of the value and the optimal strategy by a martingale distortion\ntransformation. We also establish the asymptotic optimality in all admissible\ncontrols of a zeroth order trading strategy. Then, we extend the discussions to\ngeneral utility functions using the epsilon-martingale decomposition technique,\nand we obtain similar asymptotic optimality results within a specific family of\nadmissible strategies.\n"
    },
    {
        "paper_id": 1706.03246,
        "authors": "Vasilya Usmanova, Yury V. Lysogorskiy and Sumiyoshi Abe",
        "title": "Aftershocks following crash of currency exchange rate: The case of\n  RUB/USD in 2014",
        "comments": "17 pages, 6 figures. The title changed. Published version",
        "journal-ref": "EPL 121, 48001 (2018)",
        "doi": "10.1209/0295-5075/121/48001",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The dynamical behavior of the currency exchange rate after its large-scale\ncatastrophe is discussed through a case study of the rate of Russian rubles to\nUS dollars after its crash in 2014. It is shown that, similarly to the case of\nthe stock market crash, the relaxation is characterized by a power law, which\nis in analogy with the Omori-Utsu law for earthquake aftershocks. The\nwaiting-time distribution is found to also obey a power law. Furthermore, the\nevent-event correlation is discussed, and the aging phenomenon and scaling\nproperty are observed. Comments are made on (non-)Markovianity of the\naftershock process and on a possible relevance of glassy dynamics to the market\nsystem after the crash.\n"
    },
    {
        "paper_id": 1706.03411,
        "authors": "Massil Achab, Emmanuel Bacry, Jean-Fran\\c{c}ois Muzy, Marcello\n  Rambaldi",
        "title": "Analysis of order book flows using a nonparametric estimation of the\n  branching ratio matrix",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We introduce a new non parametric method that allows for a direct, fast and\nefficient estimation of the matrix of kernel norms of a multivariate Hawkes\nprocess, also called branching ratio matrix. We demonstrate the capabilities of\nthis method by applying it to high-frequency order book data from the EUREX\nexchange. We show that it is able to uncover (or recover) various relationships\nbetween all the first level order book events associated with some asset when\nmapped to a 12-dimensional process. We then scale up the model so as to account\nfor events on two assets simultaneously and we discuss the joint high-frequency\ndynamics.\n"
    },
    {
        "paper_id": 1706.03502,
        "authors": "Ashwin K Seshadri",
        "title": "Economics of limiting cumulative CO2 emissions",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Global warming from carbon dioxide (CO2) is known to depend on cumulative CO2\nemissions. We introduce a model of global expenditures on limiting cumulative\nCO2 emissions, taking into account effects of decarbonization and rising global\nincome and making an approximation to the marginal abatement costs (MAC) of\nCO2. Discounted mitigation expenditures are shown to be a convex function of\ncumulative CO2 emissions. We also consider minimum-expenditure solutions for\nmeeting cumulative emissions goals, using a regularized variational method\nyielding an initial value problem in the integrated decarbonization rate. A\nquasi-stationary solution to this problem can be obtained for a special case,\nyielding decarbonization rate that is proportional to annual CO2 emissions.\nMinimum-expenditure trajectories in scenarios where CO2 emissions decrease must\nbegin with rapid decarbonization at rate decreasing with time. Due to the shape\nof global MAC the fraction of global income spent on CO2 mitigation (\"burden\")\ngenerally increases with time, as cheaper avenues for mitigation are exhausted.\nTherefore failure to rapidly decarbonize early on reduces expenditures by a\nsmall fraction (on the order of 0.01 %) of income in the present, but leads to\nmuch higher burden to future generations (on the order of 1 % of income).\n"
    },
    {
        "paper_id": 1706.03567,
        "authors": "S\\\"uhan Altay, Katia Colaneri and Zehra Eksi",
        "title": "Portfolio optimization for a large investor controlling market sentiment\n  under partial information",
        "comments": "38 pages, 5 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We consider an investor faced with the utility maximization problem in which\nthe risky asset price process has pure-jump dynamics affected by an\nunobservable continuous-time finite-state Markov chain, the intensity of which\ncan also be controlled by actions of the investor. Using the classical\nfiltering theory, we reduce this problem with partial information to one with\nfull information and solve it for logarithmic and power utility functions. In\nparticular, we apply control theory for piecewise deterministic Markov\nprocesses (PDMP) to our problem and derive the optimality equation for the\nvalue function and characterize the value function as the unique viscosity\nsolution of the associated dynamic programming equation. Finally, we provide a\ntoy example, where the unobservable state process is driven by a two-state\nMarkov chain, and discuss how investor's ability to control the intensity of\nthe state process affects the optimal portfolio strategies as well as the\noptimal wealth under both partial and full information cases.\n"
    },
    {
        "paper_id": 1706.03724,
        "authors": "Neofytos Rodosthenous and Hongzhong Zhang",
        "title": "Beating the Omega Clock: An Optimal Stopping Problem with Random\n  Time-horizon under Spectrally Negative L\\'evy Models",
        "comments": "35 pages, 1 figure. The Annals of Applied Probability, forthcoming",
        "journal-ref": null,
        "doi": "10.1214/17-AAP1322",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We study the optimal stopping of an American call option in a random\ntime-horizon under exponential spectrally negative L\\'evy models. The random\ntime-horizon is modeled as the so-called Omega default clock in insurance,\nwhich is the first time when the occupation time of the underlying L\\'evy\nprocess below a level $y$, exceeds an independent exponential random variable\nwith mean $1/q>0$. We show that the shape of the value function varies\nqualitatively with different values of $q$ and $y$. In particular, we show that\nfor certain values of $q$ and $y$, some quantitatively different but\ntraditional up-crossing strategies are still optimal, while for other values we\nmay have two disconnected continuation regions, resulting in the optimality of\ntwo-sided exit strategies. By deriving the joint distribution of the\ndiscounting factor and the underlying process under a random discount rate, we\ngive a complete characterization of all optimal exercising thresholds. Finally,\nwe present an example with a compound Poisson process plus a drifted Brownian\nmotion.\n"
    },
    {
        "paper_id": 1706.04163,
        "authors": "Felix Patzelt, Jean-Philippe Bouchaud",
        "title": "Universal scaling and nonlinearity of aggregate price impact in\n  financial markets",
        "comments": "This version: added affiliations and reference to companion paper.\n  Fixed minor typing errors",
        "journal-ref": "Phys. Rev. E 97, 012304 (2018)",
        "doi": "10.1103/PhysRevE.97.012304",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  How and why stock prices move is a centuries-old question still not answered\nconclusively. More recently, attention shifted to higher frequencies, where\ntrades are processed piecewise across different timescales. Here we reveal that\nprice impact has a universal non-linear shape for trades aggregated on any\nintra-day scale. Its shape varies little across instruments, but drastically\ndifferent master curves are obtained for order-volume and -sign impact. The\nscaling is largely determined by the relevant Hurst exponents. We further show\nthat extreme order flow imbalance is not associated with large returns. To the\ncontrary, it is observed when the price is \"pinned\" to a particular level.\nPrices move only when there is sufficient balance in the local order flow. In\nfact, the probability that a trade changes the mid-price falls to zero with\nincreasing (absolute) order-sign bias along an arc-shaped curve for all\nintra-day scales. Our findings challenge the widespread assumption of linear\naggregate impact. They imply that market dynamics on all intra-day timescales\nare shaped by correlations and bilateral adaptation in the flows of liquidity\nprovision and taking.\n"
    },
    {
        "paper_id": 1706.0421,
        "authors": "Zura Kakushadze and Willie Yu",
        "title": "Open Source Fundamental Industry Classification",
        "comments": "68 pages; leftover Word \"track-changes\" highlights removed, no\n  changes in the text",
        "journal-ref": "Data 2(2) (2017) 20",
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We provide complete source code for building a fundamental industry\nclassification based on publically available and freely downloadable data. We\ncompare various fundamental industry classifications by running a horserace of\nshort-horizon trading signals (alphas) utilizing open source heterotic risk\nmodels (https://ssrn.com/abstract=2600798) built using such industry\nclassifications. Our source code includes various stand-alone and portable\nmodules, e.g., for downloading/parsing web data, etc.\n"
    },
    {
        "paper_id": 1706.04229,
        "authors": "David Scott Hunter, Ajay Saini, Tauhid Zaman",
        "title": "Picking Winners: A Data Driven Approach to Evaluating the Quality of\n  Startup Companies",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We consider the problem of evaluating the quality of startup companies. This\ncan be quite challenging due to the rarity of successful startup companies and\nthe complexity of factors which impact such success. In this work we collect\ndata on tens of thousands of startup companies, their performance, the\nbackgrounds of their founders, and their investors. We develop a novel model\nfor the success of a startup company based on the first passage time of a\nBrownian motion. The drift and diffusion of the Brownian motion associated with\na startup company are a function of features based its sector, founders, and\ninitial investors. All features are calculated using our massive dataset. Using\na Bayesian approach, we are able to obtain quantitative insights about the\nfeatures of successful startup companies from our model.\n  To test the performance of our model, we use it to build a portfolio of\ncompanies where the goal is to maximize the probability of having at least one\ncompany achieve an exit (IPO or acquisition), which we refer to as winning.\nThis $\\textit{picking winners}$ framework is very general and can be used to\nmodel many problems with low probability, high reward outcomes, such as\npharmaceutical companies choosing drugs to develop or studios selecting movies\nto produce. We frame the construction of a picking winners portfolio as a\ncombinatorial optimization problem and show that a greedy solution has strong\nperformance guarantees. We apply the picking winners framework to the problem\nof choosing a portfolio of startup companies. Using our model for the exit\nprobabilities, we are able to construct out of sample portfolios which achieve\nexit rates as high as 60%, which is nearly double that of top venture capital\nfirms.\n"
    },
    {
        "paper_id": 1706.04518,
        "authors": "Ivan D. Breslavsky",
        "title": "Effect of Intellectual Property Policy on the Speed of Technological\n  Advancement",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper, the agent-based modeling is employed to model the effect of\nintellectual property policy at the speed of technological advancement. Every\nagent has inborn preferences towards investing their capital into independent\ntechnological development, innovation appropriation, and production. The\nrelative cost of appropriation compared to independent development is chosen as\na measure of strictness of intellectual property protection. We vary this\nparameter and look at the performance of agents with different preferences and\noverall technological progress. In general, it is found that in the specific\nsetting considered with stronger intellectual property protection leads to\nfaster progress.\n"
    },
    {
        "paper_id": 1706.04566,
        "authors": "Robert Azencott and Peng Ren and Ilya Timofeyev",
        "title": "Realized volatility and parametric estimation of Heston SDEs",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We present a detailed analysis of \\emph{observable} moments based parameter\nestimators for the Heston SDEs jointly driving the rate of returns $R_t$ and\nthe squared volatilities $V_t$. Since volatilities are not directly observable,\nour parameter estimators are constructed from empirical moments of realized\nvolatilities $Y_t$, which are of course observable. Realized volatilities are\ncomputed over sliding windows of size $\\varepsilon$, partitioned into\n$J(\\varepsilon)$ intervals. We establish criteria for the joint selection of\n$J(\\varepsilon)$ and of the sub-sampling frequency of return rates data.\n  We obtain explicit bounds for the $L^q$ speed of convergence of realized\nvolatilities to true volatilities as $\\varepsilon \\to 0$. In turn, these bounds\nprovide also $L^q$ speeds of convergence of our observable estimators for the\nparameters of the Heston volatility SDE.\n  Our theoretical analysis is supplemented by extensive numerical simulations\nof joint Heston SDEs to investigate the actual performances of our moments\nbased parameter estimators. Our results provide practical guidelines for\nadequately fitting Heston SDEs parameters to observed stock prices series.\n"
    },
    {
        "paper_id": 1706.04844,
        "authors": "Alexander Schied and Elias Strehle",
        "title": "On the minimizers of energy forms with completely monotone kernel",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Motivated by the problem of optimal portfolio liquidation under transient\nprice impact, we study the minimization of energy functionals with completely\nmonotone displacement kernel under an integral constraint. The corresponding\nminimizers can be characterized by Fredholm integral equations of the second\ntype with constant free term. Our main result states that minimizers are\nanalytic and have a power series development in terms of even powers of the\ndistance to the midpoint of the domain of definition and with nonnegative\ncoefficients. We show moreover that our minimization problem is equivalent to\nthe minimization of the energy functional under a nonnegativity constraint.\n"
    },
    {
        "paper_id": 1706.05291,
        "authors": "Antoine Jacquier, Mikko S. Pakkanen, Henry Stone",
        "title": "Pathwise large deviations for the Rough Bergomi model",
        "comments": "12 Pages",
        "journal-ref": "J. Appl. Probab. 55 (2018) 1078-1092",
        "doi": "10.1017/jpr.2018.72",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We study the small-time behaviour of the rough Bergomi model, introduced by\nBayer, Friz and Gatheral (2016), and prove a large deviations principle for a\nrescaled version of the normalised log stock price process, which then allows\nus to characterise the small-time behaviour of the implied volatility.\n"
    },
    {
        "paper_id": 1706.05543,
        "authors": "Jan Korbel, Xiongfei Jiang and Bo Zheng",
        "title": "Transfer entropy between communities in complex networks",
        "comments": null,
        "journal-ref": "Entropy 2019, 21(11), 1124",
        "doi": "10.3390/e21111124",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  With the help of transfer entropy, we analyze information flows between\ncommunities of complex networks. We show that the transfer entropy provides a\ncoherent description of interactions between communities, including non-linear\ninteractions. To put some flesh on the bare bones, we analyze transfer\nentropies between communities of five largest financial markets, represented as\nnetworks of interacting stocks. Additionally, we discuss information transfer\nof rare events, which is analyzed by R\\'enyi transfer entropy.\n"
    },
    {
        "paper_id": 1706.05703,
        "authors": "Zahra Sokoot, Navideh Modarresi, Farzaneh Niknejad",
        "title": "Modeling credit default swap premiums with stochastic recovery rate",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  There are many studies on development of models for analyzing some\nderivatives such as credit default swaps .\n"
    },
    {
        "paper_id": 1706.05735,
        "authors": "Matthew Andrews and Milan Bradonjic and Iraj Saniee",
        "title": "Quantifying the Benefits of Infrastructure Sharing",
        "comments": "Shorter version accepted to NetEcon 2017",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We analyze the benefits of network sharing between telecommunications\noperators. Sharing is seen as one way to speed the roll out of expensive\ntechnologies such as 5G since it allows the service providers to divide the\ncost of providing ubiquitous coverage. Our theoretical analysis focuses on\nscenarios with two service providers and compares the system dynamics when they\nare competing with the dynamics when they are cooperating. We show that sharing\ncan be beneficial to a service provider even when it has the power to drive the\nother service provider out of the market, a byproduct of a non-convex cost\nfunction. A key element of this study is an analysis of the competitive\nequilibria for both cooperative and non-cooperative 2-person games in the\npresence of (non-convex) cost functions that involve a fixed cost component.\n"
    },
    {
        "paper_id": 1706.05812,
        "authors": "Thomas Forss and Peter Sarlin",
        "title": "News-sentiment networks as a risk indicator",
        "comments": "28 pages, 4 figures, 2 tables, 4 pages appendix",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  To understand the relationship between news sentiment and company stock price\nmovements, and to better understand connectivity among companies, we define an\nalgorithm for measuring sentiment-based network risk. The algorithm ranks\ncompanies in networks of co-occurrences, and measures sentiment-based risk, by\ncalculating both individual risks and aggregated network risks. We extract\nrelative sentiment for companies to get a measure of individual company risk,\nand input it into our risk model together with co-occurrences of companies\nextracted from news on a quarterly basis. We can show that the highest\nquarterly risk value outputted by our risk model, is correlated to a higher\nchance of stock price decline, up to 70 days after a risk measurement. Our\nresults show that the highest difference in the probability of stock price\ndecline, compared to the benchmark containing all risk values for the same\nperiod, is during the interval from 21 to 30 days after a quarterly\nmeasurement. The highest average probability of company stock price decline, is\nfound at a delay of 28 days, after a company has reached its maximum risk\nvalue. The highest probability differences for a daily decline were calculated\nto be 13 percentage points.\n"
    },
    {
        "paper_id": 1706.05877,
        "authors": "Tyler Abbot",
        "title": "General Equilibrium Under Convex Portfolio Constraints and Heterogeneous\n  Risk Preferences",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper characterizes the equilibrium in a continuous time financial\nmarket populated by heterogeneous agents who differ in their rate of relative\nrisk aversion and face convex portfolio constraints. The model is studied in an\napplication to margin constraints and found to match real world observations\nabout financial variables and leverage cycles. It is shown how margin\nconstraints increase the market price of risk and decrease the interest rate by\nforcing more risk averse agents to hold more risky assets, producing a higher\nequity risk premium. In addition, heterogeneity and margin constraints are\nshown to produce both pro- and counter-cyclical leverage cycles. Beyond two\ntypes, it is shown how constraints can cascade and how leverage can exhibit\nhighly non-linear dynamics. Finally, empirical results are given, documenting a\nnovel stylized fact which is predicted by the model, namely that the leverage\ncycle is both pro- and counter-cyclical.\n"
    },
    {
        "paper_id": 1706.05911,
        "authors": "Mariam Barry, Giorgio Triulzi, Christopher L. Magee",
        "title": "Food Productivity Trends from Hybrid Corn: Statistical Analysis of\n  Patents and Field-test data",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this research we study productivity trends of hybrid corn - an important\nsubdomain of food production. We estimate the yearly rate of yield improvement\nof hybrid corn (measured as bushel per acre) by using both information on\nyields contained in US patent documents for patented hybrid corn varieties and\non field-test data of several hybrid corn varieties performed at US State\nlevel. We have used a generalization of Moore's law to fit productivity trends\nand obtain the performance improvement rate by analyzing time series of hybrid\ncorn performance for a period covering the last thirty years. The linear\nregressions results obtained from different data sources indicate that the\nestimated improvement rates per year are between 1.2 and 2.4 percent. In\nparticular, using yields reported in a sample of patents filed between 1985 and\n2010, we estimated an improvement rate of 0.015 (R2 = 0.74, Pvalue = 1.37 x\n10^-8). Moreover, we apply two predicting models developed by Benson and Magee\n(2015) and Triulzi and Magee (2016) that only use patent metadata to estimate\nthe rate of improvement. We compare these predicted values to the rate\nestimated using US States field-test data. We find that, due to a turning point\nin patenting practices which begun in 2008, only the predicted rate (rate =\n0.015) using patents filed before 2008 is consistent with the empirical rate.\nFinally, we also investigate at the micro level - on the basis of 70 patents\n(granted between 1986 and 2015) - whether the number of citations received by a\npatent is correlated with performance achieved by the patented variety. We find\nthat the relative performance (yield ratio) of the patented seed is positively\ncorrelated with the total number of citations received by the patent (until\nDecember 2015) but not the citations received within 3 years after the granted\nyear, with the patent application year used as control variable.\n"
    },
    {
        "paper_id": 1706.05912,
        "authors": "Emiliano Diaz",
        "title": "An\\'alisis de cointegraci\\'on con una aplicaci\\'on al mercado de deuda\n  en Estados Unidos, Canad\\'a y M\\'exico",
        "comments": "thesis, in Spanish, Instituto Technol\\'ogico aut\\'onomo de M\\'exico,\n  2016",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Certain theoretical aspects of vector autoregression (VAR) as tools to model\neconomic time series are revised, in particular their capacity to include both\nshort term and long term information. The VAR model, in its error correction\nform, is derived and the permanent-transitory decomposition of factors proposed\nby Gonzalo and Granger (1995) studied. An introductory exposition of estimation\ntheory for reduced rank models, necessary to estimate the error correction\nmodel, is given. Cointegration analysis using the VAR model is carried out for\ngovernment bond interest rates (short, medium and long term) of the United\nStates, Mexico and Canada, with the objective of finding the long-term common\nfactors that drive the system. The error correction model of this system is\nestimated using Johansen's method. Using this estimation the\npermanent-transitory decomposition of the system is calculated. Hypothesis\ntests are carried out on permanent factors to determine which of the nine rates\nstudied drive the system.\n"
    },
    {
        "paper_id": 1706.05935,
        "authors": "Ricardo Cris\\'ostomo",
        "title": "Speed and biases of Fourier-based pricing choices: A numerical analysis",
        "comments": "This is an Accepted Manuscript of an article published by Taylor &\n  Francis in International Journal of Computer Mathematics on 10 May 2017",
        "journal-ref": null,
        "doi": "10.1080/00207160.2017.1322691",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We compare the CPU effort and pricing biases of seven Fourier-based\nimplementations. Our analyses show that truncation and discretization errors\nsignificantly increase as we move away from the Black-Scholes-Merton framework.\nWe rank the speed and accuracy of the competing choices, showing which methods\nrequire smaller truncation ranges and which are the most efficient in terms of\nsampling densities. While all implementations converge well in the Bates\njump-diffusion model, Attari's formula is the only Fourier-based method that\ndoes not blow up for any Variance Gamma parameter values. In terms of speed,\nthe use of strike vector computations significantly improves the computational\nburden, rendering both fast Fourier transforms (FFT) and plain\ndelta-probability decompositions inefficient. We conclude that the multi-strike\nversion of the COS method is notably faster than any other implementation,\nwhereas the strike-optimized Carr Madan's formula is simultaneously faster and\nmore accurate than the FFT, thus questioning its use.\n"
    },
    {
        "paper_id": 1706.06007,
        "authors": "Dan Xu and Christian Beck",
        "title": "Symbolic dynamics techniques for complex systems: Application to share\n  price dynamics",
        "comments": "7 pages, 11 figures. Invited perspective for EPL",
        "journal-ref": null,
        "doi": "10.1209/0295-5075/118/30001",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The symbolic dynamics technique is well-known for low-dimensional dynamical\nsystems and chaotic maps, and lies at the roots of the thermodynamic formalism\nof dynamical systems. Here we show that this technique can also be successfully\napplied to time series generated by complex systems of much higher\ndimensionality. Our main example is the investigation of share price returns in\na coarse-grained way. A nontrivial spectrum of Renyi entropies is found. We\nstudy how the spectrum depends on the time scale of returns, the sector of\nstocks considered, as well as the number of symbols used for the symbolic\ndescription. Overall our analysis confirms that in the symbol space transition\nprobabilities of observed share price returns depend on the entire history of\nprevious symbols, thus emphasizing the need for a modelling based on\nnon-Markovian stochastic processes. Our method allows for quantitative\ncomparisons of entirely different complex systems, for example the statistics\nof symbol sequences generated by share price returns using 4 symbols can be\ncompared with that of genomic sequences.\n"
    },
    {
        "paper_id": 1706.06285,
        "authors": "Dianfa Chen, Jun Deng, Jianfen Feng, Bin Zou",
        "title": "An Explicit Default Contagion Model and Its Application to Credit\n  Derivatives Pricing",
        "comments": "46 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We propose a novel credit default model that takes into account the impact of\nmacroeconomic information and contagion effect on the defaults of obligors. We\nuse a set-valued Markov chain to model the default process, which is the set of\nall defaulted obligors in the group. We obtain analytic characterizations for\nthe default process, and use them to derive pricing formulas in explicit forms\nfor synthetic collateralized debt obligations (CDOs). Furthermore, we use\nmarket data to calibrate the model and conduct numerical studies on the tranche\nspreads of CDOs. We find evidence to support that systematic default risk\ncoupled with default contagion could have the leading component of the total\ndefault risk.\n"
    },
    {
        "paper_id": 1706.06302,
        "authors": "Sander van der Hoog",
        "title": "Deep Learning in (and of) Agent-Based Models: A Prospectus",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
        "abstract": "  A very timely issue for economic agent-based models (ABMs) is their empirical\nestimation. This paper describes a line of research that could resolve the\nissue by using machine learning techniques, using multi-layer artificial neural\nnetworks (ANNs), or so called Deep Nets. The seminal contribution by Hinton et\nal. (2006) introduced a fast and efficient training algorithm called Deep\nLearning, and there have been major breakthroughs in machine learning ever\nsince. Economics has not yet benefited from these developments, and therefore\nwe believe that now is the right time to apply Deep Learning and multi-layered\nneural networks to agent-based models in economics.\n"
    },
    {
        "paper_id": 1706.06355,
        "authors": "Mateusz Wilinski, Yuichi Ikeda and Hideaki Aoyama",
        "title": "Complex Correlation Approach for High Frequency Financial Data",
        "comments": null,
        "journal-ref": null,
        "doi": "10.1088/1742-5468/aaa8eb",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We propose a novel approach that allows to calculate Hilbert transform based\ncomplex correlation for unevenly spaced data. This method is especially\nsuitable for high frequency trading data, which are of a particular interest in\nfinance. Its most important feature is the ability to take into account\nlead-lag relations on different scales, without knowing them in advance. We\nalso present results obtained with this approach while working on Tokyo Stock\nExchange intraday quotations. We show that individual sectors and subsectors\ntend to form important market components which may follow each other with small\nbut significant delays. These components may be recognized by analysing\neigenvectors of complex correlation matrix for Nikkei 225 stocks.\nInterestingly, sectorial components are also found in eigenvectors\ncorresponding to the bulk eigenvalues, traditionally treated as noise.\n"
    },
    {
        "paper_id": 1706.06709,
        "authors": "Tat Lung Chan",
        "title": "Singular Fourier-Pad\\'e Series Expansion of European Option Prices",
        "comments": "37 pages, 14 figures and 14 tables",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  We apply a new numerical method, the singular Fourier-Pad\\'e (SFP) method\ninvented by Driscoll and Fornberg (2001, 2011), to price European-type options\nin L\\'evy and affine processes. The motivation behind this application is to\nreduce the inefficiency of current Fourier techniques when they are used to\napproximate piecewise continuous (non-smooth) probability density functions.\nWhen techniques such as fast Fourier transforms and Fourier series are applied\nto price and hedge options with non-smooth probability density functions, they\ncause the Gibbs phenomenon, accordingly, the techniques converge slowly for\ndensity functions with jumps in value or derivatives. This seriously adversely\naffects the efficiency and accuracy of these techniques. In this paper, we\nderive pricing formulae and their option Greeks using the SFP method to resolve\nthe Gibbs phenomenon and restore the global spectral convergence rate.\nMoreover, we show that our method requires a small number of terms to yield\nfast error convergence, and it is able to accurately price any European-type\noption deep in/out of the money and with very long/short maturities.\nFurthermore, we conduct an error-bound analysis of the SFP method in option\npricing. This new method performs favourably in numerical experiments compared\nwith existing techniques.\n"
    },
    {
        "paper_id": 1706.06832,
        "authors": "Eckhard Platen and Renata Rendek",
        "title": "Market Efficiency and Growth Optimal Portfolio",
        "comments": "32 pages, 5 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The paper predicts an Efficient Market Property for the equity market, where\nstocks, when denominated in units of the growth optimal portfolio (GP), have\nzero instantaneous expected returns. Well-diversified equity portfolios are\nshown to approximate the GP, which explains the well-observed good performance\nof equally weighted portfolios. The proposed hierarchically weighted index\n(HWI) is shown to be an even better proxy of the GP. It sets weights equal\nwithin industrial and geographical groupings of stocks. When using the HWI as\nproxy of the GP the Efficient Market Property cannot be easily rejected and\nappears to be very robust.\n"
    },
    {
        "paper_id": 1706.07021,
        "authors": "Roberto Baviera and Tommaso Santagostino Baldi",
        "title": "Stop-loss and Leverage in optimal Statistical Arbitrage with an\n  application to Energy market",
        "comments": "22 pages, 4 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper we develop a statistical arbitrage trading strategy with two\nkey elements in hi-frequency trading: stop-loss and leverage. We consider, as\nin Bertram (2009), a mean-reverting process for the security price with\nproportional transaction costs; we show how to introduce stop-loss and leverage\nin an optimal trading strategy.\n  We focus on repeated strategies using a self-financing portfolio. For every\ngiven stop-loss level we derive analytically the optimal investment strategy\nconsisting of optimal leverage and market entry/exit levels.\n  First we show that the optimal strategy a' la Bertram depends on the\nprobabilities to reach entry/exit levels, on expected First-Passage-Times and\non expected First-Exit-Times from an interval. Then, when the underlying\nlog-price follows an Ornstein-Uhlenbeck process, we deduce analytical\nexpressions for expected First-Exit-Times and we derive the long-run return of\nthe strategy as an elementary function of the stop-loss.\n  Following industry practice of pairs trading we consider an example of pair\nin the energy futures' market, reporting in detail the analysis for a spread on\nHeating-Oil and Gas-Oil futures in one year sample of half-an-hour market\nprices.\n"
    },
    {
        "paper_id": 1706.07216,
        "authors": "Pavel Ciaian, Miroslava Rajcaniova, d'Artis Kancs",
        "title": "Virtual Relationships: Short- and Long-run Evidence from BitCoin and\n  Altcoin Markets",
        "comments": "36 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This study empirically examines interdependencies between BitCoin and altcoin\nmarkets in the short- and long-run. We apply time-series analytical mechanisms\nto daily data of 17 virtual currencies (BitCoin + 16 alternative virtual\ncurrencies) and two Altcoin price indices for the period 2013-2016. Our\nempirical findings confirm that indeed BitCoin and Altcoin markets are\ninterdependent. The BitCoin-Altcoin price relationship is significantly\nstronger in the short-run than in the long-run. We cannot fully confirm the\nhypothesis that the BitCoin price relationship is stronger with those Altcoins\nthat are more similar in their price formation mechanism to BitCoin. In the\nlong-run, macro-financial indicators determine the altcoin price formation to a\ngreater degree than BitCoin does. The virtual currency supply is exogenous and\ntherefore plays only a limited role in the price formation.\n"
    },
    {
        "paper_id": 1706.07375,
        "authors": "Andrei Cozma and Christoph Reisinger",
        "title": "Strong convergence rates for Euler approximations to a class of\n  stochastic path-dependent volatility models",
        "comments": "34 pages, 5 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We consider a class of stochastic path-dependent volatility models where the\nstochastic volatility, whose square follows the Cox-Ingersoll-Ross model, is\nmultiplied by a (leverage) function of the spot price, its running maximum, and\ntime. We propose a Monte Carlo simulation scheme which combines a log-Euler\nscheme for the spot process with the full truncation Euler scheme or the\nbackward Euler-Maruyama scheme for the squared stochastic volatility component.\nUnder some mild regularity assumptions and a condition on the Feller ratio, we\nestablish the strong convergence with order 1/2 (up to a logarithmic factor) of\nthe approximation process up to a critical time. The model studied in this\npaper contains as special cases Heston-type stochastic-local volatility models,\nthe state-of-the-art in derivative pricing, and a relatively new class of\npath-dependent volatility models. The present paper is the first to prove the\nconvergence of the popular Euler schemes with a positive rate, which is\nmoreover consistent with that for Lipschitz coefficients and hence optimal.\n"
    },
    {
        "paper_id": 1706.07459,
        "authors": "Anatoliy Swishchuk",
        "title": "General Compound Hawkes Processes in Limit Order Books",
        "comments": "32 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper, we study various new Hawkes processes, namely, so-called\ngeneral compound and regime-switching general compound Hawkes processes to\nmodel the price processes in the limit order books. We prove Law of Large\nNumbers (LLN) and Functional Central Limit Theorems (FCLT) for these processes.\nThe latter two FCLTs are applied to limit order books where we use these\nasymptotic methods to study the link between price volatility and order flow in\nour two models by studying the diffusion limits of these price processes. The\nvolatilities of price changes are expressed in terms of parameters describing\nthe arrival rates and price changes.\n"
    },
    {
        "paper_id": 1706.07466,
        "authors": "Maha Bakoben, Tony Bellotti and Niall Adams",
        "title": "Identification of Credit Risk Based on Cluster Analysis of Account\n  Behaviours",
        "comments": "9 pages",
        "journal-ref": null,
        "doi": "10.1080/01605682.2019.1582586",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Assessment of risk levels for existing credit accounts is important to the\nimplementation of bank policies and offering financial products. This paper\nuses cluster analysis of behaviour of credit card accounts to help assess\ncredit risk level. Account behaviour is modelled parametrically and we then\nimplement the behavioural cluster analysis using a recently proposed\ndissimilarity measure of statistical model parameters. The advantage of this\nnew measure is the explicit exploitation of uncertainty associated with\nparameters estimated from statistical models. Interesting clusters of real\ncredit card behaviours data are obtained, in addition to superior prediction\nand forecasting of account default based on the clustering outcomes.\n"
    },
    {
        "paper_id": 1706.07758,
        "authors": "Victor Olkhov",
        "title": "Non-Local Macroeconomic Transactions and Credits-Loans Surface-Like\n  Waves",
        "comments": "20 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper describes surface-like waves of macroeconomic Credits-Loans\ntransactions on economic space. We use agent's risk ratings as their\ncoordinates and describe evolution of macro variables by transactions between\nagents. Aggregations of agent's variables with risk coordinates x on economic\nspace define macro variables as function of x. Aggregations of transactions\nbetween agents at point x and y determine functions of two variables (x,y) on\neconomic space. As example we study Credits transactions provided from agents\nat point x to agents at point y and thus amount of Loans received by agents at\npoint y from agents at point x at moment t during time term dt. We model\nevolution of macro transactions by hydrodynamic-like equations. Agents fill\nmacro domain on economic space that is bounded by minimum risk ratings of most\nsecure and maximum risk ratings of most risky agents. Economic and financial\nshocks can disturb steady borders of macro domain and cause perturbations of\ntransactions. Such disturbances can generate waves that can propagate along\nrisk borders alike to surface waves in fluids. As example, we describe simple\nmodel interactions between two transactions by hydrodynamic like equations in a\nclosed form. We introduce notions of \"macro accelerations\" and their potentials\nthat establish steady state distributions of transactions on economic space.\nFor this model in linear approximation we describe surface-like waves and show\nthat perturbations induced by surface-like waves can exponentially grow up\ninside macro domain and induce macro instabilities in a low risk area.\nDescription of possible steady state distributions of transactions and\nsurface-like waves on economic space might be important for macro modeling and\npolicy-making.\n"
    },
    {
        "paper_id": 1706.07759,
        "authors": "Roberto De Luca, Marco Di Mauro, Angelo Falzarano, Adele Naddeo",
        "title": "The effect of the behavior of an average consumer on the public debt\n  dynamics",
        "comments": "4 pages, 1 figures, submitted for publication to Physica A",
        "journal-ref": null,
        "doi": "10.1016/j.physa.2017.04.099",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  An important issue within the present economic crisis is understanding the\ndynamics of the public debt of a given country, and how the behavior of average\nconsumers and tax payers in that country affects it. Starting from a model of\nthe average consumer behavior introduced earlier by the authors, we propose a\nsimple model to quantitatively address this issue. The model is then studied\nand analytically solved under some reasonable simplifying assumptions. In this\nway we obtain a condition under which the public debt steadily decreases.\n"
    },
    {
        "paper_id": 1706.0776,
        "authors": "Irina Georgescu, Adolfo Crist\\'obal Campoamor, Ana Maria Lucia\n  Casademunt",
        "title": "A Possibilistic and Probabilistic Approach to Precautionary Saving",
        "comments": "Panoeconomicus, 2017",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper proposes two mixed models to study a consumer's optimal saving in\nthe presence of two types of risk.\n"
    },
    {
        "paper_id": 1706.07783,
        "authors": "Yonatan Berman",
        "title": "Intergenerational mobility measures in a bivariate normal model",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We model the joint log-income distribution of parents and children and derive\nanalytic expressions for canonical relative and absolute intergenerational\nmobility measures. We find that both types of mobility measures can be\nexpressed as a function of the other.\n"
    },
    {
        "paper_id": 1706.07821,
        "authors": "Jaydip Sen and Tamal Datta Chaudhuri",
        "title": "An Investigation of the Structural Characteristics of the Indian IT\n  Sector and the Capital Goods Sector: An Application of the R Programming in\n  Time Series Decomposition and Forecasting",
        "comments": "64 pages, 34 figures, 33 tables",
        "journal-ref": "Journal of Insurance and Financial Management, Vol 1, Issue 4\n  (2016), pp. 68 - 132",
        "doi": "10.36227/techrxiv.16640227.v1",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Time series analysis and forecasting of stock market prices has been a very\nactive area of research over the last two decades. Availability of extremely\nfast and parallel architecture of computing and sophisticated algorithms has\nmade it possible to extract, store, process and analyze high volume stock\nmarket time series data very efficiently. In this paper, we have used time\nseries data of the two sectors of the Indian economy: Information Technology\nand Capital Goods for the period January 2009 till April 2016 and have studied\nthe relationships of these two time series with the time series of DJIA index,\nNIFTY index and the US Dollar to Indian Rupee exchange rate. We establish by\ngraphical and statistical tests that while the IT sector of India has a strong\nassociation with DJIA index and the Dollar to Rupee exchange rate, the Indian\nCG sector exhibits a strong association with the NIFTY index. We contend that\nthese observations corroborate our hypotheses that the Indian IT sector is\nstrongly coupled with the world economy whereas the CG sector of India reflects\ninternal economic growth of India. We also present several models of regression\nbetween the time series which exhibit strong association among them. The\neffectiveness of these models have been demonstrated by very low values of\ntheir forecasting errors.\n"
    },
    {
        "paper_id": 1706.08361,
        "authors": "Jaydip Sen and Tamal Datta Chaudhuri",
        "title": "Decomposition of Time Series Data to Check Consistency between Fund\n  Style and Actual Fund Composition of Mutual Funds",
        "comments": "15 pages, 2 figures, 9 tables. The paper was published in the\n  Proceedings of 4th International Conference on Business Analytics and\n  Intelligence (ICBAI 2016), December 19 - 21, 2016, Indian Institute of\n  Science (IISc), Bangalore, INDIA",
        "journal-ref": null,
        "doi": "10.13140/RG.2.2.33048.19206",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We propose a novel approach for analysis of the composition of an equity\nmutual fund based on the time series decomposition of the price movements of\nthe individual stocks of the fund. The proposed scheme can be applied to check\nwhether the style proclaimed for a mutual fund actually matches with the fund\ncomposition. We have applied our proposed framework on eight well known mutual\nfunds of varying styles in the Indian financial market to check the consistency\nbetween their fund style and actual fund composition, and have obtained\nextensive results from our experiments. A detailed analysis of the results has\nshown that while in majority of the cases the actual allocations of funds are\nconsistent with the corresponding fund styles, there have been some notable\ndeviations too.\n"
    },
    {
        "paper_id": 1706.08479,
        "authors": "Kostyantyn Mazur",
        "title": "A Partial Solution to Continuous Blotto",
        "comments": "This paper cites and relies on another paper, arXiv1706.02060 \"Convex\n  Hull of (t, t^2, ..., t^N)\", that is posted on arXiv at the same time as this\n  paper Version 2: added a reference",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper analyzes the structure of mixed-strategy equilibria for Colonel\nBlotto games, where the outcome on each battlefield is a polynomial function of\nthe difference between the two players' allocations. This paper severely\nreduces the set of strategies that needs to be searched to find a Nash\nequilibrium. It finds that there exists a Nash equilibrium where both players'\nmixed strategies are discrete distributions, and it places an upper bound on\nthe number of points in the supports of these discrete distributions.\n"
    },
    {
        "paper_id": 1706.08588,
        "authors": "Anis Matoussi and Dylan Possama\\\"i and Chao Zhou",
        "title": "Corrigendum for \"Second-order reflected backward stochastic differential\n  equations\" and \"Second-order BSDEs with general reflection and game options\n  under uncertainty\"",
        "comments": "16 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The aim of this short note is to fill in a gap in our earlier paper [16] on\n2BSDEs with reflections, and to explain how to correct the subsequent results\nin the second paper [15]. We also provide more insight on the properties of\n2RBSDEs, in the light of the recent contributions [13, 23] in the so--called\n$G-$framework.\n"
    },
    {
        "paper_id": 1706.09038,
        "authors": "Anatoliy Swishchuk",
        "title": "Risk Model Based on General Compound Hawkes Process",
        "comments": "16 pages; this paper will be presented at the 21st International\n  Congress Insurance: Mathematics and Economics-IME 2017, TUW, Vienna",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper, we introduce a new model for the risk process based on general\ncompound Hawkes process (GCHP) for the arrival of claims. We call it risk model\nbased on general compound Hawkes process (RMGCHP). The Law of Large Numbers\n(LLN) and the Functional Central Limit Theorem (FCLT) are proved. We also study\nthe main properties of this new risk model, net profit condition, premium\nprinciple and ruin time (including ultimate ruin time) applying the LLN and\nFCLT for the RMGCHP. We show, as applications of our results, similar results\nfor risk model based on compound Hawkes process (RMCHP) and apply them to the\nclassical risk model based on compound Poisson process (RMCPP).\n"
    },
    {
        "paper_id": 1706.09224,
        "authors": "Takashi Kato",
        "title": "An Optimal Execution Problem with S-shaped Market Impact Functions",
        "comments": "22 pages, 2 figures, forthcoming in \"Communications on Stochastic\n  Analysis\"",
        "journal-ref": "Communications on Stochastic Analysis, Vol.11, No.3, pp.265-285\n  (2017)",
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this study, we extend the optimal execution problem with convex market\nimpact function studied in Kato (2014) to the case where the market impact\nfunction is S-shaped, that is, concave on $[0, \\bar {x}_0]$ and convex on\n$[\\bar {x}_0, \\infty )$ for some $\\bar {x}_0 \\geq 0$. We study the\ncorresponding Hamilton-Jacobi-Bellman equation and show that the optimal\nexecution speed under the S-shaped market impact is equal to zero or larger\nthan $\\bar {x}_0$. Moreover, we provide some examples of the Black-Scholes\nmodel. We show that the optimal strategy for a risk-neutral trader with small\nshares is the time-weighted average price strategy whenever the market impact\nfunction is S-shaped.\n"
    },
    {
        "paper_id": 1706.0924,
        "authors": "Shanshan Wang and Thomas Guhr",
        "title": "Local fluctuations of the signed traded volumes and the dependencies of\n  demands: a copula analysis",
        "comments": "This is the Accepted Manuscript version of an article accepted for\n  publication in Journal of Statistical Mechanics: Theory and Experiment.\n  Neither SISSA Medialab Srl nor IOP Publishing Ltd is responsible for any\n  errors or omissions in this version of the manuscript or any version derived\n  from it. The Version of Record is available online at\n  https://doi.org/10.1088/1742-5468/aab01c",
        "journal-ref": "J. Stat. Mech. (2018) 033407",
        "doi": "10.1088/1742-5468/aab01c",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We investigate how the local fluctuations of the signed traded volumes affect\nthe dependence of demands between stocks. We analyze the empirical dependence\nof demands using copulas and show that they are well described by a bivariate\n$\\mathcal{K}$ copula density function. We find that large local fluctuations\nstrongly increase the positive dependence but lower slightly the negative one\nin the copula density. This interesting feature is due to cross-correlations of\nvolume imbalances between stocks. Also, we explore the asymmetries of tail\ndependencies of the copula density, which are moderate for the negative\ndependencies but strong for the positive ones. For the latter, we reveal that\nlarge local fluctuations of the signed traded volumes trigger stronger\ndependencies of demands than of supplies, probably indicating a bull market\nwith persistent raising of prices.\n"
    },
    {
        "paper_id": 1706.09365,
        "authors": "Jiyoung Kim, Satoshi Nakano, Kazuhiko Nishimura",
        "title": "Bilateral multifactor CES general equilibrium with state-replicating\n  Armington elasticities",
        "comments": null,
        "journal-ref": "Asia-Pacific Journal of Regional Science 2018",
        "doi": "10.1007/s41685-017-0068-7",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We measure elasticity of substitution between foreign and domestic\ncommodities by two-point calibration such that the Armington aggregator can\nreplicate the two temporally distant observations of market shares and prices.\nAlong with the sectoral multifactor CES elasticities which we estimate by\nregression using a set of disaggregated linked input--output observations, we\nintegrate domestic production of two countries, namely, Japan and the Republic\nof Korea, with bilateral trade models and construct a bilateral general\nequilibrium model. Finally, we make an assessment of a tariff elimination\nscheme between the two countries.\n"
    },
    {
        "paper_id": 1706.09659,
        "authors": "Dan Pirjol, Lingjiong Zhu",
        "title": "Asymptotics for the Discrete-Time Average of the Geometric Brownian\n  Motion and Asian Options",
        "comments": "33 pages, 2 figures",
        "journal-ref": "Advances in Applied Probability, Volume 49, Issue 2, 446-480\n  (2017)",
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The time average of geometric Brownian motion plays a crucial role in the\npricing of Asian options in mathematical finance. In this paper we consider the\nasymptotics of the discrete-time average of a geometric Brownian motion sampled\non uniformly spaced times in the limit of a very large number of averaging time\nsteps. We derive almost sure limit, fluctuations, large deviations, and also\nthe asymptotics of the moment generating function of the average. Based on\nthese results, we derive the asymptotics for the price of Asian options with\ndiscrete-time averaging in the Black-Scholes model, with both fixed and\nfloating strike.\n"
    },
    {
        "paper_id": 1706.09755,
        "authors": "Carolyn E. Phelan, Daniele Marazzina, Gianluca Fusai, Guido Germano",
        "title": "Hilbert transform, spectral filters and option pricing",
        "comments": "27 pages, 12 figures, submitted",
        "journal-ref": "Ann Oper Res (2019) 282(1-2) pp273-298",
        "doi": "10.1007/s10479-018-2881-4",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We show how spectral filters can improve the convergence of numerical schemes\nwhich use discrete Hilbert transforms based on a sinc function expansion, and\nthus ultimately on the fast Fourier transform. This is relevant, for example,\nfor the computation of fluctuation identities, which give the distribution of\nthe maximum or the minimum of a random path, or the joint distribution at\nmaturity with the extrema staying below or above barriers. We use as examples\nthe methods by Feng and Linetsky (2008) and Fusai, Germano and Marazzina (2016)\nto price discretely monitored barrier options where the underlying asset price\nis modelled by an exponential L\\'evy process. Both methods show exponential\nconvergence with respect to the number of grid points in most cases, but are\nlimited to polynomial convergence under certain conditions. We relate these\nrates of convergence to the Gibbs phenomenon for Fourier transforms and achieve\nimproved results with spectral filtering.\n"
    },
    {
        "paper_id": 1706.09756,
        "authors": "Michael Kateregga, Sure Mataramvura and David Taylor",
        "title": "Parameter estimation for stable distributions with application to\n  commodity futures log returns",
        "comments": "23 pages, 11 Figures and 3 Tables",
        "journal-ref": null,
        "doi": "10.1080/23322039.2017.1318813",
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  This paper explores the theory behind the rich and robust family of\n{\\alpha}-stable distributions to estimate parameters from financial asset\nlog-returns data. We discuss four-parameter estimation methods including the\nquantiles, logarithmic moments method, maximum likelihood (ML), and the\nempirical characteristics function (ECF) method. The contribution of the paper\nis two-fold: first, we discuss the above parametric approaches and investigate\ntheir performance through error analysis. Moreover, we argue that the ECF\nperforms better than the ML over a wide range of shape parameter values,\n{\\alpha}{\\alpha} including values closest to 0 and 2 and that the ECF has a\nbetter convergence rate than the ML. Secondly, we compare the t location-scale\ndistribution to the general stable distribution and show that the former fails\nto capture skewness which might exist in the data. This is observed through\napplying the ECF to commodity futures log-returns data to obtain the skewness\nparameter.\n"
    },
    {
        "paper_id": 1706.09763,
        "authors": "Robin Nicole and Peter Sollich",
        "title": "Dynamical selection of Nash equilibria using Experience Weighted\n  Attraction Learning: emergence of heterogeneous mixed equilibria",
        "comments": "35 pages, 16 figures",
        "journal-ref": null,
        "doi": "10.1371/journal.pone.0196577",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We study the distribution of strategies in a large game that models how\nagents choose among different double auction markets. We classify the possible\nmean field Nash equilibria, which include potentially segregated states where\nan agent population can split into subpopulations adopting different\nstrategies. As the game is aggregative, the actual equilibrium strategy\ndistributions remain undetermined, however. We therefore compare with the\nresults of Experience-Weighted Attraction (EWA) learning, which at long times\nleads to Nash equilibria in the appropriate limits of large intensity of\nchoice, low noise (long agent memory) and perfect imputation of missing scores\n(fictitious play). The learning dynamics breaks the indeterminacy of the Nash\nequilibria. Non-trivially, depending on how the relevant limits are taken, more\nthan one type of equilibrium can be selected. These include the standard\nhomogeneous mixed and heterogeneous pure states, but also \\emph{heterogeneous\nmixed} states where different agents play different strategies that are not all\npure. The analysis of the EWA learning involves Fokker-Planck modeling combined\nwith large deviation methods. The theoretical results are confirmed by\nmulti-agent simulations.\n"
    },
    {
        "paper_id": 1706.09809,
        "authors": "Andreas M\\\"uhlbacher and Thomas Guhr",
        "title": "Extreme portfolio loss correlations in credit risk",
        "comments": "25 pages, 10 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The stability of the financial system is associated with systemic risk\nfactors such as the concurrent default of numerous small obligors. Hence it is\nof utmost importance to study the mutual dependence of losses for different\ncreditors in the case of large, overlapping credit portfolios. We analytically\ncalculate the multivariate joint loss distribution of several credit portfolios\non a non-stationary market. To take fluctuating asset correlations into account\nwe use an random matrix approach which preserves, as a much appreciated side\neffect, analytical tractability and drastically reduces the number of\nparameters. We show that for two disjoint credit portfolios diversification\ndoes not work in a correlated market. Additionally we find large concurrent\nportfolio losses to be rather likely. We show that significant correlations of\nthe losses emerge not only for large portfolios with thousands of credit\ncontracts but also for small portfolios consisting of a few credit contracts\nonly. Furthermore we include subordination levels, which were established in\ncollateralized debt obligations to protect the more senior tranches from high\nlosses. We analytically corroborate the observation that an extreme loss of the\nsubordinated creditor is likely to also yield a large loss of the senior\ncreditor.\n"
    },
    {
        "paper_id": 1706.10059,
        "authors": "Zhengyao Jiang, Dixing Xu, Jinjun Liang",
        "title": "A Deep Reinforcement Learning Framework for the Financial Portfolio\n  Management Problem",
        "comments": "30 pages, 5 figures, submitting to JMLR",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Financial portfolio management is the process of constant redistribution of a\nfund into different financial products. This paper presents a\nfinancial-model-free Reinforcement Learning framework to provide a deep machine\nlearning solution to the portfolio management problem. The framework consists\nof the Ensemble of Identical Independent Evaluators (EIIE) topology, a\nPortfolio-Vector Memory (PVM), an Online Stochastic Batch Learning (OSBL)\nscheme, and a fully exploiting and explicit reward function. This framework is\nrealized in three instants in this work with a Convolutional Neural Network\n(CNN), a basic Recurrent Neural Network (RNN), and a Long Short-Term Memory\n(LSTM). They are, along with a number of recently reviewed or published\nportfolio-selection strategies, examined in three back-test experiments with a\ntrading period of 30 minutes in a cryptocurrency market. Cryptocurrencies are\nelectronic and decentralized alternatives to government-issued money, with\nBitcoin as the best-known example of a cryptocurrency. All three instances of\nthe framework monopolize the top three positions in all experiments,\noutdistancing other compared trading algorithms. Although with a high\ncommission rate of 0.25% in the backtests, the framework is able to achieve at\nleast 4-fold returns in 50 days.\n"
    },
    {
        "paper_id": 1706.10141,
        "authors": "Everton M. C. Abreu, Newton J. Moura Jr., Abner D. Soares and Marcelo\n  B. Ribeiro",
        "title": "Oscillations in the Tsallis income distribution",
        "comments": "14 pages. Final version to appear in Physica A",
        "journal-ref": "Physica A 533 (2019) 121967",
        "doi": "10.1016/j.physa.2019.121967",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Oscillations in the complementary cumulative distribution function (CCDF) of\nindividual income data have been found in the data of various countries studied\nby different authors at different time periods, but the dynamical origins of\nthis behavior are currently unknown. Although these datasets can be fitted by\ndifferent functions at different income ranges, the Tsallis distribution has\nrecently been found capable of fitting the whole distribution by means of only\ntwo parameters. This procedure showed clearly such oscillatory feature in the\nentire income range feature, but made it particularly visible at the tail of\nthe distribution. Although log-periodic functions fitted to the data are\ncapable of describing this behavior, a different approach to naturally disclose\nsuch oscillatory characteristics is to allow the Tsallis $q$-parameter to\nbecome complex. In this paper we use this idea in order to describe the\nbehavior of the CCDF of the Brazilian personal income recently studied\nempirically by Soares et al.\\ (2016). Typical elements of periodic motion, such\nas amplitude and angular frequency coupled to this income analysis, were\nobtained by means of this approach. A highly non-linear function for the CCDF\nwas obtained through this methodology and a numerical test showed it capable of\nrecovering the main oscillatory feature of the original CCDF of the personal\nincome data of Brazil.\n"
    },
    {
        "paper_id": 1706.1018,
        "authors": "David Puelz, P. Richard Hahn, Carlos Carvalho",
        "title": "Regret-based Selection for Sparse Dynamic Portfolios",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper considers portfolio construction in a dynamic setting. We specify\na loss function comprised of utility and complexity components with an unknown\ntradeoff parameter. We develop a novel regret-based criterion for selecting the\ntradeoff parameter to construct optimal sparse portfolios over time.\n"
    },
    {
        "paper_id": 1706.10186,
        "authors": "Daniel Bartl, Samuel Drapeau, Ludovic Tangpi",
        "title": "Computational aspects of robust optimized certainty equivalents and\n  option pricing",
        "comments": null,
        "journal-ref": "Mathematical Finance, 30(1), 287--309, 2020",
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Accounting for model uncertainty in risk management and option pricing leads\nto infinite dimensional optimization problems which are both analytically and\nnumerically intractable. In this article we study when this hurdle can be\novercome for the so-called optimized certainty equivalent risk measure (OCE) --\nincluding the average value-at-risk as a special case. First we focus on the\ncase where the uncertainty is modeled by a nonlinear expectation penalizing\ndistributions that are \"far\" in terms of optimal-transport distance\n(Wasserstein distance for instance) from a given baseline distribution. It\nturns out that the computation of the robust OCE reduces to a finite\ndimensional problem, which in some cases can even be solved explicitly. This\nprinciple also applies to the shortfall risk measure as well as for the pricing\nof European options. Further, we derive convex dual representations of the\nrobust OCE for measurable claims without any assumptions on the set of\ndistributions. Finally, we give conditions on the latter set under which the\nrobust average value-at-risk is a tail risk measure.\n"
    },
    {
        "paper_id": 1707.00199,
        "authors": "Ying Hu, Gechun Liang, Shanjian Tang",
        "title": "Utility maximization in constrained and unbounded financial markets:\n  Applications to indifference valuation, regime switching, consumption and\n  Epstein-Zin recursive utility",
        "comments": "90 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This memoir presents a systematic study of utility maximization problems for\nan investor in constrained and unbounded financial markets. Building upon the\nfoundational work of Hu et al. (2005) [Ann. Appl. Probab., 15, 1691--1712] in a\nbounded framework, we extend our analysis to more challenging unbounded cases.\nOur methodology combines quadratic backward stochastic differential equations\nwith unbounded solutions and convex duality methods. Central to our approach is\nthe verification of the finite entropy condition, which plays a pivotal role in\nsolving the underlying utility maximization problems and establishing the\nmartingale property and convex duality representation of the value processes.\nThrough four distinct applications, we first study utility indifference\nvaluation of financial derivatives with unbounded payoffs, uncovering novel\nasymptotic behavior as the risk aversion parameter approaches zero or infinity.\nFurthermore, we study the regime switching market model with unbounded random\nendowments and consumption-investment problems with unbounded random\nendowments, both constrained to portfolios chosen from a convex and closed set.\nFinally, we investigate investment-consumption problems involving an investor\nwith Epstein-Zin recursive utility in an unbounded financial market.\n"
    },
    {
        "paper_id": 1707.00203,
        "authors": "Panpan Ren, Jiang-Lun Wu",
        "title": "Foreign exchange market modelling and an on-line portfolio selection\n  algorithm",
        "comments": "34 pages, 1 figure",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper, we introduce a matrix-valued time series model for foreign\nexchange market. We then formulate trading matrices, foreign exchange options\nand return options (matrices), as well as on-line portfolio strategies.\nMoreover, we attempt to predict returns of portfolios by developing a cross\nrate method. This leads us to construct an on-line portfolio selection\nalgorithm for this model. At the end, we prove the profitability and the\nuniversality of our algorithm.\n"
    },
    {
        "paper_id": 1707.00296,
        "authors": "Y\\'erali Gandica, Marco Valerio Geraci, Sophie B\\'ereau and Jean-Yves\n  Gnabo",
        "title": "Fragmentation, integration and macroprudential surveillance of the US\n  financial industry: Insights from network science",
        "comments": "27 pages, 13figures",
        "journal-ref": null,
        "doi": "10.1371/journal.pone.0195110",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Drawing on recent contributions inferring financial interconnectedness from\nmarket data, our paper provides new insights on the evolution of the US\nfinancial industry over a long period of time by using several tools coming\nfrom network science. Following [1] a Time-Varying Parameter Vector\nAutoRegressive (TVP-VAR) approach on stock market returns to retrieve\nunobserved directed links among financial institutions, we reconstruct a fully\ndynamic network in the sense that connections are let to evolve through time.\nThe financial system analysed consists of a large set of $155$ financial\ninstitutions that are all the banks, broker-dealers, insurance and real estate\ncompanies listed in the Standard & Poor's $500$ index over the period $1993 -\n2014$. Looking alternatively at the individual, then sector-, community- and\nsystem-wide levels, we show that network science's tools are able to support\nwell known features of the financial markets such as the dramatic fall of\nconnectivity following Lehman Brothers' collapse. More importantly, by means of\nless traditional metrics, such as sectoral interface or measurements based on\ncontagion processes, our results document the co-existence of both\nfragmentation and integration phases between firms independently from the\nsector they belong to, and in doing so, question the relevance of existing\nmacroprudential surveillance frameworks which have been mostly developed on a\nsectoral basis. Overall, our results improve our understanding of the US\nfinancial landscape and may have important implications for risk monitoring as\nwell as macroprudential policy design.\n"
    },
    {
        "paper_id": 1707.00356,
        "authors": "Maria do Rosario Grossinho, Yaser Faghan Kord, Daniel Sevcovic",
        "title": "Analytical and numerical results for American style of perpetual put\n  options through transformation into nonlinear stationary Black-Scholes\n  equations",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We analyze and calculate the early exercise boundary for a class of\nstationary generalized Black-Scholes equations in which the volatility function\ndepends on the second derivative of the option price itself. A motivation for\nstudying the nonlinear Black Scholes equation with a nonlinear volatility\narises from option pricing models including, e.g., non-zero transaction costs,\ninvestors preferences, feedback and illiquid markets effects and risk from\nunprotected portfolio. We present a method how to transform the problem of\nAmerican style of perpetual put options into a solution of an ordinary\ndifferential equation and implicit equation for the free boundary position. We\nfinally present results of numerical approximation of the early exercise\nboundary, option price and their dependence on model parameters.\n"
    },
    {
        "paper_id": 1707.00358,
        "authors": "Maria do Rosario Grossinho, Yaser Faghan Kord, Daniel Sevcovic",
        "title": "Pricing American Call Options by the Black-Scholes Equation with a\n  Nonlinear Volatility Function",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper we investigate a nonlinear generalization of the Black-Scholes\nequation for pricing American style call options in which the volatility term\nmay depend on the underlying asset price and the Gamma of the option. We\npropose a numerical method for pricing American style call options by means of\ntransformation of the free boundary problem for a nonlinear Black-Scholes\nequation into the so-called Gamma variational inequality with the new variable\ndepending on the Gamma of the option. We apply a modified projective successive\nover relaxation method in order to construct an effective numerical scheme for\ndiscretization of the Gamma variational inequality. Finally, we present several\ncomputational examples for the nonlinear Black-Scholes equation for pricing\nAmerican style call option under presence of variable transaction costs.\n"
    },
    {
        "paper_id": 1707.00529,
        "authors": "Jake Billings and Sebastian Del Barco",
        "title": "An Investigation into Laboucheres Betting System to Improve Odds of\n  Favorable Outcomes to Generate a Positive Externality Empirically",
        "comments": "30 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The Labouchere gambling system is hypothesized to increase the probability of\nwinning a predetermined arbitrary profit in a gambling system such as a coin\nflip or a roulette game in which both payouts and odds are 1:1. However, use of\nthe system increases the downside monetary risk in the event of a streak of\nmultiple losses. To begin, a player creates an arbitrary series of consecutive\nintegers with a sum equal to the desired profit from multiple rounds of\nbetting. Using the system, a player will either win an amount equal to the sum\nof the elements of the initial series or lose all of their available capital.\nThis sequence was simulated multiple times to determine the statistical\ncharacteristics of both the return and of the loss in an average round of\nbetting. By running the simulations of millions of rounds of Labouchere, it was\npossible to discern the probable outcomes of running the system using the\nLabouchere gambling sequence and plotting the results on a graph to map the\naverage return on the initial capital investment. The Labouchere system is very\npsychologically appealing to players because when applied over time it provides\nvery consistent linear returns. However, there is eventually a critical moment\nat which the available capital for betting is exceeded and a player loses all\nof their available capital. It was found that as the number of bets increased,\nthe outcome of applying the sequence approached zero.\n"
    },
    {
        "paper_id": 1707.0061,
        "authors": "Josselin Garnier and Knut Solna",
        "title": "Option Pricing under Fast-varying and Rough Stochastic Volatility",
        "comments": "arXiv admin note: text overlap with arXiv:1604.00105",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Recent empirical studies suggest that the volatilities associated with\nfinancial time series exhibit short-range correlations. This entails that the\nvolatility process is very rough and its autocorrelation exhibits sharp decay\nat the origin. Another classic stylistic feature often assumed for the\nvolatility is that it is mean reverting. In this paper it is shown that the\nprice impact of a rapidly mean reverting rough volatility model coincides with\nthat associated with fast mean reverting Markov stochastic volatility models.\nThis reconciles the empirical observation of rough volatility paths with the\ngood fit of the implied volatility surface to models of fast mean reverting\nMarkov volatilities. Moreover, the result conforms with recent numerical\nresults regarding rough stochastic volatility models. It extends the scope of\nmodels for which the asymptotic results of fast mean reverting Markov\nvolatilities are valid. The paper concludes with a general discussion of\nfractional volatility asymptotics and their interrelation. The regimes\ndiscussed there include fast and slow volatility factors with strong or small\nvolatility fluctuations and with the limits not commuting in general. The\nnotion of a characteristic term structure exponent is introduced, this exponent\ngoverns the implied volatility term structure in the various asymptotic\nregimes.\n"
    },
    {
        "paper_id": 1707.00757,
        "authors": "Jinglun Yao, Maxime Levy-Chapira, Mamikon Margaryan",
        "title": "Checking account activity and credit default risk of enterprises: An\n  application of statistical learning methods",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  The existence of asymmetric information has always been a major concern for\nfinancial institutions. Financial intermediaries such as commercial banks need\nto study the quality of potential borrowers in order to make their decision on\ncorporate loans. Classical methods model the default probability by financial\nratios using the logistic regression. As one of the major commercial banks in\nFrance, we have access to the the account activities of corporate clients. We\nshow that this transactional data outperforms classical financial ratios in\npredicting the default event. As the new data reflects the real time status of\ncash flow, this result confirms our intuition that liquidity plays an important\nrole in the phenomenon of default. Moreover, the two data sets are\nsupplementary to each other to a certain extent: the merged data has a better\nprediction power than each individual data. We have adopted some advanced\nmachine learning methods and analyzed their characteristics. The correct use of\nthese methods helps us to acquire a deeper understanding of the role of central\nfactors in the phenomenon of default, such as credit line violations and cash\ninflows.\n"
    },
    {
        "paper_id": 1707.00807,
        "authors": "Raj Kumari Bahl and Sotirios Sabanis",
        "title": "General Price Bounds for Guaranteed Annuity Options",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper, we are concerned with the valuation of Guaranteed Annuity\nOptions (GAOs) under the most generalised modelling framework where both\ninterest and mortality rates are stochastic and correlated. Pricing these type\nof options in the correlated environment is a challenging task and no closed\nform solution exists in the literature. We employ the use of doubly stochastic\nstopping times to incorporate the randomness about the time of death and employ\na suitable change of measure to facilitate the valuation of survival benefit,\nthere by adapting the payoff of the GAO in terms of the payoff of a basket call\noption. We derive general price bounds for GAOs by utilizing a conditioning\napproach for the lower bound and arithmetic-geometric mean inequality for the\nupper bound. The theory is then applied to affine models to present some very\ninteresting formulae for the bounds under the affine set up. Numerical examples\nare furnished and benchmarked against Monte Carlo simulations to estimate the\nprice of a GAO for a variety of affine processes governing the evolution of\nmortality and the interest rate.\n"
    },
    {
        "paper_id": 1707.00899,
        "authors": "Dan Pirjol, Lingjiong Zhu",
        "title": "Asymptotics for the Euler-Discretized Hull-White Stochastic Volatility\n  Model",
        "comments": "42 pages, 7 figures",
        "journal-ref": "Methodology and Computing in Applied Probability, Volume 20, Issue\n  1, 289-331 (2018)",
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We consider the stochastic volatility model $dS_t = \\sigma_t S_t\ndW_t,d\\sigma_t = \\omega \\sigma_t dZ_t$, with $(W_t,Z_t)$ uncorrelated standard\nBrownian motions. This is a special case of the Hull-White and the $\\beta=1$\n(log-normal) SABR model, which are widely used in financial practice. We study\nthe properties of this model, discretized in time under several applications of\nthe Euler-Maruyama scheme, and point out that the resulting model has certain\nproperties which are different from those of the continuous time model. We\nstudy the asymptotics of the time-discretized model in the $n\\to \\infty$ limit\nof a very large number of time steps of size $\\tau$, at fixed\n$\\beta=\\frac12\\omega^2\\tau n^2$ and $\\rho=\\sigma_0^2\\tau$, and derive three\nresults: i) almost sure limits, ii) fluctuation results, and iii) explicit\nexpressions for growth rates (Lyapunov exponents) of the positive integer\nmoments of $S_t$. Under the Euler-Maruyama discretization for $(S_t,\\log\n\\sigma_t)$, the Lyapunov exponents have a phase transition, which appears in\nnumerical simulations of the model as a numerical explosion of the asset price\nmoments. We derive criteria for the appearance of these explosions.\n"
    },
    {
        "paper_id": 1707.00917,
        "authors": "Olena Ragulina",
        "title": "Bonus--malus systems with different claim types and varying deductibles",
        "comments": "Published at http://dx.doi.org/10.15559/17-VMSTA80 in the Modern\n  Stochastics: Theory and Applications (https://www.i-journals.org/vtxpp/VMSTA)\n  by VTeX (http://www.vtex.lt/)",
        "journal-ref": "Modern Stochastics: Theory and Applications 2017, Vol. 4, No. 2,\n  141-159",
        "doi": "10.15559/17-VMSTA80",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The paper deals with bonus-malus systems with different claim types and\nvarying deductibles. The premium relativities are softened for the\npolicyholders who are in the malus zone and these policyholders are subject to\nper claim deductibles depending on their levels in the bonus-malus scale and\nthe types of the reported claims. We introduce such bonus-malus systems and\nstudy their basic properties. In particular, we investigate when it is possible\nto introduce varying deductibles, what restrictions we have and how we can do\nthis. Moreover, we deal with the special case where varying deductibles are\napplied to the claims reported by policyholders occupying the highest level in\nthe bonus-malus scale and consider two allocation principles for the\ndeductibles. Finally, numerical illustrations are presented.\n"
    },
    {
        "paper_id": 1707.00947,
        "authors": "Zhao Jianglin",
        "title": "The Role of Money in the Business Cycle",
        "comments": "I am so sorry, the hypothesis proposed by this paper would be not\n  appropriate because there is no mechanism on which can be based between money\n  and output value in this paper. The equation maybe more empty",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The aim of this paper is to reemphasize the money theory of exchange which is\ncentered on the function of exchange medium of money, and make a contribution\ntowards linearization of the quantity equation of exchange. A dynamical\nquantity equation is presented and an important balanced path of economic\nevolution is derived. To understand the business cycle we propose a hypothesis\nof natural cycle and driving cycle concerning the evolution of the balanced\npath and plentiful conclusions can be made.\n"
    },
    {
        "paper_id": 1707.00996,
        "authors": "Sylvain Gibaud, Jorgen W. Weibull",
        "title": "Accumulation of individual fitness or wealth as a population game",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The accumulation of individual fitness or wealth is modelled as a population\ngame in which pairs of individuals are recurrently and randomly matched to play\na game over a resource. In addition, all individuals have random access to a\nconstant background resource, and their fitness or wealth depreciates over\ntime. For brevity we focus on the well-known Hawk-Dove game. In the base-line\nmodel, the probability of winning a fight (that is, when both play Hawk) is the\nsame for both parties. In an extended version, the individual with higher\ncurrent fitness or wealth has a higher probability of winning. Analytical\nresults are given for the fitness/wealth distribution at any given time, for\nthe evolution of average fitness/wealth over time, and for the asymptotics with\nrespect to time and population size. Long-run average fitness/wealth is\nnon-monotonic in the value of the resource, thus providing a potential\nexplanation of the curse of the riches.\n"
    },
    {
        "paper_id": 1707.01028,
        "authors": "Guglielmo D'Amico, Montserrat Guillen, Raimondo Manca, Filippo Petroni",
        "title": "Multi-state models for evaluating conversion options in life insurance",
        "comments": "Published at http://dx.doi.org/10.15559/17-VMSTA78 in the Modern\n  Stochastics: Theory and Applications (https://www.i-journals.org/vtxpp/VMSTA)\n  by VTeX (http://www.vtex.lt/)",
        "journal-ref": "Modern Stochastics: Theory and Applications 2017, Vol. 4, No. 2,\n  127-139",
        "doi": "10.15559/17-VMSTA78",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper we propose a multi-state model for the evaluation of the\nconversion option contract. The multi-state model is based on age-indexed\nsemi-Markov chains that are able to reproduce many important aspects that\ninfluence the valuation of the option such as the duration problem, the time\nnon-homogeneity and the ageing effect. The value of the conversion option is\nevaluated after the formal description of this contract.\n"
    },
    {
        "paper_id": 1707.01167,
        "authors": "Federico Gonzalez and Mark Schervish",
        "title": "Instantaneous order impact and high-frequency strategy optimization in\n  limit order books",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We propose a limit order book (LOB) model with dynamics that account for both\nthe impact of the most recent order and the shape of the LOB. We present an\nempirical analysis showing that the type of the last order significantly alters\nthe submission rate of immediate future orders, even after accounting for the\nstate of the LOB. To model these effects jointly we introduce a discrete Markov\nchain model. Then on these improved LOB dynamics, we find the policy for\noptimal order choice and placement in the share purchasing problem by framing\nit as a Markov decision process. The optimal policy derived numerically uses\nlimit orders, cancellations and market orders. It looks to exploit the state of\nthe LOB summarized by the volume at the bid/ask and the type of the most recent\norder to obtain the best execution price, avoiding non-execution and adverse\nselection risk simultaneously. Market orders are used aggressively when the\nmid-price is expected to move adversely. Limit orders are placed under\nfavorable LOB conditions and canceled when non-execution or adverse selection\nprobability is high. Using ultra high-frequency data from the NASDAQ stock\nexchange we compare our optimal policy with other submission strategies that\nuse a subset of all available order types and show that ours significantly\noutperforms them.\n"
    },
    {
        "paper_id": 1707.01178,
        "authors": "Ariel Neufeld",
        "title": "Buy-and-Hold Property for Fully Incomplete Markets when\n  Super-replicating Markovian Claims",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We show that when the price process $S$ represents a fully incomplete market,\nthe optimal super-replication of any Markovian claim $g(S_T)$ with $g(\\cdot)$\nbeing nonnegative and lower semicontinuous is of buy-and-hold type. Since both\n(unbounded) stochastic volatility models and rough volatility models are\nexamples of fully incomplete markets, one can interpret the buy-and-hold\nproperty when super-replicating Markovian claims as a natural phenomenon in\nincomplete markets.\n"
    },
    {
        "paper_id": 1707.01237,
        "authors": "Arunangshu Biswas, Anindya Goswami and Ludger Overbeck",
        "title": "Option Pricing in a Regime Switching Stochastic Volatility Model",
        "comments": "15 pages, no figures",
        "journal-ref": "Statistics & Probability Letters 138(2018), 116-126",
        "doi": "10.1016/j.spl.2018.02.056",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In the classical model of stock prices which is assumed to be Geometric\nBrownian motion, the drift and the volatility of the prices are held constant.\nHowever, in reality, the volatility does vary. In quantitative finance, the\nHeston model has been successfully used where the volatility is expressed as a\nstochastic differential equation. In addition, we consider a regime switching\nmodel where the stock volatility dynamics depends on an underlying process\nwhich is possibly a non-Markov pure jump process. Under this model assumption,\nwe find the locally risk minimizing pricing of European type vanilla options.\nThe price function is shown to satisfy a Heston type PDE.\n"
    },
    {
        "paper_id": 1707.01284,
        "authors": "Jamal Bouoiyour (1), Refk Selmi (1) ((1) CATT)",
        "title": "The Bitcoin price formation: Beyond the fundamental sources",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Much significant research has been done to investigate various facets of the\nlink between Bitcoin price and its fundamental sources. This study goes beyond\nby looking into least to most influential factors-across the fundamental,\nmacroeconomic, financial, speculative and technical determinants as well as the\n2016 events-which drove the value of Bitcoin in times of economic and\ngeopolitical chaos. We use a Bayesian quantile regression to inspect how the\nstructure of dependence of Bitcoin price and its determinants varies across the\nentire conditional distribution of Bitcoin price movements. In doing so, three\ngroups of determinants were derived. The use of Bitcoin in trade and the\nuncertainty surrounding China's deepening slowdown, Brexit and India's\ndemonetization were found to be the most potential contributors of Bitcoin\nprice when the market is improving. The intense anxiety over Donald Trump being\nthe president of United States was shown to be a positive determinant pushing\nup the price of Bitcoin when the market is functioning around the normal mode.\nThe velocity of bitcoins in circulation, the gold price, the Venezuelan\ncurrency demonetization and the hash rate were found to be the fundamentals\ninfluencing the Bitcoin price when the market is heading into decline.\n"
    },
    {
        "paper_id": 1707.0137,
        "authors": "Andrea Fontanari, Nassim Nicholas Taleb, Pasquale Cirillo",
        "title": "Gini estimation under infinite variance",
        "comments": null,
        "journal-ref": "Physica A: Statistical Mechanics and its Applications 502,\n  256-269, 2018",
        "doi": "10.1016/j.physa.2018.02.102",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We study the problems related to the estimation of the Gini index in presence\nof a fat-tailed data generating process, i.e. one in the stable distribution\nclass with finite mean but infinite variance (i.e. with tail index\n$\\alpha\\in(1,2)$). We show that, in such a case, the Gini coefficient cannot be\nreliably estimated using conventional nonparametric methods, because of a\ndownward bias that emerges under fat tails. This has important implications for\nthe ongoing discussion about economic inequality.\n  We start by discussing how the nonparametric estimator of the Gini index\nundergoes a phase transition in the symmetry structure of its asymptotic\ndistribution, as the data distribution shifts from the domain of attraction of\na light-tailed distribution to that of a fat-tailed one, especially in the case\nof infinite variance. We also show how the nonparametric Gini bias increases\nwith lower values of $\\alpha$. We then prove that maximum likelihood estimation\noutperforms nonparametric methods, requiring a much smaller sample size to\nreach efficiency.\n  Finally, for fat-tailed data, we provide a simple correction mechanism to the\nsmall sample bias of the nonparametric estimator based on the distance between\nthe mode and the mean of its asymptotic distribution.\n"
    },
    {
        "paper_id": 1707.01436,
        "authors": "Daniel Sevcovic",
        "title": "Nonlinear Parabolic Equations arising in Mathematical Finance",
        "comments": "arXiv admin note: substantial text overlap with arXiv:1603.03874",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This survey paper is focused on qualitative and numerical analyses of fully\nnonlinear partial differential equations of parabolic type arising in financial\nmathematics. The main purpose is to review various non-linear extensions of the\nclassical Black-Scholes theory for pricing financial instruments, as well as\nmodels of stochastic dynamic portfolio optimization leading to the\nHamilton-Jacobi-Bellman (HJB) equation. After suitable transformations, both\nproblems can be represented by solutions to nonlinear parabolic equations.\nQualitative analysis will be focused on issues concerning the existence and\nuniqueness of solutions. In the numerical part we discuss a stable\nfinite-volume and finite difference schemes for solving fully nonlinear\nparabolic equations.\n"
    },
    {
        "paper_id": 1707.01457,
        "authors": "Adam Rej, Philip Seager, Jean-Philippe Bouchaud",
        "title": "You are in a drawdown. When should you start worrying?",
        "comments": "4 pages, 5 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Trading strategies that were profitable in the past often degrade with time.\nSince unlucky streaks can also hit \"healthy\" strategies, how can one detect\nthat something truly worrying is happening? It is intuitive that a drawdown\nthat lasts too long or one that is too deep should lead to a downward revision\nof the assumed Sharpe ratio of the strategy. In this note, we give a\nquantitative answer to this question based on the exact probability\ndistributions for the length and depth of the last drawdown for upward drifting\nBrownian motions. We also point out that both managers and investors tend to\nunderestimate the length and depth of drawdowns consistent with the Sharpe\nratio of the underlying strategy.\n"
    },
    {
        "paper_id": 1707.016,
        "authors": "Tomoyuki Ichiba, Seyyed Mostafa Mousavi",
        "title": "Option Pricing with Delayed Information",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We propose a model to study the effects of delayed information on option\npricing. We first talk about the absence of arbitrage in our model, and then\ndiscuss super replication with delayed information in a binomial model,\nnotably, we present a closed form formula for the price of convex contingent\nclaims. Also, we address the convergence problem as the time-step and delay\nlength tend to zero and introduce analogous results in the continuous time\nframework. Finally, we explore how delayed information exaggerates the\nvolatility smile.\n"
    },
    {
        "paper_id": 1707.02019,
        "authors": "Massimo Caccia and Bruno R\\'emillard",
        "title": "Option Pricing and Hedging for Discrete Time Autoregressive Hidden\n  Markov Model",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper we solve the discrete time mean-variance hedging problem when\nasset returns follow a multivariate autoregressive hidden Markov model. Time\ndependent volatility and serial dependence are well established properties of\nfinancial time series and our model covers both. To illustrate the relevance of\nour proposed methodology, we first compare the proposed model with the\nwell-known hidden Markov model via likelihood ratio tests and a novel\ngoodness-of-fit test on the S\\&P 500 daily returns. Secondly, we present\nout-of-sample hedging results on S\\&P 500 vanilla options as well as a trading\nstrategy based on theoretical prices, which we compare to simpler models\nincluding the classical Black-Scholes delta-hedging approach.\n"
    },
    {
        "paper_id": 1707.02087,
        "authors": "Margarita E. Fatyanova, Mikhail E. Semenov",
        "title": "Model for Constructing an Options Portfolio with a Certain Payoff\n  Function",
        "comments": "10 pages, 2 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The portfolio optimization problem is a basic problem of financial analysis.\nIn the study, an optimization model for constructing an options portfolio with\na certain payoff function has been proposed. The model is formulated as an\ninteger linear programming problem and includes an objective payoff function\nand a system of constraints. In order to demonstrate the performance of the\nproposed model, we have constructed the portfolio on the European call and put\noptions of Taiwan Futures Exchange. The optimum solution was obtained using the\nMATLAB software. Our approach is quite general and has the potential to design\noptions portfolios on financial markets.\n"
    },
    {
        "paper_id": 1707.02188,
        "authors": "Emanuele Pugliese, Lorenzo Napolitano, Andrea Zaccaria, and Luciano\n  Pietronero",
        "title": "Coherent diversification in corporate technological portfolios",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We study the relationship between firms' performance and their technological\nportfolios using tools borrowed from the complexity science. In particular, we\nask whether the accumulation of knowledge and capabilities related to a\ncoherent set of technologies leads firms to experience advantages in terms of\nproductive efficiency. To this end, we analyzed both the balance sheets and the\npatenting activity of about 70 thousand firms that have filed at least one\npatent over the period 2004-2013. From this database it is possible to define a\nmeasure of the firms' coherent diversification, based on the network of\ntechnological fields, and relate it to the firms' perfomance in terms of labor\nproductivity. Such a measure favors companies with a diversification structure\ncomprising blocks of closely related fields over firms with the same breadth of\nscope, but a more scattered diversification structure. We find that the\ncoherent diversification of firms is quantitatively related to their economic\nperformance and captures relevant information about their productive structure.\nIn particular, we prove on a statistical basis that a naive definition of\ntechnological diversification can explain labor productivity only as a proxy of\nsize and coherent diversification. This approach can be used to investigate\npossible synergies within firms and to recommend viable partners for merging\nand acquisitions.\n"
    },
    {
        "paper_id": 1707.02496,
        "authors": "Patricia Kisbye and Karem Meier",
        "title": "Consistency of extended Nelson-Siegel curve families with the Ho-Lee and\n  Hull and White short rate models",
        "comments": "13 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Nelson and Siegel curves are widely used to fit the observed term structure\nof interest rates in a particular date. By the other hand, several interest\nrate models have been developed such their initial forward rate curve can be\nadjusted to any observed data, as the Ho-Lee and the Hull and White one factor\nmodels. In this work we study the evolution of the forward curve process for\neach of this models assuming that the initial curve is of Nelson-Siegel type.\nWe conclude that the forward curve process produces curves belonging to a\nparametric family of curves that can be seen as extended Nelson and Siegel\ncurves.\n"
    },
    {
        "paper_id": 1707.02587,
        "authors": "Wilson Ye Chen, Gareth W. Peters, Richard H. Gerlach, Scott A. Sisson",
        "title": "Dynamic Quantile Function Models",
        "comments": "MATLAB code: https://github.com/wilson-ye-chen/aqua",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Motivated by the need for effectively summarising, modelling, and forecasting\nthe distributional characteristics of intra-daily returns, as well as the\nrecent work on forecasting histogram-valued time-series in the area of symbolic\ndata analysis, we develop a time-series model for forecasting\nquantile-function-valued (QF-valued) daily summaries for intra-daily returns.\nWe call this model the dynamic quantile function (DQF) model. Instead of a\nhistogram, we propose to use a $g$-and-$h$ quantile function to summarise the\ndistribution of intra-daily returns. We work with a Bayesian formulation of the\nDQF model in order to make statistical inference while accounting for parameter\nuncertainty; an efficient MCMC algorithm is developed for sampling-based\nposterior inference. Using ten international market indices and approximately\n2,000 days of out-of-sample data from each market, the performance of the DQF\nmodel compares favourably, in terms of forecasting VaR of intra-daily returns,\nagainst the interval-valued and histogram-valued time-series models.\nAdditionally, we demonstrate that the QF-valued forecasts can be used to\nforecast VaR measures at the daily timescale via a simple quantile regression\nmodel on daily returns (QR-DQF). In certain markets, the resulting QR-DQF model\nis able to provide competitive VaR forecasts for daily returns.\n"
    },
    {
        "paper_id": 1707.02736,
        "authors": "Korbinian Dress, Stefan Lessmann, Hans-J\\\"org von Mettenheim",
        "title": "Residual Value Forecasting Using Asymmetric Cost Functions",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Leasing is a popular channel to market new cars. Pricing a leasing contract\nis complicated because the leasing rate embodies an expectation of the residual\nvalue of the car after contract expiration. To aid lessors in their pricing\ndecisions, the paper develops resale price forecasting models. A peculiarity of\nthe leasing business is that forecast errors entail different costs.\nIdentifying effective ways to address this characteristic is the main objective\nof the paper. More specifically, the paper contributes to the literature\nthrough i) consolidating and integrating previous work in forecasting with\nasymmetric cost of error functions, ii) systematically evaluating previous\napproaches and comparing them to a new approach, and iii) demonstrating that\nforecasting with asymmetric cost of error functions enhances the quality of\ndecision support in car leasing. For example, under the assumption that the\ncosts of overestimating resale prices is twice that of the opposite error,\nincorporating corresponding cost asymmetry into forecast model development\nreduces decision costs by about eight percent, compared to a standard\nforecasting model. Higher asymmetry produces even larger improvements.\n"
    },
    {
        "paper_id": 1707.02853,
        "authors": "Klaus Jaffe",
        "title": "The Wealth of Nations: Complexity Science for an Interdisciplinary\n  Approach in Economics",
        "comments": "Amazon Books 2014, ISBN-13: 978-1503252721",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Classic economic science is reaching the limits of its explanatory powers.\nComplexity science uses an increasingly larger set of different methods to\nanalyze physical, biological, cultural, social, and economic factors, providing\na broader understanding of the socio-economic dynamics involved in the\ndevelopment of nations worldwide. The use of tools developed in the natural\nsciences, such as thermodynamics, evolutionary biology, and analysis of complex\nsystems, help us to integrate aspects, formerly reserved to the social\nsciences, with the natural sciences. This integration reveals details of the\nsynergistic mechanisms that drive the evolution of societies. By doing so, we\nincrease the available alternatives for economic analysis and provide ways to\nincrease the efficiency of decision-making mechanisms in complex social\ncontexts. This interdisciplinary analysis seeks to deepen our understanding of\nwhy chronic poverty is still common, and how the emergence of prosperous\ntechnological societies can be made possible. This understanding should\nincrease the chances of achieving a sustainable, harmonious and prosperous\nfuture for humanity. The analysis evidences that complex fundamental economic\nproblems require multidisciplinary approaches and rigorous application of the\nscientific method if we want to advance significantly our understanding of\nthem. The analysis reveals viable routes for the generation of wealth and the\nreduction of poverty, but also reveals huge gaps in our knowledge about the\ndynamics of our societies and about the means to guide social development\ntowards a better future for all.\n"
    },
    {
        "paper_id": 1707.03335,
        "authors": "Matteo Burzoni, Frank Riedel and H. Mete Soner",
        "title": "Viability and Arbitrage under Knightian Uncertainty",
        "comments": "to appear in Econometrica",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We reconsider the microeconomic foundations of financial economics. Motivated\nby the importance of Knightian Uncertainty in markets, we present a model that\ndoes not carry any probabilistic structure ex ante, yet is based on a common\norder. We derive the fundamental equivalence of economic viability of asset\nprices and absence of arbitrage. We also obtain a modified version of the\nFundamental Theorem of Asset Pricing using the notion of sublinear pricing\nmeasures. Different versions of the Efficient Market Hypothesis are related to\nthe assumptions one is willing to impose on the common order.\n"
    },
    {
        "paper_id": 1707.03391,
        "authors": "William Guevara-Alarc\\'on, Luz Mery Gonz\\'alez and Armando Antonio\n  Zarruk",
        "title": "The partial damage loss cover ratemaking of the automobile insurance\n  using generalized linear models",
        "comments": "in Spanish",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  It is illustrated a methodology to compute the pure premium for the\nautomobile insurance (claim frequency and severity) using generalized linear\nmodels. It is obtained the pure premium for the partial damage loss cover (PPD)\nusing a set of automobile insurance policies with an exposition of a year. It\nis found that the most influential variables in the claim frequency are the car\nproduction year, the insured's age, and the region's subscription policy and\nthe most influential variables in the claim severity are the car's value, type\nand make and the insured's gender.\n"
    },
    {
        "paper_id": 1707.03498,
        "authors": "Yerkin Kitapbayev and Tim Leung",
        "title": "Mean Reversion Trading with Sequential Deadlines and Transaction Costs",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We study the optimal timing strategies for trading a mean-reverting price\nprocess with afinite deadline to enter and a separate finite deadline to exit\nthe market. The price process is modeled by a diffusion with an affine drift\nthat encapsulates a number of well-known models,including the\nOrnstein-Uhlenbeck (OU) model, Cox-Ingersoll-Ross (CIR) model, Jacobi model,and\ninhomogeneous geometric Brownian motion (IGBM) model.We analyze three types of\ntrading strategies: (i) the long-short (long to open, short to close) strategy;\n(ii) the short-long(short to open, long to close) strategy, and (iii) the\nchooser strategy whereby the trader has the added flexibility to enter the\nmarket by taking either a long or short position, and subsequently close the\nposition. For each strategy, we solve an optimal double stopping problem with\nsequential deadlines, and determine the optimal timing of trades. Our solution\nmethodology utilizes the local time-space calculus of Peskir (2005) to derive\nnonlinear integral equations of Volterra-type that uniquely characterize the\ntrading boundaries. Numerical implementation ofthe integral equations provides\nexamples of the optimal trading boundaries.\n"
    },
    {
        "paper_id": 1707.035,
        "authors": "Olena Kostylenko, Helena Sofia Rodrigues, Delfim F. M. Torres",
        "title": "Banking risk as an epidemiological model: an optimal control approach",
        "comments": "This is a preprint of a paper whose final and definite form is in\n  'Operational Research', Springer Proceedings in Mathematics & Statistics,\n  available at [http://www.springer.com/series/10533]. Paper Submitted\n  23/March/17; Revised 29/May/17; Accepted 11/July/2017",
        "journal-ref": "Springer Proc. Math. Stat. 223 (2018), 165--176",
        "doi": "10.1007/978-3-319-71583-4_12",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The process of contagiousness spread modelling is well-known in epidemiology.\nHowever, the application of spread modelling to banking market is quite recent.\nIn this work, we present a system of ordinary differential equations,\nsimulating data from the largest European banks. Then, an optimal control\nproblem is formulated in order to study the impact of a possible measure of the\nCentral Bank in the economy. The proposed approach enables qualitative\nspecifications of contagion in banking obtainment and an adequate analysis and\nprognosis within the financial sector development and macroeconomic as a whole.\nWe show that our model describes well the reality of the largest European\nbanks. Simulations were done using MATLAB and BOCOP optimal control solver, and\nthe main results are taken for three distinct scenarios.\n"
    },
    {
        "paper_id": 1707.03516,
        "authors": "Mikhail Semenov, Daulet Smagulov",
        "title": "Portfolio Risk Assessment using Copula Models",
        "comments": "13 pages, 5 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In the paper, we use and investigate copulas models to represent multivariate\ndependence in financial time series. We propose the algorithm of risk measure\ncomputation using copula models. Using the optimal mean-$CVaR$ portfolio we\ncompute portfolio's Profit and Loss series and corresponded risk measures\ncurves. Value-at-risk and Conditional-Value-at-risk curves were simulated by\nthree copula models: full Gaussian, Student's $t$ and regular vine copula.\nThese risk curves are lower than historical values of the risk measures curve.\nAll three models have superior prediction ability than a usual empirical\nmethod. Further directions of research are described.\n"
    },
    {
        "paper_id": 1707.03542,
        "authors": "Aditya Maheshwari, Andrey Sarantsev",
        "title": "Modeling Financial System with Interbank Flows, Borrowing, and Investing",
        "comments": "27 pages, 29 figures. Keywords: systemic risk, stochastic control,\n  principal-agent problem, stationary distribution, stochastic stability,\n  Lyapunov function",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In our model, private actors with interbank cash flows similar to, but nore\ngeneral than (Carmona, Fouque, Sun, 2013) borrow from the outside economy at a\ncertain interest rate, controlled by the central bank, and invest in risky\nassets. Each private actor aims to maximize its expected terminal logarithmic\nutility. The central bank, in turn, aims to control the overall economy by\nmeans of an exponential utility function. We solve all stochastic optimal\ncontrol problems explicitly. We are able to recreate occasions such as\nliquidity trap. We study distribution of the number of defaults (net worth of a\nprivate actor going below a certain threshold).\n"
    },
    {
        "paper_id": 1707.03588,
        "authors": "Valentin Vankov Iliev",
        "title": "On Markowitz Geometry",
        "comments": "16 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  By Markowitz geometry we mean the intersection theory of ellipsoids and\naffine subspaces in a real finite-dimensional linear space. In the paper we\ngive a meticulous and self-contained treatment of this arch-classical subject,\nwhich lays a solid mathematical groundwork of Markowitz mean-variance theory of\nefficient portfolios in economics.\n"
    },
    {
        "paper_id": 1707.03715,
        "authors": "Chao Wang (1), Qian Chen (2), Richard Gerlach (1) ((1) Discipline of\n  Business Analytics, The University of Sydney, (2) HSBC Business School,\n  Peking University)",
        "title": "Bayesian Realized-GARCH Models for Financial Tail Risk Forecasting\n  Incorporating Two-sided Weibull Distribution",
        "comments": "33 pages, 5 figures, 8 tables. arXiv admin note: substantial text\n  overlap with arXiv:1612.08488",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The realized GARCH framework is extended to incorporate the two-sided Weibull\ndistribution, for the purpose of volatility and tail risk forecasting in a\nfinancial time series. Further, the realized range, as a competitor for\nrealized variance or daily returns, is employed in the realized GARCH\nframework. Further, sub-sampling and scaling methods are applied to both the\nrealized range and realized variance, to help deal with inherent\nmicro-structure noise and inefficiency. An adaptive Bayesian Markov Chain Monte\nCarlo method is developed and employed for estimation and forecasting, whose\nproperties are assessed and compared with maximum likelihood, via a simulation\nstudy. Compared to a range of well-known parametric GARCH, GARCH with two-sided\nWeibull distribution and realized GARCH models, tail risk forecasting results\nacross 7 market index return series and 2 individual assets clearly favor the\nrealized GARCH models incorporating two-sided Weibull distribution, especially\nmodels employing the sub-sampled realized variance and sub-sampled realized\nrange, over a six year period that includes the global financial crisis.\n"
    },
    {
        "paper_id": 1707.03746,
        "authors": "Mariusz Tarnopolski",
        "title": "Modeling the price of Bitcoin with geometric fractional Brownian motion:\n  a Monte Carlo approach",
        "comments": "5 pages, 3 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The long-term dependence of Bitcoin (BTC), manifesting itself through a Hurst\nexponent $H>0.5$, is exploited in order to predict future BTC/USD price. A\nMonte Carlo simulation with $10^4$ geometric fractional Brownian motion\nrealisations is performed as extensions of historical data. The accuracy of\nstatistical inferences is 10\\%. The most probable Bitcoin price at the\nbeginning of 2018 is 6358 USD.\n"
    },
    {
        "paper_id": 1707.0396,
        "authors": "Jonathan R. Sweeney, Richard E. Howitt, Hing Ling Chan, Minling Pan,\n  PingSun Leung",
        "title": "How do fishery policies affect Hawaii's longline fishing industry?\n  Calibrating a positive mathematical programming model",
        "comments": null,
        "journal-ref": "Natural Resource Modeling, 30(2) (2017)",
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We present a vessel and target-specific positive mathematical programming\nmodel (PMP) for Hawaii's longline fishing fleet. Although common in\nagricultural economics, PMP modeling is rarely attempted in fisheries. To\ndemonstrate the flexibility of the PMP framework, we separate tuna and\nswordfish production technologies into three policy relevant fishing targets.\nWe find the model most accurately predicts vessel-specific annual bigeye catch\nin the WCPO, with an accuracy of 12% to 35%, and a correlation between 0.30 and\n0.53. To demonstrate the model's usefulness to policy makers, we simulate the\neconomic impact to individual vessels from increasing and decreasing the bigeye\ncatch limit in the WCPO by 10%. Our results suggest that such policy changes\nwill have moderate impacts on most vessels, but large impacts on a few\ngenerating a fat tailed distribution. These results offer insights into the\nrange of winners and losers resulting from changes in fishery policies, and\ntherefore, which policies are more likely to gain widespread industry support.\nAs a tool for fishery management, the calibrated PMP model offers a flexible\nand easy-to-use framework, capable of capturing the heterogeneous response of\nfishing vessels to evaluate policy changes.\n"
    },
    {
        "paper_id": 1707.04149,
        "authors": "Oleg L. Kritski and Vladimir F. Zalmezh",
        "title": "Asymptotics for Greeks under the constant elasticity of variance model",
        "comments": "14 pages, 20 References",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper is concerned with the asymptotics for Greeks of European-style\noptions and the risk-neutral density function calculated under the constant\nelasticity of variance model. Formulae obtained help financial engineers to\nconstruct a perfect hedge with known behaviour and to price any options on\nfinancial assets.\n"
    },
    {
        "paper_id": 1707.04285,
        "authors": "Ricardo T. Fernholz, Robert Fernholz",
        "title": "Zipf's Law for Atlas Models",
        "comments": "Accepted for publication by the Applied Probability Trust\n  (http://www.appliedprobability.org) in the Journal of Applied Probability\n  57.4 (December 2020)",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  A set of data with positive values follows a Pareto distribution if the\nlog-log plot of value versus rank is approximately a straight line. A Pareto\ndistribution satisfies Zipf's law if the log-log plot has a slope of -1. Since\nmany types of ranked data follow Zipf's law, it is considered a form of\nuniversality. We propose a mathematical explanation for this phenomenon based\non Atlas models and first-order models, systems of positive continuous\nsemimartingales with parameters that depend only on rank. We show that the\nstable distribution of an Atlas model will follow Zipf's law if and only if two\nnatural conditions, conservation and completeness, are satisfied. Since Atlas\nmodels and first-order models can be constructed to approximate systems of\ntime-dependent rank-based data, our results can explain the universality of\nZipf's law for such systems. However, ranked data generated by other means may\nfollow non-Zipfian Pareto distributions. Hence, our results explain why Zipf's\nlaw holds for word frequency, firm size, household wealth, and city size, while\nit does not hold for earthquake magnitude, cumulative book sales, the intensity\nof solar flares, and the intensity of wars, all of which follow non-Zipfian\nPareto distributions.\n"
    },
    {
        "paper_id": 1707.04293,
        "authors": "Gunther Leobacher",
        "title": "A short introduction to quasi-Monte Carlo option pricing",
        "comments": null,
        "journal-ref": null,
        "doi": "10.1515/9783110317930.191",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  One of the main practical applications of quasi-Monte Carlo (QMC) methods is\nthe valuation of financial derivatives. We aim to give a short introduction\ninto option pricing and show how it is facilitated using QMC. We give some\npractical examples for illustration.\n"
    },
    {
        "paper_id": 1707.04475,
        "authors": "Francesca Biagini, Yinglin Zhang",
        "title": "Reduced-form framework under model uncertainty",
        "comments": null,
        "journal-ref": "Annals of Applied Probability 2019, Vol. 29, No. 4, 2481-2522",
        "doi": "10.1214/18-AAP1458",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper we introduce a sublinear conditional expectation with respect\nto a family of possibly nondominated probability measures on a progressively\nenlarged filtration. In this way, we extend the classic reduced-form setting\nfor credit and insurance markets to the case under model uncertainty, when we\nconsider a family of priors possibly mutually singular to each other.\nFurthermore, we study the superhedging approach in continuous time for payment\nstreams under model uncertainty, and establish several equivalent versions of\ndynamic robust superhedging duality. These results close the gap between robust\nframework for financial market, which is recently studied in an intensive way,\nand the one for credit and insurance markets, which is limited in the present\nliterature only to some very specific cases.\n"
    },
    {
        "paper_id": 1707.04699,
        "authors": "Sander Heinsalu",
        "title": "Good signals gone bad: dynamic signalling with switching efforts",
        "comments": "1 figure",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper examines signalling when the sender exerts effort and receives\nbenefits over time. Receivers only observe a noisy public signal about the\neffort, which has no intrinsic value.\n  The modelling of signalling in a dynamic context gives rise to novel\nequilibrium outcomes. In some equilibria, a sender with a higher cost of effort\nexerts strictly more effort than his low-cost counterpart. The low-cost type\ncan compensate later for initial low effort, but this is not worthwhile for a\nhigh-cost type. The interpretation of a given signal switches endogenously over\ntime, depending on which type the receivers expect to send it.\n  JEL classification: D82, D83, C73.\n  Keywords: Dynamic games, signalling , incomplete information\n"
    },
    {
        "paper_id": 1707.04831,
        "authors": "Xiaojiao Yu",
        "title": "Machine learning application in online lending risk prediction",
        "comments": "9 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/publicdomain/zero/1.0/",
        "abstract": "  Online leading has disrupted the traditional consumer banking sector with\nmore effective loan processing. Risk prediction and monitoring is critical for\nthe success of the business model. Traditional credit score models fall short\nin applying big data technology in building risk model. In this manuscript,\ndata with various format and size were collected from public website,\nthird-parties and assembled with client's loan application information data.\nEnsemble machine learning models, random forest model and XGBoost model, were\nbuilt and trained with the historical transaction data and subsequently tested\nwith separate data. XGBoost model shows higher K-S value, suggesting better\nclassification capability in this task. Top 10 important features from the two\nmodels suggest external data such as zhimaScore, multi-platform stacking loans\ninformation, and social network information are important factors in predicting\nloan default probability.\n"
    },
    {
        "paper_id": 1707.04838,
        "authors": "Petr Jizba, Jan Korbel, Hynek Lavi\\v{c}ka, Martin Prok\\v{s}, V\\'aclav\n  Svoboda, Christian Beck",
        "title": "Transitions between superstatistical regimes: validity, breakdown and\n  applications",
        "comments": null,
        "journal-ref": "Physica A 493 (2018), 29-46",
        "doi": "10.1016/j.physa.2017.09.109",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Superstatistics is a widely employed tool of non-equilibrium statistical\nphysics which plays an important role in analysis of hierarchical complex\ndynamical systems. Yet, its \"canonical\" formulation in terms of a single\nnuisance parameter is often too restrictive when applied to complex empirical\ndata. Here we show that a multi-scale generalization of the superstatistics\nparadigm is more versatile, allowing to address such pertinent issues as\ntransmutation of statistics or inter-scale stochastic behavior. To put some\nflesh on the bare bones, we provide a numerical evidence for a transition\nbetween two superstatistics regimes, by analyzing high-frequency (minute-tick)\ndata for share-price returns of seven selected companies. Salient issues, such\nas breakdown of superstatistics in fractional diffusion processes or connection\nwith Brownian subordination are also briefly discussed.\n"
    },
    {
        "paper_id": 1707.04868,
        "authors": "Vasilios Plakandaras, Rangan Gupta, Periklis Gogas and Theophilos\n  Papadimitriou",
        "title": "Forecasting the U.S. Real House Price Index",
        "comments": "27 pages",
        "journal-ref": "Economic Modelling, vol. 45, pp. 259-267, 2015",
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The 2006 sudden and immense downturn in U.S. House Prices sparked the 2007\nglobal financial crisis and revived the interest about forecasting such\nimminent threats for economic stability. In this paper we propose a novel\nhybrid forecasting methodology that combines the Ensemble Empirical Mode\nDecomposition (EEMD) from the field of signal processing with the Support\nVector Regression (SVR) methodology that originates from machine learning. We\ntest the forecasting ability of the proposed model against a Random Walk (RW)\nmodel, a Bayesian Autoregressive and a Bayesian Vector Autoregressive model.\nThe proposed methodology outperforms all the competing models with half the\nerror of the RW model with and without drift in out-of-sample forecasting.\nFinally, we argue that this new methodology can be used as an early warning\nsystem for forecasting sudden house prices drops with direct policy\nimplications.\n"
    },
    {
        "paper_id": 1707.0487,
        "authors": "J-F Mercure, H. Pollitt, N.R. Edwards, P.B. Holden, U. Chewpreecha, P.\n  Salas, A. Lam, F. Knobloch and J. Vinuales",
        "title": "Environmental impact assessment for climate change policy with the\n  simulation-based integrated assessment model E3ME-FTT-GENIE",
        "comments": "23 pages + Supplementary Information 26 pages",
        "journal-ref": "Energy Strategy Reviews, 20, 195-208 2018",
        "doi": "10.1016/j.esr.2018.03.003",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  A high degree of consensus exists in the climate sciences over the role that\nhuman interference with the atmosphere is playing in changing the climate.\nFollowing the Paris Agreement, a similar consensus exists in the policy\ncommunity over the urgency of policy solutions to the climate problem. The\ncontext for climate policy is thus moving from agenda setting, which has now\nbeen mostly established, to impact assessment, in which we identify policy\npathways to implement the Paris Agreement. Most integrated assessment models\ncurrently used to address the economic and technical feasibility of avoiding\nclimate change are based on engineering perspectives with a normative systems\noptimisation philosophy, suitable for agenda setting, but unsuitable to assess\nthe socio-economic impacts of a realistic baskets of climate policies. Here, we\nintroduce a fully descriptive, simulation-based integrated assessment model\ndesigned specifically to assess policies, formed by the combination of (1) a\nhighly disaggregated macro-econometric simulation of the global economy based\non time series regressions (E3ME), (2) a family of bottom-up evolutionary\nsimulations of technology diffusion based on cross-sectional discrete choice\nmodels (FTT), and (3) a carbon cycle and atmosphere circulation model of\nintermediate complexity (GENIE-1). We use this combined model to create a\ndetailed global and sectoral policy map and scenario that sets the economy on a\npathway that achieves the goals of the Paris Agreement with >66% probability of\nnot exceeding 2$^\\circ$C of global warming. We propose a blueprint for a new\nrole for integrated assessment models in this upcoming policy assessment\ncontext.\n"
    },
    {
        "paper_id": 1707.04942,
        "authors": "Christian P. Fries",
        "title": "Automatic Backward Differentiation for American Monte-Carlo Algorithms\n  (Conditional Expectation)",
        "comments": "11 pages, 1 figure",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this note we derive the backward (automatic) differentiation (adjoint\n[automatic] differentiation) for an algorithm containing a conditional\nexpectation operator. As an example we consider the backward algorithm as it is\nused in Bermudan product valuation, but the method is applicable in full\ngenerality.\n  The method relies on three simple properties: 1) a forward or backward\n(automatic) differentiation of an algorithm containing a conditional\nexpectation operator results in a linear combination of the conditional\nexpectation operators; 2) the differential of an expectation is the expectation\nof the differential $\\frac{d}{dx} E(Y) = E(\\frac{d}{dx}Y)$; 3) if we are only\ninterested in the expectation of the final result (as we are in all valuation\nproblems), we may use $E(A \\cdot E(B\\vert\\mathcal{F})) = E(E(A\\vert\\mathcal{F})\n\\cdot B)$, i.e., instead of applying the (conditional) expectation operator to\na function of the underlying random variable (continuation values), it may be\napplied to the adjoint differential. \\end{enumerate}\n  The methodology not only allows for a very clean and simple implementation,\nbut also offers the ability to use different conditional expectation estimators\nin the valuation and the differentiation.\n"
    },
    {
        "paper_id": 1707.04949,
        "authors": "Niushan Gao, Cosimo Munari",
        "title": "Surplus-invariant risk measures",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper presents a systematic study of the notion of surplus invariance,\nwhich plays a natural and important role in the theory of risk measures and\ncapital requirements. So far, this notion has been investigated in the setting\nof some special spaces of random variables. In this paper we develop a theory\nof surplus invariance in its natural framework, namely that of vector lattices.\nBesides providing a unifying perspective on the existing literature, we\nestablish a variety of new results including dual representations and\nextensions of surplus-invariant risk measures and structural results for\nsurplus-invariant acceptance sets. We illustrate the power of the lattice\napproach by specifying our results to model spaces with a dominating\nprobability, including Orlicz spaces, as well as to robust model spaces without\na dominating probability, where the standard topological techniques and\nexhaustion arguments cannot be applied.\n"
    },
    {
        "paper_id": 1707.04981,
        "authors": "Yu-Jui Huang and Zhou Zhou",
        "title": "The Optimal Equilibrium for Time-Inconsistent Stopping Problems -- the\n  Discrete-Time Case",
        "comments": null,
        "journal-ref": "SIAM Journal on Control and Optimization, Vol. 57 (2019), No. 1,\n  pp 590-609",
        "doi": "10.1137/17M1139187",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We study an infinite-horizon discrete-time optimal stopping problem under\nnon-exponential discounting. A new method, which we call the iterative\napproach, is developed to find subgame perfect Nash equilibria. When the\ndiscount function induces decreasing impatience, we establish the existence of\nan equilibrium through fixed-point iterations. Moreover, we show that there\nexists a unique optimal equilibrium, which generates larger value than any\nother equilibrium does at all times. To the best of our knowledge, this is the\nfirst time a dominating subgame perfect Nash equilibrium is shown to exist in\nthe literature of time-inconsistency.\n"
    },
    {
        "paper_id": 1707.05061,
        "authors": "Caroline Hillairet (ENSAE ParisTech), Ying Jiao (SAF), Anthony\n  R\\'eveillac (INSA Toulouse, IMT)",
        "title": "Pricing formulae for derivatives in insurance using the Malliavin\n  calculus",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper we provide a valuation formula for different classes of\nactuarial and financial contracts which depend on a general loss process, by\nusing the Malliavin calculus. In analogy with the celebrated Black-Scholes\nformula, we aim at expressing the expected cash flow in terms of a building\nblock. The former is related to the loss process which is a cumulated sum\nindexed by a doubly stochastic Poisson process of claims allowed to be\ndependent on the intensity and the jump times of the counting process. For\nexample, in the context of Stop-Loss contracts the building block is given by\nthe distribution function of the terminal cumulated loss, taken at the Value at\nRisk when computing the Expected Shortfall risk measure.\n"
    },
    {
        "paper_id": 1707.05096,
        "authors": "Michail Anthropelos, Constantinos Kardaras, Georgios Vichos",
        "title": "Effective risk aversion in thin risk-sharing markets",
        "comments": "28 pages, second revised version",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We consider thin incomplete financial markets, where traders with\nheterogeneous preferences and risk exposures have motive to behave\nstrategically regarding the demand schedules they submit, thereby impacting\nprices and allocations. We argue that traders relatively more exposed to market\nrisk tend to submit more elastic demand functions. Noncompetitive equilibrium\nprices and allocations result as an outcome of a game among traders. General\nsufficient conditions for existence and uniqueness of such equilibrium are\nprovided, with an extensive analysis of two-trader transactions. Even though\nstrategic behaviour causes inefficient social allocations, traders with\nsufficiently high risk tolerance and/or large initial exposure to market risk\nobtain more utility gain in the noncompetitive equilibrium, when compared to\nthe competitive one.\n"
    },
    {
        "paper_id": 1707.05108,
        "authors": "Andrew J. Patton and Johanna F. Ziegel and Rui Chen",
        "title": "Dynamic Semiparametric Models for Expected Shortfall (and Value-at-Risk)",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Expected Shortfall (ES) is the average return on a risky asset conditional on\nthe return being below some quantile of its distribution, namely its\nValue-at-Risk (VaR). The Basel III Accord, which will be implemented in the\nyears leading up to 2019, places new attention on ES, but unlike VaR, there is\nlittle existing work on modeling ES. We use recent results from statistical\ndecision theory to overcome the problem of \"elicitability\" for ES by jointly\nmodelling ES and VaR, and propose new dynamic models for these risk measures.\nWe provide estimation and inference methods for the proposed models, and\nconfirm via simulation studies that the methods have good finite-sample\nproperties. We apply these models to daily returns on four international equity\nindices, and find the proposed new ES-VaR models outperform forecasts based on\nGARCH or rolling window models.\n"
    },
    {
        "paper_id": 1707.05146,
        "authors": "Emanuele Pugliese, Giulio Cimini, Aurelio Patelli, Andrea Zaccaria,\n  Luciano Pietronero, Andrea Gabrielli",
        "title": "Unfolding the innovation system for the development of countries:\n  co-evolution of Science, Technology and Production",
        "comments": null,
        "journal-ref": "Scientific Reports 9, 16440 (2019)",
        "doi": "10.1038/s41598-019-52767-5",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We show that the space in which scientific, technological and economic\ndevelopments interplay with each other can be mathematically shaped using\npioneering multilayer network and complexity techniques. We build the\ntri-layered network of human activities (scientific production, patenting, and\nindustrial production) and study the interactions among them, also taking into\naccount the possible time delays. Within this construction we can identify\nwhich capabilities and prerequisites are needed to be competitive in a given\nactivity, and even measure how much time is needed to transform, for instance,\nthe technological know-how into economic wealth and scientific innovation,\nbeing able to make predictions with a very long time horizon. Quite\nunexpectedly, we find empirical evidence that the naive knowledge flow from\nscience, to patents, to products is not supported by data, being instead\ntechnology the best predictor for industrial and scientific production for the\nnext decades.\n"
    },
    {
        "paper_id": 1707.05234,
        "authors": "Dorival Le\\~ao, Alberto Ohashi and Francesco Russo",
        "title": "Discrete-type approximations for non-Markovian optimal stopping\n  problems: Part I",
        "comments": "Final version to appear in Journal of Applied Probability",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper, we present a discrete-type approximation scheme to solve\ncontinuous-time optimal stopping problems based on fully non-Markovian\ncontinuous processes adapted to the Brownian motion filtration. The\napproximations satisfy suitable variational inequalities which allow us to\nconstruct $\\epsilon$-optimal stopping times and optimal values in full\ngenerality. Explicit rates of convergence are presented for optimal values\nbased on reward functionals of path-dependent SDEs driven by fractional\nBrownian motion. In particular, the methodology allows us to design concrete\nMonte-Carlo schemes for non-Markovian optimal stopping time problems as\ndemonstrated in the companion paper by Bezerra, Ohashi and Russo.\n"
    },
    {
        "paper_id": 1707.0525,
        "authors": "S\\'ergio C. Bezerra, Alberto Ohashi, Francesco Russo and Francys de\n  Souza",
        "title": "Discrete-type approximations for non-Markovian optimal stopping\n  problems: Part II",
        "comments": "Version to appear in Methodology & Computing in Applied Probability",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper, we present a Longstaff-Schwartz-type algorithm for optimal\nstopping time problems based on the Brownian motion filtration. The algorithm\nis based on Le\\~ao, Ohashi and Russo and, in contrast to previous works, our\nmethodology applies to optimal stopping problems for fully non-Markovian and\nnon-semimartingale state processes such as functionals of path-dependent\nstochastic differential equations and fractional Brownian motions. Based on\nstatistical learning theory techniques, we provide overall error estimates in\nterms of concrete approximation architecture spaces with finite\nVapnik-Chervonenkis dimension. Analytical properties of continuation values for\npath-dependent SDEs and concrete linear architecture approximating spaces are\nalso discussed.\n"
    },
    {
        "paper_id": 1707.05253,
        "authors": "Jun Maeda and Saul D. Jacka",
        "title": "An Optimal Stopping Problem Modeling Technical Analysis",
        "comments": "16 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We present a solution to an optimal stopping problem for a process with a\nwide-class of novel dynamics. The dynamics model the support/resistance line\nconcept from financial technical analysis.\n"
    },
    {
        "paper_id": 1707.05419,
        "authors": "Nguyen Tien Zung",
        "title": "Second order stochastic differential models for financial markets",
        "comments": "21 pages, 6 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Using agent-based modelling, empirical evidence and physical ideas, such as\nthe energy function and the fact that the phase space must have twice the\ndimension of the configuration space, we argue that the stochastic differential\nequations which describe the motion of financial prices with respect to real\nworld probability measures should be of second order (and non-Markovian),\ninstead of first order models \\`a la Bachelier--Samuelson. Our theoretical\nresult in stochastic dynamical systems shows that one cannot correctly reduce\nsecond order models to first order models by simply forgetting about momenta.\nWe propose some simple second order models, including a stochastic constrained\nn-oscillator, which can explain many market phenomena, such as boom-bust\ncycles, stochastic quasi-periodic behavior, and \"hot money\" going from one\nmarket sector to another.\n"
    },
    {
        "paper_id": 1707.05508,
        "authors": "Kinjal Banerjee, Chandradew Sharma, N. Bittu",
        "title": "Plunges in the Bombay stock exchange: Characteristics and indicators",
        "comments": null,
        "journal-ref": "International Journal of Modern Physics B Vol. 31 (2017) 1750160",
        "doi": "10.1142/S0217979217501600",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We study the various sectors of the Bombay Stock Exchange (BSE) for a period\nof eight years from January 2006 to March 2014. Using the data of the daily\nreturns of a period of eight years we investigate the financial cross\ncorrelation co-efficients among the sectors of BSE and Price by Earning (PE)\nratio of BSE Sensex. We show that the behavior of these quantities during\nnormal periods and during crisis is very different. We show that the PE ratio\nshows a particular distinctive trend in the approach to a crash of the\nfinancial market and can therefore be used as an indicator of an impending\ncatastrophe. We propose that a model of analysis of crashes in a financial\nmarket can be built using two parameters: (i) the PE ratio and (ii) the largest\neigenvalue of the cross correlation matrix.\n"
    },
    {
        "paper_id": 1707.0555,
        "authors": "T. Zhang (1), G.-F. Gu (1), H.-C. Xu (1), X. Xiong (2), W. Chen (3)\n  and W.-X. Zhou (1) ((1) ECUST (2) TJU (3) SZSE)",
        "title": "Power-law tails in the distribution of order imbalance",
        "comments": null,
        "journal-ref": "Physica A 483, 201-208 (2017)",
        "doi": "10.1016/j.physa.2017.04.065",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We investigate the probability distribution of order imbalance calculated\nfrom the order flow data of 43 Chinese stocks traded on the Shenzhen Stock\nExchange. Two definitions of order imbalance are considered based on the order\nnumber and the order size. We find that the order imbalance distributions of\nindividual stocks have power-law tails. However, the tail index fluctuates\nremarkably from stock to stock. We also investigate the distributions of\naggregated order imbalance of all stocks at different timescales $\\Delta{t}$.\nWe find no clear trend in the tail index with respect $\\Delta{t}$. All the\nanalyses suggest that the distributions of order imbalance are asymmetric.\n"
    },
    {
        "paper_id": 1707.05552,
        "authors": "H.-L. Shi and W.-X. Zhou",
        "title": "Wax and wane of the cross-sectional momentum and contrarian effects:\n  Evidence from the Chinese stock markets",
        "comments": null,
        "journal-ref": "Physica A 486, 397-407 (2017)",
        "doi": "10.1016/j.physa.2017.05.078",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper investigates the time-varying risk-premium relation of the Chinese\nstock markets within the framework of cross-sectional momentum and contrarian\neffects by adopting the Capital Asset Pricing Model and the French-Fama three\nfactor model. The evolving arbitrage opportunities are also studied by\nquantifying the performance of time-varying cross-sectional momentum and\ncontrarian effects in the Chinese stock markets. The relation between the\ncontrarian profitability and market condition factors that could characterize\nthe investment context is also investigated. The results reveal that the\nrisk-premium relation varies over time, and the arbitrage opportunities based\non the contrarian portfolios wax and wane over time. The performance of\ncontrarian portfolios are highly dependent on several market conditions. The\nperiods with upward trend of market state, higher market volatility and\nliquidity, lower macroeconomics uncertainty are related to higher contrarian\nprofitability. These findings are consistent with the Adaptive Markets\nHypothesis and have practical implications for market participants.\n"
    },
    {
        "paper_id": 1707.0558,
        "authors": "Tobias Braun, Jonas A. Fiegen, Daniel C. Wagner, Sebastian M. Krause,\n  Thomas Guhr",
        "title": "Impact and Recovery Process of Mini Flash Crashes: An Empirical Study",
        "comments": null,
        "journal-ref": null,
        "doi": "10.1371/journal.pone.0196920",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In an Ultrafast Extreme Event (or Mini Flash Crash), the price of a traded\nstock increases or decreases strongly within milliseconds. We present a\ndetailed study of Ultrafast Extreme Events in stock market data. In contrast to\npopular belief, our analysis suggests that most of the Ultrafast Extreme Events\nare not primarily due to High Frequency Trading. In at least 60 percent of the\nobserved Ultrafast Extreme Events, the main cause for the events are large\nmarket orders. In times of financial crisis, large market orders are more\nlikely which can be linked to the significant increase of Ultrafast Extreme\nEvents occurrences. Furthermore, we analyze the 100 trades following each\nUltrafast Extreme Events. While we observe a tendency of the prices to\npartially recover, less than 40 percent recover completely. On the other hand\nwe find 25 percent of the Ultrafast Extreme Events to be almost recovered after\nonly one trade which differs from the usually found price impact of market\norders.\n"
    },
    {
        "paper_id": 1707.05596,
        "authors": "Xue Dong He, Xianhua Peng",
        "title": "Surplus-Invariant, Law-Invariant, and Conic Acceptance Sets Must be the\n  Sets Induced by Value-at-Risk",
        "comments": "20 pages, 0 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The regulator is interested in proposing a capital adequacy test by\nspecifying an acceptance set for firms' capital positions at the end of a given\nperiod. This set needs to be surplus-invariant, i.e., not to depend on the\nsurplus of firms' shareholders, because the test means to protect firms'\nliability holders. We prove that any surplus-invariant, law-invariant, and\nconic acceptance set must be the set of capital positions whose value-at-risk\nat a given level is less than zero. The result still holds if we replace\nconicity with numeraire-invariance, a property stipulating that whether a firm\npasses the test should not depend on the currency used to denominate its\nassets.\n"
    },
    {
        "paper_id": 1707.05604,
        "authors": "Peng Yue (ECUST), Hai-Chuan Xu (ECUST), Wei Chen (SSEC), Xiong Xiong\n  (TJU), Wei-Xing Zhou (ECUST)",
        "title": "Linear and nonlinear correlations in order aggressiveness of Chinese\n  stocks",
        "comments": "8 RevTex pages including 4 figures and 1 table",
        "journal-ref": "Fractals 25 (5), 1750041 (2017)",
        "doi": "10.1142/S0218348X17500414",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The diagonal effect of orders is well documented in different markets, which\nstates that orders are more likely to be followed by orders of the same\naggressiveness and implies the presence of short-term correlations in order\nflows. Based on the order flow data of 43 Chinese stocks, we investigate if\nthere are long-range correlations in the time series of order aggressiveness.\nThe detrending moving average analysis shows that there are crossovers in the\nscaling behaviors of overall fluctuations and order aggressiveness exhibits\nlinear long-term correlations. We design an objective procedure to determine\nthe two Hurst indexes delimited by the crossover scale. We find no correlations\nin the short term and strong correlations in the long term for all stocks\nexcept for an outlier stock. The long-term correlation is found to depend on\nseveral firm specific characteristics. We also find that there are nonlinear\nlong-term correlations in the order aggressiveness when we perform the\nmultifractal detrending moving average analysis.\n"
    },
    {
        "paper_id": 1707.05642,
        "authors": "Matthew F Dixon",
        "title": "Sequence Classification of the Limit Order Book using Recurrent Neural\n  Networks",
        "comments": "arXiv admin note: text overlap with arXiv:1705.09851",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Recurrent neural networks (RNNs) are types of artificial neural networks\n(ANNs) that are well suited to forecasting and sequence classification. They\nhave been applied extensively to forecasting univariate financial time series,\nhowever their application to high frequency trading has not been previously\nconsidered. This paper solves a sequence classification problem in which a\nshort sequence of observations of limit order book depths and market orders is\nused to predict a next event price-flip. The capability to adjust quotes\naccording to this prediction reduces the likelihood of adverse price selection.\nOur results demonstrate the ability of the RNN to capture the non-linear\nrelationship between the near-term price-flips and a spatio-temporal\nrepresentation of the limit order book. The RNN compares favorably with other\nclassifiers, including a linear Kalman filter, using S&P500 E-mini futures\nlevel II data over the month of August 2016. Further results assess the effect\nof retraining the RNN daily and the sensitivity of the performance to trade\nlatency.\n"
    },
    {
        "paper_id": 1707.05699,
        "authors": "Jean-Pascal Bassino, Pablo Jensen and Matteo Morini",
        "title": "Network analysis of Japanese global business using quasi-exhaustive\n  micro-data for Japanese overseas subsidiaries",
        "comments": "19 pages, preliminary version",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Network analysis techniques remain rarely used for understanding\ninternational management strategies. Our paper highlights their value as\nresearch tool in this field of social science using a large set of micro-data\n(20,000) to investigate the presence of networks of subsidiaries overseas. The\nresearch question is the following: to what extent did/do global Japanese\nbusiness networks mirror organizational models existing in Japan? In\nparticular, we would like to assess how much the links building such business\nnetworks are shaped by the structure of big-size industrial conglomerates of\nfirms headquartered in Japan, also described as HK. The major part of the\nacademic community in the fields of management and industrial organization\nconsiders that formal links can be identified among firms belonging to HK. Miwa\nand Ramseyer (Miwa and Ramseyer 2002; Ramseyer 2006) challenge this claim and\nargue that the evidence supporting the existence of HK is weak. So far,\nquantitative empirical investigation has been conducted exclusively using data\nfor firms incorporated in Japan. Our study tests the Miwa-Ramseyer hypothesis\n(MRH) at the global level using information on the network of Japanese\nsubsidiaries overseas. The results obtained lead us to reject the MRH for the\nglobal dataset, as well as for subsets restricted to the two main\nregions/countries of destination of Japanese foreign investment. The results\nare robust to the weighting of the links, with different specifications, and\nare observed in most industrial sectors. The global Japanese network became\nincreasingly complex during the late 20th century as a consequence of increase\nin the number of Japanese subsidiaries overseas but the key features of the\nstructure remained rather stable. We draw implications of these findings for\nacademic research in international business and for professionals involved in\ncorporate strategy.\n"
    },
    {
        "paper_id": 1707.05778,
        "authors": "Andr\\'es Garc\\'ia-Medina, Leonidas Sandoval Junior, Efra\\'in Urrutia\n  Ba\\~nuelos and A. M. Mart\\'inez-Arg\\\"uello",
        "title": "Correlations and Flow of Information between The New York Times and\n  Stock Markets",
        "comments": "18 pages, 14 figures",
        "journal-ref": null,
        "doi": "10.1016/j.physa.2018.02.154",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We use Random Matrix Theory (RMT) and information theory to analyze the\ncorrelations and flow of information between 64,939 news from The New York\nTimes and 40 world financial indices during 10 months along the period\n2015-2016. The set of news was quantified and transformed into daily polarity\ntime series using tools from sentiment analysis. Results from RMT shows that a\ncommon factor lead the world indices and news, and even share the same\ndynamics. Furthermore, the global correlation structure has found preserved\nwhen adding white noise, which indicate that correlations are not due to sample\nsize effects. Likewise, we found a lot of information flowing from news to\nworld indices for specific delay, being of practical interest for trading\npurpose. Our results suggest a deep relationship between news and world\nindices, and show a situation where news drive world market movements, giving a\nnew evidence to support behavioral finance as the current economic paradigm.\n"
    },
    {
        "paper_id": 1707.05826,
        "authors": "Saleh Albeaik, Mary Kaltenberg, Mansour Alsaleh, Cesar A. Hidalgo",
        "title": "Improving the Economic Complexity Index",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  How much knowledge is there in an economy? In recent years, data on the mix\nof products that countries export has been used to construct measures of\neconomic complexity that estimate the knowledge available in an economy and\npredict future economic growth. Here we introduce a new and simpler metric of\neconomic complexity (ECI+) that measures the total exports of an economy\ncorrected by how difficult it is to export each product. We use data from 1973\nto 2013 to compare the ability of ECI+, the Economic Complexity Index (ECI),\nand Fitness complexity, to predict future economic growth using 5, 10, and\n20-year panels in a pooled OLS, a random effects model, and a fixed effects\nmodel. We find that ECI+ outperforms ECI and Fitness in its ability to predict\neconomic growth and in the consistency of its estimators across most\neconometric specifications. On average, one standard deviation increase in ECI+\nis associated with an increase in annualized growth of about 4% to 5%. We then\ncombine ECI+ with measures of physical capital, human capital, and\ninstitutions, to find a robust model of economic growth. The ability of ECI+ to\npredict growth, and the value of its coefficient, is robust to these controls.\nAlso, we find that human capital, political stability, and control of\ncorruption; are positively associated with future economic growth, and that\nincome is negatively associated with growth, in agreement with the traditional\ngrowth literature. Finally, we use ECI+ to generate economic growth predictions\nfor the next 20 years and compare these predictions with the ones obtained\nusing ECI and Fitness. These findings improve the methods available to estimate\nthe knowledge intensity of economies and predict future economic growth.\n"
    },
    {
        "paper_id": 1707.05914,
        "authors": "Charles D. Brummitt, Kenan Huremovic, Paolo Pin, Matthew H. Bonds,\n  Fernando Vega-Redondo",
        "title": "Contagious disruptions and complexity traps in economic development",
        "comments": "Main text: 10 pages, 6 figures. Supplementary Information: 15 pages,\n  1 figure",
        "journal-ref": null,
        "doi": "10.1038/s41562-017-0190-6",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Poor economies not only produce less; they typically produce things that\ninvolve fewer inputs and fewer intermediate steps. Yet the supply chains of\npoor countries face more frequent disruptions---delivery failures, faulty\nparts, delays, power outages, theft, government failures---that systematically\nthwart the production process. To understand how these disruptions affect\neconomic development, we model an evolving input--output network in which\ndisruptions spread contagiously among optimizing agents. The key finding is\nthat a poverty trap can emerge: agents adapt to frequent disruptions by\nproducing simpler, less valuable goods, yet disruptions persist. Growing out of\npoverty requires that agents invest in buffers to disruptions. These buffers\nrise and then fall as the economy produces more complex goods, a prediction\nconsistent with global patterns of input inventories. Large jumps in economic\ncomplexity can backfire. This result suggests why \"big push\" policies can fail,\nand it underscores the importance of reliability and of gradual increases in\ntechnological complexity.\n"
    },
    {
        "paper_id": 1707.06138,
        "authors": "Jerome Detemple and Yerkin Kitapbayev",
        "title": "American Options with Discontinuous Two-Level Caps",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper examines the valuation of American capped call options with\ntwo-level caps. The structure of the immediate exercise region is significantly\nmore complex than in the classical case with constant cap. When the cap grows\nover time, making extensive use of probabilistic arguments and local time, we\nshow that the exercise region can be the union of two disconnected set.\nAlternatively, it can consist of two sets connected by a line. The problem then\nreduces to the characterization of the upper boundary of the first set, which\nis shown to satisfy a recursive integral equation. When the cap decreases over\ntime, the boundary of the exercise region has piecewise constant segments\nalternating with non-increasing segments. General representation formulas for\nthe option price, involving the exercise boundaries and the local time of the\nunderlying price process, are derived. An efficient algorithm is developed and\nnumerical results are provided.\n"
    },
    {
        "paper_id": 1707.06635,
        "authors": "ShiXue He and Marcel Ausloos",
        "title": "Impact of the Global Crisis on SME Internal vs. External Financing in\n  China",
        "comments": "17 pages, 6 tables, 43 references",
        "journal-ref": "Banking and Finance Review, 9(1) (2017) 1-17",
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Changes in the capital structure before and after the global financial crisis\nfor SMEs are studied, emphasizing their financing problems, distinguishing\nbetween internal financing and external financing determinants. The empirical\nresearch bears upon 158 small and medium-sized firms listed on Shenzhen and\nShanghai Stock Exchanges in China over the period of 2004-2014. A regression\nanalysis, along the lines of the Trade-Off Theory, shows that the leverage\ndecreases with profitability, non-debt tax shields and the liquidity, and\nincreases with firm size and tangibility. A positive relationship is found\nbetween firm growth and debt ratio, though not highly significantly. It is\nshown that the SMEs with high growth rates are those which will more easily\nobtain external financing after a financial crisis. It is recognized that the\nChina government should reconsider SMEs taxation laws.\n"
    },
    {
        "paper_id": 1707.06829,
        "authors": "Oleg Malafeyev, Konstantin Farvazov, Olga Zenovich",
        "title": "Geopolitical Model of Investment Project Implementation",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Two geopolitical actors implement a geopolitical project that involves\ntransportaion and storage of some commodities. They interact with each other\nthrough a transport network. The network consists of several interconnected\nvertices. Some of the vetrices are trading hubs, storage spaces, production\nhubs and goods buyers. Actors wish to satify the demand of buyers and recieve\nthe highest possible profit subject to compromise solution principle. A\nnumerical example is given.\n"
    },
    {
        "paper_id": 1707.06837,
        "authors": "Mikio Ito, Akihiko Noda, Tatsuma Wada",
        "title": "An Alternative Estimation Method of a Time-Varying Parameter Model",
        "comments": "35 pages, 6 tables, 4 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  A non-Bayesian, regression-based or generalized least squares (GLS)-based\napproach is formally proposed to estimate a class of time-varying AR parameter\nmodels. This approach has partly been used by Ito et al. (2014, 2016a,b), and\nis proven to be efficient because, unlike conventional methods, it does not\nrequire Kalman filtering and smoothing procedures, but yields a smoothed\nestimate that is identical to the Kalman-smoothed estimate. Unlike the maximum\nlikelihood estimator, the possibility of the pile-up problem is negligible. In\naddition, this approach enables us to deal with stochastic volatility models,\nmodels with a time-dependent variance-covariance matrix, and models with\nnon-Gaussian errors that allow us to deal with abrupt changes or structural\nbreaks in time-varying parameters.\n"
    },
    {
        "paper_id": 1707.06849,
        "authors": "Damir Filipovi\\'c, Martin Larsson, Sergio Pulido",
        "title": "Markov cubature rules for polynomial processes",
        "comments": "29 pages, 6 Figures, 2 Tables; forthcoming in Stochastic Processes\n  and their Applications",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We study discretizations of polynomial processes using finite state Markov\nprocesses satisfying suitable moment matching conditions. The states of these\nMarkov processes together with their transition probabilities can be\ninterpreted as Markov cubature rules. The polynomial property allows us to\nstudy such rules using algebraic techniques. Markov cubature rules aid the\ntractability of path-dependent tasks such as American option pricing in models\nwhere the underlying factors are polynomial processes.\n"
    },
    {
        "paper_id": 1707.0697,
        "authors": "Maxime Morariu-Patrichi, Mikko S. Pakkanen",
        "title": "Hybrid marked point processes: characterisation, existence and\n  uniqueness",
        "comments": "v6: introduction updated with reference to application of\n  state-dependent Hawkes processes",
        "journal-ref": "Market Microstructure and Liquidity 2018, Vol. 4, No. 3&4, 1950007\n  (55 pp.)",
        "doi": "10.1142/S2382626619500072",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We introduce a class of hybrid marked point processes, which encompasses and\nextends continuous-time Markov chains and Hawkes processes. While this flexible\nclass amalgamates such existing processes, it also contains novel processes\nwith complex dynamics. These processes are defined implicitly via their\nintensity and are endowed with a state process that interacts with\npast-dependent events. The key example we entertain is an extension of a Hawkes\nprocess, a state-dependent Hawkes process interacting with its state process.\nWe show the existence and uniqueness of hybrid marked point processes under\ngeneral assumptions, extending the results of Massouli\\'e (1998) on interacting\npoint processes.\n"
    },
    {
        "paper_id": 1707.07162,
        "authors": "Guilherme Demos and Didier Sornette",
        "title": "Lagrange regularisation approach to compare nested data sets and\n  determine objectively financial bubbles' inceptions",
        "comments": "14 pages, 6 Figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Inspired by the question of identifying the start time $\\tau$ of financial\nbubbles, we address the calibration of time series in which the inception of\nthe latest regime of interest is unknown. By taking into account the tendency\nof a given model to overfit data, we introduce the Lagrange regularisation of\nthe normalised sum of the squared residuals, $\\chi^{2}_{np}(\\Phi)$, to\nendogenously detect the optimal fitting window size := $w^* \\in\n[\\tau:\\bar{t}_2]$ that should be used for calibration purposes for a fixed\npseudo present time $\\bar{t}_2$. The performance of the Lagrange regularisation\nof $\\chi^{2}_{np}(\\Phi)$ defined as $\\chi^{2}_{\\lambda (\\Phi)}$ is exemplified\non a simple Linear Regression problem with a change point and compared against\nthe Residual Sum of Squares (RSS) := $\\chi^{2}(\\Phi)$ and RSS/(N-p):=\n$\\chi^{2}_{np}(\\Phi)$, where $N$ is the sample size and p is the number of\ndegrees of freedom. Applied to synthetic models of financial bubbles with a\nwell-defined transition regime and to a number of financial time series (US\nS\\&P500, Brazil IBovespa and China SSEC Indices), the Lagrange regularisation\nof $\\chi^{2}_{\\lambda}(\\Phi)$ is found to provide well-defined reasonable\ndeterminations of the starting times for major bubbles such as the bubbles\nending with the 1987 Black-Monday, the 2008 Sub-prime crisis and minor\nspeculative bubbles on other Indexes, without any further exogenous\ninformation. It thus allows one to endogenise the determination of the\nbeginning time of bubbles, a problem that had not received previously a\nsystematic objective solution.\n"
    },
    {
        "paper_id": 1707.07284,
        "authors": "Pavol Brunovsk\\'y, Ale\\v{s} \\v{C}ern\\'y, J\\'an Komadel",
        "title": "Optimal Trade Execution Under Endogenous Pressure to Liquidate: Theory\n  and Numerical Solutions",
        "comments": null,
        "journal-ref": "European Journal of Operational Research, 264(3), 1159-1171, 2018",
        "doi": "10.1016/j.ejor.2017.07.054",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We study optimal liquidation of a trading position (so-called block order or\nmeta-order) in a market with a linear temporary price impact (Kyle, 1985). We\nendogenize the pressure to liquidate by introducing a downward drift in the\nunaffected asset price while simultaneously ruling out short sales. In this\nsetting the liquidation time horizon becomes a stopping time determined\nendogenously, as part of the optimal strategy. We find that the optimal\nliquidation strategy is consistent with the square-root law which states that\nthe average price impact per share is proportional to the square root of the\nsize of the meta-order (Bershova and Rakhlin, 2013; Farmer et al., 2013; Donier\net al., 2015; T\\'oth et al., 2016).\n  Mathematically, the Hamilton-Jacobi-Bellman equation of our optimization\nleads to a severely singular and numerically unstable ordinary differential\nequation initial value problem. We provide careful analysis of related singular\nmixed boundary value problems and devise a numerically stable computation\nstrategy by re-introducing time dimension into an otherwise time-homogeneous\ntask.\n"
    },
    {
        "paper_id": 1707.07322,
        "authors": "Mohammed Berkhouch, Ghizlane Lakhnati, Marcelo Brutti Righi",
        "title": "Extended Gini-type measures of risk and variability",
        "comments": "This is a working paper consisting of 20 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The aim of this paper is to introduce a risk measure that extends the\nGini-type measures of risk and variability, the Extended Gini Shortfall, by\ntaking risk aversion into consideration. Our risk measure is coherent and\ncatches variability, an important concept for risk management. The analysis is\nmade under the Choquet integral representations framework. We expose results\nfor analytic computation under well-known distribution functions. Furthermore,\nwe provide a practical application.\n"
    },
    {
        "paper_id": 1707.07338,
        "authors": "David W. Lu",
        "title": "Agent Inspired Trading Using Recurrent Reinforcement Learning and LSTM\n  Neural Networks",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  With the breakthrough of computational power and deep neural networks, many\nareas that we haven't explore with various techniques that was researched\nrigorously in past is feasible. In this paper, we will walk through possible\nconcepts to achieve robo-like trading or advising. In order to accomplish\nsimilar level of performance and generality, like a human trader, our agents\nlearn for themselves to create successful strategies that lead to the\nhuman-level long-term rewards. The learning model is implemented in Long Short\nTerm Memory (LSTM) recurrent structures with Reinforcement Learning or\nEvolution Strategies acting as agents The robustness and feasibility of the\nsystem is verified on GBPUSD trading.\n"
    },
    {
        "paper_id": 1707.07585,
        "authors": "Zeya Zhang, Weizheng Chen and Hongfei Yan",
        "title": "Stock Prediction: a method based on extraction of news features and\n  recurrent neural networks",
        "comments": "in Chinese, The 22nd China Conference on Information Retrieval",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper proposed a method for stock prediction. In terms of feature\nextraction, we extract the features of stock-related news besides stock prices.\nWe first select some seed words based on experience which are the symbols of\ngood news and bad news. Then we propose an optimization method and calculate\nthe positive polar of all words. After that, we construct the features of news\nbased on the positive polar of their words. In consideration of sequential\nstock prices and continuous news effects, we propose a recurrent neural network\nmodel to help predict stock prices. Compared to SVM classifier with price\nfeatures, we find our proposed method has an over 5% improvement on stock\nprediction accuracy in experiments.\n"
    },
    {
        "paper_id": 1707.07618,
        "authors": "Tetsuya Takaishi",
        "title": "Statistical properties and multifractality of Bitcoin",
        "comments": "19 pages, 9 figures",
        "journal-ref": "Physica A 506 (2018) 507-519",
        "doi": "10.1016/j.physa.2018.04.046",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Using 1-min returns of Bitcoin prices, we investigate statistical properties\nand multifractality of a Bitcoin time series. We find that the 1-min return\ndistribution is fat-tailed, and kurtosis largely deviates from the Gaussian\nexpectation. Although for large sampling periods, kurtosis is anticipated to\napproach the Gaussian expectation, we find that convergence to that is very\nslow. Skewness is found to be negative at time scales shorter than one day and\nbecomes consistent with zero at time scales longer than about one week. We also\ninvestigate daily volatility-asymmetry by using GARCH, GJR, and RGARCH models,\nand find no evidence of it. On exploring multifractality using multifractal\ndetrended fluctuation analysis, we find that the Bitcoin time series exhibits\nmultifractality. The sources of multifractality are investigated, confirming\nthat both temporal correlation and the fat-tailed distribution contribute to\nit. The influence of \"Brexit\" on June 23, 2016 to GBP--USD exchange rate and\nBitcoin is examined in multifractal properties. We find that, while Brexit\ninfluenced the GBP--USD exchange rate, Bitcoin was robust to Brexit.\n"
    },
    {
        "paper_id": 1707.07797,
        "authors": "Mingsi Long and Hongzhong Zhang",
        "title": "On the optimality of threshold type strategies in single and recursive\n  optimal stopping under L\\'evy models",
        "comments": "29 pages, 4 figures",
        "journal-ref": null,
        "doi": "10.1016/j.spa.2018.08.005",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In the spirit of [Surya07'], we develop an average problem approach to prove\nthe optimality of threshold type strategies for optimal stopping of L\\'evy\nmodels with a continuous additive functional (CAF) discounting. Under\nspectrally negative models, we specialize this in terms of conditions on the\nreward function and random discounting, where we present two examples of local\ntime and occupation time discounting. We then apply this approach to recursive\noptimal stopping problems, and present simpler and neater proofs for a number\nof important results on qualitative properties of the optimal thresholds, which\nare only known under a few special cases.\n"
    },
    {
        "paper_id": 1707.07977,
        "authors": "Jamal Bouoiyour (1), Refk Selmi (1) ((1) CATT)",
        "title": "Ether: Bitcoin's competitor or ally?",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Although Bitcoin has long been dominant in the crypto scene, it is certainly\nnot alone. Ether is another cryptocurrency related project that has attracted\nan intensive attention because of its additional features. This study seeks to\ntest whether these cryptocurrencies differ in terms of their volatile and\nspeculative behaviors, hedge, safe haven and risk diversification properties.\nUsing different econometric techniques, we show that a) Bitcoin and Ether are\nvolatile and relatively more responsive to bad news, but the volatility of\nEther is more persistent than that of Bitcoin; b) for both cryptocurrencies,\nthe exuberance and the collapse of bubbles were identified, but Bitcoin appears\nmore speculative than Ether; c) there is negative and significant correlation\nbetween Bitcoin/Ether and other assets (S\\&P500 stocks, US bonds, oil), which\nwould indicate that digital currencies can hedge against the price movements of\nthese assets; d) there is negative tail independence between Bitcoin/Ether and\nother financial assets, implying that these cryptocurrencies exhibit the\nfunction of a weak safe haven; and e) The inclusion of Bitcoin/ Ether in a\nportfolio improve its efficiency in terms of higher reward-to-risk ratios. But\ninvestors who hold diversified portfolios made of stocks or bonds and Ether may\nface losses over bearish regime. In such situation, stock and bond investors\nmay take a short position on Bitcoin.\n"
    },
    {
        "paper_id": 1707.08078,
        "authors": "Brian P. Hanley",
        "title": "Equity Default Clawback Swaps to Implement Venture Banking",
        "comments": "34 pages, 18 figures, 3 equations. This version adds citation of BoE\n  clarifying how loans create deposits rather than the reverse; EDCS\n  terminology, miscellaneous clarifications. Preamble added",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  In this theoretical paper, I propose creation of a venture bank, able to\nmultiply the capital of a venture capital firm by at least 47 times, without\nrequiring access to the Federal Reserve or other central bank apart from\nsettlement. This concept rests on obtaining default swap instruments on loans\nin order to create the capital required, and expand Tier 1 and 2 base capital.\nProfitability depends on overall portfolio performance, availability of equity\ndefault swaps, cost of default swap, and the multiple of original capital (MOC)\nadopted by the venture bank. A new derivative financial instrument, the equity\ndefault swap (EDS), to cover loans made as venture investments. An EDS is\nsimilar to a credit default swap (CDS) but with some unique features. The\nfeatures and operation of these new derivative instruments are outlined along\nwith audit requirements. This instrument would be traded on open-outcry\nexchanges with special features to ensure orderly operation of the market. It\nis the creation of public markets for EDSs that makes possible the use of\npublic market pricing to indirectly provide a potential market capitalization\nfor the underlying venture-bank investment. Full coverage insulates the\nventure-bank from losses in most situations, and multiplies profitability quite\ndramatically in all scenarios. Ten year returns above 20X are attainable.\nFurther, a new feature for EDS derivatives, a clawback lien, closes out the\nequity default swap. Here it is optimized at 77%, and is to be paid back to the\nunderwriter at a future date to prevent perverse incentive to deliberately\nfail. This new feature creates an Equity Default Clawback Swap (EDCS) which can\nbe used safely. This proposal also solves an old problem in banking, because it\nmatches the term of the loan with the term of the investment. I show that the\nventure-bank investment and the EDCS underwriting business are profitable.\n"
    },
    {
        "paper_id": 1707.08464,
        "authors": "Bruno Bouchard (CEREMADE), Masaaki Fukasawa, Martin Herdegen, Johannes\n  Muhle-Karbe",
        "title": "Equilibrium Returns with Transaction Costs",
        "comments": "Finance and Stochastics, Springer Verlag (Germany), In press",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We study how trading costs are reflected in equilibrium returns. To this end,\nwe develop a tractable continuous-time risk-sharing model, where heterogeneous\nmean-variance investors trade subject to a quadratic transaction cost. The\ncorresponding equilibrium is characterized as the unique solution of a system\nof coupled but linear forward-backward stochastic differential equations.\nExplicit solutions are obtained in a number of concrete settings. The\nsluggishness of the frictional portfolios makes the corresponding equilibrium\nreturns mean-reverting. Compared to the frictionless case, expected returns are\nhigher if the more risk-averse agents are net sellers or if the asset supply\nexpands over time.\n"
    },
    {
        "paper_id": 1707.08504,
        "authors": "Zura Kakushadze and Willie Yu",
        "title": "Mutation Clusters from Cancer Exome",
        "comments": "84 pages",
        "journal-ref": "Genes 8(8) (2017) 201",
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We apply our statistically deterministic machine learning/clustering\nalgorithm *K-means (recently developed in https://ssrn.com/abstract=2908286) to\n10,656 published exome samples for 32 cancer types. A majority of cancer types\nexhibit mutation clustering structure. Our results are in-sample stable. They\nare also out-of-sample stable when applied to 1,389 published genome samples\nacross 14 cancer types. In contrast, we find in- and out-of-sample\ninstabilities in cancer signatures extracted from exome samples via nonnegative\nmatrix factorization (NMF), a computationally costly and non-deterministic\nmethod. Extracting stable mutation structures from exome data could have\nimportant implications for speed and cost, which are critical for early-stage\ncancer diagnostics such as novel blood-test methods currently in development.\n"
    },
    {
        "paper_id": 1707.08545,
        "authors": "Sebastian Herrmann and Florian Stebegg",
        "title": "Robust Pricing and Hedging around the Globe",
        "comments": "forthcoming in Annals of Applied Probability",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We consider the martingale optimal transport duality for c\\`adl\\`ag processes\nwith given initial and terminal laws. Strong duality and existence of dual\noptimizers (robust semi-static superhedging strategies) are proved for a class\nof payoffs that includes American, Asian, Bermudan, and European options with\nintermediate maturity. We exhibit an optimal superhedging strategy for which\nthe static part solves an auxiliary problem and the dynamic part is given\nexplicitly in terms of the static part.\n"
    },
    {
        "paper_id": 1707.09037,
        "authors": "Thomas Sch\\\"urmann and Ingo Hoffmann",
        "title": "On Biased Correlation Estimation",
        "comments": "5 pages, 6 figures, working paper",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In general, underestimation of risk is something which should be avoided as\nfar as possible. Especially in financial asset management, equity risk is\ntypically characterized by the measure of portfolio variance, or indirectly by\nquantities which are derived from it. Since there is a linear dependency of the\nvariance and the empirical correlation between asset classes, one is compelled\nto control or to avoid the possibility of underestimating correlation\ncoefficients. In the present approach, we formalize common practice and\nclassify these approaches by computing their probability of underestimation. In\naddition, we introduce a new estimator which is characterized by having the\nadvantage of a constant and controllable probability of underestimation. We\nprove that the new estimator is statistically consistent.\n"
    },
    {
        "paper_id": 1707.09203,
        "authors": "Roberto De Luca, Marco Di Mauro, Angelo Falzarano, Adele Naddeo",
        "title": "A hydrodynamic model for cooperating solidary countries",
        "comments": "8 pages, 2 figures",
        "journal-ref": "Eur. Phys. J. B (2017) 90, 134",
        "doi": "10.1140/epjb/e2017-80126-4",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The goal of international trade theories is to explain the exchange of goods\nand services between different countries, aiming to benefit from it. Albeit the\nidea is very simple and known since ancient history, smart policy and business\nstrategies need to be implemented by each subject, resulting in a complex as\nwell as not obvious interplay. In order to understand such a complexity,\ndifferent theories have been developed since the sixteenth century and today\nnew ideas still continue to enter the game. Among them, the so called classical\ntheories are country-based and range from Absolute and Comparative Advantage\ntheories by A. Smith and D. Ricardo to Factor Proportions theory by E.\nHeckscher and B. Ohlin. In this work we build a simple hydrodynamic model, able\nto reproduce the main conclusions of Comparative Advantage theory in its\nsimplest setup, i.e. a two-country world with country A and country B\nexchanging two goods within a genuine exchange-based economy and a trade flow\nruled only by market forces. The model is further generalized by introducing\nmoney in order to discuss its role in shaping trade patterns. Advantages and\ndrawbacks of the model are also discussed together with perspectives for its\nimprovement.\n"
    },
    {
        "paper_id": 1707.09351,
        "authors": "Klebert Kentia and Christoph K\\\"uhn",
        "title": "Nash equilibria for game contingent claims with utility-based hedging",
        "comments": "20 pages",
        "journal-ref": "SIAM Journal on Control and Optimization, 2018, 56(6), 3948-3972",
        "doi": "10.1137/17M1141059",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Game contingent claims (GCCs) generalize American contingent claims by\nallowing the writer to recall the option as long as it is not exercised, at the\nprice of paying some penalty. In incomplete markets, an appealing approach is\nto analyze GCCs like their European and American counterparts by solving option\nholder's and writer's optimal investment problems in the underlying securities.\nBy this, partial hedging opportunities are taken into account. We extend\nresults in the literature by solving the stochastic game corresponding to GCCs\nwith both continuous time stopping and trading. Namely, we construct Nash\nequilibria by rewriting the game as a non-zero-sum stopping game in which\nplayers compare payoffs in terms of their exponential utility indifference\nvalues. As a by-product, we also obtain an existence result for the optimal\nexercise time of an American claim under utility indifference valuation by\nrelating it to the corresponding nonlinear Snell envelope.\n"
    },
    {
        "paper_id": 1707.09494,
        "authors": "Tiziano De Angelis and Gabriele Stabile",
        "title": "On the free boundary of an annuity purchase",
        "comments": "34 pages, 5 figures; improved exposition",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  It is known that the decision to purchase an annuity may be associated to an\noptimal stopping problem. However, little is known about optimal strategies, if\nthe mortality force is a generic function of time and if the `subjective' life\nexpectancy of the investor differs from the `objective' one adopted by\ninsurance companies to price annuities. In this paper we address this problem\nconsidering an individual who invests in a fund and has the option to convert\nthe fund's value into an annuity at any time. We formulate the problem as a\nreal option and perform a detailed probabilistic study of the optimal stopping\nboundary. Due to the generic time-dependence of the mortality force, our\noptimal stopping problem requires new solution methods to deal with\nnon-monotonic optimal boundaries.\n"
    },
    {
        "paper_id": 1707.09609,
        "authors": "Mahdi Doostparast",
        "title": "Explicit expressions for European option pricing under a generalized\n  skew normal distribution",
        "comments": "12 Pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Under a generalized skew normal distribution we consider the problem of\nEuropean option pricing. Existence of the martingale measure is proved. An\nexplicit expression for a given European option price is presented in terms of\nthe cumulative distribution function of the univariate skew normal and the\nbivariate standard normal distributions. Some special cases are investigated in\na greater detail. To carry out the sensitivity of the option price to the skew\nparameters, numerical methods are applied. Some concluding remarks and further\nworks are given. The results obtained are extensions of the results provided by\n[4].\n"
    },
    {
        "paper_id": 1707.09801,
        "authors": "Vygintas Gontis, Aleksejus Kononovicius",
        "title": "Spurious memory in non-equilibrium stochastic models of imitative\n  behavior",
        "comments": "11 pages, 5 figures",
        "journal-ref": "Entropy 19 (8), 2017, p. 387",
        "doi": "10.3390/e19080387",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The origin of the long-range memory in the non-equilibrium systems is still\nan open problem as the phenomenon can be reproduced using models based on\nMarkov processes. In these cases a notion of spurious memory is introduced. A\ngood example of Markov processes with spurious memory is stochastic process\ndriven by a non-linear stochastic differential equation (SDE). This example is\nat odds with models built using fractional Brownian motion (fBm). We analyze\ndifferences between these two cases seeking to establish possible empirical\ntests of the origin of the observed long-range memory. We investigate\nprobability density functions (PDFs) of burst and inter-burst duration in\nnumerically obtained time series and compare with the results of fBm. Our\nanalysis confirms that the characteristic feature of the processes described by\na one-dimensional SDE is the power-law exponent $3/2$ of the burst or\ninter-burst duration PDF. This property of stochastic processes might be used\nto detect spurious memory in various non-equilibrium systems, where observed\nmacroscopic behavior can be derived from the imitative interactions of agents.\n"
    },
    {
        "paper_id": 1707.09829,
        "authors": "Marcelo Brutti Righi, Fernanda Maria M\\\"uller and Marlon Ruoso Moresco",
        "title": "On a robust risk measurement approach for capital determination errors\n  minimization",
        "comments": null,
        "journal-ref": "Insurance: Mathematics and Economics 95, 199-211 (2020)",
        "doi": "10.1016/j.insmatheco.2020.10.007",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We propose a robust risk measurement approach that minimizes the expectation\nof overestimation plus underestimation costs. We consider uncertainty by taking\nthe supremum over a collection of probability measures, relating our approach\nto dual sets in the representation of coherent risk measures. We provide\nresults that guarantee the existence of a solution and explore the properties\nof minimizer and minimum as risk and deviation measures, respectively. An\nempirical illustration is carried out to demonstrate the use of our approach in\ncapital determination.\n"
    },
    {
        "paper_id": 1708.00062,
        "authors": "Volodymyr Perederiy",
        "title": "Sparse Structural Approach for Rating Transitions",
        "comments": "Keywords: Multi-year, Lifetime, Probability of Default, PD, Default\n  Rates, Rating Transition Matrices, IFRS 9, Expected Credit Losses, ECL,\n  Through-the-Cycle, TTC, Point-in-Time, PIT, Macroeconomic Adjustments, Time\n  Series, Autoregression, Accounting, Financial Instruments, Maximum Likelihood\n  Estimation",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In banking practice, rating transition matrices have become the standard\napproach of deriving multi-year probabilities of default (PDs) from one-year\nPDs, the latter normally being available from Basel ratings. Rating transition\nmatrices have gained in importance with the newly adopted IFRS 9 accounting\nstandard. Here, the multi-year PDs can be used to calculate the so-called\nexpected credit losses (ECL) over the entire lifetime of relevant credit\nassets. A typical approach for estimating the rating transition matrices relies\non calculating empirical rating migration counts and frequencies from rating\nhistory data. For small portfolios, however, this approach often leads to zero\ncounts and high count volatility, which makes the estimations unreliable and\nunstable, and can also produce counter-intuitive prediction patterns such as\nnon-parallel/crossing forward PD patterns. This paper proposes a structural\nmodel which overcomes these problems. We make a plausible assumption of an\nunderlying autoregressive mean-reverting ability-to-pay process. With only\nthree parameters, this sparse process can well describe an entire typical\nrating transition matrix, provided the one-year PDs of the rating classes are\nspecified. The transition probabilities produced by the structural approach are\nwell-behaved by design. The approach significantly reduces the statistical\ndegrees of freedom of the estimated transition probabilities, which makes the\nrating transition matrix more reliable for small portfolios. The approach can\nbe applied to data with as few as 50 observed rating transitions. Moreover, the\napproach can be efficiently applied to data consisting of continuous PDs (prior\nto rating discretization). In the IFRS 9 context, the approach offers an\nadditional merit: it can easily account for the macroeconomic adjustments,\nwhich are required by the IFRS 9 accounting standard.\n"
    },
    {
        "paper_id": 1708.00189,
        "authors": "Chengwei Zhang, Zhiyuan Zhang",
        "title": "Sequential Sampling for CGMY Processes via Decomposition of their Time\n  Changes",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We present a new and easy-to-implement sequential sampling method for CGMY\nprocesses with either finite or infinite variation, exploiting the time change\nrepresentation of the CGMY model and a decomposition of its time change. We\nfind that the time change can be decomposed into two independent components.\nWhile the first component is a \\emph{finite} \\emph{generalized gamma\nconvolution} process whose increments can be sampled by either the exact double\nCFTP (\"coupling from the past\") method or an approximation scheme with high\nspeed and accuracy, the second component can easily be made arbitrarily small\nin the $L^1$ sense. Simulation results show that the proposed method is\nadvantageous over two existing methods under a model calibrated to historical\noption price data.\n"
    },
    {
        "paper_id": 1708.00506,
        "authors": "Roman Gayduk and Sergey Nadtochiy",
        "title": "Control-stopping Games for Market Microstructure and Beyond",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper, we present a family of a control-stopping games which arise\nnaturally in equilibrium-based models of market microstructure, as well as in\nother models with strategic buyers and sellers. A distinctive feature of this\nfamily of games is the fact that the agents do not have any exogenously given\nfundamental value for the asset, and they deduce the value of their position\nfrom the bid and ask prices posted by other agents (i.e. they are pure\nspeculators). As a result, in such a game, the reward function of each agent,\nat the time of stopping, depends directly on the controls of other players. The\nequilibrium problem leads naturally to a system of coupled control-stopping\nproblems (or, equivalently, Reflected Backward Stochastic Differential\nEquations (RBSDEs)), in which the individual reward functions (or, reflecting\nbarriers) depend on the value functions (or, solution components) of other\nagents. The resulting system, in general, presents multiple mathematical\nchallenges due to the non-standard form of coupling (or, reflection). In the\npresent case, this system is also complicated by the fact that the continuous\ncontrols of the agents, describing their posted bid and ask prices, are\nconstrained to take values in a discrete grid. The latter feature reflects the\npresence of a positive tick size in the market, and it creates additional\ndiscontinuities in the agents reward functions (or, reflecting barriers).\nHerein, we prove the existence of a solution to the associated system in a\nspecial Markovian framework, provide numerical examples, and discuss the\npotential applications.\n"
    },
    {
        "paper_id": 1708.00644,
        "authors": "Stefano Ciliberti, Emmanuel S\\'eri\\'e, Guillaume Simon, Yves\n  Lemp\\'eri\\`ere, Jean-Philippe Bouchaud",
        "title": "The \"Size Premium\" in Equity Markets: Where is the Risk?",
        "comments": "Working paper",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We find that when measured in terms of dollar-turnover, and once\n$\\beta$-neutralised and Low-Vol neutralised, the Size Effect is alive and well.\nWith a long term t-stat of $5.1$, the \"Cold-Minus-Hot\" (CMH) anomaly is\ncertainly not less significant than other well-known factors such as Value or\nQuality. As compared to market-cap based SMB, CMH portfolios are much less\nanti-correlated to the Low-Vol anomaly. In contrast with standard risk premia,\nsize-based portfolios are found to be virtually unskewed. In fact, the extreme\nrisk of these portfolios is dominated by the large cap leg; small caps actually\nhave a positive (rather than negative) skewness. The only argument that favours\na risk premium interpretation at the individual stock level is that the extreme\ndrawdowns are more frequent for small cap/turnover stocks, even after\naccounting for volatility. This idiosyncratic risk is however clearly\ndiversifiable.\n"
    },
    {
        "paper_id": 1708.00645,
        "authors": "Aur\\'elien Hazan (UPEC UP12)",
        "title": "Stock-flow consistent macroeconomic model with nonuniform distributional\n  constraint",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We report on results concerning a partially aggregated Stock Flow Consistent\n(SFC) macroeconomic model in the stationary state where the sectors of banks\nand firms are aggregated, the sector of households is dis-aggregated, and the\nprobability density function (pdf) of the wealth of households is exogenous,\nconstrained by econometric data. It is shown that the equality part of the\nconstraint can be reduced to a single constant-sum equation, which relates this\nproblem to the study of continuous mass transport problems, and to the sum of\niid random variables. Existing results can thus be applied, and provide\nmarginal probabilities, and the location of the critical point before\ncondensation occurs. Various numerical experiments are performed using Monte\nCarlo sampling of the hit-and-run type, using wealth and income data for\nFrance.\n"
    },
    {
        "paper_id": 1708.01085,
        "authors": "Alexandre Jacquemain",
        "title": "Lorenz curves interpretations of the Bruss-Duerinckx theorem for\n  resource dependent branching processes",
        "comments": "10 pages with 2 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The Bruss and Duerinckx theorem for resource dependent branching processes\nstates that the survival of any society form is nested in an envelope formed by\ntwo extreme policies. The objective of this paper is to give a novel\ninterpretation of this theorem through the use of Lorenz curves. This\nrepresentation helps us visualize how the parameters interplay. Besides, as we\nwill show, it clarifies the impact of inequality in consumption.\n"
    },
    {
        "paper_id": 1708.01161,
        "authors": "Andrea Gabrielli, Matthieu Cristelli, Dario Mazzilli, Andrea\n  Tacchella, Andrea Zaccaria, Luciano Pietronero",
        "title": "Why we like the ECI+ algorithm",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Recently a measure for Economic Complexity named ECI+ has been proposed by\nAlbeaik et al. We like the ECI+ algorithm because it is mathematically\nidentical to the Fitness algorithm, the measure for Economic Complexity we\nintroduced in 2012. We demonstrate that the mathematical structure of ECI+ is\nstrictly equivalent to that of Fitness (up to normalization and rescaling). We\nthen show how the claims of Albeaik et al. about the ability of Fitness to\ndescribe the Economic Complexity of a country are incorrect. Finally, we\nhypothesize how the wrong results reported by these authors could have been\nobtained by not iterating the algorithm.\n"
    },
    {
        "paper_id": 1708.01308,
        "authors": "Marcel Nutz, Yuchong Zhang",
        "title": "A Mean Field Competition",
        "comments": "33 pages, 3 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We introduce a mean field game with rank-based reward: competing agents\noptimize their effort to achieve a goal, are ranked according to their\ncompletion time, and paid a reward based on their relative rank. First, we\npropose a tractable Poissonian model in which we can describe the optimal\neffort for a given reward scheme. Second, we study the principal--agent problem\nof designing an optimal reward scheme. A surprising, explicit design is found\nto minimize the time until a given fraction of the population has reached the\ngoal.\n"
    },
    {
        "paper_id": 1708.01324,
        "authors": "Merve Merakli, Simge Kucukyavuz",
        "title": "Vector-Valued Multivariate Conditional Value-at-Risk",
        "comments": null,
        "journal-ref": "Operations Research Letters, 46(3), 300-305, 2018",
        "doi": "10.1016/j.orl.2018.02.006",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this study, we propose a new definition of multivariate conditional\nvalue-at-risk (MCVaR) as a set of vectors for discrete probability spaces. We\nexplore the properties of the vector-valued MCVaR (VMCVaR) and show the\nadvantages of VMCVaR over the existing definitions given for continuous random\nvariables when adapted to the discrete case.\n"
    },
    {
        "paper_id": 1708.01489,
        "authors": "Michael B. Gordy and Alexander J. McNeil",
        "title": "Spectral backtests of forecast distributions with application to risk\n  management",
        "comments": "41 pages, 3 figures, supplementary material",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We study a class of backtests for forecast distributions in which the test\nstatistic depends on a spectral transformation that weights exceedance events\nby a function of the modeled probability level. The weighting scheme is\nspecified by a kernel measure which makes explicit the user's priorities for\nmodel performance. The class of spectral backtests includes tests of\nunconditional coverage and tests of conditional coverage. We show how the class\nembeds a wide variety of backtests in the existing literature, and further\npropose novel variants which are easily implemented, well-sized and have good\npower. In an empirical application, we backtest forecast distributions for the\novernight P&L of ten bank trading portfolios. For some portfolios, test results\ndepend materially on the choice of kernel.\n"
    },
    {
        "paper_id": 1708.01561,
        "authors": "Zachary Feinstein, Weijie Pang, Birgit Rudloff, Eric Schaanning,\n  Stephan Sturm, Mackenzie Wildman",
        "title": "Sensitivity of the Eisenberg-Noe clearing vector to individual interbank\n  liabilities",
        "comments": "37 pages",
        "journal-ref": "SIAM J. Finan. Math. 9:4 (2018), 1286-1325",
        "doi": "10.1137/18M1171060",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We quantify the sensitivity of the Eisenberg-Noe clearing vector to\nestimation errors in the bilateral liabilities of a financial system in a\nstylized setting. The interbank liabilities matrix is a crucial input to the\ncomputation of the clearing vector. However, in practice central bankers and\nregulators must often estimate this matrix because complete information on\nbilateral liabilities is rarely available. As a result, the clearing vector may\nsuffer from estimation errors in the liabilities matrix. We quantify the\nclearing vector's sensitivity to such estimation errors and show that its\ndirectional derivatives are, like the clearing vector itself, solutions of\nfixed point equations. We describe estimation errors utilizing a basis for the\nspace of matrices representing permissible perturbations and derive analytical\nsolutions to the maximal deviations of the Eisenberg-Noe clearing vector. This\nallows us to compute upper bounds for the worst case perturbations of the\nclearing vector in our simple setting. Moreover, we quantify the probability of\nobserving clearing vector deviations of a certain magnitude, for uniformly or\nnormally distributed errors in the relative liability matrix.\n  Applying our methodology to a dataset of European banks, we find that\nperturbations to the relative liabilities can result in economically sizeable\ndifferences that could lead to an underestimation of the risk of contagion. Our\nresults are a first step towards allowing regulators to quantify errors in\ntheir simulations.\n"
    },
    {
        "paper_id": 1708.01665,
        "authors": "Mark Higgins",
        "title": "A Two Factor Forward Curve Model with Stochastic Volatility for\n  Commodity Prices",
        "comments": "20 pages, 8 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We describe a model for evolving commodity forward prices that incorporates\nthree important dynamics which appear in many commodity markets: mean reversion\nin spot prices and the resulting Samuelson effect on volatility term structure,\ndecorrelation of moves in different points on the forward curve, and implied\nvolatility skew and smile.\n  This model is a \"forward curve model\" - it describes the stochastic evolution\nof forward prices - rather than a \"spot model\" that models the evolution of the\nspot commodity price. Two Brownian motions drive moves across the forward\ncurve, with a third Heston-like stochastic volatility process scaling\ninstantaneous volatilities of all forward prices.\n  In addition to an efficient numerical scheme for calculating European vanilla\nand early-exercise option prices, we describe an algorithm for Monte\nCarlo-based pricing of more generic derivative payoffs which involves an\nefficient approximation for the risk neutral drift that avoids having to\nsimulate drifts for every forward settlement date required for pricing.\n"
    },
    {
        "paper_id": 1708.01678,
        "authors": "Kei Noba, Jos\\'e-Luis P\\'erez, Kazutoshi Yamazaki, Kouji Yano",
        "title": "On optimal periodic dividend strategies for L\\'evy risk processes",
        "comments": "30 pages. Forthcoming in Insurance: Mathematics and Economics",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper, we revisit the optimal periodic dividend problem, in which\ndividend payments can only be made at the jump times of an independent Poisson\nprocess. In the dual (spectrally positive L\\'evy) model, recent results have\nshown the optimality of a periodic barrier strategy, which pays dividends at\nPoissonian dividend-decision times, if and only if the surplus is above some\nlevel. In this paper, we show the optimality of this strategy for a spectrally\nnegative L\\'evy process whose dual has a completely monotone L\\'evy density.\nThe optimal strategies and value functions are concisely written in terms of\nthe scale functions. Numerical results are also provided.\n"
    },
    {
        "paper_id": 1708.0189,
        "authors": "Larry G. Epstein and Shaolin Ji",
        "title": "Optimal Learning under Robustness and Time-Consistency",
        "comments": "35 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We model learning in a continuous-time Brownian setting where there is prior\nambiguity. The associated model of preference values robustness and is\ntime-consistent. It is applied to study optimal learning when the choice\nbetween actions can be postponed, at a per-unit-time cost, in order to observe\na signal that provides information about an unknown parameter. The\ncorresponding optimal stopping problem is solved in closed-form, with a focus\non two specific settings: Ellsberg's two-urn thought experiment expanded to\nallow learning before the choice of bets, and a robust version of the classical\nproblem of sequential testing of two simple hypotheses about the unknown drift\nof a Wiener process. In both cases, the link between robustness and the demand\nfor learning is studied.\n"
    },
    {
        "paper_id": 1708.01897,
        "authors": "Mikhail Goykhman, Ali Teimouri",
        "title": "Machine learning in sentiment reconstruction of the simulated stock\n  market",
        "comments": "18 pages, 6 figures",
        "journal-ref": null,
        "doi": "10.1016/j.physa.2017.11.093",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper we continue the study of the simulated stock market framework\ndefined by the driving sentiment processes. We focus on the market environment\ndriven by the buy/sell trading sentiment process of the Markov chain type. We\napply the methodology of the Hidden Markov Models and the Recurrent Neural\nNetworks to reconstruct the transition probabilities matrix of the Markov\nsentiment process and recover the underlying sentiment states from the observed\nstock price behavior.\n"
    },
    {
        "paper_id": 1708.01974,
        "authors": "David T. Frazier, Christian P. Robert and Judith Rousseau",
        "title": "Model Misspecification in ABC: Consequences and Diagnostics",
        "comments": null,
        "journal-ref": null,
        "doi": "10.1111/369--7412/20/82421",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We analyze the behavior of approximate Bayesian computation (ABC) when the\nmodel generating the simulated data differs from the actual data generating\nprocess; i.e., when the data simulator in ABC is misspecified. We demonstrate\nboth theoretically and in simple, but practically relevant, examples that when\nthe model is misspecified different versions of ABC can yield substantially\ndifferent results. Our theoretical results demonstrate that even though the\nmodel is misspecified, under regularity conditions, the accept/reject ABC\napproach concentrates posterior mass on an appropriately defined pseudo-true\nparameter value. However, under model misspecification the ABC posterior does\nnot yield credible sets with valid frequentist coverage and has non-standard\nasymptotic behavior. In addition, we examine the theoretical behavior of the\npopular local regression adjustment to ABC under model misspecification and\ndemonstrate that this approach concentrates posterior mass on a completely\ndifferent pseudo-true value than accept/reject ABC. Using our theoretical\nresults, we suggest two approaches to diagnose model misspecification in ABC.\nAll theoretical results and diagnostics are illustrated in a simple running\nexample.\n"
    },
    {
        "paper_id": 1708.02073,
        "authors": "Luca Barbaglia, Christophe Croux and Ines Wilms",
        "title": "Volatility Spillovers and Heavy Tails: A Large t-Vector AutoRegressive\n  Approach",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Volatility is a key measure of risk in financial analysis. The high\nvolatility of one financial asset today could affect the volatility of another\nasset tomorrow. These lagged effects among volatilities - which we call\nvolatility spillovers - are studied using the Vector AutoRegressive (VAR)\nmodel. We account for the possible fat-tailed distribution of the VAR model\nerrors using a VAR model with errors following a multivariate Student\nt-distribution with unknown degrees of freedom. Moreover, we study volatility\nspillovers among a large number of assets. To this end, we use penalized\nestimation of the VAR model with t-distributed errors. We study volatility\nspillovers among energy, biofuel and agricultural commodities and reveal\nbidirectional volatility spillovers between energy and biofuel, and between\nenergy and agricultural commodities.\n"
    },
    {
        "paper_id": 1708.0218,
        "authors": "Qi-Wen Wang, Jian-Jun Shu",
        "title": "Financial option insurance",
        "comments": null,
        "journal-ref": "Risk Management-Journal of Risk Crisis and Disaster, Vol. 19, No.\n  1, pp. 72-101, 2017",
        "doi": "10.1057/s41283-016-0013-5",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The option is a financial derivative, which is regularly employed in reducing\nthe risk of its underlying securities. However, investing in option is still\nrisky. Such risk becomes much severer for speculators who utilize option as a\nmeans of leverage to increase their potential returns. In order to mitigate\nrisk on their positions, the rudimentary concept of financial option insurance\nis introduced into practice. Two starkly-dissimilar concepts of insurance and\nfinancial option are integrated into the formation of financial option\ninsurance. The proposed financial product insures investors option premiums\nwhen misfortune befalls on them. As a trade-off, they are likely to sacrifice a\nlimited portion of their potential profits. The loopholes of prevailing\nfinancial market are addressed and the void is filled by introducing a stable\nthree-entity framework. Moreover, a specifically designed mathematical model is\nproposed. It consists of two portions: the business strategy of matching and a\nverification-and-modification process. The proposed model enables the option\ninvestors with calls and puts of different moneyness to be protected by the\nissued option insurance. Meanwhile, it minimizes the exposure of option\ninsurers position to any potential losses.\n"
    },
    {
        "paper_id": 1708.02193,
        "authors": "Adam Krawiec, Tomasz Stachowiak, Marek Szydlowski",
        "title": "The phase space structure of the oligopoly dynamical system by means of\n  Darboux integrability",
        "comments": "25 pages, 4 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We investigate the dynamical complexity of Cournot oligopoly dynamics of\nthree firms by using the qualitative methods of dynamical systems to study the\nphase structure of this model. The phase space is organized with\none-dimensional and two-dimensional invariant submanifolds (for the monopoly\nand duopoly) and unique stable node (global attractor) in the positive quadrant\nof the phase space (Cournot equilibrium). We also study the integrability of\nthe system. We demonstrate the effectiveness of the method of the Darboux\npolynomials in searching for first integrals of the oligopoly. The general\nmethod as well as examples of adopting this method are presented. We study\nDarboux non-integrability of the oligopoly for linear demand functions and find\nfirst integrals of this system for special classes of the system, in\nparticular, rational integrals can be found for a quite general set of model\nparameters. We show how first integral can be useful in lowering the dimension\nof the system using the example of $n$ almost identical firms. This first\nintegral also gives information about the structure of the phase space and the\nbehaviour of trajectories in the neighbourhood of a Nash equilibrium\n"
    },
    {
        "paper_id": 1708.02365,
        "authors": "David T. Frazier, Tatsushi Oka and Dan Zhu",
        "title": "Indirect Inference with a Non-Smooth Criterion Function",
        "comments": "This paper is a revision of arXiv:1708.02365 and supersedes the\n  earlier arXiv paper \"Derivative-Based Optimization with a Non-Smooth\n  Simulated Criterion\"",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Indirect inference requires simulating realisations of endogenous variables\nfrom the model under study. When the endogenous variables are discontinuous\nfunctions of the model parameters, the resulting indirect inference criterion\nfunction is discontinuous and does not permit the use of derivative-based\noptimisation routines. Using a change of variables technique, we propose a\nnovel simulation algorithm that alleviates the discontinuities inherent in such\nindirect inference criterion functions, and permits the application of\nderivative-based optimisation routines to estimate the unknown model\nparameters. Unlike competing approaches, this approach does not rely on kernel\nsmoothing or bandwidth parameters. Several Monte Carlo examples that have\nfeatured in the literature on indirect inference with discontinuous outcomes\nillustrate the approach, and demonstrate the superior performance of this\napproach over existing alternatives.\n"
    },
    {
        "paper_id": 1708.02411,
        "authors": "Felix Patzelt, Jean-Philippe Bouchaud",
        "title": "Nonlinear price impact from linear models",
        "comments": "Companion paper to [1]: arXiv:1706.04163",
        "journal-ref": null,
        "doi": "10.1088/1742-5468/aa9335",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The impact of trades on asset prices is a crucial aspect of market dynamics\nfor academics, regulators and practitioners alike. Recently, universal and\nhighly nonlinear master curves were observed for price impacts aggregated on\nall intra-day scales [1]. Here we investigate how well these curves, their\nscaling, and the underlying return dynamics are captured by linear \"propagator\"\nmodels. We find that the classification of trades as price-changing versus\nnon-price-changing can explain the price impact nonlinearities and short-term\nreturn dynamics to a very high degree. The explanatory power provided by the\nchange indicator in addition to the order sign history increases with\nincreasing tick size. To obtain these results, several long-standing technical\nissues for model calibration and -testing are addressed. We present new\nspectral estimators for two- and three-point cross-correlations, removing the\nneed for previously used approximations. We also show when calibration is\nunbiased and how to accurately reveal previously overlooked biases. Therefore,\nour results contribute significantly to understanding both recent empirical\nresults and the properties of a popular class of impact models.\n"
    },
    {
        "paper_id": 1708.02424,
        "authors": "Juan Francisco Monge",
        "title": "Cardinality constrained portfolio selection via factor models",
        "comments": null,
        "journal-ref": "Optimization Letters, 2020",
        "doi": "10.1007/s11590-020-01571-6",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper we propose and discuss different 0-1 linear models in order to\nsolve the cardinality constrained portfolio problem by using factor models.\nFactor models are used to build portfolios to track indexes, together with\nother objectives, also need a smaller number of parameters to estimate than the\nclassical Markowitz model. The addition of the cardinality constraints limits\nthe number of securities in the portfolio. Restricting the number of securities\nin the portfolio allows us to obtain a concentrated portfolio, reduce the risk\nand limit transaction costs. To solve this problem, a pure 0-1 model is\npresented in this work, the 0-1 model is constructed by means of a piecewise\nlinear approximation. We also present a new quadratic combinatorial problem,\ncalled a minimum edge-weighted clique problem, to obtain an equality weighted\ncardinality constrained portfolio. A piecewise linear approximation for this\nproblem is presented in the context of a multi factor model. For a single\nfactor model, we present a fast heuristic, based on some theoretical results to\nobtain an equality weighted cardinality constraint portfolio. The consideration\nof a piecewise linear approximation allow us to reduce significantly the\ncomputation time required for the equivalent quadratic problem. Computational\nresults from the 0-1 models are compared to those using a state-of-the-art\nQuadratic MIP solver.\n"
    },
    {
        "paper_id": 1708.02563,
        "authors": "Ryan McCrickerd, Mikko S. Pakkanen",
        "title": "Turbocharging Monte Carlo pricing for the rough Bergomi model",
        "comments": "16 pages, 10 figures, v3: minor amendments and reformatted",
        "journal-ref": "Quantitative Finance 2018, Vol. 18, No. 11, 1877-1886",
        "doi": "10.1080/14697688.2018.1459812",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The rough Bergomi model, introduced by Bayer, Friz and Gatheral [Quant.\nFinance 16(6), 887-904, 2016], is one of the recent rough volatility models\nthat are consistent with the stylised fact of implied volatility surfaces being\nessentially time-invariant, and are able to capture the term structure of skew\nobserved in equity markets. In the absence of analytical European option\npricing methods for the model, we focus on reducing the runtime-adjusted\nvariance of Monte Carlo implied volatilities, thereby contributing to the\nmodel's calibration by simulation. We employ a novel composition of variance\nreduction methods, immediately applicable to any conditionally log-normal\nstochastic volatility model. Assuming one targets implied volatility estimates\nwith a given degree of confidence, thus calibration RMSE, the results we\ndemonstrate equate to significant runtime reductions - roughly 20 times on\naverage, across different correlation regimes.\n"
    },
    {
        "paper_id": 1708.02605,
        "authors": "Rubina Zadourian and Andreas Kl\\\"umper",
        "title": "Exact probability distribution function for the volatility of cumulative\n  production",
        "comments": "6 pages, 6 figures",
        "journal-ref": null,
        "doi": "10.1016/j.physa.2017.12.003",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper we study the volatility and its probability distribution\nfunction for the cumulative production based on the experience curve\nhypothesis. This work presents a generalization of the study of volatility in\n[1], which addressed the effects of normally distributed noise in the\nproduction process. Due to its wide applicability in industrial and\ntechnological activities we present here the mathematical foundation for an\narbitrary distribution function of the process, which we expect will pave the\nfuture research on production and market strategy.\n"
    },
    {
        "paper_id": 1708.02625,
        "authors": "Jethro Browell",
        "title": "Risk Constrained Trading Strategies for Stochastic Generation with a\n  Single-Price Balancing Market",
        "comments": null,
        "journal-ref": null,
        "doi": "10.3390/en11061345",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Due to the limited predictability of wind power and other stochastic\ngeneration, trading this energy in competitive electricity markets is\nchallenging. This paper derives revenue-maximising and risk-constrained\nstrategies for stochastic generators participating in electricity markets with\na single-price balancing mechanism. Starting from the optimal---and\nimpractical---strategy of offering zero or nominal power, which exposes the\nparticipant to potentially large imbalance costs, we develop a number of\nstrategies that control risk by hedging against penalising balancing prices in\nfavour of rewarding ones. Trading strategies are formulated in a probabilistic\nframework in order to address asymmetry in balancing prices. The large-scale\ncommunication of system information characteristic of modern power systems is\nutilised to inputs for electricity price forecasts and probabilistic system\nlength forecasts. A case study using data from the GB market in the UK is\npresented and the ability of the proposed strategies to increase revenue and\nreduce risk is demonstrated and analysed.\n"
    },
    {
        "paper_id": 1708.02715,
        "authors": "Kyle Bechler and Michael Ludkovski",
        "title": "Order Flows and Limit Order Book Resiliency on the Meso-Scale",
        "comments": "8 figures; One figure is missing due to arxiv size constraints (was\n  6MB originally)",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We investigate the behavior of limit order books on the meso-scale motivated\nby order execution scheduling algorithms. To do so we carry out empirical\nanalysis of the order flows from market and limit order submissions, aggregated\nfrom tick-by-tick data via volume-based bucketing, as well as various LOB depth\nand shape metrics. We document a nonlinear relationship between trade imbalance\nand price change, which however can be converted into a linear link by\nconsidering a weighted average of market and limit order flows. We also\ndocument a hockey-stick dependence between trade imbalance and one-sided limit\norder flows, highlighting numerous asymmetric effects between the active and\npassive sides of the LOB. To address the phenomenological features of price\nformation, book resilience, and scarce liquidity we apply a variety of\nstatistical models to test for predictive power of different predictors. We\nshow that on the meso-scale the limit order flows (as well as the relative\naddition/cancellation rates) carry the most predictive power. Another finding\nis that the deeper LOB shape, rather than just the book imbalance, is more\nrelevant on this timescale. The empirical results are based on analysis of six\nlarge-tick assets from Nasdaq.\n"
    },
    {
        "paper_id": 1708.02786,
        "authors": "Matteo Barigozzi, Lorenzo Trapani",
        "title": "Sequential testing for structural stability in approximate factor models",
        "comments": null,
        "journal-ref": null,
        "doi": "10.1016/j.spa.2020.03.003",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We develop a monitoring procedure to detect changes in a large approximate\nfactor model. Letting $r$ be the number of common factors, we base our\nstatistics on the fact that the $\\left( r+1\\right) $-th eigenvalue of the\nsample covariance matrix is bounded under the null of no change, whereas it\nbecomes spiked under changes. Given that sample eigenvalues cannot be estimated\nconsistently under the null, we randomise the test statistic, obtaining a\nsequence of \\textit{i.i.d} statistics, which are used for the monitoring\nscheme. Numerical evidence shows a very small probability of false detections,\nand tight detection times of change-points.\n"
    },
    {
        "paper_id": 1708.02984,
        "authors": "Zura Kakushadze and Willie Yu",
        "title": "Decoding Stock Market with Quant Alphas",
        "comments": "20 pages; to appear in Journal of Asset Management",
        "journal-ref": "Journal of Asset Management 19(1) (2018) 38-48",
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We give an explicit algorithm and source code for extracting expected returns\nfor stocks from expected returns for alphas. Our algorithm altogether bypasses\ncombining alphas with weights into \"alpha combos\". Simply put, we have\ndeveloped a new method for trading alphas which does not involve combining\nthem. This yields substantial cost savings as alpha combos cost hedge funds\naround 3% of the P&L, while alphas themselves cost around 10%. Also, the extra\nlayer of alpha combos, which our new method avoids, adds noise and\nsuboptimality. We also arrive at our algorithm independently by explicitly\nconstructing alpha risk models based on position data.\n"
    },
    {
        "paper_id": 1708.03099,
        "authors": "Claudio Fontana, Markus Pelger and Eckhard Platen",
        "title": "On the existence of sure profits via flash strategies",
        "comments": "16 pages; revised and expanded version",
        "journal-ref": "J. Appl. Probab. 56 (2019) 384-397",
        "doi": "10.1017/jpr.2019.32",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We introduce and study the notion of sure profit via flash strategy,\nconsisting of a high-frequency limit of buy-and-hold trading strategies. In a\nfully general setting, without imposing any semimartingale restriction, we\nprove that there are no sure profits via flash strategies if and only if asset\nprices do not exhibit predictable jumps. This result relies on the general\ntheory of processes and provides the most general formulation of the well-known\nfact that, in an arbitrage-free financial market, asset prices (including\ndividends) should not exhibit jumps of a predictable direction or magnitude at\npredictable times. We furthermore show that any price process is always\nright-continuous in the absence of sure profits. Our results are robust under\nsmall transaction costs and imply that, under minimal assumptions, price\nchanges occurring at scheduled dates should only be due to unanticipated\ninformation releases.\n"
    },
    {
        "paper_id": 1708.03242,
        "authors": "Tommi Sottinen and Lauri Viitasaari",
        "title": "Conditional-Mean Hedging Under Transaction Costs in Gaussian Models",
        "comments": "arXiv admin note: text overlap with arXiv:1706.01534",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We consider so-called regular invertible Gaussian Volterra processes and\nderive a formula for their prediction laws. Examples of such processes include\nthe fractional Brownian motions and the mixed fractional Brownian motions. As\nan application, we consider conditional-mean hedging under transaction costs in\nBlack-Scholes type pricing models where the Brownian motion is replaced with a\nmore general regular invertible Gaussian Volterra process.\n"
    },
    {
        "paper_id": 1708.03511,
        "authors": "Lorenzo Napolitano, Evangelos Evangelou, Emanuele Pugliese, Paolo\n  Zeppini, Graham Room",
        "title": "Technology networks: the autocatalytic origins of innovation",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We analyse the autocatalytic structure of technological networks and evaluate\nits significance for the dynamics of innovation patenting. To this aim, we\ndefine a directed network of technological fields based on the International\nPatents Classification, in which a source node is connected to a receiver node\nvia a link if patenting activity in the source field anticipates patents in the\nreceiver field in the same region more frequently than we would expect at\nrandom. We show that the evolution of the technology network is compatible with\nthe presence of a growing autocatalytic structure, i.e. a portion of the\nnetwork in which technological fields mutually benefit from being connected to\none another. We further show that technological fields in the core of the\nautocatalytic set display greater fitness, i.e. they tend to appear in a\ngreater number of patents, thus suggesting the presence of positive spillovers\nas well as positive reinforcement. Finally, we observe that core shifts take\nplace whereby different groups of technology fields alternate within the\nautocatalytic structure; this points to the importance of recombinant\ninnovation taking place between close as well as distant fields of the\nhierarchical classification of technological fields.\n"
    },
    {
        "paper_id": 1708.03533,
        "authors": "Luciano Celi, Claudio Della Volpe, Luca Pardi, Stefano Siboni",
        "title": "Oil economy phase plot: a physical analogy",
        "comments": "8 pages, 4 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
        "abstract": "  A phase plot of the oil economy is built using the literature data of world\noil production, price, and EROEI (Energy Returned on Energy Invested). An\nanalogy between the oil economy and the Benard convection is proposed; some\nmethods of interpretation and forecast of the system behavior are also shown\nbased on \"phase portrait\" using as main variables the price, production and\nEROEI values. A scenery is proposed on this basis.\n"
    },
    {
        "paper_id": 1708.03551,
        "authors": "Soufiane Hayou",
        "title": "On the overestimation of the largest eigenvalue of a covariance matrix",
        "comments": "15 pages, 1 figure",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper, we use a new approach to prove that the largest eigenvalue of\nthe sample covariance matrix of a normally distributed vector is bigger than\nthe true largest eigenvalue with probability 1 when the dimension is infinite.\nWe prove a similar result for the smallest eigenvalue.\n"
    },
    {
        "paper_id": 1708.03992,
        "authors": "Takaki Hayashi, Yuta Koike",
        "title": "Multi-scale analysis of lead-lag relationships in high-frequency\n  financial markets",
        "comments": "30 pages, 1 figure. Theoretical results have been improved. Empirical\n  application has been updated (there was a minor data manipulation issue in v3\n  and it has been fixed in this version)",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We propose a novel estimation procedure for scale-by-scale lead-lag\nrelationships of financial assets observed at high-frequency in a\nnon-synchronous manner. The proposed estimation procedure does not require any\ninterpolation processing of original datasets and is applicable to those with\nhighest time resolution available. Consistency of the proposed estimators is\nshown under the continuous-time framework that has been developed in our\nprevious work Hayashi and Koike (2018). An empirical application to a quote\ndataset of the NASDAQ-100 assets identifies two types of lead-lag relationships\nat different time scales.\n"
    },
    {
        "paper_id": 1708.04107,
        "authors": "Saleh Albeaik, Mary Kaltenberg, Mansour Alsaleh, C\\'esar A. Hidalgo",
        "title": "729 new measures of economic complexity (Addendum to Improving the\n  Economic Complexity Index)",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Recently we uploaded to the arxiv a paper entitled: Improving the Economic\nComplexity Index. There, we compared three metrics of the knowledge intensity\nof an economy, the original metric we published in 2009 (the Economic\nComplexity Index or ECI), a variation of the metric proposed in 2012, and a\nvariation we called ECI+. It was brought to our attention that the definition\nof ECI+ was equivalent to the variation of the metric proposed in 2012. We have\nverified this claim, and found that while the equations are not exactly the\nsame, they are similar enough to be our own oversight. More importantly, we now\nask: how many variations of the original ECI work? In this paper we provide a\nsimple unifying framework to explore multiple variations of ECI, including both\nthe original 2009 ECI and the 2012 variation. We found that a large fraction of\nvariations have a similar predictive power, indicating that the chance of\nfinding a variation of ECI that works, after the seminal 2009 measure, are\nsurprisingly high. In fact, more than 28 percent of these variations have a\npredictive power that is within 90 percent of the maximum for any variation.\nThese findings show that, once the idea of measuring economic complexity was\nout, creating a variation with a similar predictive power (like the ones\nproposed in 2012) was trivial (a 1 in 3 shot). More importantly, the result\nshow that using exports data to measure the knowledge intensity of an economy\nis a robust phenomenon that works for multiple functional forms. Moreover, the\nfact that multiple variations of the 2009 ECI perform close to the maximum,\ntells us that no variation of ECI will have a performance that is substantially\nbetter. This suggests that research efforts should focus on uncovering the\nmechanisms that contribute to the diffusion and accumulation of productive\nknowledge instead of on exploring small variations to existing measures.\n"
    },
    {
        "paper_id": 1708.04217,
        "authors": "Qidi Peng and Ran Zhao",
        "title": "A General Class of Multifractional Processes and Stock Price\n  Informativeness",
        "comments": null,
        "journal-ref": null,
        "doi": "10.1016/j.chaos.2018.08.004",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We introduce a general class of stochastic processes driven by a\nmultifractional Brownian motion (mBm) and study the estimation problems of\ntheir pointwise H\\\"older exponents (PHE) based on a new localized generalized\nquadratic variation approach (LGQV). By comparing our suggested approach with\nthe other two existing benchmark estimation approaches (classic GQV and\noscillation approach) through a simulation study, we show that our estimator\nhas better performance in the case where the observed process is some unknown\nbivariate function of time and mBm. Such multifractional processes, whose PHEs\nare time-varying, can be used to model stock prices under various market\nconditions, that are both time-dependent and region-dependent. As an\napplication to finance, an empirical study on modeling cross-listed stocks\nprovides new evidence that the equity path's roughness varies via time and the\nstock price informativeness properties from global stock markets.\n"
    },
    {
        "paper_id": 1708.04281,
        "authors": "Xiaobai Zhu, Mary Hardy and David Saunders",
        "title": "Valuation of a Bermudan DB underpin hybrid pension benefit",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper we consider three types of embedded options in pension benefit\ndesign.\n  The first is the Florida second election (FSE) option, offered to public\nemployees in the state of Florida in 2002. Employees were given the option to\nconvert from a defined contribution (DC) plan to a defined benefit (DB) plan at\na time of their choosing. The cost of the switch was assessed in terms of the\nABO (Accrued Benefit Obligation), which is the expected present value of the\naccrued DB pension at the time of the switch. If the ABO was greater than the\nDC account, the employee was required to fund the difference.\n  The second is the DB Underpin option, also known as a floor offset, under\nwhich the employee participates in a DC plan, but with a guaranteed minimum\nbenefit based on a traditional DB formula.\n  The third option can be considered a variation on each of the first two. We\nremove the requirement from the FSE option for employees to fund any shortfall\nat the switching date. The resulting option is very similar to the DB underpin,\nbut with the possibility of early exercise. Since we assume that exercise is\nonly permitted at discrete, annual intervals, this option is a Bermudan\nvariation on the DB Underpin.\n  We adopt an arbitrage-free pricing methodology to value the option. We\nanalyse and value the optimal switching strategy for the employee by\nconstructing an exercise frontier, and illustrate numerically the difference\nbetween the FSE, DB Underpin and Bermudan DB Underpin options.\n"
    },
    {
        "paper_id": 1708.04337,
        "authors": "Jos\\'e E. Figueroa-L\\'opez, Hyoeun Lee, and Raghu Pasupathy",
        "title": "Optimal placement of a small order in a diffusive limit order book",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We study the optimal placement problem of a stock trader who wishes to clear\nhis/her inventory by a predetermined time horizon t, by using a limit order or\na market order. For a diffusive market, we characterize the optimal limit order\nplacement policy and analyze its behavior under different market conditions. In\nparticular, we show that, in the presence of a negative drift, there exists a\ncritical time t0>0 such that, for any time horizon t>t0, there exists an\noptimal placement, which, contrary to earlier work, is different from one that\nis placed \"infinitesimally\" close to the best ask, such as the best bid and\nsecond best bid. We also propose a simple method to approximate the critical\ntime t0 and the optimal order placement.\n"
    },
    {
        "paper_id": 1708.04339,
        "authors": "Jos\\'e E. Figueroa-L\\'opez and Cecilia Mancini",
        "title": "Optimum thresholding using mean and conditional mean square error",
        "comments": "36 pages, 1 figure",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We consider a univariate semimartingale model for (the logarithm of) an asset\nprice, containing jumps having possibly infinite activity (IA). The\nnonparametric threshold estimator of the integrated variance IV proposed in\nMancini 2009 is constructed using observations on a discrete time grid, and\nprecisely it sums up the squared increments of the process when they are below\na threshold, a deterministic function of the observation step and possibly of\nthe coefficients of X. All the threshold functions satisfying given conditions\nallow asymptotically consistent estimates of IV, however the finite sample\nproperties of the truncated realized variation can depend on the specific\nchoice of the threshold. We aim here at optimally selecting the threshold by\nminimizing either the estimation mean square error (MSE) or the conditional\nmean square error (cMSE). The last criterion allows to reach a threshold which\nis optimal not in mean but for the specific volatility (and jumps paths) at\nhand. A parsimonious characterization of the optimum is established, which\nturns out to be asymptotically proportional to the L\\'evy's modulus of\ncontinuity of the underlying Brownian motion. Moreover, minimizing the cMSE\nenables us to propose a novel implementation scheme for approximating the\noptimal threshold. Monte Carlo simulations illustrate the superior performance\nof the proposed method.\n"
    },
    {
        "paper_id": 1708.0443,
        "authors": "Sindhuja Ranganathan, Mikko Kivel\\\"a, Juho Kanniainen",
        "title": "Dynamics of Investor Spanning Trees Around Dot-Com Bubble",
        "comments": "15 pages, 4 figures",
        "journal-ref": null,
        "doi": "10.1371/journal.pone.0198807",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We identify temporal investor networks for Nokia stock by constructing\nnetworks from correlations between investor-specific net-volumes and analyze\nchanges in the networks around dot-com bubble. We conduct the analysis\nseparately for households, non-financial institutions, and financial\ninstitutions. Our results indicate that spanning tree measures for households\nreflected the boom and crisis: the maximum spanning tree measures had clear\nupward tendency in the bull markets when the bubble was building up, and, even\nmore importantly, the minimum spanning tree measures pre-reacted the burst of\nbubble. At the same time, we find less clear reactions in minimal and maximal\nspanning trees of non-financial and financial institutions around the bubble,\nwhich suggest that household investors can have a greater herding tendency\naround bubbles.\n"
    },
    {
        "paper_id": 1708.04532,
        "authors": "Aurelio F. Bariviera, Mar\\'ia Jos\\'e Basgall, Waldo Hasperu\\'e,\n  Marcelo Naiouf",
        "title": "Some stylized facts of the Bitcoin market",
        "comments": "17 pages, 6 figures. arXiv admin note: text overlap with\n  arXiv:1605.06700",
        "journal-ref": "Physica A Statistical Mechanics and Its Applications, Vol. 484,\n  pp. 82-90 (2017)",
        "doi": "10.1016/j.physa.2017.04.159",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In recent years a new type of tradable assets appeared, generically known as\ncryptocurrencies. Among them, the most widespread is Bitcoin. Given its\nnovelty, this paper investigates some statistical properties of the Bitcoin\nmarket. This study compares Bitcoin and standard currencies dynamics and\nfocuses on the analysis of returns at different time scales. We test the\npresence of long memory in return time series from 2011 to 2017, using\ntransaction data from one Bitcoin platform. We compute the Hurst exponent by\nmeans of the Detrended Fluctuation Analysis method, using a sliding window in\norder to measure long range dependence. We detect that Hurst exponents changes\nsignificantly during the first years of existence of Bitcoin, tending to\nstabilize in recent times. Additionally, multiscale analysis shows a similar\nbehavior of the Hurst exponent, implying a self-similar process.\n"
    },
    {
        "paper_id": 1708.04711,
        "authors": "Athanasios Andrikopoulos",
        "title": "Generalizations of Szpilrajn's Theorem in economic and game theories",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Szpilrajn's Lemma entails that each partial order extends to a linear order.\nDushnik and Miller use Szpilrajn's Lemma to show that each partial order has a\nrelizer. Since then, many authors utilize Szpilrajn's Theorem and the\nWell-ordering principle to prove more general existence type theorems on\nextending binary relations. Nevertheless, we are often interested not only in\nthe existence of extensions of a binary relation $R$ satisfying certain axioms\nof orderability, but in something more: (A) The conditions of the sets of\nalternatives and the properties which $R$ satisfies to be inherited when one\npasses to any member of a subfamily of the family of extensions of $R$ and: (B)\nThe size of a family of ordering extensions of $R$, whose intersection is $R$,\nto be the smallest one. The key to addressing these kinds of problems is the\nszpilrajn inherited method. In this paper, we define the notion of\n$\\Lambda(m)$-consistency, where $m$ can reach the first infinite ordinal\n$\\omega$, and we give two general inherited type theorems on extending binary\nrelations, a Szpilrajn type and a Dushnik-Miller type theorem, which generalize\nall the well known existence and inherited type extension theorems in the\nliterature. \\keywords{Consistent binary relations, Extension theorems,\nIntersection of binary relations.\n"
    },
    {
        "paper_id": 1708.04829,
        "authors": "Foad Shokrollahi",
        "title": "Pricing compound and extendible options under mixed fractional Brownian\n  motion with jumps",
        "comments": null,
        "journal-ref": "Axioms 2019",
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This study deals with the problem of pricing compound options when the\nunderlying asset follows a mixed fractional Brownian motion with jumps. An\nanalytic formula for compound options is derived under the risk neutral\nmeasure. Then, these results are applied to value extendible options. Moreover,\nsome special cases of the formula are discussed and numerical results are\nprovided.\n"
    },
    {
        "paper_id": 1708.04869,
        "authors": "Julio Backhoff-Veraguas, Mathias Beiglb\\\"ock, Martin Huesmann, Sigrid\n  K\\\"allblad",
        "title": "Martingale Benamou--Brenier: a probabilistic perspective",
        "comments": "We have corrected some typos, upgraded the dynamic programming\n  principle, and included a proof of the Lipschitz-Markov property for the\n  transition kernel of stretched Brownian motion in dimension one",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In classical optimal transport, the contributions of Benamou-Brenier and\nMcCann regarding the time-dependent version of the problem are cornerstones of\nthe field and form the basis for a variety of applications in other\nmathematical areas.\n  We suggest a Benamou-Brenier type formulation of the martingale transport\nproblem for given $d$-dimensional distributions $\\mu, \\nu $ in convex order.\nThe unique solution $M^*=(M_t^*)_{t\\in [0,1]}$ of this problem turns out to be\na Markov-martingale which has several notable properties: In a specific sense\nit mimics the movement of a Brownian particle as closely as possible subject to\nthe conditions $M^*_0\\sim\\mu, M^*_1\\sim \\nu$. Similar to McCann's\ndisplacement-interpolation, $M^*$ provides a time-consistent interpolation\nbetween $\\mu$ and $\\nu$. For particular choices of the initial and terminal\nlaw, $M^*$ recovers archetypical martingales such as Brownian motion, geometric\nBrownian motion, and the Bass martingale. Furthermore, it yields a natural\napproximation to the local vol model and a new approach to Kellerer's theorem.\n  This article is parallel to the work of Huesmann-Trevisan, who consider a\nrelated class of problems from a PDE-oriented perspective.\n"
    },
    {
        "paper_id": 1708.04952,
        "authors": "Swati Singh and Manoj Joshi",
        "title": "New Market Creation via Innovation: A Study on Tata Nano",
        "comments": "13 pages",
        "journal-ref": "aWEshkar Vol. XIX Issue 2, Sept 2015",
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This research paper focuses on how innovations support new market creation\nemerging from latent opportunities for low-income group. It also emphasizes on\nnovel strategies that can be implemented for sustaining. The paper concludes\nwith a discussion on the implications of the study and directions to stimulate\nfuture research on the subject.\n"
    },
    {
        "paper_id": 1708.04955,
        "authors": "Kazuki Ikeda",
        "title": "qBitcoin: A Peer-to-Peer Quantum Cash System",
        "comments": "11 pages, 2 figures",
        "journal-ref": "Proceedings of the 2018 Computing Conference",
        "doi": "10.1007/978-3-030-01174-1_58",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  A decentralized online quantum cash system, called qBitcoin, is given. We\ndesign the system which has great benefits of quantization in the following\nsense. Firstly, quantum teleportation technology is used for coin transaction,\nwhich prevents from the owner of the coin keeping the original coin data even\nafter sending the coin to another. This was a main problem in a classical\ncircuit and a blockchain was introduced to solve this issue. In qBitcoin, the\ndouble-spending problem never happens and its security is guaranteed\ntheoretically by virtue of quantum information theory. Making a block is time\nconsuming and the system of qBitcoin is based on a quantum chain, instead of\nblocks. Therefore a payment can be completed much faster than Bitcoin. Moreover\nwe employ quantum digital signature so that it naturally inherits properties of\npeer-to-peer (P2P) cash system as originally proposed in Bitcoin.\n"
    },
    {
        "paper_id": 1708.05319,
        "authors": "Damiano Brigo, Marco Francischello, Andrea Pallavicini",
        "title": "An indifference approach to the cost of capital constraints: KVA and\n  beyond",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The strengthening of capital requirements has induced banks and traders to\nconsider charging a so called capital valuation adjustment (KVA) to the clients\nin OTC transactions. This roughly corresponds to charge the clients ex-ante the\nprofit requirement that is asked to the trading desk. In the following we try\nto delineate a possible way to assess the impact of capital constraints in the\nvaluation of a deal. We resort to an optimisation stemming from an indifference\npricing approach, and we study both the linear problem from the point of view\nof the whole bank and the non-linear problem given by the viewpoint of\nshareholders. We also consider the case where one optimises the median rather\nthan the mean statistics of the profit and loss distribution.\n"
    },
    {
        "paper_id": 1708.05352,
        "authors": "Antoine Jacquier, Louis Jeannerod",
        "title": "How many paths to simulate correlated Brownian motions?",
        "comments": "2 pages, 1 figure",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We provide an explicit formula giving the optimal number of paths needed to\nsimulate two correlated Brownian motions.\n"
    },
    {
        "paper_id": 1708.05689,
        "authors": "Ali Hussein Samadi, Afshin Montakhab, Hussein Marzban, Sakine Owjimehr",
        "title": "Quantum Barro--Gordon Game in Monetary Economics",
        "comments": "15 pages, 2 tables. Accepted for publication in Physica A",
        "journal-ref": "Physica A 489 (2018) 94 - 101",
        "doi": "10.1016/j.physa.2017.07.029",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Classical game theory addresses decision problems in multi-agent environment\nwhere one rational agent's decision affects other agents' payoffs. Game theory\nhas widespread application in economic, social and biological sciences. In\nrecent years quantum versions of classical games have been proposed and\nstudied. In this paper, we consider a quantum version of the classical\nBarro-Gordon game which captures the problem of time inconsistency in monetary\neconomics. Such time inconsistency refers to the temptation of weak policy\nmaker to implement high inflation when the public expects low inflation. The\ninconsistency arises when the public punishes the weak policy maker in the next\ncycle. We first present a quantum version of the Barro-Gordon game. Next, we\nshow that in a particular case of the quantum game, time-consistent Nash\nequilibrium could be achieved when public expects low inflation, thus resolving\nthe game.\n"
    },
    {
        "paper_id": 1708.05713,
        "authors": "Amir Ahmadi-Javid and Malihe Fallah-Tafti",
        "title": "Portfolio Optimization with Entropic Value-at-Risk",
        "comments": null,
        "journal-ref": "European Journal of Operational Research, 279(1), 225-241",
        "doi": "10.1016/j.ejor.2019.02.007",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The entropic value-at-risk (EVaR) is a new coherent risk measure, which is an\nupper bound for both the value-at-risk (VaR) and conditional value-at-risk\n(CVaR). As important properties, the EVaR is strongly monotone over its domain\nand strictly monotone over a broad sub-domain including all continuous\ndistributions, while well-known monotone risk measures, such as VaR and CVaR\nlack these properties. A key feature for a risk measure, besides its financial\nproperties, is its applicability in large-scale sample-based portfolio\noptimization. If the negative return of an investment portfolio is a\ndifferentiable convex function, the portfolio optimization with the EVaR\nresults in a differentiable convex program whose number of variables and\nconstraints is independent of the sample size, which is not the case for the\nVaR and CVaR. This enables us to design an efficient algorithm using\ndifferentiable convex optimization. Our extensive numerical study shows the\nhigh efficiency of the algorithm in large scales, compared to the existing\nconvex optimization software packages. The computational efficiency of the EVaR\nportfolio optimization approach is also compared with that of CVaR-based\nportfolio optimization. This comparison shows that the EVaR approach generally\nperforms similarly, and it outperforms as the sample size increases. Moreover,\nthe comparison of the portfolios obtained for a real case by the EVaR and CVaR\napproaches shows that the EVaR approach can find portfolios with better\nexpectations and VaR values at high confidence levels.\n"
    },
    {
        "paper_id": 1708.05957,
        "authors": "Roxana Dumitrescu, Romuald Elie, Wissal Sabbagh, Chao Zhou",
        "title": "A new Mertens decomposition of $\\mathscr{Y}^{g,\\xi}$-submartingale\n  systems. Application to BSDEs with weak constraints at stopping times",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We first introduce the concept of $\\mathscr{Y}^{g,\\xi}$-submartingale\nsystems, where the nonlinear operator $\\mathscr{Y}^{g,\\xi}$ corresponds to the\nfirst component of the solution of a reflected BSDE with generator $g$ and\nlower obstacle $\\xi$. We first show that, in the case of a left-limited\nright-continuous obstacle, any $\\mathscr{Y}^{g,\\xi}$-submartingale system can\nbe aggregated by a process which is right-lower semicontinuous. We then prove a\n\\textit{Mertens decomposition}, by using an original approach which does not\nmake use of the standard penalization technique. These results are in\nparticular useful for the treatment of control/stopping game problems and, to\nthe best of our knowledge, they are completely new in the literature. As an\napplication, we introduce a new class of \\textit{Backward Stochastic\nDifferential Equations (in short BSDEs) with weak constraints at stopping\ntimes}, which are related to the partial hedging of American options. We study\nthe wellposedness of such equations and, using the\n$\\mathscr{Y}^{g,\\xi}$-Mertens decomposition, we show that the family of minimal\ntime-$t$-values $Y_t$, with $(Y,Z)$ a supersolution of the BSDE with weak\nconstraints, admits a representation in terms of a reflected backward\nstochastic differential equation.\n"
    },
    {
        "paper_id": 1708.0616,
        "authors": "Amir Ahmadi-Javid and Mohsen Ebadi",
        "title": "Economic Design of Memory-Type Control Charts: The Fallacy of the\n  Formula Proposed by Lorenzen and Vance (1986)",
        "comments": "Computational Statistics, 2020",
        "journal-ref": "Ahmadi-Javid, A., & Ebadi, M. (2020). Economic design of\n  memory-type control charts: The fallacy of the formula proposed by Lorenzen\n  and Vance (1986). Computational Statistics, DOI: 10.1007/s00180-020-01019-6",
        "doi": "10.1007/s00180-020-01019-6",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The memory-type control charts, such as EWMA and CUSUM, are powerful tools\nfor detecting small quality changes in univariate and multivariate processes.\nMany papers on economic design of these control charts use the formula proposed\nby Lorenzen and Vance (1986) [Lorenzen, T. J., & Vance, L. C. (1986). The\neconomic design of control charts: A unified approach. Technometrics, 28(1),\n3-10, DOI: 10.2307/1269598]. This paper shows that this formula is not correct\nfor memory-type control charts and its values can significantly deviate from\nthe original values even if the ARL values used in this formula are accurately\ncomputed. Consequently, the use of this formula can result in charts that are\nnot economically optimal. The formula is corrected for memory-type control\ncharts, but unfortunately the modified formula is not a helpful tool from a\ncomputational perspective. We show that simulation-based optimization is a\npossible alternative method.\n"
    },
    {
        "paper_id": 1708.06233,
        "authors": "Christoph Aymanns and Jakob Foerster and Co-Pierre Georg",
        "title": "Fake News in Social Networks",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We model the spread of news as a social learning game on a network. Agents\ncan either endorse or oppose a claim made in a piece of news, which itself may\nbe either true or false. Agents base their decision on a private signal and\ntheir neighbors' past actions. Given these inputs, agents follow strategies\nderived via multi-agent deep reinforcement learning and receive utility from\nacting in accordance with the veracity of claims. Our framework yields\nstrategies with agent utility close to a theoretical, Bayes optimal benchmark,\nwhile remaining flexible to model re-specification. Optimized strategies allow\nagents to correctly identify most false claims, when all agents receive\nunbiased private signals. However, an adversary's attempt to spread fake news\nby targeting a subset of agents with a biased private signal can be successful.\nEven more so when the adversary has information about agents' network position\nor private signal. When agents are aware of the presence of an adversary they\nre-optimize their strategies in the training stage and the adversary's attack\nis less effective. Hence, exposing agents to the possibility of fake news can\nbe an effective way to curtail the spread of fake news in social networks. Our\nresults also highlight that information about the users' private beliefs and\ntheir social network structure can be extremely valuable to adversaries and\nshould be well protected.\n"
    },
    {
        "paper_id": 1708.06586,
        "authors": "Noemi Nava and T. Di Matteo and Tomaso Aste",
        "title": "Dynamic correlations at different time-scales with Empirical Mode\n  Decomposition",
        "comments": "19 pages, 11 figures",
        "journal-ref": null,
        "doi": "10.1016/j.physa.2018.02.108",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The Empirical Mode Decomposition (EMD) provides a tool to characterize time\nseries in terms of its implicit components oscillating at different\ntime-scales. We apply this decomposition to intraday time series of the\nfollowing three financial indices: the S\\&P 500 (USA), the IPC (Mexico) and the\nVIX (volatility index USA), obtaining time-varying multidimensional\ncross-correlations at different time-scales. The correlations computed over a\nrolling window are compared across the three indices, across the components at\ndifferent time-scales, at different lags and over time. We uncover a rich\nheterogeneity of interactions which depends on the time-scale and has important\nled-lag relations which can have practical use for portfolio management, risk\nestimation and investments.\n"
    },
    {
        "paper_id": 1708.06704,
        "authors": "Thomas Pedro Eggarter",
        "title": "Unemployment: Study of Causes and Possible Solutions",
        "comments": "T.P.Eggarter (physicist) passed away in August 1997. This work was\n  done during his last months of life and only locally published up to now.\n  Work is in Spanish and could be translated upon request. Please contact E.\n  Alvarez sequi@df.uba.ar",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The following measures against unemployment are proposed: In the short term,\nto promote greater income for the poorest sectors. It is shown that this can be\npaid with the resulting increased production, without losing income to the\nother economic agents. In the mid term, the creation of ad-hoc companies for\ninvestment in projects profitable but long lasting. And in the long run, the\nabandonment of the competitive models. As these proposals go against current\nideas (liberalisation, labour market flexibility, free market, etc.), the\nstatements are rigorously demonstrated, even at the risk of making the lecture\nharder.\n  Part 1 explores the problem and uses a simple model and others heuristic\narguments to create familiarity with macroeconomic models. Part 2 is a\nsimplified summary of Macroeconomic Theory textbook. It serves as a review to\nthe reader whose knowledge in economy are out of date, or as a first\napproximation to the topic if he or she does not have them. In the light of the\ntheory, economic policies are evaluated for the Argentine case in the 90's. The\nwork accepts the Keynesian explanation of unemployment (insufficient demand),\nbut we disagree on its solution (public expenditure). Finally, in Part 3 we\nelaborate and justify the proposals.\n"
    },
    {
        "paper_id": 1708.06792,
        "authors": "Mercedes Campi, Marco Due\\~nas",
        "title": "Volatility and Economic Growth in the Twentieth Century",
        "comments": "25 pages, 6 figures, 4 tables",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The twentieth century was a period of outstanding economic growth together\nwith an unequal income distribution. This paper analyses the international\ndistribution of growth rates and its dynamics during the twentieth century. We\nshow that the whole century is characterized by a high heterogeneity in the\ndistribution of GDP per capita growth rates, which is reflected in different\nshapes and a persistent asymmetry of the distributions at the regional level\nand for countries of different development levels. We find that in the context\nof the global conflicts that characterized the first half of the twentieth\ncentury and involved mainly large economies, the well-known negative scale\nrelation between volatility and size of countries is not significant. After the\nyear 1956, a redistribution of volatility leads to a significant negative\nscale-relation, which has been recently considered as a robust feature of the\nevolution of economic organizations. Our results contribute with more empirical\nfacts that call the attention to traditional macroeconomic theories to better\nexplain the underlying complexity of the growth process and sheds light on its\nhistorical evolution.\n"
    },
    {
        "paper_id": 1708.06855,
        "authors": "Adam Wu",
        "title": "Systematic Noise: Micro-movements in Equity Options Markets",
        "comments": "Undergraduate Research Paper",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Equity options are known to be notoriously difficult to price accurately, and\neven with the development of established mathematical models there are many\nassumptions that must be made about the underlying processes driving market\nmovements. As such, the theoretical prices outputted by these models are often\nslightly different from the realized or actual market price. The choice of\nmodel traders use can create many different valuations on the same asset, which\nmay lead to a form of systematic micro-movement or noise. The analysis in this\npaper demonstrates that approximately 1.7%-4.5% of market volume for options\nwritten on the SPY ETF within the last two years could potentially be due to\nsystematic noise.\n"
    },
    {
        "paper_id": 1708.06886,
        "authors": "Michael A. Kouritzin and Anne MacKay",
        "title": "VIX-linked fees for GMWBs via Explicit Solution Simulation Methods",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In a market with stochastic volatility and jumps, we consider a VIX-linked\nfee structure for variable annuity contracts with guaranteed minimum withdrawal\nbenefits (GMWB). Our goal is to assess the effectiveness of the VIX-linked fee\nstructure in decreasing the sensitivity of the insurer's liability to\nvolatility risk. Since the GMWB payoff is highly path-dependent, it is\nparticularly sensitive to volatility risk, and can also be challenging to\nprice, especially in the presence of the VIX-linked fee. In this paper, we\npresent an explicit weak solution for the value of the VA account and use it in\nMonte Carlo simulations to value the GMWB guarantee. Numerical examples are\nprovided to analyze the impact of the VIX-linked fee on the sensitivity of the\nliability to changes in market volatility.\n"
    },
    {
        "paper_id": 1708.06948,
        "authors": "Edward Hoyle, Andrea Macrina, Levent A. Meng\\\"ut\\\"urk",
        "title": "Modulated Information Flows in Financial Markets",
        "comments": "27 pages, 1 figure",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We model continuous-time information flows generated by a number of\ninformation sources that switch on and off at random times. By modulating a\nmulti-dimensional L\\'evy random bridge over a random point field, our framework\nrelates the discovery of relevant new information sources to jumps in\nconditional expectation martingales. In the canonical Brownian random bridge\ncase, we show that the underlying measure-valued process follows jump-diffusion\ndynamics, where the jumps are governed by information switches. The dynamic\nrepresentation gives rise to a set of stochastically-linked Brownian motions on\nrandom time intervals that capture evolving information states, as well as to a\nstate-dependent stochastic volatility evolution with jumps. The nature of\ninformation flows usually exhibits complex behaviour, however, we maintain\nanalytic tractability by introducing what we term the effective and\ncomplementary information processes, which dynamically incorporate active and\ninactive information, respectively. As an application, we price a financial\nvanilla option, which we prove is expressed by a weighted sum of option values\nbased on the possible state configurations at expiry. This result may be viewed\nas an information-based analogue of Merton's option price, but where\njump-diffusion arises endogenously. The proposed information flows also lend\nthemselves to the quantification of asymmetric informational advantage among\ncompetitive agents, a feature we analyse by notions of information geometry.\n"
    },
    {
        "paper_id": 1708.07037,
        "authors": "Jamal Bouoiyour (1), Refk Selmi (1), Amal Miftah (1) ((1) CATT)",
        "title": "Relationship between Remittances and Macroeconomic Variables in Times of\n  Political and Social Upheaval: Evidence from Tunisia's Arab Spring",
        "comments": "ERF 23rd Annual Conference , Mar 2017, Amman, Jordan",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  If Tunisia was hailed as a success story with its high rankings on economic,\neducational, and other indicators compared to other Arab countries, the 2011\npopular uprisings demonstrate the need for political reforms but also major\neconomic reforms. The Arab spring highlights the fragility of its main economic\npillars including the tourism and the foreign direct investment. In such\nturbulent times, the paper examines the economic impact of migrant'\nremittances, expected to have a countercyclical behavior. Our results reveal\nthat prior to the Arab Spring, the impacts of remittances on growth and\nconsumption seem negative and positive respectively, while they varyingly\ninfluence local investment. These three relationships held in the short-run. By\nconsidering the period surrounding the 2011 uprisings, the investment effect of\nremittances becomes negative and weak in the short-and medium-run, whereas\npositive and strong remittances' impacts on growth and consumption are found in\nthe long term.\n"
    },
    {
        "paper_id": 1708.07047,
        "authors": "Paolo Barucca and Fabrizio Lillo",
        "title": "Behind the price: on the role of agent's reflexivity in financial market\n  microstructure",
        "comments": "12 pages, 1 figure",
        "journal-ref": null,
        "doi": "10.1007/978-3-319-49872-0_3",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this chapter we review some recent results on the dynamics of price\nformation in financial markets and its relations with the efficient market\nhypothesis. Specifically, we present the limit order book mechanism for markets\nand we introduce the concepts of market impact and order flow, presenting their\nrecently discovered empirical properties and discussing some possible\ninterpretation in terms of agent's strategies. Our analysis confirms that\nquantitative analysis of data is crucial to validate qualitative hypothesis on\ninvestors' behavior in the regulated environment of order placement and to\nconnect these micro-structural behaviors to the properties of the collective\ndynamics of the system as a whole, such for instance market efficiency. Finally\nwe discuss the relation between some of the described properties and the theory\nof reflexivity proposing that in the process of price formation positive and\nnegative feedback loops between the cognitive and manipulative function of\nagents are present.\n"
    },
    {
        "paper_id": 1708.07061,
        "authors": "Jesus Lago, Fjo De Ridder, Peter Vrancx, Bart De Schutter",
        "title": "Forecasting day-ahead electricity prices in Europe: the importance of\n  considering market integration",
        "comments": null,
        "journal-ref": "Applied Energy, Volume 211, 1 February 2018, Pages 890-903",
        "doi": "10.1016/j.apenergy.2017.11.098",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Motivated by the increasing integration among electricity markets, in this\npaper we propose two different methods to incorporate market integration in\nelectricity price forecasting and to improve the predictive performance. First,\nwe propose a deep neural network that considers features from connected markets\nto improve the predictive accuracy in a local market. To measure the importance\nof these features, we propose a novel feature selection algorithm that, by\nusing Bayesian optimization and functional analysis of variance, evaluates the\neffect of the features on the algorithm performance. In addition, using market\nintegration, we propose a second model that, by simultaneously predicting\nprices from two markets, improves the forecasting accuracy even further. As a\ncase study, we consider the electricity market in Belgium and the improvements\nin forecasting accuracy when using various French electricity features. We show\nthat the two proposed models lead to improvements that are statistically\nsignificant. Particularly, due to market integration, the predictive accuracy\nis improved from 15.7% to 12.5% sMAPE (symmetric mean absolute percentage\nerror). In addition, we show that the proposed feature selection algorithm is\nable to perform a correct assessment, i.e. to discard the irrelevant features.\n"
    },
    {
        "paper_id": 1708.07063,
        "authors": "Panagiotis G. Papaioannou, George P. Papaioannou, Kostas Siettos,\n  Akylas Stratigakos, Christos Dikaiakos",
        "title": "Dynamic Conditional Correlation between Electricity and Stock markets\n  during the Financial Crisis in Greece",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Liberalization of electricity markets has increasingly created the need for\nunderstanding the volatility and correlation structure between electricity and\nfinancial markets. This work reveals the existence of structural changes in\ncorrelation patterns among these two markets and links the changes to both\nfundamentals and regulatory conditions prevailing in the markets, as well as\nthe current European financial crisis. We apply a Dynamic Conditional\nCorrelation (DCC) GARCH model to a set of market s fundamental variables and\nGreece s financial market and microeconomic indexes to study their interaction.\nEmphasis is given on the period of severe financial crisis of the Country to\nunderstand contagion and volatility spillover between these two markets.\n"
    },
    {
        "paper_id": 1708.07305,
        "authors": "Abduh Sayid (G-SCOP\\_GCSP), Yannick Frein (G-SCOP\\_GCSP), Ramzi\n  Hammami",
        "title": "Optimal firm's policy under lead time-and price-dependent demand:\n  interest of customers rejection policy",
        "comments": null,
        "journal-ref": "POMS 27th Annual Conference, May 2016, Orlando, Florida, United\n  States",
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Considering a lead-time-and price-sensitive demand, we investigate whether a\nclient rejection policy, modeled as M/M/1/K system, can be more profitable than\nan all-client acceptance policy, modeled as M/M/1 system. We provide analytical\ninsights for the cases with and without holding and penalty costs by comparing\nM/M/1/1 to M/M/1 models.\n"
    },
    {
        "paper_id": 1708.07394,
        "authors": "Ulrich Horst and D\\\"orte Kreher",
        "title": "Second order approximations for limit order books",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper we derive a second order approximation for an infinite\ndimensional limit order book model, in which the dynamics of the incoming order\nflow is allowed to depend on the current market price as well as on a volume\nindicator (e.g.~the volume standing at the top of the book). We study the\nfluctuations of the price and volume process relative to their first order\napproximation given in ODE-PDE form under two different scaling regimes. In the\nfirst case we suppose that price changes are really rare, yielding a constant\nfirst order approximation for the price. This leads to a measure-valued SDE\ndriven by an infinite dimensional Brownian motion in the second order\napproximation of the volume process. In the second case we use a slower\nrescaling rate, which leads to a non-degenerate first order approximation and\ngives a PDE with random coefficients in the second order approximation for the\nvolume process. Our results can be used to derive confidence intervals for\nmodels of optimal portfolio liquidation under market impact.\n"
    },
    {
        "paper_id": 1708.07469,
        "authors": "Justin Sirignano and Konstantinos Spiliopoulos",
        "title": "DGM: A deep learning algorithm for solving partial differential\n  equations",
        "comments": "Deep learning, machine learning, partial differential equations",
        "journal-ref": null,
        "doi": "10.1016/j.jcp.2018.08.029",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  High-dimensional PDEs have been a longstanding computational challenge. We\npropose to solve high-dimensional PDEs by approximating the solution with a\ndeep neural network which is trained to satisfy the differential operator,\ninitial condition, and boundary conditions. Our algorithm is meshfree, which is\nkey since meshes become infeasible in higher dimensions. Instead of forming a\nmesh, the neural network is trained on batches of randomly sampled time and\nspace points. The algorithm is tested on a class of high-dimensional free\nboundary PDEs, which we are able to accurately solve in up to $200$ dimensions.\nThe algorithm is also tested on a high-dimensional Hamilton-Jacobi-Bellman PDE\nand Burgers' equation. The deep learning algorithm approximates the general\nsolution to the Burgers' equation for a continuum of different boundary\nconditions and physical conditions (which can be viewed as a high-dimensional\nspace). We call the algorithm a \"Deep Galerkin Method (DGM)\" since it is\nsimilar in spirit to Galerkin methods, with the solution approximated by a\nneural network instead of a linear combination of basis functions. In addition,\nwe prove a theorem regarding the approximation power of neural networks for a\nclass of quasilinear parabolic PDEs.\n"
    },
    {
        "paper_id": 1708.07509,
        "authors": "Raul Rojas",
        "title": "The Keynesian Model in the General Theory: A Tutorial",
        "comments": "8 pages, 4 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This small overview of the General Theory is the kind of summary I would have\nliked to have read, before embarking in a comprehensive study of the General\nTheory at the time I was a student. As shown here, the main ideas are quite\nsimple and easy to visualize. Unfortunately, numerous introductions to\nKeynesian theory are not actually based on Keynes opus magnum, but in obscure\nneoclassical reinterpretations. This is completely pointless since Keynes' book\nis so readable.\n"
    },
    {
        "paper_id": 1708.07567,
        "authors": "Kevin Tee, Michael McCourt, Ruben Martinez-Cantin, Ian Dewancker,\n  Frank Liu",
        "title": "Active Preference Learning for Personalized Portfolio Construction",
        "comments": "4 pages, 2 figures, 1 algorithm, ICML Human in the Loop workshop",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In financial asset management, choosing a portfolio requires balancing\nreturns, risk, exposure, liquidity, volatility and other factors. These\nconcerns are difficult to compare explicitly, with many asset managers using an\nintuitive or implicit sense of their interaction. We propose a mechanism for\nlearning someone's sense of distinctness between portfolios with the goal of\nbeing able to identify portfolios which are predicted to perform well but are\ndistinct from the perspective of the user. This identification occurs, e.g., in\nthe context of Bayesian optimization of a backtested performance metric.\nNumerical experiments are presented which show the impact of personal beliefs\nin informing the development of a diverse and high-performing portfolio.\n"
    },
    {
        "paper_id": 1708.07585,
        "authors": "Wujiang Lou",
        "title": "Haircutting Non-cash Collateral",
        "comments": "26 pages, 4 figures, 7 tables; published in Risk, September 2017",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Haircutting non-cash collateral has become a key element of the post-crisis\nreform of the shadow banking system and OTC derivatives markets. This article\ndevelops a parametric haircut model by expanding haircut definitions beyond the\ntraditional value-at-risk measure and employing a double-exponential\njump-diffusion model for collateral market risk. Haircuts are solved to target\ncredit risk measurements, including probability of default, expected loss or\nunexpected loss criteria. Comparing to data-driven approach typically run on\nproxy data series, the model enables sensitivity analysis and stress test,\ncaptures market liquidity risk, allows idiosyncratic risk adjustments, and\nincorporates relevant market information. Computational results for main\nequities, securitization, and corporate bonds show potential for uses in\ncollateral agreements, e.g. CSAs, and for regulatory capital calculations.\n"
    },
    {
        "paper_id": 1708.07587,
        "authors": "Wilson Ye Chen, Richard H. Gerlach",
        "title": "Semiparametric GARCH via Bayesian model averaging",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  As the dynamic structure of the financial markets is subject to dramatic\nchanges, a model capable of providing consistently accurate volatility\nestimates must not make strong assumptions on how prices change over time. Most\nvolatility models impose a particular parametric functional form that relates\nan observed price change to a volatility forecast (news impact function). We\npropose a new class of functional coefficient semiparametric volatility models\nwhere the news impact function is allowed to be any smooth function, and study\nits ability to estimate volatilities compared to the well known parametric\nproposals, in both a simulation study and an empirical study with real\nfinancial data. We estimate the news impact function using a Bayesian model\naveraging approach, implemented via a carefully developed Markov chain Monte\nCarlo (MCMC) sampling algorithm. Using simulations we show that our flexible\nsemiparametric model is able to learn the shape of the news impact function\nfrom the observed data. When applied to real financial time series, our new\nmodel suggests that the news impact functions are significantly different in\nshapes for different asset types, but are similar for the assets of the same\ntype.\n"
    },
    {
        "paper_id": 1708.07636,
        "authors": "Christian Pinshi",
        "title": "Feedback effect between Volatility of capital flows and financial\n  stability: evidence from Democratic Republic of Congo",
        "comments": "in French. j'assume la responsabilit{\\'e} de ce document de travail",
        "journal-ref": null,
        "doi": "10.1000/xyz123",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Financial system being the place of metting capital flows (equality between\nsaving and investment), a volatility of capital flows can destroy the\nrobustness and good working of financial system, it means subvert financial\nstability. The same a weak financial system, few regulated and bad manage can\nexacerbate volatility of capital flows and finely undermine financial\nstability. The present study provides evidence on feedback effect between\nvolatility of capital flows and financial stability in Democratic republic of\nCongo (DRC), and estimate the contributions of macroeconomic and\nmacroprudential policies in the attenuation volatility of capital flows effects\non financial stability and in the prevention of instability financial.\nAssessment dynamic regression model a la Feldstein-Horioka we showed that\nfinancial system is widely supplied and financed by internationals capital\nflows. This implicate Congolese economy is financially mobile, that can be\ndangerous for financial stability. The study dynamic econometric of financial\nsystem's absolute size, we stipulate financial system has a systemic weight on\nreal economy. Hence a shock of financial system could have devastating effects\non Congolese economy. We estimate a vector autoregressive (VAR) model for prove\nthe bilateral causality and impacts of macroeconomic and macroprudential\npolicies. With regard to results, it proved on the one there is a feedback\neffect between volatility of capital flows and financial stability, on the\nother hand macroeconomic and macroprudential policies can't attenuate\nvolatility of capital flows and prevent instability financial. It prove\nmacroprudential approach is given a better result than monetary policy. The\nimplementation of framework macroprudential by Central Bank of Congo will be\nbeneficial in the realization of financial stability and attenuation volatility\nof capital flows.Keywords: Volatility of capital flows, financial stability,\nmacroeconomic and macroprudential policies\n"
    },
    {
        "paper_id": 1708.07637,
        "authors": "Tung-Lam Dao, Daniel Hoehener, Yves Lemp\\'eri\\`ere, Trung-Tu Nguyen,\n  Philip Seager, Jean-Philippe Bouchaud",
        "title": "Trends and Risk Premia: Update and Additional Plots",
        "comments": "Short note, 2 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Recently, our group has published two papers that have received some\nattention in the finance community. One is about the profitability of trend\nfollowing strategies over 200 years, the second is about the correlation\nbetween the profitability of \"Risk Premia\" and their skewness. In this short\nnote, we present two additional plots that fully corroborate our findings on\nnew data.\n"
    },
    {
        "paper_id": 1708.07661,
        "authors": "Stefan Gerhold, Paul Kr\\\"uhner",
        "title": "Dynamic trading under integer constraints",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper we investigate discrete time trading under integer constraints,\nthat is, we assume that the offered goods or shares are traded in integer\nquantities instead of the usual real quantity assumption. For finite\nprobability spaces and rational asset prices this has little effect on the core\nof the theory of no-arbitrage pricing. For price processes not restricted to\nthe rational numbers, a novel theory of integer arbitrage free pricing and\nhedging emerges. We establish an FTAP, involving a set of absolutely continuous\nmartingale measures satisfying an additional property. The set of prices of a\ncontingent claim is no longer an interval, but is either empty or dense in an\ninterval. We also discuss superhedging with integral portfolios.\n"
    },
    {
        "paper_id": 1708.07723,
        "authors": "Yann Bramoull\\'e and Kenan Huremovi\\'c",
        "title": "Promotion through Connections: Favors or Information?",
        "comments": "60 pages, 6 figures, 38 tables",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Connections appear to be helpful in many contexts, such as obtaining a job, a\npromotion, a grant, a loan, or publishing a paper. This may be due either to\nfavoritism or to information conveyed by connections. Attempts at identifying\nboth effects have relied on measures of true quality, generally built from data\ncollected long after promotion. Building on earlier work on discrimination, we\npropose a new method to identify favors and information from data collected at\nthe time of promotion. Under weak assumptions, we show that promotion decisions\nfor connected candidates look more random to the econometrician due to the\ninformation channel. We derive new identification results and estimate the\nstrength of the two effects. We adapt the control function approach to address\nthe issue of the selection into connections. Applying our methodology to\nacademic promotions in Spain and Italy, as well as political advancements in\nChina, we find evidence that connections may both convey information and\nattract favors.\n"
    },
    {
        "paper_id": 1708.07996,
        "authors": "Jean-Bernard Chatelain, Kirsten Ralf",
        "title": "A Simple Algorithm for Solving Ramsey Optimal Policy with Exogenous\n  Forcing Variables",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This algorithm extends Ljungqvist and Sargent (2012) algorithm of Stackelberg\ndynamic game to the case of dynamic stochastic general equilibrium models\nincluding exogenous forcing variables. It is based Anderson, Hansen, McGrattan,\nSargent (1996) discounted augmented linear quadratic regulator. It adds an\nintermediate step in solving a Sylvester equation. Forward-looking variables\nare also optimally anchored on forcing variables. This simple algorithm calls\nfor already programmed routines for Ricatti, Sylvester and Inverse matrix in\nMatlab and Scilab. A final step using a change of basis vector computes a\nvector auto regressive representation including Ramsey optimal policy rule\nfunction of lagged observable variables, when the exogenous forcing variables\nare not observable.\n"
    },
    {
        "paper_id": 1708.08275,
        "authors": "Jacques Tempere",
        "title": "An equilibrium-conserving taxation scheme for income from capital",
        "comments": "4 pages, 2 figures",
        "journal-ref": null,
        "doi": "10.1140/epjb/e2018-80497-x",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Under conditions of market equilibrium, the distribution of capital income\nfollows a Pareto power law, with an exponent that characterizes the given\nequilibrium. Here, a simple taxation scheme is proposed such that the post-tax\ncapital income distribution remains an equilibrium distribution, albeit with a\ndifferent exponent. This taxation scheme is shown to be progressive, and its\nparameters can be simply derived from (i) the total amount of tax that will be\nlevied, (ii) the threshold selected above which capital income will be taxed\nand (iii) the total amount of capital income. The latter can be obtained either\nby using Piketty's estimates of the capital/labor income ratio or by fitting\nthe initial Pareto exponent. Both ways moreover provide a check on the amount\nof declared income from capital.\n"
    },
    {
        "paper_id": 1708.08411,
        "authors": "Jiro Akahori and Hai Ha Pham",
        "title": "Default Contagion with Domino Effect , A First Passage Time Approach",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The present paper introduces a structural framework to model dependent\ndefaults, with a particular interest in their contagion.\n"
    },
    {
        "paper_id": 1708.08594,
        "authors": "Teruyoshi Kobayashi, Taro Takaguchi",
        "title": "Identifying relationship lending in the interbank market: A network\n  approach",
        "comments": "Main text: 39 pages, 15 figures. SI: 4 pages, 3 figures",
        "journal-ref": "Journal of Banking & Finance 97, 20-36, 2018",
        "doi": "10.1016/j.jbankfin.2018.09.018",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Relationship lending is broadly interpreted as a strong partnership between a\nlender and a borrower. Nevertheless, we still lack consensus regarding how to\nquantify the strength of a lending relationship, while simple statistics such\nas the frequency and volume of loans have been used as proxies in previous\nstudies. Here, we propose statistical tests to identify relationship lending as\na significant tie between banks. Application of the proposed method to the\nItalian interbank networks reveals that the fraction of relationship lending\namong all bilateral trades has been quite stable and that the relationship\nlenders tend to impose high interest rates at the time of financial distress.\n"
    },
    {
        "paper_id": 1708.08622,
        "authors": "Frantisek Cech, and Jozef Barunik",
        "title": "Measurement of Common Risk Factors: A Panel Quantile Regression Model\n  for Returns",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper investigates how to measure common market risk factors using newly\nproposed Panel Quantile Regression Model for Returns. By exploring the fact\nthat volatility crosses all quantiles of the return distribution and using\npenalized fixed effects estimator we are able to control for otherwise\nunobserved heterogeneity among financial assets. Direct benefits of the\nproposed approach are revealed in the portfolio Value-at-Risk forecasting\napplication, where our modeling strategy performs significantly better than\nseveral benchmark models according to both statistical and economic comparison.\nIn particular Panel Quantile Regression Model for Returns consistently\noutperforms all the competitors in the 5\\% and 10\\% quantiles. Sound\nstatistical performance translates directly into economic gains which is\ndemonstrated in the Global Minimum Value-at-Risk Portfolio and Markowitz-like\ncomparison. Overall results of our research are important for correct\nidentification of the sources of systemic risk, and are particularly attractive\nfor high dimensional applications.\n"
    },
    {
        "paper_id": 1708.08673,
        "authors": "Ron W. Nielsen",
        "title": "Changing the Direction of the Economic and Demographic Research",
        "comments": "20 pages,10 figures, 11729 words",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  A simple but useful method of reciprocal values is introduced, explained and\nillustrated. This method simplifies the analysis of hyperbolic distributions,\nwhich are causing serious problems in the demographic and economic research. It\nallows for a unique identification of hyperbolic distributions and for\nunravelling components of more complicated trajectories. This method is\nillustrated by a few examples. They show that fundamental postulates of the\ndemographic and economic research are contradicted by data, even by precisely\nthe same data, which are used in this research. The generally accepted\npostulates are based on the incorrect understanding of hyperbolic\ndistributions, which characterise the historical growth of population and the\nhistorical economic growth. In particular, data used, but never analysed,\nduring the formulation of the Unified Growth Theory show that this theory is\nbased on fundamentally incorrect premises and thus is fundamentally defective.\nApplication of this simple method of analysis points to new directions in the\ndemographic and economic research. It suggests simpler interpretations of the\nmechanism of growth. The concept or the evidence of the past primitive and\ndifficult living conditions, which might be perhaps described as some kind of\nstagnation, is not questioned or disputed. It is only demonstrated that\ntrajectories of the past economic growth and of the growth of population were\nnot reflecting any form of stagnation and thus that they were not shaped by\nthese primitive and difficult living conditions. The concept or evidence of an\nexplosion in technology, medicine, education and in the improved living\nconditions is not questioned or disputed. It is only demonstrated that this\npossible explosion is not reflected in the trajectories of the economic growth\nand of the growth of population.\n"
    },
    {
        "paper_id": 1708.08675,
        "authors": "Roxana Dumitrescu, Marie-Claire Quenez, Agn\\`es Sulem",
        "title": "American options in an imperfect market with default",
        "comments": "arXiv admin note: text overlap with arXiv:1511.09041",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We study pricing and (super)hedging for American options in an imperfect\nmarket model with default, where the imperfections are taken into account via\nthe nonlinearity of the wealth dynamics. The payoff is given by an RCLL adapted\nprocess $(\\xi_t)$. We define the {\\em seller's superhedging price} of the\nAmerican option as the minimum of the initial capitals which allow the seller\nto build up a superhedging portfolio. We prove that this price coincides with\nthe value function of an optimal stopping problem with nonlinear expectations\ninduced by BSDEs with default jump, which corresponds to the solution of a\nreflected BSDE with lower barrier. Moreover, we show the existence of a\nsuperhedging portfolio strategy. We then consider the {\\em buyer's superhedging\nprice}, which is defined as the supremum of the initial wealths which allow the\nbuyer to select an exercise time $\\tau$ and a portfolio strategy $\\varphi$ so\nthat he/she is superhedged. Under the additional assumption of left upper\nsemicontinuity along stopping times of $(\\xi_t)$, we show the existence of a\nsuperhedge $(\\tau, \\varphi)$ for the buyer, as well as a characterization of\nthe buyer's superhedging price via the solution of a nonlinear reflected BSDE\nwith upper barrier.\n"
    },
    {
        "paper_id": 1708.08695,
        "authors": "Davide Valenti, Giorgio Fazio, Bernardo Spagnolo",
        "title": "The stabilizing effect of volatility in financial markets",
        "comments": "Main article (6 pages, 3 figures) and supplemental material as an\n  ancillary PDF file (4 pages, 3 figures). Submitted to Phys. Rev. Lett",
        "journal-ref": "Phys. Rev. E 97, 062307 (2018)",
        "doi": "10.1103/PhysRevE.97.062307",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In financial markets, greater volatility is usually considered synonym of\ngreater risk and instability. However, large market downturns and upturns are\noften preceded by long periods where price returns exhibit only small\nfluctuations. To investigate this surprising feature, here we propose using the\nmean first hitting time, i.e. the average time a stock return takes to undergo\nfor the first time a large negative or positive variation, as an indicator of\nprice stability, and relate this to a standard measure of volatility. In an\nempirical analysis of daily returns for $1071$ stocks traded in the New York\nStock Exchange, we find that this measure of stability displays nonmonotonic\nbehavior, with a maximum, as a function of volatility. Also, we show that the\nstatistical properties of the empirical data can be reproduced by a nonlinear\nHeston model. This analysis implies that, contrary to conventional wisdom, not\nonly high, but also low volatility values can be associated with higher\ninstability in financial markets.\n"
    },
    {
        "paper_id": 1708.08857,
        "authors": "Mogens Graf Plessen, Alberto Bemporad",
        "title": "Stock Trading via Feedback Control: Stochastic Model Predictive or\n  Genetic?",
        "comments": "7 pages, 3 figures, 3 tables, presented as a poster at XVIII Workshop\n  on Quantitative Finance (QFW2017) in Milano on January 25-27, 2017",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We seek a discussion about the most suitable feedback control structure for\nstock trading under the consideration of proportional transaction costs.\nSuitability refers to robustness and performance capability. Both are tested by\nconsidering different one-step ahead prediction qualities, including the ideal\ncase, correct prediction of the direction of change in daily stock prices and\nthe worst-case. Feedback control structures are partitioned into two general\nclasses: stochastic model predictive control (SMPC) and genetic. For the former\nclass three controllers are discussed, whereby it is distinguished between two\nMarkowitz- and one dynamic hedging-inspired SMPC formulation. For the latter\nclass five trading algorithms are disucssed, whereby it is distinguished\nbetween two different moving average (MA) based, two trading range (TR) based,\nand one strategy based on historical optimal (HistOpt) trajectories. This paper\nalso gives a preliminary discussion about how modified dynamic hedging-inspired\nSMPC formulations may serve as alternatives to Markowitz portfolio\noptimization. The combinations of all of the eight controllers with five\ndifferent one-step ahead prediction methods are backtested for daily trading of\nthe 30 components of the German stock market index DAX for the time period\nbetween November 27, 2015 and November 25, 2016.\n"
    },
    {
        "paper_id": 1708.08904,
        "authors": "Denis Belomestny, Volker Kraetschmer",
        "title": "Minimax theorems for American options in incomplete markets without\n  time-consistency",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper we give sufficient conditions guaranteeing the validity of the\nwell-known minimax theorem for the lower Snell envelope with respect to a\nfamily of absolutely continuous probability measures. Such minimax results play\nan important role in the characterisation of arbitrage-free prices of American\ncontingent claims in incomplete markets. Our conditions do not rely on the\nnotions of stability under pasting or time-consistency and reveal some\nunexpected connection between the minimax result and the path properties of the\ncorresponding density process.\n"
    },
    {
        "paper_id": 1708.09327,
        "authors": "Aleksandra Alori\\'c, Peter Sollich and Peter McBurney",
        "title": "Spontaneous Segregation of Agents Across Double Auction Markets",
        "comments": "12 pages, 7 figures; Artificial Economics 2014 conference; Published\n  online: 17 October 2014",
        "journal-ref": "Advances in Artificial Economics (2015) pp 79-90. Lecture Notes in\n  Economics and Mathematical Systems, vol 676. Springer, Cham",
        "doi": "10.1007/978-3-319-09578-3_7",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper we investigate the possibility of spontaneous segregation into\ngroups of traders that have to choose among several markets. Even in the\nsimplest case of two markets and Zero Intelligence traders, we are able to\nobserve segregation effects below a critical value Tc of the temperature T; the\nlatter regulates how strongly traders bias their decisions towards choices with\nlarge accumulated scores. It is notable that segregation occurs even though the\ntraders are statistically homogeneous. Traders can in principle change their\nloyalty to a market, but the relevant persistence times become long below Tc.\n"
    },
    {
        "paper_id": 1708.09343,
        "authors": "Stavros Stavroyiannis",
        "title": "Value-at-Risk and Expected Shortfall for the major digital currencies",
        "comments": "9 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Digital currencies and cryptocurrencies have hesitantly started to penetrate\nthe investors, and the next step will be the regulatory risk management\nframework. We examine the Value-at-Risk and Expected Shortfall properties for\nthe major digital currencies, Bitcoin, Ethereum, Litecoin, and Ripple. The\nmethodology used is GARCH modelling followed by Filtered Historical Simulation.\nWe find that digital currencies are subject to a higher risk, therefore, to\nhigher sufficient buffer and risk capital to cover potential losses.\n"
    },
    {
        "paper_id": 1708.0952,
        "authors": "Worapree Maneesoonthorn, Gael M. Martin and Catherine S. Forbes",
        "title": "High-Frequency Jump Tests: Which Test Should We Use?",
        "comments": "This is a revised and shortened version of an earlier paper by the\n  same authors, entitled:\"Dynamic Price Jumps: the Performance of High\n  Frequency Tests and Measures, and the Robustness of Inference\". This current\n  version is forthcoming in Journal of Econometrics",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We conduct an extensive evaluation of price jump tests based on\nhigh-frequency financial data. After providing a concise review of multiple\nalternative tests, we document the size and power of all tests in a range of\nempirically relevant scenarios. Particular focus is given to the robustness of\ntest performance to the presence of jumps in volatility and microstructure\nnoise, and to the impact of sampling frequency. The paper concludes by\nproviding guidelines for empirical researchers about which test to choose in\nany given setting.\n"
    },
    {
        "paper_id": 1708.0981,
        "authors": "Alessandra Mainini, Enrico Moretto",
        "title": "Extending Yagil exchange ratio determination model to the case of\n  stochastic dividends",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This article extends, in a stochastic environment, the Yagil (1987) model\nwhich establishes, in a deterministic dividend discount model, a range for the\nexchange ratio in a stock-for-stock merger agreement. Here, we generalize\nYagil's work letting both pre- and post-merger dividends grow randomly over\ntime. If Yagil focuses only on changes in stock prices before and after the\nmerger, our stochastic environment allows to keep in account both shares'\nexpected values and variance, letting us to identify a more complex bargaining\nregion whose shape depends on mean and standard deviation of the dividends'\ngrowth rate.\n"
    },
    {
        "paper_id": 1708.0985,
        "authors": "K\\k{e}stutis Baltakys, Juho Kanniainen, Frank Emmert-Streib",
        "title": "Multilayer Aggregation with Statistical Validation: Application to\n  Investor Networks",
        "comments": null,
        "journal-ref": null,
        "doi": "10.1038/s41598-018-26575-2",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Multilayer networks are attracting growing attention in many fields,\nincluding finance. In this paper, we develop a new tractable procedure for\nmultilayer aggregation based on statistical validation, which we apply to\ninvestor networks. Moreover, we propose two other improvements to their\nanalysis: transaction bootstrapping and investor categorization. The\naggregation procedure can be used to integrate security-wise and time-wise\ninformation about investor trading networks, but it is not limited to finance.\nIn fact, it can be used for different applications, such as gene,\ntransportation, and social networks, were they inferred or observable.\nAdditionally, in the investor network inference, we use transaction\nbootstrapping for better statistical validation. Investor categorization allows\nfor constant size networks and having more observations for each node, which is\nimportant in the inference especially for less liquid securities. Furthermore,\nwe observe that the window size used for averaging has a substantial effect on\nthe number of inferred relationships. We apply this procedure by analyzing a\nunique data set of Finnish shareholders during the period 2004-2009. We find\nthat households in the capital have high centrality in investor networks,\nwhich, under the theory of information channels in investor networks suggests\nthat they are well-informed investors.\n"
    },
    {
        "paper_id": 1709.00282,
        "authors": "Victor Olkhov",
        "title": "Econophysics of Business Cycles: Aggregate Economic Fluctuations, Mean\n  Risks and Mean Square Risks",
        "comments": "31 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper presents hydrodynamic-like model of business cycles aggregate\nfluctuations of economic and financial variables. We model macroeconomics as\nensemble of economic agents on economic space and agent's risk ratings play\nrole of their coordinates. Sum of economic variables of agents with coordinate\nx define macroeconomic variables as functions of time and coordinates x. We\ndescribe evolution and interactions between macro variables on economic space\nby hydrodynamic-like equations. Integral of macro variables over economic space\ndefines aggregate economic or financial variables as functions of time t only.\nHydrodynamic-like equations define fluctuations of aggregate variables. Motion\nof agents from low risk to high risk area and back define the origin for\nrepeated fluctuations of aggregate variables. Economic or financial variables\non economic space may define statistical moments like mean risk, mean square\nrisk and higher. Fluctuations of statistical moments describe phases of\nfinancial and economic cycles. As example we present a simple model relations\nbetween Assets and Revenue-on-Assets and derive hydrodynamic-like equations\nthat describe evolution and interaction between these variables.\nHydrodynamic-like equations permit derive systems of ordinary differential\nequations that describe fluctuations of aggregate Assets, Assets mean risks and\nAssets mean square risks. Our approach allows describe business cycle aggregate\nfluctuations induced by interactions between any number of economic or\nfinancial variables.\n"
    },
    {
        "paper_id": 1709.00468,
        "authors": "Flavia Sancier and Salah Mohammed",
        "title": "An Option Pricing Model with Memory",
        "comments": null,
        "journal-ref": null,
        "doi": "10.1002/ijop.12622",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We obtain option pricing formulas for stock price models in which the drift\nand volatility terms are functionals of a continuous history of the stock\nprices. That is, the stock dynamics follows a nonlinear stochastic functional\ndifferential equation. A model with full memory is obtained via approximation\nthrough a stock price model in which the continuous path dependence does not go\nup to the present: there is a memory gap. A strong solution is obtained by\nclosing the gap. Fair option prices are obtained through an equivalent (local)\nmartingale measure via Girsanov's Theorem and therefore are given in terms of a\nconditional expectation. The models maintain the completeness of the market and\nhave no arbitrage opportunities.\n"
    },
    {
        "paper_id": 1709.00641,
        "authors": "Daniel Bartl, Michael Kupper, Thibaut Lux, Antonis Papapantoleon,\n  Stephan Eckstein (appendix)",
        "title": "Marginal and dependence uncertainty: bounds, optimal transport, and\n  sharpness",
        "comments": "24 pages, 4 figures. Revised version with new title",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Motivated by applications in model-free finance and quantitative risk\nmanagement, we consider Fr\\'echet classes of multivariate distribution\nfunctions where additional information on the joint distribution is assumed,\nwhile uncertainty in the marginals is also possible. We derive optimal\ntransport duality results for these Fr\\'echet classes that extend previous\nresults in the related literature. These proofs are based on representation\nresults for increasing convex functionals and the explicit computation of the\nconjugates. We show that the dual transport problem admits an explicit solution\nfor the function $f=1_B$, where $B$ is a rectangular subset of $\\mathbb R^d$,\nand provide an intuitive geometric interpretation of this result. The improved\nFr\\'echet--Hoeffding bounds provide ad-hoc upper bounds for these Fr\\'echet\nclasses. We show that the improved Fr\\'echet--Hoeffding bounds are pointwise\nsharp for these classes in the presence of uncertainty in the marginals, while\na counterexample yields that they are not pointwise sharp in the absence of\nuncertainty in the marginals, even in dimension 2. The latter result sheds new\nlight on the improved Fr\\'echet--Hoeffding bounds, since Tankov [30] has showed\nthat, under certain conditions, these bounds are sharp in dimension 2.\n"
    },
    {
        "paper_id": 1709.01115,
        "authors": "Lijun Bo, Agostino Capponi, and Claudia Ceci",
        "title": "Risk-Minimizing Hedging of Counterparty Risk",
        "comments": "32 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We study dynamic hedging of counterparty risk for a portfolio of credit\nderivatives. Our empirically driven credit model consists of interacting\ndefault intensities which ramp up and then decay after the occurrence of credit\nevents. Using the Galtchouk-Kunita-Watanabe decomposition of the counterparty\nrisk price payment stream, we recover a closed-form representation for the risk\nminimizing strategy in terms of classical solutions to nonlinear recursive\nsystems of Cauchy problems. We discuss applications of our framework to the\nmost prominent class of credit derivatives, including credit swap and risky\nbond portfolios, as well as first-to-default claims.\n"
    },
    {
        "paper_id": 1709.01198,
        "authors": "Daniela Castro Camilo, Miguel de Carvalho, Jennifer Wadsworth",
        "title": "Time-Varying Extreme Value Dependence with Application to Leading\n  European Stock Markets",
        "comments": "23 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Extremal dependence between international stock markets is of particular\ninterest in today's global financial landscape. However, previous studies have\nshown this dependence is not necessarily stationary over time. We concern\nourselves with modeling extreme value dependence when that dependence is\nchanging over time, or other suitable covariate. Working within a framework of\nasymptotic dependence, we introduce a regression model for the angular density\nof a bivariate extreme value distribution that allows us to assess how extremal\ndependence evolves over a covariate. We apply the proposed model to assess the\ndynamics governing extremal dependence of some leading European stock markets\nover the last three decades, and find evidence of an increase in extremal\ndependence over recent years.\n"
    },
    {
        "paper_id": 1709.01268,
        "authors": "Dat Thanh Tran, Martin Magris, Juho Kanniainen, Moncef Gabbouj,\n  Alexandros Iosifidis",
        "title": "Tensor Representation in High-Frequency Financial Data for Price Change\n  Prediction",
        "comments": "accepted in SSCI 2017, typos fixed",
        "journal-ref": "IEEE Symposium Series on Computational Intelligence (SSCI), 2017",
        "doi": "10.1109/SSCI.2017.8280812",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Nowadays, with the availability of massive amount of trade data collected,\nthe dynamics of the financial markets pose both a challenge and an opportunity\nfor high frequency traders. In order to take advantage of the rapid, subtle\nmovement of assets in High Frequency Trading (HFT), an automatic algorithm to\nanalyze and detect patterns of price change based on transaction records must\nbe available. The multichannel, time-series representation of financial data\nnaturally suggests tensor-based learning algorithms. In this work, we\ninvestigate the effectiveness of two multilinear methods for the mid-price\nprediction problem against other existing methods. The experiments in a large\nscale dataset which contains more than 4 millions limit orders show that by\nutilizing tensor representation, multilinear models outperform vector-based\napproaches and other competing ones.\n"
    },
    {
        "paper_id": 1709.01292,
        "authors": "Ulrich Horst and Wei Xu",
        "title": "A Scaling Limit for Limit Order Books Driven by Hawkes Processes",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper we derive a scaling limit for an infinite dimensional limit\norder book model driven by Hawkes random measures. The dynamics of the incoming\norder flow is allowed to depend on the current market price as well as on a\nvolume indicator. With our choice of scaling the dynamics converges to a\ncoupled SDE-ODE system where limiting best bid and ask price processes follows\na diffusion dynamics, the limiting volume density functions follows an ODE in a\nHilbert space and the limiting order arrival and cancellation intensities\nfollow a Volterra-Fredholm integral equation.\n"
    },
    {
        "paper_id": 1709.01337,
        "authors": "Felix Moldenhauer and Marcin Pitera",
        "title": "Backtesting Expected Shortfall: a simple recipe?",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We propose a new backtesting framework for Expected Shortfall that could be\nused by the regulator. Instead of looking at the estimated capital reserve and\nthe realised cash-flow separately, one could bind them into the secured\nposition, for which risk measurement is much easier. Using this simple concept\ncombined with monotonicity of Expected Shortfall with respect to its target\nconfidence level we introduce a natural and efficient backtesting framework.\nOur test statistics is given by the biggest number of worst realisations for\nthe secured position that add up to a negative total. Surprisingly, this simple\nquantity could be used to construct an efficient backtesting framework for\nunconditional coverage of Expected Shortfall in a natural extension of the\nregulatory traffic-light approach for Value-at-Risk. While being easy to\ncalculate, the test statistic is based on the underlying duality between\ncoherent risk measures and scale-invariant performance measures.\n"
    },
    {
        "paper_id": 1709.01484,
        "authors": "Zura Kakushadze, Rakesh Raghubanshi and Willie Yu",
        "title": "Estimating Cost Savings from Early Cancer Diagnosis",
        "comments": "22 pages; a trivial grammar typo corrected",
        "journal-ref": "Data 2(3) (2017) 30",
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We estimate treatment cost-savings from early cancer diagnosis. For breast,\nlung, prostate and colorectal cancers and melanoma, which account for more than\n50% of new incidences projected in 2017, we combine published cancer treatment\ncost estimates by stage with incidence rates by stage at diagnosis. We\nextrapolate to other cancer sites by using estimated national expenditures and\nincidence rates. A rough estimate for the U.S. national annual treatment\ncost-savings from early cancer diagnosis is in 11 digits. Using this estimate\nand cost-neutrality, we also estimate a rough upper bound on the cost of a\nroutine early cancer screening test.\n"
    },
    {
        "paper_id": 1709.02015,
        "authors": "Rene Carmona and Kevin Webster",
        "title": "The microstructure of high frequency markets",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We present a novel approach to describing the microstructure of high\nfrequency trading using two key elements. First we introduce a new notion of\ninformed trader which we starkly contrast to current informed trader models. We\ndescribe the exact nature of the `superior information' high frequency traders\nhave access to, and how these agents differ from the more standard `insider\ntraders' described in past papers. This then leads to a model and an empirical\nanalysis of the data which strongly supports our claims. The second key element\nis a rigorous description of clearing conditions on a limit order book and how\nto derive correct formulas for such a market. From a theoretical point of view,\nthis allows the exact identification of two frictions in the market, one of\nwhich is intimately linked to our notion of `superior information'.\nEmpirically, we show that ignoring these frictions can misrepresent the wealth\nexchanged on the market by 50%. Finally, we showcase two applications of our\napproach: we measure the profits made by high frequency traders on NASDAQ and\nre-visit the standard Black - Scholes model to determine how trading frictions\nalter the delta-hedging strategy.\n"
    },
    {
        "paper_id": 1709.02129,
        "authors": "Marcel Ausloos, Roy Cerqueti, Tariq A. Mir",
        "title": "Data science for assessing possible tax income manipulation: The case of\n  Italy",
        "comments": "38 pages, 22 figures. To be published in Chaos, Solitons and Fractals",
        "journal-ref": "Chaos, Solitons & Fractals 104 (2017) 238-256",
        "doi": "10.1016/j.chaos.2017.08.012",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper explores a real-world fundamental theme under a data science\nperspective. It specifically discusses whether fraud or manipulation can be\nobserved in and from municipality income tax size distributions, through their\naggregation from citizen fiscal reports. The study case pertains to official\ndata obtained from the Italian Ministry of Economics and Finance over the\nperiod 2007-2011. All Italian (20) regions are considered. The considered data\nscience approach concretizes in the adoption of the Benford first digit law as\nquantitative tool. Marked disparities are found, - for several regions, leading\nto unexpected \"conclusions\". The most eye browsing regions are not the expected\nones according to classical imagination about Italy financial shadow matters.\n"
    },
    {
        "paper_id": 1709.02502,
        "authors": "Simon Clinet and Yoann Potiron",
        "title": "Testing if the market microstructure noise is fully explained by the\n  informational content of some variables from the limit order book",
        "comments": "77 pages, 4 figures, 8 tables",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper, we build tests for the presence of residual noise in a model\nwhere the market microstructure noise is a known parametric function of some\nvariables from the limit order book. The tests compare two distinct\nquasi-maximum likelihood estimators of volatility, where the related model\nincludes a residual noise in the market microstructure noise or not. The limit\ntheory is investigated in a general nonparametric framework. In the presence of\nresidual noise, we examine the central limit theory of the related\nquasi-maximum likelihood estimation approach.\n"
    },
    {
        "paper_id": 1709.02667,
        "authors": "Florian K\\\"uhnlenz, Pedro H. J. Nardelli, Santtu Karhinen, Rauli\n  Svento",
        "title": "Implementing Flexible Demand: Real-time Price vs. Market Integration",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper proposes an agent-based model that combines both spot and\nbalancing electricity markets. From this model, we develop a multi-agent\nsimulation to study the integration of the consumers' flexibility into the\nsystem. Our study identifies the conditions that real-time prices may lead to\nhigher electricity costs, which in turn contradicts the usual claim that such a\npricing scheme reduces cost. We show that such undesirable behavior is in fact\nsystemic. Due to the existing structure of the wholesale market, the predicted\ndemand that is used in the formation of the price is never realized since the\nflexible users will change their demand according to such established price. As\nthe demand is never correctly predicted, the volume traded through the\nbalancing markets increases, leading to higher overall costs. In this case, the\nsystem can sustain, and even benefit from, a small number of flexible users,\nbut this solution can never upscale without increasing the total costs. To\navoid this problem, we implement the so-called \"exclusive groups.\" Our results\nillustrate the importance of rethinking the current practices so that\nflexibility can be successfully integrated considering scenarios with and\nwithout intermittent renewable sources.\n"
    },
    {
        "paper_id": 1709.02701,
        "authors": "Antoine Kornprobst",
        "title": "Winning Investment Strategies Based on Financial Crisis Indicators",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The aim of this work is to create systematic trading strategies built upon\nseveral financial crisis indicators based on the spectral properties of market\ndynamics. Within the limitations of our framework and data, we will demonstrate\nthat our systematic trading strategies are able to make money, not as a result\nof pure luck but, in a reproducible way and while avoiding the pitfall of over\nfitting, as a result of the skill of the operators and their understanding and\nknowledge of the financial market. Using singular value decomposition (SVD)\ntechniques in order to compute all spectra in an efficient way, we have built\ntwo kinds of financial crisis indicators with a demonstrable power of\nprediction. Firstly, there are those that compare at every date the\ndistribution of the eigenvalues of a covariance or correlation matrix to a\ndistribution of reference representing either a calm or agitated market\nreference. Secondly, we have those that merely compute at every date a chosen\nspectral property (trace, spectral radius or Frobenius norm) of a covariance or\ncorrelation matrix. Aggregating the signals provided by all the indicators in\norder to minimize false positive errors, we then build systematic trading\nstrategies based on a discrete set of rules governing the investment decisions\nof the investor. Finally, we compare our active strategies to a passive\nreference as well as to random strategies in order to prove the usefulness of\nour approach and the added value provided by the out-of-sample predictive power\nof the financial crisis indicators upon which our systematic trading strategies\nare built.\n"
    },
    {
        "paper_id": 1709.03169,
        "authors": "Ting-Kam Leonard Wong",
        "title": "On portfolios generated by optimal transport",
        "comments": "19 pages, 4 figures. Revised",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  First introduced by Fernholz in stochastic portfolio theory, functionally\ngenerated portfolio allows its investment performance to be attributed to\ndirectly observable and easily interpretable market quantities. In previous\nworks we showed that Fernholz's multiplicatively generated portfolio has deep\nconnections with optimal transport and the information geometry of\nexponentially concave functions. Recently, Karatzas and Ruf introduced a new\nadditive portfolio generation whose relation with optimal transport was studied\nby Vervuurt. We show that additively generated portfolio can be interpreted in\nterms of the well-known dually flat information geometry of Bregman divergence.\nMoreover, we characterize, in a sense to be made precise, all possible forms of\nfunctional portfolio constructions that contain additive and multiplicative\ngenerations as special cases. Each construction involves a divergence\nfunctional on the unit simplex measuring the market volatility captured, and\nadmits a pathwise decomposition for the portfolio value. We illustrate with an\nempirical example.\n"
    },
    {
        "paper_id": 1709.03226,
        "authors": "R.J. Sak",
        "title": "Predictive Modeling: An Optimized and Dynamic Solution Framework for\n  Systematic Value Investing",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper defines systematic value investing as an empirical optimization\nproblem. Predictive modeling is introduced as a systematic value investing\nmethodology with dynamic and optimization features. A predictive modeling\nprocess is demonstrated using financial metrics from Gray & Carlisle and\nBuffett & Clark. A 31-year portfolio backtest (1985 - 2016) compares\nperformance between predictive models and Gray & Carlisle's Quantitative Value\nstrategy. A 26-year portfolio backtest (1990 - 2016) uses an expanded set of\npredictor variables to show financial performance improvements. This paper\nincludes secondary novel contributions. Quantitative definitions are provided\nfor Buffett & Clark's value investing metrics. The \"Sak ratio\" is proposed as\nan extension to the Benjamini-Hochberg procedure for the inferential\nidentification of false positive observations.\n"
    },
    {
        "paper_id": 1709.0331,
        "authors": "Fred Espen Benth, Marco Piccirilli, Tiziano Vargiolu",
        "title": "Additive energy forward curves in a Heath-Jarrow-Morton framework",
        "comments": "28 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  One of the peculiarities of power and gas markets is the delivery mechanism\nof forward contracts. The seller of a futures contract commits to deliver, say,\npower, over a certain period, while the classical forward is a financial\nagreement settled on a maturity date. Our purpose is to design a\nHeath-Jarrow-Morton framework for an additive, mean-reverting, multicommodity\nmarket consisting of forward contracts of any delivery period. The main\nassumption is that forward prices can be represented as affine functions of a\nuniversal source of randomness. This allows us to completely characterize the\nmodels which prevent arbitrage opportunities: this boils down to finding a\ndensity between a risk-neutral measure $\\mathbb{Q}$, such that the prices of\ntraded assets like forward contracts are true $\\mathbb{Q}$-martingales, and the\nreal world probability measure $\\mathbb{P}$, under which forward prices are\nmean-reverting. The Girsanov kernel for such a transformation turns out to be\nstochastic and unbounded in the diffusion part, while in the jump part the\nGirsanov kernel must be deterministic and bounded: thus, in this respect, we\nprove two results on the martingale property of stochastic exponentials. The\nfirst allows to validate measure changes made of two components: an\nEsscher-type density and a Girsanov transform with stochastic and unbounded\nkernel. The second uses a different approach and works for the case of\ncontinuous density. We apply this framework to two models: a generalized\nLucia-Schwartz model and a cross-commodity cointegrated market.\n"
    },
    {
        "paper_id": 1709.03535,
        "authors": "Yu-Jui Huang, Adrien Nguyen-Huu, Xun Yu Zhou",
        "title": "General Stopping Behaviors of Naive and Non-Committed Sophisticated\n  Agents, with Application to Probability Distortion",
        "comments": null,
        "journal-ref": "Mathematical Finance, Vol. 30 (2020), Issue 1, pp 310-340",
        "doi": "10.1111/mafi.12224",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We consider the problem of stopping a diffusion process with a payoff\nfunctional that renders the problem time-inconsistent. We study stopping\ndecisions of naive agents who reoptimize continuously in time, as well as\nequilibrium strategies of sophisticated agents who anticipate but lack control\nover their future selves' behaviors. When the state process is one dimensional\nand the payoff functional satisfies some regularity conditions, we prove that\nany equilibrium can be obtained as a fixed point of an operator. This operator\nrepresents strategic reasoning that takes the future selves' behaviors into\naccount. We then apply the general results to the case when the agents distort\nprobability and the diffusion process is a geometric Brownian motion. The\nproblem is inherently time-inconsistent as the level of distortion of a same\nevent changes over time. We show how the strategic reasoning may turn a naive\nagent into a sophisticated one. Moreover, we derive stopping strategies of the\ntwo types of agent for various parameter specifications of the problem,\nillustrating rich behaviors beyond the extreme ones such as \"never-stopping\" or\n\"never-starting\".\n"
    },
    {
        "paper_id": 1709.03611,
        "authors": "Zheqing Zhu and Jian-guo Liu and Lei Li",
        "title": "A Modified Levy Jump-Diffusion Model Based on Market Sentiment Memory\n  for Online Jump Prediction",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper, we propose a modified Levy jump diffusion model with market\nsentiment memory for stock prices, where the market sentiment comes from data\nmining implementation using Tweets on Twitter. We take the market sentiment\nprocess, which has memory, as the signal of Levy jumps in the stock price. An\nonline learning and optimization algorithm with the Unscented Kalman filter\n(UKF) is then proposed to learn the memory and to predict possible price jumps.\nExperiments show that the algorithm provides a relatively good performance in\nidentifying asset return trends.\n"
    },
    {
        "paper_id": 1709.03803,
        "authors": "Guosheng Hu and Yuxin Hu and Kai Yang and Zehao Yu and Flood Sung and\n  Zhihong Zhang and Fei Xie and Jianguo Liu and Neil Robertson and Timothy\n  Hospedales and Qiangwei Miemie",
        "title": "Deep Stock Representation Learning: From Candlestick Charts to\n  Investment Decisions",
        "comments": "Accepted to International Conference on Acoustics, Speech and Signal\n  Processing (ICASSP) 2018",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We propose a novel investment decision strategy (IDS) based on deep learning.\nThe performance of many IDSs is affected by stock similarity. Most existing\nstock similarity measurements have the problems: (a) The linear nature of many\nmeasurements cannot capture nonlinear stock dynamics; (b) The estimation of\nmany similarity metrics (e.g. covariance) needs very long period historic data\n(e.g. 3K days) which cannot represent current market effectively; (c) They\ncannot capture translation-invariance. To solve these problems, we apply\nConvolutional AutoEncoder to learn a stock representation, based on which we\npropose a novel portfolio construction strategy by: (i) using the deeply\nlearned representation and modularity optimisation to cluster stocks and\nidentify diverse sectors, (ii) picking stocks within each cluster according to\ntheir Sharpe ratio (Sharpe 1994). Overall this strategy provides low-risk\nhigh-return portfolios. We use the Financial Times Stock Exchange 100 Index\n(FTSE 100) data for evaluation. Results show our portfolio outperforms FTSE 100\nindex and many well known funds in terms of total return in 2000 trading days.\n"
    },
    {
        "paper_id": 1709.03943,
        "authors": "Kabin Kanjamapornkul, Richard Pin\\v{c}\\'ak, Sanphet Chunithpaisan,\n  Erik Barto\\v{s}",
        "title": "Support Spinor Machine",
        "comments": "18 pages, 12 figures, 6 tables",
        "journal-ref": "Digital Signal Processing 70 (2017) 59-72",
        "doi": "10.1016/j.dsp.2017.07.023",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We generalize a support vector machine to a support spinor machine by using\nthe mathematical structure of wedge product over vector machine in order to\nextend field from vector field to spinor field. The separated hyperplane is\nextended to Kolmogorov space in time series data which allow us to extend a\nstructure of support vector machine to a support tensor machine and a support\ntensor machine moduli space. Our performance test on support spinor machine is\ndone over one class classification of end point in physiology state of time\nseries data after empirical mode analysis and compared with support vector\nmachine test. We implement algorithm of support spinor machine by using\nHolo-Hilbert amplitude modulation for fully nonlinear and nonstationary time\nseries data analysis.\n"
    },
    {
        "paper_id": 1709.04059,
        "authors": "Oleg Malafeyev, Achal Awasthi, Kaustubh S. Kambekar",
        "title": "Random walks and market efficiency in Chinese and Indian equity markets",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Hypothesis of Market Efficiency is an important concept for the investors\nacross the globe holding diversified portfolios. With the world economy getting\nmore integrated day by day, more people are investing in global emerging\nmarkets. This means that it is pertinent to understand the efficiency of these\nmarkets. This paper tests for market efficiency by studying the impact of\nglobal financial crisis of 2008 and the recent Chinese crisis of 2015 on stock\nmarket efficiency in emerging stock markets of China and India. The data for\nlast 20 years was collected from both Bombay Stock Exchange (BSE200) and the\nShanghai Stock Exchange Composite Index and divided into four sub-periods, i.e.\nbefore financial crisis period (period-I), during recession (period-II), after\nrecession and before Chinese Crisis (periodIII) and from the start of Chinese\ncrisis till date (period- IV). Daily returns for the SSE and BSE were examined\nand tested for randomness using a combination of auto correlation tests, runs\ntests and unit root tests (Augmented Dickey-Fuller) for the entire sample\nperiod and the four sub-periods. The evidence from all these tests supports\nthat both the Indian and Chinese stock markets do not exhibit weak form of\nmarket efficiency. They do not follow random walk overall and in the first\nthree periods (1996 till the 2015) implying that recession did not impact the\nmarkets to a great extent, although the efficiency in percentage terms seems to\nbe increasing after the global financial crisis of 2008.\n"
    },
    {
        "paper_id": 1709.0407,
        "authors": "Christopher J. Rook",
        "title": "Multivariate Density Modeling for Retirement Finance",
        "comments": "Full C/C++ implementation is included in the appendix",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Prior to the financial crisis mortgage securitization models increased in\nsophistication as did products built to insure against losses. Layers of\ncomplexity formed upon a foundation that could not support it and as the\nfoundation crumbled the housing market followed. That foundation was the\nGaussian copula which failed to correctly model failure-time correlations of\nderivative securities in duress. In retirement, surveys suggest the greatest\nfear is running out of money and as retirement decumulation models become\nincreasingly sophisticated, large financial firms and robo-advisors may\nguarantee their success. Similar to an investment bank failure the event of\nretirement ruin is driven by outliers and correlations in times of stress. It\nwould be desirable to have a foundation able to support the increased\ncomplexity before it forms however the industry currently relies upon similar\nGaussian (or lognormal) dependence structures. We propose a multivariate\ndensity model having fixed marginals that is tractable and fits data which are\nskewed, heavy-tailed, multimodal, i.e., of arbitrary complexity allowing for a\nrich correlation structure. It is also ideal for stress-testing a retirement\nplan by fitting historical data seeded with black swan events. A preliminary\nsection reviews all concepts before they are used and fully documented C/C++\nsource code is attached making the research self-contained. Lastly, we take the\nopportunity to challenge existing retirement finance dogma and also review some\nrecent criticisms of retirement ruin probabilities and their suggested\nreplacement metrics.\n"
    },
    {
        "paper_id": 1709.04387,
        "authors": "Michele Longo and Alessandra Mainini",
        "title": "Welfare effects of information and rationality in portfolio decisions\n  under parameter uncertainty",
        "comments": "36 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We analyze and quantify, in a financial market with parameter uncertainty and\nfor a Constant Relative Risk Aversion investor, the utility effects of two\ndifferent boundedly rational (i.e., sub-optimal) investment strategies (namely,\nmyopic and unconditional strategies) and compare them between each other and\nwith the utility effect of full information. We show that effects are mainly\ncaused by full information and predictability, being the effect of learning\nmarginal. We also investigate the saver's decision of whether to manage her/his\nportfolio personally (DIY investor) or hire, against the payment of a\nmanagement fee, a professional investor and find that delegation is mainly\nmotivated by the belief that professional advisors are, depending on investment\nhorizon and risk aversion, either better informed (\"insiders\") or more capable\nof gathering and processing information rather than their ability of learning\nfrom financial data. In particular, for very short investment horizons,\ndelegation is primarily, if not exclusively, motivated by the beliefs that\nprofessional investors are better informed.\n"
    },
    {
        "paper_id": 1709.04415,
        "authors": "Xiaoguang Huo and Feng Fu",
        "title": "Risk-Aware Multi-Armed Bandit Problem with Application to Portfolio\n  Selection",
        "comments": "15 pages, 2 figures. This is one of the student project papers arsing\n  from the Mathematics REU program at Dartmouth 2017 Summer. See\n  https://math.dartmouth.edu/~reu/ for more info. Comments are always welcome",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Sequential portfolio selection has attracted increasing interests in the\nmachine learning and quantitative finance communities in recent years. As a\nmathematical framework for reinforcement learning policies, the stochastic\nmulti-armed bandit problem addresses the primary difficulty in sequential\ndecision making under uncertainty, namely the exploration versus exploitation\ndilemma, and therefore provides a natural connection to portfolio selection. In\nthis paper, we incorporate risk-awareness into the classic multi-armed bandit\nsetting and introduce an algorithm to construct portfolio. Through filtering\nassets based on the topological structure of financial market and combining the\noptimal multi-armed bandit policy with the minimization of a coherent risk\nmeasure, we achieve a balance between risk and return.\n"
    },
    {
        "paper_id": 1709.0462,
        "authors": "Daichi Tada, Hisashi Yamamoto and Takashi Shinzato",
        "title": "Random matrix approach for primal-dual portfolio optimization problems",
        "comments": "24 pages, 4 figures",
        "journal-ref": null,
        "doi": "10.7566/JPSJ.86.124804",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper, we revisit the portfolio optimization problems of the\nminimization/maximization of investment risk under constraints of budget and\ninvestment concentration (primal problem) and the maximization/minimization of\ninvestment concentration under constraints of budget and investment risk (dual\nproblem) for the case that the variances of the return rates of the assets are\nidentical. We analyze both optimization problems by using the Lagrange\nmultiplier method and the random matrix approach. Thereafter, we compare the\nresults obtained from our proposed approach with the results obtained in\nprevious work. Moreover, we use numerical experiments to validate the results\nobtained from the replica approach and the random matrix approach as methods\nfor analyzing both the primal and dual portfolio optimization problems.\n"
    },
    {
        "paper_id": 1709.05117,
        "authors": "Jean-Philippe Bouchaud, Stanislao Gualdi, Marco Tarzia, Francesco\n  Zamponi",
        "title": "Optimal Inflation Target: Insights from an Agent-Based Model",
        "comments": "19 pages, 6 figures. The paper is under review for the online journal\n  \"Economics\". The reviews are public at this link:\n  http://www.economics-ejournal.org/economics/discussionpapers/2017-64 . This\n  version has been modified and improved following the advice of the reviewers\n  and commentators",
        "journal-ref": "Economics: The Open-Access, Open-Assessment E-Journal, 12\n  (2018-15): 1-26",
        "doi": "10.5018/economics-ejournal.ja.2018-15",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Which level of inflation should Central Banks be targeting? We investigate\nthis issue in the context of a simplified Agent Based Model of the economy.\nDepending on the value of the parameters that describe the behaviour of agents\n(in particular inflation anticipations), we find a rich variety of behaviour at\nthe macro-level. Without any active monetary policy, our ABM economy can be in\na high inflation/high output state, or in a low inflation/low output state.\nHyper-inflation, deflation and \"business cycles\" between coexisting states are\nalso found. We then introduce a Central Bank with a Taylor rule-based inflation\ntarget, and study the resulting aggregate variables. Our main result is that\ntoo-low inflation targets are in general detrimental to a CB-monitored economy.\nOne symptom is a persistent under-realisation of inflation, perhaps similar to\nthe current macroeconomic situation. Higher inflation targets are found to\nimprove both unemployment and negative interest rate episodes. Our results are\ncompared with the predictions of the standard DSGE model.\n"
    },
    {
        "paper_id": 1709.05272,
        "authors": "Luciano Pietronero, Matthieu Cristelli, Andrea Gabrielli, Dario\n  Mazzilli, Emanuele Pugliese, Andrea Tacchella, and Andrea Zaccaria",
        "title": "Economic Complexity: \"Buttarla in caciara\" vs a constructive approach",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This note is a contribution to the debate about the optimal algorithm for\nEconomic Complexity that recently appeared on ArXiv [1, 2] . The authors of [2]\neventually agree that the ECI+ algorithm [1] consists just in a renaming of the\nFitness algorithm we introduced in 2012, as we explicitly showed in [3].\nHowever, they omit any comment on the fact that their extensive numerical tests\nclaimed to demonstrate that the same algorithm works well if they name it ECI+,\nbut not if its name is Fitness. They should realize that this eliminates any\ncredibility to their numerical methods and therefore also to their new\nanalysis, in which they consider many algorithms [2]. Since by their own\nadmission the best algorithm is the Fitness one, their new claim became that\nthe search for the best algorithm is pointless and all algorithms are alike.\nThis is exactly the opposite of what they claimed a few days ago and it does\nnot deserve much comments. After these clarifications we also present a\nconstructive analysis of the status of Economic Complexity, its algorithms, its\nsuccesses and its perspectives. For us the discussion closes here, we will not\nreply to further comments.\n"
    },
    {
        "paper_id": 1709.05287,
        "authors": "Aur\\'elien Alfonsi, Jacopo Corbetta and Benjamin Jourdain",
        "title": "Sampling of probability measures in the convex order by Wasserstein\n  projection",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper, for $\\mu$ and $\\nu$ two probability measures on $\\mathbb{R}^d$\nwith finite moments of order $\\rho\\ge 1$, we define the respective projections\nfor the $W_\\rho$-Wasserstein distance of $\\mu$ and $\\nu$ on the sets of\nprobability measures dominated by $\\nu$ and of probability measures larger than\n$\\mu$ in the convex order. The $W_2$-projection of $\\mu$ can be easily computed\nwhen $\\mu$ and $\\nu$ have finite support by solving a quadratic optimization\nproblem with linear constraints. In dimension $d=1$, Gozlan et al.~(2018) have\nshown that the projections do not depend on $\\rho$. We explicit their quantile\nfunctions in terms of those of $\\mu$ and $\\nu$. The motivation is the design of\nsampling techniques preserving the convex order in order to approximate\nMartingale Optimal Transport problems by using linear programming solvers. We\nprove convergence of the Wasserstein projection based sampling methods as the\nsample sizes tend to infinity and illustrate them by numerical experiments.\n"
    },
    {
        "paper_id": 1709.05392,
        "authors": "Bogang Jun, Aamena Alshamsi, Jian Gao, Cesar A Hidalgo",
        "title": "Relatedness, Knowledge Diffusion, and the Evolution of Bilateral Trade",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  During the last decades two important contributions have reshaped our\nunderstanding of international trade. First, countries trade more with those\nwith whom they share history, language, and culture, suggesting that trade is\nlimited by information frictions. Second, countries are more likely to start\nexporting products that are similar to their current exports, suggesting that\nknowledge diffusion among related industries is a key constrain shaping the\ndiversification of exports. But does knowledge about how to export to a\ndestination also diffuses among related products and geographic neighbors? Do\ncountries need to learn how to trade each product to each destination? Here, we\nuse bilateral trade data from 2000 to 2015 to show that countries are more\nlikely to increase their exports of a product to a destination when: (i) they\nexport related products to it, (ii) they export the same product to the\nneighbor of a destination, (iii) they have neighbors who export the same\nproduct to that destination. Then, we explore the magnitude of these effects\nfor new, nascent, and experienced exporters, (exporters with and without\ncomparative advantage in a product) and also for groups of products with\ndifferent level of technological sophistication. We find that the effects of\nproduct and geographic relatedness are stronger for new exporters, and also,\nthat the effect of product relatedness is stronger for more technologically\nsophisticated products. These findings support the idea that international\ntrade is shaped by information frictions that are reduced in the presence of\nrelated products and experienced geographic neighbors.\n"
    },
    {
        "paper_id": 1709.05519,
        "authors": "Paolo Di Tella, Martin Haubold, Martin Keller-Ressel",
        "title": "Semi-Static and Sparse Variance-Optimal Hedging",
        "comments": "4 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We consider hedging of a contingent claim by a 'semi-static' strategy\ncomposed of a dynamic position in one asset and static (buy-and-hold) positions\nin other assets. We give general representations of the optimal strategy and\nthe hedging error under the criterion of variance-optimality and provide\ntractable formulas using Fourier-integration in case of the Heston model. We\nalso consider the problem of optimally selecting a sparse semi-static hedging\nstrategy, i.e. a strategy which only uses a small subset of available hedging\nassets. The developed methods are illustrated in an extended numerical example\nwhere we compute a sparse semi-static hedge for a variance swap using European\noptions as static hedging assets.\n"
    },
    {
        "paper_id": 1709.05527,
        "authors": "Paolo Di Tella, Martin Haubold, Martin Keller-Ressel",
        "title": "Semi-Static Variance-Optimal Hedging in Stochastic Volatility Models\n  with Fourier Representation",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In a financial market model, we consider the variance-optimal semi-static\nhedging of a given contingent claim, a generalization of the classic\nvariance-optimal hedging. To obtain a tractable formula for the expected\nsquared hedging error and the optimal hedging strategy, we use a Fourier\napproach in a general multidimensional semimartingale factor model. As a\nspecial case, we recover existing results for variance-optimal hedging in\naffine stochastic volatility models. We apply the theory to set up a\nvariance-optimal semi-static hedging strategy for a variance swap in both the\nHeston and the 3/2-model, the latter of which is a non-affine stochastic\nvolatility model.\n"
    },
    {
        "paper_id": 1709.05529,
        "authors": "Weipin Wu and Jianjun Gao and Duan Li and Yun Shi",
        "title": "Explicit Solution for Constrained Stochastic Linear-Quadratic Control\n  with Multiplicative Noise",
        "comments": "32 Pages, 2 Figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We study in this paper a class of constrained linear-quadratic (LQ) optimal\ncontrol problem formulations for the scalar-state stochastic system with\nmultiplicative noise, which has various applications, especially in the\nfinancial risk management. The linear constraint on both the control and state\nvariables considered in our model destroys the elegant structure of the\nconventional LQ formulation and has blocked the derivation of an explicit\ncontrol policy so far in the literature. We successfully derive in this paper\nthe analytical control policy for such a class of problems by utilizing the\nstate separation property induced from its structure. We reveal that the\noptimal control policy is a piece-wise affine function of the state and can be\ncomputed off-line efficiently by solving two coupled Riccati equations. Under\nsome mild conditions, we also obtain the stationary control policy for infinite\ntime horizon. We demonstrate the implementation of our method via some\nillustrative examples and show how to calibrate our model to solve dynamic\nconstrained portfolio optimization problems.\n"
    },
    {
        "paper_id": 1709.05594,
        "authors": "Sandro Claudio Lera and Didier Sornette",
        "title": "GDP growth rates as confined L\\'evy flights",
        "comments": null,
        "journal-ref": "Phys. Rev. E 97, 012150 (2018)",
        "doi": "10.1103/PhysRevE.97.012150",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  A new model that combines economic growth rate fluctuations at the\nmicroscopic and macroscopic level is presented. At the microscopic level, firms\nare growing at different rates while also being exposed to idiosyncratic shocks\nat the firm and sector level. We describe such fluctuations as independent\nL\\'evy-stable fluctuations, varying over multiple orders of magnitude. These\nfluctuations are aggregated and measured at the macroscopic level in averaged\neconomic output quantities such as GDP. A fundamental question is thereby to\nwhat extend individual firm size fluctuations can have a noticeable impact on\nthe overall economy. We argue that this question can be answered by considering\nthe L\\'evy fluctuations as embedded in a steep confining potential well,\nensuring nonlinear mean-reversal behavior, without having to rely on\nmicroscopic details of the system. The steepness of the potential well directly\ncontrols the extend towards which idiosyncratic shocks to firms and sectors are\ndamped at the level of the economy. Additionally, the theory naturally accounts\nfor business cycles, represented in terms of a bimodal economic output\ndistribution, and thus connects two so far unrelated fields in economics. By\nanalyzing 200 years of US GDP growth rates, we find that the model is in good\nagreement with the data.\n"
    },
    {
        "paper_id": 1709.05823,
        "authors": "Guglielmo D'Amico and Filippo Petroni",
        "title": "A new approach to the modeling of financial volumes",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper we study the high frequency dynamic of financial volumes of\ntraded stocks by using a semi-Markov approach. More precisely we assume that\nthe intraday logarithmic change of volume is described by a weighted-indexed\nsemi-Markov chain model. Based on this assumptions we show that this model is\nable to reproduce several empirical facts about volume evolution like time\nseries dependence, intra-daily periodicity and volume asymmetry. Results have\nbeen obtained from a real data application to high frequency data from the\nItalian stock market from first of January 2007 until end of December 2010.\n"
    },
    {
        "paper_id": 1709.05837,
        "authors": "Qing-Qing Yang, Wai-Ki Ching, Jia-Wen Gu, Tak Kwong Wong",
        "title": "Optimal Liquidation Problems in a Randomly-Terminated Horizon",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper, we study optimal liquidation problems in a randomly-terminated\nhorizon. We consider the liquidation of a large single-asset portfolio with the\naim of minimizing a combination of volatility risk and transaction costs\narising from permanent and temporary market impact. Three different scenarios\nare analyzed under Almgren-Chriss's market impact model to explore the relation\nbetween optimal liquidation strategies and potential inventory risk arising\nfrom the uncertainty of the liquidation horizon. For cases where no closed-form\nsolutions can be obtained, we verify comparison principles for viscosity\nsolutions and characterize the value function as the unique viscosity solution\nof the associated Hamilton-Jacobi-Bellman (HJB) equation.\n"
    },
    {
        "paper_id": 1709.06279,
        "authors": "Takumi Fukunaga and Ken Umeno",
        "title": "Universal L\\'evy's stable law of stock market and its characterization",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Price fluctuations in financial markets can be characterized by L\\'evy's\nstable distribution, which is supported by the generalized central limit\nsystem. When the stable parameters were estimated from four different stock\nmarkets in long term, they similarly indicated an unique value. On the other\nhand, when analyzed in short term, parameters and the stock prices fluctuated\nwith correlation, which shows that the stock markets are instable.\n"
    },
    {
        "paper_id": 1709.06296,
        "authors": "Nikolaus Hautsch, Stefan Voigt",
        "title": "Large-Scale Portfolio Allocation Under Transaction Costs and Model\n  Uncertainty",
        "comments": null,
        "journal-ref": "Journal of Econometrics, 2019, 212(1), p. 221-240",
        "doi": "10.1016/j.jeconom.2019.04.028",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We theoretically and empirically study portfolio optimization under\ntransaction costs and establish a link between turnover penalization and\ncovariance shrinkage with the penalization governed by transaction costs. We\nshow how the ex ante incorporation of transaction costs shifts optimal\nportfolios towards regularized versions of efficient allocations. The\nregulatory effect of transaction costs is studied in an econometric setting\nincorporating parameter uncertainty and optimally combining predictive\ndistributions resulting from high-frequency and low-frequency data. In an\nextensive empirical study, we illustrate that turnover penalization is more\neffective than commonly employed shrinkage methods and is crucial in order to\nconstruct empirically well-performing portfolios.\n"
    },
    {
        "paper_id": 1709.06348,
        "authors": "Jos\\'e-Luis P\\'erez, Kazutoshi Yamazaki, Xiang Yu",
        "title": "On the Bail-Out Optimal Dividend Problem",
        "comments": "To appear in Journal of Optimization Theory and Applications.\n  Keywords: stochastic control, scale functions, refracted-reflected L\\'evy\n  processes, bail-out dividend problem",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper studies the optimal dividend problem with capital injection under\nthe constraint that the cumulative dividend strategy is absolutely continuous.\nWe consider an open problem of the general spectrally negative case and derive\nthe optimal solution explicitly using the fluctuation identities of the\nrefracted-reflected L\\'evy process. The optimal strategy as well as the value\nfunction are concisely written in terms of the scale function. Numerical\nresults are also provided to confirm the analytical conclusions.\n"
    },
    {
        "paper_id": 1709.0638,
        "authors": "I.D. Kolesin, O.A. Malafeyev, I.V. Zaitseva, A.N. Ermakova, D.V.\n  Shlaev",
        "title": "Modeling of the Labour Force Redistribution in Investment Projects with\n  Account of their Delay",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The mathematical model of the labour force redistribution in investment\nprojects is presented in the article. The redistribution mode of funds, labour\nforce in particular, according to the equal risk approach applied to the loss\nof some assets due to delay in all the investment projects is provided in the\nmodel. The sample of the developed model for three investment projects with the\nspecified labour force volumes and their defined unit costs at the particular\nmoment is given.\n"
    },
    {
        "paper_id": 1709.0648,
        "authors": "Igor D. S. Siciliani and Marcelo H. R. Tragtenberg",
        "title": "Kinetic theory and Brazilian income distribution",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We investigate the Brazilian personal income distribution using data from\nNational Household Sample Survey (PNAD), an annual research available by the\nBrazilian Institute of Geography and Statistics (IBGE). It provides general\ncharacteristics of the country's population. Using PNAD data background we also\nconfirm the effectiveness of a semi-empirical model that reconciles Pareto\npower-law for high-income people and Boltzmann- Gibbs distribution for the rest\nof population. We use three measures of income inequality: the Pareto index,\nthe average income and the crossover income. In order to cope with many\ndimensions of the income inequality, we calculate these three indices and also\nthe Gini coefficient for the general population as well as for two kinds of\npopulation dichotomies: black / indigenous / mixed race versus white / yellow;\nand men versus women. We also followed the time series of these indices for the\nperiod 2001-2014. The results suggest a decreasing of Brazilian income\ninequality over the selected period. Another important result is that\nhistorically-disadvantaged subgroups (Women and black / indigenous / mixed\nrace),that are the majority of the population, have a more equalitarian income\ndistribution. These groups have also a smaller monthly income than the others\nand this social structure remained virtually unchanged in the period of time.\n"
    },
    {
        "paper_id": 1709.06517,
        "authors": "Hyong-Chol O., Jong-Chol Kim and Il-Gwang Jon",
        "title": "Numerical analysis for a unified 2 factor model of structural and\n  reduced form types for corporate bonds with fixed discrete coupon",
        "comments": "15 pages, 12 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/publicdomain/zero/1.0/",
        "abstract": "  Conditions of Stability for explicit finite difference scheme and some\nresults of numerical analysis for a unified 2 factor model of structural and\nreduced form types for corporate bonds with fixed discrete coupon are provided.\nIt seems to be difficult to get solution formula for PDE model which\ngeneralizes Agliardi's structural model [1] for discrete coupon bonds into a\nunified 2 factor model of structural and reduced form types and we study a\nnumerical analysis for it by explicit finite difference scheme. These equations\nare parabolic equations with 3 variables and they include mixed derivatives, so\nthe explicit finite difference scheme is not stable in general. We find\nconditions for the explicit finite difference scheme to be stable, in the case\nthat it is stable, numerically compute the price of the bond and analyze its\ncredit spread and duration.\n"
    },
    {
        "paper_id": 1709.06641,
        "authors": "Zura Kakushadze and Willie Yu",
        "title": "Dead Alphas as Risk Factors",
        "comments": "9 pages; to appear as an Invited Editorial in Journal of Asset\n  Management",
        "journal-ref": "Journal of Asset Management 19(2) (2018) 110-115, Invited\n  Editorial",
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We give an explicit algorithm and source code for extracting equity risk\nfactors from dead (a.k.a. \"flatlined\" or \"hockey-stick\") alphas and using them\nto improve performance characteristics of good (tradable) alphas. In a\nnutshell, we use dead alphas to extract directions in the space of stock\nreturns along which there is no money to be made (and/or those bets are too\nvolatile). In practice the number of dead alphas can be large compared with the\nnumber of underlying stocks and care is required in identifying the aforesaid\ndirections.\n"
    },
    {
        "paper_id": 1709.06759,
        "authors": "Vladislav Gennadievich Malyshkin",
        "title": "Market Dynamics. On A Muse Of Cash Flow And Liquidity Deficit",
        "comments": "Adjustments to software description due to API changes in\n  arXiv:1903.11530",
        "journal-ref": null,
        "doi": "10.2139/ssrn.2748679",
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  A first attempt at obtaining market--directional information from a\nnon--stationary solution of the dynamic equation \"future price tends to the\nvalue that maximizes the number of shares traded per unit time\" [1] is\npresented. We demonstrate that the concept of price impact is poorly applicable\nto market dynamics. Instead, we consider the execution flow $I=dV/dt$ operator\nwith the \"impact from the future\" term providing information about\nnot--yet--executed trades. The \"impact from the future\" on $I$ can be directly\nestimated from the already--executed trades, the directional information on\nprice is then obtained from the experimentally observed fact that the $I$ and\n$p$ operators have the same eigenfunctions (the exact result in the dynamic\nimpact approximation $p=p(I)$). The condition for \"no information about the\nfuture\" is found and directional prediction quality is discussed. This work\nmakes a substantial contribution toward solving the ultimate market dynamics\nproblem: find evidence of existence (or proof of non--existence) of an\nautomated trading machine which consistently makes positive P\\&L on a free\nmarket as an autonomous agent (aka the existence of the market dynamics\nequation). The software with a reference implementation of the theory is\nprovided.\n"
    },
    {
        "paper_id": 1709.073,
        "authors": "Milla Siikanen, K\\k{e}stutis Baltakys, Juho Kanniainen, Ravi Vatrapu,\n  Raghava Mukkamala, Abid Hussain",
        "title": "Facebook drives behavior of passive households in stock markets",
        "comments": "This paper is forthcoming in Finance Research Letters",
        "journal-ref": null,
        "doi": "10.1016/j.frl.2018.03.020",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Recent studies using data on social media and stock markets have mainly\nfocused on predicting stock returns. Instead of predicting stock price\nmovements, we examine the relation between Facebook data and investors'\ndecision making in stock markets with a unique data on investors' transactions\non Nokia. We find that the decisions to buy versus sell are associated with\nFacebook data especially for passive households and also for nonprofit\norganizations. At the same time, it seems that more sophisticated\ninvestors---financial and insurance institutions---are behaving independently\nfrom Facebook activities.\n"
    },
    {
        "paper_id": 1709.07329,
        "authors": "Dmitry Kramkov, Sergio Pulido",
        "title": "Density of the set of probability measures with the martingale\n  representation property",
        "comments": "24 pages, forthcoming in Annals of Probability",
        "journal-ref": "Ann. Probab., Volume 47, Number 4 (2019), 2563-2581",
        "doi": "10.1214/18-AOP1321",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Let $\\psi$ be a multi-dimensional random variable. We show that the set of\nprobability measures $\\mathbb{Q}$ such that the $\\mathbb{Q}$-martingale\n$S^{\\mathbb{Q}}_t=\\mathbb{E}^{\\mathbb{Q}}\\left[\\psi\\lvert\\mathcal{F}_{t}\\right]$\nhas the Martingale Representation Property (MRP) is either empty or dense in\n$\\mathcal{L}_\\infty$-norm. The proof is based on a related result involving\nanalytic fields of terminal conditions $(\\psi(x))_{x\\in U}$ and probability\nmeasures $(\\mathbb{Q}(x))_{x\\in U}$ over an open set $U$. Namely, we show that\nthe set of points $x\\in U$ such that $S_t(x) =\n\\mathbb{E}^{\\mathbb{Q}(x)}\\left[\\psi(x)\\lvert\\mathcal{F}_{t}\\right]$ does not\nhave the MRP, either coincides with $U$ or has Lebesgue measure zero. Our study\nis motivated by the problem of endogenous completeness in financial economics.\n"
    },
    {
        "paper_id": 1709.07446,
        "authors": "Daniel Q. Naiman, Edward R. Scheinerman",
        "title": "Arbitrage and Geometry",
        "comments": "22 pages, 9 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This article introduces the notion of arbitrage for a situation involving a\ncollection of investments and a payoff matrix describing the return to an\ninvestor of each investment under each of a set of possible scenarios. We\nexplain the Arbitrage Theorem, discuss its geometric meaning, and show its\nequivalence to Farkas' Lemma. We then ask a seemingly innocent question: given\na random payoff matrix, what is the probability of an arbitrage opportunity?\nThis question leads to some interesting geometry involving hyperplane\narrangements and related topics.\n"
    },
    {
        "paper_id": 1709.07527,
        "authors": "Mogens Graf Plessen, Alberto Bemporad",
        "title": "A posteriori multi-stage optimal trading under transaction costs and a\n  diversification constraint",
        "comments": "25 pages, 4 figures, 6 tables",
        "journal-ref": "The Journal of Trading Summer 2018, 13 (3) 67-83",
        "doi": "10.3905/jot.2018.1.064",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper presents a simple method for a posteriori (historical)\nmulti-variate multi-stage optimal trading under transaction costs and a\ndiversification constraint. Starting from a given amount of money in some\ncurrency, we analyze the stage-wise optimal allocation over a time horizon with\npotential investments in multiple currencies and various assets. Three variants\nare discussed, including unconstrained trading frequency, a fixed number of\ntotal admissable trades, and the waiting of a specific time-period after every\nexecuted trade until the next trade. The developed methods are based on\nefficient graph generation and consequent graph search, and are evaluated\nquantitatively on real-world data. The fundamental motivation of this work is\npreparatory labeling of financial time-series data for supervised machine\nlearning.\n"
    },
    {
        "paper_id": 1709.07682,
        "authors": "Dietmar Pfeifer, Andreas M\\\"andle, Olena Ragulina",
        "title": "New copulas based on general partitions-of-unity and their applications\n  to risk management (part II)",
        "comments": "12 pages, 24 figures, 3 tables, 10 references",
        "journal-ref": "Dependence Modeling (2017), 246 - 255",
        "doi": "10.1515/demo-2017-0014",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We present a constructive and self-contained approach to data driven infinite\npartition-of-unity copulas that were recently introduced in the literature. In\nparticular, we consider negative binomial and Poisson copulas and present a\nsolution to the problem of fitting such copulas to highly asymmetric data in\narbitrary dimensions.\n"
    },
    {
        "paper_id": 1709.0796,
        "authors": "Tudorel Andrei, Bogdan Oancea, Peter Richmond, Gurjeet Dhesi and\n  Claudiu Herteliu",
        "title": "Decomposition of the Inequality of Income Distribution by Income Types -\n  Application for Romania",
        "comments": "12 pages, 5 figures, 4 tables, 49 references",
        "journal-ref": "Entropy, volume 19, issue 9, 2017, 430",
        "doi": "10.3390/e19090430",
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  This paper identifies the salient factors that characterize the inequality\nincome distribution for Romania. Data analysis is rigorously carried out using\nsophisticated techniques borrowed from classical statistics (Theil).\nDecomposition of the inequalities measured by the Theil index is also\nperformed. This study relies on an exhaustive (11.1 million records for 2014)\ndata-set for total personal gross income of Romanian citizens.\n"
    },
    {
        "paper_id": 1709.08023,
        "authors": "S. Ali Pourmousavi, Mahdi Behrangrad, Ali Jahanbani Ardakani, M.\n  Hashem Nehrir",
        "title": "Ownership Cost Calculations for Distributed Energy Resources Using\n  Uncertainty and Risk Analyses",
        "comments": "8 pages, 7 figures, 3 tables",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Ownership cost calculation plays an important role in optimal operation of\ndistributed energy resources (DERs) and microgrids (MGs) in the future power\nsystem, known as smart grid. In this paper, a general framework for ownership\ncost calculation is proposed using uncertainty and risk analyses. Four\nownership cost calculation approaches are introduced and compared based on\ntheir associated risk values. Finally, the best method is chosen based on a\nseries of simulation results, performed for a typical diesel generator (DiG).\nAlthough simulation results are given for a DiG (as commonly used in MGs), the\nproposed approaches can be applied to other MG components, such as batteries,\nwith slight modifications, as presented in this paper. The analyses and\nproposed approaches can be useful in MG optimal design, optimal power flow, and\nmarket-based operation of the smart grid for accurate operational cost\ncalculations.\n"
    },
    {
        "paper_id": 1709.08075,
        "authors": "Ivan Guo, Gr\\'egoire Loeper, Shiyi Wang",
        "title": "Local Volatility Calibration by Optimal Transport",
        "comments": null,
        "journal-ref": "2017 MATRIX Annals, Vol. 2, 2019, 51-64",
        "doi": "10.1007/978-3-030-04161-8",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The calibration of volatility models from observable option prices is a\nfundamental problem in quantitative finance. The most common approach among\nindustry practitioners is based on the celebrated Dupire's formula [6], which\nrequires the knowledge of vanilla option prices for a continuum of strikes and\nmaturities that can only be obtained via some form of price interpolation. In\nthis paper, we propose a new local volatility calibration technique using the\ntheory of optimal transport. We formulate a time continuous martingale optimal\ntransport problem, which seeks a martingale diffusion process that matches the\nknown densities of an asset price at two different dates, while minimizing a\nchosen cost function. Inspired by the seminal work of Benamou and Brenier [1],\nwe formulate the problem as a convex optimization problem, derive its dual\nformulation, and solve it numerically via an augmented Lagrangian method and\nthe alternative direction method of multipliers (ADMM) algorithm. The solution\neffectively reconstructs the dynamic of the asset price between the two dates\nby recovering the optimal local volatility function, without requiring any time\ninterpolation of the option prices.\n"
    },
    {
        "paper_id": 1709.0809,
        "authors": "Aurelio F. Bariviera",
        "title": "The inefficiency of Bitcoin revisited: a dynamic approach",
        "comments": "Economics Letters, 2017",
        "journal-ref": null,
        "doi": "10.1016/j.econlet.2017.09.013",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This letter revisits the informational efficiency of the Bitcoin market. In\nparticular we analyze the time-varying behavior of long memory of returns on\nBitcoin and volatility 2011 until 2017, using the Hurst exponent. Our results\nare twofold. First, R/S method is prone to detect long memory, whereas DFA\nmethod can discriminate more precisely variations in informational efficiency\nacross time. Second, daily returns exhibit persistent behavior in the first\nhalf of the period under study, whereas its behavior is more informational\nefficient since 2014. Finally, price volatility, measured as the logarithmic\ndifference between intraday high and low prices exhibits long memory during all\nthe period. This reflects a different underlying dynamic process generating the\nprices and volatility.\n"
    },
    {
        "paper_id": 1709.08134,
        "authors": "Svetlozar Rachev, Frank J. Fabozzi, Boryana Racheva-Iotova, Abootaleb\n  Shirvani",
        "title": "Option Pricing with Greed and Fear Factor: The Rational Finance Approach",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We explain the main concepts of Prospect Theory and Cumulative Prospect\nTheory within the framework of rational dynamic asset pricing theory. We derive\noption pricing formulas when asset returns are altered with a generalized\nProspect Theory value function or a modified Prelec weighting probability\nfunction and introduce new parametric classes for Prospect Theory value\nfunctions and weighting probability functions consistent with rational dynamic\npricing Theory. We study the behavioral finance notion of greed and fear from\nthe point of view of rational dynamic asset pricing theory and derive the\ncorresponding option pricing formulas in the case of asset returns that follow\ncontinuous diffusion or discrete binomial trees.\n"
    },
    {
        "paper_id": 1709.08188,
        "authors": "Carol Alexander and Johannes Rauch",
        "title": "The Aggregation Property and its Applications to Realised Higher Moments",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We develop a general multivariate aggregation property which encompasses the\ndistinct versions of the property that were introduced by Neuberger [2012] and\nBondarenko [2014] independently. This way, we classify new types of model-free\nrealised characteristics for which risk premia may be estimated without bias.\nWe focus on the aggregation property for multivariate martingales and log\nmartingales, and then define realised third and fourth moments which allow\nlong-term higher-moment risk premia to be measured, efficiently and without\nbias, using high-frequency returns.\n"
    },
    {
        "paper_id": 1709.08238,
        "authors": "Martin D. Gould, Nikolaus Hautsch, Sam D. Howison, and Mason A. Porter",
        "title": "Counterparty Credit Limits: The Impact of a Risk-Mitigation Measure on\n  Everyday Trading",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  A counterparty credit limit (CCL) is a limit that is imposed by a financial\ninstitution to cap its maximum possible exposure to a specified counterparty.\nCCLs help institutions to mitigate counterparty credit risk via selective\ndiversification of their exposures. In this paper, we analyze how CCLs impact\nthe prices that institutions pay for their trades during everyday trading. We\nstudy a high-quality data set from a large electronic trading platform in the\nforeign exchange spot market, which enables institutions to apply CCLs. We find\nempirically that CCLs had little impact on the vast majority of trades in this\ndata. We also study the impact of CCLs using a new model of trading. By\nsimulating our model with different underlying CCL networks, we highlight that\nCCLs can have a major impact in some situations.\n"
    },
    {
        "paper_id": 1709.08516,
        "authors": "Marcus Cordi, Damien Challet, Ioane Muni Toke",
        "title": "Testing the causality of Hawkes processes with time reversal",
        "comments": "13 pages, 14 figures, 2 tables",
        "journal-ref": null,
        "doi": "10.1088/1742-5468/aaac3f",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We show that univariate and symmetric multivariate Hawkes processes are only\nweakly causal: the true log-likelihoods of real and reversed event time vectors\nare almost equal, thus parameter estimation via maximum likelihood only weakly\ndepends on the direction of the arrow of time. In ideal (synthetic) conditions,\ntests of goodness of parametric fit unambiguously reject backward event times,\nwhich implies that inferring kernels from time-symmetric quantities, such as\nthe autocovariance of the event rate, only rarely produce statistically\nsignificant fits. Finally, we find that fitting financial data with\nmany-parameter kernels may yield significant fits for both arrows of time for\nthe same event time vector, sometimes favouring the backward time direction.\nThis goes to show that a significant fit of Hawkes processes to real data with\nflexible kernels does not imply a definite arrow of time unless one tests it.\n"
    },
    {
        "paper_id": 1709.08621,
        "authors": "Alessandra Cretarola, Gianna Fig\\`a-Talamanca, Marco Patacca",
        "title": "A sentiment-based model for the BitCoin: theory, estimation and option\n  pricing",
        "comments": "39, pages, 7 figures, 16 tables. arXiv admin note: substantial text\n  overlap with arXiv:1702.00215",
        "journal-ref": null,
        "doi": "10.1007/s10203-019-00262-x",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In recent literature it is claimed that BitCoin price behaves more likely to\na volatile stock asset than a currency and that changes in its price are\ninfluenced by sentiment about the BitCoin system itself; in Kristoufek [10] the\nauthor analyses transaction based as well as popularity based potential drivers\nof the BitCoin price finding positive evidence. Here, we endorse this finding\nand consider a bivariate model in continuous time to describe the price\ndynamics of one BitCoin as well as a second factor, affecting the price itself,\nwhich represents a sentiment indicator. We prove that the suggested model is\narbitrage-free under a mild condition and, based on risk-neutral evaluation, we\nobtain a closed formula to approximate the price of European style derivatives\non the BitCoin. By applying the same approximation technique to the joint\nlikelihood of a discrete sample of the bivariate process, we are also able to\nfit the model to market data. This is done by using both the Volume and the\nnumber of Google searches as possible proxies for the sentiment factor.\nFurther, the performance of the pricing formula is assessed on a sample of\nmarket option prices obtained by the website deribit.com.\n"
    },
    {
        "paper_id": 1709.08755,
        "authors": "Imre Kondor, G\\'abor Papp, Fabio Caccioli",
        "title": "Analytic approach to variance optimization under an $\\ell_1$ constraint",
        "comments": "31 pages, 8 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The optimization of the variance supplemented by a budget constraint and an\nasymmetric $\\ell_1$ regularizer is carried out analytically by the replica\nmethod borrowed from the theory of disordered systems. The asymmetric\nregularizer allows us to penalize short and long positions differently, so the\npresent treatment includes the no-short-constrained portfolio optimization\nproblem as a special case. Results are presented for the out-of-sample and the\nin-sample estimator of the regularized variance, the relative estimation error,\nthe density of the assets eliminated from the portfolio by the regularizer, and\nthe distribution of the optimal portfolio weights. We have studied the\ndependence of these quantities on the ratio $r$ of the portfolio's dimension\n$N$ to the sample size $T$, and on the strength of the regularizer. We have\nchecked the analytic results by numerical simulations, and found general\nagreement. Regularization extends the interval where the optimization can be\ncarried out, and suppresses the large sample fluctuations, but the performance\nof $\\ell_1$ regularization is rather disappointing: if the sample size is large\nrelative to the dimension, i.e. $r$ is small, the regularizer does not play any\nrole, while for $r$'s where the regularizer starts to be felt the estimation\nerror is already so large as to make the whole optimization exercise pointless.\nWe find that the $\\ell_1$ regularization can eliminate at most half the assets\nfrom the portfolio, corresponding to this there is a critical ratio $r=2$\nbeyond which the $\\ell_1$ regularized variance cannot be optimized: the\nregularized variance becomes constant over the simplex. These facts do not seem\nto have been noticed in the literature.\n"
    },
    {
        "paper_id": 1709.09068,
        "authors": "Stoyan V. Stoyanov, Svetlozar T. Rachev, Stefan Mittnik, Frank J.\n  Fabozzi",
        "title": "Pricing derivatives in Hermite markets",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We present a new framework for Hermite fractional financial markets,\ngeneralizing the fractional Brownian motion and fractional Rosenblatt markets.\nConsidering pure and mixed Hermite markets, we introduce a strategy-specific\narbitrage tax on the rate of transaction volume acceleration of the hedging\nportfolio as the prices of risky assets change, allowing us to transform\nHermite markets with arbitrage opportunities to markets with no arbitrage\nopportunities within the class of Markov trading strategies.\n"
    },
    {
        "paper_id": 1709.09252,
        "authors": "Delia Coculescu and Monique Jeanblanc",
        "title": "Some No-Arbitrage Rules For Converging Asset Prices under Short-Sales\n  Constraints",
        "comments": "23 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Under short sales prohibitions, no free lunch with vanishing risk (NFLVR-S)\nis known to be equivalent to the existence of an equivalent supermartingale\nmeasure for the price processes (Pulido [22]). For two given price processes,\nwe translate the property (NFLVR-S) in terms of so called structure conditions\nand we introduce the concept of fundamental supermartingale measure. When a\ncertain condition necessary to the construction of the fundamental martingale\nmeasure is not fulfilled, we provide the corresponding arbitrage portfolios.\nThe motivation of our study lies in understanding the particular case of\nconverging prices, i.e., that are going to cross at a bounded random time.\n"
    },
    {
        "paper_id": 1709.09255,
        "authors": "Delia Coculescu and Gabriele Visentin",
        "title": "A default system with overspilling contagion",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In classical contagion models, default systems are Markovian conditionally on\nthe observation of their stochastic environment, with interacting intensities.\nThis necessitates that the environment evolves autonomously and is not\ninfluenced by the history of the default events. We extend the classical\nliterature and allow a default system to have a contagious impact on its\nenvironment. In our framework, contagion can either be contained within the\ndefault system (i.e., direct contagion from a counterparty to another) or spill\nfrom the default system over its environment (indirect contagion). This type of\nmodel is of interest whenever one wants to capture within a model possible\nimpacts of the defaults of a class of debtors on the more global economy and\nvice versa.\n"
    },
    {
        "paper_id": 1709.09373,
        "authors": "Luigi Di Caro, Marco Guerzoni, Massimiliano Nuccio, Giovanni Siragusa",
        "title": "A Bimodal Network Approach to Model Topic Dynamics",
        "comments": "topic modeling, LDA, bimodal network, topic dynamics, economic\n  thought 26 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper presents an intertemporal bimodal network to analyze the evolution\nof the semantic content of a scientific field within the framework of topic\nmodeling, namely using the Latent Dirichlet Allocation (LDA). The main\ncontribution is the conceptualization of the topic dynamics and its\nformalization and codification into an algorithm. To benchmark the\neffectiveness of this approach, we propose three indexes which track the\ntransformation of topics over time, their rate of birth and death, and the\nnovelty of their content. Applying the LDA, we test the algorithm both on a\ncontrolled experiment and on a corpus of several thousands of scientific papers\nover a period of more than 100 years which account for the history of the\neconomic thought.\n"
    },
    {
        "paper_id": 1709.09442,
        "authors": "Yan Dolinsky and Jonathan Zouari",
        "title": "Market Delay and G-expectations",
        "comments": "14 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We study super-replication of contingent claims in markets with delayed\nfiltration. The first result in this paper reveals that in the Black--Scholes\nmodel with constant delay the super-replication price is prohibitively costly\nand leads to trivial buy-and-hold strategies. Our second result says that the\nscaling limit of super--replication prices for binomial models with a fixed\nnumber of times of delay $H$ is equal to the $G$--expectation with volatility\nuncertainty interval $[0,\\sigma\\sqrt{H+1}]$.\n"
    },
    {
        "paper_id": 1709.09465,
        "authors": "Romain Blanchard, Laurence Carassus",
        "title": "Convergence of utility indifference prices to the superreplication price\n  in a multiple-priors framework",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper formulates an utility indifference pricing model for investors\ntrading in a discrete time financial market under non-dominated model\nuncertainty. The investors preferences are described by strictly increasing\nconcave random functions defined on the positive axis. We prove that under\nsuitable conditions the multiple-priors utility indifference prices of a\ncontingent claim converge to its multiple-priors superreplication price.\n  We also revisit the notion of certainty equivalent for random utility\nfunctions and establish its relation with the absolute risk aversion.\n"
    },
    {
        "paper_id": 1709.09495,
        "authors": "Carlo Brugna and Giuseppe Toscani",
        "title": "Kinetic models for goods exchange in a multi-agent market",
        "comments": null,
        "journal-ref": null,
        "doi": "10.1016/j.physa.2018.02.070",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We introduce a system of kinetic equations describing an exchange market\nconsisting of two populations of agents (dealers and speculators) expressing\nthe same preferences for two goods, but applying different strategies in their\nexchanges. We describe the trading of the goods by means of some fundamental\nrules in price theory, in particular by using Cobb-Douglas utility functions\nfor the exchange. The strategy of the speculators is to recover maximal utility\nfrom the trade by suitably acting on the percentage of goods which are\nexchanged. This microscopic description leads to a system of linear\nBoltzmann-type equations for the probability distributions of the goods on the\ntwo populations, in which the post-interaction variables depend from the\npre-interaction ones in terms of the mean quantities of the goods present in\nthe market. In this case, it is shown analytically that the strategy of the\nspeculators can drive the price of the two goods towards a zone in which there\nis a marked utility for their group. Also, the general system of nonlinear\nkinetic equations of Boltzmann type for the probability distributions of the\ngoods on the two populations is described in details. Numerical experiments\nthen show how the policy of speculators can modify the final price of goods in\nthis nonlinear setting.\n"
    },
    {
        "paper_id": 1709.09822,
        "authors": "Sang Il Lee, Seong Joon Yoo",
        "title": "Threshold-Based Portfolio: The Role of the Threshold and Its\n  Applications",
        "comments": "20 pages, 7 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper aims at developing a new method by which to build a data-driven\nportfolio featuring a target risk-return. We first present a comparative study\nof recurrent neural network models (RNNs), including a simple RNN, long\nshort-term memory (LSTM), and gated recurrent unit (GRU) for selecting the best\npredictor to use in portfolio construction. The models are applied to the\ninvestment universe consisted of ten stocks in the S&P500. The experimental\nresults shows that LSTM outperforms the others in terms of hit ratio of\none-month-ahead forecasts. We then build predictive threshold-based portfolios\n(TBPs) that are subsets of the universe satisfying given threshold criteria for\nthe predicted returns. The TBPs are rebalanced monthly to restore equal weights\nto each security within the TBPs. We find that the risk and return profile of\nthe realized TBP represents a monotonically increasing frontier on the\nrisk-return plane, where the equally weighted portfolio (EWP) of all ten stocks\nplays a role in their lower bound. This shows the availability of TBPs in\ntargeting specific risk-return levels, and an EWP based on all the assets plays\na role in the reference portfolio of TBPs. In the process, thresholds play\ndominant roles in characterizing risk, return, and the prediction accuracy of\nthe subset. The TBP is more data-driven in designing portfolio target risk and\nreturn than existing ones, in the sense that it requires no prior knowledge of\nfinance such as financial assumptions, financial mathematics, or expert\ninsights. In a practical application, we present the TBP management procedure\nfor a time horizon extending over multiple time periods; we also discuss their\napplication to mean-variance portfolios to reduce estimation risk.\n"
    },
    {
        "paper_id": 1709.09858,
        "authors": "Marco Torregrossa and Giuseppe Toscani",
        "title": "Wealth distribution in presence of debts. A Fokker--Planck description",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We consider here a Fokker--Planck equation with variable coefficient of\ndiffusion which appears in the modeling of the wealth distribution in a\nmulti-agent society. At difference with previous studies, to describe a society\nin which agents can have debts, we allow the wealth variable to be negative. It\nis shown that, even starting with debts, if the initial mean wealth is assumed\npositive, the solution of the Fokker--Planck equation is such that debts are\nabsorbed in time, and a unique equilibrium density located in the positive part\nof the real axis will be reached.\n"
    },
    {
        "paper_id": 1709.09955,
        "authors": "Anna Casta\\~ner (UB), M Merc\\`e Claramunt (UB)",
        "title": "Equilibrium distributions and discrete Schur-constant models",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper introduces Schur-constant equilibrium distribution models of\ndimension n for arithmetic non-negative random variables. Such a model is\ndefined through the (several orders) equilibrium distributions of a univariate\nsurvival function. First, the bivariate case is considered and analyzed in\ndepth, stressing the main characteristics of the Poisson case. The analysis is\nthen extended to the multivariate case. Several properties are derived,\nincluding the implicit correlation and the distribution of the sum.\n"
    },
    {
        "paper_id": 1709.10141,
        "authors": "Vicky Henderson, Kamil Klad\\'ivko, Michael Monoyios, Christoph\n  Reisinger",
        "title": "Executive stock option exercise with full and partial information on a\n  drift change point",
        "comments": "48 pages, final version, accepted for publication in SIAM Journal on\n  Financial Mathematics",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We analyse the optimal exercise of an executive stock option (ESO) written on\na stock whose drift parameter falls to a lower value at a change point, an\nexponentially distributed random time independent of the Brownian motion\ndriving the stock. Two agents, who do not trade the stock, have differing\ninformation on the change point, and seek to optimally exercise the option by\nmaximising its discounted payoff under the physical measure. The first agent\nhas full information, and observes the change point. The second agent has\npartial information and filters the change point from price observations. This\nscenario is designed to mimic the positions of two employees of varying\nseniority, a fully informed executive and a partially informed less senior\nemployee, each of whom receives an ESO. The partial information scenario yields\na model under the observation filtration $\\widehat{\\mathbb{F}}$ in which the\nstock drift becomes a diffusion driven by the innovations process, an\n$\\widehat{\\mathbb{F}}$-Brownian motion also driving the stock under\n$\\widehat{\\mathbb{F}}$, and the partial information optimal stopping value\nfunction has two spatial dimensions. We rigorously characterise the free\nboundary PDEs for both agents, establish shape and regularity properties of the\nassociated optimal exercise boundaries, and prove the smooth pasting property\nin both information scenarios, exploiting some stochastic flow ideas to do so\nin the partial information case. We develop finite difference algorithms to\nnumerically solve both agents' exercise and valuation problems and illustrate\nthat the additional information of the fully informed agent can result in\nexercise patterns which exploit the information on the change point, lending\ncredence to empirical studies which suggest that privileged information of bad\nnews is a factor leading to early exercise of ESOs prior to poor stock price\nperformance.\n"
    },
    {
        "paper_id": 1709.10277,
        "authors": "Kartik Anand, Jonathan Khedair, and Reimer Kuehn",
        "title": "A Structural Model for Fluctuations in Financial Markets",
        "comments": "16 pages, 8 (multi-part) figures",
        "journal-ref": "Phys. Rev. E 97, 052312 (2018)",
        "doi": "10.1103/PhysRevE.97.052312",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper we provide a comprehensive analysis of a structural model for\nthe dynamics of prices of assets traded in a market originally proposed in [1].\nThe model takes the form of an interacting generalization of the geometric\nBrownian motion model. It is formally equivalent to a model describing the\nstochastic dynamics of a system of analogue neurons, which is expected to\nexhibit glassy properties and thus many meta-stable states in a large portion\nof its parameter space. We perform a generating functional analysis,\nintroducing a slow driving of the dynamics to mimic the effect of slowly\nvarying macro-economic conditions. Distributions of asset returns over various\ntime separations are evaluated analytically and are found to be fat-tailed in a\nmanner broadly in line with empirical observations. Our model also allows to\nidentify collective, interaction mediated properties of pricing distributions\nand it predicts pricing distributions which are significantly broader than\ntheir non-interacting counterparts, if interactions between prices in the model\ncontain a ferro-magnetic bias. Using simulations, we are able to substantiate\none of the main hypotheses underlying the original modelling, viz. that the\nphenomenon of volatility clustering can be rationalised in terms of an\ninterplay between the dynamics within meta-stable states and the dynamics of\noccasional transitions between them.\n"
    },
    {
        "paper_id": 1709.10295,
        "authors": "J\\'er\\^ome Spielmann (LAREMA)",
        "title": "Classification of the Bounds on the Probability of Ruin for L{\\'e}vy\n  Processes with Light-tailed Jumps",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this note, we study the ultimate ruin probabilities of a real-valued\nL{\\'e}vy process X with light-tailed negative jumps. It is well-known that, for\nsuch L{\\'e}vy processes, the probability of ruin decreases as an exponential\nfunction with a rate given by the root of the Laplace exponent, when the\ninitial value goes to infinity. Under the additional assumption that X has\nintegrable positive jumps, we show how a finer analysis of the Laplace exponent\ngives in fact a complete description of the bounds on the probability of ruin\nfor this class of L{\\'e}vy processes. This leads to the identification of a\ncase that is not considered in the literature and for which we give an example.\nWe then apply the result to various risk models and in particular the\nCram{\\'e}r-Lundberg model perturbed by Brownian motion.\n"
    },
    {
        "paper_id": 1709.10384,
        "authors": "Donatella Danielli, Arshak Petrosyan, and Camelia A. Pop",
        "title": "Obstacle problems for nonlocal operators",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We prove existence, uniqueness, and regularity of viscosity solutions to the\nstationary and evolution obstacle problems defined by a class of nonlocal\noperators that are not stable-like and may have supercritical drift. We give\nsufficient conditions on the coefficients of the operator to obtain H\\\"older\nand Lipschitz continuous solutions. The class of nonlocal operators that we\nconsider include non-Gaussian asset price models widely used in mathematical\nfinance, such as Variance Gamma Processes and Regular L\\'evy Processes of\nExponential type. In this context, the viscosity solutions that we analyze\ncoincide with the prices of perpetual and finite expiry American options.\n"
    },
    {
        "paper_id": 1709.10402,
        "authors": "Krishna Dasaratha",
        "title": "Distributions of Centrality on Networks",
        "comments": null,
        "journal-ref": "Games and Economic Behavior, Vol. 122, July 2020, 1-27",
        "doi": "10.1016/j.geb.2020.03.008",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We provide a framework for determining the centralities of agents in a broad\nfamily of random networks. Current understanding of network centrality is\nlargely restricted to deterministic settings, but practitioners frequently use\nrandom network models to accommodate data limitations or prove asymptotic\nresults. Our main theorems show that on large random networks, centrality\nmeasures are close to their expected values with high probability. We\nillustrate the economic consequences of these results by presenting three\napplications: (1) In network formation models based on community structure\n(called stochastic block models), we show network segregation and differences\nin community size produce inequality. Benefits from peer effects tend to accrue\ndisproportionately to bigger and better-connected communities. (2) When link\nprobabilities depend on geography, we can compute and compare the centralities\nof agents in different locations. (3) In models where connections depend on\nseveral independent characteristics, we give a formula that determines\ncentralities 'characteristic-by-characteristic'. The basic techniques from\nthese applications, which use the main theorems to reduce questions about\nrandom networks to deterministic calculations, extend to many network games.\n"
    },
    {
        "paper_id": 1709.10478,
        "authors": "Josue Ortega, Philipp Hergovich",
        "title": "The Strength of Absent Ties: Social Integration via Online Dating",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We used to marry people to whom we were somehow connected. Since we were more\nconnected to people similar to us, we were also likely to marry someone from\nour own race. However, online dating has changed this pattern; people who meet\nonline tend to be complete strangers. We investigate the effects of those\npreviously absent ties on the diversity of modern societies.\n  We find that social integration occurs rapidly when a society benefits from\nnew connections. Our analysis of state-level data on interracial marriage and\nbroadband adoption (proxy for online dating) suggests that this integration\nprocess is significant and ongoing.\n"
    },
    {
        "paper_id": 1710.00231,
        "authors": "Anastasia Borovykh, Andrea Pascucci, Stefano la Rovere",
        "title": "Systemic risk in a mean-field model of interbank lending with\n  self-exciting shocks",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper we consider a mean-field model of interacting diffusions for\nthe monetary reserves in which the reserves are subjected to a self- and\ncross-exciting shock. This is motivated by the financial acceleration and fire\nsales observed in the market. We derive a mean-field limit using a weak\nconvergence analysis and find an explicit measure-valued process associated\nwith a large interbanking system. We define systemic risk indicators and\nderive, using the limiting process, several law of large numbers results and\nverify these numerically. We conclude that self-exciting shocks increase the\nsystemic risk in the network and their presence in interbank networks should\nnot be ignored.\n"
    },
    {
        "paper_id": 1710.00431,
        "authors": "Zachariah Peterson",
        "title": "Kelly's Criterion in Portfolio Optimization: A Decoupled Problem",
        "comments": "16 pages, 3 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Kelly's Criterion is well known among gamblers and investors as a method for\nmaximizing the returns one would expect to observe over long periods of betting\nor investing. These ideas are conspicuously absent from portfolio optimization\nproblems in the financial and automation literature. This paper will show how\nKelly's Criterion can be incorporated into standard portfolio optimization\nmodels. The model developed here combines risk and return into a single\nobjective function by incorporating a risk parameter. This model is then solved\nfor a portfolio of 10 stocks from a major stock exchange using a differential\nevolution algorithm. Monte Carlo calculations are used to verify the accuracy\nof the results obtained from differential evolution. The results show that\nevolutionary algorithms can be successfully applied to solve a portfolio\noptimization problem where returns are calculated by applying Kelly's Criterion\nto each of the assets in the portfolio.\n"
    },
    {
        "paper_id": 1710.00859,
        "authors": "Jinglun Yao, Sabine Laurent, Brice B\\'enaben",
        "title": "Managing Volatility Risk: An Application of Karhunen-Lo\\`eve\n  Decomposition and Filtered Historical Simulation",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Implied volatilities form a well-known structure of smile or surface which\naccommodates the Bachelier model and observed market prices of interest rate\noptions. For the swaptions that we study, three parameters are taken into\naccount for indexing the implied volatilities and form a \"volatility cube\":\nstrike (or moneyness), time to maturity of the option contract, duration of the\nunderlying swap contract. It should be noted that the implied volatility\nstructure changes across time, which makes it important to study its dynamics\nin order to well manage the volatility risk. As volatilities are correlated\nacross the cube, it is preferable to decompose the dynamics on orthogonal\nprincipal components, which is the idea of Karhunen-Lo\\`eve decomposition that\nwe have adopted in the article. The projections on principal components are\ninvestigated by Filtered Historical Simulation in order to predict the Value at\nRisk (VaR), which is then examined by standard tests and non-arbitrage\ncondition to ensure its appropriateness.\n"
    },
    {
        "paper_id": 1710.00897,
        "authors": "Kamil Kladivko and Mihail Zervos",
        "title": "Valuation of Employee Stock Options (ESOs) by means of Mean-Variance\n  Hedging",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We consider the problem of ESO valuation in continuous time. In particular,\nwe consider models that assume that an appropriate random time serves as a\nproxy for anything that causes the ESO's holder to exercise the option early,\nnamely, reflects the ESO holder's job termination risk as well as early\nexercise behaviour. In this context, we study the problem of ESO valuation by\nmeans of mean-variance hedging. Our analysis is based on dynamic programming\nand uses PDE techniques. We also express the ESO's value that we derive as the\nexpected discounted payoff that the ESO yields with respect to an equivalent\nmartingale measure, which does not coincide with the minimal martingale measure\nor the variance-optimal measure. Furthermore, we present a numerical study that\nillustrates aspects or our theoretical results.\n"
    },
    {
        "paper_id": 1710.01141,
        "authors": "Jean-Philippe Aguilar",
        "title": "A series representation for the Black-Scholes formula",
        "comments": "v1-2, 13 pages, some details added. arXiv admin note: text overlap\n  with arXiv:1609.00987",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We prove and test an efficient series representation for the European\nBlack-Scholes call, which generalizes and refines previously known\napproximations, and works in every market configuration.\n"
    },
    {
        "paper_id": 1710.01227,
        "authors": "Igor Halperin",
        "title": "Keep It Real: Tail Probabilities of Compound Heavy-Tailed Distributions",
        "comments": "23 pages, 3 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We propose an analytical approach to the computation of tail probabilities of\ncompound distributions whose individual components have heavy tails. Our\napproach is based on the contour integration method, and gives rise to a\nrepresentation of the tail probability of a compound distribution in the form\nof a rapidly convergent one-dimensional integral involving a discontinuity of\nthe imaginary part of its moment generating function across a branch cut. The\nlatter integral can be evaluated in quadratures, or alternatively represented\nas an asymptotic expansion. Our approach thus offers a viable (especially at\nhigh percentile levels) alternative to more standard methods such as Monte\nCarlo or the Fast Fourier Transform, traditionally used for such problems. As a\npractical application, we use our method to compute the operational Value at\nRisk (VAR) of a financial institution, where individual losses are modeled as\nspliced distributions whose large loss components are given by power-law or\nlognormal distributions. Finally, we briefly discuss extensions of the present\nformalism for calculation of tail probabilities of compound distributions made\nof compound distributions with heavy tails.\n"
    },
    {
        "paper_id": 1710.01452,
        "authors": "Xuefeng Gao, Xiang Zhou, Lingjiong Zhu",
        "title": "Transform Analysis for Hawkes Processes with Applications in Dark Pool\n  Trading",
        "comments": null,
        "journal-ref": "Quantitative Finance, 2018 Vol. 18, No. 2, 265-282",
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Hawkes processes are a class of simple point processes that are self-exciting\nand have clustering effect, with wide applications in finance, social networks\nand many other fields. This paper considers a self-exciting Hawkes process\nwhere the baseline intensity is time-dependent, the exciting function is a\ngeneral function and the jump sizes of the intensity process are independent\nand identically distributed non-negative random variables. This Hawkes model is\nnon-Markovian in general. We obtain closed-form formulas for the Laplace\ntransform, moments and the distribution of the Hawkes process. To illustrate\nthe applications of our results, we use the Hawkes process to model the\nclustered arrival of trades in a dark pool and analyze various performance\nmetrics including time-to-first-fill, time-to-complete-fill and the expected\nfill rate of a resting dark order.\n"
    },
    {
        "paper_id": 1710.01501,
        "authors": "Chung-Han Hsieh and B. Ross Barmish",
        "title": "On Inefficiency of Markowitz-Style Investment Strategies When Drawdown\n  is Important",
        "comments": "This paper has been published in Proceedings of 56th IEEE Conference\n  on Decision and Control (CDC) 2017",
        "journal-ref": "Proceedings of the IEEE Conference on Decision and Control (CDC),\n  pp .3075 - 3080, 2017",
        "doi": "10.1109/CDC.2017.8264108",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The focal point of this paper is the issue of \"drawdown\" which arises in\nrecursive betting scenarios and related applications in the stock market.\nRoughly speaking, drawdown is understood to mean drops in wealth over time from\npeaks to subsequent lows. Motivated by the fact that this issue is of paramount\nconcern to conservative investors, we dispense with the classical variance as\nthe risk metric and work with drawdown and mean return as the risk-reward pair.\nIn this setting, the main results in this paper address the so-called\n\"efficiency\" of linear time-invariant (LTI) investment feedback strategies\nwhich correspond to Markowitz-style schemes in the finance literature. Our\nanalysis begins with the following principle which is widely used in finance:\nGiven two investment opportunities, if one of them has higher risk and lower\nreturn, it will be deemed to be inefficient or strictly dominated and generally\nrejected in the marketplace. In this framework, with risk-reward pair as\ndescribed above, our main result is that classical Markowitz-style strategies\nare inefficient. To establish this, we use a new investment strategy which\ninvolves a time-varying linear feedback block K(k), called the drawdown\nmodulator. Using this instead of the original LTI feedback block K in the\nMarkowitz scheme, the desired domination is obtained. As a bonus, it is also\nseen that the modulator assures a worst-case level of drawdown protection with\nprobability one.\n"
    },
    {
        "paper_id": 1710.01503,
        "authors": "Chung-Han Hsieh and B. Ross Barmish",
        "title": "On Drawdown-Modulated Feedback Control in Stock Trading",
        "comments": "Proc. 20th IFAC World Congress (IFAC WC 2017), Toulouse, France, July\n  9-14, 2017 (accepted)",
        "journal-ref": "IFAC-PapersOnLine Volume 50, Issue 1, 2017",
        "doi": "10.1016/j.ifacol.2017.08.167",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Control of drawdown, that is, the control of the drops in wealth over time\nfrom peaks to subsequent lows, is of great concern from a risk management\nperspective. With this motivation in mind, the focal point of this paper is to\naddress the drawdown issue in a stock trading context. Although our analysis\ncan be carried out without reference to control theory, to make the work\naccessible to this community, we use the language of feedback systems. The\ntakeoff point for the results to follow, which we call the Drawdown Modulation\nLemma, characterizes any investment which guarantees that the percentage\ndrawdown is no greater than a prespecified level with probability one. With the\naid of this lemma, we introduce a new scheme which we call the\ndrawdown-modulated feedback control. To illustrate the power of the theory, we\nconsider a drawdown-constrained version of the well-known Kelly Optimization\nProblem which involves maximizing the expected logarithmic growth of the\ntrader's account value. As the drawdown parameter dmax in our new formulation\ntends to one, we recover existing results as a special case. This new theory\nleads to an optimal investment strategy whose application is illustrated via an\nexample with historical stock-price data.\n"
    },
    {
        "paper_id": 1710.01578,
        "authors": "Steffen Schuldenzucker, Sven Seuken, Stefano Battiston",
        "title": "The Computational Complexity of Financial Networks with Credit Default\n  Swaps",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The 2008 financial crisis has been attributed to \"excessive complexity\" of\nthe financial system due to financial innovation. We employ computational\ncomplexity theory to make this notion precise. Specifically, we consider the\nproblem of clearing a financial network after a shock. Prior work has shown\nthat when banks can only enter into simple debt contracts with each other, then\nthis problem can be solved in polynomial time. In contrast, if they can also\nenter into credit default swaps (CDSs), i.e., financial derivative contracts\nthat depend on the default of another bank, a solution may not even exist.\n  In this work, we show that deciding if a solution exists is NP-complete if\nCDSs are allowed. This remains true if we relax the problem to\n$\\varepsilon$-approximate solutions, for a constant $\\varepsilon$. We further\nshow that, under sufficient conditions where a solution is guaranteed to exist,\nthe approximate search problem is PPAD-complete for constant $\\varepsilon$. We\nthen try to isolate the \"origin\" of the complexity. It turns out that already\ndetermining which banks default is hard. Further, we show that the complexity\nis not driven by the dependence of counterparties on each other, but rather\nhinges on the presence of so-called naked CDSs. If naked CDSs are not present,\nwe receive a simple polynomial-time algorithm. Our results are of practical\nimportance for regulators' stress tests and regulatory policy.\n"
    },
    {
        "paper_id": 1710.01768,
        "authors": "Ron W. Nielsen",
        "title": "Explaining the Mechanism of Growth in the Past Two Million Years Vol. I",
        "comments": "85 pages, 44,161 words. arXiv admin note: substantial text overlap\n  with arXiv:1708.08673, arXiv:1601.07291, arXiv:1601.04686, arXiv:1510.00992,\n  arXiv:1603.01685, arXiv:1509.06612",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Economic growth and the growth of human population in the past 2,000,000\nyears are extensively examined. Data are found to be in a clear contradiction\nof the currently accepted explanations of the mechanism of growth, which\nrevolve around two fundamental but incorrect doctrines: (1) the doctrine of\nstagnation (inappropriately labelled also as Malthusian stagnation, because\nMalthus never claimed that his positive checks would cause a long-lasting and\nwide-spread stagnation) and (2) the doctrine of explosion described also as a\ntakeoff, sprint, spike or by other similar attributes. These doctrines and\nother related postulates are contradicted even by precisely the same data,\nwhich are used in the economic research and by the research results published\nin a prestigious scientific journal as early as in 1960. The generally accepted\nexplanations are not based on a rigorous analysis of data but on impressions\ncreated by the easily misleading features of hyperbolic distributions. Two\nleading theories: the Demographic Transitions Theory (or Model) and the Unified\nGrowth Theory are fundamentally incorrect. Descriptions of the past\nsocio-economic conditions are not questioned. They might have been harsh,\ndifficult and primitive but they are not reflected in the growth trajectories.\nThey did not create stagnation in the economic growth and in the growth of\npopulation. Likewise, impacts of the Industrial Revolution on many aspects of\nlife are not questioned. It is only demonstrated that this event had absolutely\nno impact on shaping growth trajectories. A general law of growth is formulated\nand used to explain the mechanism of growth of human population and of economic\ngrowth. The growth was predominantly hyperbolic. Such a growth is described by\nexceptionally simple mathematical function and the explanation of the mechanism\nof growth turns out to be also simple.\n"
    },
    {
        "paper_id": 1710.01786,
        "authors": "Chung-Han Hsieh, B. Ross Barmish, and John A. Gubner",
        "title": "Kelly Betting Can Be Too Conservative",
        "comments": "Accepted in 2016 IEEE 55th Conference on Decision and Control (CDC)",
        "journal-ref": "Proceedings of the IEEE Conference on Decision and Control (CDC),\n  pp .3695-3701, 2016",
        "doi": "10.1109/CDC.2016.7798825",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Kelly betting is a prescription for optimal resource allocation among a set\nof gambles which are typically repeated in an independent and identically\ndistributed manner. In this setting, there is a large body of literature which\nincludes arguments that the theory often leads to bets which are \"too\naggressive\" with respect to various risk metrics. To remedy this problem, many\npapers include prescriptions for scaling down the bet size. Such schemes are\nreferred to as Fractional Kelly Betting. In this paper, we take the opposite\ntack. That is, we show that in many cases, the theoretical Kelly-based results\nmay lead to bets which are \"too conservative\" rather than too aggressive. To\nmake this argument, we consider a random vector X with its assumed probability\ndistribution and draw m samples to obtain an empirically-derived counterpart\nXhat. Subsequently, we derive and compare the resulting Kelly bets for both X\nand Xhat with consideration of sample size m as part of the analysis. This\nleads to identification of many cases which have the following salient feature:\nThe resulting bet size using the true theoretical distribution for X is much\nsmaller than that for Xhat. If instead the bet is based on empirical data,\n\"golden\" opportunities are identified which are essentially rejected when the\npurely theoretical model is used. To formalize these ideas, we provide a result\nwhich we call the Restricted Betting Theorem. An extreme case of the theorem is\nobtained when X has unbounded support. In this situation, using X, the Kelly\ntheory can lead to no betting at all.\n"
    },
    {
        "paper_id": 1710.01787,
        "authors": "Chung-Han Hsieh and B. Ross Barmish",
        "title": "On Kelly Betting: Some Limitations",
        "comments": "Accepted in 53rd Annual Allerton Conference on Communication,\n  Control, and Computing, 2015",
        "journal-ref": "Proceedings of the Annual Allerton Conference on Communication,\n  Control, and Computing, pp.165-172, 2015",
        "doi": "10.1109/ALLERTON.2015.7447000",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The focal point of this paper is the so-called Kelly Criterion, a\nprescription for optimal resource allocation among a set of gambles which are\nrepeated over time. The criterion calls for maximization of the expected value\nof the logarithmic growth of wealth. While significant literature exists\nproviding the rationale for such an optimization, this paper concentrates on\nthe limitations of the Kelly-based theory. To this end, we fill a void in\npublished results by providing specific examples quantifying what difficulties\nare encountered when Taylor-style approximations are used and when wealth\ndrawdowns are considered. For the case of drawdown, we describe some research\ndirections which we feel are promising for improvement of the theory.\n"
    },
    {
        "paper_id": 1710.01797,
        "authors": "Kathrin Glau, Paul Herold, Dilip B. Madan, Christian P\\\"otz",
        "title": "The Chebyshev method for the implied volatility",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The implied volatility is a crucial element of any financial toolbox, since\nit is used for quoting and the hedging of options as well as for model\ncalibration. In contrast to the Black-Scholes formula its inverse, the implied\nvolatility, is not explicitly available and numerical approximation is\nrequired. We propose a bivariate interpolation of the implied volatility\nsurface based on Chebyshev polynomials. This yields a closed-form approximation\nof the implied volatility, which is easy to implement and to maintain. We prove\na subexponential error decay. This allows us to obtain an accuracy close to\nmachine precision with polynomials of a low degree. We compare the performance\nof the method in terms of runtime and accuracy to the most common reference\nmethods. In contrast to existing interpolation methods, the proposed method is\nable to compute the implied volatility for all relevant option data. In this\ncontext, numerical experiments confirm a considerable increase in efficiency,\nespecially for large data sets.\n"
    },
    {
        "paper_id": 1710.02127,
        "authors": "Yang Xu",
        "title": "Intervention On Default Contagion Under Partial Information",
        "comments": "63 pages, 9 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We model the default contagion process in a large heterogeneous financial\nnetwork under the interventions of a regulator (a central bank) with only\npartial information which is a more realistic setting than most current\nliterature. We provide the analytical results for the asymptotic optimal\nintervention policies and the asymptotic magnitude of default contagion in\nterms of the network characteristics. We extend the results of Amini et al.\n(2013) to incorporate interventions and the model of Amini et al. (2015); Amini\net al. (2017) to heterogeneous networks with a given degree sequence and\narbitrary initial equity levels. The insights from the results are that the\noptimal intervention policy is \"monotonic\" in terms of the intervention cost,\nthe closeness to invulnerability and connectivity. Moreover, we should keep\nintervening on a bank once we have intervened on it. Our simulation results\nshow a good agreement with the theoretical results.\n"
    },
    {
        "paper_id": 1710.02435,
        "authors": "Philipp J. Kremer, Sangkyun Lee, Malgorzata Bogdan, Sandra Paterlini",
        "title": "Sparse Portfolio Selection via the sorted $\\ell_{1}$-Norm",
        "comments": "41 pages",
        "journal-ref": "Journal of Banking & Finance, Volume 110, January 2020, 105687",
        "doi": "10.1016/j.jbankfin.2019.105687",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We introduce a financial portfolio optimization framework that allows us to\nautomatically select the relevant assets and estimate their weights by relying\non a sorted $\\ell_1$-Norm penalization, henceforth SLOPE. Our approach is able\nto group constituents with similar correlation properties, and with the same\nunderlying risk factor exposures. We show that by varying the intensity of the\npenalty, SLOPE can span the entire set of optimal portfolios on the\nrisk-diversification frontier, from minimum variance to the equally weighted.\nTo solve the optimization problem, we develop a new efficient algorithm, based\non the Alternating Direction Method of Multipliers. Our empirical analysis\nshows that SLOPE yields optimal portfolios with good out-of-sample risk and\nreturn performance properties, by reducing the overall turnover through more\nstable asset weight estimates. Moreover, using the automatic grouping property\nof SLOPE, new portfolio strategies, such as SLOPE-MV, can be developed to\nexploit the data-driven detected similarities across assets.\n"
    },
    {
        "paper_id": 1710.02669,
        "authors": "Daniel Kosiorowski, Dominik Mielczarek, Jerzy P. Rydlewski",
        "title": "Aggregated moving functional median in robust prediction of hierarchical\n  functional time series - an application to forecasting web portal users\n  behaviors",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this article, a new nonparametric and robust method of forecasting\nhierarchical functional time series is presented. The method is compared with\nHyndman and Shang's method with respect to their unbiasedness, effectiveness,\nrobustness, and computational complexity. Taking into account results of the\nanalytical, simulation and empirical studies, we come to the conclusion that\nour proposal is superior over the proposal of Hyndman and Shang with respect to\nsome statistical criteria and especially with respect to robustness and\ncomputational complexity. An empirical usefulness of our method is presented on\nexample of management of a certain web portal divided into four subservices. An\nextensive simulation study involving hierarchical systems consisted of FAR(1)\nprocesses and Wiener processes has been conducted as well.\n"
    },
    {
        "paper_id": 1710.02755,
        "authors": "Agnibho Roy, Abhishek Mohan",
        "title": "An Optimized Microeconomic Modeling System for Analyzing Industrial\n  Externalities in Non-OECD Countries",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper, we provide an integrated systems modeling approach to\nanalyzing global externalities from a microeconomic perspective. Various forms\nof policy (fiscal, monetary, etc.) have addressed flaws and market failures in\nmodels, but few have been able to successfully eliminate modern externalities\nthat remain an environmental and human threat. We assess three primary global\nindustries (pollution, agriculture, and energy) with respect to non-OECD\nentities through both qualitative and quantitative studies. By combining key\nmutual points of specific externalities present within each respective\nindustry, we are able to propose an alternative and optimized solution to\ninternalizing them via incentives and cooperative behavior rather than by\ntraditional Pigouvian taxes and subsidies.\n"
    },
    {
        "paper_id": 1710.02838,
        "authors": "Itai Areili, Yakov Babichenko, Rann Smorodinsky",
        "title": "Robust Forecast Aggregation",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Bayesian experts who are exposed to different evidence often make\ncontradictory probabilistic forecasts. An aggregator, ignorant of the\nunderlying model, uses this to calculate her own forecast. We use the notions\nof scoring rules and regret to propose a natural way to evaluate an aggregation\nscheme. We focus on a binary state space and construct low regret aggregation\nschemes whenever there are only two experts which are either Blackwell-ordered\nor receive conditionally i.i.d. signals. In contrast, if there are many experts\nwith conditionally i.i.d. signals, then no scheme performs (asymptotically)\nbetter than a $(0.5,0.5)$ forecast.\n"
    },
    {
        "paper_id": 1710.0316,
        "authors": "Dan Pirjol, Jing Wang, Lingjiong Zhu",
        "title": "Short Maturity Forward Start Asian Options in Local Volatility Models",
        "comments": "43 pages, 6 figures, 3 tables",
        "journal-ref": "Applied Mathematical Finance 2019, Vol. 26, No. 3, 187-221",
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We study the short maturity asymptotics for prices of forward start Asian\noptions under the assumption that the underlying asset follows a local\nvolatility model. We obtain asymptotics for the cases of out-of-the-money,\nin-the-money, and at-the-money, considering both fixed strike and floating\nAsian options. The exponential decay of the price of an out-of-the-money\nforward start Asian option is handled using large deviations theory, and is\ncontrolled by a rate function which is given by a double-layer optimization\nproblem. In the Black-Scholes model, the calculation of the rate function is\nsimplified further to the solution of a non-linear equation. We obtain closed\nform for the rate function, as well as its asymptotic behaviors when the strike\nis extremely large, small, or close to the initial price of the underlying\nasset.\n"
    },
    {
        "paper_id": 1710.03161,
        "authors": "Chris Kenyon and Mourad Berrahoui and Benjamin Poncet",
        "title": "Counterparty Trading Limits Revisited:CSAs, IM, SwapAgent(r), from PFE\n  to PFL",
        "comments": "14 pages, 6 figures, 2 tables",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The utility of Potential Future Exposure (PFE) for counterparty trading\nlimits is being challenged by new market developments, notably widespread\nregulatory Initial Margin (using 99% 10-day exposure), and netting of trade and\ncollateral flows. However PFE has pre-existing challenges w.r.t.\nportfolios/distributions, collateralization, netting set seniority, and\noverlaps with CVA. We introduce Potential Future Loss (PFL) which combines\nexpected shortfall (ES) and loss given default (LGD) as a replacement for PFE.\nWith two additional variants Adjusted PFL (aPFL) and Protected Adjusted PFL\n(paPFL) these deal with both new and pre-existing challenges. We provide a\ntheoretical background and numerical examples.\n"
    },
    {
        "paper_id": 1710.03205,
        "authors": "Svetlozar Rachev, Stoyan Stoyanov, Frank J. Fabozzi",
        "title": "Behavioral Finance Option Pricing Formulas Consistent with Rational\n  Dynamic Asset Pricing",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We derive behavioral finance option pricing formulas consistent with the\nrational dynamic asset pricing theory. In the existing behavioral finance\noption pricing formulas, the price process of the representative agent is not a\nsemimartingale, which leads to arbitrage opportunities for the option seller.\nIn the literature on behavioral finance option pricing it is allowed the option\nbuyer and seller to have different views on the instantaneous mean return of\nthe underlying price process, which leads to arbitrage opportunities according\nto Black (1972). We adjust the behavioral finance option pricing formulas to be\nconsistent with the rational dynamic asset pricing theory, by introducing\ntransaction costs on the velocity of trades which offset the gains from the\narbitrage trades.\n"
    },
    {
        "paper_id": 1710.03211,
        "authors": "Svetlozar Rachev, Stoyan Stoyanov, Stefan Mittnik, Frank J. Fabozzi,\n  Abootaleb Shirvani",
        "title": "Behavioral Finance -- Asset Prices Predictability, Equity Premium\n  Puzzle, Volatility Puzzle: The Rational Finance Approach",
        "comments": null,
        "journal-ref": null,
        "doi": "10.13140/RG.2.2.29789.77287",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper we address three main objections of behavioral finance to the\ntheory of rational finance, considered as anomalies the theory of rational\nfinance cannot explain: Predictability of asset returns, The Equity Premium,\n(The Volatility Puzzle. We offer resolutions of those objections within the\nrational finance. We do not claim that those are the only possible explanations\nof the anomalies, but offer statistical models within the rational theory of\nfinance which can be used without relying on behavioral finance assumptions\nwhen searching for explanations of those anomalies.\n"
    },
    {
        "paper_id": 1710.03252,
        "authors": "Valeria Bignozzi, Claudio Macci, Lea Petrella",
        "title": "Large deviations for risk measures in finite mixture models",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Due to their heterogeneity, insurance risks can be properly described as a\nmixture of different fixed models, where the weights assigned to each model may\nbe estimated empirically from a sample of available data. If a risk measure is\nevaluated on the estimated mixture instead of the (unknown) true one, then it\nis important to investigate the committed error. In this paper we study the\nasymptotic behaviour of estimated risk measures, as the data sample size tends\nto infinity, in the fashion of large deviations. We obtain large deviation\nresults by applying the contraction principle, and the rate functions are given\nby a suitable variational formula; explicit expressions are available for\nmixtures of two models. Finally, our results are applied to the most common\nrisk measures, namely the quantiles, the Expected Shortfall and the shortfall\nrisk measures.\n"
    },
    {
        "paper_id": 1710.03267,
        "authors": "Abhishek Mohan, Agnibho Roy",
        "title": "A Strategic Investment Framework for Biotechnology Markets via Dynamic\n  Asset Allocation and Class Diversification",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper, we propose an innovative investment framework incorporating\nasset allocation and class diversification oriented specifically for the\nbiotechnology industry. With growing interests and capitalization in multiple\nbiotech markets, investors require a more dynamic method of managing their\nassets within individual portfolios for optimal return efficiency. By selecting\na single firm representative of identified industry trends, analyzing financial\nmetrics relevant to the suggested approaches, and assessing financial health,\nwe developed an adaptable investment methodology. We also performed analyses of\nindustrial viability and investigated the implications of the selected\nstrategies, with which we were able to optimize our framework for versatile\napplication within specialized biotech markets.\n"
    },
    {
        "paper_id": 1710.03506,
        "authors": "Ingemar Kaj and Mine Caglar",
        "title": "A buffer Hawkes process for limit order books",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We introduce a Markovian single point process model, with random intensity\nregulated through a buffer mechanism and a self-exciting effect controlling the\narrival stream to the buffer. The model applies the principle of the Hawkes\nprocess in which point process jumps generate a shot-noise intensity field.\nUnlike the Hawkes case, the intensity field is fed into a separate buffer, the\nsize of which is the driving intensity of new jumps. In this manner, the\nintensity loop portrays mutual-excitation of point process events and buffer\nsize dynamics. This scenario is directly applicable to the market evolution of\nlimit order books, with buffer size being the current number of limit orders\nand the jumps representing the execution of market orders. We give a branching\nprocess representation of the point process and prove that the scaling limit is\nBrownian motion with explicit volatility.\n"
    },
    {
        "paper_id": 1710.03526,
        "authors": "M\\'onica Clavel, Jes\\'us Arteaga-Ortiz, Rub\\'en Fern\\'andez-Ortiz,\n  Pablo Dorta-Gonz\\'alez",
        "title": "Measuring the gradualist approach to internationalization",
        "comments": "18 pages, 7 tables",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  The objective of this paper is to fill a gap in the literature on\ninternationalization, in relation to the absence of objective and measurable\nperformance indicators on the process of how firms sequentially enter external\nmarkets. To that end, this research develops a quantitative tool that can be\nused as a performance indicator of gradualness for firms entering external\nmarkets at a sectoral level. The performance indicator is based on firms'\nexport volume, number of years of exporting, geographic areas targeted for\nexport, and when exports were initiated for each area. Additionally, the\nindicator is tested empirically in the Spanish wine sector. The main\ncontribution of this study is the creation of an international priority index\nwhich serves as a valuable and reliable tool because of its potential use in\nother industry sectors and geographic areas, allowing us to analyze how\ngeographically differentiated internationalization strategies develop.\n"
    },
    {
        "paper_id": 1710.03734,
        "authors": "Michael Benzaquen and Jean-Philippe Bouchaud",
        "title": "Market impact with multi-timescale liquidity",
        "comments": "17 pages, 6 figures, 1 table",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We present an extended version of the recently proposed \"LLOB\" model for the\ndynamics of latent liquidity in financial markets. By allowing for finite\ncancellation and deposition rates within a continuous reaction-diffusion setup,\nwe account for finite memory effects on the dynamics of the latent order book.\nWe compute in particular the finite memory corrections to the square root\nimpact law, as well as the impact decay and the permanent impact of a\nmeta-order. The latter is found to be linear in the traded volume and\nindependent of the trading rate, as dictated by no-arbitrage arguments. In\naddition, we consider the case of a spectrum of cancellation and deposition\nrates, which allows us to obtain a square root impact law for moderate\nparticipation rates, as observed empirically. Our multi-scale framework also\nprovides an alternative solution to the so-called price diffusivity puzzle in\nthe presence of a long-range correlated order flow.\n"
    },
    {
        "paper_id": 1710.0382,
        "authors": "G. R. Grimmett, F. Pukelsheim, V. Ram\\'irez Gonz\\'alez, W.\n  S{\\l}omczy\\'nski, K. \\.Zyczkowski",
        "title": "A 700-seat no-loss composition for the 2019 European Parliament",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The following paper is part of the authors' response to an invitation from\nthe Constitutional Affairs Committee (AFCO) of the European Parliament to\nadvise on mathematical methods for the allocation of Parliamentary seats\nbetween the 27 Member States following the planned departure of the United\nKingdom in 2019. The authors were requested to propose a method that respects\nthe usual conditions of EU law, and with the additional property that no Member\nState (other than the UK) receives fewer that its 2014 allocation. This paper\nwas delivered to the AFCO on 21 August 2017, for consideration by the AFCO at\nits meeting in Strasbourg on 11 September 2017.\n"
    },
    {
        "paper_id": 1710.0387,
        "authors": "Matthew F Dixon",
        "title": "A High Frequency Trade Execution Model for Supervised Learning",
        "comments": "arXiv admin note: substantial text overlap with arXiv:1707.05642,\n  High Frequency, 2018",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper introduces a high frequency trade execution model to evaluate the\neconomic impact of supervised machine learners. Extending the concept of a\nconfusion matrix, we present a 'trade information matrix' to attribute the\nexpected profit and loss of the high frequency strategy under execution\nconstraints, such as fill probabilities and position dependent trade rules, to\ncorrect and incorrect predictions. We apply the trade execution model and trade\ninformation matrix to Level II E-mini S&P 500 futures history and demonstrate\nan estimation approach for measuring the sensitivity of the P&L to the error of\na Recurrent Neural Network. Our approach directly evaluates the performance\nsensitivity of a market making strategy to prediction error and augments\ntraditional market simulation based testing.\n"
    },
    {
        "paper_id": 1710.04273,
        "authors": "Justin Sirignano, Konstantinos Spiliopoulos",
        "title": "Stochastic Gradient Descent in Continuous Time: A Central Limit Theorem",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Stochastic gradient descent in continuous time (SGDCT) provides a\ncomputationally efficient method for the statistical learning of\ncontinuous-time models, which are widely used in science, engineering, and\nfinance. The SGDCT algorithm follows a (noisy) descent direction along a\ncontinuous stream of data. The parameter updates occur in continuous time and\nsatisfy a stochastic differential equation. This paper analyzes the asymptotic\nconvergence rate of the SGDCT algorithm by proving a central limit theorem\n(CLT) for strongly convex objective functions and, under slightly stronger\nconditions, for non-convex objective functions as well. An $L^{p}$ convergence\nrate is also proven for the algorithm in the strongly convex case. The\nmathematical analysis lies at the intersection of stochastic analysis and\nstatistical learning.\n"
    },
    {
        "paper_id": 1710.04363,
        "authors": "Lingqi Gu and Yiqing Lin and Junjian Yang",
        "title": "Utility maximization problem under transaction costs: optimal dual\n  processes and stability",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper discusses the num\\'eraire-based utility maximization problem in\nmarkets with proportional transaction costs. In particular, the investor is\nrequired to liquidate all her position in stock at the terminal time. We first\nobserve the stability of the primal and dual value functions as well as the\nconvergence of the primal and dual optimizers when perturbations occur on the\nutility function and on the physical probability. We then study the properties\nof the optimal dual process (ODP), that is, a process from the dual domain that\ninduces the optimality of the dual problem. When the market is driven by a\ncontinuous process, we construct the ODP for the problem in the limiting market\nby a sequence of ODPs corresponding to the problems with small misspecificated\nparameters. Moreover, we prove that this limiting ODP defines a shadow price.\n"
    },
    {
        "paper_id": 1710.04455,
        "authors": "Frank Emmert-Streib, Aliyu Musa, Kestutis Baltakys, Juho Kanniainen,\n  Shailesh Tripathi, Olli Yli-Harja, Herbert Jodlbauer and Matthias Dehmer",
        "title": "Computational Analysis of the structural properties of Economic and\n  Financial Networks",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In recent years, methods from network science are gaining rapidly interest in\neconomics and finance. A reason for this is that in a globalized world the\ninterconnectedness among economic and financial entities are crucial to\nunderstand and networks provide a natural framework for representing and\nstudying such systems. In this paper, we are surveying the use of networks and\nnetwork-based methods for studying economy related questions. We start with a\nbrief overview of graph theory and basic definitions. Then we discuss\ndescriptive network measures and network complexity measures for quantifying\nstructural properties of economic networks. Finally, we discuss different\nnetwork and tree structures as relevant for applications.\n"
    },
    {
        "paper_id": 1710.04579,
        "authors": "Stanislaus Maier-Paape and Qiji Jim Zhu",
        "title": "A General Framework for Portfolio Theory. Part I: theory and various\n  models",
        "comments": null,
        "journal-ref": "Risks 2018, 6(2), 53",
        "doi": "10.3390/risks6020053",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Utility and risk are two often competing measurements on the investment\nsuccess. We show that efficient trade-off between these two measurements for\ninvestment portfolios happens, in general, on a convex curve in the two\ndimensional space of utility and risk. This is a rather general pattern. The\nmodern portfolio theory of Markowitz [H. Markowitz, Portfolio Selection, 1959]\nand its natural generalization, the capital market pricing model, [W. F.\nSharpe, Mutual fund performance , 1966] are special cases of our general\nframework when the risk measure is taken to be the standard deviation and the\nutility function is the identity mapping. Using our general framework, we also\nrecover the results in [R. T. Rockafellar, S. Uryasev and M. Zabarankin, Master\nfunds in portfolio analysis with general deviation measures, 2006] that extends\nthe capital market pricing model to allow for the use of more general deviation\nmeasures. This generalized capital asset pricing model also applies to e.g.\nwhen an approximation of the maximum drawdown is considered as a risk measure.\nFurthermore, the consideration of a general utility function allows to go\nbeyond the \"additive\" performance measure to a \"multiplicative\" one of\ncumulative returns by using the log utility. As a result, the growth optimal\nportfolio theory [J. Lintner, The valuation of risk assets and the selection of\nrisky investments in stock portfolios and capital budgets, 1965] and the\nleverage space portfolio theory [R. Vince, The Leverage Space Trading Model,\n2009] can also be understood under our general framework. Thus, this general\nframework allows a unification of several important existing portfolio theories\nand goes much beyond.\n"
    },
    {
        "paper_id": 1710.04818,
        "authors": "Stanislaus Maier-Paape and Qiji Jim Zhu",
        "title": "A General Framework for Portfolio Theory. Part II: drawdown risk\n  measures",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The aim of this paper is to provide several examples of convex risk measures\nnecessary for the application of the general framework for portfolio theory of\nMaier-Paape and Zhu, presented in Part I of this series (arXiv:1710.04579\n[q-fin.PM]). As alternative to classical portfolio risk measures such as the\nstandard deviation we in particular construct risk measures related to the\ncurrent drawdown of the portfolio equity. Combined with the results of Part I\n(arXiv:1710.04579 [q-fin.PM]), this allows us to calculate efficient portfolios\nbased on a drawdown risk measure constraint.\n"
    },
    {
        "paper_id": 1710.05114,
        "authors": "Anastasis Kratsios and Cody B. Hyndman",
        "title": "Deep Learning in a Generalized HJM-type Framework Through Arbitrage-Free\n  Regularization",
        "comments": "23 Pages + References",
        "journal-ref": null,
        "doi": "10.3390/risks8020040",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We introduce a regularization approach to arbitrage-free factor-model\nselection. The considered model selection problem seeks to learn the closest\narbitrage-free HJM-type model to any prespecified factor-model. An asymptotic\nsolution to this, a priori computationally intractable, problem is represented\nas the limit of a 1-parameter family of optimizers to computationally tractable\nmodel selection tasks. Each of these simplified model-selection tasks seeks to\nlearn the most similar model, to the prescribed factor-model, subject to a\npenalty detecting when the reference measure is a local martingale-measure for\nthe entire underlying financial market. A simple expression for the penalty\nterms is obtained in the bond market withing the affine-term structure setting,\nand it is used to formulate a deep-learning approach to arbitrage-free affine\nterm-structure modelling. Numerical implementations are also performed to\nevaluate the performance in the bond market.\n"
    },
    {
        "paper_id": 1710.05131,
        "authors": "Michael Ludkovski and Xuwei Yang",
        "title": "Mean Field Game Approach to Production and Exploration of Exhaustible\n  Commodities",
        "comments": "32 pages, 7 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In a game theoretic framework, we study energy markets with a continuum of\nhomogenous producers who produce energy from an exhaustible resource such as\noil. Each producer simultaneously optimizes production rate that drives her\nrevenues, as well as exploration effort to replenish her reserves. This\nexploration activity is modeled through a controlled point process that leads\nto stochastic increments to reserves level. The producers interact with each\nother through the market price that depends on the aggregate production. We\nemploy a mean field game approach to solve for a Markov Nash equilibrium and\ndevelop numerical schemes to solve the resulting system of non-local HJB and\ntransport equations with non-local coupling. A time-stationary formulation is\nalso explored, as well as the fluid limit where exploration becomes\ndeterministic.\n"
    },
    {
        "paper_id": 1710.05168,
        "authors": "Longjie Jia, Martijn Pistorius, Harry Zheng",
        "title": "Dynamic Portfolio Optimization with Looping Contagion Risk",
        "comments": "26 pages, 6 figures. This paper has been presented on 10th Bachelier\n  Conference in Dublin",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper we consider a utility maximization problem with defaultable\nstocks and looping contagion risk. We assume that the default intensity of one\ncompany depends on the stock prices of itself and other companies, and the\ndefault of the company induces immediate drops in the stock prices of the\nsurviving companies. We prove that the value function is the unique viscosity\nsolution of the HJB equation. We also perform some numerical tests to compare\nand analyse the statistical distributions of the terminal wealth of log utility\nand power utility based on two strategies, one using the full information of\nintensity process and the other a proxy constant intensity process.\n"
    },
    {
        "paper_id": 1710.05204,
        "authors": "Michael Ludkovski and James Risk",
        "title": "Sequential Design and Spatial Modeling for Portfolio Tail Risk\n  Measurement",
        "comments": "40 pages, 11 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We consider calculation of capital requirements when the underlying economic\nscenarios are determined by simulatable risk factors. In the respective nested\nsimulation framework, the goal is to estimate portfolio tail risk, quantified\nvia VaR or TVaR of a given collection of future economic scenarios representing\nfactor levels at the risk horizon. Traditionally, evaluating portfolio losses\nof an outer scenario is done by computing a conditional expectation via\ninner-level Monte Carlo and is computationally expensive. We introduce several\ninter-related machine learning techniques to speed up this computation, in\nparticular by properly accounting for the simulation noise. Our main workhorse\nis an advanced Gaussian Process (GP) regression approach which uses\nnonparametric spatial modeling to efficiently learn the relationship between\nthe stochastic factors defining scenarios and corresponding portfolio value.\nLeveraging this emulator, we develop sequential algorithms that adaptively\nallocate inner simulation budgets to target the quantile region. The GP\nframework also yields better uncertainty quantification for the resulting\nVaR/TVaR estimators that reduces bias and variance compared to existing\nmethods. We illustrate the proposed strategies with two case-studies in two and\nsix dimensions.\n"
    },
    {
        "paper_id": 1710.05513,
        "authors": "Ziping Zhao and Daniel P. Palomar",
        "title": "Robust Maximum Likelihood Estimation of Sparse Vector Error Correction\n  Model",
        "comments": "5 pages, 3 figures, to appear in Proc. of the 2017 5th IEEE Global\n  Conference on Signal and Information Processing (GlobalSIP)",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In econometrics and finance, the vector error correction model (VECM) is an\nimportant time series model for cointegration analysis, which is used to\nestimate the long-run equilibrium variable relationships. The traditional\nanalysis and estimation methodologies assume the underlying Gaussian\ndistribution but, in practice, heavy-tailed data and outliers can lead to the\ninapplicability of these methods. In this paper, we propose a robust model\nestimation method based on the Cauchy distribution to tackle this issue. In\naddition, sparse cointegration relations are considered to realize feature\nselection and dimension reduction. An efficient algorithm based on the\nmajorization-minimization (MM) method is applied to solve the proposed\nnonconvex problem. The performance of this algorithm is shown through numerical\nsimulations.\n"
    },
    {
        "paper_id": 1710.05542,
        "authors": "Bertram D\\\"uring and Alexander Pitkin",
        "title": "Efficient hedging in Bates model using high-order compact finite\n  differences",
        "comments": "9 pages, 7 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We evaluate the hedging performance of a high-order compact finite difference\nscheme from [4] for option pricing in Bates model. We compare the scheme's\nhedging performance to standard finite difference methods in different\nexamples. We observe that the new scheme outperforms a standard, second-order\ncentral finite difference approximation in all our experiments.\n"
    },
    {
        "paper_id": 1710.05829,
        "authors": "Anastasis Kratsios and Cody B. Hyndman",
        "title": "Non-Euclidean Conditional Expectation and Filtering",
        "comments": "This updated version focuses on non Euclidean filtering applications.\n  The content on geometric learning from version one separated and expanded in\n  our paper \"The NEU Meta-Algorithm for Geometric Learning with Applications in\n  Finance\" [arXiv:1809.00082]",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  A non-Euclidean generalization of conditional expectation is introduced and\ncharacterized as the minimizer of expected intrinsic squared-distance from a\nmanifold-valued target. The computational tractable formulation expresses the\nnon-convex optimization problem as transformations of Euclidean conditional\nexpectation. This gives computationally tractable filtering equations for the\ndynamics of the intrinsic conditional expectation of a manifold-valued signal\nand is used to obtain accurate numerical forecasts of efficient portfolios by\nincorporating their geometric structure into the estimates.\n"
    },
    {
        "paper_id": 1710.06132,
        "authors": "Mario Coccia",
        "title": "Disruptive firms",
        "comments": "Keywords: Disruptive Technologies; Disruptive Firms, Radical\n  Innovations, R&D Management, Competitive Advantage, Industrial Change. JEL\n  codes: L20; O32; O33",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This study proposes the concept of disruptive firms: they are firms with\nmarket leadership that deliberate introduce new and improved generations of\ndurable goods that destroy, directly or indirectly, similar products present in\nmarkets in order to support their competitive advantage and/or market\nleadership. These disruptive firms support technological and industrial change\nand induce consumers to buy new products to adapt to new socioeconomic\nenvironment. In particular, disruptive firms generate and spread path-breaking\ninnovations in order to achieve and sustain the goal of a (temporary) profit\nmonopoly. This organizational behaviour and strategy of disruptive firms\nsupport technological change. This study can be useful for bringing a new\nperspective to explain and generalize one of the determinants that generates\ntechnological and industrial change. Overall, then this study suggests that one\nof the general sources of technological change is due to disruptive firms\n(subjects), rather than disruptive technologies (objects), that generate market\nshifts in a Schumpeterian world of innovation-based competition.\n"
    },
    {
        "paper_id": 1710.06285,
        "authors": "Yaneer Bar-Yam, Jean Langlois-Meurinne, Mari Kawakatsu, Rodolfo Garcia",
        "title": "Preliminary steps toward a universal economic dynamics for monetary and\n  fiscal policy",
        "comments": "45 pages, 15 figures; Expanded overview and references",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We consider the relationship between economic activity and intervention,\nincluding monetary and fiscal policy, using a universal dynamic framework.\nCentral bank policies are designed for growth without excess inflation.\nHowever, unemployment, investment, consumption, and inflation are interlinked.\nUnderstanding dynamics is crucial to assessing the effects of policy,\nespecially in the aftermath of the financial crisis. Here we lay out a program\nof research into monetary and economic dynamics and preliminary steps toward\nits execution. We use principles of response theory to derive implications for\npolicy. We find that the current approach, which considers the overall money\nsupply, is insufficient to regulate economic growth. While it can achieve some\ndegree of control, optimizing growth also requires a fiscal policy balancing\nmonetary injection between two dominant loop flows, the consumption and wages\nloop, and investment and returns loop. The balance arises from a composite of\ngovernment tax, entitlement, subsidy policies, corporate policies, as well as\nmonetary policy. We show empirically that a transition occurred in 1980 between\ntwo regimes--an oversupply to the consumption and wages loop, to an oversupply\nof the investment and returns loop. The imbalance is manifest in savings and\nborrowing by consumers and investors, and in inflation. The latter increased\nuntil 1980, and decreased subsequently, resulting in a zero rate largely\nunrelated to the financial crisis. Three recessions and the financial crisis\nare part of this dynamic. Optimizing growth now requires shifting the balance.\nOur analysis supports advocates of greater income and / or government support\nfor the poor who use a larger fraction of income for consumption. This promotes\ninvestment due to growth in demand. Otherwise, investment opportunities are\nlimited, capital remains uninvested, and does not contribute to growth.\n"
    },
    {
        "paper_id": 1710.0635,
        "authors": "Ilija I. Zovko",
        "title": "Navigating dark liquidity (How Fisher catches Poisson in the Dark)",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In order to reduce signalling, traders may resort to limiting access to dark\nvenues and imposing limits on minimum fill sizes they are willing to trade.\nHowever, doing this also restricts the liquidity available to the trader since\nan ever increasing quantity of orders are traded by algos in clips. An\nalternative is to attempt to monitor signalling in real time and dynamically\nmake adjustments to the dark liquidity accessed.\n  In practice, price slippage against the order is commonly taken as an\nindication of signalling. However, estimating slippage is difficult and\nrequires a large number of fills to reliably detect it. Ultimately, even if\ndetected, it fails to capture an important element of causality between dark\nfills and lit prints - a signature of information leakage. In the extreme, this\ncan lead to scaling back trading at a time when slippage is caused by a\ncompeting trader consuming liquidity, and the appropriate action would be to\nscale trading up -- not down -- in order to capture good prices.\n  In this paper we describe a methodology aimed to address this dichotomy of\ntrading objectives, allowing to maximally capture available liquidity while at\nthe same time protecting the trader from excessive signalling. The method is\ndesigned to profile dark liquidity in a dynamic fashion, on a per fill basis,\nin contrast to historical venue analyses based on estimated slippage. This\nallows for a dynamic and real-time control of the desired liquidity exposure.\n"
    },
    {
        "paper_id": 1710.06526,
        "authors": "Jaroslav Borovicka and John Stachurski",
        "title": "Necessary and Sufficient Conditions for Existence and Uniqueness of\n  Recursive Utilities",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We study existence, uniqueness and computability of solutions for a class of\ndiscrete time recursive utilities models. By combining two streams of the\nrecent literature on recursive preferences---one that analyzes principal\neigenvalues of valuation operators and another that exploits the theory of\nmonotone concave operators---we obtain conditions that are both necessary and\nsufficient for existence and uniqueness of solutions. We also show that the\nnatural iterative algorithm is convergent if and only if a solution exists.\nConsumption processes are allowed to be nonstationary.\n"
    },
    {
        "paper_id": 1710.06893,
        "authors": "Sara M. Clifton, Eileen Herbers, Jack Chen, Daniel M. Abrams",
        "title": "The tipping point: a mathematical model for the profit-driven\n  abandonment of restaurant tipping",
        "comments": "14 pages, 5 figures, supplementary material included",
        "journal-ref": "Chaos 28, 023109 (2018)",
        "doi": "10.1063/1.5004711",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The custom of voluntarily tipping for services rendered has gone in and out\nof fashion in America since its introduction in the 19th century. Restaurant\nowners that ban tipping in their establishments often claim that social justice\ndrives their decisions, but we show that rational profit-maximization may also\njustify the decisions. Here, we propose a conceptual model of restaurant\ncompetition for staff and customers, and we show that there exists a critical\nconventional tip rate at which restaurant owners should eliminate tipping to\nmaximize profit. Because the conventional tip rate has been increasing steadily\nfor the last several decades, our model suggests that restaurant owners may\nabandon tipping en masse when that critical tip rate is reached.\n"
    },
    {
        "paper_id": 1710.0703,
        "authors": "Masaaki Fujii, Akihiko Takahashi, Masayuki Takahashi",
        "title": "Asymptotic Expansion as Prior Knowledge in Deep Learning Method for high\n  dimensional BSDEs",
        "comments": "Forthcoming in APFM",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We demonstrate that the use of asymptotic expansion as prior knowledge in the\n\"deep BSDE solver\", which is a deep learning method for high dimensional BSDEs\nproposed by Weinan E, Han & Jentzen (2017), drastically reduces the loss\nfunction and accelerates the speed of convergence. We illustrate the technique\nand its implications by using Bergman's model with different lending and\nborrowing rates as a typical model for FVA as well as a class of solvable BSDEs\nwith quadratic growth drivers. We also present an extension of the deep BSDE\nsolver for reflected BSDEs representing American option prices.\n"
    },
    {
        "paper_id": 1710.07331,
        "authors": "Linda Ponta, Anna Carbone",
        "title": "Information measure for financial time series: quantifying short-term\n  market heterogeneity",
        "comments": null,
        "journal-ref": null,
        "doi": "10.1016/j.physa.2018.06.085",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  A well-interpretable measure of information has been recently proposed based\non a partition obtained by intersecting a random sequence with its moving\naverage. The partition yields disjoint sets of the sequence, which are then\nranked according to their size to form a probability distribution function and\nfinally fed in the expression of the Shannon entropy. In this work, such\nentropy measure is implemented on the time series of prices and volatilities of\nsix financial markets. The analysis has been performed, on tick-by-tick data\nsampled every minute for six years of data from 1999 to 2004, for a broad range\nof moving average windows and volatility horizons. The study shows that the\nentropy of the volatility series depends on the individual market, while the\nentropy of the price series is practically a market-invariant for the six\nmarkets. Finally, a cumulative information measure - the `Market Heterogeneity\nIndex'- is derived from the integral of the proposed entropy measure. The\nvalues of the Market Heterogeneity Index are discussed as possible tools for\noptimal portfolio construction and compared with those obtained by using the\nSharpe ratio a traditional risk diversity measure.\n"
    },
    {
        "paper_id": 1710.0734,
        "authors": "Lukas Pastorek",
        "title": "Frequency Based Index Estimating the Subclusters' Connection Strength",
        "comments": "Acta aerarii publici - 2013",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper, a frequency coefficient based on the Sen-Shorrocks-Thon (SST)\npoverty index notion is proposed. The clustering SST index can be used as the\nmethod for determination of the connection between similar neighbor\nsub-clusters. Consequently, connections can reveal existence of natural\nhomogeneous. Through estimation of the connection strength, we can also verify\ninformation about the estimated number of natural clusters that is necessary\nassumption of efficient market segmentation and campaign management and\nfinancial decisions. The index can be used as the complementary tool for the\nU-matrix visualization. The index is tested on an artificial dataset with known\nparameters and compared with results obtained by the Unified-distance matrix\nmethod.\n"
    },
    {
        "paper_id": 1710.0747,
        "authors": "Jing-Chao Chen, Yu Zhou, Xi Wang",
        "title": "Profitability of simple stationary technical trading rules with\n  high-frequency data of Chinese Index Futures",
        "comments": "24 pages,11 figures, 19 tables",
        "journal-ref": null,
        "doi": "10.1016/j.physa.2017.11.088",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Technical trading rules have been widely used by practitioners in financial\nmarkets for a long time. The profitability remains controversial and few\nconsider the stationarity of technical indicators used in trading rules. We\nconvert MA, KDJ and Bollinger bands into stationary processes and investigate\nthe profitability of these trading rules by using 3 high-frequency data(15s,30s\nand 60s) of CSI300 Stock Index Futures from January 4th 2012 to December 31st\n2016. Several performance and risk measures are adopted to assess the practical\nvalue of all trading rules directly while ADF-test is used to verify the\nstationarity and SPA test to check whether trading rules perform well due to\nintrinsic superiority or pure luck. The results show that there are several\nsignificant combinations of parameters for each indicator when transaction\ncosts are not taken into consideration. Once transaction costs are included,\ntrading profits will be eliminated completely. We also propose a method to\nreduce the risk of technical trading rules.\n"
    },
    {
        "paper_id": 1710.07481,
        "authors": "Christian Bayer, Peter K. Friz, Paul Gassiat, Joerg Martin, Benjamin\n  Stemper",
        "title": "A regularity structure for rough volatility",
        "comments": "Dedicated to Professor Jim Gatheral on the occasion of his 60th\n  birthday",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  A new paradigm recently emerged in financial modelling: rough (stochastic)\nvolatility, first observed by Gatheral et al. in high-frequency data,\nsubsequently derived within market microstructure models, also turned out to\ncapture parsimoniously key stylized facts of the entire implied volatility\nsurface, including extreme skews that were thought to be outside the scope of\nstochastic volatility. On the mathematical side, Markovianity and, partially,\nsemi-martingality are lost. In this paper we show that Hairer's regularity\nstructures, a major extension of rough path theory, which caused a revolution\nin the field of stochastic partial differential equations, also provides a new\nand powerful tool to analyze rough volatility models.\n"
    },
    {
        "paper_id": 1710.07492,
        "authors": "Michael B. Giles, Francisco Bernal",
        "title": "Multilevel estimation of expected exit times and other functionals of\n  stopped diffusions",
        "comments": "21 pages, 3 figures, to appear in SIAM/ASA Journal on Uncertainty\n  Quantification",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper proposes and analyses a new multilevel Monte Carlo method for the\nestimation of mean exit times for multi-dimensional Brownian diffusions, and\nassociated functionals which correspond to solutions to high-dimensional\nparabolic PDEs through the Feynman-Kac formula. In particular, it is proved\nthat the complexity to achieve an $\\varepsilon$ root-mean-square error is\n$O(\\varepsilon^{-2}\\, |\\!\\log \\varepsilon|^3)$.\n"
    },
    {
        "paper_id": 1710.07894,
        "authors": "Lesiba Ch. Galane, Rafa{\\l} M. {\\L}ochowski, Farai J. Mhlanga",
        "title": "On the quadratic variation of the model-free price paths with jumps",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We prove that the model-free typical (in the sense of Vovk) c\\`adl\\`ag price\npaths with mildly restricted downward jumps possess quadratic variation which\ndoes not depend on the specific sequence of partitions as long as these\npartitions are obtained from stopping times such that the oscillations of a\npath on the consecutive (half-open on the right) intervals of these partitions\ntend (in a specified sense) to 0. Finally, we also define quasi-explicit,\npartition independent quantities which tend to this quadratic variation.\n"
    },
    {
        "paper_id": 1710.07911,
        "authors": "Gaoyue Guo and Jan Obloj",
        "title": "Computational Methods for Martingale Optimal Transport problems",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We establish numerical methods for solving the martingale optimal transport\nproblem (MOT) - a version of the classical optimal transport with an additional\nmartingale constraint on transport's dynamics. We prove that the MOT value can\nbe approximated using linear programming (LP) problems which result from a\ndiscretisation of the marginal distributions combined with a suitable\nrelaxation of the martingale constraint. Specialising to dimension one, we\nprovide bounds on the convergence rate of the above scheme. We also show a\nstability result under only partial specification of the marginal\ndistributions. Finally, we specialise to a particular discretisation scheme\nwhich preserves the convex ordering and does not require the martingale\nrelaxation. We introduce an entropic regularisation for the corresponding LP\nproblem and detail the corresponding iterative Bregman projection. We also\nrewrite its dual problem as a minimisation problem without constraint and solve\nit by computing the concave envelope of scattered data.\n"
    },
    {
        "paper_id": 1710.07918,
        "authors": "Haoyong Chen, Lijia Han",
        "title": "Electricity Market Theory Based on Continuous Time Commodity Model",
        "comments": "17 pages, 8 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The recent research report of U.S. Department of Energy prompts us to\nre-examine the pricing theories applied in electricity market design. The\ntheory of spot pricing is the basis of electricity market design in many\ncountries, but it has two major drawbacks: one is that it is still based on the\ntraditional hourly scheduling/dispatch model, ignores the crucial time\ncontinuity in electric power production and consumption and does not treat the\ninter-temporal constraints seriously; the second is that it assumes that the\nelectricity products are homogeneous in the same dispatch period and cannot\ndistinguish the base, intermediate and peak power with obviously different\ntechnical and economic characteristics. To overcome the shortcomings, this\npaper presents a continuous time commodity model of electricity, including spot\npricing model and load duration model. The market optimization models under the\ntwo pricing mechanisms are established with the Riemann and Lebesgue integrals\nrespectively and the functional optimization problem are solved by the\nEuler-Lagrange equation to obtain the market equilibria. The feasibility of\npricing according to load duration is proved by strict mathematical derivation.\nSimulation results show that load duration pricing can correctly identify and\nvalue different attributes of generators, reduce the total electricity\npurchasing cost, and distribute profits among the power plants more equitably.\nThe theory and methods proposed in this paper will provide new ideas and\ntheoretical foundation for the development of electric power markets.\n"
    },
    {
        "paper_id": 1710.07959,
        "authors": "Shanshan Wang, Sebastian Neus\\\"u{\\ss} and Thomas Guhr",
        "title": "Grasping asymmetric information in market impacts",
        "comments": null,
        "journal-ref": "Eur. Phys. J. B (2018) 91: 266",
        "doi": "10.1140/epjb/e2018-80599-5",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The price impact for a single trade is estimated by the immediate response on\nan event time scale, i.e., the immediate change of midpoint prices before and\nafter a trade. We work out the price impacts across a correlated financial\nmarket. We quantify the asymmetries of the distributions and of the market\nstructures of cross-impacts, and find that the impacts across the market are\nasymmetric and non-random. Using spectral statistics and Shannon entropy, we\nvisualize the asymmetric information in price impacts. Also, we introduce an\nentropy of impacts to estimate the randomness between stocks. We show that the\nuseful information is encoded in the impacts corresponding to small entropy.\nThe stocks with large number of trades are more likely to impact others, while\nthe less traded stocks have higher probability to be impacted by others.\n"
    },
    {
        "paper_id": 1710.0845,
        "authors": "Peter A. Forsyth, George Labahn",
        "title": "$\\epsilon$-Monotone Fourier Methods for Optimal Stochastic Control in\n  Finance",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Stochastic control problems in finance often involve complex controls at\ndiscrete times. As a result numerically solving such problems, for example\nusing methods based on partial differential or integro-differential equations,\ninevitably give rise to low order accuracy, usually at most second order. In\nmany cases one can make use of Fourier methods to efficiently advance solutions\nbetween control monitoring dates and then apply numerical optimization methods\nacross decision times. However Fourier methods are not monotone and as a result\ngive rise to possible violations of arbitrage inequalities. This is problematic\nin the context of control problems, where the control is determined by\ncomparing value functions. In this paper we give a preprocessing step for\nFourier methods which involves projecting the Green's function onto the set of\nlinear basis functions. The resulting algorithm is guaranteed to be monotone\n(to within a tolerance), $\\ell_\\infty$-stable and satisfies an\n$\\epsilon$-discrete comparison principle. In addition the algorithm has the\nsame complexity per step as a standard Fourier method while at the same time\nhaving second order accuracy for smooth problems.\n"
    },
    {
        "paper_id": 1710.0886,
        "authors": "Jean de Carufel and Martin Brooks and Michael Stieber and Paul Britton",
        "title": "A Topological Approach to Scaling in Financial Data",
        "comments": "13 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  There is a large body of work, built on tools developed in mathematics and\nphysics, demonstrating that financial market prices exhibit self-similarity at\ndifferent scales. In this paper, we explore the use of analytical topology to\ncharacterize financial price series. While wavelet and Fourier transforms\ndecompose a signal into sets of wavelets and power spectrum respectively, the\napproach presented herein decomposes a time series into components of its total\nvariation. This property is naturally suited for the analysis of scaling\ncharacteristics in fractals.\n"
    },
    {
        "paper_id": 1710.09419,
        "authors": "Bent Flyvbjerg, Chi-keung Hon, and Wing Huen Fok",
        "title": "Reference Class Forecasting for Hong Kong's Major Roadworks Projects",
        "comments": null,
        "journal-ref": "Proceedings of the Institution of Civil Engineers 169, November\n  2016, Issue CE6, pp. 17-24",
        "doi": "10.1680/jcien.15.00075",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Reference class forecasting is a method to remove optimism bias and strategic\nmisrepresentation in infrastructure projects and programmes. In 2012 the Hong\nKong government's Development Bureau commissioned a feasibility study on\nreference class forecasting in Hong Kong - a first for the Asia-Pacific region.\nThis study involved 25 roadwork projects, for which forecast costs and\ndurations were compared with actual outcomes. The analysis established and\nverified the statistical distribution of the forecast accuracy at various\nstages of project development, and benchmarked the projects against a sample of\n863 similar projects. The study contributed to the understanding of how to\nimprove forecasts by de-biasing early estimates, explicitly considering the\nrisk appetite of decision makers, and safeguarding public funding allocation by\nbalancing exceedance and under-use of project budgets.\n"
    },
    {
        "paper_id": 1710.09476,
        "authors": "Matthew Lorig, Zhou Zhou, Bin Zou",
        "title": "A Mathematical Analysis of Technical Analysis",
        "comments": "29 pages, 0 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper, we investigate trading strategies based on exponential moving\naverages (ExpMAs) of an underlying risky asset. We study both logarithmic\nutility maximization and long-term growth rate maximization problems and find\nclosed-form solutions when the drift of the underlying is modeled by either an\nOrnstein-Uhlenbeck process or a two-state continuous-time Markov chain. For the\ncase of an Ornstein-Uhlenbeck drift, we carry out several Monte Carlo\nexperiments in order to investigate how the performance of optimal ExpMA\nstrategies is affected by variations in model parameters and by transaction\ncosts.\n"
    },
    {
        "paper_id": 1710.09587,
        "authors": "Taras Bodnar, Solomiia Dmytriv, Nestor Parolya and Wolfgang Schmid",
        "title": "Tests for the weights of the global minimum variance portfolio in a\n  high-dimensional setting",
        "comments": "16 pages, 10 figures (final version, accepted for publication in IEEE\n  Transactions of Signal Processing)",
        "journal-ref": "IEEE Transactions on Signal Processing, Volume: 67, Issue: 17,\n  2019",
        "doi": "10.1109/TSP.2019.2929964",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this study, we construct two tests for the weights of the global minimum\nvariance portfolio (GMVP) in a high-dimensional setting, namely, when the\nnumber of assets $p$ depends on the sample size $n$ such that $\\frac{p}{n}\\to c\n\\in (0,1)$ as $n$ tends to infinity. In the case of a singular covariance\nmatrix with rank equal to $q$ we assume that $q/n\\to \\tilde{c}\\in(0, 1)$ as\n$n\\to\\infty$. The considered tests are based on the sample estimator and on the\nshrinkage estimator of the GMVP weights. We derive the asymptotic distributions\nof the test statistics under the null and alternative hypotheses. Moreover, we\nprovide a simulation study where the power functions and the receiver operating\ncharacteristic curves of the proposed tests are compared with other existing\napproaches. We observe that the test based on the shrinkage estimator performs\nwell even for values of $c$ close to one.\n"
    },
    {
        "paper_id": 1710.09678,
        "authors": "Bent Flyvbjerg and J. Rodney Turner",
        "title": "Do Classics Exist in Megaproject Management?",
        "comments": null,
        "journal-ref": null,
        "doi": "10.1016/j.ijproman.2017.07.006",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper asks, \"Do classics exist in megaproject management?\" We identify\nthree types of classic texts: conventional, Kuhnian, and citation classics. We\nfind that the answer to our question depends on the definition of \"classic\"\nemployed. First, \"citation classics\" do exist in megaproject management, and\nthey perform remarkably well when compared to the rest of the management\nliterature. A preliminary Top Ten of citation classics is presented. Second,\nthere is no indication that \"conventional classics\" exist in megaproject\nmanagement, i.e., texts recognized as definitive by a majority of experts.\nThird, there is also no consensus as to whether \"Kuhnian classics\" exist, i.e.,\ntexts with paradigmatic clout. The importance of classics seems to be accepted,\nhowever, just as work to develop, discuss, and consolidate classics is seen as\nessential by megaproject scholars. A set of guidelines is presented for\ndeveloping classics in megaproject management research.\n"
    },
    {
        "paper_id": 1710.10143,
        "authors": "Mika J. Straka, Guido Caldarelli, Tiziano Squartini, Fabio Saracco",
        "title": "From Ecology to Finance (and Back?): Recent Advancements in the Analysis\n  of Bipartite Networks",
        "comments": "26 pages, 12 Figures",
        "journal-ref": "J. Stat. Phys. (2018)",
        "doi": "10.1007/s10955-018-2039-4",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Bipartite networks provide an insightful representation of many systems,\nranging from mutualistic networks of species interactions to investment\nnetworks in finance. The analysis of their topological structures has revealed\nthe ubiquitous presence of properties which seem to characterize many -\napparently different - systems. Nestedness, for example, has been observed in\nplants-pollinator as well as in country-product trade networks. This has raised\nquestions about the significance of these patterns, which are often believed to\nconstitute a genuine signature of self-organization. Here, we review several\nmethods that have been developed for the analysis of such evidence. Due to the\ninterdisciplinary character of complex networks, tools developed in one field,\nfor example ecology, can greatly enrich other areas of research, such as\neconomy and finance, and vice versa. With this in mind, we briefly review\nseveral entropy-based bipartite null models that have been recently proposed\nand discuss their application to several real-world systems. The focus on these\nmodels is motivated by the fact that they show three very desirable features:\nanalytical character, general applicability and versatility. In this respect,\nentropy-based methods have been proven to perform satisfactorily both in\nproviding benchmarks for testing evidence-based null hypotheses and in\nreconstructing unknown network configurations from partial information. On top\nof that, entropy-based models have been successfully employed to analyze\necological as well as economic systems, thus representing an ideal,\ninterdisciplinary tool to approach the study of bipartite complex systems.\n[...]\n"
    },
    {
        "paper_id": 1710.10293,
        "authors": "Damir Filipovic and Martin Larsson and Tony Ware",
        "title": "Polynomial processes for power prices",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Polynomial processes have the property that expectations of polynomial\nfunctions (of degree $n$, say) of the future state of the process conditional\non the current state are given by polynomials (of degree $\\leq n$) of the\ncurrent state. Here we explore the application of polynomial processes in the\ncontext of structural models for energy prices. We focus on the example of\nAlberta power prices, derive one- and two-factor models for spot prices. We\nexamine their performance in numerical experiments, and demonstrate that the\nrichness of the dynamics they are able to generate makes them well suited for\nmodelling even extreme examples of energy price behaviour.\n"
    },
    {
        "paper_id": 1710.10377,
        "authors": "Divesh Aggarwal, Gavin K. Brennen, Troy Lee, Miklos Santha, Marco\n  Tomamichel",
        "title": "Quantum attacks on Bitcoin, and how to protect against them",
        "comments": "21 pages, 6 figures. For a rough update on the progress of Quantum\n  devices and prognostications on time from now to break Digital signatures,\n  see https://www.quantumcryptopocalypse.com/quantum-moores-law/",
        "journal-ref": "Ledger, [S.l.], v. 3, oct. 2018",
        "doi": "10.5195/ledger.2018.127",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The key cryptographic protocols used to secure the internet and financial\ntransactions of today are all susceptible to attack by the development of a\nsufficiently large quantum computer. One particular area at risk are\ncryptocurrencies, a market currently worth over 150 billion USD. We investigate\nthe risk of Bitcoin, and other cryptocurrencies, to attacks by quantum\ncomputers. We find that the proof-of-work used by Bitcoin is relatively\nresistant to substantial speedup by quantum computers in the next 10 years,\nmainly because specialized ASIC miners are extremely fast compared to the\nestimated clock speed of near-term quantum computers. On the other hand, the\nelliptic curve signature scheme used by Bitcoin is much more at risk, and could\nbe completely broken by a quantum computer as early as 2027, by the most\noptimistic estimates. We analyze an alternative proof-of-work called Momentum,\nbased on finding collisions in a hash function, that is even more resistant to\nspeedup by a quantum computer. We also review the available post-quantum\nsignature schemes to see which one would best meet the security and efficiency\nrequirements of blockchain applications.\n"
    },
    {
        "paper_id": 1710.10487,
        "authors": "Jingtang Ma, Wenyuan Li, Harry Zheng",
        "title": "Dual control Monte Carlo method for tight bounds of value function under\n  Heston stochastic volatility model",
        "comments": "25 pages, 2 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The aim of this paper is to study the fast computation of the lower and upper\nbounds on the value function for utility maximization under the Heston\nstochastic volatility model with general utility functions. It is well known\nthere is a closed form solution of the HJB equation for power utility due to\nits homothetic property. It is not possible to get closed form solution for\ngeneral utilities and there is little literature on the numerical scheme to\nsolve the HJB equation for the Heston model. In this paper we propose an\nefficient dual control Monte Carlo method for computing tight lower and upper\nbounds of the value function. We identify a particular form of the dual control\nwhich leads to the closed form upper bound for a class of utility functions,\nincluding power, non-HARA and Yarri utilities. Finally, we perform some\nnumerical tests to see the efficiency, accuracy, and robustness of the method.\nThe numerical results support strongly our proposed scheme.\n"
    },
    {
        "paper_id": 1710.10692,
        "authors": "Wenhao Li, Bolong Wang, Tianxiang Shen, Ronghua Zhu, Dehui Wang",
        "title": "Research on ruin probability of risk model based on AR(1) series",
        "comments": "9 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this text, we establish the risk model based on AR(1) series and propose\nthe basic model which has a dependent structure under intensity of claim\nnumber. Considering some properties of the risk model, we take advantage of\nnewton iteration method to figure out the adjustment coefficient and estimate\nthe exponential upper bound of ruin probability. This is significant to refine\nthe research of ruin theory. As a result, our theory will help develop\ninsurance industry stably.\n"
    },
    {
        "paper_id": 1710.10711,
        "authors": "Archil Gulisashvili",
        "title": "Large deviation principle for Volterra type fractional stochastic\n  volatility models",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We study fractional stochastic volatility models in which the volatility\nprocess is a positive continuous function $\\sigma$ of a continuous Gaussian\nprocess $\\widehat{B}$. Forde and Zhang established a large deviation principle\nfor the log-price process in such a model under the assumptions that the\nfunction $\\sigma$ is globally H\\\"{o}lder-continuous and the process\n$\\widehat{B}$ is fractional Brownian motion. In the present paper, we prove a\nsimilar small-noise large deviation principle under weaker restrictions on\n$\\sigma$ and $\\widehat{B}$. We assume that $\\sigma$ satisfies a mild local\nregularity condition, while the process $\\widehat{B}$ is a Volterra type\nGaussian process. Under an additional assumption of the self-similarity of the\nprocess $\\widehat{B}$, we derive a large deviation principle in the small-time\nregime. As an application, we obtain asymptotic formulas for binary options,\ncall and put pricing functions, and the implied volatility in certain mixed\nregimes.\n"
    },
    {
        "paper_id": 1710.1098,
        "authors": "Matteo Serafino, Andrea Gabrielli, Guido Caldarelli, Giulio Cimini",
        "title": "Statistical validation of financial time series via visibility graph",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Statistical physics of complex systems exploits network theory not only to\nmodel, but also to effectively extract information from many dynamical\nreal-world systems. A pivotal case of study is given by financial systems:\nmarket prediction represents an unsolved scientific challenge yet with crucial\nimplications for society, as financial crises have devastating effects on real\neconomies. Thus, nowadays the quest for a robust estimator of market efficiency\nis both a scientific and institutional priority. In this work we study the\nvisibility graphs built from the time series of several trade market indices.\nWe propose a validation procedure for each link of these graphs against a null\nhypothesis derived from ARCH-type modeling of such series. Building on this\nframework, we devise a market indicator that turns out to be highly correlated\nand even predictive of financial instability periods.\n"
    },
    {
        "paper_id": 1710.11019,
        "authors": "Florian Knobloch, Hector Pollitt, Unnada Chewpreecha, Vassilis\n  Daioglou, Jean-Francois Mercure",
        "title": "Simulating the deep decarbonisation of residential heating for limiting\n  global warming to 1.5C",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Whole-economy scenarios for limiting global warming to 1.5C suggest that\ndirect carbon emissions in the buildings sector should decrease to almost zero\nby 2050, but leave unanswered the question how this could be achieved by\nreal-world policies. We take a modelling-based approach for simulating which\npolicy measures could induce an almost-complete decarbonisation of residential\nheating, the by far largest source of direct emissions in residential\nbuildings. Under which assumptions is it possible, and how long would it take?\nPolicy effectiveness highly depends on behavioural decision- making by\nhouseholds, especially in a context of deep decarbonisation and rapid\ntransformation. We therefore use the non-equilibrium bottom-up model FTT:Heat\nto simulate policies for a transition towards low-carbon heating in a context\nof inertia and bounded rationality, focusing on the uptake of heating\ntechnologies. Results indicate that the near-zero decarbonisation is achievable\nby 2050, but requires substantial policy efforts. Policy mixes are projected to\nbe more effective and robust for driving the market of efficient low-carbon\ntechnologies, compared to the reliance on a carbon tax as the only policy\ninstrument. In combination with subsidies for renewables, near-complete\ndecarbonisation could be achieved with a residential carbon tax of\n50-200Euro/tCO2. The policy-induced technology transition would increase\naverage heating costs faced by households initially, but could also lead to\ncost reductions in most world regions in the medium term. Model projections\nillustrate the uncertainty that is attached to household behaviour for\nprematurely replacing heating systems.\n"
    },
    {
        "paper_id": 1710.11065,
        "authors": "Zied Ben Salah, Jos\\'e Garrido",
        "title": "On Fair Reinsurance Premiums; Capital Injections in a Perturbed Risk\n  Model",
        "comments": "23 pages, 3 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We consider a risk model where deficits after ruin are covered by a new type\nof reinsurance contract that provides capital injections. To allow the\ninsurance company's survival after ruin, the reinsurer injects capital only at\nruin times caused by jumps larger than a chosen retention level. Otherwise\ncapital must be raised from the shareholders for small deficits. The problem\nhere is to determine adequate reinsurance premiums. It seems fair to base the\nnet reinsurance premium on the discounted expected value of any future capital\ninjections. Inspired by the results of Huzak et al. (2004) and Ben Salah (2014)\non successive ruin events, we show that an explicit formula for these\nreinsurance premiums exists in a setting where aggregate claims are modeled by\na subordinator and a Brownian perturbation. Here ruin events are due either to\nBrownian oscillations or jumps and reinsurance capital injections only apply in\nthe latter case. The results are illustrated explicitly for two specific risk\nmodels and in some numerical examples.\n"
    },
    {
        "paper_id": 1710.11184,
        "authors": "Tianyu Cui, Francesco Caravelli, Cozmin Ududec",
        "title": "Correlations and Clustering in Wholesale Electricity Markets",
        "comments": "30 pages, several pictures",
        "journal-ref": "Physica A 492 (2018)",
        "doi": "10.1016/j.physa.2017.11.077",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We study the structure of locational marginal prices in day-ahead and\nreal-time wholesale electricity markets. In particular, we consider the case of\ntwo North American markets and show that the price correlations contain\ninformation on the locational structure of the grid. We study various\nclustering methods and introduce a type of correlation function based on event\nsynchronization for spiky time series, and another based on string correlations\nof location names provided by the markets. This allows us to reconstruct\naspects of the locational structure of the grid.\n"
    },
    {
        "paper_id": 1710.11232,
        "authors": "Elisa Alos, Antoine Jacquier, Jorge Leon",
        "title": "The implied volatility of Forward-Start options: ATM short-time level,\n  skew and curvature",
        "comments": "18 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Using Malliavin Calculus techniques, we derive closed-form expressions for\nthe at-the-money behaviour of the forward implied volatility, its skew and its\ncurvature, in general Markovian stochastic volatility models with continuous\npaths.\n"
    },
    {
        "paper_id": 1710.11432,
        "authors": "Qizhu Liang and Jie Xiong",
        "title": "Stochastic maximum principle under probability distortion",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Within the framework of the cumulative prospective theory of Kahneman and\nTversky, this paper considers a continuous-time behavioral portfolio selection\nproblem whose model includes both running and terminal terms in the objective\nfunctional. Despite the existence of S-shaped utility functions and probability\ndistortions, a necessary condition for optimality is derived. The results are\napplied to various examples.\n"
    },
    {
        "paper_id": 1710.11435,
        "authors": "Giorgia Callegaro and Lucio Fiorin and Andrea Pallavicini",
        "title": "Quantization goes Polynomial",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Quantization algorithms have been successfully adopted to option pricing in\nfinance thanks to the high convergence rate of the numerical approximation. In\nparticular, very recently, recursive marginal quantization has been proven to\nbe a flexible and versatile tool when applied to stochastic volatility\nprocesses. In this paper we apply for the first time quantization techniques to\nthe family of polynomial processes, by exploiting their peculiar nature. We\nfocus our analysis on the stochastic volatility Jacobi process, by presenting\ntwo alternative quantization procedures: the first is a new discretization\ntechnique, whose foundation lies on the polynomial structure of the underlying\nprocess and which is suitable for vanilla option pricing, the second is based\non recursive marginal quantization and it allows for pricing of (vanilla and)\nexotic derivatives. We prove theoretical results to assess the induced\napproximation errors, and we describe in numerical examples practical tools for\nfast vanilla and exotic option pricing.\n"
    },
    {
        "paper_id": 1710.11512,
        "authors": "Fabio Caccioli, Paolo Barucca, and Teruyoshi Kobayashi",
        "title": "Network models of financial systemic risk: A review",
        "comments": "33 pages, 6 figures",
        "journal-ref": "Journal of Computational Social Science (2018) 1: 81-114",
        "doi": "10.1007/s42001-017-0008-3",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The global financial system can be represented as a large complex network in\nwhich banks, hedge funds and other financial institutions are interconnected to\neach other through visible and invisible financial linkages. Recently, a lot of\nattention has been paid to the understanding of the mechanisms that can lead to\na breakdown of this network. This can happen when the existing financial links\nturn from being a means of risk diversification to channels for the propagation\nof risk across financial institutions. In this review article, we summarize\nrecent developments in the modeling of financial systemic risk. We focus in\nparticular on network approaches, such as models of default cascades due to\nbilateral exposures or to overlapping portfolios, and we also report on recent\nfindings on the empirical structure of interbank networks. The current review\nprovides a landscape of the newly arising interdisciplinary field lying at the\nintersection of several disciplines, such as network science, physics,\nengineering, economics, and ecology.\n"
    },
    {
        "paper_id": 1711.00307,
        "authors": "Fred Espen Benth, Asma Khedher, Mich\\`ele Vanmaele",
        "title": "Pricing of commodity derivatives on processes with memory",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Spot option prices, forwards and options on forwards relevant for the\ncommodity markets are computed when the underlying process S is modelled as an\nexponential of a process {\\xi} with memory as e.g. a L\\'evy semi-stationary\nprocess. Moreover a risk premium \\r{ho} representing storage costs,\nilliquidity, convenience yield or insurance costs is explicitly modelled as an\nOrnstein-Uhlenbeck type of dynamics with a mean level that depends on the same\nmemory term as the commodity. Also the interest rate is assumed to be\nstochastic. To show the existence of an equivalent pricing measure Q for S we\nrelate the stochastic differential equation for {\\xi} to the generalised\nLangevin equation. When the interest rate is deterministic the process ({\\xi};\n\\r{ho}) has an affine structure under the pricing measure Q and an explicit\nexpression for the option price is derived in terms of the Fourier transform of\nthe payoff function.\n"
    },
    {
        "paper_id": 1711.0037,
        "authors": "Michel Baes, Cosimo Munari",
        "title": "A continuous selection for optimal portfolios under convex risk measures\n  does not always exist",
        "comments": "arXiv admin note: text overlap with arXiv:1702.01936",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  One of the crucial problems in mathematical finance is to mitigate the risk\nof a financial position by setting up hedging positions of eligible financial\nsecurities. This leads to focusing on set-valued maps associating to any\nfinancial position the set of those eligible payoffs that reduce the risk of\nthe position to a target acceptable level at the lowest possible cost. Among\nother properties of such maps, the ability to ensure lower semicontinuity and\ncontinuous selections is key from an operational perspective. It is known that\nlower semicontinuity generally fails in an infinite-dimensional setting. In\nthis note we show that neither lower semicontinuity nor, more surprisingly, the\nexistence of continuous selections can be a priori guaranteed even in a\nfinite-dimensional setting. In particular, this failure is possible under\narbitrage-free markets and convex risk measures.\n"
    },
    {
        "paper_id": 1711.00427,
        "authors": "Eyal Neuman and Mathieu Rosenbaum",
        "title": "Fractional Brownian motion with zero Hurst parameter: a rough volatility\n  viewpoint",
        "comments": "13 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Rough volatility models are becoming increasingly popular in quantitative\nfinance. In this framework, one considers that the behavior of the\nlog-volatility process of a financial asset is close to that of a fractional\nBrownian motion with Hurst parameter around 0.1. Motivated by this, we wish to\ndefine a natural and relevant limit for the fractional Brownian motion when $H$\ngoes to zero. We show that once properly normalized, the fractional Brownian\nmotion converges to a Gaussian random distribution which is very close to a\nlog-correlated random field.\n"
    },
    {
        "paper_id": 1711.00443,
        "authors": "John Armstrong, Damiano Brigo",
        "title": "Optimizing S-shaped utility and implications for risk management",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We consider market players with tail-risk-seeking behaviour as exemplified by\nthe S-shaped utility introduced by Kahneman and Tversky. We argue that risk\nmeasures such as value at risk (VaR) and expected shortfall (ES) are\nineffective in constraining such players. We show that, in many standard market\nmodels, product design aimed at utility maximization is not constrained at all\nby VaR or ES bounds: the maximized utility corresponding to the optimal payoff\nis the same with or without ES constraints. By contrast we show that, in\nreasonable markets, risk management constraints based on a second more\nconventional concave utility function can reduce the maximum S-shaped utility\nthat can be achieved by the investor, even if the constraining utility function\nis only rather modestly concave. It follows that product designs leading to\nunbounded S-shaped utilities will lead to unbounded negative expected\nconstraining utilities when measured with such conventional utility functions.\nTo prove these latter results we solve a general problem of optimizing an\ninvestor expected utility under risk management constraints where both investor\nand risk manager have conventional concave utility functions, but the investor\nhas limited liability. We illustrate our results throughout with the example of\nthe Black--Scholes option market. These results are particularly important\ngiven the historical role of VaR and that ES was endorsed by the Basel\ncommittee in 2012--2013.\n"
    },
    {
        "paper_id": 1711.00708,
        "authors": "Stefan Rass",
        "title": "On Game-Theoretic Risk Management (Part Three) - Modeling and\n  Applications",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The game-theoretic risk management framework put forth in the precursor\nreports \"Towards a Theory of Games with Payoffs that are\nProbability-Distributions\" (arXiv:1506.07368 [q-fin.EC]) and \"Algorithms to\nCompute Nash-Equilibria in Games with Distributions as Payoffs\"\n(arXiv:1511.08591v1 [q-fin.EC]) is herein concluded by discussing how to\nintegrate the previously developed theory into risk management processes. To\nthis end, we discuss how loss models (primarily but not exclusively\nnon-parametric) can be constructed from data. Furthermore, hints are given on\nhow a meaningful game theoretic model can be set up, and how it can be used in\nvarious stages of the ISO 27000 risk management process. Examples related to\nadvanced persistent threats and social engineering are given. We conclude by a\ndiscussion on the meaning and practical use of (mixed) Nash equilibria\nequilibria for risk management.\n"
    },
    {
        "paper_id": 1711.00737,
        "authors": "Martin Keller-Ressel",
        "title": "Erratum to: `Yield curve shapes and the asymptotic short rate\n  distribution in affine one-factor models'",
        "comments": "minor update",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper corrects an error in [Keller-Ressel, M. and Steiner T. \"Yield\ncurve shapes and the asymptotic short rate distribution in affine one-factor\nmodels.\" Finance and Stochastics 12.2 (2008): 149-172]. The error concerns the\ncorrect expression for the boundary between normal and humped yield curve\nbehavior in affine one-factor short-rate models.\n"
    },
    {
        "paper_id": 1711.01017,
        "authors": "Arash Fahim and Wan-Yu Tsai",
        "title": "A Numerical Scheme for A Singular control problem:\n  Investment-Consumption Under Proportional Transaction Costs",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper concerns the numerical solution of a fully nonlinear parabolic\ndouble obstacle problem arising from a finite portfolio selection with\nproportional transaction costs. We consider the optimal allocation of wealth\namong multiple stocks and a bank account in order to maximize the finite\nhorizon discounted utility of consumption. The problem is mainly governed by a\ntime-dependent Hamilton-Jacobi-Bellman equation with gradient constraints. We\npropose a numerical method which is composed of Monte Carlo simulation to take\nadvantage of the high-dimensional properties and finite difference method to\napproximate the gradients of the value function. Numerical results illustrate\nbehaviors of the optimal trading strategies and also satisfy all qualitative\nproperties proved in Dai et al. (2009) and Chen and Dai (2013).\n"
    },
    {
        "paper_id": 1711.01756,
        "authors": "Renko Siebols",
        "title": "Cash Accumulation Strategy based on Optimal Replication of Random Claims\n  with Ordinary Integrals",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper presents a numerical model to solve the problem of cash\naccumulation strategies for products with an unknown future price, like assets.\nStock prices are modeled by a discretized Wiener Process, and by the means of\nordinary integrals this Wiener Process will be exactly matched at a preset\nterminal time. Three applications of the model are presented: accumulating cash\nfor a single asset, for set of different assets, and for a proportion of the\nexcess achieved by a certain asset. Furthermore, an analysis of the efficiency\nof the model as function of different parameters is performed.\n"
    },
    {
        "paper_id": 1711.0176,
        "authors": "Calisto Guambe and Rodwell Kufakunesu",
        "title": "Optimal investment-consumption and life insurance selection problem\n  under inflation. A BSDE approach",
        "comments": "19 pages in PDF format",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We discuss an optimal investment, consumption and insurance problem of a wage\nearner under inflation. Assume a wage earner investing in a real money account\nand three asset prices, namely: a real zero coupon bond, the inflation-linked\nreal money account and a risky share described by jump-diffusion processes.\nUsing the theory of quadratic-exponential backward stochastic differential\nequation (BSDE) with jumps approach, we derive the optimal strategy for the two\ntypical utilities (exponential and power) and the value function is\ncharacterized as a solution of BSDE with jumps. Finally, we derive the explicit\nsolutions for the optimal investment in both cases of exponential and power\nutility functions for a diffusion case.\n"
    },
    {
        "paper_id": 1711.0214,
        "authors": "Matyas Barczy, Mohamed Ben Alaya, Ahmed Kebaier, Gyula Pap",
        "title": "Asymptotic properties of maximum likelihood estimator for the growth\n  rate of a stable CIR process based on continuous time observations",
        "comments": "47 pages. In Appendices we recall some notions and statements from\n  arXiv:1509.08869 and arXiv:1609.05865",
        "journal-ref": "Statistics 53(3), (2019), 533-568",
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We consider a stable Cox--Ingersoll--Ross process driven by a standard Wiener\nprocess and a spectrally positive strictly stable L\\'evy process, and we study\nasymptotic properties of the maximum likelihood estimator (MLE) for its growth\nrate based on continuous time observations. We distinguish three cases:\nsubcritical, critical and supercritical. In all cases we prove strong\nconsistency of the MLE in question, in the subcritical case asymptotic\nnormality, and in the supercritical case asymptotic mixed normality are shown\nas well. In the critical case the description of the asymptotic behavior of the\nMLE in question remains open.\n"
    },
    {
        "paper_id": 1711.02573,
        "authors": "Torsten Trimborn, Martin Frank, Stephan Martin",
        "title": "Mean Field Limit of a Behavioral Financial Market Model",
        "comments": null,
        "journal-ref": null,
        "doi": "10.1016/j.physa.2018.03.079",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In the past decade there has been a growing interest in agent-based\neconophysical financial market models. The goal of these models is to gain\nfurther insights into stylized facts of financial data. We derive the mean\nfield limit of the econophysical model by Cross, Grinfeld, Lamba and Seaman\n(Physica A, 354) and show that the kinetic limit is a good approximation of the\noriginal model. Our kinetic model is able to replicate some of the most\nprominent stylized facts, namely fat-tails of asset returns, uncorrelated stock\nprice returns and volatility clustering. Interestingly, psychological\nmisperceptions of investors can be accounted to be the origin of the appearance\nof stylized facts. The mesoscopic model allows us to study the model\nanalytically. We derive steady state solutions and entropy bounds of the\ndeterministic skeleton. These first analytical results already guide us to\nexplanations for the complex dynamics of the model.\n"
    },
    {
        "paper_id": 1711.026,
        "authors": "Brian P. Hanley",
        "title": "The perverse incentive for insurance instruments that are derivatives:\n  solving the jackpot problem with a clawback lien for default insurance notes",
        "comments": "10 pages, 8 figures; EDCS terminolgy, miscellaneous clarifications",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  When an insurance note is also a derivative a serious problem arises because\na derivative must be fulfilled immediately. This feature of derivatives\nprevents claims processing procedures that screen out ineligible claims. This,\nin turn, creates a perverse incentive for insured holders of notes to commit\nacts that result in payment. This problem first surfaced with CDS contracts,\nwhich are part of a class of loan insurance I term a default insurance note.\n  Without an address to this problem, within the average range of returns for a\nlarge venture capital portfolio, a venture-bank makes less money the better\ntheir investments do, in a continuous function. The highest rate of return is a\ntotal loss, 64% more than a top portfolio.\n  Here, a strategy for removing this perverse incentive is defined, consisting\nof a clawback lien that returns part of the payment value as a lien on the firm\nthat is the beneficiary of the insurance. This is presented as the final major\ncomponent for implementing a default insurance note system so that\nventure-banking can operate to maximum benefit. Removing the perverse incentive\nalso minimizes disincentive for underwriters to deny DIN coverage to new\nventure capital firms, or to those firms that have historical earnings which\nare below average.\n"
    },
    {
        "paper_id": 1711.02626,
        "authors": "Balazs Vedres, Carl Nordlund",
        "title": "Dis-embedded Openness: Inequalities in European Economic Integration at\n  the Sectoral Level",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The process of European integration resulted in a marked increase in\ntransnational economic flows, yet regional inequalities along many\ndevelopmental indicators remain. We analyze the unevenness of European\neconomies with respect to the embedding of export sectors in upstream domestic\nflows, and their dependency on dominant export partners. We use the WIOD data\nset of sectoral flows for the period of 1995-2011 for 24 European countries. We\nfound that East European economies were significantly more likely to experience\nincreasing unevenness and dependency with increasing openness, while core\ncountries of Europe managed to decrease their unevenness while increasing their\nopenness. Nevertheless, by analyzing the trajectories of changes for each\ncountry, we see that East European countries are also experiencing a turning\npoint, either switching to a path similar to the core, or to a retrograde path\nwith decreasing openness. We analyze our data using pooled time series models\nand case studies of country trajectories.\n"
    },
    {
        "paper_id": 1711.02764,
        "authors": "Daniel Bartl, Michael Kupper, Ariel Neufeld",
        "title": "Pathwise superhedging on prediction sets",
        "comments": null,
        "journal-ref": "Finance and Stochastics, 24(1): 215-248, 2020",
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper we provide a pricing-hedging duality for the model-independent\nsuperhedging price with respect to a prediction set $\\Xi\\subseteq C[0,T]$,\nwhere the superhedging property needs to hold pathwise, but only for paths\nlying in $\\Xi$. For any Borel measurable claim $\\xi$ which is bounded from\nbelow, the superhedging price coincides with the supremum over all pricing\nfunctionals $\\mathbb{E}_{\\mathbb{Q}}[\\xi]$ with respect to martingale measures\n$\\mathbb{Q}$ concentrated on the prediction set $\\Xi$. This allows to include\nbeliefs in future paths of the price process expressed by the set $\\Xi$, while\neliminating all those which are seen as impossible. Moreover, we provide\nseveral examples to justify our setup.\n"
    },
    {
        "paper_id": 1711.02784,
        "authors": "Nassif Ghoussoub, Young-Heon Kim, and Tongseok Lim",
        "title": "Optimal Brownian Stopping between radially symmetric marginals in\n  general dimensions",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Given an initial (resp., terminal) probability measure $\\mu$ (resp., $\\nu$)\non $\\mathbb{R}^d$, we characterize those optimal stopping times $\\tau$ that\nmaximize or minimize the functional $\\mathbb{E} |B_0 - B_\\tau|^{\\alpha}$,\n$\\alpha > 0$, where $(B_t)_t$ is Brownian motion with initial law $B_0\\sim \\mu$\nand with final distribution --once stopped at $\\tau$-- equal to $B_\\tau\\sim\n\\nu$.\n  The existence of such stopping times is guaranteed by Skorohod-type\nembeddings of probability measures in \"subharmoic order\" into Brownian motion.\nThis problem is equivalent to an optimal mass transport problem with certain\nconstraints, namely the optimal subharmonic martingale transport. Under the\nassumption of radial symmetry on $\\mu$ and $\\nu$, we show that the optimal\nstopping time is a hitting time of a suitable barrier, hence is non-randomized\nand is unique.\n"
    },
    {
        "paper_id": 1711.02808,
        "authors": "Kevin Fergusson and Eckhard Platen",
        "title": "Less-Expensive Valuation of Long Term Annuities Linked to Mortality,\n  Cash and Equity",
        "comments": "30-40 pages, 11 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper proposes a paradigm shift in the valuation of long term annuities,\naway from classical no-arbitrage valuation towards valuation under the real\nworld probability measure. Furthermore, we apply this valuation method to two\nexamples of annuity products, one having annual payments linked to a mortality\nindex and the savings account and the other having annual payments linked to a\nmortality index and an equity index with a guarantee that is linked to the same\nmortality index and the savings account. Out-of-sample hedge simulations\ndemonstrate the effectiveness of real world valuation.\n  In contrast to risk neutral valuation, which is a form of relative valuation,\nthe long term average excess return of the equity market comes into play.\nInstead of the savings account, the num\\'eraire portfolio is employed as the\nfundamental unit of value in the analysis. The num\\'eraire portfolio is the\nstrictly positive, tradable portfolio that when used as benchmark makes all\nbenchmarked nonnegative portfolios supermartingales. The benchmarked real world\nvalue of a benchmarked contingent claim equals its real world conditional\nexpectation. This yields the minimal possible value for its hedgeable part and\nminimizes the fluctuations for its benchmarked hedge error. Under classical\nassumptions, actuarial and risk neutral valuation emerge as special cases of\nthe proposed real world valuation. In long term liability and asset valuation,\nthe proposed real world valuation can lead to significantly lower values than\nsuggested by classical approaches when an equivalent risk neutral probability\nmeasure does not exist.\n"
    },
    {
        "paper_id": 1711.02925,
        "authors": "Martin Magris, Perttu Barholm, Juho Kanniainen",
        "title": "Implied volatility smile dynamics in the presence of jumps",
        "comments": "Paper accepted and presented at IAAE 2017 conference, June 26-30\n  2017, Sapporo, Japan",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The main purpose of this work is to examine the behavior of the implied\nvolatility smiles around jumps, contributing to the literature with a\nhigh-frequency analysis of the smile dynamics based on intra-day option data.\nFrom our high-frequency SPX S\\&P500 index option dataset, we utilize the first\nthree principal components to characterize the implied volatility smile and\nanalyze its dynamics by the distribution of the scores' means and variances and\nother statistics for the first hour of the day, in scenarios where jumps are\ndetected and not. Our analyses clearly suggest that changes in the volatility\nsmiles have abnormal properties around jumps compared with the absence of\njumps, regardless of maturity and type of the option.\n"
    },
    {
        "paper_id": 1711.02939,
        "authors": "Zhou Yang, Gechun Liang, Chao Zhou",
        "title": "Constrained portfolio-consumption strategies with uncertain parameters\n  and borrowing costs",
        "comments": "35 pages, 8 tables, 1 figure",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper studies the properties of the optimal portfolio-consumption\nstrategies in a {finite horizon} robust utility maximization framework with\ndifferent borrowing and lending rates. In particular, we allow for constraints\non both investment and consumption strategies, and model uncertainty on both\ndrift and volatility. With the help of explicit solutions, we quantify the\nimpacts of uncertain market parameters, portfolio-consumption constraints and\nborrowing costs on the optimal strategies and their time monotone properties.\n"
    },
    {
        "paper_id": 1711.03023,
        "authors": "Yuri F. Saporito, Xu Yang, Jorge P. Zubelli",
        "title": "The Calibration of Stochastic-Local Volatility Models - An Inverse\n  Problem Perspective",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We tackle the calibration of the so-called Stochastic-Local Volatility (SLV)\nmodel. This is the class of financial models that combines the local and\nstochastic volatility features and has been subject of the attention by many\nresearchers recently. More precisely, given a local volatility surface and a\nchoice of stochastic volatility parameters, we calibrate the corresponding\nleverage function. Our approach makes use of regularization techniques from the\ninverse-problem theory, respecting the integrity of the data and thus avoiding\ndata interpolation. The result is a stable and robust algorithm which is\nresilient to instabilities in the regions of low probability density of the\nspot price and of the instantaneous variance. We substantiate our claims with\nnumerical experiments using simulated as well as real data.\n"
    },
    {
        "paper_id": 1711.03078,
        "authors": "Blanka Horvath, Antoine Jacquier, Aitor Muguruza, Andreas Sojmark",
        "title": "Functional central limit theorems for rough volatility",
        "comments": "40 pages, 18 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The non-Markovian nature of rough volatility processes makes Monte Carlo\nmethods challenging and it is in fact a major challenge to develop fast and\naccurate simulation algorithms. We provide an efficient one for stochastic\nVolterra processes, based on an extension of Donsker's approximation of\nBrownian motion to the fractional Brownian case with arbitrary Hurst exponent\n$H \\in (0,1)$. Some of the most relevant consequences of this `rough Donsker\n(rDonsker) Theorem' are functional weak convergence results in Skorokhod space\nfor discrete approximations of a large class of rough stochastic volatility\nmodels. This justifies the validity of simple and easy-to-implement Monte-Carlo\nmethods, for which we provide detailed numerical recipes. We test these against\nthe current benchmark Hybrid scheme~\\cite{BLP17} and find remarkable agreement\n(for a large range of values of~$H$). This rDonsker Theorem further provides a\nweak convergence proof for the Hybrid scheme itself, and allows to construct\nbinomial trees for rough volatility models, the first available scheme (in the\nrough volatility context) for early exercise options such as American or\nBermudan options.\n"
    },
    {
        "paper_id": 1711.03188,
        "authors": "Alon Dourban and Liron Yedidsion",
        "title": "Optimal Purchasing Policy For Mean-Reverting Items in a Finite Horizon",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this research we study a finite horizon optimal purchasing problem for\nitems with a mean reverting price process. Under this model a fixed amount of\nidentical items are bought under a given deadline, with the objective of\nminimizing the cost of their purchasing price and associated holding cost. We\nprove that the optimal policy for minimizing the expected cost is in the form\nof a time-variant threshold function that defines the price region in which a\npurchasing decision is optimal. We construct the threshold function with a\nsimple algorithm that is based on a dynamic programming procedure that\ncalculates the cost function. As part of this procedure we also introduce\nexplicit equations for the crossing time probability and the overshoot\nexpectation of the price process with respect to the threshold function. The\ncharacteristics and dynamics of the threshold function are analyzed with\nrespect to time, holding cost, and different parameters of the price process,\nand yields meaningful practical insights, as well as theoretical insights.\n"
    },
    {
        "paper_id": 1711.03291,
        "authors": "Torsten Trimborn, Lorenzo Pareschi, Martin Frank",
        "title": "Portfolio Optimization and Model Predictive Control: A Kinetic Approach",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper, we introduce a large system of interacting financial agents in\nwhich each agent is faced with the decision of how to allocate his capital\nbetween a risky stock or a risk-less bond. The investment decision of\ninvestors, derived through an optimization, drives the stock price. The model\nhas been inspired by the econophysical Levy-Levy-Solomon model (Economics\nLetters, 45). The goal of this work is to gain insights into the stock price\nand wealth distribution. We especially want to discover the causes for the\nappearance of power-laws in financial data. We follow a kinetic approach\nsimilar to (D. Maldarella, L. Pareschi, Physica A, 391) and derive the mean\nfield limit of our microscopic agent dynamics. The novelty in our approach is\nthat the financial agents apply model predictive control (MPC) to approximate\nand solve the optimization of their utility function. Interestingly, the MPC\napproach gives a mathematical connection between the two opponent economic\nconcepts of modeling financial agents to be rational or boundedly rational.\nFurthermore, this is to our knowledge the first kinetic portfolio model which\nconsiders a wealth and stock price distribution simultaneously. Due to our\nkinetic approach, we can study the wealth and price distribution on a\nmesoscopic level. The wealth distribution is characterized by a lognormal law.\nFor the stock price distribution, we can either observe a lognormal behavior in\nthe case of long-term investors or a power-law in the case of high-frequency\ntrader. Furthermore, the stock return data exhibits a fat-tail, which is a well\nknown characteristic of real financial data.\n"
    },
    {
        "paper_id": 1711.03534,
        "authors": "Martin Magris, Jiyeong Kim, Esa Rasanen, Juho Kanniainen",
        "title": "Long-range Auto-correlations in Limit Order Book Markets: Inter- and\n  Cross-event Analysis",
        "comments": "Paper submitted for the IEEE SSCI 2017 conference. November 27 -\n  December 1 2017, Honolulu, USA. This paper is protected by copyright\n  agreement, however its pubblication on ArXiv is permitted based on IEEE\n  policy",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Long-range correlation in financial time series reflects the complex dynamics\nof the stock markets driven by algorithms and human decisions. Our analysis\nexploits ultra-high frequency order book data from NASDAQ Nordic over a period\nof three years to numerically estimate the power-law scaling exponents using\ndetrended fluctuation analysis (DFA). We address inter-event durations (order\nto order, trade to trade, cancel to cancel) as well as cross-event durations\n(time from order submission to its trade or cancel). We find strong evidence of\nlong-range correlation, which is consistent across different stocks and\nvariables. However, given the crossovers in the DFA fluctuation functions, our\nresults indicate that the long-range correlation in inter-event durations\nbecomes stronger over a longer time scale, i.e., when moving from a range of\nhours to days and further to months. We also observe interesting associations\nbetween the scaling exponent and a number of economic variables, in particular,\nin the inter-trade time series.\n"
    },
    {
        "paper_id": 1711.03642,
        "authors": "Bernardo D'Auria and Jos\\'e Antonio Salmer\\'on",
        "title": "Optimal portfolios with anticipating information on the stochastic\n  interest rate",
        "comments": "19 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  By employing the technique of enlargement of filtrations, we demonstrate how\nto incorporate information about the future trend of the stochastic interest\nrate process into a financial model. By modeling the interest rate as an affine\ndiffusion process, we obtain explicit formulas for the additional expected\nlogarithmic utility in solving the optimal portfolio problem. We begin by\nsolving the problem when the additional information directly refers to the\ninterest rate process, and then extend the analysis to the case where the\ninformation relates to the values of an underlying Markov chain. The dynamics\nof this chain may depend on anticipated market information, jump at predefined\nepochs, and modulate the parameters of the stochastic interest rate process.\nThe theoretical study is then complemented by an illustrative numerical\nanalysis.\n"
    },
    {
        "paper_id": 1711.03733,
        "authors": "Xavier Warin",
        "title": "Variance optimal hedging with application to Electricity markets",
        "comments": "17 pages, 2 figures, 11 tables",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In Electricity markets, illiquidity, transaction costs and market price\ncharacteristics prevent managers to replicate exactly contracts. A residual\nrisk is always present and the hedging strategy depends on a risk criterion\nchosen. We present an algorithm to hedge a position for a mean variance\ncriterion taking into account the transaction cost and the small depth of the\nmarket. We show its effectiveness on a typical problem coming from the field of\nelectricity markets.\n"
    },
    {
        "paper_id": 1711.03744,
        "authors": "Cheng-Der Fuh, Chuan-Ju Wang",
        "title": "Efficient Exponential Tilting for Portfolio Credit Risk",
        "comments": "39 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper considers the problem of measuring the credit risk in portfolios\nof loans, bonds, and other instruments subject to possible default under\nmulti-factor models. Due to the amount of the portfolio, the heterogeneous\neffect of obligors, and the phenomena that default events are rare and mutually\ndependent, it is difficult to calculate portfolio credit risk either by means\nof direct analysis or crude Monte Carlo under such models. To capture the\nextreme dependence among obligors, we provide an efficient simulation method\nfor multi-factor models with a normal mixture copula that allows the\nmultivariate defaults to have an asymmetric distribution, while most of the\nliterature focuses on simulating one-dimensional cases. To this end, we first\npropose a general account of an importance sampling algorithm based on an\nunconventional exponential embedding, which is related to the classical\nsufficient statistic. Note that this innovative tilting device is more suitable\nfor the multivariate normal mixture model than traditional one-parameter\ntilting methods and is of independent interest. Next, by utilizing a fast\ncomputational method for how the rare event occurs and the proposed importance\nsampling method, we provide an efficient simulation algorithm to estimate the\nprobability that the portfolio incurs large losses under the normal mixture\ncopula. Here the proposed simulation device is based on importance sampling for\na joint probability other than the conditional probability used in previous\nstudies. Theoretical investigations and simulation studies, which include an\nempirical example, are given to illustrate the method.\n"
    },
    {
        "paper_id": 1711.03875,
        "authors": "Ariel Neufeld, Mario Sikic",
        "title": "Nonconcave Robust Optimization with Discrete Strategies under Knightian\n  Uncertainty",
        "comments": "arXiv admin note: text overlap with arXiv:1610.09230",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We study robust stochastic optimization problems in the quasi-sure setting in\ndiscrete-time. The strategies in the multi-period-case are restricted to those\ntaking values in a discrete set. The optimization problems under consideration\nare not concave. We provide conditions under which a maximizer exists. The\nclass of problems covered by our robust optimization problem includes optimal\nstopping and semi-static trading under Knightian uncertainty.\n"
    },
    {
        "paper_id": 1711.04174,
        "authors": "Ariel Navon, Yosi Keller",
        "title": "Financial Time Series Prediction Using Deep Learning",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this work we present a data-driven end-to-end Deep Learning approach for\ntime series prediction, applied to financial time series. A Deep Learning\nscheme is derived to predict the temporal trends of stocks and ETFs in NYSE or\nNASDAQ. Our approach is based on a neural network (NN) that is applied to raw\nfinancial data inputs, and is trained to predict the temporal trends of stocks\nand ETFs. In order to handle commission-based trading, we derive an investment\nstrategy that utilizes the probabilistic outputs of the NN, and optimizes the\naverage return. The proposed scheme is shown to provide statistically\nsignificant accurate predictions of financial market trends, and the investment\nstrategy is shown to be profitable under this challenging setup. The\nperformance compares favorably with contemporary benchmarks along two-years of\nback-testing.\n"
    },
    {
        "paper_id": 1711.04219,
        "authors": "Yanlin Qu and Randall R. Rojas",
        "title": "Closed-form Solutions of Relativistic Black-Scholes Equations",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Drawing insights from the triumph of relativistic over classical mechanics\nwhen velocities approach the speed of light, we explore a similar improvement\nto the seminal Black-Scholes (Black and Scholes (1973)) option pricing formula\nby considering a relativist version of it, and then finding a respective\nsolution. We show that our solution offers a significant improvement over\ncompeting solutions (e.g., Romero and Zubieta-Martinez (2016)), and obtain a\nnew closed-form option pricing formula, containing the speed limit of\ninformation transfer c as a new parameter. The new formula is rigorously shown\nto converge to the Black-Scholes formula as c goes to infinity. When c is\nfinite, the new formula can flatten the standard volatility smile which is more\nconsistent with empirical observations. In addition, an alternative family of\ndistributions for stock prices arises from our new formula, which offer a\nbetter fit, are shown to converge to lognormal, and help to better explain the\nvolatility skew.\n"
    },
    {
        "paper_id": 1711.04717,
        "authors": "J. P. Bouchaud, S. Ciliberti, Y. Lemp\\'eri\\`ere, A. Majewski, P.\n  Seager, K. Sin Ronia",
        "title": "Black was right: Price is within a factor 2 of Value",
        "comments": "9 pages, 5 figures, some refs. added",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We provide further evidence that markets trend on the medium term (months)\nand mean-revert on the long term (several years). Our results bolster Black's\nintuition that prices tend to be off roughly by a factor of 2, and take years\nto equilibrate. The story behind these results fits well with the existence of\ntwo types of behaviour in financial markets: \"chartists\", who act as trend\nfollowers, and \"fundamentalists\", who set in when the price is clearly out of\nline. Mean-reversion is a self-correcting mechanism, tempering (albeit only\nweakly) the exuberance of financial markets.\n"
    },
    {
        "paper_id": 1711.05289,
        "authors": "T. R. Hurd",
        "title": "Bank Panics and Fire Sales, Insolvency and Illiquidity",
        "comments": "29 pages, 1 figure",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Banking system crises are complex events that in a short span of time can\ninflict extensive damage to banks themselves and to the external economy. The\ncrisis literature has so far identified a number of distinct effects or\nchannels that can propagate distress contagiously both directly within the\nbanking network itself and indirectly, between the network and the external\neconomy. These contagious effects, and the potential events that trigger these\neffects, can explain most aspects of past crises, and are thought to be likely\nto dominate future financial crises. Since the current international financial\nregulatory regime based on the Basel III Accord does a good job of ensuring\nthat banks are resilient to such contagion effects taken one at a time,\nsystemic risk theorists increasingly understand that future crises are likely\nto be dominated by the spillovers between distinct contagion channels. The\npresent paper aims to provide a model for systemic risk that is comprehensive\nenough to include the important contagion channels identified in the\nliterature. In such a model one can hope to understand the dangerous spillover\neffects that are expected to dominate future crises. To rein in the number and\ncomplexity of the modelling assumptions, two requirements are imposed, neither\nof which is yet well-known or established in the main stream of systemic risk\nresearch. The first, called stock-flow consistency, demands that the financial\nsystem follows a rigorous set of rules based on accounting principles. The\nsecond requirement, called Asset-Liability symmetry, implies that every\nproposed contagion channel has a dual channel obtained by interchanging assets\nand liabilities, and that these dual channel pairs have a symmetric\nmathematical representation.\n"
    },
    {
        "paper_id": 1711.05598,
        "authors": "Bowen Cai",
        "title": "Customer Selection Model with Grouping and Hierarchical Ranking Analysis",
        "comments": "24 pages, Preliminary Report",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The purpose of this study was to build a customer selection model based on 20\ndimensions, including customer codes, total contribution, assets, deposit,\nprofit, profit rate, trading volume, trading amount, turnover rate, order\namount, withdraw amount, withdraw rate, process fee, process fee submitted,\nprocess fee retained, net process fee retained, interest revenue, interest\nreturn, exchange house return I and exchange house return II to group and rank\ncustomers. The traditional way to group customers in securities or futures\ncompanies is simply based on their assets. However, grouping customers with\nrespect to only one dimension cannot give us a full picture about customers'\nattributions. It is hard to group customers' with similar attributions or\nvalues into one group if we just consider assets as the only grouping\ncriterion. Nowadays, securities or futures companies usually group customers\nbased on managers' experience with lack of quantitative analysis, which is not\neffective. Therefore, we use kmeans unsupervised learning methods to group\ncustomers with respect to significant dimensions so as to cluster customers\nwith similar attributions together. Grouping is our first step. It is the\nhorizontal analysis in customer study. The next step is customer ranking. It is\nthe longitudinal analysis. It ranks customers by assigning each customer with a\ncertain score given by our weighted customer value calculation formula.\nTherefore, by grouping and ranking customers, we can differentiate our\ncustomers and rank them based on values instead of blindly reaching everyone.\n"
    },
    {
        "paper_id": 1711.05681,
        "authors": "Stanislav Anatolyev and Jozef Barunik",
        "title": "Forecasting dynamic return distributions based on ordered binary choice",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We present a simple approach to forecasting conditional probability\ndistributions of asset returns. We work with a parsimonious specification of\nordered binary choice regression that imposes a connection on sign\npredictability across different quantiles. The model forecasts the future\nconditional probability distributions of returns quite precisely when using a\npast indicator and past volatility proxy as predictors. Direct benefits of the\nmodel are revealed in an empirical application to the 29 most liquid U.S.\nstocks. The forecast probability distribution is translated to significant\neconomic gains in a simple trading strategy. Our approach can also be useful in\nmany other applications where conditional distribution forecasts are desired.\n"
    },
    {
        "paper_id": 1711.05784,
        "authors": "Sofia Torreggiani, Giuseppe Mangioni, Michael J. Puma, Giorgio Fagiolo",
        "title": "Identifying the community structure of the international food-trade\n  multi network",
        "comments": "47 pages, 19 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Achieving international food security requires improved understanding of how\ninternational trade networks connect countries around the world through the\nimport-export flows of food commodities. The properties of food trade networks\nare still poorly documented, especially from a multi-network perspective. In\nparticular, nothing is known about the community structure of food networks,\nwhich is key to understanding how major disruptions or 'shocks' would impact\nthe global food system. Here we find that the individual layers of this network\nhave densely connected trading groups, a consistent characteristic over the\nperiod 2001 to 2011. We also fit econometric models to identify social,\neconomic and geographic factors explaining the probability that any two\ncountries are co-present in the same community. Our estimates indicate that the\nprobability of country pairs belonging to the same food trade community depends\nmore on geopolitical and economic factors -- such as geographical proximity and\ntrade agreements co-membership -- than on country economic size and/or income.\nThis is in sharp contrast with what we know about bilateral-trade determinants\nand suggests that food country communities behave in ways that can be very\ndifferent from their non-food counterparts.\n"
    },
    {
        "paper_id": 1711.06164,
        "authors": "Paulo Murilo Castro de Oliveira",
        "title": "Rich or poor: Who should pay higher tax rates?",
        "comments": "5 pages, 3 figures",
        "journal-ref": "Europhysics Letters, 119, 40007 (2017)",
        "doi": "10.1209/0295-5075/119/40007",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  A dynamic agent model is introduced with an annual random wealth\nmultiplicative process followed by taxes paid according to a linear\nwealth-dependent tax rate. If poor agents pay higher tax rates than rich\nagents, eventually all wealth becomes concentrated in the hands of a single\nagent. By contrast, if poor agents are subject to lower tax rates, the economic\ncollective process continues forever.\n"
    },
    {
        "paper_id": 1711.06185,
        "authors": "A. Q. Barbi and G. A. Prataviera",
        "title": "Nonlinear dependencies on Brazilian equity network from mutual\n  information minimum spanning trees",
        "comments": "21 pages, 5 figures, 2 tables. Revised manuscript. Published in\n  Physica A: Statistical Mechanics and its Applications",
        "journal-ref": "Physica A: Statistical Mechanics and its Applications, Volume 523,\n  1 June 2019, Pages 876-885",
        "doi": "10.1016/j.physa.2019.04.147",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Mutual information minimum spanning trees are used to explore nonlinear\ndependencies on Brazilian equity network in the periods from June/01/2015 to\nJanuary/26/2016, in which Brazil was under the government of President Dilma\nRousseff, and from January/27/2016 to September/08/2016 which includes the\ngovernment transition from President Dilma Rousseff to President Michel Temer.\nMinimum spanning trees from mutual information and linear correlation between\nstocks returns were obtained and compared. Mutual information minimum spanning\ntrees present higher degree of robustness and evidence of power law tail in the\nweighted degree distribution, indicating more risk in terms of volatility\ntransmission than it is expected by the analysis based on linear correlation.\nIn particular, a remarkable increase of stock returns nonlinear dependencies\nindicates that the period including the government transition is more risky in\nterms of volatility transmission network structure. Also, we found evidence of\nnetwork structure and stock performance relationship. Besides, those results\nemphasize the usefulness of mutual information network analysis for\nidentification of Financial Markets features due to nonlinear dependencies.\n"
    },
    {
        "paper_id": 1711.06403,
        "authors": "\\c{C}a\\u{g}{\\i}n Ararat, \\\"Ozlem \\c{C}avu\\c{s}, Ali \\.Irfan\n  Mahmuto\\u{g}ullar{\\i}",
        "title": "Multi-objective risk-averse two-stage stochastic programming problems",
        "comments": "43 pages, 6 tables, 8 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We consider a multi-objective risk-averse two-stage stochastic programming\nproblem with a multivariate convex risk measure. We suggest a convex vector\noptimization formulation with set-valued constraints and propose an extended\nversion of Benson's algorithm to solve this problem. Using Lagrangian duality,\nwe develop scenario-wise decomposition methods to solve the two scalarization\nproblems appearing in Benson's algorithm. Then, we propose a procedure to\nrecover the primal solutions of these scalarization problems from the solutions\nof their Lagrangian dual problems. Finally, we test our algorithms on a\nmulti-asset portfolio optimization problem under transaction costs.\n"
    },
    {
        "paper_id": 1711.06466,
        "authors": "David Hobson and Dominykas Norgilas",
        "title": "Robust bounds for the American Put",
        "comments": "31 pages, 14 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We consider the problem of finding a model-free upper bound on the price of\nan American put given the prices of a family of European puts on the same\nunderlying asset. Specifically we assume that the American put must be\nexercised at either $T_1$ or $T_2$ and that we know the prices of all vanilla\nEuropean puts with these maturities. In this setting we find a model which is\nconsistent with European put prices and an associated exercise time, for which\nthe price of the American put is maximal. Moreover we derive a cheapest\nsuperhedge. The model associated with the highest price of the American put is\nconstructed from the left-curtain martingale transport of Beiglb\\\"{o}ck and\nJuillet.\n"
    },
    {
        "paper_id": 1711.06565,
        "authors": "Jun-Ya Gotoh, Michael Jong Kim, Andrew E.B. Lim",
        "title": "Calibration of Distributionally Robust Empirical Optimization Models",
        "comments": "51 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We study the out-of-sample properties of robust empirical optimization\nproblems with smooth $\\phi$-divergence penalties and smooth concave objective\nfunctions, and develop a theory for data-driven calibration of the non-negative\n\"robustness parameter\" $\\delta$ that controls the size of the deviations from\nthe nominal model. Building on the intuition that robust optimization reduces\nthe sensitivity of the expected reward to errors in the model by controlling\nthe spread of the reward distribution, we show that the first-order benefit of\n``little bit of robustness\" (i.e., $\\delta$ small, positive) is a significant\nreduction in the variance of the out-of-sample reward while the corresponding\nimpact on the mean is almost an order of magnitude smaller. One implication is\nthat substantial variance (sensitivity) reduction is possible at little cost if\nthe robustness parameter is properly calibrated. To this end, we introduce the\nnotion of a robust mean-variance frontier to select the robustness parameter\nand show that it can be approximated using resampling methods like the\nbootstrap. Our examples show that robust solutions resulting from \"open loop\"\ncalibration methods (e.g., selecting a $90\\%$ confidence level regardless of\nthe data and objective function) can be very conservative out-of-sample, while\nthose corresponding to the robustness parameter that optimizes an estimate of\nthe out-of-sample expected reward (e.g., via the bootstrap) with no regard for\nthe variance are often insufficiently robust.\n"
    },
    {
        "paper_id": 1711.06679,
        "authors": "Martin Herdegen, Sebastian Herrmann",
        "title": "Strict Local Martingales and Optimal Investment in a Black-Scholes Model\n  with a Bubble",
        "comments": "39 pages; forthcoming in 'Mathematical Finance'",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  There are two major streams of literature on the modeling of financial\nbubbles: the strict local martingale framework and the Johansen-Ledoit-Sornette\n(JLS) financial bubble model. Based on a class of models that embeds the JLS\nmodel and can exhibit strict local martingale behavior, we clarify the\nconnection between these previously disconnected approaches. While the original\nJLS model is never a strict local martingale, there are relaxations which can\nbe strict local martingales and which preserve the key assumption of a\nlog-periodic power law for the hazard rate of the time of the crash. We then\nstudy the optimal investment problem for an investor with constant relative\nrisk aversion in this model. We show that for positive instantaneous expected\nreturns, investors with relative risk aversion above one always ride the\nbubble.\n"
    },
    {
        "paper_id": 1711.07133,
        "authors": "A. Itkin, V. Shcherbakov, A. Veygman",
        "title": "Influence of jump-at-default in IR and FX on Quanto CDS prices",
        "comments": "33 pages, 10 figures, 2 tables",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We propose a new model for pricing Quanto CDS and risky bonds. The model\noperates with four stochastic factors, namely: hazard rate, foreign exchange\nrate, domestic interest rate, and foreign interest rate, and also allows for\njumps-at-default in the FX and foreign interest rates. Corresponding systems of\nPDEs are derived similar to how this is done in Bielecki at al., 2005. A\nlocalized version of the RBF partition of unity method is used to solve these\n4D PDEs. The results of our numerical experiments presented in the paper\nqualitatively explain the discrepancies observed in the marked values of CDS\nspreads traded in domestic and foreign economies.\n"
    },
    {
        "paper_id": 1711.07279,
        "authors": "Paul McCloud",
        "title": "Information and Arbitrage: Applications of Quantum Groups in\n  Mathematical Finance",
        "comments": "37 pages, 28 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  The relationship between expectation and price is commonly established with\ntwo principles: no-arbitrage, which asserts that both maps are positive; and\nequivalence, which asserts that the maps share the same null events.\nConstructed from the Arrow-Debreu securities, classical and quantum models of\neconomics are then distinguished by their respective use of classical and\nquantum logic, following the program of von Neumann.\n  In this essay, the operations and axioms of quantum groups are discovered in\nthe minimal preconditions of stochastic and functional calculus, making this\nthe natural domain for the axiomatic development of mathematical finance.\nQuantum economics emerges from the twin pillars of the Gelfand-Naimark-Segal\nconstruction, implementing the principle of no-arbitrage, and the Radon-Nikodym\ntheorem, implementing the principle of equivalence.\n  Exploiting quantum group duality, a holographic principle that exchanges the\nroles of state and observable creates two distinct economic models from the\nsame set of elementary valuations. Advocating on the grounds that this contains\nand extends classical economics, noncommutativity is presented as a modelling\nresource, with novel applications in the pricing of options and other\nderivative securities.\n"
    },
    {
        "paper_id": 1711.07335,
        "authors": "Takashi Kato",
        "title": "Asymptotic Analysis for Spectral Risk Measures Parameterized by\n  Confidence Level",
        "comments": "30 pages, 11 figures",
        "journal-ref": "Journal of Mathematical Finance, Vol.8, No.1, pp.197-226 (2018)",
        "doi": "10.4236/jmf.2018.81015",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We study the asymptotic behavior of the difference $\\Delta \\rho ^{X,\nY}_\\alpha := \\rho _\\alpha (X + Y) - \\rho _\\alpha (X)$ as $\\alpha \\rightarrow\n1$, where $\\rho_\\alpha $ is a risk measure equipped with a confidence level\nparameter $0 < \\alpha < 1$, and where $X$ and $Y$ are non-negative random\nvariables whose tail probability functions are regularly varying. The case\nwhere $\\rho _\\alpha $ is the value-at-risk (VaR) at $\\alpha $, is treated in\nKato (2017). This paper investigates the case where $\\rho _\\alpha $ is a\nspectral risk measure that converges to the worst-case risk measure as $\\alpha\n\\rightarrow 1$. We give the asymptotic behavior of the difference between the\nmarginal risk contribution and the Euler contribution of $Y$ to the portfolio\n$X + Y$. Similarly to Kato (2017), our results depend primarily on the relative\nmagnitudes of the thicknesses of the tails of $X$ and $Y$. We also conducted a\nnumerical experiment, finding that when the tail of $X$ is sufficiently thicker\nthan that of $Y$, $\\Delta \\rho ^{X, Y}_\\alpha $ does not increase monotonically\nwith $\\alpha$ and takes a maximum at a confidence level strictly less than $1$.\n"
    },
    {
        "paper_id": 1711.0763,
        "authors": "Shanshan Wang, Sebastian Neus\\\"u{\\ss} and Thomas Guhr",
        "title": "Statistical properties of market collective responses",
        "comments": null,
        "journal-ref": "Eur. Phys. J. B (2018) 91: 191",
        "doi": "10.1140/epjb/e2018-80665-0",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We empirically analyze the price and liquidity responses to trade signs,\ntraded volumes and signed traded volumes. Utilizing the singular value\ndecomposition, we explore the interconnections of price responses and of\nliquidity responses across the whole market. The statistical characteristics of\ntheir singular vectors are well described by the $t$ location-scale\ndistribution. Furthermore, we discuss the relation between prices and liquidity\nwith respect to their overlapping factors. The factors of price and liquidity\nchanges are non-random when these factors are related to the traded volumes.\nThis means that the traded volumes play a critical role in the price change\ninduced by the liquidity change. In contrast, the two kinds of factors are\nweakly overlapping when they are related to the trade signs and signed traded\nvolumes. Hence, an imbalance of liquidity is related to the price change.\n"
    },
    {
        "paper_id": 1711.07677,
        "authors": "Elisa Letizia, Fabrizio Lillo",
        "title": "Corporate payments networks and credit risk rating",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Aggregate and systemic risk in complex systems are emergent phenomena\ndepending on two properties: the idiosyncratic risks of the elements and the\ntopology of the network of interactions among them. While a significant\nattention has been given to aggregate risk assessment and risk propagation once\nthe above two properties are given, less is known about how the risk is\ndistributed in the network and its relations with the topology. We study this\nproblem by investigating a large proprietary dataset of payments among 2.4M\nItalian firms, whose credit risk rating is known. We document significant\ncorrelations between local topological properties of a node (firm) and its\nrisk. Moreover we show the existence of an homophily of risk, i.e. the tendency\nof firms with similar risk profile to be statistically more connected among\nthemselves. This effect is observed when considering both pairs of firms and\ncommunities or hierarchies identified in the network. We leverage this\nknowledge to show the predictability of the missing rating of a firm using only\nthe network properties of the associated node.\n"
    },
    {
        "paper_id": 1711.07731,
        "authors": "Iacopo Savelli, Bertrand Corn\\'elusse, Antonio Giannitrapani, Simone\n  Paoletti, Antonio Vicino",
        "title": "A New Approach to Electricity Market Clearing With Uniform Purchase\n  Price and Curtailable Block Orders",
        "comments": "15 pages, 7 figures",
        "journal-ref": null,
        "doi": "10.1016/j.apenergy.2018.06.003",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The European market clearing problem is characterized by a set of\nheterogeneous orders and rules that force the implementation of heuristic and\niterative solving methods. In particular, curtailable block orders and the\nuniform purchase price (UPP) pose serious difficulties. A block is an order\nthat spans over multiple hours, and can be either fully accepted or fully\nrejected. The UPP prescribes that all consumers pay a common price, i.e., the\nUPP, in all the zones, while producers receive zonal prices, which can differ\nfrom one zone to another.\n  The market clearing problem in the presence of both the UPP and block orders\nis a major open issue in the European context. The UPP scheme leads to a\nnon-linear optimization problem involving both primal and dual variables,\nwhereas block orders introduce multi-temporal constraints and binary variables\ninto the problem. As a consequence, the market clearing problem in the presence\nof both blocks and the UPP can be regarded as a non-linear integer programming\nproblem involving both primal and dual variables with complementary and\nmulti-temporal constraints.\n  The aim of this paper is to present a non-iterative and heuristic-free\napproach for solving the market clearing problem in the presence of both\ncurtailable block orders and the UPP. The solution is exact, with no\napproximation up to the level of resolution of current market data. By\nresorting to an equivalent UPP formulation, the proposed approach results in a\nmixed-integer linear program, which is built starting from a non-linear integer\nbilevel programming problem. Numerical results using real market data are\nreported to show the effectiveness of the proposed approach. The model has been\nimplemented in Python, and the code is freely available on a public repository.\n"
    },
    {
        "paper_id": 1711.07753,
        "authors": "Maissa Tamraz and Yaming Yang",
        "title": "Price Optimisation for New Business",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This contribution is concerned with price optimisation of the new business\nfor a non-life product. Due to high competition in the insurance market,\nnon-life insurers are interested in increasing their conversion rates on new\nbusiness based on some profit level. In this respect, we consider the\ncompetition in the market to model the probability of accepting an offer for a\nspecific customer. We study two optimisation problems relevant for the insurer\nand present some algorithmic solutions for both continuous and discrete case.\nFinally, we provide some applications to a motor insurance dataset.\n"
    },
    {
        "paper_id": 1711.08043,
        "authors": "Damir Filipovi\\'c and Martin Larsson",
        "title": "Polynomial Jump-Diffusion Models",
        "comments": "Forthcoming in Stochastic Systems",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We develop a comprehensive mathematical framework for polynomial\njump-diffusions in a semimartingale context, which nest affine jump-diffusions\nand have broad applications in finance. We show that the polynomial property is\npreserved under polynomial transformations and L\\'evy time change. We present a\ngeneric method for option pricing based on moment expansions. As an\napplication, we introduce a large class of novel financial asset pricing models\nwith excess log returns that are conditional L\\'evy based on polynomial\njump-diffusions.\n"
    },
    {
        "paper_id": 1711.08245,
        "authors": "Penny Mealy, J. Doyne Farmer and Alexander Teytelboym",
        "title": "Interpreting Economic Complexity",
        "comments": "Revision of previous version",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Two network measures known as the Economic Complexity Index (ECI) and Product\nComplexity Index (PCI) have provided important insights into patterns of\neconomic development. We show that the ECI and PCI are equivalent to a spectral\nclustering algorithm that partitions a similarity graph into two parts. The\nmeasures are also related to various dimensionality reduction methods and can\nbe interpreted as vectors that determine distances between nodes based on their\nsimilarity. Our results shed a new light on the ECI's empirical success in\nexplaining cross-country differences in GDP/capita and economic growth, which\nis often linked to the diversity of country export baskets. In fact, countries\nwith high (low) ECI tend to specialize in high (low) PCI products. We also find\nthat the ECI and PCI uncover economically informative specialization patterns\nacross US states and UK regions.\n"
    },
    {
        "paper_id": 1711.08282,
        "authors": "F.M. Stefan and A.P.F. Atman",
        "title": "Asymmetric return rates and wealth distribution influenced by the\n  introduction of technical analysis into a behavioral agent based model",
        "comments": "18 pages and 46 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Behavioral Finance has become a challenge to the scientific community. Based\non the assumption that behavioral aspects of investors may explain some\nfeatures of the Stock Market, we propose an agent based model to study\nquantitatively this relationship. In order to approximate the simulated market\nto the complexity of real markets, we consider that the investors are connected\namong them through a small world network; each one has its own psychological\nprofile (Imitation, Anti-Imitation, Random); two different strategies for\ndecision making: one of them is based on the trust neighborhood of the investor\nand the other one considers a technical analysis, the momentum of the market\nindex technique. We analyze the market index fluctuations, the wealth\ndistribution of the investors according to their psychological profiles and the\nrate of return distribution. Moreover, we analyze the influence of changing the\npsychological profile of the hub of the network and report interesting results\nwhich show how and when anti-imitation becomes the most profitable strategy for\ninvestment. Besides this, an intriguing asymmetry of the return rate\ndistribution is explained considering the behavioral aspect of the investors.\nThis asymmetry is quite robust being observed even when a completely different\nalgorithm to calculate the decision making of the investors was applied to it,\na remarkable result which, up to our knowledge, has never been reported before.\n"
    },
    {
        "paper_id": 1711.08356,
        "authors": "Foad Shokrollahi",
        "title": "Valuation of equity warrants for uncertain financial market",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper, within the framework of uncertainty theory, the valuation of\nequity warrants is investigated. Different from the methods of probability\ntheory, the equity warrants pricing problem is solved by using the method of\nuncertain calculus. Based on the assumption that the firm price follows an\nuncertain differential equation, the equity warrants pricing formula is\nobtained for uncertain stock model.\n"
    },
    {
        "paper_id": 1711.08633,
        "authors": "Henri G\\'erard (CERMICS), Michel de Lara (CERMICS), Jean-Philippe\n  Chancelier (CERMICS, MATHRISK)",
        "title": "Equivalence Between Time Consistency and Nested Formula",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  You are a financial analyst. At the beginning of every week, you are able to\nrank every pair of stochastic processes starting from that week up to the\nhorizon. Suppose that two processes are equal at the beginning of the week.\nYour ranking procedure is time consistent if the ranking does not change\nbetween this week and the next one. In this paper, we propose a minimalist\ndefinition of Time Consistency (TC) between two (assessment) mappings. With\nvery few assumptions, we are able to prove an equivalence between Time\nConsistency and a Nested Formula (NF) between the two mappings. Thus, in a\nsense, two assessments are consistent if and only if one is factored into the\nother. We review the literature and observe that the various definitions of TC\n(or of NF) are special cases of ours, as they always include additional\nassumptions. By stripping off these additional assumptions, we present an\noverview of the literature where the contribution of each author is\nenlightened.\n"
    },
    {
        "paper_id": 1711.08799,
        "authors": "Kamilla Sabitova",
        "title": "Impact of Cross-Listing Chinese Stock Returns. A and N Shares Rate of\n  Return Comparison",
        "comments": null,
        "journal-ref": null,
        "doi": "10.13140/RG.2.2.12115.68642",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The paper examines the Chinese market reaction to the ADR issue by comparing\nreturns and their stochastic variances of the Chinese firms cross-listed in the\nU.S. stock market. First, It was implemented capital asset pricing model (CAPM)\nto determine expected returns A and N shares. The CAPM provided with a\nmethodology to quantify risk and translate that risk into estimates of expected\nreturn on equity. Overall findings document that N shares of Chinese entities\nlisted on U.S. market were greatly affected by economic turmoil during the\nperiod of World Financial Crises 2007-2008 than the A shares listed on the\nlocal market. After in order to test the hypothesis of beneficial\ncross-listing, it was implemented an event study method and the returns was\nmodeled following GARCH process, which assumes homoscedasticity in residual\nreturns. The results indicate a significant negative abnormal market return on\nan ADR listing date. The return volatilities after the listing date are\ncompared to those before the listing. Four out of ten companies experienced\nincreased volatility of local return after the cross-listing. Keywords:\ncross-listing, ADR, rate of return, volatility, CAPM, GARCH model, N shares, A\nshares\n"
    },
    {
        "paper_id": 1711.08883,
        "authors": "Masahiko Egami, Rusudan Kevkhishvili",
        "title": "A Direct Solution Method for Pricing Options in Regime-switching Models",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Pricing financial or real options with arbitrary payoffs in regime-switching\nmodels is an important problem in finance. Mathematically, it is to solve,\nunder certain standard assumptions, a general form of optimal stopping problems\nin regime-switching models. In this article, we reduce an optimal stopping\nproblem with an arbitrary value function in a two-regime environment to a pair\nof optimal stopping problems without regime switching. We then propose a method\nfor finding optimal stopping rules using the techniques available for\nnon-switching problems. In contrast to other methods, our systematic solution\nprocedure is more direct since we first obtain the explicit form of the value\nfunctions. In the end, we discuss an option pricing problem which may not be\ndealt with by the conventional methods, demonstrating the simplicity of our\napproach.\n"
    },
    {
        "paper_id": 1711.09193,
        "authors": "Damien Ackerer and Damir Filipovic",
        "title": "Option Pricing with Orthogonal Polynomial Expansions",
        "comments": "forthcoming in Mathematical Finance, 38 pages, 3 tables, 7 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We derive analytic series representations for European option prices in\npolynomial stochastic volatility models. This includes the Jacobi, Heston,\nStein-Stein, and Hull-White models, for which we provide numerical case\nstudies. We find that our polynomial option price series expansion performs as\nefficiently and accurately as the Fourier transform based method in the nested\naffine cases. We also derive and numerically validate series representations\nfor option Greeks. We depict an extension of our approach to exotic options\nwhose payoffs depend on a finite number of prices.\n"
    },
    {
        "paper_id": 1711.09445,
        "authors": "Stoyan V. Stoyanov, Yong Shin Kim, Svetlozar T. Rachev, Frank J.\n  Fabozzi",
        "title": "Option pricing for Informed Traders",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper we extend the theory of option pricing to take into account and\nexplain the empirical evidence for asset prices such as non-Gaussian returns,\nlong-range dependence, volatility clustering, non-Gaussian copula dependence,\nas well as theoretical issues such as asymmetric information and the presence\nof limited arbitrage opportunities\n"
    },
    {
        "paper_id": 1711.09852,
        "authors": "Slobodan Milovanovi\\'c and Victor Shcherbakov",
        "title": "Pricing Derivatives under Multiple Stochastic Factors by Localized\n  Radial Basis Function Methods",
        "comments": "The authors contributed equally to this work",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We propose two localized Radial Basis Function (RBF) methods, the Radial\nBasis Function Partition of Unity method (RBF-PUM) and the Radial Basis\nFunction generated Finite Differences method (RBF-FD), for solving financial\nderivative pricing problems arising from market models with multiple stochastic\nfactors. We demonstrate the useful features of the proposed methods, such as\nhigh accuracy, sparsity of the differentiation matrices, mesh-free nature and\nmulti-dimensional extendability, and show how to apply these methods for\nsolving time-dependent higher-dimensional PDEs in finance. We test these\nmethods on several problems that incorporate stochastic asset, volatility, and\ninterest rate dynamics by conducting numerical experiments. The results\nillustrate the capability of both methods to solve the problems to a sufficient\naccuracy within reasonable time. Both methods exhibit similar orders of\nconvergence, which can be further improved by a more elaborate choice of the\nmethod parameters. Finally, we discuss the parallelization potentials of the\nproposed methods and report the speedup on the example of RBF-FD.\n"
    },
    {
        "paper_id": 1711.10013,
        "authors": "Olivares Pablo, Villamor Enrique",
        "title": "Valuing Exchange Options Under an Ornstein-Uhlenbeck Covariance Model",
        "comments": "27 pages, 7 figures, 2 tables",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper we study the pricing of exchange options under a dynamic\ndescribed by stochastic correlation with random jumps. In particular, we\nconsider a Ornstein-Uhlenbeck covariance model with Levy Background Noise\nProcess driven by Inverse Gaussian subordinators. We use expansion in terms of\nTaylor polynomials and cubic splines to approximately compute the price of the\nderivative contract. Our findings show that this approach provides an efficient\nway to compute the price when compared with a Monte Carlo method while\nmaintaining an equivalent degree of accuracy with the latter.\n"
    },
    {
        "paper_id": 1711.10096,
        "authors": "Yoshiaki Nakada",
        "title": "The effects of energy and commodity prices on commodity output in a\n  three-factor, two-good general equilibrium trade model",
        "comments": "23 pages, 2 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We analyze the effects of energy and commodity prices on commodity output\nusing a three-factor, two-good general equilibrium trade model with three\nfactors: capital, labor, and imported energy. We derive a sufficient condition\nfor each sign pattern of each relationship to hold, which no other studies have\nderived. We assume factor-intensity ranking is constant and use the EWS\n(economy-wide substitution)-ratio vector and the Hadamard product in the\nanalysis. The results reveal that the position of the EWS-ratio vector\ndetermines the relationships. Specifically, the strengthening (resp. reduction)\nof import restrictions can increase (resp. decrease) the commodity output of\nexportables, if capital and labor, domestic factors, are economy-wide\ncomplements. This seems paradoxical.\n"
    },
    {
        "paper_id": 1711.10138,
        "authors": "Yoshiaki Nakada",
        "title": "Comment on Suzuki's rebuttal of Batra and Casas",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Batra and Casas (1976) claimed that 'a strong Rybczynski result' arises in\nthe three-factor two-good general equilibrium trade model. In subsequent\ncomments, Suzuki (1983) contended that this could not be the case. Among his\ncomments, Suzuki found that the set of three equations holds for the\nAllen-partial elasticity of substitution under the assumption of perfect\ncomplementarity, and he applied these to his analysis. In the following, I\ndemonstrate that these are impossible, hence his dissenting proof is not\nplausible.\n"
    },
    {
        "paper_id": 1711.1021,
        "authors": "Nicole B\\\"auerle, Alexander Glauner",
        "title": "Optimal Risk Allocation in Reinsurance Networks",
        "comments": null,
        "journal-ref": "Insurance: Mathematics and Economics 82, 37-47, 2018",
        "doi": "10.1016/j.insmatheco.2018.06.009",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper we consider reinsurance or risk sharing from a macroeconomic\npoint of view. Our aim is to find socially optimal reinsurance treaties. In our\nsetting we assume that there are $n$ insurance companies each bearing a certain\nrisk and one representative reinsurer. The optimization problem is to minimize\nthe sum of all capital requirements of the insurers where we assume that all\ninsurance companies use a form of Range-Value-at-Risk. We show that in case all\ninsurers use Value-at-Risk and the reinsurer's premium principle satisfies\nmonotonicity, then layer reinsurance treaties are socially optimal. For this\nresult we do not need any dependence structure between the risks. In the\ngeneral setting with Range-Value-at-Risk we obtain again the optimality of\nlayer reinsurance treaties under further assumptions, in particular under the\nassumption that the individual risks are positively dependent through the\nstochastic ordering. At the end, we discuss the difference between socially\noptimal reinsurance treaties and individually optimal ones by looking at a\nnumber of special cases.\n"
    },
    {
        "paper_id": 1711.10303,
        "authors": "Emmanuel Lepinette and Ilya Molchanov",
        "title": "Conditional cores and conditional convex hulls of random sets",
        "comments": "31 page, this work is a substantial extension of a part from\n  arXiv:1605.07884",
        "journal-ref": "Journal of Mathematical Analysis and Its Applications, 2019, 478,\n  368-392",
        "doi": "10.1016/j.jmaa.2019.05.010",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We define two non-linear operations with random (not necessarily closed) sets\nin Banach space: the conditional core and the conditional convex hull. While\nthe first is sublinear, the second one is superlinear (in the reverse set\ninclusion ordering). Furthermore, we introduce the generalised conditional\nexpectation of random closed sets and show that it is sandwiched between the\nconditional core and the conditional convex hull. The results rely on\nmeasurability properties of not necessarily closed random sets considered from\nthe point of view of the families of their selections. Furthermore, we develop\nanalytical tools suitable to handle random convex (not necessarily compact)\nsets in Banach spaces; these tools are based on considering support functions\nas functions of random arguments. The paper is motivated by applications to\nassessing multivariate risks in mathematical finance.\n"
    },
    {
        "paper_id": 1711.10552,
        "authors": "George P Papaioannou, Christos Dikaiakos, Anargyros Dramountanis,\n  Dionysios S Georgiadis and Panagiotis G Papaioannou",
        "title": "Using nonlinear stochastic and deterministic (chaotic tools) to test the\n  EMH of two Electricity Markets the case of Italy and Greece",
        "comments": "arXiv admin note: text overlap with arXiv:cond-mat/0103621 by other\n  authors",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Utilization of non-linear tools to characterize the state of development of\nthe electricity markets in Italy and Greece. This is equivalent to testing the\nEfficient Market Hypothesis on these markets. The tools include a variety of\ncomplexity measures like Maximal Lyapunov and Hurst exponents and HHI index for\nmarket concentration and Entropy, a measure of uncertainty and complexity in a\ndynamical system, applied on the electricity wholesale marginal prices PUN and\nSMP of Italy and Greece.Our aim is to measure the complexity and dimensionality\nof the manifold on which the underlying stochastic dynamical system, govenring\nthe prices, evolve. We also use the conditional volatility of prices, which is\na measure of the market risk, and its connection with stability, and Hurst\nexponent to investigate the properties of the fluctuations of the prices which\nare the footprints of the idiosyncrracies of each market.\n"
    },
    {
        "paper_id": 1711.1064,
        "authors": "Zura Kakushadze and Willie Yu",
        "title": "Notes on Fano Ratio and Portfolio Optimization",
        "comments": "29 pages; a few trivial typos corrected, no other changes",
        "journal-ref": "Journal of Risk & Control 5(1) (2018) 1-33",
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We discuss - in what is intended to be a pedagogical fashion - generalized\n\"mean-to-risk\" ratios for portfolio optimization. The Sharpe ratio is only one\nexample of such generalized \"mean-to-risk\" ratios. Another example is what we\nterm the Fano ratio (which, unlike the Sharpe ratio, is independent of the time\nhorizon). Thus, for long-only portfolios optimizing the Fano ratio generally\nresults in a more diversified and less skewed portfolio (compared with\noptimizing the Sharpe ratio). We give an explicit algorithm for such\noptimization. We also discuss (Fano-ratio-inspired) long-short strategies that\noutperform those based on optimizing the Sharpe ratio in our backtests.\n"
    },
    {
        "paper_id": 1711.11003,
        "authors": "Zhiyuan Liu, M. Dashti Moghaddam, R. A. Serota",
        "title": "Distributions of Historic Market Data - Stock Returns",
        "comments": "13 pages, 10 figures, 1 table",
        "journal-ref": "European Physical Journal B (2019) 92: 60",
        "doi": "10.1140/epjb/e2019-90218-8",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We show that the moments of the distribution of historic stock returns are in\nexcellent agreement with the Heston model and not with the multiplicative\nmodel, which predicts power-law tails of volatility and stock returns. We also\nshow that the mean realized variance of returns is a linear function of the\nnumber of days over which the returns are calculated. The slope is determined\nby the mean value of the variance (squared volatility) in the mean-reverting\nstochastic volatility models, such as Heston and multiplicative, independent of\nstochasticity. The distribution function of stock returns, which rescales with\nthe increase of the number of days of return, is obtained from the steady-state\nvariance distribution function using the product distribution with the normal\ndistribution.\n"
    },
    {
        "paper_id": 1711.11429,
        "authors": "Yoshiaki Nakada",
        "title": "Factor endowment -- commodity output relationships in a three-factor,\n  two-good general equilibrium trade model",
        "comments": "arXiv admin note: substantial text overlap with arXiv:1711.10096",
        "journal-ref": "The Natural Resource Economics Review (2017), 22: 61-98. Issue\n  date is 25-Mar-2017.\n  https://repository.kulib.kyoto-u.ac.jp/dspace/handle/2433/219123",
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We analyze the Rybczynski sign pattern, which expresses the factor endowment\n- commodity output relationships in a three-factor, two-good general\nequilibrium trade model. The relationship determines whether a strong\nRybczynski result holds. We search for a sufficient condition for each\nRybczynski sign pattern to hold in a systematic manner, which no other studies\nhave derived. We assume factor-intensity ranking is constant. We use the EWS\n(economy-wide substitution)-ratio vector and the Hadamard product in our\nanalysis. We show that the position of the EWS-ratio vector determines the\nRybczynski sign pattern. This article provides a basis for further\napplications.\n"
    },
    {
        "paper_id": 1712.00001,
        "authors": "Tiago Fernandes",
        "title": "Some Physics Notions on Monetary Standard",
        "comments": "9 pages, no figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Regardless of the gold-standard being considered as outdated, it provides\nvaluable signs concerning the development of novel monetary standards, better\nadjusted to the current macroeconomic environment. By using a point of view of\nclassical physics, the intent of this work is doing a review of the concept of\nmonetary standard and show that the energy matrix of an economy together with a\nnew monetary standard, based on the energy supply capacity, can play an\nessential role in the sustainable growth.\n"
    },
    {
        "paper_id": 1712.00064,
        "authors": "Lily Hu and Yiling Chen",
        "title": "A Short-term Intervention for Long-term Fairness in the Labor Market",
        "comments": "10 pages",
        "journal-ref": null,
        "doi": "10.1145/3178876.3186044",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The persistence of racial inequality in the U.S. labor market against a\ngeneral backdrop of formal equality of opportunity is a troubling phenomenon\nthat has significant ramifications on the design of hiring policies. In this\npaper, we show that current group disparate outcomes may be immovable even when\nhiring decisions are bound by an input-output notion of \"individual fairness.\"\nInstead, we construct a dynamic reputational model of the labor market that\nillustrates the reinforcing nature of asymmetric outcomes resulting from\ngroups' divergent accesses to resources and as a result, investment choices. To\naddress these disparities, we adopt a dual labor market composed of a Temporary\nLabor Market (TLM), in which firms' hiring strategies are constrained to ensure\nstatistical parity of workers granted entry into the pipeline, and a Permanent\nLabor Market (PLM), in which firms hire top performers as desired. Individual\nworker reputations produce externalities for their group; the corresponding\nfeedback loop raises the collective reputation of the initially disadvantaged\ngroup via a TLM fairness intervention that need not be permanent. We show that\nsuch a restriction on hiring practices induces an equilibrium that, under\nparticular market conditions, Pareto-dominates those arising from strategies\nthat statistically discriminate or employ a \"group-blind\" criterion. The\nenduring nature of equilibria that are both inequitable and Pareto suboptimal\nsuggests that fairness interventions beyond procedural checks of hiring\ndecisions will be of critical importance in a world where machines play a\ngreater role in the employment process.\n"
    },
    {
        "paper_id": 1712.00077,
        "authors": "Carolyn E. Phelan, Daniele Marazzina, Gianluca Fusai, Guido Germano",
        "title": "Fluctuation identities with continuous monitoring and their application\n  to price barrier options",
        "comments": "30 pages, 13 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We present a numerical scheme to calculate fluctuation identities for\nexponential L\\'evy processes in the continuous monitoring case. This includes\nthe Spitzer identities for touching a single upper or lower barrier, and the\nmore difficult case of the two-barriers exit problem. These identities are\ngiven in the Fourier-Laplace domain and require numerical inverse transforms.\nThus we cover a gap in the literature that has mainly studied the discrete\nmonitoring case; indeed, there are no existing numerical methods that deal with\nthe continuous case. As a motivating application we price continuously\nmonitored barrier options with the underlying asset modelled by an exponential\nL\\'evy process. We perform a detailed error analysis of the method and develop\nerror bounds to show how the performance is limited by the truncation error of\nthe sinc-based fast Hilbert transform used for the Wiener-Hopf factorisation.\nBy comparing the results for our new technique with those for the discretely\nmonitored case (which is in the Fourier-$z$ domain) as the monitoring time step\napproaches zero, we show that the error convergence with continuous monitoring\nrepresents a limit for the discretely monitored scheme.\n"
    },
    {
        "paper_id": 1712.0013,
        "authors": "Marcel Ausloos",
        "title": "Hint of a Universal Law for the Financial Gains of Competitive Sport\n  Teams. The case of Tour de France cycle race",
        "comments": "7 pages, 2 figures, 13 references",
        "journal-ref": "Frontiers in Physics 5: 59 (Nov 2017)",
        "doi": "10.3389/fphy.2017.00059",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This short note is intended as a \"Letter to the Editor\" Perspective in order\nthat it serves as a contribution, in view of reaching the physics community\ncaring about rare events and scaling laws and unexpected findings, on a domain\nof wide interest: sport and money. It is apparent from the data reported and\ndiscussed below that the scarcity of such data does not allow to recommend a\ncomplex elaboration of an agent based model, - at this time. In some sense,\nthis also means that much data on sport activities is not necessarily given in\nterms of physics prone materials, but it could be, and would then attract much\nattention. Nevertheless the findings tie the data to well known scaling laws\nand physics processes. It is found that a simple scaling law describes the\ngains of teams in recent bicycle races, like the Tour de France. An analogous\ncase, ranking teams in Formula 1 races, is shown in an Appendix\n"
    },
    {
        "paper_id": 1712.00131,
        "authors": "Jing Shi, Marcel Ausloos, Tingting Zhu",
        "title": "Benford's law first significant digit and distribution distances for\n  testing the reliability of financial reports in developing countries",
        "comments": "22 pages, 34 references, 4 figures, 7 tables; to be published in\n  Physica A",
        "journal-ref": null,
        "doi": "10.1016/j.physa.2017.11.017",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We discuss a common suspicion about reported financial data, in 10 industrial\nsectors of the 6 so called \"main developing countries\" over the time interval\n[2000-2014]. These data are examined through Benford's law first significant\ndigit and through distribution distances tests. It is shown that several\nvisually anomalous data have to be a priori removed. Thereafter, the\ndistributions much better follow the first digit significant law, indicating\nthe usefulness of a Benford's law test from the research starting line. The\nsame holds true for distance tests. A few outliers are pointed out.\n"
    },
    {
        "paper_id": 1712.00235,
        "authors": "G\\\"okhan Ceyhan, Nermin Elif Kurt, H. Bahadir Sahin, K\\\"ur\\c{s}ad\n  Derinkuyu",
        "title": "Empirical comparison of three models for determining market clearing\n  prices in Turkish day-ahead electricity market",
        "comments": "7 pages, 3 figures, 14th International Conference on the European\n  Energy Market (EEM), 2017",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Bidders in day-ahead electricity markets want to sell/buy electricity when\ntheir bids generate positive surplus and not to take an action when the reverse\nholds. However, non-convexities in these markets cause conflicts between the\nactions that the bidders want to take and the actual market results. In this\nwork, we investigate the non-convex market clearing problem of Turkish market\noperator and propose three different rule sets. The first rule set allows both\nrejection of bids with positive surplus and acceptance of bids with negative\nsurplus. The second and the third sets only allow one of these conflicted\ncases. By using total surplus maximization as the objective, we formulate three\nmodels and statistically explore their performance with the real data taken\nfrom Turkish market operator.\n"
    },
    {
        "paper_id": 1712.00463,
        "authors": "Lena Schutte",
        "title": "Retirement Wealth under Fixed Limits: The Optimal Strategy for\n  Exponential Utility",
        "comments": "Masters Thesis",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  For an exponential utility maximizing investment strategy in a Black-Scholes\nSetting, fixed upper and lower constraints are introduced on the terminal\nwealth. This is equivalent to combining the optimal strategy with options. The\nresulting distribution is investigated in terms of change of quantiles. The\ntheory is illustrated with quantitative examples, including an assessment of\nthe effects of restricting the strategy to positive investments.\n"
    },
    {
        "paper_id": 1712.00504,
        "authors": "Rui Luo, Weinan Zhang, Xiaojun Xu, and Jun Wang",
        "title": "A Neural Stochastic Volatility Model",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper, we show that the recent integration of statistical models with\ndeep recurrent neural networks provides a new way of formulating volatility\n(the degree of variation of time series) models that have been widely used in\ntime series analysis and prediction in finance. The model comprises a pair of\ncomplementary stochastic recurrent neural networks: the generative network\nmodels the joint distribution of the stochastic volatility process; the\ninference network approximates the conditional distribution of the latent\nvariables given the observables. Our focus here is on the formulation of\ntemporal dynamics of volatility over time under a stochastic recurrent neural\nnetwork framework. Experiments on real-world stock price datasets demonstrate\nthat the proposed model generates a better volatility estimation and prediction\nthat outperforms mainstream methods, e.g., deterministic models such as GARCH\nand its variants, and stochastic models namely the MCMC-based model\n\\emph{stochvol} as well as the Gaussian process volatility model \\emph{GPVol},\non average negative log-likelihood.\n"
    },
    {
        "paper_id": 1712.00585,
        "authors": "Oleg Malafeyev, Achal Awasthi",
        "title": "Dynamic optimization of a portfolio",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper, we consider the problem of optimization of a portfolio\nconsisting of securities. An investor with an initial capital, is interested in\nconstructing a portfolio of securities. If the prices of securities change, the\ninvestor shall decide on reallocation of the portfolio. At each moment of time,\nthe prices of securities change and the investor is interested in constructing\na dynamic portfolio of securities. The investor wishes to maximize the value of\nhis portfolio at the end of time $T$. We use a novel theoretical approach based\non dynamic programming to solve the age old problem of dynamic programming. We\nconsider two cases i.e. Deterministic and Stochastic to approach the problem\nand show how the portfolio is maximized using dynamic programming.\n"
    },
    {
        "paper_id": 1712.00602,
        "authors": "ManYing Kang and Marcel Ausloos",
        "title": "An Inverse Problem Study: Credit Risk Ratings as a Determinant of\n  Corporate Governance and Capital Structure in Emerging Markets: Evidence from\n  Chinese Listed Companies",
        "comments": "25 pages, 6 tables",
        "journal-ref": "Economies 2017, 5, 41",
        "doi": "10.3390/economies5040041",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Credit risk rating is shown to be a relevant determinant in order to estimate\ngood corporate governance and to self-optimize capital structure. The\nconclusion is argued from a study on a selected (and justified) sample of (182)\ncompanies listed on the Shanghai Stock Exchange and the Shenzhen Stock Exchange\nand which use the same Shanghai Brilliance Credit Rating & Investors Service\nCompany assessment criteria, for their credit ratings, from 2010 to 2015.\nPractically, 3 debt ratios are examined in terms of 11 characteristic\nvariables. Moreover, any relationship between credit rating and corporate\ngovernance can be thought to be an interesting finding. The relationship\nbetween credit rating and leverage is not as evident as that found by other\nresearchers from different countries; it is significantly positively related to\nthe outside director, firm size, tangible assets and firm age, and CEO and\nchairman office plurality. However, leverage is found to be negatively\ncorrelated with board size, profitability, growth opportunity, and non-debt tax\nshield. Credit rating is positively associated with leverage, but in a less\nsignificant way. CEO-Board chairship duality is insignificantly related to\nleverage. The non-debt tax shield is significantly correlated with leverage.\nThe correlation coefficient between CEO duality and auditor is positive but\nweakly significant, but seems not consistent with expectations. Finally,\nprofitability cause could be regarded as an interesting finding. Indeed, there\nis an inverse correlation between profitability and total debt (Notice that the\nresult supports the pecking order theory). In conclusion, it appears that\ncredit rating has less effect on the so listed large Chinese companies than in\nother countries. Nevertheless, the perspective of assessing credit risk rating\nby relevant agencies is indubitably a recommended time dependent leverage\ndeterminant.\n"
    },
    {
        "paper_id": 1712.00975,
        "authors": "Dat Thanh Tran, Alexandros Iosifidis, Juho Kanniainen, Moncef Gabbouj",
        "title": "Temporal Attention augmented Bilinear Network for Financial Time-Series\n  Data Analysis",
        "comments": "12 pages, 4 figures, 3 tables",
        "journal-ref": null,
        "doi": "10.1109/TNNLS.2018.2869225",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Financial time-series forecasting has long been a challenging problem because\nof the inherently noisy and stochastic nature of the market. In the\nHigh-Frequency Trading (HFT), forecasting for trading purposes is even a more\nchallenging task since an automated inference system is required to be both\naccurate and fast. In this paper, we propose a neural network layer\narchitecture that incorporates the idea of bilinear projection as well as an\nattention mechanism that enables the layer to detect and focus on crucial\ntemporal information. The resulting network is highly interpretable, given its\nability to highlight the importance and contribution of each temporal instance,\nthus allowing further analysis on the time instances of interest. Our\nexperiments in a large-scale Limit Order Book (LOB) dataset show that a\ntwo-hidden-layer network utilizing our proposed layer outperforms by a large\nmargin all existing state-of-the-art results coming from much deeper\narchitectures while requiring far fewer computations.\n"
    },
    {
        "paper_id": 1712.00979,
        "authors": "Thomas Gueudr\\'e and David Martin",
        "title": "The balance of growth and risk in population dynamics",
        "comments": "11 pages, 5 figures",
        "journal-ref": "EPL, 121 (2018) 68005",
        "doi": "10.1209/0295-5075/121/68005",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Essential to each other, growth and exploration are jointly observed in\npopulations, be it alive such as animals and cells or inanimate such as goods\nand money. But their ability to move, crucial to cope with uncertainty and\noptimize returns, is tempered by the space/time properties of the environment.\nWe investigate how the environment shape optimal growth and population\ndistribution in such conditions. We uncover a trade-off between risks and\nreturns by revisiting a common growth model over general graphs. Our results\nreveal a rich and nuanced picture: fruitful strategies commonly lead to risky\npositions, but this tension may nonetheless be alleviated by the geometry of\nthe explored space. The applicability of our conclusions is subsequently\nillustrated over an empirical study of financial data.\n"
    },
    {
        "paper_id": 1712.0106,
        "authors": "Amirhossein Sobhani, Mariyan Milev",
        "title": "A Numerical Method for Pricing Discrete Double Barrier Option by\n  Lagrange Interpolation on Jacobi Node",
        "comments": "arXiv admin note: substantial text overlap with arXiv:1703.09129",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper, a rapid and high accurate numerical method for pricing\ndiscrete single and double barrier knock-out call options is presented.\nAccording to the well-known Black-Scholes framework, the price of option in\neach monitoring date could be calculate by computing a recursive integral\nformula upon the heat equation solution. We have approximated these recursive\nsolutions with the aim of Lagrange interpolation on Jacobi polynomials node.\nAfter that, an operational matrix, that makes our computation significantly\nfast, has been driven. The most important feature of this method is that its\nCPU time dose not increase when the number of monitoring dates increases. The\nnumerical results confirm the accuracy and efficiency of the presented\nnumerical algorithm.\n"
    },
    {
        "paper_id": 1712.01085,
        "authors": "Hyeong-Ohk Bae, Seung-yeon Cho, Sang-hyeok Lee, Seok-Bae Yun",
        "title": "A particle model for the herding phenomena induced by dynamic market\n  signals",
        "comments": "31 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper, we study the herding phenomena in financial markets arising\nfrom the combined effect of (1) non-coordinated collective interactions between\nthe market players and (2) concurrent reactions of market players to dynamic\nmarket signals. By interpreting the expected rate of return of an asset and the\nfavorability on that asset as position and velocity in phase space, we\nconstruct an agent-based particle model for herding behavior in finance. We\nthen define two types of herding functionals using this model, and show that\nthey satisfy a Gronwall type estimate and a LaSalle type invariance property\nrespectively, leading to the herding behavior of the market players. Various\nnumerical tests are presented to numerically verify these results.\n"
    },
    {
        "paper_id": 1712.01137,
        "authors": "Dieter Hendricks, Adam Cobb, Richard Everett, Jonathan Downing and\n  Stephen J. Roberts",
        "title": "Inferring agent objectives at different scales of a complex adaptive\n  system",
        "comments": "6 pages, 3 figures, NIPS 2017 Workshop on Learning in the Presence of\n  Strategic Behaviour (MLStrat)",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We introduce a framework to study the effective objectives at different time\nscales of financial market microstructure. The financial market can be regarded\nas a complex adaptive system, where purposeful agents collectively and\nsimultaneously create and perceive their environment as they interact with it.\nIt has been suggested that multiple agent classes operate in this system, with\na non-trivial hierarchy of top-down and bottom-up causation classes with\ndifferent effective models governing each level. We conjecture that agent\nclasses may in fact operate at different time scales and thus act differently\nin response to the same perceived market state. Given scale-specific temporal\nstate trajectories and action sequences estimated from aggregate market\nbehaviour, we use Inverse Reinforcement Learning to compute the effective\nreward function for the aggregate agent class at each scale, allowing us to\nassess the relative attractiveness of feature vectors across different scales.\nDifferences in reward functions for feature vectors may indicate different\nobjectives of market participants, which could assist in finding the scale\nboundary for agent classes. This has implications for learning algorithms\noperating in this domain.\n"
    },
    {
        "paper_id": 1712.01319,
        "authors": "Saul Jacka, Seb Armstrong and Abdel Berkaoui",
        "title": "Multi-currency reserving for coherent risk measures",
        "comments": "Revision to previous submission. A definition is changed and an error\n  corrected",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We examine the problem of dynamic reserving for risk in multiple currencies\nunder a general coherent risk measure. The reserver requires to hedge risk in a\ntime-consistent manner by trading in baskets of currencies. We show that\nreserving portfolios in multiple currencies $\\mathbf{V}$ are time-consistent\nwhen (and only when) a generalisation of Delbaen's m-stability condition\n\\cite{D06}, termed optional $\\V$-m-stability, holds. We prove a version of the\nFundamental Theorem of Asset Pricing in this context. We show that this problem\nis equivalent to dynamic trading across baskets of currencies (rather than just\npairwise trades) in a market with proportional transaction costs and with a\nfrictionless final period.\n"
    },
    {
        "paper_id": 1712.01385,
        "authors": "Paul McCloud",
        "title": "Quantum Bounds for Option Prices",
        "comments": "33 pages, 18 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Option pricing is the most elemental challenge of mathematical finance.\nKnowledge of the prices of options at every strike is equivalent to knowing the\nentire pricing distribution for a security, as derivatives contingent on the\nsecurity can be replicated using options. The available data may be\ninsufficient to determine this distribution precisely, however, and the\nquestion arises: What are the bounds for the option price at a specified\nstrike, given the market-implied constraints?\n  Positivity of the price map imposed by the principle of no-arbitrage is here\nutilised, via the Gelfand-Naimark-Segal construction, to transform the problem\ninto the domain of operator algebras. Optimisation in this larger context is\nessentially geometric, and the outcome is simultaneously super-optimal for all\ncommutative subalgebras.\n  This generates an upper bound for the price of a basket option. With\ninnovative decomposition of the assets in the basket, the result is used to\ncreate converging families of price bounds for vanilla options, interpolate the\nvolatility smile, price options on cross FX rates, and analyse the\nrelationships between swaption and caplet prices.\n"
    },
    {
        "paper_id": 1712.01479,
        "authors": "Simon Clinet and Yoann Potiron",
        "title": "Estimation for high-frequency data under parametric market\n  microstructure noise",
        "comments": "42 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We develop a general class of noise-robust estimators based on the existing\nestimators in the non-noisy high-frequency data literature. The microstructure\nnoise is a parametric function of the limit order book. The noise-robust\nestimators are constructed as plug-in versions of their counterparts, where we\nreplace the efficient price, which is non-observable, by an estimator based on\nthe raw price and limit order book data. We show that the technology can be\napplied to five leading examples where, depending on the problem, price\npossibly includes infinite jump activity and sampling times encompass\nasynchronicity and endogeneity.\n"
    },
    {
        "paper_id": 1712.02003,
        "authors": "Nathan C. Frey, Sakib Matin, H. Eugene Stanley, Michael Salinger",
        "title": "Universal fluctuations in growth dynamics of economic systems",
        "comments": "15 pages, 7 figures",
        "journal-ref": "Scientific Reports 9, 713 (2019)",
        "doi": "10.1038/s41598-018-38088-z",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The growth of business firms is an example of a system of complex interacting\nunits that resembles complex interacting systems in nature such as earthquakes.\nRemarkably, work in econophysics has provided evidence that the statistical\nproperties of the growth of business firms follow the same sorts of power laws\nthat characterize physical systems near their critical points. Given how\neconomies change over time, whether these statistical properties are\npersistent, robust, and universal like those of physical systems remains an\nopen question. Here, we show that the scaling properties of firm growth\npreviously demonstrated for publicly-traded U.S. manufacturing firms from 1974\nto 1993 apply to the same sorts of firms from 1993 to 2015, to firms in other\nbroad sectors (such as materials), and to firms in new sectors (such as\nInternet services). We measure virtually the same scaling exponent for\nmanufacturing for the 1993 to 2015 period as for the 1974 to 1993 period and\nvirtually the same scaling exponent for other sectors as for manufacturing.\nFurthermore, we show that fluctuations of the growth rate for new industries\nself-organize into a power law over relatively short time scales.\n"
    },
    {
        "paper_id": 1712.02136,
        "authors": "Ziniu Hu, Weiqing Liu, Jiang Bian, Xuanzhe Liu, Tie-Yan Liu",
        "title": "Listening to Chaotic Whispers: A Deep Learning Framework for\n  News-oriented Stock Trend Prediction",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Stock trend prediction plays a critical role in seeking maximized profit from\nstock investment. However, precise trend prediction is very difficult since the\nhighly volatile and non-stationary nature of stock market. Exploding\ninformation on Internet together with advancing development of natural language\nprocessing and text mining techniques have enable investors to unveil market\ntrends and volatility from online content. Unfortunately, the quality,\ntrustworthiness and comprehensiveness of online content related to stock market\nvaries drastically, and a large portion consists of the low-quality news,\ncomments, or even rumors. To address this challenge, we imitate the learning\nprocess of human beings facing such chaotic online news, driven by three\nprinciples: sequential content dependency, diverse influence, and effective and\nefficient learning. In this paper, to capture the first two principles, we\ndesigned a Hybrid Attention Networks to predict the stock trend based on the\nsequence of recent related news. Moreover, we apply the self-paced learning\nmechanism to imitate the third principle. Extensive experiments on real-world\nstock market data demonstrate the effectiveness of our approach.\n"
    },
    {
        "paper_id": 1712.02138,
        "authors": "Anshul Verma, Riccardo Junior Buonocore, Tiziana di Matteo",
        "title": "A cluster driven log-volatility factor model: a deepening on the source\n  of the volatility clustering",
        "comments": null,
        "journal-ref": "Quantitative Finance, 2018",
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We introduce a new factor model for log volatilities that performs\ndimensionality reduction and considers contributions globally through the\nmarket, and locally through cluster structure and their interactions. We do not\nassume a-priori the number of clusters in the data, instead using the Directed\nBubble Hierarchical Tree (DBHT) algorithm to fix the number of factors. We use\nthe factor model and a new integrated non parametric proxy to study how\nvolatilities contribute to volatility clustering. Globally, only the market\ncontributes to the volatility clustering. Locally for some clusters, the\ncluster itself contributes statistically to volatility clustering. This is\nsignificantly advantageous over other factor models, since the factors can be\nchosen statistically, whilst also keeping economically relevant factors.\nFinally, we show that the log volatility factor model explains a similar amount\nof memory to a Principal Components Analysis (PCA) factor model and an\nexploratory factor model.\n"
    },
    {
        "paper_id": 1712.02164,
        "authors": "Giorgio Ferrari, Tiziano Vargiolu",
        "title": "On the Singular Control of Exchange Rates",
        "comments": "35 pages; 6 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Consider the problem of a central bank that wants to manage the exchange rate\nbetween its domestic currency and a foreign one. The central bank can purchase\nand sell the foreign currency, and each intervention on the exchange market\nleads to a proportional cost whose instantaneous marginal value depends on the\ncurrent level of the exchange rate. The central bank aims at minimizing the\ntotal expected costs of interventions on the exchange market, plus a total\nexpected holding cost. We formulate this problem as an infinite time-horizon\nstochastic control problem with controls that have paths which are locally of\nbounded variation. The exchange rate evolves as a general linearly controlled\none-dimensional diffusion, and the two nondecreasing processes giving the\nminimal decomposition of a bounded-variation control model the cumulative\namount of foreign currency that has been purchased and sold by the central\nbank. We provide a complete solution to this problem by finding the explicit\nexpression of the value function and a complete characterization of the optimal\ncontrol. At each instant of time, the optimally controlled exchange rate is\nkept within a band whose size is endogenously determined as part of the\nsolution to the problem. We also study the expected exit time from the band,\nand the sensitivity of the width of the band with respect to the model's\nparameters in the case when the exchange rate evolves (in absence of any\nintervention) as an Ornstein-Uhlenbeck process, and the marginal costs of\ncontrols are constant. The techniques employed in the paper are those of the\ntheory of singular stochastic control and of one-dimensional diffusions.\n"
    },
    {
        "paper_id": 1712.02182,
        "authors": "Louis R. Eeckhoudt, Roger J. A. Laeven, Harris Schlesinger",
        "title": "Risk Apportionment: The Dual Story",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  By specifying model free preferences towards simple nested classes of lottery\npairs, we develop the dual story to stand on equal footing with that of\n(primal) risk apportionment. The dual story provides an intuitive\ninterpretation, and full characterization, of dual counterparts of such\nconcepts as prudence and temperance. The direction of preference between these\nnested classes of lottery pairs is equivalent to signing the successive\nderivatives of the probability weighting function within Yaari's (1987) dual\ntheory. We explore implications of our results for optimal portfolio choice and\nshow that the sign of the third derivative of the probability weighting\nfunction may be naturally linked to a self-protection problem.\n"
    },
    {
        "paper_id": 1712.02661,
        "authors": "Alexander Haluszczynski, Ingo Laut, Heike Modest and Christoph R\\\"ath",
        "title": "Linear and nonlinear market correlations: characterizing financial\n  crises and portfolio optimization",
        "comments": "12 pages, 11 figures, Phys. Rev. E, accepted",
        "journal-ref": null,
        "doi": "10.1103/PhysRevE.96.062315",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Pearson correlation and mutual information based complex networks of the\nday-to-day returns of US S&P500 stocks between 1985 and 2015 have been\nconstructed in order to investigate the mutual dependencies of the stocks and\ntheir nature. We show that both networks detect qualitative differences\nespecially during (recent) turbulent market periods thus indicating strongly\nfluctuating interconnections between the stocks of different companies in\nchanging economic environments. A measure for the strength of nonlinear\ndependencies is derived using surrogate data and leads to interesting\nobservations during periods of financial market crises. In contrast to the\nexpectation that dependencies reduce mainly to linear correlations during\ncrises we show that (at least in the 2008 crisis) nonlinear effects are\nsignificantly increasing. It turns out that the concept of centrality within a\nnetwork could potentially be used as some kind of an early warning indicator\nfor abnormal market behavior as we demonstrate with the example of the 2008\nsubprime mortgage crisis. Finally, we apply a Markowitz mean variance portfolio\noptimization and integrate the measure of nonlinear dependencies to scale the\ninvestment exposure. This leads to significant outperformance as compared to a\nfully invested portfolio.\n"
    },
    {
        "paper_id": 1712.02735,
        "authors": "Anatoliy Swishchuk, Zijia Wang",
        "title": "Variance and Volatility Swaps and Futures Pricing for Stochastic\n  Volatility Models",
        "comments": "2 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this chapter, we consider volatility swap, variance swap and VIX future\npricing under different stochastic volatility models and jump diffusion models\nwhich are commonly used in financial market. We use convexity correction\napproximation technique and Laplace transform method to evaluate volatility\nstrikes and estimate VIX future prices. In empirical study, we use Markov chain\nMonte Carlo algorithm for model calibration based on S&P 500 historical data,\nevaluate the effect of adding jumps into asset price processes on volatility\nderivatives pricing, and compare the performance of different pricing\napproaches.\n"
    },
    {
        "paper_id": 1712.0286,
        "authors": "Amir Ahmadi-Javid and Mohsen Ebadi",
        "title": "Remarks on Bayesian Control Charts",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  There is a considerable amount of ongoing research on the use of Bayesian\ncontrol charts for detecting a shift from a good quality distribution to a bad\nquality distribution in univariate and multivariate processes. It is widely\nclaimed that Bayesian control charts are economically optimal; see, for\nexample, Calabrese (1995) [Bayesian process control for attributes. Management\nScience, DOI: 10.1287/mnsc.41.4.637] and Makis (2008) [Multivariate Bayesian\ncontrol chart. Operations Research, DOI: 10.1287/opre.1070.0495]. Some\nresearchers also generalize the optimality of controls defined based on\nposterior probabilities to the class of partially observable Markov decision\nprocesses. This note points out that the existing Bayesian control charts\ncannot generally be optimal because many years ago an analytical counterexample\nwas provided by Taylor (1965) [Markovian sequential replacement processes. The\nAnnals of Mathematical Statistics, DOI: 10.1214/aoms/1177699796].\n"
    },
    {
        "paper_id": 1712.03044,
        "authors": "Jos\\'e Igor Morlanes",
        "title": "Mixed Models as an Alternative to Farima",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We construct a new process using a fractional Brownian motion and a\nfractional Ornstein-Uhlenbeck process of the Second Kind as building blocks. We\nconsider the increments of the new process in discrete time and, as a result,\nwe obtain a more parsimonious process with similar autocovariance structure to\nthat of a FARIMA. In practice, variance of the new increment process is a\nclosed-form expression easier to compute than that of FARIMA.\n"
    },
    {
        "paper_id": 1712.03106,
        "authors": "Anatoliy Swishchuk, Bruno Remillard, Robert Elliott, Jonathan\n  Chavez-Casillas",
        "title": "Compound Hawkes Processes in Limit Order Books",
        "comments": "24 pages, 16 figures. arXiv admin note: substantial text overlap with\n  arXiv:1706.07459",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper we introduce two new Hawkes processes, namely, compound and\nregime-switching compound Hawkes processes, to model the price processes in\nlimit order books. We prove Law of Large Numbers and Functional Central Limit\nTheorems (FCLT) for both processes. The two FCLTs are applied to limit order\nbooks where we use these asymptotic methods to study the link between price\nvolatility and order flow in our two models by using the diffusion limits of\nthese price processes. The volatilities of price changes are expressed in terms\nof parameters describing the arrival rates and price changes. We also present\nsome numerical examples.\n"
    },
    {
        "paper_id": 1712.03566,
        "authors": "Yong Shin Kim, Stoyan Stoyanov, Svetlozar Rachev, Frank J. Fabozzi",
        "title": "Enhancing Binomial and Trinomial Equity Option Pricing Models",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We extend the classical Cox-Ross-Rubinstein binomial model in two ways. We\nfirst develop a binomial model with time-dependent parameters that equate all\nmoments of the pricing tree increments with the corresponding moments of the\nincrements of the limiting It\\^o price process. Second, we introduce a new\ntrinomial model in the natural (historical) world, again fitting all moments of\nthe pricing tree increments to the corresponding geometric Brownian motion. We\nintroduce the risk-neutral trinomial tree and derive a hedging strategy based\non an additional perpetual derivative used as a second asset for hedging in any\nnode of the trinomial pricing tree.\n"
    },
    {
        "paper_id": 1712.03681,
        "authors": "Alberto F. Boix and Adri\\'an Segura Moreiras",
        "title": "Revisiting the determinacy on New Keynesian Models: A survey",
        "comments": "16 pages, comments are welcome. Changes with respect to the first\n  version: change of title and updated references",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The goal of this paper is to review some analytic techniques that are\npotentially useful to shed light on the determinacy question that arises in New\nKeynesian models as result of a combination of several monetary policy rules;\nin these models, we provide conditions to guarantee existence and uniqueness of\nequilibrium by means of results that are obtained from theoretical analysis. In\nparticular, these methods confirm the well known fact that Taylor--like rules\nin interest rate setting are not the only way to reach determinacy of the\nrational expectations equilibrium in the New Keynesian setting. The key\ntechnical tool we use for that purposes is the so--called Budan--Fourier\nTheorem, that we review along the paper. All the ideas and techniques presented\nhave been already used, our contribution that might be original here are the\norganization and emphasis.\n"
    },
    {
        "paper_id": 1712.04117,
        "authors": "Jacob Ferguson",
        "title": "The Calculus of Democratization and Development",
        "comments": "15 pages, 9 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In accordance with \"Democracy's Effect on Development: More Questions than\nAnswers\", we seek to carry out a study in following the description in the\n'Questions for Further Study.' To that end, we studied 33 countries in the\nSub-Saharan Africa region, who all went through an election which should signal\na \"step-up\" for their democracy, one in which previously homogenous regimes\ntransfer power to an opposition party that fairly won the election. After doing\nso, liberal-democracy indicators and democracy indicators were evaluated in the\nfive years prior to and after the election took place, and over that ten-year\nperiod, we examine the data for trends. If we see positive or negative trends\nover this time horizon, we are able to conclude that it was the recent increase\nin the quality of their democracy which led to it. Having investigated examples\nof this in depth, there seem to be three main archetypes which drive the\nresults. Countries with positive results to their democracy from the election\nhave generally positive effects on their development, countries with more\n\"plateau\" like results also did well, but countries for whom the descent to\nauthoritarianism was continued by this election found more negative results.\n"
    },
    {
        "paper_id": 1712.04418,
        "authors": "Zbigniew Palmowski and Joanna Tumilewicz",
        "title": "Fair valuation of L\\'evy-type drawdown-drawup contracts with general\n  insured and penalty functions",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper, we analyse some equity-linked contracts that are related to\ndrawdown and drawup events based on assets governed by a geometric spectrally\nnegative L\\'evy process. Drawdown and drawup refer to the differences between\nthe historical maximum and minimum of the asset price and its current value,\nrespectively. We consider four contracts. In the first contract, a protection\nbuyer pays a premium with a constant intensity $p$ until the drawdown of fixed\nsize occurs. In return, he/she receives a certain insured amount at the\ndrawdown epoch, which depends on the drawdown level at that moment. Next, the\ninsurance contract may expire earlier if a certain fixed drawup event occurs\nprior to the fixed drawdown. The last two contracts are extensions of the\nprevious ones but with an additional cancellable feature that allows the\ninvestor to terminate the contracts earlier. In these cases, a fee for early\nstopping depends on the drawdown level at the stopping epoch. In this work, we\nfocus on two problems: calculating the fair premium $p$ for basic contracts and\nfinding the optimal stopping rule for the polices with a cancellable feature.\nTo do this, we use a fluctuation theory of L\\'evy processes and rely on a\ntheory of optimal stopping.\n"
    },
    {
        "paper_id": 1712.04609,
        "authors": "Igor Halperin",
        "title": "QLBS: Q-Learner in the Black-Scholes(-Merton) Worlds",
        "comments": "30 pages (minor changes in the presentation, updated references)",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper presents a discrete-time option pricing model that is rooted in\nReinforcement Learning (RL), and more specifically in the famous Q-Learning\nmethod of RL. We construct a risk-adjusted Markov Decision Process for a\ndiscrete-time version of the classical Black-Scholes-Merton (BSM) model, where\nthe option price is an optimal Q-function, while the optimal hedge is a second\nargument of this optimal Q-function, so that both the price and hedge are parts\nof the same formula. Pricing is done by learning to dynamically optimize\nrisk-adjusted returns for an option replicating portfolio, as in the Markowitz\nportfolio theory. Using Q-Learning and related methods, once created in a\nparametric setting, the model is able to go model-free and learn to price and\nhedge an option directly from data, and without an explicit model of the world.\nThis suggests that RL may provide efficient data-driven and model-free methods\nfor optimal pricing and hedging of options, once we depart from the academic\ncontinuous-time limit, and vice versa, option pricing methods developed in\nMathematical Finance may be viewed as special cases of model-based\nReinforcement Learning. Further, due to simplicity and tractability of our\nmodel which only needs basic linear algebra (plus Monte Carlo simulation, if we\nwork with synthetic data), and its close relation to the original BSM model, we\nsuggest that our model could be used for benchmarking of different RL\nalgorithms for financial trading applications\n"
    },
    {
        "paper_id": 1712.04612,
        "authors": "Igor Halperin",
        "title": "Inverse Reinforcement Learning for Marketing",
        "comments": "18 pages, 5 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Learning customer preferences from an observed behaviour is an important\ntopic in the marketing literature. Structural models typically model\nforward-looking customers or firms as utility-maximizing agents whose utility\nis estimated using methods of Stochastic Optimal Control. We suggest an\nalternative approach to study dynamic consumer demand, based on Inverse\nReinforcement Learning (IRL). We develop a version of the Maximum Entropy IRL\nthat leads to a highly tractable model formulation that amounts to\nlow-dimensional convex optimization in the search for optimal model parameters.\nUsing simulations of consumer demand, we show that observational noise for\nidentical customers can be easily confused with an apparent consumer\nheterogeneity.\n"
    },
    {
        "paper_id": 1712.04844,
        "authors": "Anastasis Kratsios",
        "title": "Optimal Stochastic Decensoring and Applications to Calibration of Market\n  Models",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Typically flat filling, linear or polynomial interpolation methods to\ngenerate missing historical data. We introduce a novel optimal method for\nrecreating data generated by a diffusion process. The results are then applied\nto recreate historical data for stocks.\n"
    },
    {
        "paper_id": 1712.04863,
        "authors": "Longfeng Zhao, Gang-Jin Wang, Mingang Wang, Weiqi Bao, Wei Li, H.\n  Eugene Stanley",
        "title": "Stock market as temporal network",
        "comments": null,
        "journal-ref": null,
        "doi": "10.1016/j.physa.2018.05.039",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Financial networks have become extremely useful in characterizing the\nstructure of complex financial systems. Meanwhile, the time evolution property\nof the stock markets can be described by temporal networks. We utilize the\ntemporal network framework to characterize the time-evolving correlation-based\nnetworks of stock markets. The market instability can be detected by the\nevolution of the topology structure of the financial networks. We employ the\ntemporal centrality as a portfolio selection tool. Those portfolios, which are\ncomposed of peripheral stocks with low temporal centrality scores, have\nconsistently better performance under different portfolio optimization schemes,\nsuggesting that the temporal centrality measure can be used as new portfolio\noptimization and risk management tools. Our results reveal the importance of\nthe temporal attributes of the stock markets, which should be taken serious\nconsideration in real life applications.\n"
    },
    {
        "paper_id": 1712.0499,
        "authors": "Jean-Philippe Aguilar, Cyril Coste, Jan Korbel",
        "title": "Series representation of the pricing formula for the European option\n  driven by space-time fractional diffusion",
        "comments": "24 pages, 2 figures This paper is now published (in revised form),\n  and is available online at http://www.degruyter.com/view/j/fca, so always\n  cite it with the journal's coordinates",
        "journal-ref": "Fractional calculus and applied analysis 21(4), 2018, 981-1004",
        "doi": "10.1515/fca-2018-0054",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper, we show that the price of an European call option, whose\nunderlying asset price is driven by the space-time fractional diffusion, can be\nexpressed in terms of rapidly convergent double-series. The series formula can\nbe obtained from the Mellin-Barnes representation of the option price with help\nof residue summation in $\\mathbb{C}^2$. We also derive the series\nrepresentation for the associated risk-neutral factors, obtained by Esscher\ntransform of the space-time fractional Green functions.\n"
    },
    {
        "paper_id": 1712.05031,
        "authors": "Guy Metcalfe",
        "title": "The Mathematics of Market Timing",
        "comments": "18 pages, 6 figures",
        "journal-ref": "Metcalfe G (2018) The mathematics of market timing. PLoS ONE\n  13(7): e0200561",
        "doi": "10.1371/journal.pone.0200561",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Market timing is an investment technique that tries to continuously switch\ninvestment into assets forecast to have better returns. What is the likelihood\nof having a successful market timing strategy? With an emphasis on modeling\nsimplicity, I calculate the feasible set of market timing portfolios using\nindex mutual fund data for perfectly timed (by hindsight) all or nothing\nquarterly switching between two asset classes, US stocks and bonds over the\ntime period 1993--2017. The historical optimal timing path of switches is shown\nto be indistinguishable from a random sequence. The key result is that the\nprobability distribution function of market timing returns is asymetric, that\nthe highest probability outcome for market timing is a below median return. Put\nanother way, simple math says market timing is more likely to lose than to\nwin---even before accounting for costs. The median of the market timing return\nprobability distribution can be directly calculated as a weighted average of\nthe returns of the model assets with the weights given by the fraction of time\neach asset has a higher return than the other. For the time period of the data\nthe median return was close to, but not identical with, the return of a static\n60:40 stock:bond portfolio. These results are illustrated through Monte Carlo\nsampling of timing paths within the feasible set and by the observed return\npaths of several market timing mutual funds.\n"
    },
    {
        "paper_id": 1712.05121,
        "authors": "Vygintas Gontis, Aleksejus Kononovicius",
        "title": "The consentaneous model of the financial markets exhibiting spurious\n  nature of long-range memory",
        "comments": "16 pages, 6 figures",
        "journal-ref": "Physica A 505 (2018): 1075-1083",
        "doi": "10.1016/j.physa.2018.04.053",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  It is widely accepted that there is strong persistence in the volatility of\nfinancial time series. The origin of the observed persistence, or long-range\nmemory, is still an open problem as the observed phenomenon could be a spurious\neffect. Earlier we have proposed the consentaneous model of the financial\nmarkets based on the non-linear stochastic differential equations. The\nconsentaneous model successfully reproduces empirical probability and power\nspectral densities of volatility. This approach is qualitatively different from\nmodels built using fractional Brownian motion. In this contribution we\ninvestigate burst and inter-burst duration statistics of volatility in the\nfinancial markets employing the consentaneous model. Our analysis provides an\nevidence that empirical statistical properties of burst and inter-burst\nduration can be explained by non-linear stochastic differential equations\ndriving the volatility in the financial markets. This serves as an strong\nargument that long-range memory in finance can have spurious nature.\n"
    },
    {
        "paper_id": 1712.05254,
        "authors": "Foad Shokrollahi",
        "title": "The evaluation of geometric Asian power options under time changed mixed\n  fractional Brownian motion",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The aim of this paper is to evaluate geometric Asian option by a mixed\nfractional subdiffusive Black-Scholes model. We derive a pricing formula for\ngeometric Asian option when the underlying stock follows a time changed mixed\nfractional Brownian motion. We then apply the results to price Asian power\noptions on the stocks that pay constant dividends when the payoff is a power\nfunction. Finally, lower bound of Asian options and some special cases are\nprovided.\n"
    },
    {
        "paper_id": 1712.05527,
        "authors": "Gery Geenens and Richard Dunn",
        "title": "A nonparametric copula approach to conditional Value-at-Risk",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Value-at-Risk and its conditional allegory, which takes into account the\navailable information about the economic environment, form the centrepiece of\nthe Basel framework for the evaluation of market risk in the banking sector. In\nthis paper, a new nonparametric framework for estimating this conditional\nValue-at-Risk is presented. A nonparametric approach is particularly pertinent\nas the traditionally used parametric distributions have been shown to be\ninsufficiently robust and flexible in most of the equity-return data sets\nobserved in practice. The method extracts the quantile of the conditional\ndistribution of interest, whose estimation is based on a novel estimator of the\ndensity of the copula describing the dynamic dependence observed in the series\nof returns. Real-world back-testing analyses demonstrate the potential of the\napproach, whose performance may be superior to its industry counterparts.\n"
    },
    {
        "paper_id": 1712.05676,
        "authors": "Lijun Bo, Huafu Liao and Xiang Yu",
        "title": "Risk Sensitive Portfolio Optimization with Default Contagion and\n  Regime-Switching",
        "comments": "Final version. SIAM Journal on Control and Optimization, forthcoming;\n  Keywords: Default contagion; regime switching; countably infinite states;\n  risk sensitive control; recursive dynamical programming equations;\n  verification theorems",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We study an open problem of risk-sensitive portfolio allocation in a\nregime-switching credit market with default contagion. The state space of the\nMarkovian regime-switching process is assumed to be a countably infinite set.\nTo characterize the value function, we investigate the corresponding recursive\ninfinite-dimensional nonlinear dynamical programming equations (DPEs) based on\ndefault states. We propose to work in the following procedure: Applying the\ntheory of monotone dynamical system, we first establish the existence and\nuniqueness of classical solutions to the recursive DPEs by a truncation\nargument in the finite state space. The associated optimal feedback strategy is\ncharacterized by developing a rigorous verification theorem. Building upon\nresults in the first stage, we construct a sequence of approximating risk\nsensitive control problems with finite states and prove that the resulting\nsmooth value functions will converge to the classical solution of the original\nsystem of DPEs. The construction and approximation of the optimal feedback\nstrategy for the original problem are also thoroughly discussed.\n"
    },
    {
        "paper_id": 1712.0584,
        "authors": "Daniel Bj\\\"orkegren and Darrell Grissen",
        "title": "Behavior Revealed in Mobile Phone Usage Predicts Loan Repayment",
        "comments": null,
        "journal-ref": "The World Bank Economic Review, 34(3), 2020, 618-634",
        "doi": "10.1093/wber/lhz006",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Many households in developing countries lack formal financial histories,\nmaking it difficult for firms to extend credit, and for potential borrowers to\nreceive it. However, many of these households have mobile phones, which\ngenerate rich data about behavior. This article shows that behavioral\nsignatures in mobile phone data predict default, using call records matched to\nrepayment outcomes for credit extended by a South American telecom. On a sample\nof individuals with (thin) financial histories, our method actually outperforms\nmodels using credit bureau information, both within time and when tested on a\ndifferent time period. But our method also attains similar performance on those\nwithout financial histories, who cannot be scored using traditional methods.\nIndividuals in the highest quintile of risk by our measure are 2.8 times more\nlikely to default than those in the lowest quintile. The method forms the basis\nfor new forms of credit that reach the unbanked.\n"
    },
    {
        "paper_id": 1712.06263,
        "authors": "Tetsuya Takaishi and Ting Ting Chen",
        "title": "The relationship between trading volumes, number of transactions, and\n  stock volatility in GARCH models",
        "comments": "4 pages, 1 figure",
        "journal-ref": "J. Phys.: Conf. Ser. 738 (2016) 012097",
        "doi": "10.1088/1742-6596/738/1/012097",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We examine the relationship between trading volumes, number of transactions,\nand volatility using daily stock data of the Tokyo Stock Exchange. Following\nthe mixture of distributions hypothesis, we use trading volumes and the number\nof transactions as proxy for the rate of information arrivals affecting stock\nvolatility. The impact of trading volumes or number of transactions on\nvolatility is measured using the generalized autoregressive conditional\nheteroscedasticity (GARCH) model. We find that the GARCH effects, that is,\npersistence of volatility, is not always removed by adding trading volumes or\nnumber of transactions, indicating that trading volumes and number of\ntransactions do not adequately represent the rate of information arrivals.\n"
    },
    {
        "paper_id": 1712.06358,
        "authors": "Kiran Sharma, Anamika, Anindya S. Chakrabarti, Anirban Chakraborti,\n  Sujoy Chakravarty",
        "title": "The Saga of KPR: Theoretical and Experimental developments",
        "comments": "14 pages, Submitted for publication in a special issue in Science and\n  Culture (Kolkata, India)",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this article, we present a brief narration of the origin and the overview\nof the recent developments done on the Kolkata Paise Restaurant (KPR) problem,\nwhich can serve as a prototype for a broader class of resource allocation\nproblems in the presence of a large number of competing agents, typically\nstudied using coordination and anti-coordination games. We discuss the KPR and\nits several extensions, as well as its applications in many economic and social\nphenomena. We end the article with some discussions on our ongoing experimental\nanalysis of the same problem. We demonstrate that this provides an interesting\npicture of how people analyze complex situations, and design their strategies\nor react to them.\n"
    },
    {
        "paper_id": 1712.06466,
        "authors": "Roberto Baviera",
        "title": "Back-of-the-envelope swaptions in a very parsimonious multicurve\n  interest rate model",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We propose an elementary model to price European physical delivery swaptions\nin multicurve setting with a simple exact closed formula. The proposed model is\nvery parsimonious: it is a three-parameter multicurve extension of the\ntwo-parameter Hull-White (1990) model. The model allows also to obtain simple\nformulas for all other plain vanilla Interest Rate derivatives. Calibration\nissues are discussed in detail.\n"
    },
    {
        "paper_id": 1712.06664,
        "authors": "Young Shin Kim, Stoyan Stoyanov, Svetlozar Rachev, and Frank J.\n  Fabozzi",
        "title": "Another Look at the Ho-Lee Bond Option Pricing Model",
        "comments": null,
        "journal-ref": "The Journal of Derivatives Summer 2018, 25 (4) 48-53",
        "doi": "10.3905/jod.2018.25.4.048",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper, we extend the classical Ho-Lee binomial term structure model\nto the case of time-dependent parameters and, as a result, resolve a drawback\nassociated with the model. This is achieved with the introduction of a more\nflexible no-arbitrage condition in contrast to the one assumed in the Ho-Lee\nmodel.\n"
    },
    {
        "paper_id": 1712.0732,
        "authors": "Yuri F. Saporito",
        "title": "First-Order Asymptotics of Path-Dependent Derivatives in Multiscale\n  Stochastic Volatility Environment",
        "comments": null,
        "journal-ref": null,
        "doi": "10.1142/S0219024918500243",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper, we extend the first-order asymptotics analysis of Fouque et\nal. to general path-dependent financial derivatives using Dupire's functional\nIto calculus. The main conclusion is that the market group parameters\ncalibrated to vanilla options can be used to price to the same order exotic,\npath-dependent derivatives as well. Under general conditions, the first-order\ncondition is represented by a conditional expectation that could be numerically\nevaluated. Moreover, if the path-dependence is not too severe, we are able to\nfind path-dependent closed-form solutions equivalent to the fist-order\napproximation of path-independent options derived in Fouque et al.\nAdditionally, we exemplify the results with Asian options and options on\nquadratic variation.\n"
    },
    {
        "paper_id": 1712.07383,
        "authors": "Bruno Bouchard (CEREMADE), Ki Chau (CWI), Arij Manai (UM), Ahmed\n  Sid-Ali",
        "title": "Monte-Carlo methods for the pricing of American options: a semilinear\n  BSDE point of view",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We extend the viscosity solution characterization proved in [5] for call/put\nAmerican option prices to the case of a general payoff function in a\nmulti-dimensional setting: the price satisfies a semilinear re-action/diffusion\ntype equation. Based on this, we propose two new numerical schemes inspired by\nthe branching processes based algorithm of [8]. Our numerical experiments show\nthat approximating the discontinu-ous driver of the associated\nreaction/diffusion PDE by local polynomials is not efficient, while a simple\nrandomization procedure provides very good results.\n"
    },
    {
        "paper_id": 1712.07649,
        "authors": "Valerii Salov",
        "title": "Trading Strategies with Position Limits",
        "comments": "64 pages, 17 figures, 6 tables, 123 references",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Whether you trade futures for yourself or a hedge fund, your strategy is\ncounted. Long and short position limits make the number of unique strategies\nfinite. Formulas of the numbers of strategies, transactions, do nothing actions\nare derived. A discrete distribution of actions, corresponding probability\nmass, cumulative distribution and characteristic functions, moments, extreme\nvalues are presented. Strategies time slice distributions are determined.\nVector properties of trading strategies are studied. Algebraic not associative,\ncommutative, initial magmas with invertible elements control trading positions\nand strategies. Maximum profit strategies, MPS, and optimal trading elements\ncan define trading patterns. Dynkin introduced the term interpreted in English\nas \"Markov time\" in 1963. Neftci applied it for the formalization of Technical\nAnalysis in 1991.\n"
    },
    {
        "paper_id": 1712.07699,
        "authors": "Daniel Bartl, Patrick Cheridito and Michael Kupper",
        "title": "Robust expected utility maximization with medial limits",
        "comments": null,
        "journal-ref": "Journal of Mathematical Analysis and Applications, 471(1-2),\n  752-775, 2019",
        "doi": "10.1016/j.jmaa.2018.11.012",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper we study a robust expected utility maximization problem with\nrandom endowment in discrete time. We give conditions under which an optimal\nstrategy exists and derive a dual representation for the optimal utility. Our\napproach is based on a general representation result for monotone convex\nfunctionals, a functional version of Choquet's capacitability theorem and\nmedial limits. The novelty is that it works under nondominated model\nuncertainty without any assumptions of time-consistency. As applications, we\ndiscuss robust utility maximization problems with moment constraints,\nWasserstein constraints and Wasserstein penalties.\n"
    },
    {
        "paper_id": 1712.07796,
        "authors": "Kein Joe Lau, Yong Kheng Goh and An-Chow Lai",
        "title": "Gibbs sampler with jump diffusion model: application in European call\n  option and annuity",
        "comments": "15 pages, 5 figures, Submitted to IJTAF",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper, we are presenting a method for estimation of market parameters\nmodeled by jump diffusion process. The method proposed is based on Gibbs\nsampler, while the market parameters are the drift, the volatility, the jump\nintensity and its rate of occurrence. Demonstration on how to use these\nparameters to estimate the fair price of European call option and annuity will\nbe shown, for the situation where the market is modeled by jump diffusion\nprocess with different intensity and occurrence. The results is compared to\nconventional options to observe the impact of jump effects.\n"
    },
    {
        "paper_id": 1712.07806,
        "authors": "Yu-Jui Huang, Zhou Zhou",
        "title": "Optimal Equilibria for Time-Inconsistent Stopping Problems in Continuous\n  Time",
        "comments": null,
        "journal-ref": "Mathematical Finance, Vol. 30 (2020), Issue 3, pp 1103-1134",
        "doi": "10.1111/mafi.12229",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  For an infinite-horizon continuous-time optimal stopping problem under\nnon-exponential discounting, we look for an optimal equilibrium, which\ngenerates larger values than any other equilibrium does on the entire state\nspace. When the discount function is log sub-additive and the state process is\none-dimensional, an optimal equilibrium is constructed in a specific form,\nunder appropriate regularity and integrability conditions. While there may\nexist other optimal equilibria, we show that they can differ from the\nconstructed one in very limited ways. This leads to a sufficient condition for\nthe uniqueness of optimal equilibria, up to some closedness condition. To\nillustrate our theoretic results, comprehensive analysis is carried out for\nthree specific stopping problems, concerning asset liquidation and real options\nvaluation. For each one of them, an optimal equilibrium is characterized\nthrough an explicit formula.\n"
    },
    {
        "paper_id": 1712.08137,
        "authors": "Marcellino Gaudenzi and Alice Spangaro and Patrizia Stucchi",
        "title": "Efficient European and American option pricing under a jump-diffusion\n  process",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  When the underlying asset displays oscillations, spikes or heavy-tailed\ndistributions, the lognormal diffusion process (for which Black and Scholes\ndeveloped their momentous option pricing formula) is inadequate: in order to\novercome these real world difficulties many models have been developed. Merton\nproposed a jump-diffusion model, where the dynamics of the price of the\nunderlying are subject to variations due to a Brownian process and also to\npossible jumps, driven by a compound Poisson process. Merton's model admits a\nseries solution for the European option price, and there have been a lot of\nattempts to obtain a discretisation of the Merton model with tree methods in\norder to price American or more complex options, e. g. Amin, the $O(n^3)$\nprocedure by Hilliard and Schwartz and the $O(n^{2.5})$ procedure by Dai et al.\nHere, starting from the implementation of the seven-nodes procedure by Hilliard\nand Schwartz, we prove theoretically that it is possible to reduce the\ncomplexity to $O(n \\ln n)$ in the European case and $O(n^2 \\ln n)$ in the\nAmerican put case. These theoretical results can be obtained through suitable\ntruncation of the lattice structure and the proofs provide closed formulas for\nthe truncation limitations.\n"
    },
    {
        "paper_id": 1712.08247,
        "authors": "Igor V. Kravchenko, Vladislav V. Kravchenko, Sergii M. Torba, Jos\\'e\n  Carlos Dias",
        "title": "Pricing double barrier options on homogeneous diffusions: a Neumann\n  series of Bessel functions representation",
        "comments": "19 pages, 4 tables, 3 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper develops a novel analytically tractable Neumann series of Bessel\nfunctions representation for pricing (and hedging) European-style double\nbarrier knock-out options, which can be applied to the whole class of\none-dimensional time-homogeneous diffusions even for the cases where the\ncorresponding transition density is not known. The proposed numerical method is\nshown to be efficient and simple to implement. To illustrate the flexibility\nand computational power of the algorithm, we develop an extended jump to\ndefault model that is able to capture several empirical regularities commonly\nobserved in the literature.\n"
    },
    {
        "paper_id": 1712.08329,
        "authors": "Antoine Lejay (TOSCA, IECL), Paolo Pigato (WIAS)",
        "title": "A threshold model for local volatility: evidence of leverage and mean\n  reversion effects on historical data",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In financial markets, low prices are generally associated with high\nvolatilities and vice-versa, this well known stylized fact usually being\nreferred to as leverage effect. We propose a local volatility model, given by a\nstochastic differential equation with piecewise constant coefficients, which\naccounts of leverage and mean-reversion effects in the dynamics of the prices.\nThis model exhibits a regime switch in the dynamics accordingly to a certain\nthreshold. It can be seen as a continuous-time version of the Self-Exciting\nThreshold Autoregressive (SETAR) model. We propose an estimation procedure for\nthe volatility and drift coefficients as well as for the threshold level.\nParameters estimated on the daily prices of 348 stocks of NYSE and S\\&P 500, on\ndifferent time windows, show consistent empirical evidence for leverageeffects.\nMean-reversion effects are also detected, most markedly in crisis periods.\n"
    },
    {
        "paper_id": 1712.08654,
        "authors": "Rehana Naz",
        "title": "Closed-form Solutions for the Lucas-Uzawa model: Unique or Multiple",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Naz and Chaudhry [3] established multiple closed-form solutions for the basic\nLucas-Uzawa model. According to Boucekkine and Ruiz-Tamarit [1] and Chilarescu\n[2] unique closed-form solutions exist for the basic Lucas-Uzawa model. We\nequate expressions for variables h(t) and u(t). We provide here condition for\nthe unique closed-form solution and proposed an open question for evaluation of\nintegral in closed-form. A similar analysis is carried out for the Lucas-Uzawa\nmodel with logarithmic utility preferences.\n"
    },
    {
        "paper_id": 1712.08716,
        "authors": "Artem Hulko and Mark Whitmeyer",
        "title": "A Game of Random Variables",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper analyzes a simple game with $n$ players. We fix a mean, $\\mu$, in\nthe interval $[0, 1]$ and let each player choose any random variable\ndistributed on that interval with the given mean. The winner of the zero-sum\ngame is the player whose random variable has the highest realization. We show\nthat the position of the mean within the interval is paramount. Remarkably, if\nthe given mean is above a crucial threshold then the unique equilibrium must\ncontain a point mass on $1$. The cutoff is strictly decreasing in the number of\nplayers, $n$; and for fixed $\\mu$, as the number of players is increased, each\nplayer places more weight on $1$ at equilibrium. We characterize the\nequilibrium as the number of players goes to infinity.\n"
    },
    {
        "paper_id": 1712.08876,
        "authors": "Jian Gao",
        "title": "Maximizing the Collective Learning Effects in Regional Economic\n  Development",
        "comments": null,
        "journal-ref": "2017 14th ICCWAMTIP, IEEE, 2017, pp. 337-341",
        "doi": "10.1109/ICCWAMTIP.2017.8301509",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Collective learning in economic development has been revealed by recent\nempirical studies, however, investigations on how to benefit most from its\neffects remain still lacking. In this paper, we explore the maximization of the\ncollective learning effects using a simple propagation model to study the\ndiversification of industries on real networks built on Brazilian labor data.\nFor the inter-regional learning, we find an optimal strategy that makes a\nbalance between core and periphery industries in the initial activation,\nconsidering the core-periphery structure of the industry space--a network\nrepresentation of the relatedness between industries. For the inter-regional\nlearning, we find an optimal strategy that makes a balance between nearby and\ndistant regions in establishing new spatial connections, considering the\nspatial structure of the integrated adjacent network that connects all regions.\nOur findings suggest that the near to by random strategies are likely to make\nthe best use of the collective learning effects in advancing regional economic\ndevelopment practices.\n"
    },
    {
        "paper_id": 1712.09087,
        "authors": "Valentina V. Tarasova, Vasily E. Tarasov",
        "title": "Dynamic intersectoral models with power-law memory",
        "comments": "24 pages, pdf",
        "journal-ref": "Communications in Nonlinear Science and Numerical Simulation.\n  2018. Vol. 54. P. 100-117",
        "doi": "10.1016/j.cnsns.2017.05.015",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Intersectoral dynamic models with power-law memory are proposed. The\nequations of open and closed intersectoral models, in which the memory effects\nare described by the Caputo derivatives of non-integer orders, are derived. We\nsuggest solutions of these equations, which have the form of linear\ncombinations of the Mittag-Leffler functions and which are characterized by\ndifferent effective growth rates. Examples of intersectoral dynamics with\npower-law memory are suggested for two sectoral cases. We formulate two\nprinciples of intersectoral dynamics with memory: the principle of changing of\ntechnological growth rates and the principle of domination change. It has been\nshown that in the input-output economic dynamics the effects of fading memory\ncan change the economic growth rate and dominant behavior of economic sectors.\n"
    },
    {
        "paper_id": 1712.09088,
        "authors": "Valentina V. Tarasova, Vasily E. Tarasov",
        "title": "Concept of dynamic memory in economics",
        "comments": "32 pages, pdf",
        "journal-ref": "Communications in Nonlinear Science and Numerical Simulation.\n  2018. Vol.55. P.127-145",
        "doi": "10.1016/j.cnsns.2017.06.032",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper we discuss a concept of dynamic memory and an application of\nfractional calculus to describe the dynamic memory. The concept of memory is\nconsidered from the standpoint of economic models in the framework of\ncontinuous time approach based on fractional calculus. We also describe some\ngeneral restrictions that can be imposed on the structure and properties of\ndynamic memory. These restrictions include the following three principles: (a)\nthe principle of fading memory; (b) the principle of memory homogeneity on time\n(the principle of non-aging memory); (c) the principle of memory reversibility\n(the principle of memory recovery). Examples of different memory functions are\nsuggested by using the fractional calculus. To illustrate an application of the\nconcept of dynamic memory in economics we consider a generalization of the\nHarrod-Domar model, where the power-law memory is taken into account.\n"
    },
    {
        "paper_id": 1712.09092,
        "authors": "Valentina V. Tarasova, Vasily E. Tarasov",
        "title": "Logistic map with memory from economic model",
        "comments": "19 pages, pdf",
        "journal-ref": "Chaos, Solitons & Fractals. 2017. Vol. 95. P. 84-91",
        "doi": "10.1016/j.chaos.2016.12.012",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  A generalization of the economic model of logistic growth, which takes into\naccount the effects of memory and crises, is suggested. Memory effect means\nthat the economic factors and parameters at any given time depend not only on\ntheir values at that time, but also on their values at previous times. For the\nmathematical description of the memory effects, we use the theory of\nderivatives of non-integer order. Crises are considered as sharp splashes\n(bursts) of the price, which are mathematically described by the\ndelta-functions. Using the equivalence of fractional differential equations and\nthe Volterra integral equations, we obtain discrete maps with memory that are\nexact discrete analogs of fractional differential equations of economic\nprocesses. We derive logistic map with memory, its generalizations, and\n\"economic\" discrete maps with memory from the fractional differential\nequations, which describe the economic natural growth with competition,\npower-law memory and crises.\n"
    },
    {
        "paper_id": 1712.09108,
        "authors": "Vladimir Vovk",
        "title": "Non-stochastic portfolio theory",
        "comments": "16 pages, 1 figure; Working Paper 51 at\n  http://probabilityandfinance.com",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper studies a non-stochastic version of Fernholz's stochastic\nportfolio theory for a simple model of stock markets with continuous price\npaths. It establishes non-stochastic versions of the most basic results of\nstochastic portfolio theory and discusses connections with Stroock-Varadhan\nmartingales.\n"
    },
    {
        "paper_id": 1712.09201,
        "authors": "Peter Kritzer, Gunther Leobacher, Michaela Sz\\\"olgyenyi, Stefan\n  Thonhauser",
        "title": "Approximation methods for piecewise deterministic Markov processes and\n  their costs",
        "comments": null,
        "journal-ref": "Scandinavian Actuarial Journal 2019",
        "doi": "10.1080/03461238.2018.1560357",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper, we analyse piecewise deterministic Markov processes, as\nintroduced in Davis (1984). Many models in insurance mathematics can be\nformulated in terms of the general concept of piecewise deterministic Markov\nprocesses. In this context, one is interested in computing certain quantities\nof interest such as the probability of ruin of an insurance company, or the\ninsurance company's value, defined as the expected discounted future dividend\npayments until the time of ruin. Instead of explicitly solving the\nintegro-(partial) differential equation related to the quantity of interest\nconsidered (an approach which can only be used in few special cases), we adapt\nthe problem in a manner that allows us to apply deterministic numerical\nintegration algorithms such as quasi-Monte Carlo rules; this is in contrast to\napplying random integration algorithms such as Monte Carlo. To this end, we\nreformulate a general cost functional as a fixed point of a particular integral\noperator, which allows for iterative approximation of the functional.\nFurthermore, we introduce a smoothing technique which is applied to the\nintegrands involved, in order to use error bounds for deterministic cubature\nrules. On the analytical side, we prove a convergence result for our PDMP\napproximation, which is of independent interest as it justifies phase-type\napproximations on the process level. We illustrate the smoothing technique for\na risk-theoretic example, and provide a comparative study of deterministic and\nMonte Carlo integration.\n"
    },
    {
        "paper_id": 1712.09575,
        "authors": "Valentina V. Tarasova, Vasily E. Tarasov",
        "title": "Economic interpretation of fractional derivatives",
        "comments": "10 pages, 2 figures, LaTeX",
        "journal-ref": "Progress in Fractional Differentiation and Applications. 2017.\n  Vol.3. No.1. P.1-7",
        "doi": "10.18576/pfda/030101",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  An economic interpretation of the Caputo derivatives of non-integer orders is\nproposed. The suggested economic interpretation of the fractional derivatives\nis based on a generalization of average and marginal values of economic\nindicators. We formulate an economic interpretation by using the concept of the\nT-indicator that allows us to describe economic processes with memory. The\nstandard average and marginal values of indicator are special cases of the\nproposed T-indicator, when the order is equal to zero and one, respectively.\nThe fractional derivatives are interpreted as economic characteristics\n(indicators) that are intermediate between the standard average and marginal\nvalues of indicators.\n"
    },
    {
        "paper_id": 1712.09592,
        "authors": "O.B. Sezer, M. Ozbayoglu, E. Dogdu",
        "title": "An Artificial Neural Network-based Stock Trading System Using Technical\n  Analysis and Big Data Framework",
        "comments": "ACM Southeast Conference, ACMSE 2017, Kennesaw State University, GA,\n  U.S.A., 13-15 April, 2017",
        "journal-ref": "ACM Southeast Conference, ACMSE 2017, Kennesaw State University,\n  GA, U.S.A., 13-15 April, 2017",
        "doi": "10.1145/3077286.3077294",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper, a neural network-based stock price prediction and trading\nsystem using technical analysis indicators is presented. The model developed\nfirst converts the financial time series data into a series of buy-sell-hold\ntrigger signals using the most commonly preferred technical analysis\nindicators. Then, a Multilayer Perceptron (MLP) artificial neural network (ANN)\nmodel is trained in the learning stage on the daily stock prices between 1997\nand 2007 for all of the Dow30 stocks. Apache Spark big data framework is used\nin the training stage. The trained model is then tested with data from 2007 to\n2017. The results indicate that by choosing the most appropriate technical\nindicators, the neural network model can achieve comparable results against the\nBuy and Hold strategy in most of the cases. Furthermore, fine tuning the\ntechnical indicators and/or optimization strategy can enhance the overall\ntrading performance.\n"
    },
    {
        "paper_id": 1712.09605,
        "authors": "Valentina V. Tarasova, Vasily E. Tarasov",
        "title": "Accelerators in macroeconomics: Comparison of discrete and continuous\n  approaches",
        "comments": "12 pages, pdf",
        "journal-ref": "American Journal of Economics and Business Administration. 2017.\n  Vol.9. No.3. P.47-55",
        "doi": "10.3844/ajebasp.2017.47.55",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We prove that the standard discrete-time accelerator equation cannot be\nconsidered as an exact discrete analog of the continuous-time accelerator\nequation. This leads to fact that the standard discrete-time macroeconomic\nmodels cannot be considered as exact discretization of the corresponding\ncontinuous-time models. As a result, the equations of the continuous and\nstandard discrete models have different solutions and can predict the different\nbehavior of the economy. In this paper, we propose a self-consistent\ndiscrete-time description of the economic accelerators that is based on the\nexact finite differences. For discrete-time approach, the model equations with\nexact differences have the same solutions as the corresponding continuous-time\nmodels and these discrete and continuous models describe the same behavior of\nthe economy. Using the Harrod-Domar growth model as an example, we show that\nequations of the continuous-time model and the suggested exact discrete model\nhave the same solutions and these models predict the same behavior of the\neconomy.\n"
    },
    {
        "paper_id": 1712.09854,
        "authors": "Takaki Hayashi, Yuta Koike",
        "title": "No arbitrage and lead-lag relationships",
        "comments": "16 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The existence of time-lagged cross-correlations between the returns of a pair\nof assets, which is known as the lead-lag relationship, is a well-known\nstylized fact in financial econometrics. Recently some continuous-time models\nhave been proposed to take account of the lead-lag relationship. Such a model\ndoes not follow a semimartingale as long as the lead-lag relationship is\npresent, so it admits an arbitrage without market frictions. In this paper we\nshow that they are free of arbitrage if we take account of market frictions\nsuch as the presence of minimal waiting time on subsequent transactions or\ntransaction costs.\n"
    },
    {
        "paper_id": 1712.09969,
        "authors": "Russell Stanley Q. Geronimo",
        "title": "De Facto Control: Applying Game Theory to the Law on Corporate\n  Nationality",
        "comments": null,
        "journal-ref": "Philippine Law Journal, Vol. 90 Issue No. 2 (2017)",
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  One unexamined assumption in foreign ownership regulation is the notion that\nmajority voting rights translate to 'effective control'. This assumption is so\ndeeply entrenched in foreign investments law that possession of majority voting\nrights can determine the nationality of a corporation and its capacity to\nengage in partially nationalized economic activities. The fact, however, is\nthat minority stockholders can possess a degree of voting power higher than\nwhat their shareholding size might suggest. Voting power is not the same as\nvoting weight and is not measured simply by the proportion or number of votes a\nstockholder may cast in a stockholder meeting. This paper proposes and\ndemonstrates a method for calculating 'effective control' based on given voting\nthresholds and voting weights. It also shows instances where the 'effective\ncontrol' of a foreign minority stockholder appears to comply with foreign\nequity limitations, but has a 'real' voting power grossly beyond the allowable\nthreshold.\n"
    },
    {
        "paper_id": 1712.09978,
        "authors": "Russell Stanley Q. Geronimo",
        "title": "Why Long-Term Debt Instruments Cannot Be Deposit Substitutes",
        "comments": null,
        "journal-ref": "Lexkhoj Int'l J. Crim. L. Vol. 2 Issue No. 4 (2017)",
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The definition of deposit substitutes in Philippine tax law fails to consider\nthe maturity of a debt instrument. This makes it possible for long-term bonds\nto be considered as deposit substitutes if they meet the 20-lender rule,\ntaxable at 20% final tax. However, long-term debt instruments cannot\nrealistically function as deposit substitutes even if they fall in the hands of\n20 or more lenders. First, long-term debt instruments cannot simultaneously\nreplicate the twin features of capital preservation and liquidity, which are\nintegral to the nature of a deposit substitute. Second, deposit substitutes are\nan integral part of the maturity transformation process (i.e. short-term\nborrowing for the purpose of long-term lending) in financial intermediaries,\nwhich means that they should have low borrowing cost, made possible only by\nhaving short-term maturity. To prove these propositions, this paper situates\nthe function of deposit substitutes within the context of shadow banking, where\nsaid instruments originated and are generally used. To show the incompatibility\nbetween a deposit substitute and a long-term debt instrument, the paper applies\nthe fundamental theory of bond values to 10-year zero-coupon treasury notes\ncalled 'PEACe Bonds' in Banco De Oro, et al. vs. Republic (2015 and 2016). The\npaper recommends that deposit substitutes should be limited to debt instruments\nwith maturity of not more than 1 year.\n"
    },
    {
        "paper_id": 1712.09987,
        "authors": "Russell Stanley Q. Geronimo",
        "title": "How Short Sales Circumvent the Capital Gains Tax System",
        "comments": null,
        "journal-ref": "Philippine Law Journal Vol. 90 Issue No. 01 (2016)",
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Through a short sale, a person borrows a share of stock from a lender, sells\nthe borrowed share to a third person at the current price, and purchases an\nidentical share in the market at a future date and at a future price to replace\nthe borrowed share of stock. This only makes sense if the short seller\nanticipates a downward trend in share price. The short seller incurs a gain if\nshare price decreases because the cost of replacing the borrowed share falls\nbelow the selling price. The reverse is true in an ordinary sale, where a\nperson owning a share of stock incurs a loss if price decreases because the\nselling price falls below the basis or acquisition cost. Therefore, when a\ntaxpayer simultaneously owns a share of stock and short sells an identical\nstock, any gain in an ordinary sale of the owned stock is offset by a\ncorresponding loss in the short sale of the borrowed identical stock, vice\nversa. This offsetting effect, in turn, creates an unexpected tax deferral\nopportunity abused in other jurisdictions and which remains unregulated in the\nPhilippine tax system.\n"
    },
    {
        "paper_id": 1712.10105,
        "authors": "Ben-zhang Yang, Jia Yue and Nan-jing Huang",
        "title": "Variance swaps under L\\'{e}vy process with stochastic volatility and\n  stochastic interest rate in incomplete markets",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper focuses on the pricing of the variance swap in an incomplete\nmarket where the stochastic interest rate and the price of the stock are\nrespectively driven by Cox-Ingersoll-Ross model and Heston model with\nsimultaneous L\\'{e}vy jumps. By using the equilibrium framework, we obtain the\npricing kernel and the equivalent martingale measure. Moreover, under the\nforward measure instead of the risk neural measure, we give the closed-form\nsolution for the fair delivery price of the discretely sampled variance swap by\nemploying the joint moment generating function of the underlying processes.\nFinally, we provide some numerical examples to depict that the values of\nvariance swaps not only depend on the stochastic interest rates but also\nincrease in the presence of jump risks.\n"
    },
    {
        "paper_id": 1712.10274,
        "authors": "Muhammad Mohsin Hakeem, Ken-ichi Suzuki",
        "title": "Foreign Portfolio Investment and Economy: The Network Perspective",
        "comments": null,
        "journal-ref": "International Journal of Economics, Finance and Business\n  Management Studies, Vol. 3, No. 1, pp. 15-26, April 2017",
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The European Union and Eurozone present an inquisitive case of strongly\ninterconnected network with high degree of dependence among nodes. This\nresearch focused on investment network of European Union and its major trading\npartners for specific time period 2001 to 2014. The changing investment\npatterns within Eurozone suggest strong financial and trade links with central\nand large economies. This study is about the association between portfolio\ninvestment and economic indicators with respect to financial networks. The\nanalysis used the strongly connected investment network of Eurozone and its\nlarge trading partners. A strong correlation between, increasing or decreasing\ninvestment patterns with economic indicators of particular economy was found.\nInterestingly correlation patterns for network members other than Eurozone\nstates were not as strong and depicted mild behavior. This as well, explains\nthe significance of interconnectedness level among nodes of one network with\nvarying centrality measures. Investment network visualization techniques helped\nto validate the results based on network`s statistical measures.\n"
    },
    {
        "paper_id": 1712.10287,
        "authors": "Reginald D. Smith",
        "title": "Bitcoin Average Dormancy: A Measure of Turnover and Trading Activity",
        "comments": "12 pages, 5 figures; accepted and to appear in Ledger",
        "journal-ref": "Ledger, 3, 91-99 (2018)",
        "doi": "10.5195/ledger.2018.99",
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Attempts to accurately measure the monetary velocity or related properties of\nbitcoin used in transactions have often attempted to either directly apply\ndefinitions from traditional macroeconomic theory or to use specialized metrics\nrelative to the properties of the Blockchain like bitcoin days destroyed. In\nthis paper, it is demonstrated that beyond being a useful metric, bitcoin days\ndestroyed has mathematical properties that allow you to calculate the average\ndormancy (time since last use in a transaction) of the bitcoins used in\ntransactions over a given time period. In addition, bitcoin days destroyed is\nshown to have another unexpected significance as the average size of the pool\nof traded bitcoins by virtue of the expression Little's Law, though only under\nlimited conditions.\n"
    },
    {
        "paper_id": 1801.00058,
        "authors": "Anibal Galindro, Delfim F. M. Torres",
        "title": "A simple mathematical model for unemployment: a case study in Portugal\n  with optimal control",
        "comments": "This is a preprint of a paper whose final and definite form will\n  appear in 'Statistics, Optimization and Information Computing' (see\n  [http://www.IAPress.org]). Submitted 27-Oct-2017; revised 19-Dec-2017;\n  accepted 29-Dec-2017",
        "journal-ref": "Stat. Optim. Inf. Comput. 6 (2018), no. 1, 116--129",
        "doi": "10.19139/soic.v6i1.470",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We propose a simple mathematical model for unemployment. Despite its\nsimpleness, we claim that the model is more realistic and useful than recent\nmodels available in the literature. A case study with real data from Portugal\nsupports our claim. An optimal control problem is formulated and solved, which\nprovides some non-trivial and interesting conclusions.\n"
    },
    {
        "paper_id": 1801.00091,
        "authors": "Raeid Saqur and Nicole Langballe",
        "title": "PrivySense: $\\underline{Pri}$ce $\\underline{V}$olatilit$\\underline{y}$\n  based $\\underline{Sen}$timent$\\underline{s}$ $\\underline{E}$stimation from\n  Financial News using Machine Learning",
        "comments": "Initial draft, updates are w.i.p",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  As machine learning ascends the peak of computer science zeitgeist, the usage\nand experimentation with sentiment analysis using various forms of textual data\nseems pervasive. The effect is especially pronounced in formulating securities\ntrading strategies, due to a plethora of reasons including the relative ease of\nimplementation and the abundance of academic research suggesting automated\nsentiment analysis can be productively used in trading strategies. The source\ndata for such analyzers ranges a broad spectrum like social media feeds,\nmicro-blogs, real-time news feeds, ex-post financial data etc. The abstract\ntechnique underlying these analyzers involve supervised learning of sentiment\nclassification where the classifier is trained on annotated source corpus, and\naccuracy is measured by testing how well the classifiers generalizes on unseen\ntest data from the corpus. Post training, and validation of fitted models, the\nclassifiers are used to execute trading strategies, and the corresponding\nreturns are compared with appropriate benchmark returns (for e.g., the S&P500\nreturns).\n  In this paper, we introduce $\\underline{a\\ novel\\ technique\\ of\\ using\\\nprice\\ volatilities\\ to\\ empirically\\ determine\\ the\\ sentiment\\ in\\ news\\\ndata}$, instead of the traditional reverse approach. We also perform meta\nsentiment analysis by evaluating the efficacy of existing sentiment classifiers\nand the precise definition of sentiment from securities trading context. We\nscrutinize the efficacy of using human-annotated sentiment classification and\nthe tacit assumptions that introduces subjective bias in existing financial\nnews sentiment classifiers.\n"
    },
    {
        "paper_id": 1801.00185,
        "authors": "Piero Mazzarisi, Paolo Barucca, Fabrizio Lillo, Daniele Tantari",
        "title": "A dynamic network model with persistent links and node-specific latent\n  variables, with an application to the interbank market",
        "comments": "19 pages, 6 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We propose a dynamic network model where two mechanisms control the\nprobability of a link between two nodes: (i) the existence or absence of this\nlink in the past, and (ii) node-specific latent variables (dynamic fitnesses)\ndescribing the propensity of each node to create links. Assuming a Markov\ndynamics for both mechanisms, we propose an Expectation-Maximization algorithm\nfor model estimation and inference of the latent variables. The estimated\nparameters and fitnesses can be used to forecast the presence of a link in the\nfuture. We apply our methodology to the e-MID interbank network for which the\ntwo linkage mechanisms are associated with two different trading behaviors in\nthe process of network formation, namely preferential trading and trading\ndriven by node-specific characteristics. The empirical results allow to\nrecognise preferential lending in the interbank market and indicate how a\nmethod that does not account for time-varying network topologies tends to\noverestimate preferential linkage.\n"
    },
    {
        "paper_id": 1801.00253,
        "authors": "Kiran Sharma, Subhradeep Das and Anirban Chakraborti",
        "title": "Global Income Inequality and Savings: A Data Science Perspective",
        "comments": "8 pages, 6 figures. IEEE format. Accepted for publication in 5th IEEE\n  DSAA 2018 conference at Torino, Italy",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  A society or country with income equally distributed among its people is\ntruly a fiction! The phenomena of socioeconomic inequalities have been plaguing\nmankind from times immemorial. We are interested in gaining an insight about\nthe co-evolution of the countries in the inequality space, from a data science\nperspective. For this purpose, we use the time series data for Gini indices of\ndifferent countries, and construct the equal-time cross-correlation matrix. We\nthen use this to construct a similarity matrix and generate a map with the\ncountries as different points generated through a multi-dimensional scaling\ntechnique. We also produce a similar map of different countries using the time\nseries data for Gross Domestic Savings (% of GDP). We also pose a different,\nyet significant, question: Can higher savings moderate the income inequality?\nIn this paper, we have tried to address this question through another data\nscience technique - linear regression, to seek an empirical linkage between the\nincome inequality and savings, mainly for relatively small or closed economies.\nThis question was inspired from an existing theoretical model proposed by\nChakraborti-Chakrabarti (2000), based on the principle of kinetic theory of\ngases. We tested our model empirically using Gini index and Gross Domestic\nSavings, and observed that the model holds reasonably true for many economies\nof the world.\n"
    },
    {
        "paper_id": 1801.00266,
        "authors": "Marzia De Donno, Zbigniew Palmowski, and Joanna Tumilewicz",
        "title": "Double continuation regions for American and Swing options with negative\n  discount rate in L\\'evy models",
        "comments": "arXiv admin note: text overlap with arXiv:1505.07313 by other authors",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper we study perpetual American call and put options in an\nexponential L\\'evy model. We consider a negative effective discount rate which\narises in a number of financial applications including stock loans and real\noptions, where the strike price can potentially grow at a higher rate than the\noriginal discount factor. We show that in this case a double continuation\nregion arises and we identify the two critical prices. We also generalize this\nresult to multiple stopping problems of Swing type, that is, when successive\nexercise opportunities are separated by i.i.d. random refraction times. We\nconduct an extensive numerical analysis for the Black-Scholes model and the\njump-diffusion model with exponentially distributed jumps.\n"
    },
    {
        "paper_id": 1801.00362,
        "authors": "Vadim Kaushansky, Alexander Lipton, Christoph Reisinger",
        "title": "Transition probability of Brownian motion in the octant and its\n  application to default modeling",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We derive a semi-analytic formula for the transition probability of\nthree-dimensional Brownian motion in the positive octant with absorption at the\nboundaries. Separation of variables in spherical coordinates leads to an\neigenvalue problem for the resulting boundary value problem in the two angular\ncomponents. The main theoretical result is a solution to the original problem\nexpressed as an expansion into special functions and an eigenvalue which has to\nbe chosen to allow a matching of the boundary condition. We discuss and test\nseveral computational methods to solve a finite-dimensional approximation to\nthis nonlinear eigenvalue problem. Finally, we apply our results to the\ncomputation of default probabilities and credit valuation adjustments in a\nstructural credit model with mutual liabilities.\n"
    },
    {
        "paper_id": 1801.00372,
        "authors": "Tim Leung, Jiao Li, Xin Li",
        "title": "Optimal Timing to Trade Along a Randomized Brownian Bridge",
        "comments": "25 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper studies an optimal trading problem that incorporates the trader's\nmarket view on the terminal asset price distribution and uninformative noise\nembedded in the asset price dynamics. We model the underlying asset price\nevolution by an exponential randomized Brownian bridge (rBb) and consider\nvarious prior distributions for the random endpoint. We solve for the optimal\nstrategies to sell a stock, call, or put, and analyze the associated delayed\nliquidation premia. We solve for the optimal trading strategies numerically and\ncompare them across different prior beliefs. Among our results, we find that\ndisconnected continuation/exercise regions arise when the trader prescribe a\ntwo-point discrete distribution and double exponential distribution.\n"
    },
    {
        "paper_id": 1801.00588,
        "authors": "Xi Zhang, Yunjia Zhang, Senzhang Wang, Yuntao Yao, Binxing Fang,\n  Philip S. Yu",
        "title": "Improving Stock Market Prediction via Heterogeneous Information Fusion",
        "comments": "Accepted by Knowledge-Based Systems",
        "journal-ref": null,
        "doi": "10.1016/j.knosys.2017.12.025",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Traditional stock market prediction approaches commonly utilize the\nhistorical price-related data of the stocks to forecast their future trends. As\nthe Web information grows, recently some works try to explore financial news to\nimprove the prediction. Effective indicators, e.g., the events related to the\nstocks and the people's sentiments towards the market and stocks, have been\nproved to play important roles in the stocks' volatility, and are extracted to\nfeed into the prediction models for improving the prediction accuracy. However,\na major limitation of previous methods is that the indicators are obtained from\nonly a single source whose reliability might be low, or from several data\nsources but their interactions and correlations among the multi-sourced data\nare largely ignored.\n  In this work, we extract the events from Web news and the users' sentiments\nfrom social media, and investigate their joint impacts on the stock price\nmovements via a coupled matrix and tensor factorization framework.\nSpecifically, a tensor is firstly constructed to fuse heterogeneous data and\ncapture the intrinsic relations among the events and the investors' sentiments.\nDue to the sparsity of the tensor, two auxiliary matrices, the stock\nquantitative feature matrix and the stock correlation matrix, are constructed\nand incorporated to assist the tensor decomposition. The intuition behind is\nthat stocks that are highly correlated with each other tend to be affected by\nthe same event. Thus, instead of conducting each stock prediction task\nseparately and independently, we predict multiple correlated stocks\nsimultaneously through their commonalities, which are enabled via sharing the\ncollaboratively factorized low rank matrices between matrices and the tensor.\nEvaluations on the China A-share stock data and the HK stock data in the year\n2015 demonstrate the effectiveness of the proposed model.\n"
    },
    {
        "paper_id": 1801.00597,
        "authors": "Xi Zhang, Jiawei Shi, Di Wang, Binxing Fang",
        "title": "Exploiting Investors Social Network for Stock Prediction in China's\n  Market",
        "comments": "accepted by Journal of Computational Science",
        "journal-ref": null,
        "doi": "10.1016/j.jocs.2017.10.013",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Recent works have shown that social media platforms are able to influence the\ntrends of stock price movements. However, existing works have majorly focused\non the U.S. stock market and lacked attention to certain emerging countries\nsuch as China, where retail investors dominate the market. In this regard, as\nretail investors are prone to be influenced by news or other social media,\npsychological and behavioral features extracted from social media platforms are\nthought to well predict stock price movements in the China's market. Recent\nadvances in the investor social network in China enables the extraction of such\nfeatures from web-scale data. In this paper, on the basis of tweets from\nXueqiu, a popular Chinese Twitter-like social platform specialized for\ninvestors, we analyze features with regard to collective sentiment and\nperception on stock relatedness and predict stock price movements by employing\nnonlinear models. The features of interest prove to be effective in our\nexperiments.\n"
    },
    {
        "paper_id": 1801.00681,
        "authors": "Shuheng Wang, Guohao Li, Yifan Bao",
        "title": "A novel improved fuzzy support vector machine based stock price trend\n  forecast model",
        "comments": "This paper is accepted by the International Conference on Innovations\n  in Economic Management and Social Science (IEMSS 2017) and International\n  Conference on Humanities, Management Engineering and Education Technology\n  (HMEET 2017)",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Application of fuzzy support vector machine in stock price forecast. Support\nvector machine is a new type of machine learning method proposed in 1990s. It\ncan deal with classification and regression problems very successfully. Due to\nthe excellent learning performance of support vector machine, the technology\nhas become a hot research topic in the field of machine learning, and it has\nbeen successfully applied in many fields. However, as a new technology, there\nare many limitations to support vector machines. There is a large amount of\nfuzzy information in the objective world. If the training of support vector\nmachine contains noise and fuzzy information, the performance of the support\nvector machine will become very weak and powerless. As the complexity of many\nfactors influence the stock price prediction, the prediction results of\ntraditional support vector machine cannot meet people with precision, this\nstudy improved the traditional support vector machine fuzzy prediction\nalgorithm is proposed to improve the new model precision. NASDAQ Stock Market,\nStandard & Poor's (S&P) Stock market are considered. Novel advanced- fuzzy\nsupport vector machine (NA-FSVM) is the proposed methodology.\n"
    },
    {
        "paper_id": 1801.0098,
        "authors": "Ale\\v{s} \\v{C}ern\\'y and Igor Melicher\\v{c}\\'ik",
        "title": "Simple Explicit Formula for Near-Optimal Stochastic Lifestyling",
        "comments": null,
        "journal-ref": "European Journal of Operational Research 284(2), 769-778, 2020",
        "doi": "10.1016/j.ejor.2019.12.032",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In life-cycle economics the Samuelson paradigm (Samuelson, 1969) states that\nthe optimal investment is in constant proportions out of lifetime wealth\ncomposed of current savings and the present value of future income. It is well\nknown that in the presence of credit constraints this paradigm no longer\napplies. Instead, optimal lifecycle investment gives rise to so-called\nstochastic lifestyling (Cairns et al., 2006), whereby for low levels of\naccumulated capital it is optimal to invest fully in stocks and then gradually\nswitch to safer assets as the level of savings increases. In stochastic\nlifestyling not only does the ratio between risky and safe assets change but\nalso the mix of risky assets varies over time. While the existing literature\nrelies on complex numerical algorithms to quantify optimal lifestyling the\npresent paper provides a simple formula that captures the main essence of the\nlifestyling effect with remarkable accuracy.\n"
    },
    {
        "paper_id": 1801.01093,
        "authors": "Angelica Gianfreda and Francesco Ravazzolo and Luca Rossini",
        "title": "Comparing the Forecasting Performances of Linear Models for Electricity\n  Prices with High RES Penetration",
        "comments": "Forthcoming in \"International Journal of Forecasting\"",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper compares alternative univariate versus multivariate models,\nfrequentist versus Bayesian autoregressive and vector autoregressive\nspecifications, for hourly day-ahead electricity prices, both with and without\nrenewable energy sources. The accuracy of point and density forecasts are\ninspected in four main European markets (Germany, Denmark, Italy and Spain)\ncharacterized by different levels of renewable energy power generation. Our\nresults show that the Bayesian VAR specifications with exogenous variables\ndominate other multivariate and univariate specifications, in terms of both\npoint and density forecasting.\n"
    },
    {
        "paper_id": 1801.01205,
        "authors": "Julien Hok, Philip Ngare, Antonis Papapantoleon",
        "title": "Expansion formulas for European quanto options in a local volatility\n  FX-LIBOR model",
        "comments": "33 pages, 7 figures, forthcoming in IJTAF",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We develop an expansion approach for the pricing of European quanto options\nwritten on LIBOR rates (of a foreign currency). We derive the dynamics of the\nsystem of foreign LIBOR rates under the domestic forward measure and then\nconsider the price of the quanto option. In order to take the skew/smile effect\nobserved in fixed income and FX markets into account, we consider local\nvolatility models for both the LIBOR and the FX rate. Because of the structure\nof the local volatility function, a closed form solution for quanto option\nprices does not exist. Using expansions around a proxy related to log-normal\ndynamics, we derive approximation formulas of Black--Scholes type for the\nprice, that have the benefit of giving very rapid numerical procedures. Our\nexpansion formulas have the major advantage that they allow for an accurate\nestimation of the error, using Malliavin calculus, which is directly related to\nthe maturity of the option, the payoff, and the level and curvature of the\nlocal volatility function. These expansions also illustrate the impact of the\nquanto drift adjustment, while the numerical experiments show an excellent\naccuracy.\n"
    },
    {
        "paper_id": 1801.01243,
        "authors": "Johan Dahlin, Adrian Wills, Brett Ninness",
        "title": "Constructing Metropolis-Hastings proposals using damped BFGS updates",
        "comments": "16 pages, 2 figures. Accepted for publication in the Proceedings of\n  the 18th IFAC Symposium on System Identification (SYSID)",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The computation of Bayesian estimates of system parameters and functions of\nthem on the basis of observed system performance data is a common problem\nwithin system identification. This is a previously studied issue where\nstochastic simulation approaches have been examined using the popular\nMetropolis--Hastings (MH) algorithm. This prior study has identified a\nrecognised difficulty of tuning the {proposal distribution so that the MH\nmethod provides realisations with sufficient mixing to deliver efficient\nconvergence. This paper proposes and empirically examines a method of tuning\nthe proposal using ideas borrowed from the numerical optimisation literature\naround efficient computation of Hessians so that gradient and curvature\ninformation of the target posterior can be incorporated in the proposal.\n"
    },
    {
        "paper_id": 1801.01777,
        "authors": "Masaya Abe, Hideki Nakayama",
        "title": "Deep Learning for Forecasting Stock Returns in the Cross-Section",
        "comments": "12 pages, 2 figures, 8 tables, accepted at PAKDD 2018",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Many studies have been undertaken by using machine learning techniques,\nincluding neural networks, to predict stock returns. Recently, a method known\nas deep learning, which achieves high performance mainly in image recognition\nand speech recognition, has attracted attention in the machine learning field.\nThis paper implements deep learning to predict one-month-ahead stock returns in\nthe cross-section in the Japanese stock market and investigates the performance\nof the method. Our results show that deep neural networks generally outperform\nshallow neural networks, and the best networks also outperform representative\nmachine learning models. These results indicate that deep learning shows\npromise as a skillful machine learning method to predict stock returns in the\ncross-section.\n"
    },
    {
        "paper_id": 1801.01811,
        "authors": "Torsten Trimborn, Philipp Otte, Simon Cramer, Max Beikirch, Emma\n  Pabich, Martin Frank",
        "title": "SABCEMM-A Simulator for Agent-Based Computational Economic Market Models",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We introduce the simulation tool SABCEMM (Simulator for Agent-Based\nComputational Economic Market Models) for agent-based computational economic\nmarket (ABCEM) models. Our simulation tool is implemented in C++ and we can\neasily run ABCEM models with several million agents. The object-oriented\nsoftware design enables the isolated implementation of building blocks for\nABCEM models, such as agent types and market mechanisms. The user can design\nand compare ABCEM models in a unified environment by recombining existing\nbuilding blocks using the XML-based SABCEMM configuration file. We introduce an\nabstract ABCEM model class which our simulation tool is built upon.\nFurthermore, we present the software architecture as well as computational\naspects of SABCEMM. Here, we focus on the efficiency of SABCEMM with respect to\nthe run time of our simulations. We show the great impact of different random\nnumber generators on the run time of ABCEM models. The code and documentation\nis published on GitHub at https://github.com/SABCEMM/SABCEMM, such that all\nresults can be reproduced by the reader.\n"
    },
    {
        "paper_id": 1801.01948,
        "authors": "Steven D. Moffitt",
        "title": "Why Markets are Inefficient: A Gambling \"Theory\" of Financial Markets\n  For Practitioners and Theorists",
        "comments": "31 pages, 6 figures, available also on SSRN",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The purpose of this article is to propose a new \"theory,\" the Strategic\nAnalysis of Financial Markets (SAFM) theory, that explains the operation of\nfinancial markets using the analytical perspective of an enlightened gambler.\nThe gambler understands that all opportunities for superior performance arise\nfrom suboptimal decisions by humans, but understands also that knowledge of\nhuman decision making alone is not enough to understand market behavior --- one\nmust still model how those decisions lead to market prices. Thus are there\nthree parts to the model: gambling theory, human decision making, and strategic\nproblem solving. A new theory is necessary because at this writing in 2017,\nthere is no theory of financial markets acceptable to both practitioners and\ntheorists. Theorists' efficient market theory, for example, cannot explain\nbubbles and crashes nor the exceptional returns of famous investors and\nspeculators such as Warren Buffett and George Soros. At the same time, a new\ntheory must be sufficiently quantitative, explain market \"anomalies\" and\nprovide predictions in order to satisfy theorists. It is hoped that the SAFM\nframework will meet these requirements.\n"
    },
    {
        "paper_id": 1801.02091,
        "authors": "Tathagata Banerjee, Alex Bernstein, Zachary Feinstein",
        "title": "Dynamic Clearing and Contagion in Financial Networks",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper we introduce a generalized extension of the Eisenberg-Noe model\nof financial contagion to allow for time dynamics of the interbank liabilities,\nincluding a dynamic examination of default risk. This framework separates the\ncash account and long-term capital account to more accurately model the health\nof a financial institution. In doing so, such a system allows us to distinguish\nbetween delinquency and default as well as between defaults resulting from\neither insolvency or illiquidity.\n"
    },
    {
        "paper_id": 1801.02205,
        "authors": "Danilo Delpini, Stefano Battiston, Guido Caldarelli, Massimo Riccaboni",
        "title": "The Network of U.S. Mutual Fund Investments: Diversification, Similarity\n  and Fragility throughout the Global Financial Crisis",
        "comments": "27 pages, 6 figures, 1 table",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Network theory proved recently to be useful in the quantification of many\nproperties of financial systems. The analysis of the structure of investment\nportfolios is a major application since their eventual correlation and overlap\nimpact the actual risk diversification by individual investors. We investigate\nthe bipartite network of US mutual fund portfolios and their assets. We follow\nits evolution during the Global Financial Crisis and analyse the interplay\nbetween diversification, as understood in classical portfolio theory, and\nsimilarity of the investments of different funds. We show that, on average,\nportfolios have become more diversified and less similar during the crisis.\nHowever, we also find that large overlap is far more likely than expected from\nmodels of random allocation of investments. This indicates the existence of\nstrong correlations between fund portfolio strategies. We introduce a\nsimplified model of propagation of financial shocks, that we exploit to show\nthat a systemic risk component origins from the similarity of portfolios. The\nnetwork is still vulnerable after crisis because of this effect, despite the\nincrease in the diversification of portfolios. Our results indicate that\ndiversification may even increase systemic risk when funds diversify in the\nsame way. Diversification and similarity can play antagonistic roles and the\ntrade-off between the two should be taken into account to properly assess\nsystemic risk.\n"
    },
    {
        "paper_id": 1801.02422,
        "authors": "Pengyu Zhu",
        "title": "A quantitative approach to choose among multiple mutually exclusive\n  decisions: comparative expected utility theory",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Mutually exclusive decisions have been studied for decades. Many well-known\ndecision theories have been defined to help people either to make rational\ndecisions or to interpret people's behaviors, such as expected utility theory,\nregret theory, prospect theory, and so on. The paper argues that none of these\ndecision theories are designed to provide practical, normative and quantitative\napproaches for multiple mutually exclusive decisions. Different decision-makers\nshould naturally make different choices for the same decision question, as they\nhave different understandings and feelings on the same possible outcomes.The\nauthor tries to capture the different understandings and feelings from\ndifferent decision-makers, and model them into a quantitative decision\nevaluation process, which everyone could benefit from. The basic elements in\nclassic expected utility theory are kept in the new decision theory, but the\ninfluences from mutually exclusive decisions will also be modeled into the\nevaluation process. This may sound like regret theory, but the new approach is\ndesigned to fit multiple mutually exclusive decision scenarios, and it does not\nrequire a definition of probability weighting function. The new theory is\ndesigned to be simple and straightforward to use, and the results are expected\nto be rational for each decision-maker.\n"
    },
    {
        "paper_id": 1801.02444,
        "authors": "R. Simon, S. Spiez, H. Torunczyk",
        "title": "Games of Incomplete Information and Myopic Equilibria",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  A new concept of an equilibrium in games is introduced that solves an open\nquestion posed by A. Neyman.\n"
    },
    {
        "paper_id": 1801.02681,
        "authors": "Mercedes Campi, Marco Due\\~nas, Le Li, Huabin Wu",
        "title": "Diversification, economies of scope, and exports growth of Chinese firms",
        "comments": "22 pages, 4 figures, 3 tables",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In the 1990s, China started a process of structural reforms and of trade\nliberalization, which was followed by the accession to the World Trade\nOrganization (WTO) in 2001. In this paper, we analyze trade patterns of Chinese\nfirms for the period 2000-2006, characterized by a notable increase in exports\nvolumes. Theoretically, in a more open economy, firms are expected to move from\nthe production of a set of less-competitive products towards more\ninternationally competitive ones, which implies specialization. We study\nseveral stylized facts on the distribution of Chinese firms trade and growth\nrates, and we analyze whether firms have diversified or specialized their trade\npatterns between 2000 and 2006. We show that Chinese export patterns are very\nheterogeneous, that the volatility of growth rates depends on the level of\nexports, and that volatility is stronger after trade liberalization. Both,\ndiversification in products and destinations have a positive impact on trade\ngrowth, but diversification of destinations has a stronger effect. We conclude\nthat the success of Chinese exports is not only due to an increase in the\nintensive margin, related to the existence of economies of scale, but also due\nto an increase in the extensive margin, related to the existence of economies\nof scope.\n"
    },
    {
        "paper_id": 1801.02719,
        "authors": "Blanka Horvath and Oleg Reichmann",
        "title": "Dirichlet Forms and Finite Element Methods for the SABR Model",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We propose a deterministic numerical method for pricing vanilla options under\nthe SABR stochastic volatility model, based on a finite element discretization\nof the Kolmogorov pricing equations via non-symmetric Dirichlet forms. Our\npricing method is valid under mild assumptions on parameter configurations of\nthe process both in moderate interest rate environments and in near-zero\ninterest rate regimes such as the currently prevalent ones. The parabolic\nKolmogorov pricing equations for the SABR model are degenerate at the origin,\nyielding non-standard partial differential equations, for which conventional\npricing methods ---designed for non-degenerate parabolic equations---\npotentially break down. We derive here the appropriate analytic setup to handle\nthe degeneracy of the model at the origin. That is, we construct an evolution\ntriple of suitably chosen Sobolev spaces with singular weights, consisting of\nthe domain of the SABR-Dirichlet form, its dual space, and the pivotal Hilbert\nspace. In particular, we show well-posedness of the variational formulation of\nthe SABR-pricing equations for vanilla and barrier options on this triple.\nFurthermore, we present a finite element discretization scheme based on a\n(weighted) multiresolution wavelet approximation in space and a $\\theta$-scheme\nin time and provide an error analysis for this discretization.\n"
    },
    {
        "paper_id": 1801.02935,
        "authors": "Jonas Crevecoeur, Katrien Antonio and Roel Verbelen",
        "title": "Modeling the number of hidden events subject to observation delay",
        "comments": null,
        "journal-ref": null,
        "doi": "10.1016/j.ejor.2019.02.044",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper considers the problem of predicting the number of events that have\noccurred in the past, but which are not yet observed due to a delay. Such\ndelayed events are relevant in predicting the future cost of warranties,\npricing maintenance contracts, determining the number of unreported claims in\ninsurance and in modeling the outbreak of diseases. Disregarding these\nunobserved events results in a systematic underestimation of the event\noccurrence process. Our approach puts emphasis on modeling the time between the\noccurrence and observation of the event, the so-called observation delay. We\npropose a granular model for the heterogeneity in this observation delay based\non the occurrence day of the event and on calendar day effects in the\nobservation process, such as weekday and holiday effects. We illustrate this\napproach on a European general liability insurance data set where the\noccurrence of an accident is reported to the insurer with delay.\n"
    },
    {
        "paper_id": 1801.02994,
        "authors": "Steven D. Moffitt",
        "title": "On a Constructive Theory of Markets",
        "comments": "21 pages, 2 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This article is a prologue to the article \"Why Markets are Inefficient: A\nGambling 'Theory' of Financial Markets for Practitioners and Theorists.\" It\npresents important background for that article --- why gambling is important,\neven necessary, for real-world traders --- the reason for the superiority of\nthe strategic/gambling approach to the competing market ideologies of market\nfundamentalism and the scientific approach --- and its potential to uncover\nprofitable trading systems. Much of this article was drawn from Chapter 1 of\nthe book \"The Strategic Analysis of Financial Markets (in 2 volumes)\" World\nScientific, 2017.\n"
    },
    {
        "paper_id": 1801.03018,
        "authors": "Yun-Cheng Tsai, Jun-Hao Chen, Jun-Jie Wang",
        "title": "Predict Forex Trend via Convolutional Neural Networks",
        "comments": "30 pages, 41 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Deep learning is an effective approach to solving image recognition problems.\nPeople draw intuitive conclusions from trading charts; this study uses the\ncharacteristics of deep learning to train computers in imitating this kind of\nintuition in the context of trading charts. The three steps involved are as\nfollows: 1. Before training, we pre-process the input data from quantitative\ndata to images. 2. We use a convolutional neural network (CNN), a type of deep\nlearning, to train our trading model. 3. We evaluate the model's performance in\nterms of the accuracy of classification. A trading model is obtained with this\napproach to help devise trading strategies. The main application is designed to\nhelp clients automatically obtain personalized trading strategies.\n"
    },
    {
        "paper_id": 1801.0305,
        "authors": "V\\'ictor Gallego, Pablo Su\\'arez-Garc\\'ia, Pablo Angulo, David\n  G\\'omez-Ullate",
        "title": "Assessing the effect of advertising expenditures upon sales: a Bayesian\n  structural time series model",
        "comments": "Published at Applied Stochastic Models in Business and Industry,\n  https://onlinelibrary.wiley.com/doi/full/10.1002/asmb.2460",
        "journal-ref": "Appl Stochastic Models Bus Ind. 2019; 1-13",
        "doi": "10.1002/asmb.2460",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We propose a robust implementation of the Nerlove--Arrow model using a\nBayesian structural time series model to explain the relationship between\nadvertising expenditures of a country-wide fast-food franchise network with its\nweekly sales. Thanks to the flexibility and modularity of the model, it is well\nsuited to generalization to other markets or situations. Its Bayesian nature\nfacilitates incorporating \\emph{a priori} information (the manager's views),\nwhich can be updated with relevant data. This aspect of the model will be used\nto present a strategy of budget scheduling across time and channels.\n"
    },
    {
        "paper_id": 1801.03523,
        "authors": "Fernando Fernandes Neto",
        "title": "Generative Models for Stochastic Processes Using Convolutional Neural\n  Networks",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The present paper aims to demonstrate the usage of Convolutional Neural\nNetworks as a generative model for stochastic processes, enabling researchers\nfrom a wide range of fields (such as quantitative finance and physics) to\ndevelop a general tool for forecasts and simulations without the need to\nidentify/assume a specific system structure or estimate its parameters.\n"
    },
    {
        "paper_id": 1801.03574,
        "authors": "Matteo Burzoni and Mario Sikic",
        "title": "Robust martingale selection problem and its connections to the\n  no-arbitrage theory",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We analyze the martingale selection problem of Rokhlin (2006) in a pointwise\n(robust) setting. We derive conditions for solvability of this problem and show\nhow it is related to the classical no-arbitrage deliberations. We obtain\nversions of the Fundamental Theorem of Asset Pricing in examples spanning\nfrictionless markets, models with proportional transaction costs and also\nmodels for illiquid markets. In all these examples, we also incorporate trading\nconstraints.\n"
    },
    {
        "paper_id": 1801.03678,
        "authors": "Tianhao Zhi (SYU), Zhongfei Li (SYU), Zhiqiang Jiang (ECUST), Lijian\n  Wei (SYU), Didier Sornette (ETH Zurich)",
        "title": "Is there a housing bubble in China",
        "comments": "24 pages, 3 figures, and 10 tables",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  There is a growing concern in recent years over the potential formation of\nbubbles in the Chinese real estate market. This paper aims to conduct a series\nof bubble diagnostic analysis over nine representative Chinese cities from two\naspects. First, we investigate whether the prices had been significantly\ndeviating from economic fundamentals by applying a standard Engle-Granger\ncointegration test. Second, we apply the Log-Periodic-Power-Law-Singularity\n(LPPLS) model to detect whether there is any evidence of unsustainable,\nself-reinforcing speculative behaviours amongst the price series. We propose\nthat, given the heterogeneity that exists amongst cities with different types\nof bubble signatures, it is vital to conduct bubble diagnostic tests and\nimplement relevant policies toward specific bubble characteristics, rather than\nenforcing one-that-fits-for-all type policy that does not take into account\nsuch heterogeneity.\n"
    },
    {
        "paper_id": 1801.0368,
        "authors": "Ole Peters and Alexander Adamou",
        "title": "The time interpretation of expected utility theory",
        "comments": "8 pages, 3 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Ergodicity economics is a new branch of economic theory that notes the\nconceptual difference between time averages and expectation values, which\ncoincide only for ergodic observables. It postulates that individual agents\nmaximise the time average growth rate of wealth, known widely as growth\noptimality. This contrasts with the dominant behavioural model in economics,\nexpected utility theory, in which agents maximise expectation values of changes\nin psychologically transformed wealth. Historically, growth optimality was\nexplored for additive and multiplicative gambles. Here we apply it to a general\nclass of wealth dynamics, extending the range of economic situations where it\nmay be used. Moreover, we show a correspondence between growth optimality and\nexpected utility theory, in which the ergodicity transformation in the former\nis identified as the utility function in the latter. This correspondence offers\na theoretical basis for choosing utility functions and predicts that wealth\ndynamics are strong determinants of risk preferences.\n"
    },
    {
        "paper_id": 1801.0372,
        "authors": "Olfa Draouil, Bernt {\\O}ksendal",
        "title": "Viable Insider Markets",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  We consider the problem of optimal inside portfolio $\\pi(t)$ in a financial\nmarket with a corresponding wealth process $X(t)=X^{\\pi}(t)$ modelled by\n\\begin{align}\\label{eq0.1} \\begin{cases}\ndX(t)&=\\pi(t)X(t)[\\alpha(t)dt+\\beta(t)dB(t)]; \\quad t\\in[0, T] X(0)&=x_0>0,\n\\end{cases} \\end{align} where $B(\\cdot)$ is a Brownian motion. We assume that\nthe insider at time $t$ has access to market information $\\varepsilon_t>0$\nunits ahead of time, in addition to the history of the market up to time $t$.\nThe problem is to find an insider portfolio $\\pi^{*}$ which maximizes the\nexpected logarithmic utility $J(\\pi)$ of the terminal wealth, i.e. such that\n$$\\sup_{\\pi}J(\\pi)= J(\\pi^{*}), \\text {where } J(\\pi)=\n\\mathbb{E}[\\log(X^{\\pi}(T))].$$ The insider market is called \\emph{viable} if\nthis value is finite. We study under what inside information flow $\\mathbb{H}$\nthe insider market is viable or not. For example, assume that for all $t<T$ the\ninsider knows the value of $B(t+\\epsilon_t)$, where $t + \\epsilon_t \\geq T$\nconverges monotonically to $T$ from above as $t$ goes to $T$ from below. Then\n(assuming that the insider has a perfect memory) at time $t$ she has the inside\ninformation $\\mathcal{H}_t$, consisting of the history $\\mathcal{F}_t$ of\n$B(s); 0 \\leq s \\leq t$ plus all the values of Brownian motion in the interval\n$[t+\\epsilon_t, \\epsilon_0]$, i.e. we have the enlarged filtration\n\\begin{equation}\\label{eq0.2} \\mathbb{H}=\\{\\mathcal{H}_t\\}_{t\\in[0.T]},\\quad\n\\mathcal{H}_t=\\mathcal{F}_t\\vee\\sigma(B(t+\\epsilon_t+r),0\\leq r \\leq\n\\epsilon_0-t-\\epsilon_t), \\forall t\\in [0,T]. \\end{equation} Using forward\nintegrals, Hida-Malliavin calculus and Donsker delta functionals we show that\nif $$\\int_0^T\\frac{1}{\\varepsilon_t}dt=\\infty,$$ then the insider market is not\nviable.\n"
    },
    {
        "paper_id": 1801.03873,
        "authors": "Libo Li",
        "title": "Characterisation of honest times and optional semimartingales of\n  class-($\\Sigma$)",
        "comments": "Key words and phrases. Random times, Honest times, Azema\n  supermartingale, Additive decomposition, Multiplicative decomposition,\n  Optional supermartingales, Ladlag processes",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Given a finite honest time, we first show that the associated Az\\'ema\noptional supermartingale can be expressed as the drawdown and the relative\ndrawdown of some local optional supermartingales with continuous running\nsupremum. The relative drawdown representation then allows us to provide a\ncharacterisation of finite honest times using a family of non-negative local\noptional supermartingales with continuous running supremum which converges to\nzero at infinity. Then we extend the notion of semimartingales of\nclass-$(\\Sigma)$ by allowing for jumps in its finite variation part of the\nsemimartingale decomposition. This enables one to establish the\nMadan-Roynette-Yor option pricing formula for a larger class of processes, and\nfinally, we apply the extended formula to the construction of finite honest\ntimes.\n"
    },
    {
        "paper_id": 1801.04045,
        "authors": "Jiro Akahori, Flavia Barsotti, Yuri Imamura",
        "title": "Asymptotic Static Hedge via Symmetrization",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper is a continuation of Akahori-Barsotti-Imamura (2017) and where the\nauthors i) showed that a payment at a random time, which we call timing risk,\nis decomposed into an integral of static positions of knock-in type barrier\noptions, ii) proposed an iteration of static hedge of a timing risk by\nregarding the hedging error by a static hedge strategy of Bowie-Carr type with\nrespect to a barrier option as a timing risk, and iii) showed that the error\nconverges to zero by infinitely many times of iteration under a condition on\nthe integrability of a relevant function. Even though many diffusion models\nincluding generic 1-dimensional ones satisfy the required condition, a\nconstruction of the iterated static hedge that is applicable to any uniformly\nelliptic diffusions is postponed to the present paper because of its\nmathematical difficulty. We solve the problem in this paper by relying on the\nsymmetrization, a technique first introduced in Imamura-Ishigaki-Okumura (2014)\nand generalized in Akahori-Imamura (2014), and also work on parametrix, a\nclassical technique from perturbation theory to construct a fundamental\nsolution of a partial differential equation. Due to a lack of continuity in the\ndiffusion coefficient, however, a careful study of the integrability of the\nrelevant functions is required. The long lines of proof itself could be a\ncontribution to the parametrix analysis.\n"
    },
    {
        "paper_id": 1801.0408,
        "authors": "N. Packham",
        "title": "Optimal contracts under competition when uncertainty from adverse\n  selection and moral hazard are present",
        "comments": null,
        "journal-ref": null,
        "doi": "10.1016/j.spl.2018.01.014",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In a continuous-time setting where a risk-averse agent controls the drift of\nan output process driven by a Brownian motion, optimal contracts are linear in\nthe terminal output; this result is well-known in a setting with moral hazard\nand -under stronger assumptions - adverse selection. We show that this result\ncontinues to hold when in addition reservation utilities are type-dependent.\nThis type of problem occurs in the study of optimal compensation problems\ninvolving competing principals.\n"
    },
    {
        "paper_id": 1801.04112,
        "authors": "Sebastian Bayer, Timo Dimitriadis",
        "title": "Regression Based Expected Shortfall Backtesting",
        "comments": null,
        "journal-ref": "Journal of Financial Econometrics (2020+, forthcoming)",
        "doi": "10.1093/jjfinec/nbaa013",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper introduces novel backtests for the risk measure Expected Shortfall\n(ES) following the testing idea of Mincer and Zarnowitz (1969). Estimating a\nregression framework for the ES stand-alone is infeasible, and thus, our tests\nare based on a joint regression for the Value at Risk and the ES, which allows\nfor different test specifications. These ES backtests are the first which\nsolely backtest the ES in the sense that they only require ES forecasts as\ninput parameters. As the tests are potentially subject to model\nmisspecification, we provide asymptotic theory under misspecification for the\nunderlying joint regression. We find that employing a misspecification robust\ncovariance estimator substantially improves the tests' performance. We compare\nour backtests to existing approaches and find that our tests outperform the\ncompetitors throughout all considered simulations. In an empirical\nillustration, we apply our backtests to ES forecasts for 200 stocks of the S&P\n500 index.\n"
    },
    {
        "paper_id": 1801.04218,
        "authors": "Alex Lamarche-Perrin (Phys-ENS), Andr\\'e Orl\\'ean (PSE), Pablo Jensen\n  (Phys-ENS)",
        "title": "Coexistence of several currencies in presence of increasing returns to\n  adoption",
        "comments": null,
        "journal-ref": null,
        "doi": "10.1016/j.physa.2017.12.117",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We present a simplistic model of the competition between different\ncurrencies. Each individual is free to choose the currency that minimizes his\ntransaction costs, which arise whenever his exchanging relations have chosen a\ndifferent currency. We show that competition between currencies does not\nnecessarily converge to the emergence of a single currency. For large systems,\nwe prove that two distinct communities using different currencies in the\ninitial state will remain forever in this fractionalized state.\n"
    },
    {
        "paper_id": 1801.04491,
        "authors": "Salvatore Federico, Mauro Rosestolato, Elisa Tacconi",
        "title": "Irreversible investment with fixed adjustment costs: a stochastic\n  impulse control approach",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We consider an optimal stochastic impulse control problem over an infinite\ntime horizon motivated by a model of irreversible investment choices with fixed\nadjustment costs. By employing techniques of viscosity solutions and relying on\nsemiconvexity arguments, we prove that the value function is a classical\nsolution to the associated quasi-variational inequality. This enables us to\ncharacterize the structure of the continuation and action regions and construct\nan optimal control. Finally, we focus on the linear case, discussing, by a\nnumerical analysis, the sensitivity of the solution with respect to the\nrelevant parameters of the problem.\n"
    },
    {
        "paper_id": 1801.04841,
        "authors": "Juan Jose Viquez, Alexander Campos, Jorge Loria, Luis Alfredo Mendoza,\n  Jorge Aurelio Viquez",
        "title": "Demographic Modeling Via 3-dimensional Markov Chains",
        "comments": "in Spanish",
        "journal-ref": "Revista de Matematica: Teoria y Aplicaciones - 2018",
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This article presents a new model for demographic simulation which can be\nused to forecast and estimate the number of people in pension funds\n(contributors and retirees) as well as workers in a public institution.\nFurthermore, the model introduces opportunities to quantify the financial ows\ncoming from future populations such as salaries, contributions, salary\nsupplements, employer contribution to savings/pensions, among others. The\nimplementation of this probabilistic model will be of great value in the\nactuarial toolbox, increasing the reliability of the estimations as well as\nallowing deeper demographic and financial analysis given the reach of the\nmodel. We introduce the mathematical model, its first moments, and how to\nadjust the required probabilities, showing at the end an example where the\nmodel was applied to a public institution with real data.\n"
    },
    {
        "paper_id": 1801.0491,
        "authors": "Matthieu Barbier and D.-S. Lee",
        "title": "Urn model for products' shares in international trade",
        "comments": "9 pages, 5 figures",
        "journal-ref": "J. Stat. Mech. (2017) 123403",
        "doi": "10.1088/1742-5468/aa9bb9",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  International trade fluxes evolve as countries revise their portfolios of\ntrade products towards economic development. Accordingly products' shares in\ninternational trade vary with time, reflecting the transfer of capital between\ndistinct industrial sectors. Here we analyze the share of hundreds of product\ncategories in world trade for four decades and find a scaling law obeyed by the\nannual variation of product share, which informs us of how capital flows and\ninteracts over the product space. A model of stochastic transfer of capital\nbetween products based on the observed scaling relation is proposed and shown\nto reproduce exactly the empirical share distribution. The model allows\nanalytic solutions as well as numerical simulations, which predict a\npseudo-condensation of capital onto few product categories and when it will\noccur. At the individual level, our model finds certain products unpredictable,\nthe excess or deficient growth of which with respect to the model prediction is\nshown to be correlated with the nature of goods.\n"
    },
    {
        "paper_id": 1801.04994,
        "authors": "Andrea Macrina and Obeid Mahomed",
        "title": "Consistent Valuation Across Curves Using Pricing Kernels",
        "comments": "56 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The general problem of asset pricing when the discount rate differs from the\nrate at which an asset's cash flows accrue is considered. A pricing kernel\nframework is used to model an economy that is segmented into distinct markets,\neach identified by a yield curve having its own market, credit and liquidity\nrisk characteristics. The proposed framework precludes arbitrage within each\nmarket, while the definition of a curve-conversion factor process links all\nmarkets in a consistent arbitrage-free manner. A pricing formula is then\nderived, referred to as the across-curve pricing formula, which enables\nconsistent valuation and hedging of financial instruments across curves (and\nmarkets). As a natural application, a consistent multi-curve framework is\nformulated for emerging and developed inter-bank swap markets, which highlights\nan important dual feature of the curve-conversion factor process. Given this\nmulti-curve framework, existing multi-curve approaches based on HJM and\nrational pricing kernel models are recovered, reviewed and generalised, and\nsingle-curve models extended. In another application, inflation-linked,\ncurrency-based, and fixed-income hybrid securities are shown to be consistently\nvalued using the across-curve valuation method.\n"
    },
    {
        "paper_id": 1801.05279,
        "authors": "Jos\\'e Moran, Jean-Philippe Bouchaud",
        "title": "Greedy algorithms and Zipf laws",
        "comments": null,
        "journal-ref": null,
        "doi": "10.1088/1742-5468/aab50a",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We consider a simple model of firm/city/etc. growth based on a multi-item\ncriterion: whenever entity B fares better that entity A on a subset of $M$\nitems out of $K$, the agent originally in A moves to B. We solve the model\nanalytically in the cases $K=1$ and $K \\to \\infty$. The resulting stationary\ndistribution of sizes is generically a Zipf-law provided $M > K/2$. When $M\n\\leq K/2$, no selection occurs and the size distribution remains thin-tailed.\nIn the special case $M=K$, one needs to regularise the problem by introducing a\nsmall \"default\" probability $\\phi$. We find that the stationary distribution\nhas a power-law tail that becomes a Zipf-law when $\\phi \\to 0$. The approach to\nthe stationary state can also been characterized, with strong similarities with\na simple \"aging\" model considered by Barrat & M\\'ezard.\n"
    },
    {
        "paper_id": 1801.05295,
        "authors": "Paolo Cremonesi and Chiara Francalanci and Alessandro Poli and Roberto\n  Pagano and Luca Mazzoni and Alberto Maggioni and Mehdi Elahi",
        "title": "Social Network based Short-Term Stock Trading System",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper proposes a novel adaptive algorithm for the automated short-term\ntrading of financial instrument. The algorithm adopts a semantic sentiment\nanalysis technique to inspect the Twitter posts and to use them to predict the\nbehaviour of the stock market. Indeed, the algorithm is specifically developed\nto take advantage of both the sentiment and the past values of a certain\nfinancial instrument in order to choose the best investment decision. This\nallows the algorithm to ensure the maximization of the obtainable profits by\ntrading on the stock market. We have conducted an investment simulation and\ncompared the performance of our proposed with a well-known benchmark (DJTATO\nindex) and the optimal results, in which an investor knows in advance the\nfuture price of a product. The result shows that our approach outperforms the\nbenchmark and achieves the performance score close to the optimal result.\n"
    },
    {
        "paper_id": 1801.05352,
        "authors": "Fl\\'avio L. Pinheiro, Aamena Alshamsi, Dominik Hartmann, Ron Boschma\n  and C\\'esar A. Hidalgo",
        "title": "Shooting High or Low: Do Countries Benefit from Entering Unrelated\n  Activities?",
        "comments": "43 pages, 13 figures, 2 appendices",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Countries tend to diversify their exports by entering products that are\nrelated to their current exports. Yet this average behavior is not\nrepresentative of every diversification path. In this paper, we introduce a\nmethod to identify periods when countries enter unrelated products. We analyze\nthe economic diversification paths of 93 countries between 1965 and 2014 and\nfind that countries enter unrelated products in only about 7.2% of all\nobservations. We find that countries enter more unrelated products when they\nare at an intermediate level of economic development, and when they have higher\nlevels of human capital. Finally, we ask whether countries entering more\nunrelated products grow faster than those entering only related products. The\ndata shows that countries that enter more unrelated activities experience a\nsmall but significant increase in future economic growth, compared to countries\nwith a similar level of income, human capital, capital stock per worker, and\neconomic complexity.\n"
    },
    {
        "paper_id": 1801.05409,
        "authors": "Quinn Culver, Dennis Heitmann, Christian Wei{\\ss}",
        "title": "The Influence of Seed Selection on the Solvency II Ratio",
        "comments": "2 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This article contains the first published example of a real economic balance\nsheet where the Solvency II ratio substantially depends on the seed selected\nfor the random number generator (RNG) used. The theoretical background and the\nmain quality criteria for RNGs are explained in detail. To serve as a gauge for\nRNGs, a definition of true randomness is given. Quality tests that RNGs should\npass in order to generate stable results when used in risk management under\nSolvency II are described.\n"
    },
    {
        "paper_id": 1801.05446,
        "authors": "Martin Falcke and V. Nicolai Friedhoff",
        "title": "The Stretch to Stray on Time: Resonant Length of Random Walks in a\n  Transient",
        "comments": null,
        "journal-ref": null,
        "doi": "10.1063/1.5023164",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  First-passage times in random walks have a vast number of diverse\napplications in physics, chemistry, biology, and finance. In general,\nenvironmental conditions for a stochastic process are not constant on the time\nscale of the average first-passage time, or control might be applied to reduce\nnoise. We investigate moments of the first-passage time distribution under a\ntransient describing relaxation of environmental conditions. We solve the\nLaplace-transformed (generalized) master equation analytically using a novel\nmethod that is applicable to general state schemes. The first-passage time from\none end to the other of a linear chain of states is our application for the\nsolutions. The dependence of its average on the relaxation rate obeys a power\nlaw for slow transients. The exponent $\\nu$ depends on the chain length $N$\nlike $\\nu=-N/(N+1)$ to leading order. Slow transients substantially reduce the\nnoise of first-passage times expressed as the coefficient of variation (CV),\neven if the average first-passage time is much longer than the transient. The\nCV has a pronounced minimum for some lengths, which we call resonant lengths.\nThese results also suggest a simple and efficient noise control strategy, and\nare closely related to the timing of repetitive excitations, coherence\nresonance and information transmission by noisy excitable systems. A resonant\nnumber of steps from the inhibited state to the excitation threshold and slow\nrecovery from negative feedback provide optimal timing noise reduction and\ninformation transmission.\n"
    },
    {
        "paper_id": 1801.05597,
        "authors": "Takuji Arai, Yuto Imai and Ryo Nakashima",
        "title": "Numerical analysis on quadratic hedging strategies for normal inverse\n  Gaussian models",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The authors aim to develop numerical schemes of the two representative\nquadratic hedging strategies: locally risk minimizing and mean-variance hedging\nstrategies, for models whose asset price process is given by the exponential of\na normal inverse Gaussian process, using the results of Arai et al. \\cite{AIS},\nand Arai and Imai. Here normal inverse Gaussian process is a framework of\nL\\'evy processes frequently appeared in financial literature. In addition, some\nnumerical results are also introduced.\n"
    },
    {
        "paper_id": 1801.05673,
        "authors": "Cheikh Mbaye and Fr\\'ed\\'eric Vrins",
        "title": "A subordinated CIR intensity model with application to Wrong-Way risk\n  CVA",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Credit Valuation Adjustment (CVA) pricing models need to be both flexible and\ntractable. The survival probability has to be known in closed form (for\ncalibration purposes), the model should be able to fit any valid Credit Default\nSwap (CDS) curve, should lead to large volatilities (in line with CDS options)\nand finally should be able to feature significant Wrong-Way Risk (WWR) impact.\nThe Cox-Ingersoll-Ross model (CIR) combined with independent positive jumps and\ndeterministic shift (JCIR++) is a very good candidate : the variance (and thus\ncovariance with exposure, i.e. WWR) can be increased with the jumps, whereas\nthe calibration constraint is achieved via the shift. In practice however,\nthere is a strong limit on the model parameters that can be chosen, and thus on\nthe resulting WWR impact. This is because only non-negative shifts are allowed\nfor consistency reasons, whereas the upwards jumps of the JCIR++ need to be\ncompensated by a downward shift. To limit this problem, we consider the\ntwo-side jump model recently introduced by Mendoza-Arriaga \\& Linetsky, built\nby time-changing CIR intensities. In a multivariate setup like CVA,\ntime-changing the intensity partly kills the potential correlation with the\nexposure process and destroys WWR impact. Moreover, it can introduce a forward\nlooking effect that can lead to arbitrage opportunities. In this paper, we use\nthe time-changed CIR process in a way that the above issues are avoided. We\nshow that the resulting process allows to introduce a large WWR effect compared\nto the JCIR++ model. The computation cost of the resulting Monte Carlo\nframework is reduced by using an adaptive control variate procedure.\n"
    },
    {
        "paper_id": 1801.05752,
        "authors": "Rilwan Adewoyin",
        "title": "Part 1: Training Sets & ASG Transforms",
        "comments": "This was an undergraduate project, subsequently the research was not\n  exhaustive",
        "journal-ref": null,
        "doi": "10.13140/RG.2.2.25313.81760",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper, I discuss a method to tackle the issues arising from the small\ndata-sets available to data-scientists when building price predictive\nalgorithms that use monthly/quarterly macro-financial indicators. I approach\nthis by training separate classifiers on the equivalent dataset from a range of\ncountries. Using these classifiers, a three level meta learning algorithm (MLA)\nis developed. I develop a transform, ASG, to create a country agnostic proxy\nfor the macro-financial indicators. Using these proposed methods, I investigate\nthe degree to which a predictive algorithm for the US 5Y bond price,\npredominantly using macro-financial indicators, can outperform an identical\nalgorithm which only uses statistics deriving from previous price.\n  This was an undergraduate project, subsequently the research was not\nexhaustive.\n"
    },
    {
        "paper_id": 1801.05759,
        "authors": "Christos Ellinas, Neil Allan, Caroline Coombe",
        "title": "Evaluating the role of risk networks on risk identification,\n  classification and emergence",
        "comments": "21 pages, 7 Figures, 4 tables, To appear in Journal of Network Theory\n  in Finance",
        "journal-ref": null,
        "doi": "10.21314/JNTF.2017.032",
        "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
        "abstract": "  Modern society heavily relies on strongly connected, socio-technical systems.\nAs a result, distinct risks threatening the operation of individual systems can\nno longer be treated in isolation. Consequently, risk experts are actively\nseeking for ways to relax the risk independence assumption that undermines\ntypical risk management models. Prominent work has advocated the use of risk\nnetworks as a way forward. Yet, the inevitable biases introduced during the\ngeneration of these survey-based risk networks limit our ability to examine\ntheir topology, and in turn challenge the utility of the very notion of a risk\nnetwork. To alleviate these concerns, we proposed an alternative methodology\nfor generating weighted risk networks. We subsequently applied this methodology\nto an empirical dataset of financial data. This paper reports our findings on\nthe study of the topology of the resulting risk network. We observed a modular\ntopology, and reasoned on its use as a robust risk classification framework.\nUsing these modules, we highlight a tendency of specialization during the risk\nidentification process, with some firms being solely focused on a subset of the\navailable risk classes. Finally, we considered the independent and systemic\nimpact of some risks and attributed possible mismatches to their emerging\nnature.\n"
    },
    {
        "paper_id": 1801.0576,
        "authors": "Zura Kakushadze and Jim Kyung-Soo Liew",
        "title": "CryptoRuble: From Russia with Love",
        "comments": "20 pages",
        "journal-ref": "Risk, January 2018, pp. 53-54; World Economics 19(4) (2018)\n  165-187",
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We discuss Russia's underlying motives for issuing its government-backed\ncryptocurrency, CryptoRuble, and the implications thereof and of other\nlikely-soon-forthcoming government-issued cryptocurrencies to some stakeholders\n(populace, governments, economy, finance, etc.), existing decentralized\ncryptocurrencies (such as Bitcoin and Ethereum), as well as the future of the\nworld monetary system (the role of the U.S. therein and a necessity for the\nU.S. to issue CryptoDollar), including a future algorithmic universal world\ncurrency that may also emerge. We further provide a comprehensive list of\nreferences on cryptocurrencies.\n"
    },
    {
        "paper_id": 1801.0577,
        "authors": "Anas Yassine (MSFGR), Abdelmadjid Ibenrissoul",
        "title": "The macroeconomics determinants of default of the borrowers: The case of\n  Moroccan bank",
        "comments": "in French",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This article aims to explore an empirical approach to analyze the\nmacroeconomicsdeterminants of default of borrowers. For this purpose, we have\nmeasured the impact of the adverse economic conditions on the degradation of\nthe credit portfolio quality.In our paper, we have shed more light on the\nquestion of the aggravation of default rate. For this, we have undertaken\neconometric modeling of the default rate distribution of a Moroccan bank while\nwe inspired from some studies carried out. Our findings demonstrate that the\ndecline in the economic situation has a positive impact on default of\nborrowers. Hence, the bank also has responsibility for monitoring the adverse\neconomic conditions.\n"
    },
    {
        "paper_id": 1801.05947,
        "authors": "Tetsuya Takaishi",
        "title": "Large-Scale Simulation of Multi-Asset Ising Financial Markets",
        "comments": "10 pages, 9 figures",
        "journal-ref": "J. Phys.: Conf. Ser. 820 (2017) 012016",
        "doi": "10.1088/1742-6596/820/1/012016",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We perform a large-scale simulation of an Ising-based financial market model\nthat includes 300 asset time series. The financial system simulated by the\nmodel shows a fat-tailed return distribution and volatility clustering and\nexhibits unstable periods indicated by the volatility index measured as the\naverage of absolute-returns. Moreover, we determine that the cumulative risk\nfraction, which measures the system risk, changes at high volatility periods.\nWe also calculate the inverse participation ratio (IPR) and its higher-power\nversion, IPR6, from the absolute-return cross-correlation matrix. Finally, we\nshow that the IPR and IPR6 also change at high volatility periods.\n"
    },
    {
        "paper_id": 1801.06028,
        "authors": "Sylvia Gottschalk",
        "title": "A closed-form formula for pricing bonds between coupon payments",
        "comments": "revised version that corrects some typos, Mathematical Finance\n  Letters (2018)",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We derive a closed-form formula for computing bond prices between coupon\npayments. Our results cover both the `Treasury' and the `Street' pricing\nmethods used by sovereign and corporate issuers. We apply our formulas to two\nUK gilts, the 8% Treasury Gilt 2015, and the 0.5% Treasury Gilt 2022, and show\nthat we can obtain the dirty price of these bonds at any date with a minimum of\ncalculations, and without intensive computational resources.\n"
    },
    {
        "paper_id": 1801.06077,
        "authors": "Igor Halperin",
        "title": "The QLBS Q-Learner Goes NuQLear: Fitted Q Iteration, Inverse RL, and\n  Option Portfolios",
        "comments": "18 pages, 5 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The QLBS model is a discrete-time option hedging and pricing model that is\nbased on Dynamic Programming (DP) and Reinforcement Learning (RL). It combines\nthe famous Q-Learning method for RL with the Black-Scholes (-Merton) model's\nidea of reducing the problem of option pricing and hedging to the problem of\noptimal rebalancing of a dynamic replicating portfolio for the option, which is\nmade of a stock and cash. Here we expand on several NuQLear (Numerical\nQ-Learning) topics with the QLBS model. First, we investigate the performance\nof Fitted Q Iteration for a RL (data-driven) solution to the model, and\nbenchmark it versus a DP (model-based) solution, as well as versus the BSM\nmodel. Second, we develop an Inverse Reinforcement Learning (IRL) setting for\nthe model, where we only observe prices and actions (re-hedges) taken by a\ntrader, but not rewards. Third, we outline how the QLBS model can be used for\npricing portfolios of options, rather than a single option in isolation, thus\nproviding its own, data-driven and model independent solution to the (in)famous\nvolatility smile problem of the Black-Scholes model.\n"
    },
    {
        "paper_id": 1801.06141,
        "authors": "Yiannis A. Papadopoulos and Alan L. Lewis",
        "title": "A First Option Calibration of the GARCH Diffusion Model by a PDE Method",
        "comments": "29 pages, 7 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Time-series calibrations often suggest that the GARCH diffusion model could\nalso be a suitable candidate for option (risk-neutral) calibration. But unlike\nthe popular Heston model, it lacks a fast, semi-analytic solution for the\npricing of vanilla options, perhaps the main reason why it is not used in this\nway. In this paper we show how an efficient finite difference-based PDE solver\ncan effectively replace analytical solutions, enabling accurate option\ncalibrations in less than a minute. The proposed pricing engine is shown to be\nrobust under a wide range of model parameters and combines smoothly with\nblack-box optimizers. We use this approach to produce a first PDE calibration\nof the GARCH diffusion model to SPX options and present some benchmark results\nfor future reference.\n"
    },
    {
        "paper_id": 1801.06373,
        "authors": "Christian Hotz-Behofsits, Florian Huber and Thomas O. Z\\\"orner",
        "title": "Predicting crypto-currencies using sparse non-Gaussian state space\n  models",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper we forecast daily returns of crypto-currencies using a wide\nvariety of different econometric models. To capture salient features commonly\nobserved in financial time series like rapid changes in the conditional\nvariance, non-normality of the measurement errors and sharply increasing\ntrends, we develop a time-varying parameter VAR with t-distributed measurement\nerrors and stochastic volatility. To control for overparameterization, we rely\non the Bayesian literature on shrinkage priors that enables us to shrink\ncoefficients associated with irrelevant predictors and/or perform model\nspecification in a flexible manner. Using around one year of daily data we\nperform a real-time forecasting exercise and investigate whether any of the\nproposed models is able to outperform the naive random walk benchmark. To\nassess the economic relevance of the forecasting gains produced by the proposed\nmodels we moreover run a simple trading exercise.\n"
    },
    {
        "paper_id": 1801.06416,
        "authors": "Jim Gatheral and Martin Keller-Ressel",
        "title": "Affine forward variance models",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We introduce the class of affine forward variance (AFV) models of which both\nthe conventional Heston model and the rough Heston model are special cases. We\nshow that AFV models can be characterized by the affine form of their cumulant\ngenerating function, which can be obtained as solution of a convolution Riccati\nequation. We further introduce the class of affine forward order flow intensity\n(AFI) models, which are structurally similar to AFV models, but driven by jump\nprocesses, and which include Hawkes-type models. We show that the cumulant\ngenerating function of an AFI model satisfies a generalized convolution Riccati\nequation and that a high-frequency limit of AFI models converges in\ndistribution to the AFV model.\n"
    },
    {
        "paper_id": 1801.06425,
        "authors": "Constantinos Kardaras and Scott Robertson",
        "title": "Ergodic robust maximization of asymptotic growth",
        "comments": "34 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We consider the problem of robustly maximizing the growth rate of investor\nwealth in the presence of model uncertainty. Possible models are all those\nunder which the assets' region $E$ and instantaneous covariation $c$ are known,\nand where additionally the assets are stable in that their occupancy time\nmeasures converge to a law with density $p$. This latter assumption is\nmotivated by the observed stability of ranked relative market capitalizations\nfor equity markets. We seek to identify the robust optimal growth rate, as well\nas a trading strategy which achieves this rate in all models. Under minimal\nassumptions upon $(E,c,p)$, we identify the robust growth rate with the\nDonsker-Varadhan rate function from occupancy time Large Deviations theory. We\nalso prove existence of, and explicitly identify, the optimal trading strategy.\nWe then apply our results in the case of drift uncertainty for ranked relative\nmarket capitalizations. Assuming regularity under symmetrization for the\ncovariance and limiting density of the ranked capitalizations, we explicitly\nidentify the robust optimal trading strategy in this setting.\n"
    },
    {
        "paper_id": 1801.06575,
        "authors": "Bahram Sanginabadi",
        "title": "USDA Forecasts: A meta-analysis study",
        "comments": "19 pages, 4 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  The primary goal of this study is doing a meta-analysis research on two\ngroups of published studies. First, the ones that focus on the evaluation of\nthe United States Department of Agriculture (USDA) forecasts and second, the\nones that evaluate the market reactions to the USDA forecasts. We investigate\nfour questions. 1) How the studies evaluate the accuracy of the USDA forecasts?\n2) How they evaluate the market reactions to the USDA forecasts? 3) Is there\nany heterogeneity in the results of the mentioned studies? 4) Is there any\npublication bias? About the first question, while some researchers argue that\nthe forecasts are unbiased, most of them maintain that they are biased,\ninefficient, not optimal, or not rational. About the second question, while a\nfew studies claim that the forecasts are not newsworthy, most of them maintain\nthat they are newsworthy, provide useful information, and cause market\nreactions. About the third and the fourth questions, based on our findings,\nthere are some clues that the results of the studies are heterogeneous, but we\ndidn't find enough evidences of publication bias.\n"
    },
    {
        "paper_id": 1801.06595,
        "authors": "Ricardo Antunes, Daniel Birchal, Jo\\~ao M\\'arcio Abijaodi, Paulo Abreu\n  and Rog\\'erio Peixoto",
        "title": "Modelo de maturidade em gerenciamento de riscos em projetos (Project\n  Risk Management Model Maturity)",
        "comments": "projects, risks, project risk management, maturity model. 145 pages,\n  in Portuguese",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The globalization feeded by the technology explosion that begans in the end\nof the last century, started the world to change faster every day. The only\ntoday's certain is the tomorrow's uncertain. Risk is defined as uncertain where\none or many causes composed of ocurrence probality can generate an impact or\nconsequence (threat if negative and oportunity if positive, to a determinated\ngoal). The Risk Management is composed of culture, procedure and process of an\norganization or individual care of uncertain, aiming to minimize threats e\nmaximizing the oportunities, to reach a desired goal. The \"Risk maturity model\nin projects\" proposed on this document, wants to measure the organizations\ncapacity and skills to manage the riks involved in projects when adopting a\ngeneric risk management methodology.\n"
    },
    {
        "paper_id": 1801.06651,
        "authors": "Andreas Kaloudis, Dimitrios Tsolis",
        "title": "Capital Structure in U.S., a Quantile Regression Approach with\n  Macroeconomic Impacts",
        "comments": "16 pages, 3 tables, working draft",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The major perspective of this paper is to provide more evidence into the\nempirical determinants of capital structure adjustment in different\nmacroeconomics states by focusing and discussing the relative importance of\nfirm-specific and macroeconomic characteristics from an alternative scope in\nU.S. This study extends the empirical research on the topic of capital\nstructure by focusing on a quantile regression method to investigate the\nbehavior of firm-specific characteristics and macroeconomic variables across\nall quantiles of distribution of leverage (total debt, long-terms debt and\nshort-terms debt). Thus, based on a partial adjustment model, we find that\nlong-term and short-term debt ratios varying regarding their partial adjustment\nspeeds; the short-term debt raises up while the long-term debt ratio slows down\nfor same periods.\n"
    },
    {
        "paper_id": 1801.06727,
        "authors": "Denisa Roberts and Douglas Patterson",
        "title": "A Second Order Cumulant Spectrum Test That a Stochastic Process is\n  Strictly Stationary and a Step Toward a Test for Graph Signal Strict\n  Stationarity",
        "comments": "6 pages",
        "journal-ref": "NeurIPS 2018 Workshop for the Spatiotemporal Domain",
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This article develops a statistical test for the null hypothesis of strict\nstationarity of a discrete time stochastic process in the frequency domain.\nWhen the null hypothesis is true, the second order cumulant spectrum is zero at\nall the discrete Fourier frequency pairs in the principal domain. The test uses\na window averaged sample estimate of the second order cumulant spectrum to\nbuild a test statistic with an asymptotic complex standard normal distribution.\nWe derive the test statistic, study the properties of the test and demonstrate\nits application using 137Cs gamma ray decay data. Future areas of research\ninclude testing for strict stationarity of graph signals, with applications in\nlearning convolutional neural networks on graphs, denoising, and inpainting.\n"
    },
    {
        "paper_id": 1801.06737,
        "authors": "Chung-Han Hsieh, B. Ross Barmish, John A. Gubner",
        "title": "At What Frequency Should the Kelly Bettor Bet?",
        "comments": "To appear in Proceedings of the IEEE American Control Conference\n  (ACC), 2018",
        "journal-ref": "Proceedings of the IEEE American Control Conference (ACC), 2018",
        "doi": "10.23919/ACC.2018.8431224",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We study the problem of optimizing the betting frequency in a dynamic game\nsetting using Kelly's celebrated expected logarithmic growth criterion as the\nperformance metric. The game is defined by a sequence of bets with independent\nand identically distributed returns X(k). The bettor selects the fraction of\nwealth K wagered at k = 0 and waits n steps before updating the bet size.\nBetween updates, the proceeds from the previous bets remain at risk in the\nspirit of \"buy and hold.\" Within this context, the main questions we consider\nare as follows: How does the optimal performance, we call it gn*, change with\nn? Does the high-frequency case, n = 1, always lead to the best performance?\nWhat are the effects of accrued interest and transaction costs? First, we\nprovide rather complete answers to these questions for the important special\ncase when X(k) in {-1,1} is a Bernoulli random variable with probability p that\nX(k) = 1. This serves as an entry point for future research using a binomial\nlattice model for stock trading. The latter sections focus on more general\nprobability distributions for X(k) and two conjectures. The first conjecture is\nsimple to state: Absent transaction costs, gn* is non-increasing in n. The\nsecond conjecture involves the technical condition which we call the sufficient\nattractiveness inequality. We first prove that satisfaction of this inequality\nis sufficient to guarantee that the low-frequency bettor using large n can\nmatch the performance of the high-frequency bettor using n = 1. Subsequently,\nwe conjecture, and provide supporting evidence that this condition is also\nnecessary.\n"
    },
    {
        "paper_id": 1801.0686,
        "authors": "Mikl\\'os R\\'asonyi and Andrea Meireles-Rodrigues",
        "title": "On Utility Maximisation Under Model Uncertainty in Discrete-Time Markets",
        "comments": "R\\'asonyi is supported by the \"Lend\\\"ulet\" grant LP2015-6 of the\n  Hungarian Academy of Sciences and by the NKFIH (National Research,\n  Development and Innovation Office, Hungary) grant KH 126505. Part of this\n  research was carried out while Meireles-Rodrigues was affiliated with the\n  School of Mathematical Sciences, DCU; and visiting the Alfr\\'ed R\\'enyi\n  Institute of Mathematics. 23 pp",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We study the problem of maximising terminal utility for an agent facing model\nuncertainty, in a frictionless discrete-time market with one safe asset and\nfinitely many risky assets. We show that an optimal investment strategy exists\nif the utility function, defined either over the positive real line or over the\nwhole real line, is bounded from above. We further find that the boundedness\nassumption can be dropped provided that we impose suitable integrability\nconditions, related to some strengthened form of no-arbitrage. These results\nare obtained in an alternative framework for model uncertainty, where all\npossible dynamics of the stock prices are represented by a collection of\nstochastic processes on the same filtered probability space, rather than by a\nfamily of probability measures.\n"
    },
    {
        "paper_id": 1801.06862,
        "authors": "Hiroyuki Kasahara and Katsumi Shimotsu",
        "title": "Testing the Number of Regimes in Markov Regime Switching Models",
        "comments": "72 pages, 2 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Markov regime switching models have been used in numerous empirical studies\nin economics and finance. However, the asymptotic distribution of the\nlikelihood ratio test statistic for testing the number of regimes in Markov\nregime switching models has been an unresolved problem. This paper derives the\nasymptotic distribution of the likelihood ratio test statistic for testing the\nnull hypothesis of $M_0$ regimes against the alternative hypothesis of $M_0 +\n1$ regimes for any $M_0 \\geq 1$ both under the null hypothesis and under local\nalternatives. We show that the contiguous alternatives converge to the null\nhypothesis at a rate of $n^{-1/8}$ in regime switching models with normal\ndensity. The asymptotic validity of the parametric bootstrap is also\nestablished.\n"
    },
    {
        "paper_id": 1801.06896,
        "authors": "Theo Diamandis, Yonathan Murin, Andrea Goldsmith",
        "title": "Ranking Causal Influence of Financial Markets via Directed Information\n  Graphs",
        "comments": "To be presented at Conference on Information Sciences and Systems\n  (CISS) 2018",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  A non-parametric method for ranking stock indices according to their mutual\ncausal influences is presented. Under the assumption that indices reflect the\nunderlying economy of a country, such a ranking indicates which countries exert\nthe most economic influence in an examined subset of the global economy. The\nproposed method represents the indices as nodes in a directed graph, where the\nedges' weights are estimates of the pair-wise causal influences, quantified\nusing the directed information functional. This method facilitates using a\nrelatively small number of samples from each index. The indices are then ranked\naccording to their net-flow in the estimated graph (sum of the incoming weights\nsubtracted from the sum of outgoing weights). Daily and minute-by-minute data\nfrom nine indices (three from Asia, three from Europe and three from the US)\nwere analyzed. The analysis of daily data indicates that the US indices are the\nmost influential, which is consistent with intuition that the indices\nrepresenting larger economies usually exert more influence. Yet, it is also\nshown that an index representing a small economy can strongly influence an\nindex representing a large economy if the smaller economy is indicative of a\nlarger phenomenon. Finally, it is shown that while inter-region interactions\ncan be captured using daily data, intra-region interactions require more\nfrequent samples.\n"
    },
    {
        "paper_id": 1801.06966,
        "authors": "Farouq Abdulaziz Masoudy",
        "title": "Accurate Evaluation of Asset Pricing Under Uncertainty and Ambiguity of\n  Information",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/publicdomain/zero/1.0/",
        "abstract": "  Since exchange economy considerably varies in the market assets, asset prices\nhave become an attractive research area for investigating and modeling\nambiguous and uncertain information in today markets. This paper proposes a new\ngenerative uncertainty mechanism based on the Bayesian Inference and\nCorrentropy (BIC) technique for accurately evaluating asset pricing in markets.\nThis technique examines the potential processes of risk, ambiguity, and\nvariations of market information in a controllable manner. We apply the new BIC\ntechnique to a consumption asset-pricing model in which the consumption\nvariations are modeled using the Bayesian network model with observing the\ndynamics of asset pricing phenomena in the data. These dynamics include the\nprocyclical deviations of price, the countercyclical deviations of equity\npremia and equity volatility, the leverage impact and the mean reversion of\nexcess returns. The key findings reveal that the precise modeling of asset\ninformation can estimate price changes in the market effectively.\n"
    },
    {
        "paper_id": 1801.07044,
        "authors": "Ralph Rudd, Thomas A. McWalter, Joerg Kienitz, Eckhard Platen",
        "title": "Quantization Under the Real-world Measure: Fast and Accurate Valuation\n  of Long-dated Contracts",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper provides a methodology for fast and accurate pricing of the\nlong-dated contracts that arise as the building blocks of insurance and pension\nfund agreements. It applies the recursive marginal quantization (RMQ) and joint\nrecursive marginal quantization (JRMQ) algorithms outside the framework of\ntraditional risk-neutral methods by pricing options under the real-world\nprobability measure, using the benchmark approach. The benchmark approach is\nreviewed, and the real-world pricing theorem is presented and applied to\nvarious long-dated claims to obtain less expensive prices than suggested by\ntraditional risk-neutral valuation. The growth-optimal portfolio (GOP), the\ncentral object of the benchmark approach, is modelled using the time-dependent\nconstant elasticity of variance model (TCEV). Analytic European option prices\nare derived and the RMQ algorithm is used to efficiently and accurately price\nBermudan options on the GOP. The TCEV model is then combined with a $3/2$\nstochastic short-rate model and RMQ is used to price zero-coupon bonds and\nzero-coupon bond options, highlighting the departure from risk-neutral pricing.\n"
    },
    {
        "paper_id": 1801.07213,
        "authors": "Anirban Chakraborti, Kiran Sharma, Hirdesh K. Pharasi, Sourish Das,\n  Rakesh Chatterjee and Thomas H. Seligman",
        "title": "Characterization of catastrophic instabilities: Market crashes as\n  paradigm",
        "comments": "17 pages, 4 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Catastrophic events, though rare, do occur and when they occur, they have\ndevastating effects. It is, therefore, of utmost importance to understand the\ncomplexity of the underlying dynamics and signatures of catastrophic events,\nsuch as market crashes. For deeper understanding, we choose the US and Japanese\nmarkets from 1985 onward, and study the evolution of the cross-correlation\nstructures of stock return matrices and their eigenspectra over different short\ntime-intervals or \"epochs\". A slight non-linear distortion is applied to the\ncorrelation matrix computed for any epoch, leading to the emerging spectrum of\neigenvalues. The statistical properties of the emerging spectrum display: (i)\nthe shape of the emerging spectrum reflects the market instability, (ii) the\nsmallest eigenvalue may be able to statistically distinguish the nature of a\nmarket turbulence or crisis -- internal instability or external shock, and\n(iii) the time-lagged smallest eigenvalue has a statistically significant\ncorrelation with the mean market cross-correlation. The smallest eigenvalue\nseems to indicate that the financial market has become more turbulent in a\nsimilar way as the mean does. Yet we show features of the smallest eigenvalue\nof the emerging spectrum that distinguish different types of market\ninstabilities related to internal or external causes. Based on the paradigmatic\ncharacter of financial time series for other complex systems, the capacity of\nthe emerging spectrum to understand the nature of instability may be a new\nfeature, which can be broadly applied.\n"
    },
    {
        "paper_id": 1801.07309,
        "authors": "Robert Fernholz",
        "title": "Numeraire markets",
        "comments": "10 pages, 2 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In a stock market, the numeraire portfolio, if it exists, is the portfolio\nwith the highest expected logarithmic growth rate at all times. A numeraire\nmarket is a stock market for which the market portfolio is the numeraire\nportfolio. We study open markets, markets comprising the higher capitalization\nstocks within a broad equity universe. The stocks we consider are represented\nby continuous semimartingales, and we construct an example of a numeraire\nmarket that is asymptotically stable.\n"
    },
    {
        "paper_id": 1801.07358,
        "authors": "Luting Li and Hao Xing",
        "title": "Capital allocation under the Fundamental Review of Trading Book",
        "comments": "25 pages, 4 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Facing the FRTB, banks need to allocate their capital to each business units\nor risk positions to evaluate the capital efficiency of their strategies. This\npaper proposes two computationally efficient allocation methods which are\nweighted according to liquidity horizon. Both methods provide more stable and\nless negative allocations under the FRTB than under the current regulatory\nframework.\n"
    },
    {
        "paper_id": 1801.07512,
        "authors": "Justin Delloye, R\\'emi Lemoy, Geoffrey Caruso",
        "title": "Alonso and the Scaling of Urban Profiles",
        "comments": "37 pages, 7 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The scaling of urban characteristics with total population has become an\nimportant research field since one needs to better understand the challenges of\nurban densification. Yet urban scaling research is largely disconnected from\nintra-urban structure. In contrast, the monocentric model of Alonso provides a\nresidential choice-based theory to urban density profiles. However, it is\nsilent about how these profiles scale with population, thus preventing\nempirical scaling studies to anchor in a strong micro-economic theory. This\npaper bridges this gap by introducing power laws for land, income and transport\ncost in the Alonso model. From this augmented model, we derive the conditions\nat which the equilibrium urban structure matches recent empirical findings\nabout the scaling of urban land and population density profiles in European\ncities. We find that the Alonso model is theoretically compatible with the\nobserved scaling of population density profiles and satisfactorily represents\nEuropean cities. This compatibility however challenges current empirical\nunderstanding of wage and transport cost elasticities with population, and\nrequires a scaling of the housing land profile that is different from the\nobserved. Our results call for revisiting theories about land development and\nhousing processes as well as the empirics of agglomeration benefits and\ntransport costs.\n"
    },
    {
        "paper_id": 1801.07595,
        "authors": "Zailei Cheng and Youngsoo Seol",
        "title": "Gaussian Approximation of a Risk Model with Non-Stationary Hawkes\n  Arrivals of Claims",
        "comments": "21 pages,3 figures. arXiv admin note: text overlap with\n  arXiv:1607.06624, arXiv:1702.05852, arXiv:1309.7621 by other authors",
        "journal-ref": "Methodology and Computing in Applied Probability 2019",
        "doi": "10.1007/s11009-019-09722-8",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We consider a classical risk process with arrival of claims following a\nnon-stationary Hawkes process. We study the asymptotic regime when the premium\nrate and the baseline intensity of the claims arrival process are large, and\nclaim size is small. The main goal of the article is to establish a diffusion\napproximation by verifying a functional central limit theorem and to compute\nthe ruin probability in finite-time horizon. Numerical results will also be\ngiven.\n"
    },
    {
        "paper_id": 1801.07784,
        "authors": "Eyal Neuman and Alexander Schied",
        "title": "Protecting Pegged Currency Markets from Speculative Investors",
        "comments": "16 pages, 5 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We consider a stochastic game between a trader and a central bank in a target\nzone market with a lower currency peg. This currency peg is maintained by the\ncentral bank through the generation of permanent price impact, thereby\naggregating an ever increasing risky position in foreign reserves. We describe\nthis situation mathematically by means of two coupled singular control\nproblems, where the common singularity arises from a local time along a random\ncurve. Our first result identifies a certain local time as that central bank\nstrategy for which this risk position is minimized. We then consider the\nworst-case situation the central bank may face by identifying that strategy of\nthe strategic investor that maximizes the expected inventory of the central\nbank under a cost criterion, thus establishing a Stackelberg equilibrium in our\nmodel.\n"
    },
    {
        "paper_id": 1801.07817,
        "authors": "Johannes Ruf and Kangjianan Xie",
        "title": "Generalised Lyapunov Functions and Functionally Generated Trading\n  Strategies",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper investigates the dependence of functional portfolio generation,\nintroduced by Fernholz (1999), on an extra finite variation process. The\nframework of Karatzas and Ruf (2017) is used to formulate conditions on trading\nstrategies to be strong arbitrage relative to the market over sufficiently\nlarge time horizons. A mollification argument and Komlos theorem yield a\ngeneral class of potential arbitrage strategies. These theoretical results are\ncomplemented by several empirical examples using data from the S&P 500 stocks.\n"
    },
    {
        "paper_id": 1801.07941,
        "authors": "Aurelio F. Bariviera, Angelo Plastino, George Judge",
        "title": "Spurious seasonality detection: a non-parametric test proposal",
        "comments": null,
        "journal-ref": "Econometrics 6, no. 1: 3",
        "doi": "10.3390/econometrics6010003",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper offers a general and comprehensive definition of the\nday-of-the-week effect. Using symbolic dynamics, we develop a unique test based\non ordinal patterns in order to detect it. This test uncovers the fact that the\nso-called \"day-of-the-week\" effect is partly an artifact of the hidden\ncorrelation structure of the data. We present simulations based on artificial\ntime series as well. Whereas time series generated with long memory are prone\nto exhibit daily seasonality, pure white noise signals exhibit no pattern\npreference. Since ours is a non parametric test, it requires no assumptions\nabout the distribution of returns so that it could be a practical alternative\nto conventional econometric tests. We made also an exhaustive application of\nthe here proposed technique to 83 stock indices around the world. Finally, the\npaper highlights the relevance of symbolic analysis in economic time series\nstudies.\n"
    },
    {
        "paper_id": 1801.0796,
        "authors": "Martin Iglesias Caride, Aurelio F. Bariviera, Laura Lanzarini",
        "title": "Stock returns forecast: an examination by means of Artificial Neural\n  Networks",
        "comments": null,
        "journal-ref": "Studies in Systems, Decision and Control, vol 125. Springer, Cham",
        "doi": "10.1007/978-3-319-69989-9_23",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The validity of the Efficient Market Hypothesis has been under severe\nscrutiny since several decades. However, the evidence against it is not\nconclusive. Artificial Neural Networks provide a model-free means to analize\nthe prediction power of past returns on current returns. This chapter analizes\nthe predictability in the intraday Brazilian stock market using a\nbackpropagation Artificial Neural Network. We selected 20 stocks from Bovespa\nindex, according to different market capitalization, as a proxy for stock size.\nWe find that predictability is related to capitalization. In particular, larger\nstocks are less predictable than smaller ones.\n"
    },
    {
        "paper_id": 1801.08007,
        "authors": "Ricardo Crisostomo, Lorena Couso",
        "title": "Financial density forecasts: A comprehensive comparison of risk-neutral\n  and historical schemes",
        "comments": "Journal of Forecasting, 2018",
        "journal-ref": null,
        "doi": "10.1002/for.2521",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We investigate the forecasting ability of the most commonly used benchmarks\nin financial economics. We approach the usual caveats of probabilistic\nforecasts studies -small samples, limited models and non-holistic validations-\nby performing a comprehensive comparison of 15 predictive schemes during a time\nperiod of over 21 years. All densities are evaluated in terms of their\nstatistical consistency, local accuracy and forecasting errors. Using a new\ncomposite indicator, the Integrated Forecast Score (IFS), we show that\nrisk-neutral densities outperform historical-based predictions in terms of\ninformation content. We find that the Variance Gamma model generates the\nhighest out-of-sample likelihood of observed prices and the lowest predictive\nerrors, whereas the ARCH-based GJR-FHS delivers the most consistent forecasts\nacross the entire density range. In contrast, lognormal densities, the Heston\nmodel or the Breeden-Litzenberger formula yield biased predictions and are\nrejected in statistical tests.\n"
    },
    {
        "paper_id": 1801.08215,
        "authors": "Elisa Alos, Rupak Chatterjee, Sebastian Tudor, and Tai-Ho Wang",
        "title": "Target volatility option pricing in lognormal fractional SABR model",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We examine in this article the pricing of target volatility options in the\nlognormal fractional SABR model. A decomposition formula by Ito's calculus\nyields a theoretical replicating strategy for the target volatility option,\nassuming the accessibilities of all variance swaps and swaptions. The same\nformula also suggests an approximation formula for the price of target\nvolatility option in small time by the technique of freezing the coefficient.\nAlternatively, we also derive closed formed expressions for a small volatility\nof volatility expansion of the price of target volatility option. Numerical\nexperiments show accuracy of the approximations in a reasonably wide range of\nparameters.\n"
    },
    {
        "paper_id": 1801.08222,
        "authors": "J. Lussange, A. Belianin, S. Bourgeois-Gironde, B. Gutkin",
        "title": "A bright future for financial agent-based models",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The history of research in finance and economics has been widely impacted by\nthe field of Agent-based Computational Economics (ACE). While at the same time\nbeing popular among natural science researchers for its proximity to the\nsuccessful methods of physics and chemistry for example, the field of ACE has\nalso received critics by a part of the social science community for its lack of\nempiricism. Yet recent trends have shifted the weights of these general\narguments and potentially given ACE a whole new range of realism. At the base\nof these trends are found two present-day major scientific breakthroughs: the\nsteady shift of psychology towards a hard science due to the advances of\nneuropsychology, and the progress of artificial intelligence and more\nspecifically machine learning due to increasing computational power and big\ndata. These two have also found common fields of study in the form of\ncomputational neuroscience, and human-computer interaction, among others. We\noutline here the main lines of a computational research study of collective\neconomic behavior via Agent-Based Models (ABM) or Multi-Agent System (MAS),\nwhere each agent would be endowed with specific cognitive and behavioral biases\nknown to the field of neuroeconomics, and at the same time autonomously\nimplement rational quantitative financial strategies updated by machine\nlearning. We postulate that such ABMs would offer a whole new range of realism.\n"
    },
    {
        "paper_id": 1801.08256,
        "authors": "Ishanu Chattopadhyay",
        "title": "A Hilbert Space of Stationary Ergodic Processes",
        "comments": "10 pages, 3 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Identifying meaningful signal buried in noise is a problem of interest\narising in diverse scenarios of data-driven modeling. We present here a\ntheoretical framework for exploiting intrinsic geometry in data that resists\nnoise corruption, and might be identifiable under severe obfuscation. Our\napproach is based on uncovering a valid complete inner product on the space of\nergodic stationary finite valued processes, providing the latter with the\nstructure of a Hilbert space on the real field. This rigorous construction,\nbased on non-standard generalizations of the notions of sum and scalar\nmultiplication of finite dimensional probability vectors, allows us to\nmeaningfully talk about \"angles\" between data streams and data sources, and,\nmake precise the notion of orthogonal stochastic processes. In particular, the\nrelative angles appear to be preserved, and identifiable, under severe noise,\nand will be developed in future as the underlying principle for robust\nclassification, clustering and unsupervised featurization algorithms.\n"
    },
    {
        "paper_id": 1801.08346,
        "authors": "Abdulnasser Hatemi-J and Youssef El-Khatib",
        "title": "Valuation of Currency Options in Markets with a Crunch",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This work studies the valuation of currency options in markets suffering from\na financial crisis. We consider a European option where the underlying asset is\na foreign currency. We assume that the value of the underlying asset is a\nstochastic process that follows a modified Black-Scholes model with an\naugmented stochastic volatility. Under these settings, we provide a closed form\nsolution for the option-pricing problem on foreign currency for the European\ncall and put options. A mathematical proof is provided for the underlying\nsolution. In addition, simulation results and an application are provided.\n"
    },
    {
        "paper_id": 1801.08675,
        "authors": "Omar El Euch, Masaaki Fukasawa, Jim Gatheral, Mathieu Rosenbaum",
        "title": "Short-term at-the-money asymptotics under stochastic volatility models",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  A small-time Edgeworth expansion of the density of an asset price is given\nunder a general stochastic volatility model, from which asymptotic expansions\nof put option prices and at-the-money implied volatilities follow. A limit\ntheorem for at-the-money implied volatility skew and curvature is also given as\na corollary. The rough Bergomi model is treated as an example.\n"
    },
    {
        "paper_id": 1801.08804,
        "authors": "Henrik Dam, Andrea Macrina, David Skovmand, David Sloth",
        "title": "Rational Models for Inflation-Linked Derivatives",
        "comments": "32 pages, 4 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We construct models for the pricing and risk management of inflation-linked\nderivatives. The models are rational in the sense that linear payoffs written\non the consumer price index have prices that are rational functions of the\nstate variables. The nominal pricing kernel is constructed in a multiplicative\nmanner that allows for closed-form pricing of vanilla inflation products\nsuchlike zero-coupon swaps, year-on-year swaps, caps and floors, and the exotic\nlimited-price-index swap. We study the conditions necessary for the\nmultiplicative nominal pricing kernel to give rise to short rate models for the\nnominal interest rate process. The proposed class of pricing kernel models\nretains the attractive features of a nominal multi-curve interest rate model,\nsuch as closed-form pricing of nominal swaptions, and it isolates the so-called\ninflation convexity-adjustment term arising from the covariance between the\nunderlying stochastic drivers. We conclude with examples of how the model can\nbe calibrated to EUR data.\n"
    },
    {
        "paper_id": 1801.08852,
        "authors": "Boris Buchmann, Kevin W. Lu, Dilip B. Madan",
        "title": "Calibration for Weak Variance-Alpha-Gamma Processes",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The weak variance-alpha-gamma process is a multivariate L\\'evy process\nconstructed by weakly subordinating Brownian motion, possibly with correlated\ncomponents with an alpha-gamma subordinator. It generalises the\nvariance-alpha-gamma process of Semeraro constructed by traditional\nsubordination. We compare three calibration methods for the weak\nvariance-alpha-gamma process, method of moments, maximum likelihood estimation\n(MLE) and digital moment estimation (DME). We derive a condition for Fourier\ninvertibility needed to apply MLE and show in our simulations that MLE produces\na better fit when this condition holds, while DME produces a better fit when it\nis violated. We also find that the weak variance-alpha-gamma process exhibits a\nwider range of dependence and produces a significantly better fit than the\nvariance-alpha-gamma process on an S&P500-FTSE100 data set, and that DME\nproduces the best fit in this situation.\n"
    },
    {
        "paper_id": 1801.09004,
        "authors": "Fabio Baione, Paolo De Angelis and Ivan Granito",
        "title": "On a capital allocation principle coherent with the Solvency 2 standard\n  formula",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Solvency II Directive 2009/138/EC requires an insurance and reinsurance\nundertakings assessment of a Solvency Capital Requirement by means of the\nso-called \"Standard Formula\" or by means of partial or full internal models.\nFocusing on the first approach, the bottom-up aggregation formula proposed by\nthe regulator permits a capital reduction due to diversification effect,\naccording to the typical subadditivity property of risk measures. However, once\nthe overall capital has been assessed no specific allocation formula is\nprovided or required in order to evaluate the contribution of each risk source\non the overall SCR. The aim of this paper is to provide a closed formula for\ncapital allocation fully coherent with the Solvency II Capital Requirement\nassessed by means of Standard Formula. The solution proposed permits a top-down\napproach to assess the allocated SCR among the risks considered in the\nmultilevel aggregation scheme established by Solvency II. Besides, we\ndemonstrate that the allocation formula here proposed is consistent with the\nEuler's allocation principle\n"
    },
    {
        "paper_id": 1801.09315,
        "authors": "Hyungbin Park",
        "title": "A representative agent model based on risk-neutral prices",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper, we determine a representative agent model based on\nrisk-neutral information. The main idea is that the pricing kernel is\ntransition independent, which is supported by the well-known capital asset\npricing theory. Determining the representative agent model is closely related\nto the eigenpair problem of a second-order differential operator. The purpose\nof this paper is to find all such eigenpairs which are financially or\neconomically meaningful. We provide a necessary and sufficient condition for\nthe existence of such pairs, and prove that that all the possible eignepairs\ncan be expressed as a one-parameter family. Finally, we find a representative\nagent model derived from the eigenpairs.\n"
    },
    {
        "paper_id": 1801.09362,
        "authors": "Young Shin Kim",
        "title": "First Passage Time for Tempered Stable Process and Its Application to\n  Perpetual American Option and Barrier Option Pricing",
        "comments": null,
        "journal-ref": "Comput Manag Sci (2019) 16: 187-215",
        "doi": "10.1007/s10287-018-0326-9",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper, we will discuss an approximation of the characteristic\nfunction of the first passage time for a Levy process using the martingale\napproach. The characteristic function of the first passage time of the tempered\nstable process is provided explicitly or by an indirect numerical method. This\nwill be applied to the perpetual American option pricing and the barrier option\npricing. Numerical illustrations are provided for the calibrated parameters\nusing the market call and put prices.\n"
    },
    {
        "paper_id": 1801.09458,
        "authors": "Stefan Gerhold, Christoph Gerstenecker, Arpad Pinter",
        "title": "Moment Explosions in the Rough Heston Model",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We show that the moment explosion time in the rough Heston model [El Euch,\nRosenbaum 2016, arxiv:1609.02108] is finite if and only if it is finite for the\nclassical Heston model. Upper and lower bounds for the explosion time are\nestablished, as well as an algorithm to compute the explosion time (under some\nrestrictions). We show that the critical moments are finite for all maturities.\nFor negative correlation, we apply our algorithm for the moment explosion time\nto compute the lower critical moment.\n"
    },
    {
        "paper_id": 1801.0974,
        "authors": "Sebastian Poledna, Stefan Hochrainer-Stigler, Michael Gregor Miess,\n  Peter Klimek, Stefan Schmelzer, Johannes Sorger, Elena Shchekinova, Elena\n  Rovenskaya, JoAnne Linnerooth-Bayer, Ulf Dieckmann and Stefan Thurner",
        "title": "When does a disaster become a systemic event? Estimating indirect\n  economic losses from natural disasters",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Reliable estimates of indirect economic losses arising from natural disasters\nare currently out of scientific reach. To address this problem, we propose a\nnovel approach that combines a probabilistic physical damage catastrophe model\nwith a new generation of macroeconomic agent-based models (ABMs). The ABM moves\nbeyond the state of the art by exploiting large data sets from detailed\nnational accounts, census data, and business information, etc., to simulate\ninteractions of millions of agents representing \\emph{each} natural person or\nlegal entity in a national economy. The catastrophe model introduces a copula\napproach to assess flood losses, considering spatial dependencies of the flood\nhazard. These loss estimates are used in a damage scenario generator that\nprovides input for the ABM, which then estimates indirect economic losses due\nto the event. For the first time, we are able to link environmental and\neconomic processes in a computer simulation at this level of detail. We show\nthat moderate disasters induce comparably small but positive short- to\nmedium-term, and negative long-term economic impacts. Large-scale events,\nhowever, trigger a pronounced negative economic response immediately after the\nevent and in the long term, while exhibiting a temporary short- to medium-term\neconomic boost. We identify winners and losers in different economic sectors,\nincluding the fiscal consequences for the government. We quantify the critical\ndisaster size beyond which the resilience of an economy to rebuild reaches its\nlimits. Our results might be relevant for the management of the consequences of\nsystemic events due to climate change and other disasters.\n"
    },
    {
        "paper_id": 1801.09956,
        "authors": "Shota Gugushvili, Frank van der Meulen, Moritz Schauer, Peter Spreij",
        "title": "Nonparametric Bayesian volatility estimation",
        "comments": null,
        "journal-ref": "2017 MATRIX Annals, Springer International Publishing, 2019",
        "doi": "10.1007/978-3-030-04161-8_19",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Given discrete time observations over a fixed time interval, we study a\nnonparametric Bayesian approach to estimation of the volatility coefficient of\na stochastic differential equation. We postulate a histogram-type prior on the\nvolatility with piecewise constant realisations on bins forming a partition of\nthe time interval. The values on the bins are assigned an inverse Gamma Markov\nchain (IGMC) prior. Posterior inference is straightforward to implement via\nGibbs sampling, as the full conditional distributions are available explicitly\nand turn out to be inverse Gamma. We also discuss in detail the hyperparameter\nselection for our method. Our nonparametric Bayesian approach leads to good\npractical results in representative simulation examples. Finally, we apply it\non a classical data set in change-point analysis: weekly closings of the\nDow-Jones industrial averages.\n"
    },
    {
        "paper_id": 1801.10088,
        "authors": "Ben Hambly, Andreas Sojmark",
        "title": "An SPDE Model for Systemic Risk with Endogenous Contagion",
        "comments": "44 pages, 5 page appendix, 4 figures; revised edition with improved\n  presentation and updated proofs as well as some new content",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We propose a dynamic mean field model for `systemic risk' in large financial\nsystems, which we derive from a system of interacting diffusions on the\npositive half-line with an absorbing boundary at the origin. These diffusions\nrepresent the distances-to-default of financial institutions and absorption at\nzero corresponds to default. As a way of modelling correlated exposures and\nherd behaviour, we consider a common source of noise and a form of\nmean-reversion in the drift. Moreover, we introduce an endogenous contagion\nmechanism whereby the default of one institution can cause a drop in the\ndistances-to-default of the other institutions. In this way, we aim to capture\nkey `system-wide' effects on risk. The resulting mean field limit is\ncharacterized uniquely by a nonlinear SPDE on the half-line with a Dirichlet\nboundary condition. The density of this SPDE gives the conditional law of a\nnon-standard `conditional' McKean--Vlasov diffusion, for which we provide a\nnovel upper Dirichlet heat kernel type estimate that is essential to the\nproofs. Depending on the realizations of the common noise and the rate of mean\nreversion, the SPDE can exhibit rapid accelerations in the loss of mass at the\nboundary. In other words, the contagion mechanism can give rise to periods of\nsignificant systemic default clustering.\n"
    },
    {
        "paper_id": 1801.10359,
        "authors": "Eduardo Abi Jaber (CEREMADE), Omar El Euch (X)",
        "title": "Multi-factor approximation of rough volatility models",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Rough volatility models are very appealing because of their remarkable fit of\nboth historical and implied volatilities. However, due to the non-Markovian and\nnon-semimartingale nature of the volatility process, there is no simple way to\nsimulate efficiently such models, which makes risk management of derivatives an\nintricate task. In this paper, we design tractable multi-factor stochastic\nvolatility models approximating rough volatility models and enjoying a\nMarkovian structure. Furthermore, we apply our procedure to the specific case\nof the rough Heston model. This in turn enables us to derive a numerical method\nfor solving fractional Riccati equations appearing in the characteristic\nfunction of the log-price in this setting.\n"
    },
    {
        "paper_id": 1801.10487,
        "authors": "Sebastian Poledna, Abraham Hinteregger, and Stefan Thurner",
        "title": "Identifying systemically important companies in the entire liability\n  network of a small open economy",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  To a large extent, the systemic importance of financial institutions is\nrelated to the topology of financial liability networks. In this work we\nreconstruct and analyze the - to our knowledge - largest financial network that\nhas been studied up to now. This financial liability network consists of 51,980\nfirms and 796 banks. It represents 80.2% of total liabilities towards banks by\nfirms and all interbank liabilities from the entire Austrian banking system. We\nfind that firms contribute to systemic risk in similar ways as banks do. In\nparticular, we identify several medium-sized banks and firms with total assets\nbelow 1 bln. EUR that are systemically important in the entire financial\nnetwork. We show that the notion of systemically important financial\ninstitutions (SIFIs) or global and domestic systemically important banks\n(G-SIBs or D-SIBs) can be straightforwardly extended to firms. We find that\nfirms introduce slightly more systemic risk than banks. In Austria in 2008, the\ntotal systemic risk of the interbank network amounts to only 29% of the total\nsystemic risk of the entire financial network, consisting of firms and banks.\n"
    },
    {
        "paper_id": 1801.10498,
        "authors": "Tolulope Fadina, Thorsten Schmidt",
        "title": "Ambiguity in defaultable term structure models",
        "comments": "14 pages, corrected typos",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We introduce the concept of no-arbitrage in a credit risk market under\nambiguity considering an intensity-based framework. We assume the default\nintensity is not exactly known but lies between an upper and lower bound. By\nmeans of the Girsanov theorem, we start from the reference measure where the\nintensity is equal to $1$ and construct the set of equivalent martingale\nmeasures. From this viewpoint, the credit risky case turns out to be similar to\nthe case of drift uncertainty in the $G$-expectation framework. Finally, we\nderive the interval of no-arbitrage prices for general bond prices in a\nMarkovian setting.\n"
    },
    {
        "paper_id": 1801.10515,
        "authors": "Anton Pichler, Sebastian Poledna, and Stefan Thurner",
        "title": "Systemic-risk-efficient asset allocation: Minimization of systemic risk\n  as a network optimization problem",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Systemic risk arises as a multi-layer network phenomenon. Layers represent\ndirect financial exposures of various types, including interbank liabilities,\nderivative- or foreign exchange exposures. Another network layer of systemic\nrisk emerges through common asset holdings of financial institutions. Strongly\noverlapping portfolios lead to similar exposures that are caused by price\nmovements of the underlying financial assets. Based on the knowledge of\nportfolio holdings of financial agents we quantify systemic risk of overlapping\nportfolios. We present an optimization procedure, where we minimize the\nsystemic risk in a given financial market by optimally rearranging overlapping\nportfolio networks, under the constraints that the expected returns and risks\nof the individual portfolios are unchanged. We explicitly demonstrate the power\nof the method on the overlapping portfolio network of sovereign exposure\nbetween major European banks by using data from the European Banking Authority\nstress test of 2016. We show that systemic-risk-efficient allocations are\naccessible by the optimization. In the case of sovereign exposure, systemic\nrisk can be reduced by more than a factor of two, with- out any detrimental\neffects for the individual banks. These results are confirmed by a simple\nsimulation of fire sales in the government bond market. In particular we show\nthat the contagion probability is reduced dramatically in the optimized\nnetwork.\n"
    },
    {
        "paper_id": 1801.10583,
        "authors": "Rick Steinert, Florian Ziel",
        "title": "Short- to Mid-term Day-Ahead Electricity Price Forecasting Using Futures",
        "comments": null,
        "journal-ref": "The Energy Journal, 40.1 (2019) 105-127",
        "doi": "10.5547/01956574.40.1.rste",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Due to the liberalization of markets, the change in the energy mix and the\nsurrounding energy laws, electricity research is a dynamically altering field\nwith steadily changing challenges. One challenge especially for investment\ndecisions is to provide reliable short to mid-term forecasts despite high\nvariation in the time series of electricity prices. This paper tackles this\nissue in a promising and novel approach. By combining the precision of\neconometric autoregressive models in the short-run with the expectations of\nmarket participants reflected in future prices for the short- and mid-run we\nshow that the forecasting performance can be vastly increased while maintaining\nhourly precision. We investigate the day-ahead electricity price of the EPEX\nSpot for Germany and Austria and setup a model which incorporates the Phelix\nfuture of the EEX for Germany and Austria. The model can be considered as an\nAR24-X model with one distinct model for each hour of the day. We are able to\nshow that future data contains relevant price information for future time\nperiods of the day-ahead electricity price. We show that relying only on\ndeterministic external regressors can provide stability for forecast horizons\nof multiple weeks. By implementing a fast and efficient lasso estimation\napproach we demonstrate that our model can outperform several other models in\nthe literature.\n"
    },
    {
        "paper_id": 1802.00311,
        "authors": "Sebastian Poledna, Seraf\\'in Mart\\'inez-Jaramillo, Fabio Caccioli, and\n  Stefan Thurner",
        "title": "Quantification of systemic risk from overlapping portfolios in the\n  financial system",
        "comments": "arXiv admin note: text overlap with arXiv:1505.04276",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Financial markets are exposed to systemic risk, the risk that a substantial\nfraction of the system ceases to function and collapses. Systemic risk can\npropagate through different mechanisms and channels of contagion. One important\nform of financial contagion arises from indirect interconnections between\nfinancial institutions mediated by financial markets. This indirect\ninterconnection occurs when financial institutions invest in common assets and\nis referred to as overlapping portfolios. In this work we quantify systemic\nrisk from indirect interconnections between financial institutions. Having\ncomplete information of security holdings of major Mexican financial\nintermediaries and the ability to uniquely identify securities in their\nportfolios, allows us to represent the Mexican financial system as a bipartite\nnetwork of securities and financial institutions. This makes it possible to\nquantify systemic risk arising from overlapping portfolios. We show that\nfocusing only on direct exposures underestimates total systemic risk levels by\nup to 50%. By representing the financial system as a multi-layer network of\ndirect exposures (default contagion) and indirect exposures (overlapping\nportfolios) we estimate the mutual influence of different channels of\ncontagion. The method presented here is the first objective data-driven\nquantification of systemic risk on national scales that includes overlapping\nportfolios.\n"
    },
    {
        "paper_id": 1802.01113,
        "authors": "R. J. Buonocore, G. Brandi, R. N. Mantegna, T. Di Matteo",
        "title": "On the interplay between multiscaling and stocks dependence",
        "comments": "19 pages, 8 figures, 9 tables",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We find a nonlinear dependence between an indicator of the degree of\nmultiscaling of log-price time series of a stock and the average correlation of\nthe stock with respect to the other stocks traded in the same market. This\nresult is a robust stylized fact holding for different financial markets. We\ninvestigate this result conditional on the stocks' capitalization and on the\nkurtosis of stocks' log-returns in order to search for possible confounding\neffects. We show that a linear dependence with the logarithm of the\ncapitalization and the logarithm of kurtosis does not explain the observed\nstylized fact, which we interpret as being originated from a deeper\nrelationship.\n"
    },
    {
        "paper_id": 1802.01143,
        "authors": "Shan Lu, Jichang Zhao and Huiwen Wang",
        "title": "The Power of Trading Polarity: Evidence from China Stock Market Crash",
        "comments": "The data set can be freely downloaded through:\n  https://doi.org/10.6084/m9.figshare.5835936.v1",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The imbalance of buying and selling functions profoundly in the formation of\nmarket trends, however, a fine-granularity investigation of the imbalance is\nstill missing. This paper investigates a unique transaction dataset that\nenables us to inspect the imbalance of buying and selling on the man-times\nlevel at high frequency, what we call 'trading polarity', for a large\ncross-section of stocks from Shenzhen Stock Exchange. The trading polarity\nmeasures the market sentiment toward stocks from a view of very essence of\ntrading desire. When using the polarity to examine market crash, we find that\ntrading polarity successfully reflects the changing of market-level behavior in\nterms of its flipping times, depth, and length. We further investigate the\nrelationship between polarity and return. At market-level, trading polarity is\nnegatively correlated with returns, while at stock-level, this correlation\nchanges according to market conditions, which becomes a good signal of market\npsychology transition. Also, the significant correlation disclosed by the\nmarket polarity and market emotion implies that our presented polarity, which\nessentially calculated in the context of high-frequency trading data, can\nreal-timely reflect the sentiment of the market. The trading polarity indeed\nprovides a new way to understand and foresee the market behavior.\n"
    },
    {
        "paper_id": 1802.01219,
        "authors": "Vladimir Vovk and Glenn Shafer",
        "title": "A game-theoretic derivation of the $\\sqrt{dt}$ effect",
        "comments": "17 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We study the origins of the $\\sqrt{dt}$ effect in finance and SDE. In\nparticular, we show, in the game-theoretic framework, that market volatility is\na consequence of the absence of riskless opportunities for making money and\nthat too high volatility is also incompatible with such opportunities. More\nprecisely, riskless opportunities for making money arise whenever a traded\nsecurity has fractal dimension below or above that of the Brownian motion and\nits price is not almost constant and does not become extremely large. This is a\nsimple observation known in the measure-theoretic mathematical finance. At the\nend of the article we also consider the case of non-zero interest rate.\n  This version of the article was essentially written in March 2005 but remains\na working paper.\n"
    },
    {
        "paper_id": 1802.01307,
        "authors": "Sander Willems",
        "title": "Asian Option Pricing with Orthogonal Polynomials",
        "comments": "Forthcoming in Quantitative Finance",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper we derive a series expansion for the price of a continuously\nsampled arithmetic Asian option in the Black-Scholes setting. The expansion is\nbased on polynomials that are orthogonal with respect to the log-normal\ndistribution. All terms in the series are fully explicit and no numerical\nintegration nor any special functions are involved. We provide sufficient\nconditions to guarantee convergence of the series. The moment indeterminacy of\nthe log-normal distribution introduces an asymptotic bias in the series,\nhowever we show numerically that the bias can safely be ignored in practice.\n"
    },
    {
        "paper_id": 1802.01393,
        "authors": "Lorenz Schneider and Bertrand Tavin",
        "title": "Seasonal Stochastic Volatility and the Samuelson Effect in Agricultural\n  Futures Markets",
        "comments": "arXiv admin note: substantial text overlap with arXiv:1506.05911",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We introduce a multi-factor stochastic volatility model for commodities that\nincorporates seasonality and the Samuelson effect. Conditions on the seasonal\nterm under which the corresponding volatility factor is well-defined are given,\nand five different specifications of the seasonality pattern are proposed. We\ncalculate the joint characteristic function of two futures prices for different\nmaturities in the risk-neutral measure. The model is then presented under the\nphysical measure, and its state-space representation is derived, in order to\nestimate the parameters with the Kalman filter for time series of corn, cotton,\nsoybean, sugar and wheat futures from 2007 to 2017. The seasonal model\nsignificantly outperforms the nested non-seasonal model in all five markets,\nand we show which seasonality patterns are particularly well-suited in each\ncase. We also confirm the importance of correctly modelling the Samuelson\neffect in order to account for futures with different maturities. Our results\nare clearly confirmed in a robustness check carried out with an alternative\ndataset of constant maturity futures for the same agricultural markets.\n"
    },
    {
        "paper_id": 1802.0154,
        "authors": "Guglielmo D'Amico, Ada Lika, Filippo Petroni",
        "title": "Indexed Markov Chains for financial data: testing for the number of\n  states of the index process",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  A new branch based on Markov processes is developing in the recent literature\nof financial time series modeling. In this paper, an Indexed Markov Chain has\nbeen used to model high frequency price returns of quoted firms. The\npeculiarity of this type of model is that through the introduction of an Index\nprocess it is possible to consider the market volatility endogenously and two\nvery important stylized facts of financial time series can be taken into\naccount: long memory and volatility clustering. In this paper, first we propose\na method for the optimal determination of the state space of the Index process\nwhich is based on a change-point approach for Markov chains. Furthermore we\nprovide an explicit formula for the probability distribution function of the\nfirst change of state of the index process. Results are illustrated with an\napplication to intra-day prices of a quoted Italian firm from January $1^{st}$,\n2007 to December $31^{st}$ 2010.\n"
    },
    {
        "paper_id": 1802.01556,
        "authors": "Vladimir Vovk and Glenn Shafer",
        "title": "Game-Theoretic Capital Asset Pricing in Continuous Time",
        "comments": "10 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We derive formulas for the performance of capital assets in continuous time\nfrom an efficient market hypothesis, with no stochastic assumptions and no\nassumptions about the beliefs or preferences of investors. Our efficient market\nhypothesis says that a speculator with limited means cannot beat a particular\nindex by a substantial factor. Our results include a formula that resembles the\nclassical CAPM formula for the expected simple return of a security or\nportfolio.\n  This version of the article was essentially written in December 2001 but\nremains a working paper.\n"
    },
    {
        "paper_id": 1802.01641,
        "authors": "Blanka Horvath and Antoine Jacquier and Peter Tankov",
        "title": "Volatility options in rough volatility models",
        "comments": "52 pages, 33 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We discuss the pricing and hedging of volatility options in some rough\nvolatility models. First, we develop efficient Monte Carlo methods and\nasymptotic approximations for computing option prices and hedge ratios in\nmodels where log-volatility follows a Gaussian Volterra process. While\nproviding a good fit for European options, these models are unable to reproduce\nthe VIX option smile observed in the market, and are thus not suitable for VIX\nproducts. To accommodate these, we introduce the class of modulated Volterra\nprocesses, and show that they successfully capture the VIX smile.\n"
    },
    {
        "paper_id": 1802.01861,
        "authors": "Javier Franco-Pedroso and Joaquin Gonzalez-Rodriguez and Jorge Cubero\n  and Maria Planas and Rafael Cobo and Fernando Pablos",
        "title": "Generating virtual scenarios of multivariate financial data for\n  quantitative trading applications",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper, we present a novel approach to the generation of virtual\nscenarios of multivariate financial data of arbitrary length and composition of\nassets. With this approach, decades of realistic time-synchronized data can be\nsimulated for a large number of assets, producing diverse scenarios to test and\nimprove quantitative investment strategies. Our approach is based on the\nanalysis and synthesis of the time-dependent individual and joint\ncharacteristics of real financial time series, using stochastic sequences of\nmarket trends to draw multivariate returns from time-dependent probability\nfunctions preserving both distributional properties of asset returns and\ntime-dependent correlation among time series. Moreover, new time-synchronized\nassets can be arbitrarily generated through a PCA-based procedure to obtain any\nnumber of assets in the final virtual scenario. For the validation of such\nsimulated data, they are tested with an extensive set of measurements showing a\nsignificant degree of agreement with the reference performance of real\nfinancial series, better than that obtained with other classical and\nstate-of-the-art approaches.\n"
    },
    {
        "paper_id": 1802.01921,
        "authors": "Damien Challet and Nikita Gourianov",
        "title": "Dynamical regularities of US equities opening and closing auctions",
        "comments": "20 pages, 14 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We first investigate the evolution of opening and closing auctions volumes of\nUS equities along the years. We then report dynamical properties of pre-auction\nperiods: the indicative match price is strongly mean-reverting because the\nimbalance is; the final auction price reacts to a single auction order\nplacement or cancellation in markedly different ways in the opening and closing\nauctions when computed conditionally on imbalance improving or worsening\nevents; the indicative price reverts towards the mid price of the regular limit\norder book but is not especially bound to the spread.\n"
    },
    {
        "paper_id": 1802.01991,
        "authors": "Zdzislaw Burda, Pawel Wojcieszak, Konrad Zuchniak",
        "title": "Dynamics of Wealth Inequality",
        "comments": "17 pages, 8 figures (references added, some material moved to\n  appendix)",
        "journal-ref": "Comptes Rendus Physique 20, 349 (2019)",
        "doi": "10.1016/j.crhy.2019.05.011",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We study an agent-based model of evolution of wealth distribution in a\nmacro-economic system. The evolution is driven by multiplicative stochastic\nfluctuations governed by the law of proportionate growth and interactions\nbetween agents. We are mainly interested in interactions increasing wealth\ninequality that is in a local implementation of the accumulated advantage\nprinciple. Such interactions destabilise the system. They are confronted in the\nmodel with a global regulatory mechanism which reduces wealth inequality. There\nare different scenarios emerging as a net effect of these two competing\nmechanisms. When the effect of the global regulation (economic interventionism)\nis too weak the system is unstable and it never reaches equilibrium. When the\neffect is sufficiently strong the system evolves towards a limiting stationary\ndistribution with a Pareto tail. In between there is a critical phase. In this\nphase the system may evolve towards a steady state with a multimodal wealth\ndistribution. The corresponding cumulative density function has a\ncharacteristic stairway pattern which reflects the effect of economic\nstratification. The stairs represent wealth levels of economic classes\nseparated by wealth gaps. As we show, the pattern is typical for macro-economic\nsystems with a limited economic freedom. One can find such a multimodal pattern\nin empirical data, for instance, in the highest percentile of wealth\ndistribution for the population in urban areas of China.\n"
    }
]