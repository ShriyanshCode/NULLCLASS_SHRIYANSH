[
    {
        "paper_id": 2112.00949,
        "authors": "Andrey Itkin, Alexander Lipton, Dmitry Muravey",
        "title": "Multilayer heat equations and their solutions via oscillating integral\n  transforms",
        "comments": "43 pages, 5 figures, 1 table",
        "journal-ref": null,
        "doi": "10.1016/j.physa.2022.127544",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  By expanding the Dirac delta function in terms of the eigenfunctions of the\ncorresponding Sturm-Liouville problem, we construct some new (oscillating)\nintegral transforms. These transforms are then used to solve various finance,\nphysics, and mathematics problems, which could be characterized by the\nexistence of a multilayer spatial structure and moving (time-dependent)\nboundaries (internal interfaces) between the layers. Thus, constructed\nsolutions are semi-analytical and extend the authors' previous work (Itkin,\nLipton, Muravey, Multilayer heat equations: application to finance, FMF, 1,\n2021). However, our new method doesn't duplicate the previous one but provides\nalternative representations of the solution which have different properties and\nserve other purposes.\n"
    },
    {
        "paper_id": 2112.01046,
        "authors": "Shixi Kang, Jingwen Tan",
        "title": "Can Education Motivate Individual Health Demands? Dynamic Pseudo-panel\n  Evidence from China's Immigration",
        "comments": "13 pages, 4 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Enhancing residents' willingness to participate in basic health services is a\nkey initiative to optimize the allocation of health care resources and promote\nequitable improvements in group health. This paper investigates the effect of\neducation on resident health record completion rates using a system GMM model\nbased on pseudo-panel that consisting of five-year cross-sectional data. To\nmitigate possible endogeneity, this paper controls for cohort effects while\nalso attenuating dynamic bias in the estimation from a dynamic perspective and\nprovides robust estimates based on multi-model regression. The results show\nthat (1) education can give positive returns on health needs to the mobile\npopulation under the static perspective, and such returns are underestimated\nwhen cohort effects are ignored; (2) there is a significant cumulative effect\nof file completion rate under the dynamic perspective, and file completion in\nprevious years will have a positive effect on the current year. (3)The positive\nrelationship between education and willingness to make health decisions is also\ncharacterized by heterogeneity by gender, generation, and education level\nitself. Among them, education is more likely to promote decision-making\nintentions among men and younger groups, and this motivational effect is more\nsignificant among those who received basic education.\n"
    },
    {
        "paper_id": 2112.01166,
        "authors": "Shujian Liao, Jian Chen and Hao Ni",
        "title": "Forex Trading Volatility Prediction using Neural Network Models",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  In this paper, we investigate the problem of predicting the future volatility\nof Forex currency pairs using the deep learning techniques. We show\nstep-by-step how to construct the deep-learning network by the guidance of the\nempirical patterns of the intra-day volatility. The numerical results show that\nthe multiscale Long Short-Term Memory (LSTM) model with the input of\nmulti-currency pairs consistently achieves the state-of-the-art accuracy\ncompared with both the conventional baselines, i.e. autoregressive and GARCH\nmodel, and the other deep learning models.\n"
    },
    {
        "paper_id": 2112.01237,
        "authors": "Vincent Schlatt, Johannes Sedlmeir, Simon Feulner, Nils Urbach",
        "title": "Designing a Framework for Digital KYC Processes Built on\n  Blockchain-Based Self-Sovereign Identity",
        "comments": null,
        "journal-ref": "Information & Management, Special Issue \"Blockchain Innovations:\n  Business Opportunities and Management Challenges\", 2021",
        "doi": "10.1016/j.im.2021.103553",
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  Know your customer (KYC) processes place a great burden on banks, because\nthey are costly, inefficient, and inconvenient for customers. While blockchain\ntechnology is often mentioned as a potential solution, it is not clear how to\nuse the technology's advantages without violating data protection regulations\nand customer privacy. We demonstrate how blockchain-based self-sovereign\nidentity (SSI) can solve the challenges of KYC. We follow a rigorous design\nscience research approach to create a framework that utilizes SSI in the KYC\nprocess, deriving nascent design principles that theorize on blockchain's role\nfor SSI.\n"
    },
    {
        "paper_id": 2112.01287,
        "authors": "Purba Banerjee, Vasudeva Murthy, Shashi Jain",
        "title": "Method of lines for valuation and sensitivities of Bermudan options",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper, we present a computationally efficient technique based on the\n\\emph{Method of Lines} (MOL) for the approximation of the Bermudan option\nvalues via the associated partial differential equations (PDEs). The MOL\nconverts the Black Scholes PDE to a system of ordinary differential equations\n(ODEs). The solution of the system of ODEs so obtained only requires spatial\ndiscretization and avoids discretization in time. Additionally, the exact\nsolution of the ODEs can be obtained efficiently using the exponential matrix\noperation, making the method computationally attractive and straightforward to\nimplement. An essential advantage of the proposed approach is that the\nassociated Greeks can be computed with minimal additional computations. We\nillustrate, through numerical experiments, the efficacy of the proposed method\nin pricing and computation of the sensitivities for a European call,\ncash-or-nothing, powered option, and Bermudan put option.\n"
    },
    {
        "paper_id": 2112.01749,
        "authors": "Ummuhabeeba Chaliyan and Mini P. Thomas",
        "title": "Financial Markets, Financial Institutions and International Trade:\n  Examining the causal links for Indian Economy",
        "comments": "26 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  This study investigates whether a uni-directional or bi-directional causal\nrelationship exists between financial development and international trade for\nIndian economy, during the time period from 1980 to 2019. The empirical\nanalysis utilizes three measures of financial development created by IMF,\nnamely, financial institutional development index, financial market development\nindex and a composite index of financial development, encompassing dimensions\nof financial access, depth and efficiency. Johansen cointegration, vector error\ncorrection model and vector auto regressive model are estimated to examine the\nlong run relationship and short run dynamics among the variables of interest.\nThe econometric results indicate that there is indeed a long run causal\nrelationship between the composite index of financial development and trade\nopenness. Cointegration is also found to exist between trade openness and index\nof financial market development. However, there is no evidence of cointegration\nbetween financial institutional development and trade openness. Granger\ncausality test results indicate the presence of uni-directional causality\nrunning from composite index of financial development to trade openness.\nFinancial market development is also found to Granger cause trade openness. In\ncontrast, trade openness is found to promote financial institutional\ndevelopment in the short run. Empirical evidence thus underlines the importance\nof formulating policies which recognize the role of well-developed financial\nmarkets in accelerating international trade of Indian economy.\n"
    },
    {
        "paper_id": 2112.01841,
        "authors": "Roberto Daluiso, Emanuele Nastasi, Andrea Pallavicini, Stefano Polo",
        "title": "Reinforcement learning for options on target volatility funds",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this work we deal with the funding costs rising from hedging the risky\nsecurities underlying a target volatility strategy (TVS), a portfolio of risky\nassets and a risk-free one dynamically rebalanced in order to keep the realized\nvolatility of the portfolio on a certain level. The uncertainty in the TVS\nrisky portfolio composition along with the difference in hedging costs for each\ncomponent requires to solve a control problem to evaluate the option prices. We\nderive an analytical solution of the problem in the Black and Scholes (BS)\nscenario. Then we use Reinforcement Learning (RL) techniques to determine the\nfund composition leading to the most conservative price under the local\nvolatility (LV) model, for which an a priori solution is not available. We show\nhow the performances of the RL agents are compatible with those obtained by\napplying path-wise the BS analytical strategy to the TVS dynamics, which\ntherefore appears competitive also in the LV scenario.\n"
    },
    {
        "paper_id": 2112.02063,
        "authors": "Jafet Baca",
        "title": "Shock Symmetry and Business Cycle Synchronization: Is Monetary\n  Unification Feasible among CAPADR Countries?",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  In light of the ongoing integration efforts, the question of whether CAPADR\neconomies may benefit from a single currency arises naturally. This paper\nexamines the feasibility of an Optimum Currency Area (OCA) within seven CAPADR\ncountries. We estimate SVAR models to retrieve demand and supply shocks between\n2009:01 - 2020:01 and determine their extent of symmetry. We then go on to\ncompute two regional indicators of dispersion and the cost of inclusion into a\nhypothetical OCA for each country. Our results indicate that asymmetric shocks\ntend to prevail. In addition, the dispersion indexes show that business cycles\nhave become more synchronous over time. However, CAPADR countries are still\nsources of cyclical divergence, so that they would incur significant costs in\nterms of cycle correlation whenever they pursue currency unification. We\nconclude that the region does not meet the required symmetry and synchronicity\nfor an OCA to be appropiate.\n"
    },
    {
        "paper_id": 2112.02095,
        "authors": "Francisco Caio Lima Paiva, Leonardo Kanashiro Felizardo, Reinaldo\n  Augusto da Costa Bianchi and Anna Helena Reali Costa",
        "title": "Intelligent Trading Systems: A Sentiment-Aware Reinforcement Learning\n  Approach",
        "comments": "9 pages, 5 figures, To appear in the Proceedings of the 2nd ACM\n  International Conference on AI in Finance (ICAIF'21), November 3-5, 2021,\n  Virtual Event, USA",
        "journal-ref": null,
        "doi": "10.1145/3490354.3494445",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The feasibility of making profitable trades on a single asset on stock\nexchanges based on patterns identification has long attracted researchers.\nReinforcement Learning (RL) and Natural Language Processing have gained\nnotoriety in these single-asset trading tasks, but only a few works have\nexplored their combination. Moreover, some issues are still not addressed, such\nas extracting market sentiment momentum through the explicit capture of\nsentiment features that reflect the market condition over time and assessing\nthe consistency and stability of RL results in different situations. Filling\nthis gap, we propose the Sentiment-Aware RL (SentARL) intelligent trading\nsystem that improves profit stability by leveraging market mood through an\nadaptive amount of past sentiment features drawn from textual news. We\nevaluated SentARL across twenty assets, two transaction costs, and five\ndifferent periods and initializations to show its consistent effectiveness\nagainst baselines. Subsequently, this thorough assessment allowed us to\nidentify the boundary between news coverage and market sentiment regarding the\ncorrelation of price-time series above which SentARL's effectiveness is\noutstanding.\n"
    },
    {
        "paper_id": 2112.02228,
        "authors": "Marina Di Giacinto, Claudio Tebaldi, and Tai-Ho Wang",
        "title": "Optimal order execution under price impact: A hybrid model",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper we explore optimal liquidation in a market populated by a\nnumber of heterogeneous market makers that have limited inventory-carrying and\nrisk-bearing capacity. We derive a reduced form model for the dynamic of their\naggregated inventory considering a proper scaling limit. The resulting price\nimpact profile is shown to depend on the characteristics and relative\nimportance of their inventories. The model is flexible enough to reproduce the\nempirically documented power law behavior of the price impact function. For any\nchoice of the market makers characteristics, optimal execution within this\nmodeling approach can be recast as a linear-quadratic stochastic control\nproblem in which the value function and the associated optimal trading rate can\nbe obtained semi-explicitly subject to solving a differential matrix Riccati\nequation. Numerical simulations are conducted to illustrate the performance of\nthe resulting optimal liquidation strategy in relation to standard benchmarks.\nRemarkably, they show that the increase in performance is determined by a\nsubstantial reduction of higher order moment risk.\n"
    },
    {
        "paper_id": 2112.02269,
        "authors": "Alexander Barzykin, Philippe Bergault, Olivier Gu\\'eant",
        "title": "Market making by an FX dealer: tiers, pricing ladders and hedging rates\n  for optimal risk control",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Dealers make money by providing liquidity to clients but face flow\nuncertainty and thus price risk. They can efficiently skew their prices and\nwait for clients to mitigate risk (internalization), or trade with other\ndealers in the open market to hedge their position and reduce their inventory\n(externalization). Of course, the better control associated with\nexternalization comes with transaction costs and market impact. The\ninternalization vs. externalization dilemma has been a topic of recent active\ndiscussion within the foreign exchange (FX) community. This paper offers an\noptimal control framework for market making tackling both pricing and hedging,\nthus answering a question well known to dealers: `to hedge, or not to hedge?'\n"
    },
    {
        "paper_id": 2112.02284,
        "authors": "Jianming Xia",
        "title": "Optimal Investment with Risk Controlled by Weighted Entropic Risk\n  Measures",
        "comments": "33 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  A risk measure that is consistent with the second-order stochastic dominance\nand additive for sums of independent random variables can be represented as a\nweighted entropic risk measure (WERM). The expected utility maximization\nproblem with risk controlled by WERM and a related risk minimization problem\nare investigated in this paper. The latter is same to a problem of maximizing a\nweighted average of constant-absolute-risk-aversion (CARA) certainty\nequivalents. The solutions of all the optimization problems are explicitly\ncharacterized and an iterative method of the solutions is provided.\n"
    },
    {
        "paper_id": 2112.02365,
        "authors": "Yiheng Sun, Tian Lu, Cong Wang, Yuan Li, Huaiyu Fu, Jingran Dong,\n  Yunjie Xu",
        "title": "TransBoost: A Boosting-Tree Kernel Transfer Learning Algorithm for\n  Improving Financial Inclusion",
        "comments": "Accepted at AAAI-22",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The prosperity of mobile and financial technologies has bred and expanded\nvarious kinds of financial products to a broader scope of people, which\ncontributes to advocating financial inclusion. It has non-trivial social\nbenefits of diminishing financial inequality. However, the technical challenges\nin individual financial risk evaluation caused by the distinct characteristic\ndistribution and limited credit history of new users, as well as the\ninexperience of newly-entered companies in handling complex data and obtaining\naccurate labels, impede further promoting financial inclusion. To tackle these\nchallenges, this paper develops a novel transfer learning algorithm (i.e.,\nTransBoost) that combines the merits of tree-based models and kernel methods.\nThe TransBoost is designed with a parallel tree structure and efficient weights\nupdating mechanism with theoretical guarantee, which enables it to excel in\ntackling real-world data with high dimensional features and sparsity in $O(n)$\ntime complexity. We conduct extensive experiments on two public datasets and a\nunique large-scale dataset from Tencent Mobile Payment. The results show that\nthe TransBoost outperforms other state-of-the-art benchmark transfer learning\nalgorithms in terms of prediction accuracy with superior efficiency, shows\nstronger robustness to data sparsity, and provides meaningful model\ninterpretation. Besides, given a financial risk level, the TransBoost enables\nfinancial service providers to serve the largest number of users including\nthose who would otherwise be excluded by other algorithms. That is, the\nTransBoost improves financial inclusion.\n"
    },
    {
        "paper_id": 2112.02368,
        "authors": "Engel John C. Dela Vega and Robert J. Elliott",
        "title": "A stochastic control approach to bid-ask price modelling",
        "comments": "22 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper develops a model for the bid and ask prices of a European type\nasset by formulating a stochastic control problem. The state process is\ngoverned by a modified geometric Brownian motion whose drift and diffusion\ncoefficients depend on a Markov chain. A Girsanov theorem for Markov chains is\nimplemented for the change of coefficients, including the diffusion coefficient\nwhich cannot be changed by the usual Girsanov theorem for Brownian motion. The\nprice of a European type asset is then determined using an Esscher transform\nand a system of partial differential equations. A dynamic programming principle\nand a maximum/minimum principle associated with the stochastic control problem\nare then derived to model bid and ask prices. These prices are not quotes of\ntraders or market makers but represent estimates in our model on which\nreasonable quantities could be traded.\n"
    },
    {
        "paper_id": 2112.0244,
        "authors": "Claudio Fontana, Alessandro Gnoatto, and Guillaume Szulda",
        "title": "CBI-time-changed L\\'evy processes for multi-currency modeling",
        "comments": null,
        "journal-ref": "Annals of Operations Research, 336: 127-152, 2024",
        "doi": "10.1007/s10479-022-04982-z",
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  We develop a stochastic volatility framework for modeling multiple currencies\nbased on CBI-time-changed L\\'evy processes. The proposed framework captures the\ntypical risk characteristics of FX markets and is coherent with the symmetries\nof FX rates. Moreover, due to the self-exciting behavior of CBI processes, the\nvolatilities of FX rates exhibit self-exciting dynamics. By relying on the\ntheory of affine processes, we show that our approach is analytically tractable\nand that the model structure is invariant under a suitable class of\nrisk-neutral measures. A semi-closed pricing formula for currency options is\nobtained by Fourier methods. We propose two calibration methods, also by\nrelying on deep-learning techniques, and show that a simple specification of\nthe model can achieve a good fit to market data on a currency triangle.\n"
    },
    {
        "paper_id": 2112.02449,
        "authors": "Paulo H. dos Santos, Igor D. S. Siciliani and M.H.R. Tragtenberg",
        "title": "Optimal Income Crossover for Two-Class Model Using Particle Swarm\n  Optimization",
        "comments": "16 pages, 14 figures, submitted to Physical Review E",
        "journal-ref": null,
        "doi": "10.1103/PhysRevE.106.034313",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Personal income distribution may exhibit a two-class structure, such that the\nlower income class of the population (85-98%) is described by exponential\nBoltzmann-Gibbs distribution, whereas the upper income class (15-2%) has a\nPareto power-law distribution. We propose a method, based on a theoretical and\nnumerical optimization scheme, which allows us to determine the crossover\nincome between the distributions, the temperature of the Boltzmann-Gibbs\ndistribution and the Pareto index. Using this method, the Brazilian income\ndistribution data provided by the National Household Sample Survey was studied.\nThe data was stratified into two dichotomies (sex/gender and color/race), so\nthe model was tested using different subsets along with accessing the economic\ndifferences between these groups. Lastly, we analyse the temporal evolution of\nthe parameters of our model and the Gini coefficient discussing the implication\non the Brazilian income inequality. To our knowledge, for the first time an\noptimization method is proposed in order to find a continuous two-class income\ndistribution, which is able to delimit the boundaries of the two distributions.\nIt also gives a measure of inequality which is a function that depends only on\nthe Pareto index and the percentage of people in the high income region. It was\nfound a temporal dynamics relation, that may be general, between the Pareto and\nthe percentage of people described by the Pareto tail.\n"
    },
    {
        "paper_id": 2112.02607,
        "authors": "Jacob Turton, Ali Kabiri, David Tuckett, Robert Elliott Smith, David\n  P. Vinson",
        "title": "Differentiating Approach and Avoidance from Traditional Notions of\n  Sentiment in Economic Contexts",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  There is growing interest in the role of sentiment in economic\ndecision-making. However, most research on the subject has focused on positive\nand negative valence. Conviction Narrative Theory (CNT) places Approach and\nAvoidance sentiment (that which drives action) at the heart of real-world\ndecision-making, and argues that it better captures emotion in financial\nmarkets. This research, bringing together psychology and machine learning,\nintroduces new techniques to differentiate Approach and Avoidance from positive\nand negative sentiment on a fundamental level of meaning. It does this by\ncomparing word-lists, previously constructed to capture these concepts in text\ndata, across a large range of semantic features. The results demonstrate that\nAvoidance in particular is well defined as a separate type of emotion, which is\nevaluative/cognitive and action-orientated in nature. Refining the Avoidance\nword-list according to these features improves macroeconomic models, suggesting\nthat they capture the essence of Avoidance and that it plays a crucial role in\ndriving real-world economic decision-making.\n"
    },
    {
        "paper_id": 2112.02672,
        "authors": "V\\'it Mach\\'a\\v{c}ek",
        "title": "Globalization of Scientific Communication: Evidence from authors in\n  academic journals by country of origin",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  This study measures the tendency to publish in international scientific\njournals. For each of nearly 35 thousands Scopus-indexed journals, we derive\nseven globalization indicators based on the composition of authors by country\nof origin and other characteristics. These are subsequently scaled up to the\nlevel of 174 countries and 27 disciplines between 2005 and 2017. The results\nindicate that advanced countries maintain high globalization of scientific\ncommunication that is not varying across disciplines. Social sciences and\nhealth sciences are less globalized than physical and life sciences. Countries\nof the former Soviet bloc score far lower on the globalization measures,\nespecially in social sciences or health sciences. Russia remains among the\nleast globalized during the whole period, with no upward trend. Contrary, China\nhas profoundly globalized its science system, gradually moving from the lowest\nglobalization figures to the world average. The paper concludes with\nreflections on measurement issues and policy implications.\n"
    },
    {
        "paper_id": 2112.02877,
        "authors": "Thomas Cherico Wanger, Francis Dennig, Manuel Toledo-Hern\\'andez, Teja\n  Tscharntke, Eric F. Lambin",
        "title": "Cocoa pollination, biodiversity-friendly production, and the global\n  market",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Production of cocoa, the third largest trade commodity globally has\nexperienced climate related yield stagnation since 2016, forcing farmers to\nexpand production in forested habitats and to shift from nature friendly\nagroforestry systems to intensive monocultures. The goal for future large-scale\ncocoa production combines high yields with biodiversity friendly management\ninto a climate adapted smart agroforestry system (SAS). As pollination\nlimitation is a key driver of global production, we use data of more than\n150,000 cocoa farms and results of hand pollination experiments to show that\nmanually enhancing cocoa pollination (hereafter manual pollination) can produce\nSAS. Manual pollination can triple farm yields and double farmers annual profit\nin the major producer countries Ivory Coast, Ghana, and Indonesia, and can\nincrease global cocoa supplies by up to 13%. We propose a win win scenario to\nmitigate negative long term price and socioeconomic effects, whereby manual\npollination compensates only for yield losses resulting from climate and\ndisease related decreases in production area and conversion of monocultures\ninto agroforestry systems. Our results highlight that yields in biodiversity\nfriendly and climate adapted SAS can be similar to yields currently only\nachieved in monocultures. Adoption of manual pollination could be achieved\nthrough wider implementation of ecocertification standards, carbon markets, and\nzero deforestation pledges.\n"
    },
    {
        "paper_id": 2112.02893,
        "authors": "Ian M. Trotter, Torjus F. Bolkesj{\\o}, Eirik O. J{\\aa}stad, Jon Gustav\n  Kirkerud",
        "title": "Increased Electrification of Heating and Weather Risk in the Nordic\n  Power System",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  Weather is one of the main drivers of both the power demand and supply,\nespecially in the Nordic region which is characterized by high heating needs\nand a high share of renewable energy. Furthermore, ambitious decarbonization\nplans may cause power to replace fossil fuels for heating in the Nordic region,\nat the same time as large wind power expansions are expected, resulting in even\ngreater exposure to weather risk. In this study, we quantify the increase in\nweather risk resulting from replacing fossil fuels with power for heating in\nthe Nordic region, at the same time as variable renewable generation expands.\nFirst, we calibrate statistical weather-driven power consumption models for\neach of the countries Norway, Sweden, Denmark, and Finland. Then, we modify the\nweather sensitivity of the models to simulate different levels of heating\nelectrification, and use 300 simulated weather years to investigate how\ndiffering weather conditions impact power consumption at each electrification\nlevel. The results show that full replacement of fossil fuels by power for\nheating in 2040 leads to an increase in annual consumption of 155 TWh (30%)\ncompared to a business-as-usual scenario during an average weather year, but a\n178 TWh (34%) increase during a one-in-twenty weather year. However, the\nincrease in the peak consumption is greater: around 50% for a normal weather\nyear, and 70% for a one-in-twenty weather year. Furthermore, wind and solar\ngeneration contribute little during the consumption peaks. The increased\nweather sensitivity caused by heating electrification causes greater total\nload, but also causes a significant increase in inter-annual, seasonal, and\nintra-seasonal variations. We conclude that heating electrification must be\naccompanied by an increase in power system flexibility to ensure a stable and\nsecure power supply.\n"
    },
    {
        "paper_id": 2112.02944,
        "authors": "Thibault Jaisson",
        "title": "Deep differentiable reinforcement learning and optimal trading",
        "comments": "26 pages, 12 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  In many reinforcement learning applications, the underlying environment\nreward and transition functions are explicitly known differentiable functions.\nThis enables us to use recent research which applies machine learning tools to\nstochastic control to find optimal action functions. In this paper, we define\ndifferentiable reinforcement learning as a particular case of this research. We\nfind that incorporating deep learning in this framework leads to more accurate\nand stable solutions than those obtained from more generic actor critic\nalgorithms. We apply this deep differentiable reinforcement learning (DDRL)\nalgorithm to the problem of one asset optimal trading strategies in various\nenvironments where the market dynamics are known. Thanks to the stability of\nthis method, we are able to efficiently find optimal strategies for complex\nmulti-scale market models. We also extend these methods to simultaneously find\noptimal action functions for a wide range of environment parameters. This makes\nit applicable to real life financial signals and portfolio optimization where\nthe expected return has multiple time scales. In the case of a slow and a fast\nalpha signal, we find that the optimal trading strategy consists in using the\nfast signal to time the trades associated to the slow signal.\n"
    },
    {
        "paper_id": 2112.02947,
        "authors": "Yuhan Su, Zeyu Sun, Jiarong Li, Xianghui Yuan",
        "title": "The Price Impact of Generalized Order Flow Imbalance",
        "comments": "9 pages,5 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Order flow imbalance can explain short-term changes in stock price. This\npaper considers the change of non-minimum quotation units in real transactions,\nand proposes a generalized order flow imbalance construction method to improve\nOrder Flow Imbalance (OFI) and Stationarized Order Flow Imbalance (log-OFI).\nBased on the high-frequency order book snapshot data, we conducted an empirical\nanalysis of the CSI 500 constituent stocks. In order to facilitate the\npresentation, we selected 10 stocks for comparison. The two indicators after\nthe improvement of the generalized order flow imbalance construction method\nboth show a better ability to explain changes in stock prices. Especially\nGeneralized Stationarized Order Flow Imbalance (log-GOFI), using a linear\nregression model, on the time scales of 30 seconds, 1 minute, and 5 minutes,\nthe average R-squared out of sample compared with Order Flow Imbalance (OFI)\n32.89%, 38.13% and 42.57%, respectively increased to 83.57%, 85.37% and 86.01%.\nIn addition, we found that the interpretability of Generalized Stationarized\nOrder Flow Imbalance (log-GOFI) showed stronger stability on all three time\nscales.\n"
    },
    {
        "paper_id": 2112.02961,
        "authors": "Alessandro Micheli, Johannes Muhle-Karbe and Eyal Neuman",
        "title": "Closed-Loop Nash Competition for Liquidity",
        "comments": "41 pages, 5 figures, supplementary appendix",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  We study a multi-player stochastic differential game, where agents interact\nthrough their joint price impact on an asset that they trade to exploit a\ncommon trading signal. In this context, we prove that a closed-loop Nash\nequilibrium exists if the price impact parameter is small enough. Compared to\nthe corresponding open-loop Nash equilibrium, both the agents' optimal trading\nrates and their performance move towards the central-planner solution, in that\nexcessive trading due to lack of coordination is reduced. However, the size of\nthis effect is modest for plausible parameter values.\n"
    },
    {
        "paper_id": 2112.03031,
        "authors": "Chengyuan Han and Hannes Hilger and Eva Mix and Philipp C. B\\\"ottcher\n  and Mark Reyers and Christian Beck and Dirk Witthaut and Leonardo Rydin\n  Gorj\\~ao",
        "title": "Complexity and Persistence of Price Time Series of the European\n  Electricity Spot Market",
        "comments": "16 pages, 10 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The large variability of renewable power sources is a central challenge in\nthe transition to a sustainable energy system. Electricity markets are central\nfor the coordination of electric power generation. These markets rely evermore\non short-term trading to facilitate the balancing of power generation and\ndemand and to enable systems integration of small producers. Electricity prices\nin these spot markets show pronounced fluctuations, featuring extreme peaks as\nwell as occasional negative prices. In this article, we analyse electricity\nprice time series from the European EPEX market, in particular the hourly\nday-ahead, hourly intraday, and 15-min intraday market prices. We quantify the\nfluctuations, correlations, and extreme events and reveal different time scales\nin the dynamics of the market. The short-term fluctuations show remarkably\ndifferent characteristics for time scales below and above 12 hours.\nFluctuations are strongly correlated and persistent below 12 hours, which\ncontributes to extreme price events and a strong multifractal behaviour. On\nlonger time scales, they get anti-correlated and price time series revert to\ntheir mean, witnessed by a stark decrease of the Hurst coefficient after 12\nhours. The long-term behaviour is strongly influenced by the evolution of a\nlarge-scale weather patterns with a typical time scale of four days. We\nelucidate this dependence in detail using a classification into circulation\nweather types. The separation in time scales enables a superstatistical\ntreatment, which confirms the characteristic time scale of four days, and\nmotivates the use of $q$-Gaussian distributions as the best fit to the empiric\ndistribution of electricity prices.\n"
    },
    {
        "paper_id": 2112.03075,
        "authors": "Tobias Fissler, Michael Merz, Mario V. W\\\"uthrich",
        "title": "Deep Quantile and Deep Composite Model Regression",
        "comments": "32 pages, 6 figures",
        "journal-ref": "Insurance: Mathematics and Economics, 2023, Volume 109",
        "doi": "10.1016/j.insmatheco.2023.01.001",
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  A main difficulty in actuarial claim size modeling is that there is no simple\noff-the-shelf distribution that simultaneously provides a good distributional\nmodel for the main body and the tail of the data. In particular, covariates may\nhave different effects for small and for large claim sizes. To cope with this\nproblem, we introduce a deep composite regression model whose splicing point is\ngiven in terms of a quantile of the conditional claim size distribution rather\nthan a constant. To facilitate M-estimation for such models, we introduce and\ncharacterize the class of strictly consistent scoring functions for the triplet\nconsisting a quantile, as well as the lower and upper expected shortfall beyond\nthat quantile. In a second step, this elicitability result is applied to fit\ndeep neural network regression models. We demonstrate the applicability of our\napproach and its superiority over classical approaches on a real accident\ninsurance data set.\n"
    },
    {
        "paper_id": 2112.0317,
        "authors": "Zhijing Zhang, Yue Yu, Qinghua Ma, Haixiang Yao",
        "title": "A revised comparison between FF five-factor model and three-factor\n  model,based on China's A-share market",
        "comments": "17 pages, under review",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  In allusion to some contradicting results in existing research, this paper\nselects China's latest stock data from 2005 to 2020 for empirical analysis. By\nchoosing this periods' data, we avoid the periods of China's significant stock\nmarket reforms to reduce the impact of the government's policy on the factor\neffect. In this paper, the redundant factors (HML, CMA) are orthogonalized, and\nthe regression analysis of 5*5 portfolio of Size-B/M and Size-Inv is carried\nout with these two orthogonalized factors. It found that the HML and the CMA\nare still significant in many portfolios, indicating that they have a strong\nexplanatory ability, which is also consistent with the results of GRS test. All\nthese show that the five-factor model has a better ability to explain the\nexcess return rate. In the concrete analysis, this paper uses the methods of\nthe five-factor 25-group portfolio returns calculation, the five-factor\nregression analysis, the orthogonal treatment, the five-factor 25-group\nregression and the GRS test to more comprehensively explain the excellent\nexplanatory ability of the five-factor model to the excess return. Then, we\nanalyze the possible reasons for the strong explanatory ability of the HML, CMA\nand RMW from the aspects of price to book ratio, turnover rate and correlation\ncoefficient. We also give a detailed explanation of the results, and analyze\nthe changes of China's stock market policy and investors' investment style\nrecent years. Finally, this paper attempts to put forward some useful\nsuggestions on the development of asset pricing model and China's stock market.\n"
    },
    {
        "paper_id": 2112.03171,
        "authors": "Bent Flyvbjerg, Dirk W. Bester",
        "title": "The Cost-Benefit Fallacy: Why Cost-Benefit Analysis Is Broken and How to\n  Fix It",
        "comments": "25 pages, 1 table, 1 figure",
        "journal-ref": "Journal of Benefit-Cost Analysis, October, 2021, pp. 1-25",
        "doi": "10.1017/bca.2021.9",
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Most cost-benefit analyses assume that the estimates of costs and benefits\nare more or less accurate and unbiased. But what if, in reality, estimates are\nhighly inaccurate and biased? Then the assumption that cost-benefit analysis is\na rational way to improve resource allocation would be a fallacy. Based on the\nlargest dataset of its kind, we test the assumption that cost and benefit\nestimates of public investments are accurate and unbiased. We find this is not\nthe case with overwhelming statistical significance. We document the extent of\ncost overruns, benefit shortfalls, and forecasting bias in public investments.\nWe further assess whether such inaccuracies seriously distort effective\nresource allocation, which is found to be the case. We explain our findings in\nbehavioral terms and explore their policy implications. Finally, we conclude\nthat cost-benefit analysis of public investments stands in need of reform and\nwe outline four steps to such reform.\n"
    },
    {
        "paper_id": 2112.03172,
        "authors": "Mayukh Mukhopadhyay and Kaushik Ghosh",
        "title": "Market Microstructure of Non Fungible Tokens",
        "comments": "10 pages,3 figures",
        "journal-ref": "Five Shades of Emerging Business Cases, Chapter 3, Eliva Press,\n  Moldova, EU, pp. 26-38, 2021, ISBN 9781636483955",
        "doi": "10.5281/zenodo.5654779",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Non Fungible Token (NFT) Industry has been witnessing multi-million dollar\ntrade in recent times. With rapid innovation of the NFT market environment by\ntechnology, innovation, and decentralization, it is becoming hard to\ndistinguish between genuine NFT from fads and scams. This article discuss the\nNFT market microstructure, with a focus on price formation, market structure,\ntransparency, and applications to other financial areas. Market manipulation in\nNFT market with the context of wash-sale patterns has also been surveyed. The\narticle concludes by providing pointers on due-diligence activity that can be\nadopted by investors to mitigate NFT trading risk.\n"
    },
    {
        "paper_id": 2112.03193,
        "authors": "Kumar Yashaswi",
        "title": "Posterior Cramer-Rao Lower Bound based Adaptive State Estimation for\n  Option Price Forecasting",
        "comments": "9 pages, 3 Figures, 2 tables, Keywords- Option Theory; Stochastic\n  volatility; Bayesian Filtering; Particle Filter; Posterior Cramer-Rao Lower\n  Bound (PCRLB)",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  The use of Bayesian filtering has been widely used in mathematical finance,\nprimarily in Stochastic Volatility models. They help in estimating unobserved\nlatent variables from observed market data. This field saw huge developments in\nrecent years, because of the increased computational power and increased\nresearch in the model parameter estimation and implied volatility theory. In\nthis paper, we design a novel method to estimate underlying states (volatility\nand risk) from option prices using Bayesian filtering theory and Posterior\nCramer-Rao Lower Bound (PCRLB), further using it for option price prediction.\nSeveral Bayesian filters like Extended Kalman Filter (EKF), Unscented Kalman\nFilter (UKF), Particle Filter (PF) are used for latent state estimation of\nBlack-Scholes model under a GARCH model dynamics. We employ an Average and Best\ncase switching strategy for adaptive state estimation of a non-linear,\ndiscrete-time state space model (SSM) like Black-Scholes, using PCRLB based\nperformance measure to judge the best filter at each time step [1]. Since\nestimating closed-form solution of PCRLB is non-trivial, we employ a particle\nfilter based approximation of PCRLB based on [2]. We test our proposed\nframework on option data from S$\\&$P 500, estimating the underlying state from\nthe real option price, and using it to estimate theoretical price of the option\nand forecasting future prices. Our proposed method performs much better than\nthe individual applied filter used for estimating the underlying state and\nsubstantially improve forecasting capabilities.\n"
    },
    {
        "paper_id": 2112.03513,
        "authors": "Leonardo Rydin Gorj\\~ao and Dirk Witthaut and Pedro G. Lind and Wided\n  Medjroubi",
        "title": "Change of persistence in European electricity spot prices",
        "comments": "7 pages, 2 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The European Power Exchange has introduced day-ahead auctions and continuous\ntrading spot markets to facilitate the insertion of renewable electricity.\nThese markets are designed to balance excess or lack of power in short time\nperiods, which leads to a large stochastic variability of the electricity\nprices. Furthermore, the different markets show different stochastic memory in\ntheir electricity price time series, which seem to be the cause for the large\nvolatility. In particular, we show the antithetical temporal correlation in the\nintraday 15 minutes spot markets in comparison to the day-ahead hourly market.\nWe contrast the results from Detrended Fluctuation Analysis (DFA) to a new\nmethod based on the Kramers--Moyal equation in scale. For very short term\n($<12$ hours), all price time series show positive temporal correlations (Hurst\nexponent $H>0.5$) except for the intraday 15 minute market, which shows strong\nnegative correlations ($H<0.5$). For longer term periods covering up to two\ndays, all price time series are anti-correlated ($H<0.5$).\n"
    },
    {
        "paper_id": 2112.03718,
        "authors": "Martin Tegner and Stephen Roberts",
        "title": "A Bayesian take on option pricing with Gaussian processes",
        "comments": "arXiv admin note: text overlap with arXiv:1901.06021",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Local volatility is a versatile option pricing model due to its state\ndependent diffusion coefficient. Calibration is, however, non-trivial as it\ninvolves both proposing a hypothesis model of the latent function and a method\nfor fitting it to data. In this paper we present novel Bayesian inference with\nGaussian process priors. We obtain a rich representation of the local\nvolatility function with a probabilistic notion of uncertainty attached to the\ncalibrate. We propose an inference algorithm and apply our approach to S&P 500\nmarket data.\n"
    },
    {
        "paper_id": 2112.03789,
        "authors": "Julia Ackermann, Thomas Kruse, Mikhail Urusov",
        "title": "Self-exciting price impact via negative resilience in stochastic order\n  books",
        "comments": "27 pages; title of first version: On effects of negative resilience\n  on optimal trade execution in stochastic order books",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Most of the existing literature on optimal trade execution in limit order\nbook models assumes that resilience is positive. But negative resilience also\nhas a natural interpretation, as it models self-exciting behaviour of the price\nimpact, where trading activities of the large investor stimulate other market\nparticipants to trade in the same direction. In the paper we discuss several\nnew qualitative effects on optimal trade execution that arise when we allow\nresilience to take negative values. We do this in a framework where both market\ndepth and resilience are stochastic processes.\n"
    },
    {
        "paper_id": 2112.03868,
        "authors": "Domonkos F. Vamossy and Rolf Skog",
        "title": "EmTract: Extracting Emotions from Social Media",
        "comments": "Substantial changes to the project",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We develop an open-source tool (EmTract) that extracts emotions from social\nmedia text tailed for financial context. To do so, we annotate ten thousand\nshort messages from a financial social media platform (StockTwits) and combine\nit with open-source emotion data. We then use a pre-tuned NLP model,\nDistilBERT, augment its embedding space by including 4,861 tokens (emojis and\nemoticons), and then fit it first on the open-source emotion data, then\ntransfer it to our annotated financial social media data. Our model outperforms\ncompeting open-source state-of-the-art emotion classifiers, such as Emotion\nEnglish DistilRoBERTa-base on both human and chatGPT annotated data. Compared\nto dictionary based methods, our methodology has three main advantages for\nresearch in finance. First, our model is tailored to financial social media\ntext; second, it incorporates key aspects of social media data, such as\nnon-standard phrases, emojis, and emoticons; and third, it operates by\nsequentially learning a latent representation that includes features such as\nword order, word usage, and local context. Using EmTract, we explore the\nrelationship between investor emotions expressed on social media and asset\nprices. We show that firm-specific investor emotions are predictive of daily\nprice movements. Our findings show that emotions and market dynamics are\nclosely related, and we provide a tool to help study the role emotions play in\nfinancial markets.\n"
    },
    {
        "paper_id": 2112.03874,
        "authors": "Yuanlu Bai, Henry Lam, Svitlana Vyetrenko, Tucker Balch",
        "title": "Efficient Calibration of Multi-Agent Simulation Models from Output\n  Series with Bayesian Optimization",
        "comments": "This paper has been accepted and will be published in ICAIF 2022\n  proceedings",
        "journal-ref": null,
        "doi": "10.1145/3533271.3561755",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Multi-agent simulation is commonly used across multiple disciplines,\nspecifically in artificial intelligence in recent years, which creates an\nenvironment for downstream machine learning or reinforcement learning tasks. In\nmany practical scenarios, however, only the output series that result from the\ninteractions of simulation agents are observable. Therefore, simulators need to\nbe calibrated so that the simulated output series resemble historical -- which\namounts to solving a complex simulation optimization problem. In this paper, we\npropose a simple and efficient framework for calibrating simulator parameters\nfrom historical output series observations. First, we consider a novel concept\nof eligibility set to bypass the potential non-identifiability issue. Second,\nwe generalize the two-sample Kolmogorov-Smirnov (K-S) test with Bonferroni\ncorrection to test the similarity between two high-dimensional distributions,\nwhich gives a simple yet effective distance metric between the output series\nsample sets. Third, we suggest using Bayesian optimization (BO) and\ntrust-region BO (TuRBO) to minimize the aforementioned distance metric.\nFinally, we demonstrate the efficiency of our framework using numerical\nexperiments both on a multi-agent financial market simulator.\n"
    },
    {
        "paper_id": 2112.03946,
        "authors": "Ashish Kumar, Abeer Alsadoon, P. W. C. Prasad, Salma Abdullah, Tarik\n  A. Rashid, Duong Thu Hang Pham, Tran Quoc Vinh Nguyen",
        "title": "Generative Adversarial Network (GAN) and Enhanced Root Mean Square Error\n  (ERMSE): Deep Learning for Stock Price Movement Prediction",
        "comments": "18 pages. Multimed Tools Appl, 2021",
        "journal-ref": null,
        "doi": "10.1007/s11042-021-11670-w",
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  The prediction of stock price movement direction is significant in financial\ncircles and academic. Stock price contains complex, incomplete, and fuzzy\ninformation which makes it an extremely difficult task to predict its\ndevelopment trend. Predicting and analysing financial data is a nonlinear,\ntime-dependent problem. With rapid development in machine learning and deep\nlearning, this task can be performed more effectively by a purposely designed\nnetwork. This paper aims to improve prediction accuracy and minimizing\nforecasting error loss through deep learning architecture by using Generative\nAdversarial Networks. It was proposed a generic model consisting of Phase-space\nReconstruction (PSR) method for reconstructing price series and Generative\nAdversarial Network (GAN) which is a combination of two neural networks which\nare Long Short-Term Memory (LSTM) as Generative model and Convolutional Neural\nNetwork (CNN) as Discriminative model for adversarial training to forecast the\nstock market. LSTM will generate new instances based on historical basic\nindicators information and then CNN will estimate whether the data is predicted\nby LSTM or is real. It was found that the Generative Adversarial Network (GAN)\nhas performed well on the enhanced root mean square error to LSTM, as it was\n4.35% more accurate in predicting the direction and reduced processing time and\nRMSE by 78 secs and 0.029, respectively. This study provides a better result in\nthe accuracy of the stock index. It seems that the proposed system concentrates\non minimizing the root mean square error and processing time and improving the\ndirection prediction accuracy, and provides a better result in the accuracy of\nthe stock index.\n"
    },
    {
        "paper_id": 2112.04181,
        "authors": "Chris Kenyon and Mourad Berrahoui and Andrea Macrina",
        "title": "Sustainability Manifesto for Financial Products: Carbon Equivalence\n  Principle",
        "comments": "12 pages, 1 table, 1 figure",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Sustainability is a key point for financial markets and the label \"Green\" is\nan attempt to address this. Acquisition of the label \"Green\" for financial\nproducts carries potential benefits, hence the controversy and attractiveness\nof the label. However, such a binary label inadequately represents the carbon\nimpact - we use carbon as a useful simplification of sustainability. Carbon\nimpact has a range either size of zero. Both carbon emissions, and\nsequestration of carbon, are possible results of financial products. A binary\nlabel does not allow differentiation between a carbon neutral investment and a\ncoal power plant. Carbon impact has timing and duration, a planted forest takes\ntime to grow, a coal power plant takes time to emit. Hence we propose the\nCarbon Equivalence Principle (CEP) for financial products: that the carbon\neffect of a financial product shall be included as a linked term sheet\ncompatible with existing bank systems. This can either be a single flow, i.e.,\na summary carbon flow, or a linked termsheet describing the carbon impacts in\nvolume and time. The CEP means that the carbon impact of investment follows the\nmoney. Making carbon impacts consistent with existing bank systems enables\ndirect alignment of financial product use and sustainability, improving on\nnon-compatible disclosure proposals.\n"
    },
    {
        "paper_id": 2112.04218,
        "authors": "Jorge Carrera, Gabriel Montes-Rojas, Fernando Toledo",
        "title": "Global Financial Cycle, Commodity Terms of Trade and Financial Spreads\n  in Emerging Markets and Developing Economies",
        "comments": "32 pages, 14 figures, 2 tables",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/publicdomain/zero/1.0/",
        "abstract": "  We study the diffusion of shocks in the global financial cycle and global\nliquidity conditions to emerging and developing economies. We show that the\nclassification according to their external trade patterns (as commodities' net\nexporters or net importers) allows to evaluate the relative importance of\ninternational monetary spillovers and their impact on the domestic financial\ncycle volatility -i.e., the coefficient of variation of financial spreads and\nrisks. Given the relative importance of commodity trade in the economic\nstructure of these countries, our study reveals that the sign and size of the\ntrade balance of commodity goods are key parameters to rationalize the impact\nof global financial and liquidity conditions. Hence, the sign and volume of\ncommodity external trade will define the effect on countries' financial\nspreads. We implement a two-equation dynamic panel data model for 33 countries\nduring 1999:Q1-2020:Q4 that identifies the effect of global conditions on the\ncountries' commodities terms of trade and financial spreads, first in a direct\nway, and then by a feedback mechanism by which the terms of trade have an\nasymmetric additional influence on spreads.\n"
    },
    {
        "paper_id": 2112.04245,
        "authors": "Michele Vodret, Iacopo Mastromatteo, Bence T\\'oth and Michael\n  Benzaquen",
        "title": "Do fundamentals shape the price response? A critical assessment of\n  linear impact models",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We compare the predictions of the stationary Kyle model, a microfounded\nmulti-step linear price impact model in which market prices forecast\nfundamentals through information encoded in the order flow, with those of the\npropagator model, a purely data-driven model in which trades mechanically\nimpact prices with a time-decaying kernel. We find that, remarkably, both\nmodels predict the exact same price dynamics at high frequency, due to the\nemergence of universality at small time scales. On the other hand, we find\nthose models to disagree on the overall strength of the impact function by a\nquantity that we are able to relate to the amount of excess-volatility in the\nmarket. We reveal a crossover between a high-frequency regime in which the\nmarket reacts sub-linearly to the signed order flow, to a low-frequency regime\nin which prices respond linearly to order flow imbalances. Overall, we\nreconcile results from the literature on market microstructure (sub-linearity\nin the price response to traded volumes) with those relating to\nmacroeconomically relevant timescales (in which a linear relation is typically\nassumed).\n"
    },
    {
        "paper_id": 2112.04297,
        "authors": "N.S.Gonchar, O.P.Dovzhyk, A.S.Zhokhin, W.H.Kozyrski, A.P.Makhort",
        "title": "Mathematical Model of International Trade and Global Economy",
        "comments": "48 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  This work was partially supported by the Program of Fundamental Research of\nthe Department of Physics and Astronomy of the National Academy of Sciences of\nUkraine \"Mathematical models of non equilibrium processes in open systems\" N\n0120U100857.\n"
    },
    {
        "paper_id": 2112.04332,
        "authors": "I. Martin-de-Santos",
        "title": "Aproximacion a los estudios sobre la economia en la Segunda Republica\n  espanola hasta 1936 -- Approaches to the economics of the Spanish Second\n  Republic prior to 1936",
        "comments": "pp. 191-212, in Spanish, 2 figures",
        "journal-ref": "Revista de Historiografia, number 29, 2018",
        "doi": "10.20318/revhisto.2018.4297",
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  Macroeconomic data on the Spanish economy during the Second Republic is not\naccurate, the interpretation of historical events from the figures obtained is\ndivergent and misleading. Hasty laws were enacted in attempts to resolve social\nproblems arising mainly from deep economic inequalities, but they were often\nnothing more than declarations of good intentions. Spain suffered in the\naftermath of the international economic downturn as it began to be felt at the\nend of the dictatorship of General Primo de Rivera. Economic policy was\ndeveloped under the Constitution,but, despite the differences between the first\nand second biennium, there was a tendency to maintain the guidelines from the\nprevious stage and in general, sometimes unfairly, it aimed at least to avoid\nthe destabilization of the financial system. Nonetheless, it ultimately failed\nto achieve its goals, mainly because of the frequent changes of government\nmediated by a social crisis of greater significance that had relegated economic\nissues into the background.\n"
    },
    {
        "paper_id": 2112.04366,
        "authors": "I. Mart\\'in-de-Santos",
        "title": "La mujer a trav\\'es de los personajes femeninos en el cine de tem\\'atica\n  financiera -- Women through female characters in financial topics films",
        "comments": "21 pages, in Spanish, 1 appendix",
        "journal-ref": "Aida Mar\\'ia de Vicente Dom\\'inguez y Javier Sierra S\\'anchez\n  (eds.). La representaci\\'on audiovisual de la ciencia en el entorno digital.\n  Madrid: McGraw-Hill, 2021, pp. 411-432. ISBN 13-978-84-486-3190-1 ;\n  84-486-3190-0",
        "doi": "10.5281/zenodo.5704697",
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  Analysis and assessment of female characters in economic financial themes\nfilms that have appeared in some thirty relevant films, from the beginning of\ncinema, selected from various bibliographies. Descriptive, comparative and\nimpartial study. Quantitative techniques are applied to measure the influence\nof women in terms of protagonism, positive values, negative values, compliance\nor non-compliance with the Bechdel test, and results are evaluated. It is based\non two previous binding publications: Marzal et al. The crisis of the real.\nRepresentations of the 2008 financial crisis in contemporary audiovisual (2018)\nand Mart\\'in-de-Santos, Financial administration through cinema. Analytical and\ncritical study of the most interesting films for university education (20219.\nRealistic film versions are contrasted with actual events today.\n"
    },
    {
        "paper_id": 2112.04553,
        "authors": "Ben Hambly, Renyuan Xu and Huining Yang",
        "title": "Recent Advances in Reinforcement Learning in Finance",
        "comments": "60 pages, 1 figure",
        "journal-ref": null,
        "doi": "10.13140/RG.2.2.30278.40002",
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  The rapid changes in the finance industry due to the increasing amount of\ndata have revolutionized the techniques on data processing and data analysis\nand brought new theoretical and computational challenges. In contrast to\nclassical stochastic control theory and other analytical approaches for solving\nfinancial decision-making problems that heavily reply on model assumptions, new\ndevelopments from reinforcement learning (RL) are able to make full use of the\nlarge amount of financial data with fewer model assumptions and to improve\ndecisions in complex financial environments. This survey paper aims to review\nthe recent developments and use of RL approaches in finance. We give an\nintroduction to Markov decision processes, which is the setting for many of the\ncommonly used RL approaches. Various algorithms are then introduced with a\nfocus on value and policy based methods that do not require any model\nassumptions. Connections are made with neural networks to extend the framework\nto encompass deep RL algorithms. Our survey concludes by discussing the\napplication of these RL algorithms in a variety of decision-making problems in\nfinance, including optimal execution, portfolio optimization, option pricing\nand hedging, market making, smart order routing, and robo-advising.\n"
    },
    {
        "paper_id": 2112.04566,
        "authors": "Victor Olkhov",
        "title": "Theoretical Economics and the Second-Order Economic Theory. What is it?",
        "comments": "13 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The economic and financial variables of economic agents determine\nmacroeconomic variables. Current models consider agents' variables that are\ndetermined by the sums of values and volumes of agents' trades during some time\ninterval {\\Delta}. We call them first-order economic variables. We describe how\nthe volatilities and correlations of market trade values and volumes determine\nprice volatility. We argue that such a link requests consideration of agents'\neconomic variables of the second order that are composed of sums of squares of\nagents' transactions during {\\Delta}. Almost any variable of the first order\nshould be complemented by its second-order pair. Respectively, the sums of\nagents' second-order variables introduce macroeconomic variables of the second\norder. The description of the first- and second-order macroeconomic variables\nestablishes the subject of second-order economic theory. We highlight that the\ncomplexity of second-order economic theory essentially restricts any hopes for\nprecise predictions of price probability and, at best, could provide estimates\nof price volatility. That limits the predictions of price probability to\nGauss's approximations only.\n"
    },
    {
        "paper_id": 2112.04576,
        "authors": "Kumar Yashaswi",
        "title": "Adaptive calibration of Heston Model using PCRLB based switching Filter",
        "comments": "7 Pages, 5 Figures, 1 Table, Keywords- Stochastic volatility; Heston\n  Model; Normal MLE; Bayesian Filtering; Posterior Cramer-Rao Lower Bound\n  (PCRLB). arXiv admin note: text overlap with arXiv:2112.03193",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Stochastic volatility models have existed in Option pricing theory ever since\nthe crash of 1987 which violated the Black-Scholes model assumption of constant\nvolatility. Heston model is one such stochastic volatility model that is widely\nused for volatility estimation and option pricing. In this paper, we design a\nnovel method to estimate parameters of Heston model under state-space\nrepresentation using Bayesian filtering theory and Posterior Cramer-Rao Lower\nBound (PCRLB), integrating it with Normal Maximum Likelihood Estimation (NMLE)\nproposed in [1]. Several Bayesian filters like Extended Kalman Filter (EKF),\nUnscented Kalman Filter (UKF), Particle Filter (PF) are used for latent state\nand parameter estimation. We employ a switching strategy proposed in [2] for\nadaptive state estimation of the non-linear, discrete-time state-space model\n(SSM) like Heston model. We use a particle filter approximated PCRLB [3] based\nperformance measure to judge the best filter at each time step. We test our\nproposed framework on pricing data from S&P 500 and NSE Index, estimating the\nunderlying volatility and parameters from the index. Our proposed method is\ncompared with the VIX measure and historical volatility for both the indexes.\nThe results indicate an effective framework for estimating volatility\nadaptively with changing market dynamics.\n"
    },
    {
        "paper_id": 2112.04755,
        "authors": "Uta Pigorsch and Sebastian Sch\\\"afer",
        "title": "High-Dimensional Stock Portfolio Trading with Deep Reinforcement\n  Learning",
        "comments": "14 pages, 5 figures, 2 tables",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper proposes a Deep Reinforcement Learning algorithm for financial\nportfolio trading based on Deep Q-learning. The algorithm is capable of trading\nhigh-dimensional portfolios from cross-sectional datasets of any size which may\ninclude data gaps and non-unique history lengths in the assets. We sequentially\nset up environments by sampling one asset for each environment while rewarding\ninvestments with the resulting asset's return and cash reservation with the\naverage return of the set of assets. This enforces the agent to strategically\nassign capital to assets that it predicts to perform above-average. We apply\nour methodology in an out-of-sample analysis to 48 US stock portfolio setups,\nvarying in the number of stocks from ten up to 500 stocks, in the selection\ncriteria and in the level of transaction costs. The algorithm on average\noutperforms all considered passive and active benchmark investment strategies\nby a large margin using only one hyperparameter setup for all portfolios.\n"
    },
    {
        "paper_id": 2112.04824,
        "authors": "Nils Bertschinger, Axel A. Araneda",
        "title": "Cross-ownership as a structural explanation for rising correlations in\n  crisis times",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper, we examine the interlinkages among firms through a financial\nnetwork where cross-holdings on both equity and debt are allowed. We relate\nmathematically the correlation among equities with the unconditional\ncorrelation of the assets, the values of their business assets and the\nsensitivity of the network, particularly the $\\Delta$-Greek. We noticed also\nthis relation is independent of the Equities level. Besides, for the two-firms\ncase, we analytically demonstrate that the equities correlation is always\nhigher than the correlation of the assets; showing this issue by numerical\nillustrations. Finally, we study the relation between equity correlations and\nasset prices, where the model arrives to an increase in the former due to a\nfall in the assets.\n"
    },
    {
        "paper_id": 2112.05302,
        "authors": "Peter Reinhard Hansen, Zhuo Huang, Chen Tong, Tianyi Wang",
        "title": "Realized GARCH, CBOE VIX, and the Volatility Risk Premium",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  We show that the Realized GARCH model yields close-form expression for both\nthe Volatility Index (VIX) and the volatility risk premium (VRP). The Realized\nGARCH model is driven by two shocks, a return shock and a volatility shock, and\nthese are natural state variables in the stochastic discount factor (SDF). The\nvolatility shock endows the exponentially affine SDF with a compensation for\nvolatility risk. This leads to dissimilar dynamic properties under the physical\nand risk-neutral measures that can explain time-variation in the VRP. In an\nempirical application with the S&P 500 returns, the VIX, and the VRP, we find\nthat the Realized GARCH model significantly outperforms conventional GARCH\nmodels.\n"
    },
    {
        "paper_id": 2112.05308,
        "authors": "Chen Tong and Peter Reinhard Hansen and Zhuo Huang",
        "title": "Option Pricing with State-dependent Pricing Kernel",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  We introduce a new volatility model for option pricing that combines Markov\nswitching with the Realized GARCH framework. This leads to a novel pricing\nkernel with a state-dependent variance risk premium and a pricing formula for\nEuropean options, which is derived with an analytical approximation method. We\napply the Markov switching Realized GARCH model to S&P 500 index options from\n1990 to 2019 and find that investors' aversion to volatility-specific risk is\ntime-varying. The proposed framework outperforms competing models and reduces\n(in-sample and out-of-sample) option pricing errors by 15% or more.\n"
    },
    {
        "paper_id": 2112.05811,
        "authors": "Pengcheng You, Yan Jiang, Enoch Yeung, Dennice F. Gayme, Enrique\n  Mallada",
        "title": "On the Stability, Economic Efficiency and Incentive Compatibility of\n  Electricity Market Dynamics",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper focuses on the operation of an electricity market that accounts\nfor participants that bid at a sub-minute timescale. To that end, we model the\nmarket-clearing process as a dynamical system, called market dynamics, which is\ntemporally coupled with the grid frequency dynamics and is thus required to\nguarantee system-wide stability while meeting the system operational\nconstraints. We characterize participants as price-takers who rationally update\ntheir bids to maximize their utility in response to real-time schedules of\nprices and dispatch. For two common bidding mechanisms, based on quantity and\nprice, we identify a notion of alignment between participants' behavior and\nplanners' goals that leads to a saddle-based design of the market that\nguarantees convergence to a point meeting all operational constraints. We\nfurther explore cases where this alignment property does not hold and observe\nthat misaligned participants' bidding can destabilize the closed-loop system.\nWe thus design a regularized version of the market dynamics that recovers all\nthe desirable stability and steady-state performance guarantees. Numerical\ntests validate our results on the IEEE 39-bus system.\n"
    },
    {
        "paper_id": 2112.05822,
        "authors": "Kevin L. McKinney, John M. Abowd, and Hubert P. Janicki",
        "title": "U.S. Long-Term Earnings Outcomes by Sex, Race, Ethnicity, and Place of\n  Birth",
        "comments": "77 pages, 42 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  This paper is part of the Global Income Dynamics Project cross-country\ncomparison of earnings inequality, volatility, and mobility. Using data from\nthe U.S. Census Bureau's Longitudinal Employer-Household Dynamics (LEHD)\ninfrastructure files we produce a uniform set of earnings statistics for the\nU.S. From 1998 to 2019, we find U.S. earnings inequality has increased and\nvolatility has decreased. The combination of increased inequality and reduced\nvolatility suggest earnings growth differs substantially across different\ndemographic groups. We explore this further by estimating 12-year average\nearnings for a single cohort of age 25-54 eligible workers. Differences in\nlabor supply (hours paid and quarters worked) are found to explain almost 90%\nof the variation in worker earnings, although even after controlling for labor\nsupply substantial earnings differences across demographic groups remain\nunexplained. Using a quantile regression approach, we estimate counterfactual\nearnings distributions for each demographic group. We find that at the bottom\nof the earnings distribution differences in characteristics such as hours paid,\ngeographic division, industry, and education explain almost all the earnings\ngap, however above the median the contribution of the differences in the\nreturns to characteristics becomes the dominant component.\n"
    },
    {
        "paper_id": 2112.0629,
        "authors": "Stefan Bornholdt (Bremen University)",
        "title": "A q-spin Potts model of markets: Gain-loss asymmetry in stock indices as\n  an emergent phenomenon",
        "comments": "14 pages, 3 figures",
        "journal-ref": "Physica A 588 (2022) 126565",
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Spin models of markets inspired by physics models of magnetism, as the Ising\nmodel, allow for the study of the collective dynamics of interacting agents in\na market. The number of possible states has been mostly limited to two (buy or\nsell) or three options. However, herding effects of competing stocks and the\ncollective dynamics of a whole market may escape our reach in the simplest\nmodels. Here I study a q-spin Potts model version of a simple Ising market\nmodel to represent the dynamics of a stock market index in a spin model. As a\nresult, a self-organized gain-loss asymmetry in the time series of an index\nvariable composed of stocks in this market is observed.\n"
    },
    {
        "paper_id": 2112.06357,
        "authors": "Jorrit Gosens, Alex Turnbull, Frank Jotzo",
        "title": "An installation-level model of China's coal sector shows how its\n  decarbonization and energy security plans will reduce overseas coal imports",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-sa/4.0/",
        "abstract": "  China aims for net-zero carbon emissions by 2060, and an emissions peak\nbefore 2030. This will reduce its consumption of coal for power generation and\nsteel making. Simultaneously, China aims for improved energy security,\nprimarily with expanded domestic coal production and transport infrastructure.\nHere, we analyze effects of both these pressures on seaborne coal imports, with\na purpose-built model of China's coal production, transport, and consumption\nsystem with installation-level geospatial and technical detail. This represents\na 1000-fold increase in granularity versus earlier models, allowing\nrepresentation of aspects that have previously been obscured. We find that\nreduced Chinese coal consumption affects seaborne imports much more strongly\nthan domestic supply. Recent expansions of rail and port capacity, which reduce\ncosts of getting domestic coal to Southern coastal provinces, will further\nreduce demand for seaborne thermal coal and amplify the effect of\ndecarbonisation on coal imports. Seaborne coking coal imports are also likely\nto fall, because of expanded supply of cheap and high quality coking coal from\nneighbouring Mongolia.\n"
    },
    {
        "paper_id": 2112.06393,
        "authors": "Yi Liang and James Unwin",
        "title": "COVID-19 Forecasts via Stock Market Indicators",
        "comments": "11 pages, 9 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Reliable short term forecasting can provide potentially lifesaving insights\ninto logistical planning, and in particular, into the optimal allocation of\nresources such as hospital staff and equipment. By reinterpreting COVID-19\ndaily cases in terms of candlesticks, we are able to apply some of the most\npopular stock market technical indicators to obtain predictive power over the\ncourse of the pandemics. By providing a quantitative assessment of MACD, RSI,\nand candlestick analyses, we show their statistical significance in making\npredictions for both stock market data and WHO COVID-19 data. In particular, we\nshow the utility of this novel approach by considering the identification of\nthe beginnings of subsequent waves of the pandemic. Finally, our new methods\nare used to assess whether current health policies are impacting the growth in\nnew COVID-19 cases.\n"
    },
    {
        "paper_id": 2112.06534,
        "authors": "Ludger Overbeck and Florian Schindler",
        "title": "Scalar systemic risk measures and Aumann-Shapley allocations",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  We study two different contributions to the theory of (scalar) systemic risk\nmeasures. Namely the first aggregate or axiomatic approach and the first inject\ncapital approach. For this purpose we establish a general framework, which is\nrich enough to embed both approaches. It turns out that in most relevant\nsituations systemic risk measures of the first inject capital approach have a\nrepresentation in the more general axiomatic approach. Moreover, we study\ncapital allocation rules (CARs). In both situations there exist canonical ways\nto answer the capital allocation problem. Additionally, a capital allocation\nrule (CAR) in the spirit of Aumann-Shapley is introduced, which gives us the\nopportunity to compute systemic capital allocations regardless of the risk\nmeasurement approach. This CAR also serves as an instrument to compare both\napproaches and to identify commonalities.\n"
    },
    {
        "paper_id": 2112.06544,
        "authors": "Sebastiano Michele Zema, Giorgio Fagiolo, Tiziano Squartini, Diego\n  Garlaschelli",
        "title": "Mesoscopic Structure of the Stock Market and Portfolio Optimization",
        "comments": "21 pages, 8 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  The idiosyncratic (microscopic) and systemic (macroscopic) components of\nmarket structure have been shown to be responsible for the departure of the\noptimal mean-variance allocation from the heuristic `equally-weighted'\nportfolio. In this paper, we exploit clustering techniques derived from Random\nMatrix Theory (RMT) to study a third, intermediate (mesoscopic) market\nstructure that turns out to be the most stable over time and provides important\npractical insights from a portfolio management perspective. First, we\nillustrate the benefits, in terms of predicted and realized risk profiles, of\nconstructing portfolios by filtering out both random and systemic co-movements\nfrom the correlation matrix. Second, we redefine the portfolio optimization\nproblem in terms of stock clusters that emerge after filtering. Finally, we\npropose a new wealth allocation scheme that attaches equal importance to stocks\nbelonging to the same community and show that it further increases the\nreliability of the constructed portfolios. Results are robust across different\ntime spans, cross-sectional dimensions and set of constraints defining the\noptimization problem\n"
    },
    {
        "paper_id": 2112.06552,
        "authors": "Jaros{\\l}aw Kwapie\\'n, Marcin W\\k{a}torek, Stanis{\\l}aw Dro\\.zd\\.z",
        "title": "Cryptocurrency Market Consolidation in 2020--2021",
        "comments": null,
        "journal-ref": "Entropy 2021, 23(12), 1674",
        "doi": "10.3390/e23121674",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Time series of price returns for 80 of the most liquid cryptocurrencies\nlisted on Binance are investigated for the presence of detrended\ncross-correlations. A spectral analysis of the detrended correlation matrix and\na topological analysis of the minimal spanning trees calculated based on this\nmatrix are applied for different positions of a moving window. The\ncryptocurrencies become more strongly cross-correlated among themselves than\nthey used to be before. The average cross-correlations increase with time on a\nspecific time scale in a way that resembles the Epps effect amplification when\ngoing from past to present. The minimal spanning trees also change their\ntopology and, for the short time scales, they become more centralized with\nincreasing maximum node degrees, while for the long time scales they become\nmore distributed, but also more correlated at the same time. Apart from the\ninter-market dependencies, the detrended cross-correlations between the\ncryptocurrency market and some traditional markets, like the stock markets,\ncommodity markets, and Forex, are also analyzed. The cryptocurrency market\nshows higher levels of cross-correlations with the other markets during the\nsame turbulent periods, in which it is strongly cross-correlated itself.\n"
    },
    {
        "paper_id": 2112.06602,
        "authors": "Ling Wang, Mei Choi Chiu, Hoi Ying Wong",
        "title": "Time-consistent mean-variance reinsurance-investment problem with\n  long-range dependent mortality rate",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper investigates the time-consistent mean-variance\nreinsurance-investment (RI) problem faced by life insurers. Inspired by recent\nfindings that mortality rates exhibit long-range dependence (LRD), we examine\nthe effect of LRD on RI strategies. We adopt the Volterra mortality model\nproposed in Wang et al.(2021) to incorporate LRD into the mortality rate\nprocess and describe insurance claims using a compound Poisson process with the\nintensity represented by stochastic mortality rate. Under the open-loop\nequilibrium mean-variance criterion, we derive explicit equilibrium RI controls\nand study the uniqueness of these controls in cases of constant and\nstate-dependent risk aversion. We simultaneously resolve difficulties arising\nfrom unbounded non-Markovian parameters and sudden increases in the insurer's\nwealth process. We also use a numerical study to reveal the influence of LRD on\nequilibrium strategies.\n"
    },
    {
        "paper_id": 2112.06605,
        "authors": "Ge-zhi Wu, Da-ming You",
        "title": "Will enterprise digital transformation affect diversification strategy?",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  This paper empirically examines the impact of enterprise digital\ntransformation on the level of enterprise diversification. It is found that the\ndigital transformation of enterprises has significantly improved the level of\nenterprise diversification, and the conclusion has passed a series of\nrobustness tests and endogenous tests. Through mechanism analysis, we find that\nthe promotion effect of enterprise digital transformation on enterprise\ndiversification is mainly realized through market power channel and firm risk\nchannel, the pursuit of establishing market power, monopoly profits and\nchallenge the monopolistic position of market occupiers based on digital\ntransformation and the decentralization strategy to deal with the risks\nassociated with digital transformation are important reasons for enterprises to\nadopt diversification strategy under the background of digital transformation.\nAlthough the organization costs channel, transaction costs channel, block\nholder control channel, industry type and information asymmetry channel have\nsome influence on the main effect of this paper, they are not the main channel\nbecause they have not passed the inter group regression coefficient difference\ntest statistically.\n"
    },
    {
        "paper_id": 2112.06646,
        "authors": "Vincent Yuansang Zha",
        "title": "The Burst Market: the Next Leap for Humanity",
        "comments": "19 pages, no figure",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
        "abstract": "  Contemporary society grapples with a critical challenge in knowledge sharing:\nthe scarcity of rapid, yet specific advice from relevant individuals. This\nsituation underscores a deficiency in the existing labor market, hereafter\nreferred to as the Conventional Market (CM), which struggles to support \"Burst\nJobs\" - brief conversational tasks. This paper identifies high transaction\ncosts, owing to technological limitations, as the primary reason for this\ninefficiency, causing huge job opportunity losses and human intelligence\nunderutilization. In response, this study introduces the concept of an online\nBurst Market (BM), an innovative platform where individuals can instantaneously\noffer advice for a fee through video or audio conversations lasting as briefly\nas a few seconds or minutes. This paper examines the BM's potential to\nrevolutionize job creation, alleviate poverty, address aging society problem,\ncounteract AI-led unemployment, maximize human values with \"Lifelong Working\",\nand redefine the essence of job and the global economy.\n"
    },
    {
        "paper_id": 2112.06706,
        "authors": "Ling Wang, Kexin Chen, Mei Choi Chiu, Hoi Ying Wong",
        "title": "Optimal Expansion of Business Opportunity",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Any firm whose business strategy has an exposure constraint that limits its\npotential gain naturally considers expansion, as this can increase its\nexposure. We model business expansion as an enlargement of the opportunity set\nfor business policies. However, expansion is irreversible and has an\nopportunity cost attached. We use the expected optimization of utility to\nformulate this as a novel stochastic control problem combined with an optimal\nstopping time, and we derive an explicit solution for exponential utility. We\napply the framework to an investment and a reinsurance scenario. In the\ninvestment problem, the cost and incentives to increase the trading exposure\nare analyzed, while the optimal timing for an insurer to launch its reinsurance\nbusiness is investigated in the reinsurance problem. Our model predicts that\nthe additional income gained through business expansion is the key incentive\nfor a decision to expand. Interestingly, companies may have this incentive but\nare likely to wait for a period of time before expanding, although situations\nof zero opportunity cost or specific restrictive conditions on the model\nparameters are exceptions to waiting. The business policy remains on the\nboundary of the opportunity set before expansion during the waiting period. The\nlength of the waiting period is related to the opportunity cost, return, and\nrisk of the expanded business.\n"
    },
    {
        "paper_id": 2112.06708,
        "authors": "Martin Herdegen and David Hobson and Joseph Jerome",
        "title": "Proper solutions for Epstein-Zin Stochastic Differential Utility",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  In this article, we consider the optimal investment-consumption problem for\nan agent with preferences governed by Epstein--Zin stochastic differential\nutility (EZ-SDU) who invests in a constant-parameter Black-Scholes-Merton\nmarket over the infinite horizon. The parameter combinations that we consider\nin this paper are such that the risk aversion parameter $R$ and the elasticity\nof intertemporal complementarity $S$ satisfy $\\theta=\\frac{1-R}{1-S}>1$. In\nthis sense, this paper is complementary to Herdegen, Hobson and Jerome\n[arXiv:2107.06593].\n  The main novelty of the case $\\theta>1$ (as opposed to $\\theta\\in(0,1)$) is\nthat there is an infinite family of utility processes associated to every\nnonzero consumption stream. To deal with this issue, we introduce the\neconomically motivated notion of a proper utility process, where, roughly\nspeaking, a utility process is proper if it is nonzero whenever future\nconsumption is nonzero.\n  We then proceed to show that for a very wide class of consumption streams\n$C$, there exists a proper utility process $V$ associated to $C$. Furthermore,\nfor a wide class of consumption streams $C$, the proper utility process $V$ is\nunique. Finally, we solve the optimal investment-consumption problem in a\nconstant parameter financial market, where we optimise over the\nright-continuous attainable consumption streams that have a unique proper\nutility process associated to them.\n"
    },
    {
        "paper_id": 2112.06753,
        "authors": "Xiao-Yang Liu, Jingyang Rui, Jiechao Gao, Liuqing Yang, Hongyang Yang,\n  Zhaoran Wang, Christina Dan Wang, Jian Guo",
        "title": "FinRL-Meta: A Universe of Near-Real Market Environments for Data-Driven\n  Deep Reinforcement Learning in Quantitative Finance",
        "comments": "Workshop on Data Centric AI, 35th Conference on Neural Information\n  Processing Systems (NeurIPS 2021)",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Deep reinforcement learning (DRL) has shown huge potentials in building\nfinancial market simulators recently. However, due to the highly complex and\ndynamic nature of real-world markets, raw historical financial data often\ninvolve large noise and may not reflect the future of markets, degrading the\nfidelity of DRL-based market simulators. Moreover, the accuracy of DRL-based\nmarket simulators heavily relies on numerous and diverse DRL agents, which\nincreases demand for a universe of market environments and imposes a challenge\non simulation speed. In this paper, we present a FinRL-Meta framework that\nbuilds a universe of market environments for data-driven financial\nreinforcement learning. First, FinRL-Meta separates financial data processing\nfrom the design pipeline of DRL-based strategy and provides open-source data\nengineering tools for financial big data. Second, FinRL-Meta provides hundreds\nof market environments for various trading tasks. Third, FinRL-Meta enables\nmultiprocessing simulation and training by exploiting thousands of GPU cores.\nOur codes are available online at\nhttps://github.com/AI4Finance-Foundation/FinRL-Meta.\n"
    },
    {
        "paper_id": 2112.06807,
        "authors": "Jovanka Lili Matic, Natalie Packham, Wolfgang Karl H\\\"ardle",
        "title": "Hedging Cryptocurrency Options",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/publicdomain/zero/1.0/",
        "abstract": "  The cryptocurrency market is volatile, non-stationary and non-continuous.\nTogether with liquid derivatives markets, this poses a unique opportunity to\nstudy risk management, especially the hedging of options, in a turbulent\nmarket. We study the hedge behaviour and effectiveness for the class of affine\njump diffusion models and infinite activity Levy processes. First, market data\nis calibrated to stochastic volatility inspired (SVI)-implied volatility\nsurfaces to price options. To cover a wide range of market dynamics, we\ngenerate Monte Carlo price paths using an SVCJ model (stochastic volatility\nwith correlated jumps), a close-to-actual-market GARCH-filtered kernel density\nestimation as well as a historical backtest. In all three settings, options are\ndynamically hedged with Delta, Delta-Gamma, Delta-Vega and Minimum Variance\nstrategies. Including a wide range of market models allows to understand the\ntrade-off in the hedge performance between complete, but overly parsimonious\nmodels, and more complex, but incomplete models. The calibration results reveal\na strong indication for stochastic volatility, low jump frequency and evidence\nof infinite activity. Short-dated options are less sensitive to volatility or\nGamma hedges. For longer-dated options, tail risk is consistently reduced by\nmultiple-instrument hedges, in particular by employing complete market models\nwith stochastic volatility.\n"
    },
    {
        "paper_id": 2112.06823,
        "authors": "Magnus Wiese, Ben Wood, Alexandre Pachoud, Ralf Korn, Hans Buehler,\n  Phillip Murray, Lianjun Bai",
        "title": "Multi-Asset Spot and Option Market Simulation",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We construct realistic spot and equity option market simulators for a single\nunderlying on the basis of normalizing flows. We address the\nhigh-dimensionality of market observed call prices through an arbitrage-free\nautoencoder that approximates efficient low-dimensional representations of the\nprices while maintaining no static arbitrage in the reconstructed surface.\nGiven a multi-asset universe, we leverage the conditional invertibility\nproperty of normalizing flows and introduce a scalable method to calibrate the\njoint distribution of a set of independent simulators while preserving the\ndynamics of each simulator. Empirical results highlight the goodness of the\ncalibrated simulators and their fidelity.\n"
    },
    {
        "paper_id": 2112.07016,
        "authors": "Andrew Butler and Roy H. Kwon",
        "title": "Data-driven integration of norm-penalized mean-variance portfolios",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Mean-variance optimization (MVO) is known to be sensitive to estimation error\nin its inputs. Norm penalization of MVO programs is a regularization technique\nthat can mitigate the adverse effects of estimation error. We augment the\nstandard MVO program with a convex combination of parameterized $L_1$ and\n$L_2$-norm penalty functions. The resulting program is a parameterized\nquadratic program (QP) whose dual is a box-constrained QP. We make use of\nrecent advances in neural network architecture for differentiable QPs and\npresent a data-driven framework for optimizing parameterized norm-penalties to\nminimize the downstream MVO objective. We present a novel technique for\ncomputing the derivative of the optimal primal solution with respect to the\nparameterized $L_1$-norm penalty by implicit differentiation of the dual\nprogram. The primal solution is then recovered from the optimal dual variables.\nHistorical simulations using US stocks and global futures data demonstrate the\nbenefit of the data-driven optimization approach.\n"
    },
    {
        "paper_id": 2112.07218,
        "authors": "Di Ao, Jing Gao, Zhijie Lai, Sen Li",
        "title": "Regulating Transportation Network Companies with a Mixture of Autonomous\n  Vehicles and For-Hire Human Drivers",
        "comments": null,
        "journal-ref": "Transportation Research Part A: Policy and Practice, Volume 180,\n  2024, 103975, ISSN 0965-8564",
        "doi": "10.1016/j.tra.2024.103975",
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  This paper investigates the equity impacts of autonomous vehicles (AV) on\nfor-hire human drivers and passengers in a ride-hailing market, and examines\nregulation policies that protect human drivers and improve transport equity for\nride-hailing passengers. We consider a transportation network companies (TNC)\nthat employs a mixture of AVs and human drivers to provide ride-hailing\nservices. The TNC platform determines the spatial prices, fleet size, human\ndriver payments, and vehicle relocation strategies to maximize its profit,\nwhile individual passengers choose between different transport modes to\nminimize their travel costs. A market equilibrium model is proposed to capture\nthe interactions among passengers, human drivers, AVs, and TNC over the\ntransportation network. The overall problem is formulated as a non-concave\nprogram, and an algorithm is developed to derive its approximate solution with\na theoretical performance guarantee. Our study shows that TNC prioritizes AV\ndeployment in higher-demand areas to make a higher profit. As AVs flood into\nthese higher-demand areas, they compete with human drivers in the urban core\nand push them to relocate to suburbs. This leads to reduced earning\nopportunities for human drivers and increased spatial inequity for passengers.\nTo mitigate these concerns, we consider: (a) a minimum wage for human drivers;\nand (b) a restrictive pickup policy that prohibits AVs from picking up\npassengers in higher-demand areas. In the former case, we show that a minimum\nwage for human drivers will protect them from the negative impact of AVs with\nnegligible impacts on passengers. However, there exists a threshold beyond\nwhich the minimum wage will trigger the platform to replace the majority of\nhuman drivers with AVs.\n"
    },
    {
        "paper_id": 2112.07247,
        "authors": "Tim T. Pedersen, Mikael Skou Andersen, Marta Victoria, Gorm B.\n  Andresen",
        "title": "30.000 ways to reach 55% decarbonization of the European electricity\n  sector",
        "comments": null,
        "journal-ref": null,
        "doi": "10.1016/j.isci.2023.106677",
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Climate change mitigation is a global challenge that, however, needs to be\nresolved by national-level authorities, resembling a tragedy of the commons.\nThis paradox is reflected at European scale, as climate commitments are made by\nthe EU collectively, but implementation is the responsibility of individual\nMember States. Here, we investigate 30.000 near-optimal effort-sharing\nscenarios where the European electricity sector is decarbonized by at least 55%\nrelative to 1990, in line with 2030 ambitions. Using a highly detailed\nbrownfield electricity system optimization model, the optimal electricity\nsystem is simulated for a suite of effort-sharing scenarios. Results reveal\nlarge inequalities in the efforts required to decarbonize national electricity\nsectors, with some countries facing cost-optimal pathways to reach 55% emission\nreductions, while others are confronted with relatively high abatement costs.\nSpecifically, we find that several countries with modest or low levels of GDP\nper capita will experience high abatement costs, and when passed over into\nelectricity prices this may lead to increased energy poverty in certain parts\nof Europe\n"
    },
    {
        "paper_id": 2112.07268,
        "authors": "Jingwen Tan and Shixi Kang",
        "title": "Finding the Instrumental Variables of Household Registration: A\n  discussion of the impact of China's household registration system on the\n  citizenship of the migrant population",
        "comments": "12 pages, 2 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Due to the specificity of China's dualistic household registration system and\nthe differences in the rights and interests attached to it, household\nregistration is prevalent as a control variable in the empirical evidence. In\nthe context of family planning policies, this paper proposes to use family size\nand number of children as instrumental variables for household registration,\nand discusses qualitatively and statistically verifies their relevance and\nexogeneity, while empirically analyzing the impact of the household\nregistration system on citizenship of the mobile population. After controlling\nfor city, individual control variables and fixed effects, the following\nconclusions are drawn: family size and number of children pass the\nover-identification test when used as instrumental variables for household\nregistration; non-agricultural households have about 20.2% lower settlement\nintentions and 7.28% lower employment levels in inflow cities than agricultural\nhouseholds; the mechanism of the effect of the nature of household registration\non employment still holds for the non-mobile population group.\n"
    },
    {
        "paper_id": 2112.07273,
        "authors": "Jingwen Tan and Shixi Kang",
        "title": "Urban Housing Prices and Migration's Fertility Intentions: Based on the\n  2018 China Migrants' Dynamic Survey",
        "comments": "11 pages, 0 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  While the size of China's mobile population continues to expand, the\nfertility rate is significantly lower than the stable generation replacement\nlevel of the population, and the structural imbalance of human resource supply\nhas attracted widespread attention. This paper uses LPM and Probit models to\nestimate the impact of house prices on the fertility intentions of the mobile\npopulation based on data from the 2018 National Mobile Population Dynamics\nMonitoring Survey. The lagged land sales price is used as an instrumental\nvariable of house price to mitigate the potential endogeneity problem. The\nresults show that for every 100\\% increase in the ratio of house price to\nhousehold income of mobile population, the fertility intention of the female\nmobile population of working age at the inflow location will decrease by\n4.42\\%, and the marginal effect of relative house price on labor force\nfertility intention is EXP(-0.222); the sensitivity of mobile population\nfertility intention to house price is affected by the moderating effect of\ninfrastructure construction at the inflow location. The willingness to have\nchildren in the inflow area is higher for female migrants of working age with\nlower age, smaller family size and higher education. Based on the above\nfindings, the study attempts to provide a new practical perspective for the\nmainline institutional change and balanced economic development in China's\neconomic transition phase.\n"
    },
    {
        "paper_id": 2112.07277,
        "authors": "Louis Balzer and Ludovic Leclercq (Universit\\'e Gustave Eiffel, ENTPE)",
        "title": "Modal equilibrium of a tradable credit scheme with a trip-based MFD and\n  logit-based decision-making",
        "comments": "See final version published in TR Part C:\n  https://doi.org/10.1016/j.trc.2022.103642",
        "journal-ref": null,
        "doi": "10.1016/j.trc.2022.103642",
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  The literature about tradable credit schemes (TCS) as a demand management\nsystem alleviating congestion flourished in the past decade. Most proposed\nformulations are based on static models and thus do not account for the\ncongestion dynamics. This paper considers elastic demand and implements a TCS\nto foster modal shift by restricting the number of cars allowed in the network\nover the day. A trip-based Macroscopic Fundamental Diagram (MFD) model\nrepresents the traffic dynamics at the whole urban scale. We assume the users\nhave different OD pairs and choose between driving their car or riding the\ntransit following a logit model. We aim to compute the modal shares and credit\nprice at equilibrium under TCS. The travel times are linearized with respect to\nthe modal shares to improve the convergence. We then present a method to find\nthe credit charge minimizing the total travel time alone or combined with the\ncarbon emission. The proposed methodology is illustrated with a typical demand\nprofile from 7:00 to 10:00 for Lyon Metropolis. We show that traffic dynamics\nand trip heterogeneity matter when deriving the modal equilibrium under a TCS.\nA method is described to compute the linearization of the travel times and\ncompared against a classical descend method (MSA). The proposed linearization\nis a promising tool to circumvent the complexity of the implicit formulation of\nthe trip-based MFD. Under an optimized TCS, the total travel time decreases by\n17% and the carbon emission by 45% by increasing the PT share by 24 points.\n"
    },
    {
        "paper_id": 2112.07278,
        "authors": "Shuzhen Yang",
        "title": "Compensatory model for quantile estimation and application to VaR",
        "comments": "23 pages, 6 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In contrast to the usual procedure of estimating the distribution of a time\nseries and then obtaining the quantile from the distribution, we develop a\ncompensatory model to improve the quantile estimation under a given\ndistribution estimation. A novel penalty term is introduced in the compensatory\nmodel. We prove that the penalty term can control the convergence error of the\nquantile estimation of a given time series, and obtain an adaptive adjusted\nquantile estimation. Simulation and empirical analysis indicate that the\ncompensatory model can significantly improve the performance of the value at\nrisk (VaR) under a given distribution estimation.\n"
    },
    {
        "paper_id": 2112.07314,
        "authors": "Ritika Jain and Shreya Biswas",
        "title": "The road to safety- Examining the nexus between road infrastructure and\n  crime in rural India",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  This study examines the relationship between road infrastructure and crime\nrate in rural India using a nationally representative survey. On the one hand,\nbuilding roads in villages may increase connectivity, boost employment, and\nlead to better living standards, reducing criminal activities. On the other\nhand, if the benefits of roads are non-uniformly distributed among villagers,\nit may lead to higher inequality and possibly higher crime. We empirically test\nthe relationship using the two waves of the Indian Human Development Survey. We\nuse an instrumental variable estimation strategy and observe that building\nroads in rural parts of India has reduced crime. The findings are robust to\nrelaxing the strict instrument exogeneity condition and using alternate\nmeasures. On exploring the pathways, we find that improved street lighting,\nbetter public bus services and higher employment are a few of the direct\npotential channels through which road infrastructure impedes crime. We also\nfind a negative association between villages with roads and various types of\ninequality measures confirming the broad economic benefits of roads. Our study\nalso highlights that the negative impact of roads on crime is more pronounced\nin states with weaker institutions and higher income inequality.\n"
    },
    {
        "paper_id": 2112.07335,
        "authors": "Songyan Hou, Thomas Krabichler, Marcus Wunsch",
        "title": "Deep Partial Hedging",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Using techniques from deep learning (cf. [B\\\"uh+19]), we show that neural\nnetworks can be trained successfully to replicate the modified payoff functions\nthat were first derived in the context of partial hedging by [FL00]. Not only\ndoes this approach better accommodate the realistic setting of hedging in\ndiscrete time, it also allows for the inclusion of transaction costs as well as\ngeneral market dynamics.\n"
    },
    {
        "paper_id": 2112.07386,
        "authors": "Andrea Barbon, Angelo Ranaldo",
        "title": "On The Quality Of Cryptocurrency Markets: Centralized Versus\n  Decentralized Exchanges",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We compare the market quality of centralized crypto exchanges (CEXs) such as\nBinance and Kraken to decentralized blockchain-based venues (DEXs) such as\nUniswap v2 and v3. After discussing the microstructure of such exchanges, we\nanalyze two key aspects of market quality: transaction costs and deviations\nfrom the no-arbitrage condition. We find that CEXs and DEXs operate on roughly\nequal footing in terms of transaction costs, particularly in light of recent\ninnovations in DEX protocols. Moreover, while CEXs provide superior price\nefficiency, DEXs eliminate custodian risk. These complementary advantages may\nexplain why both market structures coexist.\n"
    },
    {
        "paper_id": 2112.07464,
        "authors": "Andrew Butler and Roy Kwon",
        "title": "Efficient differentiable quadratic programming layers: an ADMM approach",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Recent advances in neural-network architecture allow for seamless integration\nof convex optimization problems as differentiable layers in an end-to-end\ntrainable neural network. Integrating medium and large scale quadratic programs\ninto a deep neural network architecture, however, is challenging as solving\nquadratic programs exactly by interior-point methods has worst-case cubic\ncomplexity in the number of variables. In this paper, we present an alternative\nnetwork layer architecture based on the alternating direction method of\nmultipliers (ADMM) that is capable of scaling to problems with a moderately\nlarge number of variables. Backward differentiation is performed by implicit\ndifferentiation of the residual map of a modified fixed-point iteration.\nSimulated results demonstrate the computational advantage of the ADMM layer,\nwhich for medium scaled problems is approximately an order of magnitude faster\nthan the OptNet quadratic programming layer. Furthermore, our novel\nbackward-pass routine is efficient, from both a memory and computation\nstandpoint, in comparison to the standard approach based on unrolled\ndifferentiation or implicit differentiation of the KKT optimality conditions.\nWe conclude with examples from portfolio optimization in the integrated\nprediction and optimization paradigm.\n"
    },
    {
        "paper_id": 2112.07521,
        "authors": "Christian Bongiorno, Damien Challet",
        "title": "Non-linear shrinkage of the price return covariance matrix is far from\n  optimal for portfolio optimisation",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Portfolio optimization requires sophisticated covariance estimators that are\nable to filter out estimation noise. Non-linear shrinkage is a popular\nestimator based on how the Oracle eigenvalues can be computed using only data\nfrom the calibration window. Contrary to common belief, NLS is not optimal for\nportfolio optimization because it does not minimize the right cost function\nwhen the asset dependence structure is non-stationary. We instead derive the\noptimal target. Using historical data, we quantify by how much NLS can be\nimproved. Our findings reopen the question of how to build the covariance\nmatrix estimator for portfolio optimization in realistic conditions.\n"
    },
    {
        "paper_id": 2112.08071,
        "authors": "Dhruv Rawat, Sujay Patni and Ram Mehta",
        "title": "Stock prices and Macroeconomic indicators: Investigating a correlation\n  in Indian context",
        "comments": "Inappropriate methodology",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  The objective of this paper is to find the existence of a relationship\nbetween stock market prices and the fundamental macroeconomic indicators. We\nbuild a Vector Auto Regression (VAR) model comprising of nine major\nmacroeconomic indicators (interest rate, inflation, exchange rate, money\nsupply, gdp, fdi, trade-gdp ratio, oil prices, gold prices) and then try to\nforecast them for next 5 years. Finally we calculate cross-correlation of these\nforecasted values with the BSE Sensex closing price for each of those years. We\nfind very high correlation of the closing price with exchange rate and money\nsupply in the Indian economy.\n"
    },
    {
        "paper_id": 2112.08291,
        "authors": "Michele Azzone and Roberto Baviera",
        "title": "A fast Monte Carlo scheme for additive processes and option pricing",
        "comments": null,
        "journal-ref": "A fast Monte Carlo scheme for additive processes and option\n  pricing, 20, 31 (2023)",
        "doi": "10.1007/s10287-023-00463-1",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper, we present a very fast Monte Carlo scheme for additive\nprocesses: the computational time is of the same order of magnitude of standard\nalgorithms for Brownian motions. We analyze in detail numerical error sources\nand propose a technique that reduces the two major sources of error. We also\ncompare our results with a benchmark method: the jump simulation with Gaussian\napproximation. We show an application to additive normal tempered stable\nprocesses, a class of additive processes that calibrates ``exactly\" the implied\nvolatility surface.Numerical results are relevant. This fast algorithm is also\nan accurate tool for pricing path-dependent discretely-monitoring options with\nerrors of one bp or below.\n"
    },
    {
        "paper_id": 2112.08534,
        "authors": "Kieran Wood, Sven Giegerich, Stephen Roberts, Stefan Zohren",
        "title": "Trading with the Momentum Transformer: An Intelligent and Interpretable\n  Architecture",
        "comments": "included motivation for attention mechanism and additional\n  architecture details",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We introduce the Momentum Transformer, an attention-based deep-learning\narchitecture, which outperforms benchmark time-series momentum and\nmean-reversion trading strategies. Unlike state-of-the-art Long Short-Term\nMemory (LSTM) architectures, which are sequential in nature and tailored to\nlocal processing, an attention mechanism provides our architecture with a\ndirect connection to all previous time-steps. Our architecture, an\nattention-LSTM hybrid, enables us to learn longer-term dependencies, improves\nperformance when considering returns net of transaction costs and naturally\nadapts to new market regimes, such as during the SARS-CoV-2 crisis. Via the\nintroduction of multiple attention heads, we can capture concurrent regimes, or\ntemporal dynamics, which are occurring at different timescales. The Momentum\nTransformer is inherently interpretable, providing us with greater insights\ninto our deep-learning momentum trading strategy, including the importance of\ndifferent factors over time and the past time-steps which are of the greatest\nsignificance to the model.\n"
    },
    {
        "paper_id": 2112.09015,
        "authors": "Qinkai Chen, Christian-Yann Robert",
        "title": "Multivariate Realized Volatility Forecasting with Graph Neural Network",
        "comments": "13 pages, 6 tables, 4 figures",
        "journal-ref": null,
        "doi": "10.1145/3533271.3561663",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The existing publications demonstrate that the limit order book data is\nuseful in predicting short-term volatility in stock markets. Since stocks are\nnot independent, changes on one stock can also impact other related stocks. In\nthis paper, we are interested in forecasting short-term realized volatility in\na multivariate approach based on limit order book data and relational data. To\nachieve this goal, we introduce Graph Transformer Network for Volatility\nForecasting. The model allows to combine limit order book features and an\nunlimited number of temporal and cross-sectional relations from different\nsources. Through experiments based on about 500 stocks from S&P 500 index, we\nfind a better performance for our model than for other benchmarks.\n"
    },
    {
        "paper_id": 2112.09065,
        "authors": "Alberto Bracci, J\\\"orn Boehnke, Abeer ElBahrawy, Nicola Perra,\n  Alexander Teytelboym, Andrea Baronchelli",
        "title": "Macroscopic properties of buyer-seller networks in online marketplaces",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Online marketplaces are the main engines of legal and illegal e-commerce, yet\ntheir empirical properties are poorly understood due to the absence of\nlarge-scale data. We analyze two comprehensive datasets containing 245M\ntransactions (16B USD) that took place on online marketplaces between 2010 and\n2021, covering 28 dark web marketplaces, i.e., unregulated markets whose main\ncurrency is Bitcoin, and 144 product markets of one popular regulated\ne-commerce platform. We show that transactions in online marketplaces exhibit\nstrikingly similar patterns despite significant differences in language,\nlifetimes, products, regulation, and technology. Specifically, we find\nremarkable regularities in the distributions of transaction amounts, number of\ntransactions, inter-event times and time between first and last transactions.\nWe show that buyer behavior is affected by the memory of past interactions and\nuse this insight to propose a model of network formation reproducing our main\nempirical observations. Our findings have implications for understanding market\npower on online marketplaces as well as inter-marketplace competition, and\nprovide empirical foundation for theoretical economic models of online\nmarketplaces.\n"
    },
    {
        "paper_id": 2112.09342,
        "authors": "Takanori Adachi, Yusuke Naritomi",
        "title": "Discrete signature and its application to finance",
        "comments": "12 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Signatures, one of the key concepts of rough path theory, have recently\ngained prominence as a means to find appropriate feature sets in machine\nlearning systems. In this paper, in order to compute signatures directly from\ndiscrete data without going through the transformation to continuous data, we\nintroduced a discretized version of signatures, called \"flat discrete\nsignatures\". We showed that the flat discrete signatures can represent the\nquadratic variation that has a high relevance in financial applications. We\nalso introduced the concept of \"discrete signatures\" that is a generalization\nof \"flat discrete signatures\". This concept is defined to reflect the fact that\ndata closer to the current time is more important than older data, and is\nexpected to be applied to time series analysis. As an application of discrete\nsignatures, we took up a stock market related problem and succeeded in\nperforming a good estimation with fewer data points than before.\n"
    },
    {
        "paper_id": 2112.09465,
        "authors": "C\\'onall Kelly and Gabriel J. Lord",
        "title": "An adaptive splitting method for the Cox-Ingersoll-Ross process",
        "comments": "29 pages, 3 figures, 2 tables, published in Applied Numerical\n  Mathematics",
        "journal-ref": "Applied Numerical Mathematics, Volume 186, 2023, Pages 252-273",
        "doi": "10.1016/j.apnum.2023.01.014",
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  We propose a new splitting method for strong numerical solution of the\nCox-Ingersoll-Ross model. For this method, applied over both deterministic and\nadaptive random meshes, we prove a uniform moment bound and strong error\nresults of order $1/4$ in $L_1$ and $L_2$ for the parameter regime\n$\\kappa\\theta>\\sigma^2$. We then extend the new method to cover all parameter\nvalues by introducing a \\emph{soft zero} region (where the deterministic flow\ndetermines the approximation) giving a hybrid type method to deal with the\nreflecting boundary. From numerical simulations we observe a rate of order $1$\nwhen $\\kappa\\theta>\\sigma^2$ rather than $1/4$. Asymptotically, for large\nnoise, we observe that the rates of convergence decrease similarly to those of\nother schemes but that the proposed method making use of adaptive timestepping\ndisplays smaller error constants.\n"
    },
    {
        "paper_id": 2112.09478,
        "authors": "Johannes Jarke-Neuert, Grischa Perino, Henrike Schwickert",
        "title": "Free-Riding for Future: Field Experimental Evidence of Strategic\n  Substitutability in Climate Protest",
        "comments": "21 pages, 6 pages appendix, 4 figures",
        "journal-ref": "Nature Climate Change, 13, 1197-1202 (2023)",
        "doi": "10.1038/s41558-023-01833-y",
        "license": "http://creativecommons.org/publicdomain/zero/1.0/",
        "abstract": "  We test the hypothesis that protest participation decisions in an adult\npopulation of potential climate protesters are interdependent. Subjects\n(n=1,510) from the four largest German cities were recruited two weeks before\nprotest date. We measured participation (ex post) and beliefs about the other\nsubjects' participation (ex ante) in an online survey, used a randomized\ninformational intervention to induce exogenous variance in beliefs, and\nestimated the causal effect of a change in belief on the probability of\nparticipation using a control function approach. Participation decisions are\nfound to be strategic substitutes: a one percentage-point increase of belief\ncauses a .67 percentage-point decrease in the probability of participation in\nthe average subject.\n"
    },
    {
        "paper_id": 2112.09534,
        "authors": "Qi Chen and Chao Guo",
        "title": "Path Integral Method for Proportional Step and Proportional\n  Double-Barrier Step Option Pricing",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Path integral method in quantum mechanics provides a new thinking for barrier\noption pricing. For proportional step options, the option price changing\nprocess is similar to the one dimensional trapezoid potential barrier\nscattering problem in quantum mechanics; for double-barrier step options, the\noption price changing process is analogous to a particle moving in a finite\nsymmetric square potential well. Using path integral method, the analytical\nexpressions of pricing kernel and option price could be derived. Numerical\nresults of option price as a function of underlying price, potential and\nexercise price are shown, which are consistent with the results given by\nmathematical method.\n"
    },
    {
        "paper_id": 2112.09783,
        "authors": "Andrey Fradkin, David Holtz",
        "title": "More Reviews May Not Help: Evidence from Incentivized First Reviews on\n  Airbnb",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Online reviews are typically written by volunteers and, as a consequence,\ninformation about seller quality may be under-provided in digital marketplaces.\nWe study the extent of this under-provision in a large-scale randomized\nexperiment conducted by Airbnb. In this experiment, buyers are offered a coupon\nto review listings that have no prior reviews. The treatment induces additional\nreviews and these reviews tend to be more negative than reviews in the control\ngroup, consistent with selection bias in reviewing. Reviews induced by the\ntreatment result in a temporary increase in transactions but these transactions\nare for fewer nights, on average. The effects on transactions and nights per\ntransaction cancel out so that there is no detectable effect on total nights\nsold and revenue. Measures of transaction quality in the treatment group fall,\nsuggesting that incentivized reviews do not improve matching. We show how\nmarket conditions and the design of the reputation system can explain our\nfindings.\n"
    },
    {
        "paper_id": 2112.09807,
        "authors": "Hayden Brown",
        "title": "Dollar Cost Averaging Returns Estimation",
        "comments": "23 pages, 12 figures",
        "journal-ref": null,
        "doi": "10.1142/S0219024923500036",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Given a geometric Brownian motion wealth process, a log-Normal lower bound is\nconstructed for the returns of a regular investing schedule. The distribution\nparameters of this bound are computed recursively. For dollar cost averaging\n(equal amounts in equal time intervals), parameters are computed in closed\nform. A lump sum (single amount at time 0) investing schedule is described\nwhich achieves a terminal wealth distribution that matches the wealth\ndistribution indicated by the lower bound. Results are applied to annual\nreturns of the S&P Composite Index from the last 150 years. Among data analysis\nresults, the probability of negative returns is less than 2.5% when annual\ndollar cost averaging lasts over 40 years.\n"
    },
    {
        "paper_id": 2112.09816,
        "authors": "Yu Hu, Miguel Armada, Maria Jesus Sanchez",
        "title": "Potential utilization of Battery Energy Storage Systems (BESS) in the\n  major European electricity markets",
        "comments": null,
        "journal-ref": null,
        "doi": "10.1016/j.apenergy.2022.119512",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Given the declining cost of battery technology in the last decade, nowadays\nBESS becomes a more attractive solution in electrical power systems. The\nobjective of this work is to analyze the potential utilization of BESS in the\nmajor European electricity markets. A general payoff model for BESS operation\nis proposed to correctly address the operational flexibility of battery\nsystems. Utilization factors such as potentially profitable utilization time\nand rate are calculated for common applications including energy arbitrage and\nfrequency support services using real market information. The result shows that\nunder the current empirical estimation of the battery cost and lifetime, BESS\nis not feasible for energy arbitrage in most of the European electricity\nmarkets. However, BESS shows clearly and significantly higher potential in\nproviding frequency support services. The result suggests that, when the\nfrequency containment reserve is remunerable, the potentially profitable\nutilization of BESS has become already accretive in most of the European\ncountries. For example from January to September 2021, the potentially\nprofitable utilization rate has reached almost 100% for the FCR-N service in\nthe Danish market. Comparing the regional electricity markets in Europe, BESS\nhas shown significant potential in becoming a feasible solution in Central\nWestern Europe and parts of Northern Europe by providing frequency regulation\nservices. Meanwhile, in the British Isles and some other islanded local\nmarkets, a remarkable level of scarcity of flexibility has been revealed by the\ninvestigation, and the potential of BESS would also be considerably\nencouraging.\n"
    },
    {
        "paper_id": 2112.0985,
        "authors": "Takanori Ida, Takunori Ishihara, Koichiro Ito, Daido Kido, Toru\n  Kitagawa, Shosei Sakaguchi and Shusaku Sasaki",
        "title": "Paternalism, Autonomy, or Both? Experimental Evidence from Energy Saving\n  Programs",
        "comments": "46 pages, 8 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Identifying who should be treated is a central question in economics. There\nare two competing approaches to targeting - paternalistic and autonomous. In\nthe paternalistic approach, policymakers optimally target the policy given\nobservable individual characteristics. In contrast, the autonomous approach\nacknowledges that individuals may possess key unobservable information on\nheterogeneous policy impacts, and allows them to self-select into treatment. In\nthis paper, we propose a new approach that mixes paternalistic assignment and\nautonomous choice. Our approach uses individual characteristics and empirical\nwelfare maximization to identify who should be treated, untreated, and decide\nwhether to be treated themselves. We apply this method to design a targeting\npolicy for an energy saving programs using data collected in a randomized field\nexperiment. We show that optimally mixing paternalistic assignments and\nautonomous choice significantly improves the social welfare gain of the policy.\nExploiting random variation generated by the field experiment, we develop a\nmethod to estimate average treatment effects for each subgroup of individuals\nwho would make the same autonomous treatment choice. Our estimates confirm that\nthe estimated assignment policy optimally allocates individuals to be treated,\nuntreated, or choose themselves based on the relative merits of paternalistic\nassignments and autonomous choice for individuals types.\n"
    },
    {
        "paper_id": 2112.09959,
        "authors": "Viet Anh Nguyen and Soroosh Shafiee and Damir Filipovi\\'c and Daniel\n  Kuhn",
        "title": "Mean-Covariance Robust Risk Measurement",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  We introduce a universal framework for mean-covariance robust risk\nmeasurement and portfolio optimization. We model uncertainty in terms of the\nGelbrich distance on the mean-covariance space, along with prior structural\ninformation about the population distribution. Our approach is related to the\ntheory of optimal transport and exhibits superior statistical and computational\nproperties than existing models. We find that, for a large class of risk\nmeasures, mean-covariance robust portfolio optimization boils down to the\nMarkowitz model, subject to a regularization term given in closed form. This\nincludes the finance standards, value-at-risk and conditional value-at-risk,\nand can be solved highly efficiently.\n"
    },
    {
        "paper_id": 2112.10084,
        "authors": "Guijin Son and Joocheol Kim",
        "title": "Neural Networks for Delta Hedging",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  The Black-Scholes model, defined under the assumption of a perfect financial\nmarket, theoretically creates a flawless hedging strategy allowing the trader\nto evade risks in a portfolio of options. However, the concept of a \"perfect\nfinancial market,\" which requires zero transaction and continuous trading, is\nchallenging to meet in the real world. Despite such widely known limitations,\nacademics have failed to develop alternative models successful enough to be\nlong-established. In this paper, we explore the landscape of Deep Neural\nNetworks(DNN) based hedging systems by testing the hedging capacity of the\nfollowing neural architectures: Recurrent Neural Networks, Temporal\nConvolutional Networks, Attention Networks, and Span Multi-Layer Perceptron\nNetworks. In addition, we attempt to achieve even more promising results by\ncombining traditional derivative hedging models with DNN based approaches.\nLastly, we construct \\textbf{NNHedge}, a deep learning framework that provides\nseamless pipelines for model development and assessment for the experiments.\n"
    },
    {
        "paper_id": 2112.10139,
        "authors": "Yanqing Ma, Carmine Ventre, Maria Polukarov",
        "title": "Denoised Labels for Financial Time-Series Data via Self-Supervised\n  Learning",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The introduction of electronic trading platforms effectively changed the\norganisation of traditional systemic trading from quote-driven markets into\norder-driven markets. Its convenience led to an exponentially increasing amount\nof financial data, which is however hard to use for the prediction of future\nprices, due to the low signal-to-noise ratio and the non-stationarity of\nfinancial time series. Simpler classification tasks -- where the goal is to\npredict the directions of future price movement -- via supervised learning\nalgorithms, need sufficiently reliable labels to generalise well. Labelling\nfinancial data is however less well defined than other domains: did the price\ngo up because of noise or because of signal? The existing labelling methods\nhave limited countermeasures against noise and limited effects in improving\nlearning algorithms. This work takes inspiration from image classification in\ntrading and success in self-supervised learning. We investigate the idea of\napplying computer vision techniques to financial time-series to reduce the\nnoise exposure and hence generate correct labels. We look at the label\ngeneration as the pretext task of a self-supervised learning approach and\ncompare the naive (and noisy) labels, commonly used in the literature, with the\nlabels generated by a denoising autoencoder for the same downstream\nclassification task. Our results show that our denoised labels improve the\nperformances of the downstream learning algorithm, for both small and large\ndatasets. We further show that the signals we obtain can be used to effectively\ntrade with binary strategies. We suggest that with proposed techniques,\nself-supervised learning constitutes a powerful framework for generating\n\"better\" financial labels that are useful for studying the underlying patterns\nof the market.\n"
    },
    {
        "paper_id": 2112.10209,
        "authors": "F.G. Bellora, G. Mazzei, M. Maurette",
        "title": "Option Pricing Model with Transaction Costs",
        "comments": "5 pages",
        "journal-ref": "MACI 6 2017 p.569-573",
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
        "abstract": "  The author presents alternatives to the Black-Scholes european call option\npricing model by incorporating different transaction cost structures in the\nreplicating strategy. In particular, an exponentially decreasing structure is\nproposed and developed.\n"
    },
    {
        "paper_id": 2112.10213,
        "authors": "Ren\\'e A\\\"id, Lamia Ben Ajmia, M'hamed Ga\\\"igi, Mohamed Mnif",
        "title": "Nonzero-sum stochastic impulse games with an application in competitive\n  retail energy markets",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-sa/4.0/",
        "abstract": "  We study a nonzero-sum stochastic differential game with both players\nadopting impulse controls, on a finite time horizon. The objective of each\nplayer is to maximize her total expected discounted profits. The resolution\nmethodology relies on the connection between Nash equilibrium and the\ncorresponding system of quasi-variational inequalities (QVIs in short). We\nprove, by means of the weak dynamic programming principle for the stochastic\ndifferential game, that the value function of each player is a constrained\nviscosity solution to the associated QVIs system in the class of linear growth\nfunctions. We also introduce a family of value functions converging to our\nvalue function of each player, and which is characterized as the unique\nconstrained viscosity solutions of an approximation of our QVIs system. This\nconvergence result is useful for numerical purpose. We apply a probabilistic\nnumerical scheme which approximates the solution of the QVIs system to the case\nof the competition between two electricity retailers. We show how our model\nreproduces the qualitative behaviour of electricity retail competition.\n"
    },
    {
        "paper_id": 2112.10447,
        "authors": "Battulga Gankhuu",
        "title": "Rainbow Options under Bayesian MS-VAR Process",
        "comments": "This version has some mistakes. After correction, we published it\n  another journal",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  This paper presents pricing and hedging methods for rainbow options and\nlookback options under Bayesian Markov-Switching Vector Autoregressive\n(MS--VAR) process. Here we assumed that a regime-switching process is generated\nby a homogeneous Markov process. An advantage of our model is it depends on\neconomic variables and simple as compared with previous existing papers.\n"
    },
    {
        "paper_id": 2112.10672,
        "authors": "Jorgen Vitting Andersen, Roy Cerqueti, Jessica Riccioni",
        "title": "Rational expectations as a tool for predicting failure of weighted\n  k-out-of-n reliability systems",
        "comments": "28 pages, 7 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Here we introduce the idea of using rational expectations, a core concept in\neconomics and finance, as a tool to predict the optimal failure time for a wide\nclass of weighted k-out-of-n reliability systems. We illustrate the concept by\napplying it to systems which have components with heterogeneous failure times.\nDepending on the heterogeneous distributions of component failure, we find\ndifferent measures to be optimal for predicting the failure time of the total\nsystem. We give examples of how, as a given system deteriorates over time, one\ncan issue different optimal predictions of system failure by choosing among a\nset of time-dependent measures.\n"
    },
    {
        "paper_id": 2112.11059,
        "authors": "Maximilien Germain (EDF R\\&D OSIRIS, EDF R\\&D, EDF, LPSM (UMR\\_8001)),\n  Huy\\^en Pham (LPSM (UMR\\_8001), CREST, FiME Lab), Xavier Warin (EDF R\\&D\n  OSIRIS, EDF R\\&D, EDF, FiME Lab)",
        "title": "A level-set approach to the control of state-constrained McKean-Vlasov\n  equations: application to renewable energy storage and portfolio selection",
        "comments": "To appear in Numerical Algebra, Control and Optimization",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We consider the control of McKean-Vlasov dynamics (or mean-field control)\nwith probabilistic state constraints. We rely on a level-set approach which\nprovides a representation of the constrained problem in terms of an\nunconstrained one with exact penalization and running maximum or integral cost.\nThe method is then extended to the common noise setting. Our work extends\n(Bokanowski, Picarelli, and Zidani, SIAM J. Control Optim. 54.5 (2016), pp.\n2568--2593) and (Bokanowski, Picarelli, and Zidani, Appl. Math. Optim. 71\n(2015), pp. 125--163) to a mean-field setting. The reformulation as an\nunconstrained problem is particularly suitable for the numerical resolution of\nthe problem, that is achieved from an extension of a machine learning algorithm\nfrom (Carmona, Lauri{\\`e}re, arXiv:1908.01613 to appear in Ann. Appl. Prob.,\n2019). A first application concerns the storage of renewable electricity in the\npresence of mean-field price impact and another one focuses on a mean-variance\nportfolio selection problem with probabilistic constraints on the wealth. We\nalso illustrate our approach for a direct numerical resolution of the primal\nMarkowitz continuous-time problem without relying on duality.\n"
    },
    {
        "paper_id": 2112.11263,
        "authors": "Luis Delgado, G\\'erald Gurtner, Tatjana Boli\\'c, Lorenzo Castelli",
        "title": "Estimating economic severity of Air Traffic Flow Management regulations",
        "comments": "19 pages, 14 figures",
        "journal-ref": "Transportation Research Part C: Emerging Technologies, 125, 103054\n  (2021)",
        "doi": "10.1016/j.trc.2021.103054",
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  The development of trajectory-based operations and the rolling network\noperations plan in European air traffic management network implies a move\ntowards more collaborative, strategic flight planning. This opens up the\npossibility for inclusion of additional information in the collaborative\ndecision-making process. With that in mind, we define the indicator for the\neconomic risk of network elements (e.g., sectors or airports) as the expected\ncosts that the elements impose on airspace users due to Air Traffic Flow\nManagement (ATFM) regulations. The definition of the indicator is based on the\nanalysis of historical ATFM regulations data, that provides an indication of\nthe risk of accruing delay. This risk of delay is translated into a monetary\nrisk for the airspace users, creating the new metric of the economic risk of a\ngiven airspace element. We then use some machine learning techniques to find\nthe parameters leading to this economic risk. The metric is accompanied by an\nindication of the accuracy of the delay cost prediction model. Lastly, the\neconomic risk is transformed into a qualitative economic severity\nclassification. The economic risks and consequently economic severity can be\nestimated for different temporal horizons and time periods providing an\nindicator which can be used by Air Navigation Service Providers to identify\nareas which might need the implementation of strategic measures (e.g.,\nresectorisation or capacity provision change), and by Airspace Users to\nconsider operation of routes which use specific airspace regions.\n"
    },
    {
        "paper_id": 2112.11265,
        "authors": "Marcus C. Christiansen",
        "title": "On the decomposition of an insurer's profits and losses",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Current reporting standards for insurers require a decomposition of observed\nprofits and losses in such a way that changes in the insurer's balance sheet\ncan be attributed to specified risk factors. Generating such a decomposition is\na nontrivial task because balance sheets generally depend on the risk factors\nin a non-linear way. This paper starts from an axiomatic perspective on profit\nand loss decompositions and finds that the axioms necessarily lead to\ninfinitesimal sequential updating (ISU) decompositions, provided that the\nlatter exist and are stable, whereas the current practice is rather to use\nsequential updating (SU) decompositions. The generality of the axiomatic\napproach makes the results useful also beyond insurance applications wherever\nprofits and losses shall be additively decomposed in a risk-oriented manner.\n"
    },
    {
        "paper_id": 2112.11338,
        "authors": "Olukunle O. Owolabi, Toryn L. J. Schafer, Georgia E. Smits, Sanhita\n  Sengupta, Sean E. Ryan, Lan Wang, David S. Matteson, Mila Getmansky Sherman,\n  Deborah A. Sunter",
        "title": "Role of Variable Renewable Energy Penetration on Electricity Price and\n  its Volatility Across Independent System Operators in the United States",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The U.S. electrical grid has undergone substantial transformation with\nincreased penetration of wind and solar -- forms of variable renewable energy\n(VRE). Despite the benefits of VRE for decarbonization, it has garnered some\ncontroversy for inducing unwanted effects in regional electricity markets. In\nthis study, the role of VRE penetration is examined on the system electricity\nprice and price volatility based on hourly, real-time, historical data from six\nIndependent System Operators (ISOs) in the U.S. using quantile and skew\nt-distribution regressions. After correcting for temporal effects, we found an\nincrease in VRE penetration is associated with decrease in system electricity\nprice in all ISOs studied. The increase in VRE penetration is associated with\ndecrease in temporal price volatility in five out of six ISOs studied. The\nrelationships are non-linear. These results are consistent with the modern\nportfolio theory where diverse volatile assets may lead to more stable and less\nrisky portfolios.\n"
    },
    {
        "paper_id": 2112.11499,
        "authors": "Attila Lajos Makai, Szabolcs R\\'amh\\'ap",
        "title": "The Changing Role of Entrepreneurial Universities in the Altering\n  Innovation Policy: Opportunities Arising from the Paradigm Change in Light of\n  the Experience of Sz\\'echenyi Istv\\'an University",
        "comments": null,
        "journal-ref": "POLG\\'ARI SZEMLE: GAZDAS\\'AGI \\'ES T\\'ARSADALMI FOLY\\'OIRAT 16 :\n  Special Issue pp. 297-313. , 17 p. (2020)",
        "doi": "10.24307/psz.2020.1219",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The progress made by the entrepreneurial university, which is a newly\nemerging category in Hungarian higher education after its change of model, has\nnot only deepened relations between universities and the industry and\nintensified the technology and knowledge transfer processes, but also increased\nthe role of universities in shaping regional innovation policy. This\ntransformation places co-operation between the actors of the regional\ninnovation ecosystem and the relationships between the economic, governmental\nand academic systems into a new framework. The purpose of this paper is to\ndescribe the process of the change in the model through a specific example, and\nto outline the future possibilities of university involvement in the currently\nchanging Hungarian innovation policy system.\n"
    },
    {
        "paper_id": 2112.11562,
        "authors": "Timo Mitze and Teemu Makkonen",
        "title": "Can large-scale R&I funding stimulate post-crisis recovery growth?\n  Evidence for Finland during COVID-19",
        "comments": "30 pages, 6 figures, 5 tables",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  The COVID-19 pandemic and subsequent public health restrictions led to a\nsignificant slump in economic activities around the globe. This slump has met\nby various policy actions to cushion the detrimental socio-economic\nconsequences of the COVID-19 crisis and eventually bring the economy back on\ntrack. We provide an ex-ante evaluation of the effectiveness of a massive\nincrease in research and innovation (R&I) funding in Finland to stimulate\npost-crisis recovery growth through an increase in R&I activities of Finnish\nfirms. We make use of the fact that novel R&I grants for firms in disruptive\ncircumstances granted in 2020 were allocated through established R&I policy\nchannels. This allows us to estimate the structural link between R&I funding\nand economic growth for Finnish NUTS-3 regions using pre-COVID-19 data.\nEstimates are then used to forecast regional recovery growth out of sample and\nto quantify the growth contribution of R&I funding. Depending on the chosen\nscenario, our forecasts point to a mean recovery growth rate of GDP between\n2-4% in 2021 after a decline of up to -2.5% in 2020. R&I funding constitutes a\nsignificant pillar of the recovery process with mean contributions in terms of\nGDP growth of between 0.4% and 1%.\n"
    },
    {
        "paper_id": 2112.11563,
        "authors": "Tom\\'a\\v{s} Evan and Vladim\\'ir Hol\\'y",
        "title": "Cultural Diversity and Its Impact on Governance",
        "comments": null,
        "journal-ref": "(2023) Socio-Economic Planning Sciences, 89, 101681",
        "doi": "10.1016/j.seps.2023.101681",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Hofstede's six cultural dimensions make it possible to measure the culture of\ncountries but are criticized for assuming the homogeneity of each country. In\nthis paper, we propose two measures based on Hofstede's cultural dimensions\nwhich take into account the heterogeneous structure of citizens with respect to\ntheir countries of origin. Using these improved measures, we study the\ninfluence of heterogeneous culture and cultural diversity on the quality of\ninstitutions measured by the six worldwide governance indicators. We use a\nlinear regression model allowing for dependence in spatial and temporal\ndimensions as well as high correlation between the governance indicators. Our\nresults show that the effect of cultural diversity improves some of the\ngovernance indicators while worsening others depending on the individual\nHofstede cultural dimension.\n"
    },
    {
        "paper_id": 2112.11564,
        "authors": "Reka Sundaram-Stukel and Richard J Davidson",
        "title": "Associational and plausible causal effects of COVID-19 public health\n  policies on economic and mental distress",
        "comments": "Pages 8, figures 2 in main text",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Background The COVID-19 pandemic has increased mental distress globally. The\nproportion of people reporting anxiety is 26%, and depression is 34% points.\nDisentangling associational and causal contributions of behavior, COVID-19\ncases, and economic distress on mental distress will dictate different\nmitigation strategies to reduce long-term pandemic-related mental distress.\nMethods We use the Household Pulse Survey (HPS) April 2020 to February 2021\ndata to examine mental distress among U.S. citizens attributable to COVID-19.\nWe combined HPS survey data with publicly available state-level weekly:\nCOVID-19 case and death data from the Centers for Disease Control, public\npolicies, and Apple and Google mobility data. Finally, we constructed economic\nand mental distress measures to estimate structural models with lag dependent\nvariables to tease out public health policies' associational and causal path\ncoefficients on economic and mental distress. Findings From April 2020 to\nFebruary 2021, we found that anxiety and depression had steadily climbed in the\nU.S. By design, mobility restrictions primarily affected public health policies\nwhere businesses and restaurants absorbed the biggest hit. Period t-1 COVID-19\ncases increased job loss by 4.1% and economic distress by 6.3% points in the\nsame period. Job-loss and housing insecurity in t-1 increased period t mental\ndistress by 29.1% and 32.7%, respectively. However, t-1 food insecurity\ndecreased mental distress by 4.9% in time t. The pandemic-related potential\ncausal path coefficient of period t-1 economic distress on period t depression\nis 57.8%, and anxiety is 55.9%. Thus, we show that period t-1 COVID-19 case\ninformation, behavior, and economic distress may be causally associated with\npandemic related period t mental distress.\n"
    },
    {
        "paper_id": 2112.11565,
        "authors": "Shyam Raman, Paul Lushenko, Sarah Kreps",
        "title": "Double Standards: The Implications of Near Certainty Drone Strikes in\n  Pakistan",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In 2013, U.S. President Barack Obama announced a policy to minimize civilian\ncasualties following drone strikes in undeclared theaters of war. The policy\ncalibrated Obamas approval of strikes against the near certainty of no civilian\ncasualties. Scholars do not empirically study the merits of Obamas policy.\nRather, they rely on descriptive trends for civilian casualties in Pakistan to\njustify competing claims for the policys impact. We provide a novel estimate\nfor the impact of Obamas policy for civilian casualties in Pakistan following\nU.S. drone strikes. We employ a regression discontinuity design to estimate the\neffect of Obamas policy for civilian casualties, strike precision, and adverted\ncivilian casualties. We find a discontinuity in civilian casualties\napproximately two years before Obamas policy announcement, corroborating our\nprimary research including interviews with senior officials responsible for\nimplementing the near certainty standard. After confirming the sharp cutoff, we\nestimate the policy resulted in a reduction of 12 civilian deaths per month or\n2 casualties per strike. The policy also enhanced the precision of U.S. drone\nstrikes to the point that they only killed the intended targets. Finally, we\nuse a Monte Carlo simulation to estimate that the policy adverted 320 civilian\ncasualties. We then conduct a Value of Statistical Life calculation to show\nthat the adverted civilian casualties represent a gain of 80 to 260 million\nU.S. dollars. In addition to conditioning social and political outcomes, then,\nthe near certainty standard also imposed economic implications that are much\nless studied.\n"
    },
    {
        "paper_id": 2112.11808,
        "authors": "Damiano Brigo, Federico Graceffa and Alexander Kalinin",
        "title": "Mild to classical solutions for XVA equations under stochastic\n  volatility",
        "comments": null,
        "journal-ref": null,
        "doi": "10.1137/22M1506882",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We extend the valuation of contingent claims in presence of default,\ncollateral and funding to a random functional setting and characterise\npre-default value processes by martingales. Pre-default value semimartingales\ncan also be described by BSDEs with random path-dependent coefficients and\nmartingales as drivers. En route, we generalise previous settings by relaxing\nconditions on the available market information, allowing for an arbitrary\ndefault-free filtration and constructing a broad class of default times.\nMoreover, under stochastic volatility, we characterise pre-default value\nprocesses via mild solutions to parabolic semilinear PDEs and give sufficient\nconditions for mild solutions to exist uniquely and to be classical.\n"
    },
    {
        "paper_id": 2112.11822,
        "authors": "Francesco Ciampi, Monica Faraoni, Jacopo Ballerini, Francesco Meli",
        "title": "The co-evolutionary relationship between digitalization and\n  organizational agility: Ongoing debates, theoretical developments and future\n  research perspectives",
        "comments": null,
        "journal-ref": "Technological Forecasting and Social Change, Vol. 176, 2022,\n  121383",
        "doi": "10.1016/j.techfore.2021.121383",
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  This study is the first to provide a systematic review of the literature\nfocused on the relationship between digitalization and organizational agility\n(OA). It applies the bibliographic coupling method to 171 peer-reviewed\ncontributions published by 30 June 2021. It uses the digitalization perspective\nto investigate the enablers, barriers and benefits of processes aimed at\nproviding firms with the agility required to effectively face increasingly\nturbulent environments. Three different, though interconnected, thematic\nclusters are discovered and analysed, respectively focusing on big-data\nanalytic capabilities as crucial drivers of OA, the relationship between\ndigitalization and agility at a supply chain level, and the role of information\ntechnology capabilities in improving OA. By adopting a dynamic capabilities\nperspective, this study overcomes the traditional view, which mainly considers\ndigital capabilities enablers of OA, rather than as possible outcomes. Our\nfindings reveal that, in addition to being complex, the relationship between\ndigitalization and OA has a bidirectional character. This study also identifies\nextant research gaps and develops 13 original research propositions on possible\nfuture research pathways and new managerial solutions.\n"
    },
    {
        "paper_id": 2112.11867,
        "authors": "Emily Schulte, Fabian Scheller, Wilmer Pasut, Thomas Bruckner",
        "title": "Product traits, decision-makers, and household low-carbon technology\n  adoptions: moving beyond single empirical studies",
        "comments": null,
        "journal-ref": "Energy Research & Social Science 83 (2022): 102313",
        "doi": "10.1016/j.erss.2021.102313",
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  Although single empirical studies provide important insights into who adopts\na specific LCT for what reason, fundamental questions concerning the relations\nbetween decision subject (= who decides), decision object (= what is decided\nupon) and context (= when and where it is decided) remain unanswered. In this\npaper, this research gap is addressed by deriving a decision framework for\nresidential decision-making, suggesting that traits of decision subject and\nobject are determinants of financial, environmental, symbolic, normative,\neffort and technical considerations preceding adoption. Thereafter, the\ndecision framework is initially verified by employing literature on the\nadoption of photovoltaic systems, energy-efficient appliances and green\ntariffs. Of the six proposed relations, two could be confirmed (financial and\nenvironmental), one could be rejected (effort), and three could neither be\nconfirmed nor rejected due to lacking evidence. Future research on LCT adoption\ncould use the decision framework as a guidepost to establish a more coordinated\nand integrated approach, ultimately allowing to address fundamental questions.\n"
    },
    {
        "paper_id": 2112.11931,
        "authors": "Attila Lajos Makai",
        "title": "Startup Ecosystem Rankings",
        "comments": null,
        "journal-ref": "HUNGARIAN STATISTICAL REVIEW: JOURNAL OF THE HUNGARIAN CENTRAL\n  STATISTICAL OFFICE 4 : 2 pp. 70-94. , 25 p. (2021)",
        "doi": "10.35618/hsr2021.02.en070",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The number, importance, and popularity of rankings measuring innovation\nperformance and the strength and resources of ecosystems that provide its\nspatial framework are on an increasing trend globally. In addition to\ninfluencing the specific decisions taken by economic actors, these rankings\nsignificantly impact the development of innovation-related policies at\nregional, national, and international levels. The importance of startup\necosystems is proven by the growing scientific interest, which is demonstrated\nby the increasing number of related scientific articles. The concept of the\nstartup ecosystem is a relatively new category, the application of which in\neveryday and scientific life has been gaining ground since the end of the\n2000s. In parallel, of course, the demand for measurability and comparability\nhas emerged among decision-makers and scholars. This demand is met by startup\necosystem rankings, which now measure and rank the performance of individual\necosystems on a continental and global scale. However, while the number of\nscientific publications examining rankings related to higher education,\neconomic performance, or even innovation, can be measured in the order of\nthousands, scientific research has so far rarely or tangentially addressed the\nrankings of startup ecosystems. This study and the related research intend to\nfill this gap by presenting and analysing the characteristics of global\nrankings and identifying possible future research directions.\n"
    },
    {
        "paper_id": 2112.11963,
        "authors": "Dena Firoozi, Arvind V Shrivats, Sebastian Jaimungal",
        "title": "Principal agent mean field games in REC markets",
        "comments": "Work in progress",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Principal agent games are a growing area of research which focuses on the\noptimal behaviour of a principal and an agent, with the former contracting work\nfrom the latter, in return for providing a monetary award. While this field\ncanonically considers a single agent, the situation where multiple agents, or\neven an infinite amount of agents are contracted by a principal are growing in\nprominence and pose interesting and realistic problems. Here, agents form a\nNash equilibrium among themselves, and a Stackelberg equilibrium between\nthemselves as a collective and the principal.\n  We apply this framework to the problem of implementing Renewable Energy\nCertificate (REC) markets, where the principal requires regulated firms (power\ngenerators) to pay a non-compliance penalty which is inversely proportional to\nthe amount of RECs they have. RECs can be obtained by generating electricity\nfrom clean sources or purchasing on the market. The agents react to this\npenalty and optimize their behaviours to navigate the system at minimum cost.\nIn the agents' model we incorporate market clearing as well as agent\nheterogeneity. For a given market design, we find the Nash equilibrium among\nagents using techniques from mean field games. We then use techniques from\nextended McKean-Vlasov control problems to solve the principal (regulators)\nproblem, who aim to choose the penalty function in such a way that balances\nenvironmental and revenue impacts optimally. We find through these techniques\nthat the optimal penalty function is linear in the agents' state, suggesting\nthe optimal emissions regulation market is more akin to a tax or rebate,\nregardless of the principal's utility function.\n"
    },
    {
        "paper_id": 2112.11968,
        "authors": "Edoardo Berton and Lorenzo Mercuri",
        "title": "An Efficient Unified Approach for Spread Option Pricing in a Copula\n  Market Model",
        "comments": null,
        "journal-ref": null,
        "doi": "10.1007/s10479-023-05549-2",
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  In this study, we propose a new formula for spread option pricing with the\ndependence of two assets described by a copula function. The advantage of the\nproposed method is that it requires only the numerical evaluation of a\none-dimensional integral. Any univariate stock price process, admitting an\naffine characteristic function, can be used in our formula to get an efficient\nnumerical procedure for computing spread option prices. In the numerical\nanalysis we present a comparison with Monte Carlo simulation methods to assess\nthe performance of our approach, assuming that the univariate stock price\nfollows three widely applied models: Variance Gamma, Heston's Stochastic\nVolatility and Affine Heston Nandi GARCH(1,1) model.\n"
    },
    {
        "paper_id": 2112.12031,
        "authors": "Christis Katsouris",
        "title": "Optimal Portfolio Choice and Stock Centrality for Tail Risk Events",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  We propose a novel risk matrix to characterize the optimal portfolio choice\nof an investor with tail concerns. The diagonal of the matrix contains the\nValue-at-Risk of each asset in the portfolio and the off-diagonal the pairwise\nDelta-CoVaR measures reflecting tail connections between assets. First, we\nderive the conditions under which the associated quadratic risk function has a\nclosed-form solution. Second, we examine the relationship between portfolio\nrisk and eigenvector centrality. Third, we show that portfolio risk is not\nnecessarily increasing with respect to stock centrality. Forth, we demonstrate\nunder certain conditions that asset centrality increases the optimal weight\nallocation of the asset to the portfolio. Overall, our empirical study\nindicates that a network topology which exhibits low connectivity is\noutperformed by high connectivity based on a Sharpe ratio test.\n"
    },
    {
        "paper_id": 2112.12179,
        "authors": "Oliver Chiriac, Jonathan Hall",
        "title": "Henderson--Chu model extended to two heterogeneous groups",
        "comments": "Not complete. Needs revision",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The goal of this paper is to revise the Henderson-Chu approach by dividing\nthe commuters into two income-based groups: the `rich' and the `poor'. The rich\nare clearly more flexible in their arrival times and would rather have more\nschedule delay cost instead of travel delay. The poor are quite inflexible as\nthey have to arrive at work at a specified time -- travel delay cost is\npreferred over schedule delay cost. We combined multiple models of peak-load\nbottleneck congestion with and without toll-pricing to generate a Pareto\nimprovement in Lexus lanes.\n"
    },
    {
        "paper_id": 2112.12459,
        "authors": "Kazuki Kanehira and Norikazu Todoroki",
        "title": "Stationarity analysis of the stock market data and its application to\n  mechanical trading",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  This study proposes a scheme for stationarity analysis of stock price\nfluctuations based on KM$_2$O-Langevin theory. Using this scheme, we classify\nthe time-series data of stock price fluctuations into three periods:\nstationary, non-stationary, and intermediate. We then suggest an example of a\nlow-risk stock trading strategy to demonstrate the usefulness of our scheme by\nusing actual stock index data. Our strategy uses a trend-based indicator,\nmoving averages, for stationary periods and an oscillator-based indicator,\npsychological lines, for non-stationary periods to make trading decisions.\nFinally, we confirm that our strategy is a safe trading strategy with small\nmaximum drawdown by back testing on the Nikkei Stock Average. Our study, the\nfirst to apply the stationarity analysis of KM$_2$O-Langevin theory to actual\nmechanical trading, opens up new avenues for stock price prediction.\n"
    },
    {
        "paper_id": 2112.12464,
        "authors": "Emily Schulte, Fabian Scheller, Daniel Sloot, Thomas Bruckner",
        "title": "A meta-analysis of residential PV adoption: the important role of\n  perceived benefits, intentions and antecedents in solar energy acceptance",
        "comments": null,
        "journal-ref": "Energy Research & Social Science 84 (2022): 102339",
        "doi": "10.1016/j.erss.2021.102339",
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  The adoption of residential photovoltaic systems (PV) is seen as an important\npart of the sustainable energy transition. To facilitate this process, it is\ncrucial to identify the determinants of solar adoption. This paper follows a\nmeta-analytical structural equation modeling approach, presenting a\nmeta-analysis of studies on residential PV adoption intention, and assessing\nfour behavioral models based on the theory of planned behavior to advance\ntheory development. Of 653 initially identified studies, 110 remained for\nfull-text screening. Only eight studies were sufficiently homogeneous, provided\nbivariate correlations, and could thus be integrated into the meta-analysis.The\npooled correlations across primary studies revealed medium to large\ncorrelations between environmental concern, novelty seeking, perceived\nbenefits, subjective norm and intention to adopt a residential PV system,\nwhereas socio-demographic variables were uncorrelated with intention.\nMeta-analytical structural equation modeling revealed a model (N = 1,714) in\nwhich adoption intention was predicted by benefits and perceived behavioral\ncontrol, and benefits in turn could be explained by environmental concern,\nnovelty seeking, and subjective norm. Our results imply that measures should\nprimarily focus on enhancing the perception of benefits. Based on obstacles we\nencountered within the analysis, we suggest guidelines to facilitate the future\naggregation of scientific evidence, such as the systematic inclusion of key\nvariables and reporting of bivariate correlations.\n"
    },
    {
        "paper_id": 2112.12621,
        "authors": "Michalis Drouvelis, Johannes Jarke-Neuert and Johannes Lohse",
        "title": "Should transparency be (in-)transparent? On monitoring aversion and\n  cooperation in teams",
        "comments": "13 pages excluding appendix, 22 pages including appendix, 3 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Many modern organisations employ methods which involve monitoring of\nemployees' actions in order to encourage teamwork in the workplace. While\nmonitoring promotes a transparent working environment, the effects of making\nmonitoring itself transparent may be ambiguous and have received surprisingly\nlittle attention in the literature. Using a novel laboratory experiment, we\ncreate a working environment in which first movers can (or cannot) observe\nsecond mover's monitoring at the end of a round. Our framework consists of a\nstandard repeated sequential Prisoner's Dilemma, where the second mover can\nobserve the choices made by first movers either exogenously or endogenously. We\nshow that mutual cooperation occurs significantly more frequently when\nmonitoring is made transparent. Additionally, our results highlight the key\nrole of conditional cooperators (who are more likely to monitor) in promoting\nteamwork. Overall, the observed cooperation enhancing effects are due to\nmonitoring actions that carry information about first movers who use it to\nbetter screen the type of their co-player and thereby reduce the risk of being\nexploited.\n"
    },
    {
        "paper_id": 2112.12766,
        "authors": "Anna Josephson",
        "title": "Intra-Household Management of Joint Resources: Evidence from Malawi",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  In models of intra-household resource allocation, the earnings from joint\nwork between two or more household members are often omitted. I test\nassumptions about complete pooling of resources within a household, by\naccounting for income earned jointly by multiple household members, in addition\nto income earned individually by men and women. Applied in the case of Malawi,\nI find that by explicitly including intra-household collaboration, I find\nevidence of partial income pooling and partial insurance within the household,\nspecifically for expenditures on food. Importantly, including joint income\nreveals dynamics between household members, as well as opportunities and\nvulnerabilities which may previously be obfuscated in simpler, binary\nspecifications. Contrasting with previous studies and empirical practice, my\nfindings suggest that understanding detailed intra-household interactions and\ntheir outcomes on household behavior have important consequences for household\nresource allocation and decision making.\n"
    },
    {
        "paper_id": 2112.13041,
        "authors": "Babacar Seck and Robert J. Elliott",
        "title": "Regime Switching Entropic Risk Measures on Crude Oil Pricing",
        "comments": "20 pages, 6 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  This paper introduces a new type of risk measures, namely regime switching\nentropic risk measures, and study their applicability through simulations. The\nstate of the economy is incorporated into the entropic risk formulation by\nusing a Markov chain. Closed formulae of the risk measure are obtained for\nfutures on crude oil derivatives. The applicability of these new types of risk\nmeasures is based on the study of the risk aversion parameter and the\nconvenience yield. The numerical results show a term structure and a\nmean-reverting behavior of the convenience yield.\n"
    },
    {
        "paper_id": 2112.13127,
        "authors": "Katerina Rigana, Ernst-Jan Camiel Wit and Samantha Cook",
        "title": "Using Network-based Causal Inference to Detect the Sources of Contagion\n  in the Currency Market",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  Contagion is an extremely important topic in finance. Contagion is at the\ncore of most major financial crises, in particular the 2008 financial crisis.\nAlthough various approaches to quantifying contagion have been proposed, many\nof them lack a causal interpretation. We will present a new measure for\ncontagion among individual currencies within the Foreign exchange market and\nshow how the paths of contagion work within the Forex using causal inference.\nThis approach will allow us to pinpoint sources of contagion and to find which\ncurrencies offer good options for diversification and which are more\nsusceptible to systemic risk, ultimately resulting in feedback on the level of\nglobal systemic risk.\n"
    },
    {
        "paper_id": 2112.13213,
        "authors": "Rama Cont, Mihai Cucuringu, Chao Zhang",
        "title": "Cross-Impact of Order Flow Imbalance in Equity Markets",
        "comments": "33 pages, 10 figures, 11 tables",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  We investigate the impact of order flow imbalance (OFI) on price movements in\nequity markets in a multi-asset setting. First, we propose a systematic\napproach for combining OFIs at the top levels of the limit order book into an\nintegrated OFI variable which better explains price impact, compared to the\nbest-level OFI. We show that once the information from multiple levels is\nintegrated into OFI, multi-asset models with cross-impact do not provide\nadditional explanatory power for contemporaneous impact compared to a sparse\nmodel without cross-impact terms. On the other hand, we show that lagged\ncross-asset OFIs do improve the forecasting of future returns. We also\nestablish that this lagged cross-impact mainly manifests at short-term horizons\nand decays rapidly in time.\n"
    },
    {
        "paper_id": 2112.13383,
        "authors": "Longfeng Zhao, Chao Wang, Gang-Jin Wang, H. Eugene Stanley, Lin Chen",
        "title": "Community detection and portfolio optimization",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Community detection methods can be used to explore the structure of complex\nsystems. The well-known modular configurations in complex financial systems\nindicate the existence of community structures. Here we analyze the community\nproperties of correlation-based networks in worldwide stock markets and use\ncommunity information to construct portfolios. Portfolios constructed using\ncommunity detection methods perform well. Our results can be used as new\nportfolio optimization and risk management tools.\n"
    },
    {
        "paper_id": 2112.13414,
        "authors": "Anthony Coache and Sebastian Jaimungal",
        "title": "Reinforcement Learning with Dynamic Convex Risk Measures",
        "comments": "26 pages, 9 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-sa/4.0/",
        "abstract": "  We develop an approach for solving time-consistent risk-sensitive stochastic\noptimization problems using model-free reinforcement learning (RL).\nSpecifically, we assume agents assess the risk of a sequence of random\nvariables using dynamic convex risk measures. We employ a time-consistent\ndynamic programming principle to determine the value of a particular policy,\nand develop policy gradient update rules that aid in obtaining optimal\npolicies. We further develop an actor-critic style algorithm using neural\nnetworks to optimize over policies. Finally, we demonstrate the performance and\nflexibility of our approach by applying it to three optimization problems:\nstatistical arbitrage trading strategies, financial hedging, and obstacle\navoidance robot control.\n"
    },
    {
        "paper_id": 2112.13593,
        "authors": "Shwai He and Shi Gu",
        "title": "Multi-modal Attention Network for Stock Movements Prediction",
        "comments": "The AAAI-22 Workshop on Knowledge Discovery from Unstructured Data in\n  Financial Services (KDF 2022)",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  Stock prices move as piece-wise trending fluctuation rather than a purely\nrandom walk. Traditionally, the prediction of future stock movements is based\non the historical trading record. Nowadays, with the development of social\nmedia, many active participants in the market choose to publicize their\nstrategies, which provides a window to glimpse over the whole market's attitude\ntowards future movements by extracting the semantics behind social media.\nHowever, social media contains conflicting information and cannot replace\nhistorical records completely. In this work, we propose a multi-modality\nattention network to reduce conflicts and integrate semantic and numeric\nfeatures to predict future stock movements comprehensively. Specifically, we\nfirst extract semantic information from social media and estimate their\ncredibility based on posters' identity and public reputation. Then we\nincorporate the semantic from online posts and numeric features from historical\nrecords to make the trading strategy. Experimental results show that our\napproach outperforms previous methods by a significant margin in both\nprediction accuracy (61.20\\%) and trading profits (9.13\\%). It demonstrates\nthat our method improves the performance of stock movements prediction and\ninforms future research on multi-modality fusion towards stock prediction.\n"
    },
    {
        "paper_id": 2112.13842,
        "authors": "Shifa Taslim Chowdhury, Mohammad Nur Nobi and Anm Moinul Islam",
        "title": "Economics of Innovation and Perceptions of Renewed Education and\n  Curriculum Design in Bangladesh",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  The creative Education system is one of the effective education systems in\nmany countries like Finland, Denmark, and South Korea. Bangladesh Government\nhas also launched the creative curriculum system in 2009 in both primary and\nsecondary levels, where changes have been made in educational contents and exam\nquestion patterns. These changes in the previous curriculum aimed to avoid\nmemorization and less creativity and increase the students' level of\nunderstanding and critical thinking. Though the Government has taken these\nsteps, the quality of the educational system in Bangladesh is still\ndeteriorating. Since the curriculum has been changed recently, this policy\nissue got massive attention of the people because the problem of a substandard\neducation system has arisen. Many students have poor performances in\nexaminations, including entrance hall exams in universities and board\nexaminations. This deteriorating situation is mostly for leakage of question\npaper, inadequate equipment and materials, and insufficient training. As a\nresult, the existing education system has failed to provide the standard level\nof education. This research will discuss and find why this creative educational\nsystem is getting impacted by these factors. It will be qualitative research. A\nsystematic questionnaire will interview different school teachers, parents,\nexperts, and students.\n"
    },
    {
        "paper_id": 2112.13849,
        "authors": "Roxana Guti\\'errez-Romero",
        "title": "The Long-Run Impact of Electoral Violence on Health and Human Capital in\n  Kenya",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper examines the long-term effects of prenatal, childhood, and teen\nexposure to electoral violence on health and human capital. Furthermore, it\ninvestigates whether these effects are passed down to future generations. We\nexploit the temporal and spatial variation of electoral violence in Kenya\nbetween 1992 and 2013 in conjunction with a nationally representative survey to\nidentify people exposed to such violence. Using coarsened matching, we find\nthat exposure to electoral violence between prenatal and the age of sixteen\nreduces adult height. Previous research has demonstrated that protracted,\nlarge-scale armed conflicts can pass down stunting effects to descendants. In\nline with these studies, we find that the low-scale but recurrent electoral\nviolence in Kenya has affected the height-for-age of children whose parents\nwere exposed to such violence during their growing years. Only boys exhibit\nthis intergenerational effect, possibly due to their increased susceptibility\nto malnutrition and stunting in Sub-Saharan Africa. In contrast to previous\nresearch on large-scale conflicts, childhood exposure to electoral violence has\nno long-term effect on educational attainment or household consumption per\ncapita. Most electoral violence in Kenya has occurred during school breaks,\nwhich may have mitigated its long-term effects on human capital and earning\ncapacity.\n"
    },
    {
        "paper_id": 2112.1385,
        "authors": "Imryoung Jeong and Hyunjoo Yang",
        "title": "Using maps to predict economic activity",
        "comments": "24 pages including references and appendix, 9 figures, 1 table",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  We introduce a novel machine learning approach to leverage historical and\ncontemporary maps and systematically predict economic statistics. Our simple\nalgorithm extracts meaningful features from the maps based on their color\ncompositions for predictions. We apply our method to grid-level population\nlevels in Sub-Saharan Africa in the 1950s and South Korea in 1930, 1970, and\n2015. Our results show that maps can reliably predict population density in the\nmid-20th century Sub-Saharan Africa using 9,886 map grids (5km by 5 km).\nSimilarly, contemporary South Korean maps can generate robust predictions on\nincome, consumption, employment, population density, and electric consumption.\nIn addition, our method is capable of predicting historical South Korean\npopulation growth over a century.\n"
    },
    {
        "paper_id": 2112.13911,
        "authors": "Philip Lubin and Alexander N. Cohen",
        "title": "The Economics of Interstellar Flight",
        "comments": "To be published in a special issue of Acta Astronautica (early 2022)",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Large scale directed energy offers the possibility of radical transformation\nin a variety of areas, including the ability to achieve relativistic flight\nthat will enable the first interstellar missions, as well as rapid\ninterplanetary transit. In addition, the same technology will allow for\nlong-range beamed power for ion, ablation, and thermal engines, as well as\nlong-range recharging of distant spacecraft, long-range and ultra high\nbandwidth laser communications, and many additional applications that include\nremote composition analysis, manipulation of asteroids, and full planetary\ndefense. Directed energy relies on photonics which, like electronics, is an\nexponentially expanding growth area driven by diverse economic interests that\nallows transformational advances in space exploration and capability. We have\nmade enormous technological progress in the last few years to enable this\nlong-term vision. In addition to the technological challenges, we must face the\neconomic challenges to bring the vision to reality. The path ahead requires a\nfundamental change in the system designs to allow for the radical cost\nreductions required. To afford the full-scale realization of this vision we\nwill need to bring to fore integrated photonics and mass production as a path\nforward. Fortunately, integrated photonics is a technology driven by vast\nconsumer need for high speed data delivery. We outline the fundamental physics\nthat drive the economics and derive an analytic cost model that allows us to\nlogically plan the path ahead.\n"
    },
    {
        "paper_id": 2112.14033,
        "authors": "Marek Rutkowski and Matthew Bickersteth",
        "title": "Pricing and Hedging of SOFR Derivatives under Differential Funding Costs\n  and Collateralization",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Since the 1970s, the LIBOR has served as a fundamental measure for floating\nterm rates across multiple currencies and maturities. Loans and many derivative\nsecurities, including swaps, caps and swaptions, still rely on LIBOR as the\nreference forward-looking term rate. However, in 2017 the Financial Conduct\nAuthority announced the discontinuation of LIBOR from the end of 2021 and the\nNew York Fed declared the backward-looking SOFR as a candidate for a new\nreference rate for interest rate swaps denominated in U.S. dollars. We first\noutline the classical single-curve modelling framework before transitioning to\nthe multi-curve framework where we examine arbitrage-free pricing and hedging\nof SOFR-linked swaps without and with collateral backing. As hedging\ninstruments, we take liquidly traded SOFR futures and either common or\nidiosyncratic funding rates for the hedge and margin account. For concreteness,\na one-factor model based on Vasicek's equation is used to specify the joint\ndynamics of several overnight interest rates, including the SOFR, EFFR, and\nunsecured funding rate, although multi-factor term structure models could also\nbe employed.\n"
    },
    {
        "paper_id": 2112.14161,
        "authors": "Cecilia Aubrun, Michael Benzaquen, Jean-Philippe Bouchaud",
        "title": "On Hawkes Processes with Infinite Mean Intensity",
        "comments": "5 pages, 3 figures",
        "journal-ref": null,
        "doi": "10.1103/PhysRevE.105.L032101",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The stability condition for Hawkes processes and their non-linear extensions\nusually relies on the condition that the mean intensity is a finite constant.\nIt follows that the total endogeneity ratio needs to be strictly smaller than\nunity. In the present note we argue that it is possible to have a total\nendogeneity ratio greater than unity without rendering the process unstable. In\nparticular, we show that, provided the endogeneity ratio of the linear Hawkes\ncomponent is smaller than unity, Quadratic Hawkes processes are always\nstationary, although with infinite mean intensity when the total endogenity\nratio exceeds one. This results from a subtle compensation between the\ninhibiting realisations (mean-reversion) and their exciting counterparts\n(trends).\n"
    },
    {
        "paper_id": 2112.14247,
        "authors": "Aleksandar Arandjelovi\\'c, Thorsten Rheinl\\\"ander, Pavel V. Shevchenko",
        "title": "Importance sampling for option pricing with feedforward neural networks",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We study the problem of reducing the variance of Monte Carlo estimators\nthrough performing suitable changes of the sampling measure which are induced\nby feedforward neural networks. To this end, building on the concept of vector\nstochastic integration, we characterize the Cameron-Martin spaces of a large\nclass of Gaussian measures which are induced by vector-valued continuous local\nmartingales with deterministic covariation. We prove that feedforward neural\nnetworks enjoy, up to an isometry, the universal approximation property in\nthese topological spaces. We then prove that sampling measures which are\ngenerated by feedforward neural networks can approximate the optimal sampling\nmeasure arbitrarily well. We conclude with a comprehensive numerical study\npricing path-dependent European options for asset price models that incorporate\nfactors such as changing business activity, knock-out barriers, dynamic\ncorrelations, and high-dimensional baskets.\n"
    },
    {
        "paper_id": 2112.1431,
        "authors": "Antoine Jacquier, Aitor Muguruza and Alexandre Pannier",
        "title": "Rough multifactor volatility for SPX and VIX options",
        "comments": "33 pages. We added Remarks 2.2, 4.2, 4.3 and Lemma 4.7",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We provide explicit small-time formulae for the at-the-money implied\nvolatility, skew and curvature in a large class of models, including rough\nvolatility models and their multi-factor versions. Our general setup\nencompasses both European options on a stock and VIX options, thereby providing\nnew insights on their joint calibration. The tools used are essentially based\non Malliavin calculus for Gaussian processes. We develop a detailed theoretical\nand numerical analysis of the two-factor rough Bergomi model and provide\ninsights on the interplay between the different parameters for joint SPX-VIX\nsmile calibration.\n"
    },
    {
        "paper_id": 2112.14377,
        "authors": "Jiequn Han, Yucheng Yang, Weinan E",
        "title": "DeepHAM: A Global Solution Method for Heterogeneous Agent Models with\n  Aggregate Shocks",
        "comments": "Slides available at\n  https://users.flatironinstitute.org/~jhan/files/DeepHAM_slides.pdf",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  An efficient, reliable, and interpretable global solution method, the Deep\nlearning-based algorithm for Heterogeneous Agent Models (DeepHAM), is proposed\nfor solving high dimensional heterogeneous agent models with aggregate shocks.\nThe state distribution is approximately represented by a set of optimal\ngeneralized moments. Deep neural networks are used to approximate the value and\npolicy functions, and the objective is optimized over directly simulated paths.\nIn addition to being an accurate global solver, this method has three\nadditional features. First, it is computationally efficient in solving complex\nheterogeneous agent models, and it does not suffer from the curse of\ndimensionality. Second, it provides a general and interpretable representation\nof the distribution over individual states, which is crucial in addressing the\nclassical question of whether and how heterogeneity matters in macroeconomics.\nThird, it solves the constrained efficiency problem as easily as it solves the\ncompetitive equilibrium, which opens up new possibilities for studying optimal\nmonetary and fiscal policies in heterogeneous agent models with aggregate\nshocks.\n"
    },
    {
        "paper_id": 2112.14409,
        "authors": "Qian Lei and Chi Seng Pun",
        "title": "Nonlocality, Nonlinearity, and Time Inconsistency in Stochastic\n  Differential Games",
        "comments": "arXiv admin note: text overlap with arXiv:2110.04237",
        "journal-ref": "Mathematical Finance (2023)",
        "doi": "10.1111/mafi.12420",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper proves the existence and uniqueness results (in the sense of\nmaximally defined regularity) as well as the stability analysis for the\nsolutions to a class of nonlocal fully-nonlinear parabolic systems, where the\nnonlocality stems from the flow feature (controlled by an external temporal\nparameter) of the systems. The derived mathematical results generalize the\ntheory of stochastic differential games to incorporate with behavioral factors\nsuch as time-inconsistent preferences, which facilitate developments of many\nstudies in financial economics including robust stochastic controls and games\nunder relative performance concerns. Moreover, with the well-posedness results,\nwe establish a general multidimensional Feynman--Kac formula in the presence of\nnonlocality (time inconsistency).\n"
    },
    {
        "paper_id": 2112.14451,
        "authors": "Pengyu Wei, Zuo Quan Xu",
        "title": "Dynamic growth-optimum portfolio choice under risk control",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper studies a mean-risk portfolio choice problem for log-returns in a\ncontinuous-time, complete market. This is a growth-optimal problem with risk\ncontrol. The risk of log-returns is measured by weighted Value-at-Risk (WVaR),\nwhich is a generalization of Value-at-Risk (VaR) and Expected Shortfall (ES).\nWe characterize the optimal terminal wealth up to the concave envelope of a\ncertain function, and obtain analytical expressions for the optimal wealth and\nportfolio policy when the risk is measured by VaR or ES. In addition, we find\nthat the efficient frontier is a concave curve that connects the minimum-risk\nportfolio with the growth optimal portfolio, as opposed to the vertical line\nwhen WVaR is used on terminal wealth. Our results advocate the use of mean-WVaR\ncriterion for log-returns instead of terminal wealth in dynamic portfolio\nchoice.\n"
    },
    {
        "paper_id": 2112.14462,
        "authors": "Zongxia Liang and Fengyi Yuan",
        "title": "Equilibrium master equations for time-inconsistent problems with\n  distribution dependent rewards",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We provide a unified approach to find equilibrium solutions for\ntime-inconsistent problems with distribution dependent rewards, which are\nimportant to the study of behavioral finance and economics. Our approach is\nbased on {\\it equilibrium master equation}, a non-local partial differential\nequation on Wasserstein space. We refine the classical notion of derivatives\nwith respect to distribution and establish It$\\hato$'s formula in the sense of\nsuch refined derivatives. Our approach is inspired by theories of Mckean-Vlasov\nstochastic control and mean field games, but is significantly different from\nboth in that: we prohibit marginal distribution of state to be an input of\nclosed loop control; we solve the best reaction to individual selves in an\nintra-person game instead of the best reaction to large populations as in mean\nfield games. As applications, we reexamine the dynamic portfolio choice problem\nwith rank dependent utility based on the proposed novel approach. We also\nrecover the celebrated extended HJB equation when the reward of the problem has\na nonlinear function of expectation while reformulating and weakening the\nassumptions needed. Most importantly, we provide a procedure to find an\nequilibrium solution of a dynamic mean-ES portfolio choice problem, which is\ncompletely new to the literature.\n"
    },
    {
        "paper_id": 2112.14514,
        "authors": "Kota Ogasawara",
        "title": "Technology, Institution, and Regional Growth: Evidence from Mineral\n  Mining Industry in Industrializing Japan",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Coal extraction was an influential economic activity in interwar Japan.\nInitially, coal mines employed both males and females as the workforce in the\npits. However, the innovation of labor-saving technologies and the renewal of\ntraditional extraction methodology induced institutional change. This was\nmanifested by the revision of labor regulations affecting female miners in the\nearly 1930s. This dramatically changed the mining workplace, making skilled\nmales the principal miners engaged in underground work. This paper investigates\nthe impact of coal mining on regional growth and assesses how the institutional\nchanges induced by the amended labor regulations affected its processes. By\nlinking the mines' location information with both registration and census-based\nstatistics, it was found that coal mines led to remarkable population growth.\nThe labor regulations accelerated local population growth by forcing female\nminers to exit the labor market and form families. The regulations prohibited\nfemale miners from risky underground work. This reduction in occupational\nhazard also improved early-life mortality via the mortality selection mechanism\nin utero.\n"
    },
    {
        "paper_id": 2112.14529,
        "authors": "Giacomo Toscano, Giulia Livieri, Maria Elvira Mancino, Stefano Marmi",
        "title": "Volatility of volatility estimation: central limit theorems for the\n  Fourier transform estimator and empirical study of the daily time series\n  stylized facts",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We study the asymptotic normality of two feasible estimators of the\nintegrated volatility of volatility based on the Fourier methodology, which\ndoes not require the pre-estimation of the spot volatility. We show that the\nbias-corrected estimator reaches the optimal rate $n^{1/4}$, while the\nestimator without bias-correction has a slower convergence rate and a smaller\nasymptotic variance. Additionally, we provide simulation results that support\nthe theoretical asymptotic distribution of the rate-efficient estimator and\nshow the accuracy of the latter in comparison with a rate-optimal estimator\nbased on the pre-estimation of the spot volatility. Finally, using the\nrate-optimal Fourier estimator, we reconstruct the time series of the daily\nvolatility of volatility of the S\\&P500 and EUROSTOXX50 indices over long\nsamples and provide novel insight into the existence of stylized facts about\nthe volatility of volatility dynamics.\n"
    },
    {
        "paper_id": 2112.14713,
        "authors": "Attila Lajos Makai, Szabolcs Ramhap",
        "title": "Perspectives in Public and University Sector Co-operation in the Change\n  of Higher Education Model in Hungary, in Light of China's Experience",
        "comments": "in Chinese",
        "journal-ref": "POLGARI SZEMLE: GAZDASAGI \\'ES TARSADALMI FOLYOIRAT 16 : Chinese\n  Issue pp. 226-238. , 13 p. (2020)",
        "doi": "10.24307/psz.2021.0116",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The model shift of higher education in Hungary brought not only the deepening\nof university-industry relations and technology transfer processes, but also\ncontribute the emerging role of universities in shaping regional innovation\npolicy. This process provides a new framework for cooperation between actors in\nregional innovation ecosystems and raises the relationship between\neconomic-governmental-academic systems to a new level. Active involvement of\ngovernment, the predominance of state resources, and the strong\ninnovation-organizing power of higher education institutions are similarities\nthat characterize both the Hungarian and Chinese innovation systems. This paper\nattempts to gather Chinese good practices whose adaptation can contribute to\nsuccessful public-university collaboration. In the light of the examined\npractices, the processes related to the university model shift implemented so\nfar can be placed in a new context, which are presented through the example of\nthe Sz\\'echenyi Istv\\'an University of Gy\\H{o}r.\n"
    },
    {
        "paper_id": 2112.14748,
        "authors": "Giovanni Calabro', Andrea Araldo, Simon Oh, Ravi Seshadri, Giuseppe\n  Inturri and Moshe Ben-Akiva",
        "title": "Adaptive Transit Design: Optimizing Fixed and Demand Responsive\n  Multi-Modal Transportation via Continuous Approximation",
        "comments": "57 pages, 13 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  In most cities, transit consists solely of fixed-route transportation, whence\nthe inherent limited Quality of Service for travellers in suburban areas and\nduring off-peak periods. On the other hand, completely replacing fixed-route\n(FR) with demand-responsive (DR) transit would imply a huge operational cost.\nIt is still unclear how to integrate DR transportation into current transit\nsystems to take full advantage of it. We propose a Continuous Approximation\nmodel of a transit system that gets the best from fixed-route and DR\ntransportation. Our model allows deciding whether to deploy a FR or a DR\nfeeder, in each sub-region of an urban conurbation and each time of day, and to\nredesign the line frequencies and the stop spacing of the main trunk service.\nSince such a transit design can adapt to the spatial and temporal variation of\nthe demand, we call it Adaptive Transit. Numerical results show that, with\nrespect to conventional transit, Adaptive Transit significantly improves\nuser-related cost, by drastically reducing access time to the main trunk\nservice. Such benefits are particularly remarkable in the suburbs. Moreover,\nthe generalized cost, including agency and user cost, is also reduced. These\nfindings are also confirmed in scenarios with automated vehicles. Our model can\nassist in planning future-generation transit systems, able to improve urban\nmobility by appropriately combining fixed and DR transportation.\n"
    },
    {
        "paper_id": 2112.14849,
        "authors": "Christopher Boudreaux, Randall Holcombe",
        "title": "Institutional Quality and the Wealth of Autocrats",
        "comments": "20 pages",
        "journal-ref": "European Journal of Government and Economics, 6(2) 106-125, 2017",
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  One frequently given explanation for why autocrats maintain corrupt and\ninefficient institutions is that the autocrats benefit personally even though\nthe citizens of their countries are worse off. The empirical evidence does not\nsupport this hypothesis. Autocrats in countries with low-quality institutions\ndo tend to be wealthy, but typically, they were wealthy before they assumed\npower. A plausible explanation, consistent with the data, is that wealthy\nindividuals in countries with inefficient and corrupt institutions face the\nthreat of having their wealth appropriated by government, so have the incentive\nto use some of their wealth to seek political power to protect the rest of\ntheir wealth from confiscation. While autocrats may use government institutions\nto increase their wealth, autocrats in countries with low-quality institutions\ntend to be wealthy when they assume power, because wealthy individuals have the\nincentive to use their wealth to acquire political power to protect themselves\nfrom a potentially predatory government.\n"
    },
    {
        "paper_id": 2112.14902,
        "authors": "David Ardia, Keven Bluteau, Mohammad Abbas Meghani",
        "title": "Thirty Years of Academic Finance",
        "comments": null,
        "journal-ref": null,
        "doi": "10.1111/joes.12571",
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  We study how the financial literature has evolved in scale, research team\ncomposition, and article topicality across 32 finance-focused academic journals\nfrom 1992 to 2021. We document that the field has vastly expanded regarding\noutlets and published articles. Teams have become larger, and the proportion of\nwomen participating in research has increased significantly. Using the\nStructural Topic Model, we identify 45 topics discussed in the literature. We\ninvestigate the topic coverage of individual journals and can identify highly\nspecialized and generalist outlets, but our analyses reveal that most journals\nhave covered more topics over time, thus becoming more generalist. Finally, we\nfind that articles with at least one woman author focus more on topics related\nto social and governance aspects of corporate finance. We also find that teams\nwith at least one top-tier institution scholar tend to focus more on\ntheoretical aspects of finance.\n"
    },
    {
        "paper_id": 2112.15036,
        "authors": "Hugo Inzirillo and Benjamin Mat",
        "title": "Dimensionality reduction for prediction: Application to Bitcoin and\n  Ethereum",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
        "abstract": "  The objective of this paper is to assess the performances of dimensionality\nreduction techniques to establish a link between cryptocurrencies. We have\nfocused our analysis on the two most traded cryptocurrencies: Bitcoin and\nEthereum. To perform our analysis, we took log returns and added some\ncovariates to build our data set. We first introduced the pearson correlation\ncoefficient in order to have a preliminary assessment of the link between\nBitcoin and Ethereum. We then reduced the dimension of our data set using\ncanonical correlation analysis and principal component analysis. After\nperforming an analysis of the links between Bitcoin and Ethereum with both\nstatistical techniques, we measured their performance on forecasting Ethereum\nreturns with Bitcoin s features.\n"
    },
    {
        "paper_id": 2112.15129,
        "authors": "Christa Cuchiero, Francesco Guida, Luca di Persio, Sara Svaluto-Ferro",
        "title": "Measure-valued affine and polynomial diffusions",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  We introduce a class of measure-valued processes, which -- in analogy to\ntheir finite dimensional counterparts -- will be called measure-valued\npolynomial diffusions. We show the so-called moment formula, i.e.~a\nrepresentation of the conditional marginal moments via a system of finite\ndimensional linear PDEs. Furthermore, we characterize the corresponding\ninfinitesimal generators and obtain a representation analogous to polynomial\ndiffusions on $\\mathbb{R}^m_+$, in cases where their domain is large enough. In\ngeneral the infinite dimensional setting allows for richer specifications\nstrictly beyond this representation. As a special case we recover\nmeasure-valued affine diffusions, sometimes also called Dawson-Watanabe\nsuperprocesses. From a mathematical finance point of view the polynomial\nframework is especially attractive as it allows to transfer the most famous\nfinite dimensional models, such as the Black-Scholes model, to an infinite\ndimensional measure-valued setting. We outline in particular the applicability\nof our approach for term structure modeling in energy markets.\n"
    },
    {
        "paper_id": 2112.15284,
        "authors": "Thitithep Sitthiyot and Kanyarat Holasut",
        "title": "A simple method for measuring inequality",
        "comments": "9 pages, 2 tables, 7 figures",
        "journal-ref": "Palgrave Communications (2020), 6:112, pp. 1-9",
        "doi": "10.1057/s41599-020-0484-6",
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  To simultaneously overcome the limitation of the Gini index in that it is\nless sensitive to inequality at the tails of income distribution and the\nlimitation of the inter-decile ratios that ignore inequality in the middle of\nincome distribution, an inequality index is introduced. It comprises three\nindicators, namely, the Gini index, the income share held by the top 10%, and\nthe income share held by the bottom 10%. The data from the World Bank database\nand the Organization for Economic Co-operation and Development Income\nDistribution Database between 2005 and 2015 are used to demonstrate how the\ninequality index works. The results show that it can distinguish income\ninequality among countries that share the same Gini index but have different\nincome gaps between the top 10% and the bottom 10%. It could also distinguish\nincome inequality among countries that have the same ratio of income share held\nby the top 10% to income share held by the bottom 10% but differ in the values\nof the Gini index. In addition, the inequality index could capture the dynamics\nwhere the Gini index of a country is stable over time but the ratio of income\nshare of the top 10% to income share of the bottom 10% is increasing.\nFurthermore, the inequality index could be applied to other scientific\ndisciplines as a measure of statistical heterogeneity and for size\ndistributions of any non-negative quantities.\n"
    },
    {
        "paper_id": 2112.15291,
        "authors": "Thitithep Sitthiyot and Kanyarat Holasut",
        "title": "A simple method for estimating the Lorenz curve",
        "comments": "9 pages, 10 tables, 1 figure",
        "journal-ref": "Humanities and Social Sciences Communications (2021) 8:268, pp.\n  1-9",
        "doi": "10.1057/s41599-021-00948-x",
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Given many popular functional forms for the Lorenz curve do not have a\nclosed-form expression for the Gini index and no study has utilized the\nobserved Gini index to estimate parameter(s) associated with the corresponding\nparametric functional form, a simple method for estimating the Lorenz curve is\nintroduced. It utilizes 3 indicators, namely, the Gini index and the income\nshares of the bottom and the top in order to calculate the values of parameters\nassociated with the specified functional form which has a closed-form\nexpression for the Gini index. No error minimization technique is required in\norder to estimate the Lorenz curve. The data on the Gini index and the income\nshares of 4 countries that have different level of income inequality, economic,\nsociological, and regional backgrounds from the United Nations University-World\nIncome Inequality Database are used to illustrate how the simple method works.\nThe overall results indicate that the estimated Lorenz curves fit the actual\nobservations practically well. This simple method could be useful in the\nsituation where the availability of data on income distribution is low.\nHowever, if more data on income distribution are available, this study shows\nthat the specified functional form could be used to directly estimate the\nLorenz curve. Moreover, the estimated values of the Gini index calculated based\non the specified functional form are virtually identical to their actual\nobservations.\n"
    },
    {
        "paper_id": 2112.15294,
        "authors": "Thitithep Sitthiyot",
        "title": "Macroeconomic and financial management in an uncertain world: What can\n  we learn from complexity science?",
        "comments": "37 pages",
        "journal-ref": "Thammasat Economic Journal (2015) 33(3), pp. 1-37",
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  This paper discusses serious drawbacks of existing knowledge in\nmacroeconomics and finance in explaining and predicting economic and financial\nphenomena. Complexity science is proposed as an alternative approach to be used\nin order to better understand how economy and financial market work. This paper\nargues that understanding characteristics of complex system could greatly\nbenefit financial analysts, financial regulators, as well as macroeconomic\npolicy makers.\n"
    },
    {
        "paper_id": 2112.15315,
        "authors": "Rituparna Sen, Anandamayee Majumdar, Shubhangi Sikaria",
        "title": "Bayesian Testing Of Granger Causality In Functional Time Series",
        "comments": null,
        "journal-ref": "2021 Journal of Statistical Theory and Practice, 15: 40",
        "doi": "10.1007/s40953-022-00306-x",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We develop a multivariate functional autoregressive model (MFAR), which\ncaptures the cross-correlation among multiple functional time series and thus\nimproves forecast accuracy. We estimate the parameters under the Bayesian\ndynamic linear models (DLM) framework. In order to capture Granger causality\nfrom one FAR series to another we employ Bayes Factor. Motivated by the broad\napplication of functional data in finance, we investigate the causality between\nthe yield curves of two countries. Furthermore, we illustrate a climatology\nexample, examining whether the weather conditions Granger cause pollutant daily\nlevels in a city.\n"
    },
    {
        "paper_id": 2112.15321,
        "authors": "Nick James",
        "title": "Evolutionary correlation, regime switching, spectral dynamics and\n  optimal trading strategies for cryptocurrencies and equities",
        "comments": "45 pages",
        "journal-ref": null,
        "doi": "10.1016/j.physd.2022.133262",
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  This paper uses new and recently established methodologies to study the\nevolutionary dynamics of the cryptocurrency market, and compares the findings\nwith that of the equity market. We begin by applying random matrix theory and\nprincipal components analysis (PCA) to correlation matrices of both\ncollections, highlighting clear differences in the eigenspectra exhibited. We\nthen explore the heterogeneity of both asset classes, studying the time-varying\ndynamics of underlying sector behaviours, and determine the collective\nsimilarity within each collection. We then turn to a study of structural break\ndynamics and evolutionary power spectra, where we quantify the collective\naffinity in structural breaks and evolutionary behaviours of underlying sector\ntime series. Finally, we implement two algorithms simulating `portfolio choice'\ndynamics to compare the effectiveness of stock selection and sector allocation\nin cryptocurrency portfolios. There, we highlight the importance of both\nendeavours and comment on noteworthy implications for cryptocurrency portfolio\nmanagement.\n"
    },
    {
        "paper_id": 2112.15401,
        "authors": "Rados{\\l}aw A. Kycia, Agnieszka Niemczynowicz, Joanna\n  Nie\\.zurawska-Zaj\\k{a}c",
        "title": "Towards the global vision of engagement of Generation Z at the\n  workplace: Mathematical modeling",
        "comments": "14 pages, 10 figures, 2 tables",
        "journal-ref": "37th International Business Information Management Association\n  Conference (IBIMA), pp. 6084-6095 (2021); ISBN: 978-0-9998551-6-4",
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Correlation and cluster analyses (k-Means, Gaussian Mixture Models) were\nperformed on Generation Z engagement surveys at the workplace. The clustering\nindicates relations between various factors that describe the engagement of\nemployees. The most noticeable factors are a clear statement about the\nresponsibilities at work, and challenging work. These factors are essential in\npractice. The results of this paper can be used in preparing better\nmotivational systems aimed at Generation Z employees.\n"
    },
    {
        "paper_id": 2112.15426,
        "authors": "Luis A. Mateos",
        "title": "Lunatic Stocks: Moon Phases as Irregular Sampling Features for Pattern\n  Recognition in the Stock Markets",
        "comments": "6 pages, 9 images",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-sa/4.0/",
        "abstract": "  This paper presents a novel idea on incorporating the Moon phases to the\nclassic Gregorian (Solar) calendar time sampling methods for finding meaningful\npatterns in the stock markets. The four main Moon phases (New Moon, First\nquarter, Full Moon and Third quarter) are irregular in time but with well\ndefined sampling structure as the Moon orbits the Earth completing its period.\nA Full Moon may appear in one month of the year on the 2nd, on the next month\nthe Full moon may appear on the 4th and in the next ten years on the 13th of\nthe same month. This structure which is irregular in time makes it interesting\nto study together with the stock market data. Moreover, the moon affects\nmultiple physical things on the earth, such as the ocean tides, the behavior of\nliving organisms as well as humans mood and decision when risking and\ninvesting.\n"
    },
    {
        "paper_id": 2112.15431,
        "authors": "Fabio Ashtar Telarico (CSEES, FDV)",
        "title": "Forecasting pandemic tax revenues in a small, open economy",
        "comments": "Novi Ekonomist, Faculty of Business Economics Bijeljina, 2021",
        "journal-ref": null,
        "doi": "10.7251/NOEEN2129018T",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Tax analysis and forecasting of revenues are of paramount importance to\nensure fiscal policy's viability and sustainability. However, the measures\ntaken to contain the spread of the recent pandemic pose an unprecedented\nchallenge to established models and approaches. This paper proposes a model to\nforecast tax revenues in Bulgaria for the fiscal years 2020-2022 built in\naccordance with the International Monetary Fund's recommendations on a dataset\ncovering the period between 1995 and 2019. The study further discusses the\nactual trustworthiness of official Bulgarian forecasts, contrasting those\nfigures with the model previously estimated. This study's quantitative results\nboth confirm the pandemic's assumed negative impact on tax revenues and prove\nthat econometrics can be tweaked to produce consistent revenue forecasts even\nin the relatively-unexplored case of Bulgaria offering new insights to\npolicymakers and advocates.\n"
    },
    {
        "paper_id": 2112.15447,
        "authors": "K.C.Sanjeevani Perera",
        "title": "Analysis of Performance of Drivers and Usage of Overtime Hours: A Case\n  Study of a Higher Educational Institution",
        "comments": "11 pages, 3 figures, 2 tables",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  This study attempted to analyze whether there is a relationship between the\nperformance of drivers and the number of overtime hours worked by them. The\nnumber of overtime hours worked by the drivers in the pool for the years 2017\nand 2018 were extracted from the overtime registers and feedback received on\nthe performance of drivers from staff members who frequently traveled in the\nUniversity vehicles were used for this study. The overall performance of a\ndriver was decided by taking the aggregate of marks received by him for the\ntraits: skillfulness, patience, responsibility, customer service and care for\nthe vehicle. The type of vehicle the driver is assigned for is also taken into\naccount in the analysis of this study. The study revealed that there is no\nsignificant relationship between the performance of the drivers and the number\nof overtime hours worked by them but the type of vehicle and the condition of\nthe vehicle affects attracting long journeys to them which enable them to earn\nmore overtime hours.\n"
    },
    {
        "paper_id": 2112.15448,
        "authors": "Farshad Noravesh and Hamid Boustanifar",
        "title": "Exact Post-selection Inference For Tracking S&P500",
        "comments": "4 figures, 1 table",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  The problem that is solved in this paper is known as index tracking. The\nmethod of Lasso is used to reduce the dimensions of S&P500 index which has many\napplications in both investment and portfolio management algorithms. The\nnovelty of this paper is that post-selection inference is used to have better\nmodeling and inference for Lasso approach to index tracking. Both confidence\nintervals and curves indicate that the performance of Lasso type method for\ndimension reduction of S&P500 is remarkably high. Keywords: index tracking,\nlasso, post-selection inference, S&P500\n"
    },
    {
        "paper_id": 2112.15499,
        "authors": "Yuanrong Wang, Tomaso Aste",
        "title": "Dynamic Portfolio Optimization with Inverse Covariance Clustering",
        "comments": "12 pages, 2 figures, 2 tables",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-sa/4.0/",
        "abstract": "  Market conditions change continuously. However, in portfolio's investment\nstrategies, it is hard to account for this intrinsic non-stationarity. In this\npaper, we propose to address this issue by using the Inverse Covariance\nClustering (ICC) method to identify inherent market states and then integrate\nsuch states into a dynamic portfolio optimization process. Extensive\nexperiments across three different markets, NASDAQ, FTSE and HS300, over a\nperiod of ten years, demonstrate the advantages of our proposed algorithm,\ntermed Inverse Covariance Clustering-Portfolio Optimization (ICC-PO). The core\nof the ICC-PO methodology concerns the identification and clustering of market\nstates from the analytics of past data and the forecasting of the future market\nstate. It is therefore agnostic to the specific portfolio optimization method\nof choice. By applying the same portfolio optimization technique on a ICC\ntemporal cluster, instead of the whole train period, we show that one can\ngenerate portfolios with substantially higher Sharpe Ratios, which are\nstatistically more robust and resilient with great reductions in maximum loss\nin extreme situations. This is shown to be consistent across markets, periods,\noptimization methods and selection of portfolio assets.\n"
    },
    {
        "paper_id": 2201.00013,
        "authors": "Adel Daoud",
        "title": "The International Monetary Funds intervention in education systems and\n  its impact on childrens chances of completing school",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  Enabling children to acquire an education is one of the most effective means\nto reduce inequality, poverty, and ill-health globally. While in normal times a\ngovernment controls its educational policies, during times of macroeconomic\ninstability, that control may shift to supporting international organizations,\nsuch as the International Monetary Fund (IMF). While much research has focused\non which sectors has been affected by IMF policies, scholars have devoted\nlittle attention to the policy content of IMF interventions affecting the\neducation sector and childrens education outcomes: denoted IMF education\npolicies. This article evaluates the extent which IMF education policies exist\nin all programs and how these policies and IMF programs affect childrens\nlikelihood of completing schools. While IMF education policies have a small\nadverse effect yet statistically insignificant on childrens probability of\ncompleting school, these policies moderate effect heterogeneity for IMF\nprograms. The effect of IMF programs (joint set of policies) adversely effect\nchildrens chances of completing school by six percentage points. By analyzing\nhow IMF-education policies but also how IMF programs affect the education\nsector in low and middle-income countries, scholars will gain a deeper\nunderstanding of how such policies will likely affect downstream outcomes.\n"
    },
    {
        "paper_id": 2201.00119,
        "authors": "Arnab Chakrabarti, Rituparna Sen",
        "title": "Limiting Spectral Distribution of High-dimensional Hayashi-Yoshida\n  Estimator of Integrated Covariance Matrix",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper, the estimation of the Integrated Covariance matrix from\nhigh-frequency data, for high dimensional stock price process, is considered.\nThe Hayashi-Yoshida covolatility estimator is an improvement over Realized\ncovolatility for asynchronous data and works well in low dimensions. However it\nbecomes inconsistent and unreliable in the high dimensional situation. We study\nthe bulk spectrum of this matrix and establish its connection to the spectrum\nof the true covariance matrix in the limiting case where the dimension goes to\ninfinity. The results are illustrated with simulation studies in finite, but\nhigh, dimensional cases. An application to real data with tick-by-tick data on\n50 stocks is presented.\n"
    },
    {
        "paper_id": 2201.00161,
        "authors": "Thitithep Sitthiyot and Kanyarat Holasut",
        "title": "On income inequality and population size",
        "comments": "2 tables, 3 figures",
        "journal-ref": "Thammasat Review of Economic and Social Policy (2016) 2(2), pp.\n  24-48",
        "doi": "10.14456/tresp.2016.8",
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  The pursuit of having an appropriate level of income inequality should be\nviewed as one of the biggest challenges facing academic scholars as well as\npolicy makers. Unfortunately, research on this issue is currently lacking. This\nstudy is the first to introduce the theoretical concept of targeted level of\nincome inequality for a given size of population. By employing the World Bank's\ndata on population size and Gini coefficient from sixty-nine countries in 2012,\nthis study finds that the relationship between Gini coefficient and natural\nlogarithm of population size is nonlinear in the form of a second-degree\npolynomial function. The estimated results using regression analysis show that\nthe majority of countries in the sample have Gini coefficients either too high\nor too low compared to their appropriate values. These findings could be used\nas a guideline for policy makers before designing and implementing public\npolicies in order to achieve the targeted level of income inequality.\n"
    },
    {
        "paper_id": 2201.00205,
        "authors": "Farshad Noravesh and Kristiaan Kerstens",
        "title": "Some connections between higher moments portfolio optimization methods",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  In this paper, different approaches to portfolio optimization having higher\nmoments such as skewness and kurtosis are classified so that the reader can\nobserve different paradigms and approaches in this field of research which is\nessential for practitioners in Hedge Funds in particular. Several methods based\non different paradigms such as utility approach and multi-objective\noptimization are reviewed and the advantage and disadvantageous of these ideas\nare explained. Keywords: multi-objective optimization, portfolio optimization,\nscalarization, utility\n"
    },
    {
        "paper_id": 2201.00223,
        "authors": "Bruce Knuteson",
        "title": "They Still Haven't Told You",
        "comments": "14 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The world's stock markets display a decades-long pattern of overnight and\nintraday returns seemingly consistent with only one explanation: one or more\nlarge, long-lived quant firms tending to expand its portfolio early in the day\n(when its trading moves prices more) and contract its portfolio later in the\nday (when its trading moves prices less), losing money on its daily round-trip\ntrades to create mark-to-market gains on its large existing book. In the\nfourteen years since this extraordinary pattern of overnight and intraday\nreturns was first noted in the literature, no plausible alternative explanation\nhas been advanced. The main question remaining is therefore which of the few\nfirms capable of profitably trading in this manner are guilty of having done\nso. If any of this is news to you, it is because the people you trust to alert\nyou to such problems still haven't told you.\n"
    },
    {
        "paper_id": 2201.00274,
        "authors": "Mohammadreza Mahmoudi",
        "title": "COVID Lessons: Was there any way to reduce the negative effect of\n  COVID-19 on the United States economy?",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper aims to study the economic impact of COVID-19. To do that, in the\nfirst step, I showed that the adjusted SEQIER model, which is a generalization\nform of SEIR model, is a good fit to the real COVID-induced daily death data in\na way that it could capture the nonlinearities of the data very well. Then, I\nused this model with extra parameters to evaluate the economic effect of\nCOVID-19 through job market. The results show that there was a simple strategy\nthat US government could implemented in order to reduce the negative effect of\nCOVID-19. Because of that the answer to the paper's title is yes. If lockdown\npolicies consider the heterogenous characteristics of population and impose\nmore restrictions on old people and control the interactions between them and\nthe rest of population the devastating impact of COVID-19 on people lives and\nUS economy reduced dramatically. Specifically, based on this paper's results,\nthis strategy could reduce the death rate and GDP loss of the United States\n0.03 percent and 2 percent respectively. By comparing these results with actual\ndata which show death rate and GDP loss 0.1 percent and 3.5 percent\nrespectively, we could figure out that death rate reduction is 0.07 percent\nwhich means for the same percent of GDP loss executing optimal targeted policy\ncould save 2/3 lives. Approximately, 378,000 persons dead because of COVID-19\nduring 2020, hence reducing death rate to 0.03 percent means saving around\n280,000 lives, which is huge.\n"
    },
    {
        "paper_id": 2201.00345,
        "authors": "Nicolas Eschenbaum, Filip Mellgren, Philipp Zahn",
        "title": "Robust Algorithmic Collusion",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  This paper develops a formal framework to assess policies of learning\nalgorithms in economic games. We investigate whether reinforcement-learning\nagents with collusive pricing policies can successfully extrapolate collusive\nbehavior from training to the market. We find that in testing environments\ncollusion consistently breaks down. Instead, we observe static Nash play. We\nthen show that restricting algorithms' strategy space can make algorithmic\ncollusion robust, because it limits overfitting to rival strategies. Our\nfindings suggest that policy-makers should focus on firm behavior aimed at\ncoordinating algorithm design in order to make collusive policies robust.\n"
    },
    {
        "paper_id": 2201.0035,
        "authors": "Javad T. Firouzjaee and Pouriya Khaliliyan",
        "title": "The Interpretability of LSTM Models for Predicting Oil Company Stocks:\n  Impact of Correlated Features",
        "comments": "22 figures, 13 tables, accepted for publication in International\n  Journal of Energy Research",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Oil companies are among the largest companies in the world whose economic\nindicators in the global stock market have a great impact on the world\neconomy\\cite{ec00} and market due to their relation to gold\\cite{ec01}, crude\noil\\cite{ec02}, and the dollar\\cite{ec03}. This study investigates the impact\nof correlated features on the interpretability of Long Short-Term\nMemory(LSTM)\\cite{ec04} models for predicting oil company stocks. To achieve\nthis, we designed a Standard Long Short-Term Memory (LSTM) network and trained\nit using various correlated datasets. Our approach aims to improve the accuracy\nof stock price prediction by considering the multiple factors affecting the\nmarket, such as crude oil prices, gold prices, and the US dollar. The results\ndemonstrate that adding a feature correlated with oil stocks does not improve\nthe interpretability of LSTM models. These findings suggest that while LSTM\nmodels may be effective in predicting stock prices, their interpretability may\nbe limited. Caution should be exercised when relying solely on LSTM models for\nstock price prediction as their lack of interpretability may make it difficult\nto fully understand the underlying factors driving stock price movements. We\nhave employed complexity analysis to support our argument, considering that\nfinancial markets encompass a form of physical complex system\\cite{ec05}. One\nof the fundamental challenges faced in utilizing LSTM models for financial\nmarkets lies in interpreting the unexpected feedback dynamics within them.\n"
    },
    {
        "paper_id": 2201.00486,
        "authors": "Kshitija Taywade, Brent Harrison, Judy Goldsmith",
        "title": "Using Non-Stationary Bandits for Learning in Repeated Cournot Games with\n  Non-Stationary Demand",
        "comments": "13 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Many past attempts at modeling repeated Cournot games assume that demand is\nstationary. This does not align with real-world scenarios in which market\ndemands can evolve over a product's lifetime for a myriad of reasons. In this\npaper, we model repeated Cournot games with non-stationary demand such that\nfirms/agents face separate instances of non-stationary multi-armed bandit\nproblem. The set of arms/actions that an agent can choose from represents\ndiscrete production quantities; here, the action space is ordered. Agents are\nindependent and autonomous, and cannot observe anything from the environment;\nthey can only see their own rewards after taking an action, and only work\ntowards maximizing these rewards. We propose a novel algorithm 'Adaptive with\nWeighted Exploration (AWE) $\\epsilon$-greedy' which is remotely based on the\nwell-known $\\epsilon$-greedy approach. This algorithm detects and quantifies\nchanges in rewards due to varying market demand and varies learning rate and\nexploration rate in proportion to the degree of changes in demand, thus\nenabling agents to better identify new optimal actions. For efficient\nexploration, it also deploys a mechanism for weighing actions that takes\nadvantage of the ordered action space. We use simulations to study the\nemergence of various equilibria in the market. In addition, we study the\nscalability of our approach in terms number of total agents in the system and\nthe size of action space. We consider both symmetric and asymmetric firms in\nour models. We found that using our proposed method, agents are able to swiftly\nchange their course of action according to the changes in demand, and they also\nengage in collusive behavior in many simulations.\n"
    },
    {
        "paper_id": 2201.00578,
        "authors": "Matthias Niggli",
        "title": "'Moving On' -- Investigating Inventors' Ethnic Origins Using Supervised\n  Learning",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-sa/4.0/",
        "abstract": "  Patent data provides rich information about technical inventions, but does\nnot disclose the ethnic origin of inventors. In this paper, I use supervised\nlearning techniques to infer this information. To do so, I construct a dataset\nof 95'202 labeled names and train an artificial recurrent neural network with\nlong-short-term memory (LSTM) to predict ethnic origins based on names. The\ntrained network achieves an overall performance of 91% across 17 ethnic\norigins. I use this model to classify and investigate the ethnic origins of\n2.68 million inventors and provide novel descriptive evidence regarding their\nethnic origin composition over time and across countries and technological\nfields. The global ethnic origin composition has become more diverse over the\nlast decades, which was mostly due to a relative increase of Asian origin\ninventors. Furthermore, the prevalence of foreign-origin inventors is\nespecially high in the USA, but has also increased in other high-income\neconomies. This increase was mainly driven by an inflow of non-western\ninventors into emerging high-technology fields for the USA, but not for other\nhigh-income countries.\n"
    },
    {
        "paper_id": 2201.00914,
        "authors": "Chonghu Guan, Xiaomin Shi, Zuo Quan Xu",
        "title": "Continuous-time Markowitz's mean-variance model under different\n  borrowing and saving rates",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We study Markowitz's mean-variance portfolio selection problem in a\ncontinuous-time Black-Scholes market with different borrowing and saving rates.\nThe associated Hamilton-Jacobi-Bellman equation is fully nonlinear. Using a\ndelicate partial differential equation and verification argument, the value\nfunction is proven to be $C^{3,2}$ smooth. It is also shown that there are a\nborrowing boundary and a saving barrier which divide the entire trading area\ninto a borrowing-money region, an all-in-stock region, and a saving-money\nregion in ascending order. The optimal trading strategy is a mixture of\ncontinuous-time strategy (as suggested by most continuous-time models) and\ndiscontinuous-time strategy (as suggested by models with transaction costs):\none should put all her wealth in the stock in the middle all-in-stock region,\nand continuously trade it in the other two regions in a feedback form of wealth\nand time. It is never optimal to short sale the stock. Numerical examples are\nalso presented to verify the theoretical results and to give more financial\ninsights beyond them.\n"
    },
    {
        "paper_id": 2201.01026,
        "authors": "Liao Wang and Jin Yao and Xiaowei Zhang",
        "title": "How Does Risk Hedging Impact Operations? Insights from a Price-Setting\n  Newsvendor Model",
        "comments": "main body: 36 pages, 3 figures, 2 tables; supplmental material: 68\n  pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  If a financial asset's price movement impacts a firm's product demand, the\nfirm can respond to the impact by adjusting its operational decisions. For\nexample, in the automotive industry, car makers decrease the selling prices of\nfuel-inefficient cars when the oil price rises. Meanwhile, the firm can\nimplement a risk-hedging strategy using the financial asset jointly with its\noperational decisions. Motivated by this, we develop and solve a general\nrisk-management model integrating risk hedging into a price-setting newsvendor.\nThe optimal hedging strategy is calculated analytically, which leads to an\nexplicit objective function for optimizing price and ``virtual production\nquantity'' (VPQ). (The latter determines the service level, i.e., the demand\nfulfillment probability.) We find that hedging generally reduces the optimal\nprice {when the firm sets the target mean return as its production-only maximum\nexpected profit. With the same condition on the target mean return}, hedging\nalso reduces the optimal VPQ when the asset price trend positively impacts\nproduct demand; meanwhile, it may increase the VPQ by a small margin when the\nimpact is negative. We construct the return-risk efficient frontier that\ncharacterizes the optimal return-risk trade-off. Our numerical study using data\nfrom a prominent automotive manufacturer shows that the markdowns in price and\nreduction in VPQ are small under our model and that the hedging strategy\nsubstantially reduces risk without materially reducing operational profit.\n"
    },
    {
        "paper_id": 2201.01125,
        "authors": "Julian Schwierzy, Robert Dehghan, Sebastian Schmidt, Elisa Rodepeter,\n  Andreas Stoemmer, Kaan Uctum, Jan Kinne, David Lenz, Hanna Hottenrott",
        "title": "Technology Mapping Using WebAI: The Case of 3D Printing",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The diffusion of new technologies is crucial for the realization of social\nand economic returns to innovation. Tracking and mapping technology diffusion\nis, however, typically limited by the extent to which we can observe technology\nadoption. This study uses website texts to train a multilingual language model\nensemble to map technology diffusion for the case of 3D printing. The study\nidentifies relevant actors and their roles in the diffusion process. The\nresults show that besides manufacturers, service provider, retailers, and\ninformation providers play an important role. The geographic distribution of\nadoption intensity suggests that regional 3D-printing intensity is driven by\nexperienced lead users and the presence of technical universities. The overall\nadoption intensity varies by sector and firm size. These patterns indicate that\nthe approach of using webAI provides a useful and novel tool for technology\nmapping which adds to existing measures based on patents or survey data.\n"
    },
    {
        "paper_id": 2201.0116,
        "authors": "Jaehyuk Choi, Lei Lu, Heungju Park, Sungbin Sohn",
        "title": "The financial value of the within-government political network: Evidence\n  from Chinese municipal corporate bonds",
        "comments": null,
        "journal-ref": "Finance Research Letters, 47:102552, 2022",
        "doi": "10.1016/j.frl.2021.102552",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper examines the effect of the political network of Chinese municipal\nleaders on the pricing of municipal corporate bonds. Using municipal leaders'\nworking experience to measure the political network, we find that this network\nreduces the bond issuance yield spreads by improving the credit ratings of the\nissuer, the local government financing vehicle. The relationship between\npolitical networks and issuance yield spreads is strengthened in areas where\nfinancial markets and legal systems are less developed.\n"
    },
    {
        "paper_id": 2201.01163,
        "authors": "Michael Curry, Alexander Trott, Soham Phade, Yu Bai, Stephan Zheng",
        "title": "Analyzing Micro-Founded General Equilibrium Models with Many Agents\n  using Deep Reinforcement Learning",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Real economies can be modeled as a sequential imperfect-information game with\nmany heterogeneous agents, such as consumers, firms, and governments. Dynamic\ngeneral equilibrium (DGE) models are often used for macroeconomic analysis in\nthis setting. However, finding general equilibria is challenging using existing\ntheoretical or computational methods, especially when using microfoundations to\nmodel individual agents. Here, we show how to use deep multi-agent\nreinforcement learning (MARL) to find $\\epsilon$-meta-equilibria over agent\ntypes in microfounded DGE models. Whereas standard MARL fails to learn\nnon-trivial solutions, our structured learning curricula enable stable\nconvergence to meaningful solutions. Conceptually, our approach is more\nflexible and does not need unrealistic assumptions, e.g., continuous market\nclearing, that are commonly used for analytical tractability. Furthermore, our\nend-to-end GPU implementation enables fast real-time convergence with a large\nnumber of RL economic agents. We showcase our approach in open and closed\nreal-business-cycle (RBC) models with 100 worker-consumers, 10 firms, and a\nsocial planner who taxes and redistributes. We validate the learned solutions\nare $\\epsilon$-meta-equilibria through best-response analyses, show that they\nalign with economic intuitions, and show our approach can learn a spectrum of\nqualitatively distinct $\\epsilon$-meta-equilibria in open RBC models. As such,\nwe show that hardware-accelerated MARL is a promising framework for modeling\nthe complexity of economies based on microfoundations.\n"
    },
    {
        "paper_id": 2201.01227,
        "authors": "Farshad Noravesh",
        "title": "Sparse Non-Convex Optimization For Higher Moment Portfolio Management",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  One of the reasons that higher order moment portfolio optimization methods\nare not fully used by practitioners in investment decisions is the complexity\nthat these higher moments create by making the optimization problem nonconvex.\nMany few methods and theoretical results exists in the literature, but the\npresent paper uses the method of successive convex approximation for the\nmean-variance-skewness problem.\n"
    },
    {
        "paper_id": 2201.01321,
        "authors": "Nicholas S. Caros, Jinhua Zhao",
        "title": "Preparing urban mobility for the future of work",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  A gradual growth in flexible work over many decades has been suddenly and\ndramatically accelerated by the COVID-19 pandemic. The share of flexible work\ndays in the United States is forecasted to grow from 4\\% in 2018 to over 26\\%\nby 2022. This rapid and unexpected shift in the nature of work will have a\nprofound effect on the demand for, and supply of, urban transportation.\nUnderstanding how people make decisions around where and with whom to work will\nbe critical for predicting future travel patterns and designing mobility\nsystems to serve flexible commuters. To that end, this paper establishes a\nformal taxonomy for describing possible flexible work arrangements, the\nstakeholders involved and the relationships between them. An analytical\nframework is then developed for adapting existing transportation models to\nincorporate the unique dynamics of flexible work location choice. Several\nexamples are provided to demonstrate how the new taxonomy and analytical\nframework can be applied across a broad set of scenarios. Finally, a critical\nresearch agenda is proposed to create both the empirical knowledge and\nmethodological tools to prepare urban mobility for the future of work.\n"
    },
    {
        "paper_id": 2201.0133,
        "authors": "Richard J. Martin",
        "title": "The credit spread curve. I: Fundamental concepts, fitting, par-adjusted\n  spread, and expected return",
        "comments": "Presents an extended form of the forward hazard rate model; gives\n  details on CDS curve stripping; extends discussion on EM; new section on\n  accreting bonds; new section on bond forwards",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  The notion of a credit spread curve is fundamental in fixed income investing,\nbut in practice it is not `given' and needs to be constructed from bond prices\neither for a particular issuer, or for a sector rating-by-rating. Rather than\nattempting to fit spreads -- and as we discuss here, the Z-spread is unsuitable\n-- we fit parametrised survival curves. By deriving a valuation formula for a\nrisky bond, we explain and avoid the problem that bonds with a high dollar\nprice trade at a higher yield or spread than those with low dollar price (at\nthe same maturity point), even though they do not necessarily offer better\nvalue. In fact, a concise treatment of this effect is elusive, and much of the\nacademic literature on risky bond pricing, including a well-known paper by\nDuffie and Singleton (1997), is fundamentally incorrect. We then proceed to\nshow how to calculate carry, rolldown and relative value for bonds/CDS. Also,\nonce curve construction has been programmed and automated we can run it\nhistorically and assess the way a curve has moved over time. This provides the\nnecessary grounding for econometric and arbitrage-free models of curve\ndynamics, which will be pursued in later work, as well as assessing how the\nperceived relative value of a particular instrument varies over time.\n"
    },
    {
        "paper_id": 2201.01356,
        "authors": "Lendie Follett and Heath Henderson",
        "title": "A hybrid approach to targeting social assistance",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Proxy means testing (PMT) and community-based targeting (CBT) are two of the\nleading methods for targeting social assistance in developing countries. In\nthis paper, we present a hybrid targeting method that incorporates CBT's\nemphasis on local information and preferences with PMT's reliance on verifiable\nindicators. Specifically, we outline a Bayesian framework for targeting that\nresembles PMT in that beneficiary selection is based on a weighted sum of\nsociodemographic characteristics. We nevertheless propose calibrating the\nweights to preference rankings from community targeting exercises, implying\nthat the weights used by our method reflect how potential beneficiaries\nthemselves substitute sociodemographic features when making targeting\ndecisions. We discuss several practical extensions to the model, including a\ngeneralization to multiple rankings per community, an adjustment for elite\ncapture, a method for incorporating auxiliary information on potential\nbeneficiaries, and a dynamic updating procedure. We further provide an\nempirical illustration using data from Burkina Faso and Indonesia.\n"
    },
    {
        "paper_id": 2201.01392,
        "authors": "Bruce Mizrach",
        "title": "Stablecoins: Survivorship, Transactions Costs and Exchange\n  Microstructure",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Stable coins are not very stable. Cash collateralized coins are more stable,\nbut the overall failure rate is similar to tokens that are not designed to be\nstable. USD Coin, Tether and Dai have the largest Ethereum market shares, and\nthey have an average velocity nearly three times higher than M1. Centralized\nand decentralized exchanges are the most active nodes and largest holders on\nthe blockchain. Four of the top ten tokens have Herfindahl indices higher than\nthe U.S. banking system. Median gas fees for Tether rose more than twelve times\nover the last two years, and nearly twenty times for USD Coin. Transactions of\nunder 50,000 USD can generally be done more cheaply offchain. 24 hour exchange\nturnover in Tether is nearly 60 billion USD. This is comparable to the daily\nvolume at the NYSE and eight times the daily flow in money market mutual funds.\nNarrow bid-ask spreads and depth have attracted HFT participation approaching\n50%\n"
    },
    {
        "paper_id": 2201.01398,
        "authors": "Kazufumi Tsuboi, Naoya Fujiwara, and Ryo Itoh",
        "title": "Influence of trip distance and population density on intra-city mobility\n  patterns in Tokyo during COVID-19 pandemic",
        "comments": "23pages, 9 figures",
        "journal-ref": null,
        "doi": "10.1371/journal.pone.0276741",
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  This study investigates the influence of infection cases of COVID-19 and two\nnon-compulsory lockdowns on human mobility within the Tokyo metropolitan area.\nUsing the data of hourly staying population in each 500m$\\times$500m cell and\ntheir city-level residency, we show that long-distance trips or trips to\ncrowded places decrease significantly when infection cases increase. The same\nresult holds for the two lockdowns, although the second lockdown was less\neffective. Hence, Japanese non-compulsory lockdowns influence mobility in a\nsimilar way to the increase in infection cases. This means that they are\naccepted as alarm triggers for people who are at risk of contracting COVID-19.\n"
    },
    {
        "paper_id": 2201.01433,
        "authors": "Ying Hu, Xiaomin Shi, Zuo Quan Xu",
        "title": "Non-homogeneous stochastic LQ control with regime switching and random\n  coefficients",
        "comments": "arXiv admin note: text overlap with arXiv:2004.11832",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper is concerned with a general non-homogeneous stochastic linear\nquadratic (LQ) control problem with regime switching and random coefficients.\nWe obtain the explicit optimal state feedback control and optimal value for\nthis problem in terms of two systems of backward stochastic differential\nequations (BSDEs): one is the famous stochastic Riccati equation and the other\none is a new linear multi-dimensional BSDE with all coefficients being\nunbounded. The existence and uniqueness of the solutions to these two systems\nof BSDEs are proved by means of BMO martingales and contraction mapping method.\nAt last, the theory is applied to study an asset-liability management problem\nunder the mean-variance criteria.\n"
    },
    {
        "paper_id": 2201.01758,
        "authors": "Benedict Guttman-Kenney and Christopher Firth and John Gathergood",
        "title": "Buy Now, Pay Later (BNPL)...On Your Credit Card",
        "comments": null,
        "journal-ref": "Journal of Behavioral & Experimental Finance, 2023",
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  We provide the first economic research on `buy now, pay later' (BNPL): an\nunregulated FinTech credit product enabling consumers to defer payments into\ninterest-free instalments. We study BNPL using UK credit card transaction data.\nWe document consumers charging BNPL transactions to their credit card. Charging\nof BNPL to credit cards is most prevalent among younger consumers and those\nliving in the most deprived geographies. Charging a $0\\%$ interest, amortizing\nBNPL debt to credit cards - where typical interest rates are $20\\%$ and\namortization schedules decades-long - raises doubts on these consumers' ability\nto pay for BNPL. This prompts a regulatory question as to whether consumers\nshould be allowed to refinance their unsecured debt.\n"
    },
    {
        "paper_id": 2201.01874,
        "authors": "Igor Halperin, Jiayu Liu, Xiao Zhang",
        "title": "Combining Reinforcement Learning and Inverse Reinforcement Learning for\n  Asset Allocation Recommendations",
        "comments": "9 pages, 12 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
        "abstract": "  We suggest a simple practical method to combine the human and artificial\nintelligence to both learn best investment practices of fund managers, and\nprovide recommendations to improve them. Our approach is based on a combination\nof Inverse Reinforcement Learning (IRL) and RL. First, the IRL component learns\nthe intent of fund managers as suggested by their trading history, and recovers\ntheir implied reward function. At the second step, this reward function is used\nby a direct RL algorithm to optimize asset allocation decisions. We show that\nour method is able to improve over the performance of individual fund managers.\n"
    },
    {
        "paper_id": 2201.02045,
        "authors": "Holly Brannelly, Andrea Macrina, Gareth W. Peters",
        "title": "Stochastic measure distortions induced by quantile processes for risk\n  quantification and valuation",
        "comments": "32 pages, 1 figure",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We develop a novel stochastic valuation and premium calculation principle\nbased on probability measure distortions that are induced by quantile processes\nin continuous time. Necessary and sufficient conditions are derived under which\nthe quantile processes satisfy first- and second-order stochastic dominance.\nThe introduced valuation principle relies on stochastic ordering so that the\nvaluation risk-loading, and thus risk premiums, generated by the measure\ndistortion is an ordered parametric family. The quantile processes are\ngenerated by a composite map consisting of a distribution and a quantile\nfunction. The distribution function accounts for model risk in relation to the\nempirical distribution of the risk process, while the quantile function models\nthe response to the risk source as perceived by, e.g., a market agent. This\ngives rise to a system of subjective probability measures that indexes a\nstochastic valuation principle susceptible to probability measure distortions.\nWe use the Tukey-$gh$ family of quantile processes driven by Brownian motion in\nan example that demonstrates stochastic ordering. We consider the conditional\nexpectation under the distorted measure as a member of the time-consistent\nclass of dynamic valuation principles, and extend it to the setting where the\ndriving risk process is multivariate. This requires the introduction of a\ncopula function in the composite map for the construction of quantile\nprocesses, which presents another new element in the risk quantification and\nmodelling framework based on probability measure distortions induced by\nquantile processes.\n"
    },
    {
        "paper_id": 2201.02272,
        "authors": "Edward J. Oughton and William Lehr",
        "title": "Surveying 5G Techno-Economic Research to Inform the Evaluation of 6G\n  Wireless Technologies",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Techno-economic assessment is a fundamental technique engineers use for\nevaluating new communications technologies. However, despite the\ntechno-economics of the fifth cellular generation (5G) being an active research\narea, it is surprising there are few comprehensive evaluations of this growing\nliterature. With mobile network operators deploying 5G across their networks,\nit is therefore an opportune time to appraise current accomplishments and\nreview the state-of-the-art. Such insight can inform the flurry of 6G research\npapers currently underway and help engineers in their mission to provide\naffordable high-capacity, low-latency broadband connectivity, globally. The\nsurvey discusses emerging trends from the 5G techno-economic literature and\nmakes five key recommendations for the design and standardization of Next\nGeneration 6G wireless technologies.\n"
    },
    {
        "paper_id": 2201.0229,
        "authors": "Dongwei Zhao, Mehdi Jafari, Audun Botterud, Apurba Sakti",
        "title": "Strategic Storage Investment in Electricity Markets",
        "comments": null,
        "journal-ref": "2022 IEEE Power & Energy Society General Meeting (PESGM)",
        "doi": "10.1109/PESGM48719.2022.9917198",
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Arbitrage is one important revenue source for energy storage in electricity\nmarkets. However, a large amount of storage in the market will impact the\nenergy price and reduce potential revenues. This can lead to strategic\nbehaviors of profit-seeking storage investors. To study the investors'\nstrategic storage investments, we formulate a non-cooperative game between\ncompeting investors. Each investor decides the storage investment over a long\ninvestment horizon, and operates the storage for arbitrage revenues in the\ndaily electricity market. Different investors can deploy storage with different\ncharacteristics. Their decisions are coupled due to the market price that is\ndetermined by all the investors' decisions. We use market data from California\nISO to characterize the storage impact on the market price, based on which we\nestablish a centralized optimization problem to compute the market equilibrium.\nWe show that an increasing number of investors will increase the market\ncompetition, which reduces investors' profits but increases the total invested\nstorage capacity. Furthermore, we find that a slight increase in the storage\nefficiency (e.g., increased charge and discharge efficiency) can significantly\nimprove an investor's profit share in the market.\n"
    },
    {
        "paper_id": 2201.02397,
        "authors": "Mark Kiermayer, Christian Wei{\\ss}",
        "title": "Neural calibration of hidden inhomogeneous Markov chains -- Information\n  decompression in life insurance",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Markov chains play a key role in a vast number of areas, including life\ninsurance mathematics. Standard actuarial quantities as the premium value can\nbe interpreted as compressed, lossy information about the underlying Markov\nprocess. We introduce a method to reconstruct the underlying Markov chain given\ncollective information of a portfolio of contracts. Our neural architecture\nexplainably characterizes the process by explicitly providing one-step\ntransition probabilities. Further, we provide an intrinsic, economic model\nvalidation to inspect the quality of the information decompression. Lastly, our\nmethodology is successfully tested for a realistic data set of German term life\ninsurance contracts.\n"
    },
    {
        "paper_id": 2201.02441,
        "authors": "Erdinc Akyildirim, Matteo Gambara, Josef Teichmann, Syang Zhou",
        "title": "Applications of Signature Methods to Market Anomaly Detection",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Anomaly detection is the process of identifying abnormal instances or events\nin data sets which deviate from the norm significantly. In this study, we\npropose a signatures based machine learning algorithm to detect rare or\nunexpected items in a given data set of time series type. We present\napplications of signature or randomized signature as feature extractors for\nanomaly detection algorithms; additionally we provide an easy, representation\ntheoretic justification for the construction of randomized signatures. Our\nfirst application is based on synthetic data and aims at distinguishing between\nreal and fake trajectories of stock prices, which are indistinguishable by\nvisual inspection. We also show a real life application by using transaction\ndata from the cryptocurrency market. In this case, we are able to identify pump\nand dump attempts organized on social networks with F1 scores up to 88% by\nmeans of our unsupervised learning algorithm, thus achieving results that are\nclose to the state-of-the-art in the field based on supervised learning.\n"
    },
    {
        "paper_id": 2201.02568,
        "authors": "Debasis Kundu",
        "title": "Stationary GE-Process and its Application in Analyzing Gold Price Data",
        "comments": "26 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/publicdomain/zero/1.0/",
        "abstract": "  In this paper we introduce a new discrete time and continuous state space\nstationary process $\\{X_n; n = 1, 2, \\ldots \\}$, such that $X_n$ follows a\ntwo-parameter generalized exponential (GE) distribution. Joint distribution\nfunctions, characterization and some dependency properties of this new process\nhave been investigated. The GE-process has three unknown parameters, two shape\nparameters and one scale parameter, and due to this reason it is more flexible\nthan the existing exponential process. In presence of the scale parameter, if\nthe two shape parameters are equal, then the maximum likelihood estimators of\nthe unknown parameters can be obtained by solving one non-linear equation and\nif the two shape parameters are arbitrary, then the maximum likelihood\nestimators can be obtained by solving a two dimensional optimization problem.\nTwo {\\color{black} synthetic} data sets, and one real gold-price data set have\nbeen analyzed to see the performance of the proposed model in practice. Finally\nsome generalizations have been indicated.\n"
    },
    {
        "paper_id": 2201.02587,
        "authors": "Zineb El Filali Ech-Chafiq (DAO), Pierre Henry-Labordere (CMAP),\n  J\\'er\\^ome Lelong (DAO)",
        "title": "Pricing Bermudan options using regression trees/random forests",
        "comments": null,
        "journal-ref": "SIAM Journal on Financial Mathematics, In press",
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The value of an American option is the maximized value of the discounted cash\nflows from the option. At each time step, one needs to compare the immediate\nexercise value with the continuation value and decide to exercise as soon as\nthe exercise value is strictly greater than the continuation value. We can\nformulate this problem as a dynamic programming equation, where the main\ndifficulty comes from the computation of the conditional expectations\nrepresenting the continuation values at each time step. In (Longstaff and\nSchwartz, 2001), these conditional expectations were estimated using\nregressions on a finite-dimensional vector space (typically a polynomial\nbasis). In this paper, we follow the same algorithm; only the conditional\nexpectations are estimated using Regression trees or Random forests. We discuss\nthe convergence of the LS algorithm when the standard least squares regression\nis replaced with regression trees. Finally, we expose some numerical results\nwith regression trees and random forests. The random forest algorithm gives\nexcellent results in high dimensions.\n"
    },
    {
        "paper_id": 2201.02729,
        "authors": "Bohdan M. Pavlyshenko",
        "title": "Bitcoin Price Predictive Modeling Using Expert Correction",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  The paper studies the linear model for Bitcoin price which includes\nregression features based on Bitcoin currency statistics, mining processes,\nGoogle search trends, Wikipedia pages visits. The pattern of deviation of\nregression model prediction from real prices is simpler comparing to price time\nseries. It is assumed that this pattern can be predicted by an experienced\nexpert. In such a way, using the combination of the regression model and expert\ncorrection, one can receive better results than with either regression model or\nexpert opinion only. It is shown that Bayesian approach makes it possible to\nutilize the probabilistic approach using distributions with fat tails and take\ninto account the outliers in Bitcoin price time series.\n"
    },
    {
        "paper_id": 2201.02752,
        "authors": "Masaaki Fukasawa",
        "title": "On asymptotically arbitrage-free approximations of the implied\n  volatility",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Following-up Fukasawa and Gatheral (Frontiers of Mathematical Finance, 2022),\nwe prove that the BBF formula, the SABR formula, and the rough SABR formula\nprovide asymptotically arbitrage-free approximations of the implied volatility\nunder, respectively, the local volatility model, the SABR model, and the rough\nSABR model.\n"
    },
    {
        "paper_id": 2201.0276,
        "authors": "F\\'elix Lirio-Loli and William Dextre-Mart\\'inez",
        "title": "Bibliometric analysis of the scientific production found in Scopus and\n  Web of Science about business administration",
        "comments": "11 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Introduction: This study analyzes the scientific production in business\nadministration in scientific articles based on modeling partial least squares\nstructural equations (Partial Least Squares Structural Equation Modeling\nPLS-SEM) in the 2011-2020 period. Methodology: The study is exploratory -\ndescriptive and has three phases: a) Selection of keywords and search criteria;\n(b) Search and refinement of information; c) information analysis. A method of\nbibliometric review of the specific literature has been used based on the\nanalysis of predefined indicators and completed with a qualitative content\nsynthesis. Results: A total of 167 publications were analyzed, making\ncorrelations from the year, search criteria, authors, impact factor by\nquartile, and by citation variables. More outstanding scientific production\ncomes from Scopus under the search criteria ((pls AND sem) OR \"partial least\nsquares\") AND (business OR management), being the figure of 4,870 scientific\narticles, while Web of Science accumulates 3,946 articles Conclusion: There has\nbeen a progressive growth in scientific articles with the PLS-SEM technique\nfrom 2011 to 2020. Scopus, compared to WoS, presents a more significant number\nof scientific productions with this statistical approach. The authors who\nregister scientific articles demonstrate a high H index; in addition, there is\nan important number of scientific articles with a PLS-SEM approach in\nuniversities in Malaysia that could be related to the expansion of higher\neducation in that country, as well as in Singapore, Taiwan, and Indonesia.\nFinally, business administration, accounting, and economics are outstanding\nscientific production.\n"
    },
    {
        "paper_id": 2201.02773,
        "authors": "Dylan Herman, Cody Googin, Xiaoyuan Liu, Alexey Galda, Ilya Safro, Yue\n  Sun, Marco Pistoia, Yuri Alexeev",
        "title": "A Survey of Quantum Computing for Finance",
        "comments": "60 pages, 5 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Quantum computers are expected to surpass the computational capabilities of\nclassical computers during this decade and have transformative impact on\nnumerous industry sectors, particularly finance. In fact, finance is estimated\nto be the first industry sector to benefit from quantum computing, not only in\nthe medium and long terms, but even in the short term. This survey paper\npresents a comprehensive summary of the state of the art of quantum computing\nfor financial applications, with particular emphasis on stochastic modeling,\noptimization, and machine learning, describing how these solutions, adapted to\nwork on a quantum computer, can potentially help to solve financial problems,\nsuch as derivative pricing, risk modeling, portfolio optimization, natural\nlanguage processing, and fraud detection, more efficiently and accurately. We\nalso discuss the feasibility of these algorithms on near-term quantum computers\nwith various hardware implementations and demonstrate how they relate to a wide\nrange of use cases in finance. We hope this article will not only serve as a\nreference for academic researchers and industry practitioners but also inspire\nnew ideas for future research.\n"
    },
    {
        "paper_id": 2201.02828,
        "authors": "Marcin Pitera and {\\L}ukasz Stettner",
        "title": "Discrete-time risk sensitive portfolio optimization with proportional\n  transaction costs",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  In this paper we consider a discrete-time risk sensitive portfolio\noptimization over a long time horizon with proportional transaction costs. We\nshow that within the log-return i.i.d. framework the solution to a suitable\nBellman equation exists under minimal assumptions and can be used to\ncharacterize the optimal strategies for both risk-averse and risk-seeking\ncases. Moreover, using numerical examples, we show how a Bellman equation\nanalysis can be used to construct or refine optimal trading strategies in the\npresence of transaction costs.\n"
    },
    {
        "paper_id": 2201.02857,
        "authors": "Mayukh Mukhopadhyay and Sangeeta Sahney",
        "title": "Effect of Toxic Review Content on Overall Product Sentiment",
        "comments": "43 pages,30 figures, 2 tables",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Toxic contents in online product review are a common phenomenon. A content is\nperceived to be toxic when it is rude, disrespectful, or unreasonable and make\nindividuals leave the discussion. Machine learning algorithms helps the sell\nside community to identify such toxic patterns and eventually moderate such\ninputs. Yet, the extant literature provides fewer information about the\nsentiment of a prospective consumer on the perception of a product after being\nexposed to such toxic review content. In this study, we collect a balanced data\nset of review comments from 18 different players segregated into three\ndifferent sectors from google play-store. Then we calculate the sentence-level\nsentiment and toxicity score of individual review content. Finally, we use\nstructural equation modelling to quantitatively study the influence of toxic\ncontent on overall product sentiment. We observe that comment toxicity\nnegatively influences overall product sentiment but do not exhibit a mediating\neffect over reviewer score to influence sector-wise relative rating.\n"
    },
    {
        "paper_id": 2201.02916,
        "authors": "Santiago Camara",
        "title": "TANK meets Diaz-Alejandro: Household heterogeneity, non-homothetic\n  preferences & policy design",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper studies the role of households' heterogeneity in access to\nfinancial markets and the consumption of commodity goods in the transmission of\nforeign shocks. First, I use survey data from Uruguay to show that low income\nhouseholds have poor to no access to savings technology while spending a\nsignificant share of their income on commodity-based goods. Second, I construct\na Two-Agent New Keynesian (TANK) small open economy model with two main\nfeatures: (i) limited access to financial markets, and (ii) non-homothetic\npreferences over commodity goods. I show how these features shape aggregate\ndynamics and amplify foreign shocks. Additionally, I argue that these features\nintroduce a redistribution channel for monetary policy and a rationale for\n\"fear-of-floating\" exchange rate regimes. Lastly, I study the design of optimal\npolicy regimes and find that households have opposing preferences a over\nmonetary and fiscal rules.\n"
    },
    {
        "paper_id": 2201.02919,
        "authors": "Hayato Kato, and Hirofumi Okoshi",
        "title": "Economic Integration and Agglomeration of Multinational Production with\n  Transfer Pricing",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Do low corporate taxes always favor multinational production over economic\nintegration? We propose a two-country model in which multinationals choose the\nlocations of production plants and foreign distribution affiliates and shift\nprofits between them through transfer prices. With high trade costs, plants are\nconcentrated in the low-tax country; surprisingly, this pattern reverses with\nlow trade costs. Indeed, economic integration has a non-monotonic impact:\nfalling trade costs first decrease and then increase the plant share in the\nhigh-tax country, which we empirically confirm. Moreover, allowing for transfer\npricing makes tax competition tougher and international coordination on\ntransfer-pricing regulation can be beneficial.\n"
    },
    {
        "paper_id": 2201.02958,
        "authors": "Wenjia Wang and Yanyuan Wang and Xiaowei Zhang",
        "title": "Smooth Nested Simulation: Bridging Cubic and Square Root Convergence\n  Rates in High Dimensions",
        "comments": "Main body: 46 pages, 5 figures, 5 tables; Supplemental material: 28\n  pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  Nested simulation concerns estimating functionals of a conditional\nexpectation via simulation. In this paper, we propose a new method based on\nkernel ridge regression to exploit the smoothness of the conditional\nexpectation as a function of the multidimensional conditioning variable.\nAsymptotic analysis shows that the proposed method can effectively alleviate\nthe curse of dimensionality on the convergence rate as the simulation budget\nincreases, provided that the conditional expectation is sufficiently smooth.\nThe smoothness bridges the gap between the cubic root convergence rate (that\nis, the optimal rate for the standard nested simulation) and the square root\nconvergence rate (that is, the canonical rate for the standard Monte Carlo\nsimulation). We demonstrate the performance of the proposed method via\nnumerical examples from portfolio risk management and input uncertainty\nquantification.\n"
    },
    {
        "paper_id": 2201.02983,
        "authors": "Oleh Danyliv",
        "title": "Market Impact of Small Orders",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The article is an empirical study of market impact through order book events.\nIt describes a mechanism of extracting an average participation rate and a\nmarket impact of small orders which represent individual slices of large\nmetaorders. The study is based on tick data for futures contracts. It is shown\nthat the impact could be either linear or a concave function as a function of\ntrading volume, depending on the instrument. After normalisation, this\ndependency is shown to be very similar for a wide range of instruments.\n  A simple yet effective model for market impact estimation is proposed. This\nmodel is linear in nature and is derived based on straightforward\nmicrostructure reasoning. The estimation shows satisfactory results for both\nconcave and linear market impact volume dependencies.\n"
    },
    {
        "paper_id": 2201.02987,
        "authors": "Jinping Zhang and Keming Zhang",
        "title": "Portfolio selection models based on interval-valued conditional value at\n  risk (ICVaR) and empirical analysis",
        "comments": "15 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Risk management is very important for individual investors or companies.\nThere are many ways to measure the risk of investment. Prices of risky assets\nvary rapidly and randomly due to the complexity of finance market. Random\ninterval is a good tool to describe uncertainty with both randomness and\nimprecision. Considering the uncertainty of financial market, we employ random\nintervals to describe the returns of a risk asset and consider the tail risk,\nwhich is called the interval-valued Conditional Value at Risk (ICVaR, for\nshort). Such an ICVaR is a risk measure and satisfies subadditivity. Under the\nnew risk measure ICVaR, as a manner similar to the classical portfolio model of\nMarkowitz, optimal interval-valued portfolio selection models are built. Based\non the real data from mainland Chinese stock market, the case study shows that\nour models are interpretable and consistent with the practical scenarios.\n"
    },
    {
        "paper_id": 2201.03092,
        "authors": "Xiyang Hu, Yan Huang, Beibei Li, Tian Lu",
        "title": "Uncovering the Source of Machine Bias",
        "comments": "accepted by KDD 2021, MLCM workshop",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We develop a structural econometric model to capture the decision dynamics of\nhuman evaluators on an online micro-lending platform, and estimate the model\nparameters using a real-world dataset. We find two types of biases in gender,\npreference-based bias and belief-based bias, are present in human evaluators'\ndecisions. Both types of biases are in favor of female applicants. Through\ncounterfactual simulations, we quantify the effect of gender bias on loan\ngranting outcomes and the welfare of the company and the borrowers. Our results\nimply that both the existence of the preference-based bias and that of the\nbelief-based bias reduce the company's profits. When the preference-based bias\nis removed, the company earns more profits. When the belief-based bias is\nremoved, the company's profits also increase. Both increases result from\nraising the approval probability for borrowers, especially male borrowers, who\neventually pay back loans. For borrowers, the elimination of either bias\ndecreases the gender gap of the true positive rates in the credit risk\nevaluation. We also train machine learning algorithms on both the real-world\ndata and the data from the counterfactual simulations. We compare the decisions\nmade by those algorithms to see how evaluators' biases are inherited by the\nalgorithms and reflected in machine-based decisions. We find that machine\nlearning algorithms can mitigate both the preference-based bias and the\nbelief-based bias.\n"
    },
    {
        "paper_id": 2201.03213,
        "authors": "Mei-Ling Cai, Zhang-HangJian Chen, Sai-Ping Li, Xiong Xiong, Wei\n  Zhang, Ming-Yuan Yang, Fei Ren",
        "title": "New volatility evolution model after extreme events",
        "comments": null,
        "journal-ref": null,
        "doi": "10.1016/j.chaos.2021.111608",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper, we propose a new dynamical model to study the two-stage\nvolatility evolution of stock market index after extreme events, and find that\nthe volatility after extreme events follows a stretched exponential decay in\nthe initial stage and becomes a power law decay at later times by using high\nfrequency minute data. Empirical study of the evolutionary behaviors of\nvolatility after endogenous and exogenous events further demonstrates the\ndescriptive power of our new model. To further explore the underlying\nmechanisms of volatility evolution, we introduce the sequential arrival of\ninformation hypothesis (SAIH) and the mixture of distribution hypothesis (MDH)\nto test the two-stage assumption, and find that investors transform from the\nuninformed state to the informed state in the first stage and informed\ninvestors subsequently dominate in the second stage. The testing results offer\na supporting explanation for the validity of our new model and the fitted\nvalues of relevant parameters.\n"
    },
    {
        "paper_id": 2201.03378,
        "authors": "A.H. Nzokem",
        "title": "Pricing European Options under Stochastic Volatility Models: Case of\n  five-Parameter Variance-Gamma Process",
        "comments": "28 pages",
        "journal-ref": "J. Risk Financial Manag. 2023, 16(1), 55",
        "doi": "10.3390/jrfm16010055",
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  The paper builds a Variance-Gamma (VG) model with five parameters: location\n($\\mu$), symmetry ($\\delta$), volatility ($\\sigma$), shape ($\\alpha$), and\nscale ($\\theta$); and studies its application to the pricing of European\noptions. The results of our analysis show that the five-parameter VG model is a\nstochastic volatility model with a $\\Gamma(\\alpha, \\theta)$ Ornstein-Uhlenbeck\ntype process; the associated L\\'evy density of the VG model is a KoBoL family\nof order $\\nu=0$, intensity $\\alpha$, and steepness parameters\n$\\frac{\\delta}{\\sigma^2} - \\sqrt{\\frac{\\delta^2}{\\sigma^4}+\\frac{2}{\\theta\n\\sigma^2}}$ and $\\frac{\\delta}{\\sigma^2}+\n\\sqrt{\\frac{\\delta^2}{\\sigma^4}+\\frac{2}{\\theta \\sigma^2}}$; and the VG process\nconverges asymptotically in distribution to a L\\'evy process driven by a normal\ndistribution with mean $(\\mu + \\alpha \\theta \\delta)$ and variance $\\alpha\n(\\theta^2\\delta^2 + \\sigma^2\\theta)$. The data used for empirical analysis were\nobtained by fitting the five-parameter Variance-Gamma (VG) model to the\nunderlying distribution of the daily SPY ETF data. Regarding the application of\nthe five-parameter VG model, the twelve-point rule Composite Newton-Cotes\nQuadrature and Fractional Fast Fourier (FRFT) algorithms were implemented to\ncompute the European option price. Compared to the Black-Scholes (BS) model,\nempirical evidence shows that the VG option price is underpriced for\nout-of-the-money (OTM) options and overpriced for in-the-money (ITM) options.\nBoth models produce almost the same option pricing results for deep\nout-of-the-money (OTM) and deep-in-the-money (ITM) options\n"
    },
    {
        "paper_id": 2201.03519,
        "authors": "Andrew Kirillov and Sehyun Chung",
        "title": "StableSims: Optimizing MakerDAO Liquidations 2.0 Incentives via\n  Agent-Based Modeling",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  The StableSims project set out to determine optimal parameters for the new\nauction mechanism, Liquidations 2.0, used by MakerDAO, a protocol built on\nEthereum offering a decentralized, collateralized stablecoin called Dai. We\ndeveloped an agent-based simulation that emulates both the Maker protocol smart\ncontract logic, and how profit-motivated agents (\"keepers\") will act in the\nreal world when faced with decisions such as liquidating \"vaults\"\n(collateralized debt positions) and bidding on collateral auctions. This\nresearch focuses on the incentive structure introduced in Liquidations 2.0,\nwhich implements both a constant fee (tip) and a fee proportional to vault size\n(chip) paid to keepers that liquidate vaults or restart stale collateral\nauctions. We sought to minimize the amount paid in incentives while maximizing\nthe speed with which undercollateralized vaults were liquidated. Our findings\nindicate that it is more cost-effective to increase the constant fee, as\nopposed to the proportional fee, in order to decrease the time it takes for\nkeepers to liquidate vaults.\n"
    },
    {
        "paper_id": 2201.03717,
        "authors": "Marcos Escobar-Anel, Matt Davison, Yichen Zhu",
        "title": "Derivatives-based portfolio decisions. An expected utility insight",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  This paper challenges the use of stocks in portfolio construction, instead we\ndemonstrate that Asian derivatives, straddles, or baskets could be more\nconvenient substitutes. Our results are obtained under the assumptions of the\nBlack--Scholes--Merton setting, uncovering a hidden benefit of derivatives that\ncomplements their well-known gains for hedging, risk management, and to\nincrease utility in market incompleteness. The new insights are also\ntransferable to more advanced stochastic settings. The analysis relies on the\ninfinite number of optimal choices of derivatives for a maximized expected\nutility (EUT) agent; we propose risk exposure minimization as an additional\noptimization criterion inspired by regulations. Working with two assets, for\nsimplicity, we demonstrate that only two derivatives are needed to maximize\nutility while minimizing risky exposure. In a comparison among one-asset\noptions, e.g. American, European, Asian, Calls and Puts, we demonstrate that\nthe deepest out-of-the-money Asian products available are the best choices to\nminimize exposure. We also explore optimal selections among straddles, which\nare better practical choices than out-of-the-money Calls and Puts due to\nliquidity and rebalancing needs. The optimality of multi-asset derivatives is\nalso considered, establishing that a basket option could be a better choice\nthan one-asset Asian call/put in many realistic situations.\n"
    },
    {
        "paper_id": 2201.04038,
        "authors": "Wendi Li, Xiao Yang, Weiqing Liu, Yingce Xia, Jiang Bian",
        "title": "DDG-DA: Data Distribution Generation for Predictable Concept Drift\n  Adaptation",
        "comments": "Accepted by AAAI'22",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In many real-world scenarios, we often deal with streaming data that is\nsequentially collected over time. Due to the non-stationary nature of the\nenvironment, the streaming data distribution may change in unpredictable ways,\nwhich is known as concept drift. To handle concept drift, previous methods\nfirst detect when/where the concept drift happens and then adapt models to fit\nthe distribution of the latest data. However, there are still many cases that\nsome underlying factors of environment evolution are predictable, making it\npossible to model the future concept drift trend of the streaming data, while\nsuch cases are not fully explored in previous work.\n  In this paper, we propose a novel method DDG-DA, that can effectively\nforecast the evolution of data distribution and improve the performance of\nmodels. Specifically, we first train a predictor to estimate the future data\ndistribution, then leverage it to generate training samples, and finally train\nmodels on the generated data. We conduct experiments on three real-world tasks\n(forecasting on stock price trend, electricity load and solar irradiance) and\nobtain significant improvement on multiple widely-used models.\n"
    },
    {
        "paper_id": 2201.042,
        "authors": "Erik Brynjolfsson",
        "title": "The Turing Trap: The Promise & Peril of Human-Like Artificial\n  Intelligence",
        "comments": "Forthcoming in Daedalus, April 2022. Posted with permission",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In 1950, Alan Turing proposed an imitation game as the ultimate test of\nwhether a machine was intelligent: could a machine imitate a human so well that\nits answers to questions indistinguishable from a human. Ever since, creating\nintelligence that matches human intelligence has implicitly or explicitly been\nthe goal of thousands of researchers, engineers, and entrepreneurs. The\nbenefits of human-like artificial intelligence (HLAI) include soaring\nproductivity, increased leisure, and perhaps most profoundly, a better\nunderstanding of our own minds.\n  But not all types of AI are human-like. In fact, many of the most powerful\nsystems are very different from humans. So an excessive focus on developing and\ndeploying HLAI can lead us into a trap. As machines become better substitutes\nfor human labor, workers lose economic and political bargaining power and\nbecome increasingly dependent on those who control the technology. In contrast,\nwhen AI is focused on augmenting humans rather than mimicking them, then humans\nretain the power to insist on a share of the value created. Furthermore,\naugmentation creates new capabilities and new products and services, ultimately\ngenerating far more value than merely human-like AI. While both types of AI can\nbe enormously beneficial, there are currently excess incentives for automation\nrather than augmentation among technologists, business executives, and\npolicymakers.\n"
    },
    {
        "paper_id": 2201.04393,
        "authors": "J\\'er\\'emi Assael, Laurent Carlier and Damien Challet",
        "title": "Dissecting the explanatory power of ESG features on equity returns by\n  sector, capitalization, and year with interpretable machine learning",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We systematically investigate the links between price returns and\nEnvironment, Social and Governance (ESG) scores in the European equity market.\nUsing interpretable machine learning, we examine whether ESG scores can explain\nthe part of price returns not accounted for by classic equity factors,\nespecially the market one. We propose a cross-validation scheme with random\ncompany-wise validation to mitigate the relative initial lack of quantity and\nquality of ESG data, which allows us to use most of the latest and best data to\nboth train and validate our models. Gradient boosting models successfully\nexplain the part of annual price returns not accounted for by the market\nfactor. We check with benchmark features that ESG data explain significantly\nbetter price returns than basic fundamental features alone. The most relevant\nESG score encodes controversies. Finally, we find the opposite effects of\nbetter ESG scores on the price returns of small and large capitalization\ncompanies: better ESG scores are generally associated with larger price returns\nfor the latter and reversely for the former.\n"
    },
    {
        "paper_id": 2201.04699,
        "authors": "Gabriel Borrageiro, Nick Firoozye, Paolo Barucca",
        "title": "The Recurrent Reinforcement Learning Crypto Agent",
        "comments": null,
        "journal-ref": "IEEE Access, vol. 10, pp. 38590-38599, 2022",
        "doi": "10.1109/ACCESS.2022.3166599",
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  We demonstrate a novel application of online transfer learning for a digital\nassets trading agent. This agent uses a powerful feature space representation\nin the form of an echo state network, the output of which is made available to\na direct, recurrent reinforcement learning agent. The agent learns to trade the\nXBTUSD (Bitcoin versus US Dollars) perpetual swap derivatives contract on\nBitMEX on an intraday basis. By learning from the multiple sources of impact on\nthe quadratic risk-adjusted utility that it seeks to maximise, the agent avoids\nexcessive over-trading, captures a funding profit, and can predict the market's\ndirection. Overall, our crypto agent realises a total return of 350\\%, net of\ntransaction costs, over roughly five years, 71\\% of which is down to funding\nprofit. The annualised information ratio that it achieves is 1.46.\n"
    },
    {
        "paper_id": 2201.0488,
        "authors": "Per Pettersson-Lidbom",
        "title": "Exit, Voice and Political Change: Evidence from Swedish Mass Migration\n  to the United States; A Comment",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  In this comment, I revisit the question raised in Karadja and Prawitz (2019)\nconcerning a causal relationship between mass emigration and long-run political\noutcomes. I discuss a number of potential problems with their instrumental\nvariable analysis. First, there are at least three reasons why their instrument\nviolates the exclusion restriction: (i) failing to control for internal\nmigration, (ii) insufficient control for confounders correlated with their\ninstrument, and (iii) emigration measured with a nonclassical measurement\nerror. Second, I also discuss two problems with the statistical inference, both\nof which indicate that the instrument does not fulfill the relevance condition,\ni.e., the instrument is not sufficiently correlated with the endogenous\nvariable emigration. Correcting for any of these problems reveals that there is\nno relationship between emigration and political outcomes.\n"
    },
    {
        "paper_id": 2201.04965,
        "authors": "Yu Zhao, Huaming Du, Ying Liu, Shaopeng Wei, Xingyan Chen, Fuzhen\n  Zhuang, Qing Li, Ji Liu, Gang Kou",
        "title": "Stock Movement Prediction Based on Bi-typed Hybrid-relational Market\n  Knowledge Graph via Dual Attention Networks",
        "comments": "22 pages, 5 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Stock Movement Prediction (SMP) aims at predicting listed companies' stock\nfuture price trend, which is a challenging task due to the volatile nature of\nfinancial markets. Recent financial studies show that the momentum spillover\neffect plays a significant role in stock fluctuation. However, previous studies\ntypically only learn the simple connection information among related companies,\nwhich inevitably fail to model complex relations of listed companies in the\nreal financial market. To address this issue, we first construct a more\ncomprehensive Market Knowledge Graph (MKG) which contains bi-typed entities\nincluding listed companies and their associated executives, and\nhybrid-relations including the explicit relations and implicit relations.\nAfterward, we propose DanSmp, a novel Dual Attention Networks to learn the\nmomentum spillover signals based upon the constructed MKG for stock prediction.\nThe empirical experiments on our constructed datasets against nine SOTA\nbaselines demonstrate that the proposed DanSmp is capable of improving stock\nprediction with the constructed MKG.\n"
    },
    {
        "paper_id": 2201.04981,
        "authors": "Jackson P. Lautier, Vladimir Pozdnyakov, Jun Yan",
        "title": "Pricing Time-to-Event Contingent Cash Flows: A Discrete-Time Survival\n  Analysis Approach",
        "comments": "50 pages, 6 figures, 3 tables, contact corresponding author to obtain\n  data",
        "journal-ref": null,
        "doi": "10.1016/j.insmatheco.2023.02.003",
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Prudent management of insurance investment portfolios requires competent\nasset pricing of fixed-income assets with time-to-event contingent cash flows,\nsuch as consumer asset-backed securities (ABS). Current market pricing\ntechniques for these assets either rely on a non-random time-to-event model or\nmay not utilize detailed asset-level data that is now available with most\npublic transactions. We first establish a framework capable of yielding\nestimates of the time-to-event random variable from securitization data, which\nis discrete and often subject to left-truncation and right-censoring. We then\nshow that the vector of discrete-time hazard rate estimators is asymptotically\nmultivariate normal with independent components, which has not yet been done in\nthe statistical literature in the case of both left-truncation and\nright-censoring. The time-to-event distribution estimates are then fed into our\ncash flow model, which is capable of calculating a formulaic price of a pool of\ntime-to-event contingent cash flows vis-\\'{a}-vis calculating an expected\npresent value with respect to the estimated time-to-event distribution. In an\napplication to a subset of 29,845 36-month leases from the Mercedes-Benz Auto\nLease Trust 2017-A (MBALT 2017-A) bond, our pricing model yields estimates\ncloser to the actual realized future cash flows than the non-random\ntime-to-event model, especially as the fitting window increases. Finally, in\ncertain settings, the asymptotic properties of the hazard rate estimators allow\ninvestors to assess the potential uncertainty of the price point estimates,\nwhich we illustrate for a subset of 493 24-month leases from MBALT 2017-A.\n"
    },
    {
        "paper_id": 2201.05103,
        "authors": "S{\\o}ren Fiig Jarner and Michael Preisel",
        "title": "Analysis of a five-factor capital market model",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  In this paper we analyse the five-factor capital market model of Munk et\nal.(2004). The model features a Vasicek interest rate model, an equity index\nwith mean-reverting excess return and an index for realized inflation with\nmean-reverting expectation. The primary aim of the analysis is to facilitate\nso-called exact simulation from the model on a set of discrete time points. It\nturns out that this can be achieved by sampling from a (degenerate)\nseven-dimensional normal distribution. We derive the distributional results\nnecessary and describe how to overcome the rank deficiency of the\nvariance-covariance matrix in practice.\n  The tradeable assets in the original model consist of cash, nominal bonds and\nstocks. We extend the investment universe to also include inflation bonds by\nderiving the arbitrage free break-even inflation (BEI) curve for a\nthree-parameter specification of the two market prices of inflation risk.\nFinally, we provide a number of auxiliary results regarding the dynamics of\nconstant-maturity nominal and inflation bond indices, the distribution of the\nstock index in nominal and real terms, and the distribution of the Sharpe ratio\nfor individual assets and portfolios with an application to factor investing.\n"
    },
    {
        "paper_id": 2201.05312,
        "authors": "Dean Buckner, Kevin Dowd, and Hardy Hulley",
        "title": "Arbitrage Problems with Reflected Geometric Brownian Motion",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  Contrary to the claims made by several authors, a financial market model in\nwhich the price of a risky security follows a reflected geometric Brownian\nmotion is not arbitrage-free. In fact, such models violate even the weakest\nno-arbitrage condition considered in the literature. Consequently, they do not\nadmit num\\'eraire portfolios or equivalent risk-neutral probability measures,\nwhich makes them totally unsuitable for contingent claim valuation.\nUnsurprisingly, the published option pricing formulae for such models violate\ntextbook no-arbitrage bounds.\n"
    },
    {
        "paper_id": 2201.05316,
        "authors": "Dejian Tian",
        "title": "Pricing principle via Tsallis relative entropy in incomplete market",
        "comments": "30 pages, to appear in SIAM Journal on Financial Mathematics",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  A pricing principle is introduced for non-attainable $q$-exponential bounded\ncontingent claims in an incomplete Brownian motion market setting. The buyer\nevaluates the contingent claim under the ``distorted Radon-Nikodym derivative''\nand adjustment by Tsallis relative entropy over a family of equivalent\nmartingale measures. The pricing principle is proved to be a time consistent\nand arbitrage-free pricing rule. More importantly, this pricing principle is\nfound to be closely related to backward stochastic differential equations with\ngenerators $f(y)|z|^2$ type. The pricing functional is compatible with prices\nfor attainable claims. Except translation invariance, the pricing principle\nprocesses lots of elegant properties such as monotonicity and concavity etc.\nThe pricing functional is showed between minimal martingale measure pricing and\nconditional certainty equivalent pricing under $q$-exponential utility. The\nasymptotic behavior of the pricing principle for ambiguity aversion coefficient\nis also investigated.\n"
    },
    {
        "paper_id": 2201.05375,
        "authors": "S{\\o}ren Fiig Jarner",
        "title": "Strategic mean-variance investing under mean-reverting stock returns",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  In this report we derive the strategic (deterministic) allocation to bonds\nand stocks resulting in the optimal mean-variance trade-off on a given\ninvestment horizon. The underlying capital market features a mean-reverting\nprocess for equity returns, and the primary question of interest is how\nmean-reversion effects the optimal strategy and the resulting portfolio value\nat the horizon. In particular, we are interested in knowing under which\nassumptions and on which horizons, the risk-reward trade-off is so favourable\nthat the value of the portfolio is effectively bounded from below on the\nhorizon. In this case, we might think of the portfolio as providing a\nstochastic excess return on top of a \"guarantee\" (the lower bound).\n  Deriving optimal strategies is a well-known discipline in mathematical\nfinance. The modern approach is to derive and solve the Hamilton-Jacobi-Bellman\n(HJB) differential equation characterizing the strategy leading to highest\nexpected utility, for given utility function. However, for two reasons we\napproach the problem differently in this work. First, we wish to find the\noptimal strategy depending on time only, i.e., we do not allow for dependencies\non capital market state variables, nor the value of the portfolio itself. This\nconstraint characterizes the strategic allocation of long-term investors.\nSecond, to gain insights on the role of mean-reversion, we wish to identify the\nentire family of extremal strategies, not only the optimal strategies. To\nderive the strategies we employ methods from calculus of variations, rather\nthan the usual HJB approach.\n"
    },
    {
        "paper_id": 2201.0557,
        "authors": "Jaydip Sen, Ashwin Kumar R S, Geetha Joseph, Kaushik Muthukrishnan,\n  Koushik Tulasi, and Praveen Varukolu",
        "title": "Precise Stock Price Prediction for Robust Portfolio Design from Selected\n  Sectors of the Indian Stock Market",
        "comments": "The report is 16 pages long. It contains 47 figures and 71 tables.\n  The report is based on the capstone project done in the post graduate course\n  of data science in Praxis Business School, Kolkata, India - Group 2 of Spring\n  Batch, 2021",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Stock price prediction is a challenging task and a lot of propositions exist\nin the literature in this area. Portfolio construction is a process of choosing\na group of stocks and investing in them optimally to maximize the return while\nminimizing the risk. Since the time when Markowitz proposed the Modern\nPortfolio Theory, several advancements have happened in the area of building\nefficient portfolios. An investor can get the best benefit out of the stock\nmarket if the investor invests in an efficient portfolio and could take the buy\nor sell decision in advance, by estimating the future asset value of the\nportfolio with a high level of precision. In this project, we have built an\nefficient portfolio and to predict the future asset value by means of\nindividual stock price prediction of the stocks in the portfolio. As part of\nbuilding an efficient portfolio we have studied multiple portfolio optimization\nmethods beginning with the Modern Portfolio theory. We have built the minimum\nvariance portfolio and optimal risk portfolio for all the five chosen sectors\nby using past daily stock prices over the past five years as the training data,\nand have also conducted back testing to check the performance of the portfolio.\nA comparative study of minimum variance portfolio and optimal risk portfolio\nwith equal weight portfolio is done by backtesting.\n"
    },
    {
        "paper_id": 2201.05574,
        "authors": "Yulin Liu, Yuxuan Lu, Kartik Nayak, Fan Zhang, Luyao Zhang and Yinhong\n  Zhao",
        "title": "Empirical Analysis of EIP-1559: Transaction Fees, Waiting Time, and\n  Consensus Security",
        "comments": null,
        "journal-ref": null,
        "doi": "10.1145/3548606.3559341",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  A transaction fee mechanism (TFM) is an essential component of a blockchain\nprotocol. However, a systematic evaluation of the real-world impact of TFMs is\nstill absent. Using rich data from the Ethereum blockchain, the mempool, and\nexchanges, we study the effect of EIP-1559, one of the earliest-deployed TFMs\nthat depart from the traditional first-price auction paradigm. We conduct a\nrigorous and comprehensive empirical study to examine its causal effect on\nblockchain transaction fee dynamics, transaction waiting times, and consensus\nsecurity. Our results show that EIP-1559 improves the user experience by\nmitigating intrablock differences in the gas price paid and reducing users'\nwaiting times. However, EIP-1559 has only a small effect on gas fee levels and\nconsensus security. In addition, we find that when Ether's price is more\nvolatile, the waiting time is significantly higher. We also verify that a\nlarger block size increases the presence of siblings. These findings suggest\nnew directions for improving TFMs.\n"
    },
    {
        "paper_id": 2201.05686,
        "authors": "\\c{C}a\\u{g}{\\i}n Ararat, Bar{\\i}\\c{s} Bilir, Elisa Mastrogiacomo",
        "title": "Decomposable sums and their implications on naturally quasiconvex risk\n  measures",
        "comments": "55 pages, 3 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Convexity and quasiconvexity are two properties that capture the concept of\ndiversification for risk measures. Between the two, there is natural\nquasiconvexity, an old but not so well-known property weaker than convexity but\nstronger than quasiconvexity. A detailed discussion on natural quasiconvexity\nis still missing and this paper aims to fill this gap in the setting of\nconditional risk measures. We relate natural quasiconvexity to additively\ndecomposable sums. The notion of convexity index, defined in 1980s for\nfinite-dimensional vector spaces, plays a crucial role in the discussion of\ndecomposable sums. We propose a general treatment of convexity index in\ntopological vector spaces and use it to study naturally quasiconvex risk\nmeasures. We prove that natural quasiconvexity and convexity are equivalent for\nconditional risk measures on $L^p$ spaces, $p \\geq 1$, under mild continuity\nand locality conditions. Finally, we discuss an alternative notion of locality\nwith respect to an orthonormal basis in $L^2$.\n"
    },
    {
        "paper_id": 2201.05709,
        "authors": "David Ardia, Keven Bluteau, Thien Duy Tran",
        "title": "How easy is it for investment managers to deploy their talent in green\n  and brown stocks?",
        "comments": null,
        "journal-ref": "Finance Research Letters, Volume 48, August 2022, 102992",
        "doi": "10.1016/j.frl.2022.102992",
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  We explore the realized alpha-performance heterogeneity in green and brown\nstocks' universes using the peer performance ratios of Ardia and Boudt (2018).\nFocusing on S&P 500 index firms over 2014-2020 and defining peer groups in\nterms of firms' greenhouse gas emission levels, we find that, on average, about\n20% of the stocks differentiate themselves from their peers in terms of future\nperformance. We see a much higher time-variation in this opportunity set within\nbrown stocks. Furthermore, the performance heterogeneity has decreased over\ntime, especially for green stocks, implying that it is now more difficult for\ninvestment managers to deploy their skills when choosing among low-GHG\nintensity stocks.\n"
    },
    {
        "paper_id": 2201.05854,
        "authors": "Anindya Goswami and Kuldip Singh Patel",
        "title": "Matrix method stability and robustness of compact schemes for parabolic\n  PDEs",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  The fully discrete problem for convection-diffusion equation is considered.\nIt comprises compact approximations for spatial discretization, and\nCrank-Nicolson scheme for temporal discretization. The expressions for the\nentries of inverse of tridiagonal Toeplitz matrix, and Gerschgorin circle\ntheorem have been applied to locate the eigenvalues of the amplification\nmatrix. An upper bound on the condition number of a relevant matrix is derived.\nIt is shown to be of order $\\mathcal{O}\\left(\\frac{\\delta v}{\\delta\nz^2}\\right)$, where $\\delta v$ and $\\delta z$ are time and space step sizes\nrespectively. Some numerical illustrations have been added to complement the\ntheoretical findings.\n"
    },
    {
        "paper_id": 2201.05906,
        "authors": "Mohsen Asgari, Seyed Hossein Khasteh",
        "title": "Profitable Strategy Design by Using Deep Reinforcement Learning for\n  Trades on Cryptocurrency Markets",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  Deep Reinforcement Learning solutions have been applied to different control\nproblems with outperforming and promising results. In this research work we\nhave applied Proximal Policy Optimization, Soft Actor-Critic and Generative\nAdversarial Imitation Learning to strategy design problem of three\ncryptocurrency markets. Our input data includes price data and technical\nindicators. We have implemented a Gym environment based on cryptocurrency\nmarkets to be used with the algorithms. Our test results on unseen data shows a\ngreat potential for this approach in helping investors with an expert system to\nexploit the market and gain profit. Our highest gain for an unseen 66 day span\nis 4850 US dollars per 10000 US dollars investment. We also discuss on how a\nspecific hyperparameter in the environment design can be used to adjust risk in\nthe generated strategies.\n"
    },
    {
        "paper_id": 2201.05974,
        "authors": "Kohei Hayashi and Kei Nakagawa",
        "title": "Fractional SDE-Net: Generation of Time Series Data with Long-term Memory",
        "comments": "IEEE DSAA 2022 Accepted",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper, we focus on the generation of time-series data using neural\nnetworks. It is often the case that input time-series data have only one\nrealized (and usually irregularly sampled) path, which makes it difficult to\nextract time-series characteristics, and its noise structure is more\ncomplicated than i.i.d. type. Time series data, especially from hydrology,\ntelecommunications, economics, and finance, exhibit long-term memory also\ncalled long-range dependency (LRD). The main purpose of this paper is to\nartificially generate time series with the help of neural networks, making the\nLRD of paths into account. We propose fSDE-Net: neural fractional Stochastic\nDifferential Equation Network. It generalizes the neural stochastic\ndifferential equation model by using fractional Brownian motion with a Hurst\nindex larger than half, which exhibits the LRD property. We derive the solver\nof fSDE-Net and theoretically analyze the existence and uniqueness of the\nsolution to fSDE-Net. Our experiments with artificial and real time-series data\ndemonstrate that the fSDE-Net model can replicate distributional properties\nwell.\n"
    },
    {
        "paper_id": 2201.06006,
        "authors": "Steffen Ahrens, Ciril Bosch-Rosa, Thomas Meissner",
        "title": "Intertemporal Consumption and Debt Aversion: A Replication and Extension",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
        "abstract": "  We replicate Meissner (2016), where debt aversion was reported for the first\ntime in an intertemporal consumption and saving problem. While Meissner (2016)\nuses a German sample, our participants are US undergraduate students. All of\nthe original study's main findings replicate with similar effect sizes.\nAdditionally, we extend the original analysis by introducing a new individual\nindex of debt aversion, which we use to compare debt aversion across countries.\nInterestingly, we find no significant differences in debt aversion between the\noriginal German and the new US sample. We then test whether debt aversion\ncorrelates with individual characteristics such as gender, cognitive reflection\nability, and risk aversion. Overall, this paper confirms the importance of debt\naversion in intertemporal consumption and saving problems and validates the\napproach of Meissner (2016).\n"
    },
    {
        "paper_id": 2201.06012,
        "authors": "Battulga Gankhuu",
        "title": "Augmented Dynamic Gordon Growth Model",
        "comments": "22 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  In this paper, we introduce a dynamic Gordon growth model, which is augmented\nby a time--varying spot interest rate and the Gordon growth model for\ndividends. Using the risk--neutral valuation method and locally\nrisk--minimizing strategy, we obtain pricing and hedging formulas for the\ndividend--paying European call and put options and equity--linked life\ninsurance products. Also, we provide ML estimator of the model.\n"
    },
    {
        "paper_id": 2201.06072,
        "authors": "Nemo Semret",
        "title": "Dynamics of Bitcoin mining",
        "comments": "10 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  What happens to mining when the Bitcoin price changes, when there are mining\nsupply shocks, the price of energy changes, or hardware technology evolves? We\ngive precise answers based on the technical forces and incentives in the\nsystem. We then build on these dynamics to consider value: what is the cost and\npurpose of mining, and is it worth it? Does it use too much energy, is it bad\nfor the environment? Finally we extend our analysis to the long term: is mining\neconomically feasible forever? What will the global hash rate be in 40 years?\nHow is mining impacted by the limits of computation and energy? Is it\nphysically sustainable in the long run? From first principles, we derive a\nfundamental scale-invariant feasibility constraint, which enables us to analyze\nthe interlocking dynamics, find key invariants, and answer these questions\nmathematically.\n"
    },
    {
        "paper_id": 2201.06183,
        "authors": "Kelli Francis-Staite",
        "title": "Internal multi-portfolio rebalancing processes: Linking resource\n  allocation models and biproportional matrix techniques to portfolio\n  management",
        "comments": "62 pages, 3 figures, comments welcome",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper describes multi-portfolio `internal' rebalancing processes used in\nthe finance industry. Instead of trading with the market to `externally'\nrebalance, these internal processes detail how portfolio managers buy and sell\nbetween their portfolios to rebalance. We give an overview of currently used\ninternal rebalancing processes, including one known as the `banker' process and\nanother known as the `linear' process. We prove the banker process\ndisadvantages the nominated banker portfolio in volatile markets, while the\nlinear process may advantage or disadvantage portfolios.\n  We describe an alternative process that uses the concept of\n`market-invariance'. We give analytic solutions for small cases, while in\ngeneral show that the $n$-portfolio solution and its corresponding\n`market-invariant' algorithm solve a system of nonlinear polynomial equations.\nIt turns out this algorithm is a rediscovery of the RAS algorithm (also called\nthe `iterative proportional fitting procedure') for biproportional matrices. We\nshow that this process is more equitable than the banker and linear processes,\nand demonstrate this with empirical results.\n  The market-invariant process has already been implemented by industry due to\nthe significance of these results.\n"
    },
    {
        "paper_id": 2201.06197,
        "authors": "Hayato Kato, and Toshihiro Okubo",
        "title": "The Resilience of FDI to Natural Disasters through Industrial Linkages",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  When do multinationals show resilience during natural disasters? To answer\nthis, we develop a simple model in which foreign multinationals and local firms\nin the host country are interacted through input-output linkages. When natural\ndisasters seriously hit local firms and thus increase the cost of sourcing\nlocal intermediate inputs, most multinationals may leave the host country.\nHowever, they are likely to stay if they are tightly linked with local\nsuppliers and face low trade costs of importing foreign intermediates. We\nfurther provide a number of extensions of the basic model to incorporate, for\nexample, multinationals with heterogeneous productivity and disaster\nreconstruction.\n"
    },
    {
        "paper_id": 2201.06319,
        "authors": "S\\\"oren Bettels and Sojung Kim and Stefan Weber",
        "title": "Multinomial Backtesting of Distortion Risk Measures",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We extend the scope of risk measures for which backtesting models are\navailable by proposing a multinomial backtesting method for general distortion\nrisk measures. The method relies on a stratification and randomization of risk\nlevels. We illustrate the performance of our methods in numerical case studies.\n"
    },
    {
        "paper_id": 2201.0637,
        "authors": "Tiantian Mao, Ruodu Wang, Qinyu Wu",
        "title": "Model Aggregation for Risk Evaluation and Robust Optimization",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We introduce a new approach for prudent risk evaluation based on stochastic\ndominance, which will be called the model aggregation (MA) approach. In\ncontrast to the classic worst-case risk (WR) approach, the MA approach produces\nnot only a robust value of risk evaluation but also a robust distributional\nmodel, independent of any specific risk measure. The MA risk evaluation can be\ncomputed through explicit formulas in the lattice theory of stochastic\ndominance, and under some standard assumptions, the MA robust optimization\nadmits a convex-program reformulation. The MA approach for Wasserstein and\nmean-variance uncertainty sets admits explicit formulas for the obtained robust\nmodels. Via an equivalence property between the MA and the WR approaches, new\naxiomatic characterizations are obtained for the Value-at-Risk (VaR) and the\nExpected Shortfall (ES, also known as CVaR). The new approach is illustrated\nwith various risk measures and examples from portfolio optimization.\n"
    },
    {
        "paper_id": 2201.06373,
        "authors": "Andreas Bauer, Jasna Omeragic",
        "title": "Volatility in the Relative Standard Deviation of Target Fulfilment as\n  Key Performance Indicator (KPI)",
        "comments": "12 pages, 3 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  In this study, we identify the relative standard deviation volatility (RSD\nvolatility) in the individual target time fulfilment of the complete set of\ncomparables (e.g., all individuals in the same organisational structure) as a\npossible key performance indicator (KPI) for predicting employee job\nperformance. KPIs are a well-established, measurable benchmark of an\norganisation's critical success metrics; thus, in this paper, we attempt to\nidentify employees experiencing a transition in their RSD towards a higher per\ncent deviation, indicating emerging inadequate work conditions. We believe RSD\nvolatility can be utilised as an additional assessment factor, particularly in\nprofiling.\n"
    },
    {
        "paper_id": 2201.06635,
        "authors": "Sebastien Valeyre",
        "title": "Optimal trend following portfolios",
        "comments": null,
        "journal-ref": "Journal of investment stategies 2024 12(3)",
        "doi": "10.21314/JOIS.2023.008",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper derives an optimal portfolio that is based on trend-following\nsignal. Building on an earlier related article, it provides a unifying\ntheoretical setting to introduce an autocorrelation model with the covariance\nmatrix of trends and risk premia. We specify practically relevant models for\nthe covariance matrix of trends. The optimal portfolio is decomposed into four\nbasic components that yield four basic portfolios: Markowitz, risk parity,\nagnostic risk parity, and trend following on risk parity. The overperformance\nof the proposed optimal portfolio, applied to cross-asset trading universe, is\nconfirmed by empirical backtests. We provide thus a unifying framework to\ndescribe and rationalize earlier developed portfolios.\n"
    },
    {
        "paper_id": 2201.0693,
        "authors": "David Skovmand and Jacob Bjerre Skov",
        "title": "Decomposing LIBOR in Transition: Evidence from the Futures Markets",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Applying historical data from the USD LIBOR transition period, we estimate a\njoint model for SOFR, Fed Funds, and Eurodollar futures rates as well as spot\nUSD LIBOR and term repo rates. The framework endogenously models basis spreads\nbetween each of the benchmark rates and allows for the decomposition of\nspreads. Modelling the LIBOR-OIS spread as credit and funding-liquidity\nroll-over risk, we find that the spike in the LIBOR-OIS spread during the onset\nof COVID-19 was mainly due to credit risk, while on average credit and\nfunding-liquidity risk contribute equally to the spread.\n"
    },
    {
        "paper_id": 2201.07159,
        "authors": "Fuhao Lou",
        "title": "Examining the Relations between Household Saving Rate of Rural Areas and\n  Migration",
        "comments": "18 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  China has been developing very fast since the beginning of the 21st century.\nThe net income of households has been increased a lot as well. Nonetheless,\nmigration from rural areas to urban sectors tends to keep a high saving rate\ninstead of consumption. This essay tries to use the conventional Ordinary Least\nSquare regression, along with the method of Instrument Variable to test the\nproblem of endogeneity, to discover the relationship between the saving rates\nof rural households and labor migration, controlling for other characteristic\nvariables including having insurance, marital status, education, having\nchildren, health conditions. The assumption is that migration contributes\npositively to the dependent variable, meaning that migration could increase the\nhousehold save rates. However, the conclusion is that it is negatively with the\nhousehold save rates. All the other variables regarding education, health\nconditions, marital status, insurance, and number of children are negatively\nrelated with the household saving rates.\n"
    },
    {
        "paper_id": 2201.0717,
        "authors": "Julian D. Cortes",
        "title": "What is the mission of innovation?",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  Governments and organizations recognize the need to revisit a mission-driven\ninnovation amidst national and organizational innovation policy formulations.\nNotwithstanding a fertile research agenda on mission statements (hereafter\nmission(s)), several lines of inquiry remain open, such as crossnational and\nmultisectorial studies and an examination of research knowledge intensive\ninstitutions. In this article, we identify similarities and differences in the\ncontent of missions from government, private, higher education, and health\nresearch knowledge intensive institutions in a sample of over 1,900\ninstitutions from 89 countries through the deployment of sentiment analysis,\nreadability, and lexical diversity; semantic networks; and a similarity\ncomputation between document corpus. We found that missions of research\nknowledge intensive institutions are challenging to read texts with lower\nlexical diversity that favors positive rather than negative words. In stark\ncontrast to this, the non-profit sector is consonant in multiple dimensions in\nits use of Corporate Social Responsibility jargon. The lexical appearance of\nresearch in the missions varies according to mission sectorial context, and\neach sector has a cluster specific focus. Utilizing the mission as a strategic\nplanning tool in higher income regions might serve to explain corpora\nsimilarities shared by sectors and continents.\n"
    },
    {
        "paper_id": 2201.07181,
        "authors": "Charles Goodhart, Donato Masciandaro, Stefano Ugolini (LEREPS)",
        "title": "Pandemic Recession and Helicopter Money: Venice, 1629--1631",
        "comments": null,
        "journal-ref": "Financial History Review, Cambridge University Press (CUP), In\n  press, pp.1-19",
        "doi": "10.1017/S0968565021000214",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We analyse the money-financed fiscal stimulus implemented in Venice during\nthe famine and plague of 1629--31, which was equivalent to a 'net-worth\nhelicopter money' strategy -- a monetary expansion generating losses to the\nissuer. We argue that the strategy aimed at reconciling the need to subsidize\ninhabitants suffering from containment policies with the desire to prevent an\nincrease in long-term government debt, but it generated much monetary\ninstability and had to be quickly reversed. This episode highlights the\nredistributive implications of the design of macroeconomic policies and the\nrole of political economy factors in determining such designs.\n"
    },
    {
        "paper_id": 2201.07214,
        "authors": "Mateus F. B. Granha and Andr\\'e L. M. Vilela and Chao Wang and Kenric\n  P. Nelson and H. Eugene Stanley",
        "title": "Opinion Dynamics in Financial Markets via Random Networks",
        "comments": "23 pages, 12 figures",
        "journal-ref": null,
        "doi": "10.1073/pnas.2201573119",
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  We investigate the financial market dynamics by introducing a heterogeneous\nagent-based opinion formation model. In this work, we organize the individuals\nin a financial market by their trading strategy, namely noise traders and\nfundamentalists. The opinion of a local majority compels the market exchanging\nbehavior of noise traders, whereas the global behavior of the market influences\nthe fundamentalist agents' decisions. We introduce a noise parameter $q$ to\nrepresent a level of anxiety and perceived uncertainty regarding the market\nbehavior, enabling the possibility for an adrift financial action. We place the\nindividuals as nodes in an Erd\\\"os-R\\'enyi random graph, where the links\nrepresent their social interaction. At a given time, they assume one of two\npossible opinion states $\\pm 1$ regarding buying or selling an asset. The model\nexhibits such fundamental qualitative and quantitative real-world market\nfeatures as the distribution of logarithmic returns with fat-tails, clustered\nvolatility, and long-term correlation of returns. We use Student's t\ndistributions to fit the histograms of logarithmic returns, showing the gradual\nshift from a leptokurtic to a mesokurtic regime, depending on the fraction of\nfundamentalist agents. We also compare our results with the distribution of\nlogarithmic returns of several real-world financial indices.\n"
    },
    {
        "paper_id": 2201.0722,
        "authors": "Bruno Mazorra, Victor Adan, Vanesa Daza",
        "title": "Do not rug on me: Zero-dimensional Scam Detection",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Uniswap, like other DEXs, has gained much attention this year because it is a\nnon-custodial and publicly verifiable exchange that allows users to trade\ndigital assets without trusted third parties. However, its simplicity and lack\nof regulation also makes it easy to execute initial coin offering scams by\nlisting non-valuable tokens. This method of performing scams is known as rug\npull, a phenomenon that already existed in traditional finance but has become\nmore relevant in DeFi. Various projects such as [34,37] have contributed to\ndetecting rug pulls in EVM compatible chains. However, the first longitudinal\nand academic step to detecting and characterizing scam tokens on Uniswap was\nmade in [44]. The authors collected all the transactions related to the Uniswap\nV2 exchange and proposed a machine learning algorithm to label tokens as scams.\nHowever, the algorithm is only valuable for detecting scams accurately after\nthey have been executed. This paper increases their data set by 20K tokens and\nproposes a new methodology to label tokens as scams. After manually analyzing\nthe data, we devised a theoretical classification of different malicious\nmaneuvers in Uniswap protocol. We propose various machine-learning-based\nalgorithms with new relevant features related to the token propagation and\nsmart contract heuristics to detect potential rug pulls before they occur. In\ngeneral, the models proposed achieved similar results. The best model obtained\nan accuracy of 0.9936, recall of 0.9540, and precision of 0.9838 in\ndistinguishing non-malicious tokens from scams prior to the malicious maneuver.\n"
    },
    {
        "paper_id": 2201.07457,
        "authors": "Hwai-Chung Ho",
        "title": "Forecasting the distribution of long-horizon returns with time-varying\n  volatility",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The study of long-horizon returns has received a great deal of attention in\nrecent years (see, for example, Boudoukh, Richardson, and Whitelaw (2008),\nNeuberger (2012) and Lee (2013), Fama and French (2018)). While most of the\ndiscussions are concerned with some practical issues in investment, few have\ntouched the important aspect on risk management. The approach adopted in this\narticle is to predict the future distribution of the returns of a fixed\nlong-horizon by which the risk measures of interest that come in the form of a\ndistributional functional such as the value at risk (VaR) and the conditional\ntail expectation (CTE) can be easily derived. The characteristic feature of our\napproach which requires no specification of the volatility dynamics nor\nparametric assumptions of the shock distribution extends the work by Ho et al.\n(2016) and Ho ( 2017) to a more general volatility dynamics that includes both\nthe widely-used SV model and the GARCH model (Bollerslev, 1986) as special\ncases.\n"
    },
    {
        "paper_id": 2201.07656,
        "authors": "Sergey Nadtochiy and Yuan Yin",
        "title": "Consistency of MLE for partially observed diffusions, with application\n  in market microstructure modeling",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  This paper presents a tractable sufficient condition for the consistency of\nmaximum likelihood estimators (MLEs) in partially observed diffusion models,\nstated in terms of stationary distribution of the associated fully observed\ndiffusion, under the assumption that the set of unknown parameter values is\nfinite. This sufficient condition is then verified in the context of a latent\nprice model of market microstructure, yielding consistency of maximum\nlikelihood estimators of the unknown parameters in this model. Finally, we\ncompute the latter estimators using historical financial data taken from the\nNASDAQ exchange.\n"
    },
    {
        "paper_id": 2201.07659,
        "authors": "Erhan Bayraktar, Zhenhua Wang, Zhou Zhou",
        "title": "Equilibria of Time-inconsistent Stopping for One-dimensional Diffusion\n  Processes",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We consider three equilibrium concepts proposed in the literature for\ntime-inconsistent stopping problems, including mild equilibria, weak equilibria\nand strong equilibria. The discount function is assumed to be log sub-additive\nand the underlying process is one-dimensional diffusion. We first provide\nnecessary and sufficient conditions for the characterization of weak\nequilibria. The smooth-fit condition is obtained as a by-product. Next, based\non the characterization of weak equilibria, we show that an optimal mild\nequilibrium is also weak. Then we provide conditions under which a weak\nequilibrium is strong. We further show that an optimal mild equilibrium is also\nstrong under a certain condition. Finally, we provide several examples\nincluding one shows a weak equilibrium may not be strong, and another one shows\na strong equilibrium may not be optimal mild.\n"
    },
    {
        "paper_id": 2201.07737,
        "authors": "C\\'elestin Coquid\\'e, Jos\\'e Lages, Leonardo Ermann, Dima L.\n  Shepelyansky",
        "title": "COVID-19 impact on the international trade",
        "comments": "22 pages, 2 tables, 13 figures, 2 appendices",
        "journal-ref": "Entropy 2022, 24(3), 327",
        "doi": "10.3390/e24030327",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Using the United Nations Comtrade database, we perform the Google matrix\nanalysis of the multiproduct World Trade Network (WTN) for the years 2018-2020\ncomprising the emergence of the COVID-19 as a global pandemic. The applied\nalgorithms -- the PageRank, the CheiRank and the reduced Google matrix -- take\ninto account the multiplicity of the WTN links providing new insights on the\ninternational trade comparing to the usual import-export analysis. These\nalgorithms establish new rankings and trade balances of countries and products\nconsidering every countries on equal grounds, independently of their wealth,\nand every products on the basis of their relative exchanged volumes. In\ncomparison with the pre-COVID-19 period, significant changes in these metrics\noccur for the year 2020 highlighting a major rewiring of the international\ntrade flows induced by the COVID-19 pandemic crisis. We define a new\nPageRank-CheiRank product trade balance, either export or import oriented,\nwhich is significantly perturbed by the pandemic.\n"
    },
    {
        "paper_id": 2201.0788,
        "authors": "Zhe Wang, Nicolas Privault, Claude Guet",
        "title": "Deep self-consistent learning of local volatility",
        "comments": "21 pages, 5 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
        "abstract": "  We present an algorithm for the calibration of local volatility from market\noption prices through deep self-consistent learning, by approximating both\nmarket option prices and local volatility using deep neural networks,\nrespectively. Our method uses the initial-boundary value problem of the\nunderlying Dupire's partial differential equation solved by the parameterized\noption prices to bring corrections to the parameterization in a self-consistent\nway. By exploiting the differentiability of the neural networks, we can\nevaluate Dupire's equation locally at each strike-maturity pair; while by\nexploiting their continuity, we sample strike-maturity pairs uniformly from a\ngiven domain, going beyond the discrete points where the options are quoted.\nMoreover, the absence of arbitrage opportunities are imposed by penalizing an\nassociated loss function as a soft constraint. For comparison with existing\napproaches, the proposed method is tested on both synthetic and market option\nprices, which shows an improved performance in terms of reduced interpolation\nand reprice errors, as well as the smoothness of the calibrated local\nvolatility. An ablation study has been performed, asserting the robustness and\nsignificance of the proposed method.\n"
    },
    {
        "paper_id": 2201.07975,
        "authors": "Burin Gumjudpai (NAS Mahidol University)",
        "title": "Effect Structure and Thermodynamics Formulation of Demand-side Economics",
        "comments": "10 pages, 3 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We propose concept of equation of state (EoS) effect structure in form of\ndiagrams and rules. This concept helps justifying EoS status of an empirical\nrelation. We apply the concept to closed system of consumers and we are able to\nformulate its EoS. According to the new concept, EoS are classified into three\nclasses. Manifold space of thermodynamics formulation of demand-side economics\nis identified. Formal analogies of thermodynamics and economics consumers'\nsystem are made. New quantities such as total wealth, generalized utility and\ngeneralized consumer surplus are defined. Microeconomics' concept of consumer\nsurplus is criticized and replaced with generalized consumer surplus. Smith's\nlaw of demand is included in our new paradigm as a specific case resembling\nisothermal process. Absolute zero temperature state resembles the nirvana state\nin Buddhism philosophy. Econometric modelling of consumers' EoS is proposed at\nlast.\n"
    },
    {
        "paper_id": 2201.08218,
        "authors": "Carmina Fjellstr\\\"om",
        "title": "Long Short-Term Memory Neural Network for Financial Time Series",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Performance forecasting is an age-old problem in economics and finance.\nRecently, developments in machine learning and neural networks have given rise\nto non-linear time series models that provide modern and promising alternatives\nto traditional methods of analysis. In this paper, we present an ensemble of\nindependent and parallel long short-term memory (LSTM) neural networks for the\nprediction of stock price movement. LSTMs have been shown to be especially\nsuited for time series data due to their ability to incorporate past\ninformation, while neural network ensembles have been found to reduce\nvariability in results and improve generalization. A binary classification\nproblem based on the median of returns is used, and the ensemble's forecast\ndepends on a threshold value, which is the minimum number of LSTMs required to\nagree upon the result. The model is applied to the constituents of the smaller,\nless efficient Stockholm OMX30 instead of other major market indices such as\nthe DJIA and S&P500 commonly found in literature. With a straightforward\ntrading strategy, comparisons with a randomly chosen portfolio and a portfolio\ncontaining all the stocks in the index show that the portfolio resulting from\nthe LSTM ensemble provides better average daily returns and higher cumulative\nreturns over time. Moreover, the LSTM portfolio also exhibits less volatility,\nleading to higher risk-return ratios.\n"
    },
    {
        "paper_id": 2201.08276,
        "authors": "Khalid El-Awady",
        "title": "Applicability of Large Corporate Credit Models to Small Business Risk\n  Assessment",
        "comments": "5 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
        "abstract": "  There is a massive underserved market for small business lending in the US\nwith the Federal Reserve estimating over \\$650B in unmet annual financing\nneeds. Assessing the credit risk of a small business is key to making good\ndecisions whether to lend and at what terms. Large corporations have a\nwell-established credit assessment ecosystem, but small businesses suffer from\nlimited publicly available data and few (if any) credit analysts who cover them\nclosely. We explore the applicability of (DL-based) large corporate credit risk\nmodels to small business credit rating.\n"
    },
    {
        "paper_id": 2201.08283,
        "authors": "Stefanos Bennett, Mihai Cucuringu, Gesine Reinert",
        "title": "Lead-lag detection and network clustering for multivariate time series\n  with an application to the US equity market",
        "comments": "29 pages, 28 figures; preliminary version appeared at KDD 2021 - 7th\n  SIGKKDD Workshop on Mining and Learning from Time Series (MiLeTS)",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In multivariate time series systems, it has been observed that certain groups\nof variables partially lead the evolution of the system, while other variables\nfollow this evolution with a time delay; the result is a lead-lag structure\namongst the time series variables. In this paper, we propose a method for the\ndetection of lead-lag clusters of time series in multivariate systems. We\ndemonstrate that the web of pairwise lead-lag relationships between time series\ncan be helpfully construed as a directed network, for which there exist\nsuitable algorithms for the detection of pairs of lead-lag clusters with high\npairwise imbalance. Within our framework, we consider a number of choices for\nthe pairwise lead-lag metric and directed network clustering components. Our\nframework is validated on both a synthetic generative model for multivariate\nlead-lag time series systems and daily real-world US equity prices data. We\nshowcase that our method is able to detect statistically significant lead-lag\nclusters in the US equity market. We study the nature of these clusters in the\ncontext of the empirical finance literature on lead-lag relations and\ndemonstrate how these can be used for the construction of predictive financial\nsignals.\n"
    },
    {
        "paper_id": 2201.08444,
        "authors": "Javier Garcia-Bernardo, Petr Jansk\\'y",
        "title": "Profit Shifting of Multinational Corporations Worldwide",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We exploit the new country-by-country reporting data of multinational\ncorporations, with unparalleled country coverage, to reveal the distributional\nconsequences of profit shifting. We estimate that multinational corporations\nworldwide shifted over \\$850 billion in profits in 2017, primarily to countries\nwith effective tax rates below 10\\%. Countries with lower incomes lose a larger\nshare of their total tax revenue due to profit shifting. We further show that a\nlogarithmic function is better suited for capturing the non-linear relationship\nbetween profits and tax rates than linear or quadratic functions. Our findings\nhighlight effective tax rates' importance for profit shifting and tax reforms.\n"
    },
    {
        "paper_id": 2201.08875,
        "authors": "George Bouzianis, Lane P. Hughston, Leandro S\\'anchez-Betancourt",
        "title": "Information-Based Trading",
        "comments": "29 pages, 8 figures, revised, accepted for publication in\n  International Journal of Theoretical and Applied Finance",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  We consider a pair of traders in a market where the information available to\nthe second trader is a strict subset of the information available to the first\ntrader. The traders make prices based on the information available concerning a\nsecurity that pays a random cash flow at a fixed time $T$ in the future. Market\ninformation is modelled in line with the scheme of Brody, Hughston & Macrina\n(2007, 2008) and Brody, Davis, Friedman & Hughston (2009). The risk-neutral\ndistribution of the cash flow is known to the traders, who make prices with a\nfixed multiplicative bid-offer spread and report their prices to a game master\nwho declares that a trade has been made when the bid price of one of the\ntraders crosses the offer price of the other. We prove that the value of the\nfirst trader's position is strictly greater than that of the second. The\nresults are analyzed by use of simulation studies and generalized to situations\nwhere (a) there is a hierarchy of traders, (b) there are multiple successive\ntrades, and (c) there is inventory aversion.\n"
    },
    {
        "paper_id": 2201.08995,
        "authors": "Prateek Bansal and Rubal Dua",
        "title": "Fuel consumption elasticities, rebound effect and feebate effectiveness\n  in the Indian and Chinese new car markets",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  China and India, the world's two most populous developing economies, are also\namong the world's largest automotive markets and carbon emitters. To reduce\ncarbon emissions from the passenger car sector, both countries have considered\nvarious policy levers affecting fuel prices, car prices and fuel economy. This\nstudy estimates the responsiveness of new car buyers in China and India to such\npolicy levers and drivers including income. Furthermore, we estimate the\npotential for rebound effect and the effectiveness of a feebate policy. To\naccomplish this, we developed a joint discrete-continuous model of car choice\nand usage based on revealed preference survey data from approximately 8000 new\ncar buyers from India and China who purchased cars in 2016-17. Conditional on\nbuying a new car, the fuel consumption in both markets is found to be\nrelatively unresponsive to fuel price and income, with magnitudes of elasticity\nestimates ranging from 0.12 to 0.15. For both markets, the mean segment-level\ndirect elasticities of fuel consumption relative to car price and fuel economy\nrange from 0.57 to 0.65. The rebound effect on fuel savings due to cost-free\nfuel economy improvement is found to be 17.1% for India and 18.8% for China. A\nrevenue-neutral feebate policy, with average rebates and fees of up to around\n15% of the retail price, resulted in fuel savings of around 0.7% for both\nmarkets. While the feebate policy's rebound effect is low - 7.3% for India and\n1.6% for China - it does not appear to be an effective fuel conservation\npolicy.\n"
    },
    {
        "paper_id": 2201.09058,
        "authors": "Shuo Sun, Wanqi Xue, Rundong Wang, Xu He, Junlei Zhu, Jian Li, Bo An",
        "title": "DeepScalper: A Risk-Aware Reinforcement Learning Framework to Capture\n  Fleeting Intraday Trading Opportunities",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Reinforcement learning (RL) techniques have shown great success in many\nchallenging quantitative trading tasks, such as portfolio management and\nalgorithmic trading. Especially, intraday trading is one of the most profitable\nand risky tasks because of the intraday behaviors of the financial market that\nreflect billions of rapidly fluctuating capitals. However, a vast majority of\nexisting RL methods focus on the relatively low frequency trading scenarios\n(e.g., day-level) and fail to capture the fleeting intraday investment\nopportunities due to two major challenges: 1) how to effectively train\nprofitable RL agents for intraday investment decision-making, which involves\nhigh-dimensional fine-grained action space; 2) how to learn meaningful\nmulti-modality market representation to understand the intraday behaviors of\nthe financial market at tick-level. Motivated by the efficient workflow of\nprofessional human intraday traders, we propose DeepScalper, a deep\nreinforcement learning framework for intraday trading to tackle the above\nchallenges. Specifically, DeepScalper includes four components: 1) a dueling\nQ-network with action branching to deal with the large action space of intraday\ntrading for efficient RL optimization; 2) a novel reward function with a\nhindsight bonus to encourage RL agents making trading decisions with a\nlong-term horizon of the entire trading day; 3) an encoder-decoder architecture\nto learn multi-modality temporal market embedding, which incorporates both\nmacro-level and micro-level market information; 4) a risk-aware auxiliary task\nto maintain a striking balance between maximizing profit and minimizing risk.\nThrough extensive experiments on real-world market data spanning over three\nyears on six financial futures, we demonstrate that DeepScalper significantly\noutperforms many state-of-the-art baselines in terms of four financial\ncriteria.\n"
    },
    {
        "paper_id": 2201.09064,
        "authors": "Abeer Al Yaqoobi and Marcel Ausloos",
        "title": "An Intergenerational Issue: The Equity Issues due to Public-Private\n  Partnerships. The Critical Aspect of the Social Discount Rate Choice for\n  Future Generations",
        "comments": "27 pages, 1 figure, 98 references",
        "journal-ref": "Journal of Risk and Financial Management 15, 49 (2022)",
        "doi": "10.3390/jrfm15020049",
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  This paper investigates the impact of Social Discount Rate (SDR) choice on\nintergenerational equity issues caused by Public-Private Partnerships (PPPs)\nprojects. Indeed, more PPPs mean more debt being accumulated for future\ngenerations leading to a fiscal deficit crisis. The paper draws on how the SDR\nlevel taken today distributes societies on the Social Welfare Function (SWF).\nThis is done by answering two sub-questions: (i) What is the risk of PPPs debts\nbeing off-balance sheet? (ii) How do public policies, based on the envisaged\nSDR, position society within different ethical perspectives? The answers are\nobtained from a discussion of the different SDRs (applied in the UK for\nexamples) according to the merits of the pertinent ethical theories, namely\nlibertarian, egalitarian, utilitarian and Rawlsian. We find that public\npolicymakers can manipulate the SDR to make PPPs looking like a better option\nthan the traditional financing form. However, this antagonises the Value for\nMoney principle. We also point out that public policy is not harmonised with\nethical theories. We find that at present (in the UK), the SDR is somewhere\nbetween weighted utilitarian and Rawlsian societies in the trade-off curve.\nAlas, our study finds no evidence that the (UK) government is using a\nsophisticated system to keep pace with the accumulated off-balance sheet debts.\nThus, the exact prediction of the final state is hardly made because of the\nuncertainty factor. We conclude that our study hopefully provides a good\nanalytical framework for policymakers in order to draw on the merits of ethical\ntheories before initiating public policies like PPPs.\n"
    },
    {
        "paper_id": 2201.09073,
        "authors": "Marcel Ausloos and Philippe Bronlet",
        "title": "Economic Freedom: The Top, the Bottom, and the Reality. I. 1997-2007",
        "comments": "47 pages, 44 references, 10 figures",
        "journal-ref": "Entropy 24, 38 (2022)",
        "doi": "10.3390/e24010038",
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  We recall the historically admitted prerequisites of Economic Freedom (EF).\nWe have examined 908 data points for the Economic Freedom of the World (EFW)\nindex and 1884 points for the Index of Economic Freedom (IEF); the studied\nperiods are 2000-2006 and 1997-2007, respectively, thereby following the Berlin\nwall collapse, and including Sept. 11, 2001. After discussing EFW index and\nIEF, in order to compare the indices, one needs to study their overlap in time\nand space. That leaves 138 countries to be examined over a period extending\nfrom 2000 to 2006, thus 2 sets of 862 data points. The data analysis pertains\nto the rank-size law technique. It is examined whether the distributions obey\nan exponential or a power law. A correlation with the country Gross Domestic\nProduct (GDP), an admittedly major determinant of EF, follows, distinguishing\nregional aspects, i.e. defining 6 continents. Semi-log plots show that the\nEFW-rank relationship is exponential for countries of high rank ($\\ge 20$);\noverall the log-log plots point to a behaviour close to a power law. In\ncontrast, for the IEF, the overall ranking has an exponential behaviour; but\nthe log-log plots point to the existence of a transitional point between two\ndifferent power laws, i.e., near rank 10. Moreover, log-log plots of the EFW\nindex relationship to country GDP is characterised by a power law, with a\nrather stable exponent ($\\gamma \\simeq 0.674$) as a function of time. In\ncontrast, log-log plots of the IEF relationship with the country's gross\ndomestic product point to a downward evolutive power law as a function of time.\nMarkedly the two studied indices provide different aspects of EF.\n"
    },
    {
        "paper_id": 2201.09105,
        "authors": "Chaofan Sun, Ken Seng Tan and Wei Wei",
        "title": "Credit Valuation Adjustment with Replacement Closeout: Theory and\n  Algorithms",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  The replacement closeout convention has drawn more and more attention since\nthe 2008 financial crisis. Compared with the conventional risk-free closeout,\nthe replacement closeout convention incorporates the creditworthiness of the\ncounterparty and thus providing a more accurate estimate of the Mark-to-market\nvalue of a financial claim. In contrast to the risk-free closeout, the\nreplacement closeout renders a nonlinear valuation system, which constitutes\nthe major difficulty in the valuation of the counterparty credit risk.\n  In this paper, we show how to address the nonlinearity attributed to the\nreplacement closeout in the theoretical and computational analysis. In the\ntheoretical part, we prove the unique solvability of the nonlinear valuation\nsystem and study the impact of the replacement closeout on the credit valuation\nadjustment. In the computational part, we propose a neural network-based\nalgorithm for solving the (high dimensional) nonlinear valuation system and\neffectively alleviating the curse of dimensionality. We numerically compare the\ncomputational cost for the valuations with risk-free and replacement closeouts.\nThe numerical tests confirm both the accuracy and the computational efficiency\nof our proposed algorithm for the valuation of the replacement closeout.\n"
    },
    {
        "paper_id": 2201.09108,
        "authors": "Brendan K. Beare",
        "title": "Optimal measure preserving derivatives revisited",
        "comments": "24 pages, 2 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This article clarifies the relationship between pricing kernel monotonicity\nand the existence of opportunities for stochastic arbitrage in a complete and\nfrictionless market of derivative securities written on a market portfolio. The\nrelationship depends on whether the payoff distribution of the market portfolio\nsatisfies a technical condition called adequacy, meaning that it is atomless or\nis comprised of finitely many equally probable atoms. Under adequacy, pricing\nkernel nonmonotonicity is equivalent to the existence of a strong form of\nstochastic arbitrage involving distributional replication of the market\nportfolio at a lower price. If the adequacy condition is dropped then this\nequivalence no longer holds, but pricing kernel nonmonotonicity remains\nequivalent to the existence of a weaker form of stochastic arbitrage involving\nsecond-order stochastic dominance of the market portfolio at a lower price. A\ngeneralization of the optimal measure preserving derivative is obtained which\nachieves distributional replication at the minimum cost of all second-order\nstochastically dominant securities under adequacy.\n"
    },
    {
        "paper_id": 2201.09125,
        "authors": "Jussi Heikkil\\\"a, Timo Ali-Vehmas, Julius Rissanen",
        "title": "The Link Between Standardization and Economic Growth: A Bibliometric\n  Analysis",
        "comments": null,
        "journal-ref": "International Journal of Standardization Research, 19(1), Article\n  1 (2021)",
        "doi": "10.4018/ijsr.287101",
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  We analyze the link between standardization and economic growth by\nsystematically reviewing leading economics journals, leading economic growth\nresearchers' articles, and economic growth-related books. We make the following\nobservations: 1) No article has analyzed the link between standardization and\neconomic growth in top5 economics journals between 1996 and 2018. 2) A\nrepresentative sample of the leading researchers of economic growth has\nallocated little attention to the link between standardization and economic\ngrowth. 3) Typically, economic growth textbooks do not contain \"standards\" or\n\"standardization\" in their word indexes. These findings suggest that the\neconomic growth theory has neglected the role of standardization.\n"
    },
    {
        "paper_id": 2201.0916,
        "authors": "Carter Davis, Alexandre Sollaci and James Traina",
        "title": "Profit Puzzles or: Public Firm Profits Have Fallen",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  We show that public firm profit rates fell by half since 1980. Inferred as\nthe residual from the rise of US corporate profit rates in aggregate data,\nprivate firm profit rates doubled since 1980. Public firm financial returns\nmatched their fall in profit rates, while public firm representativeness\nincreased from 30% to 60% of the US capital stock. These results imply that\ntime-varying selection biases in extrapolating public firms to the aggregate\neconomy can be severe.\n"
    },
    {
        "paper_id": 2201.09221,
        "authors": "Pengpeng Yue, Aslihan Gizem Korkmaz, Zhichao Yin, Haigang Zhou",
        "title": "The rise of digital finance: Financial inclusion or debt trap",
        "comments": null,
        "journal-ref": "Finance Research Letters, 2021, 102604",
        "doi": "10.1016/j.frl.2021.102604",
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  This study focuses on the impact of digital finance on households. While\ndigital finance has brought financial inclusion, it has also increased the risk\nof households falling into a debt trap. We provide evidence that supports this\nnotion and explain the channel through which digital finance increases the\nlikelihood of financial distress. Our results show that the widespread use of\ndigital finance increases credit market participation. The broadened access to\ncredit markets increases household consumption by changing the marginal\npropensity to consume. However, the easier access to credit markets also\nincreases the risk of households falling into a debt trap.\n"
    },
    {
        "paper_id": 2201.0927,
        "authors": "Jussi Heikkil\\\"a, Ina Laukkanen",
        "title": "Gender-specific Call of Duty: A Note on the Neglect of Conscription in\n  Gender Equality Indices",
        "comments": null,
        "journal-ref": "Jussi Heikkila & Ina Laukkanen (2020) Gender-specific Call of\n  Duty: A Note on the Neglect of Conscription in Gender Equality Indices,\n  Defence and Peace Economics, forthcoming",
        "doi": "10.1080/10242694.2020.1844400",
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  We document that existing gender equality indices do not account for\ngender-specific mandatory peace-time conscription (compulsory military\nservice). This suggests that gender-specific conscription is not considered to\nbe an important gender issue. If an indicator measuring the gender equality of\nmandatory conscription was to be included in gender equality indices with\nappropriate weight, then the relative rankings of countries in terms of\nmeasured gender equality could be affected. In the context of the Nordic\ncountries, this would mean that Finland and Denmark - the countries with\nmandatory conscription for men only - would have worse scores with respect to\ngender equality compared to Sweden and Norway, countries with conscription for\nboth men and women - and Iceland, which has no mandatory conscription,\nregardless of gender.\n"
    },
    {
        "paper_id": 2201.09319,
        "authors": "Nikolas Michael, Mihai Cucuringu, Sam Howison",
        "title": "Option Volume Imbalance as a predictor for equity market returns",
        "comments": "43 pages, 33 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We investigate the use of the normalized imbalance between option volumes\ncorresponding to positive and negative market views, as a predictor for\ndirectional price movements in the spot market. Via a nonlinear analysis, and\nusing a decomposition of aggregated volumes into five distinct market\nparticipant classes, we find strong signs of predictability of excess market\novernight returns. The strongest signals come from Market-Maker volumes. Among\nother findings, we demonstrate that most of the predictability stems from\nhigh-implied-volatility option contracts, and that the informational content of\nput option volumes is greater than that of call options.\n"
    },
    {
        "paper_id": 2201.09406,
        "authors": "Lijun Bo, Agostino Capponi, Chao Zhou",
        "title": "Power Forward Performance in Semimartingale Markets with Stochastic\n  Integrated Factors",
        "comments": "This work was intended as a replacement of arXiv:1811.11899 and any\n  subsequent updates will appear there",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We study the forward investment performance process (FIPP) in an incomplete\nsemimartingale market model with closed and convex portfolio constraints, when\nthe investor's risk preferences are of the power form. We provide necessary and\nsufficient conditions for the existence of such FIPP. In a semimartingale\nfactor model, we show that the FIPP can be recovered as a triplet of processes\nwhich admit an integral representation with respect to semimartingales. Using\nan integrated stochastic factor model, we relate the factor representation of\nthe triplet of processes to the smooth solution of an ill-posed partial\nintegro-differential Hamilton-Jacobi-Bellman (HJB) equation. We develop\nexplicit constructions for the class of time-monotone FIPPs, generalizing\nexisting results from Brownian to semimartingale market models.\n"
    },
    {
        "paper_id": 2201.09434,
        "authors": "Shi Bo, Minheng Xiao",
        "title": "Data-Driven Risk Measurement by SV-GARCH-EVT Model",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  This paper aims to more effectively manage and mitigate stock market risks by\naccurately characterizing financial market returns and volatility. We enhance\nthe Stochastic Volatility (SV) model by incorporating fat-tailed distributions\nand leverage effects, estimating model parameters using Markov Chain Monte\nCarlo (MCMC) methods. By integrating extreme value theory (EVT) to fit the tail\ndistribution of standard residuals, we develop the SV-EVT-VaR-based dynamic\nmodel. Our empirical analysis, using daily S\\&P 500 index data and simulated\nreturns, shows that SV-EVT-based models outperform others in backtesting. These\nmodels effectively capture the fat-tailed properties of financial returns and\nthe leverage effect, proving superior for out-of-sample data analysis.\n"
    },
    {
        "paper_id": 2201.09516,
        "authors": "Peng Wu, Jean-Fran\\c{c}ois Muzy and Emmanuel Bacry",
        "title": "From Rough to Multifractal volatility: the log S-fBM model",
        "comments": "30 pages, 13 figures",
        "journal-ref": null,
        "doi": "10.1016/j.physa.2022.127919",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We introduce a family of random measures $M_{H,T} (d t)$, namely log S-fBM,\nsuch that, for $H>0$, $M_{H,T}(d t) = e^{\\omega_{H,T}(t)} d t$ where\n$\\omega_{H,T}(t)$ is a Gaussian process that can be considered as a stationary\nversion of an $H$-fractional Brownian motion. Moreover, when $H \\to 0$, one has\n$M_{H,T}(d t) \\rightarrow {\\widetilde M}_{T}(d t)$ (in the weak sense) where\n${\\widetilde M}_{T}(d t)$ is the celebrated log-normal multifractal random\nmeasure (MRM). Thus, this model allows us to consider, within the same\nframework, the two popular classes of multifractal ($H = 0$) and rough\nvolatility ($0<H < 1/2$) models. The main properties of the log S-fBM are\ndiscussed and their estimation issues are addressed. We notably show that the\ndirect estimation of $H$ from the scaling properties of $\\ln(M_{H,T}([t,\nt+\\tau]))$, at fixed $\\tau$, can lead to strongly over-estimating the value of\n$H$. We propose a better GMM estimation method which is shown to be valid in\nthe high-frequency asymptotic regime. When applied to a large set of empirical\nvolatility data, we observe that stock indices have values around $H=0.1$ while\nindividual stocks are characterized by values of $H$ that can be very close to\n$0$ and thus well described by a MRM. We also bring evidence that unlike the\nlog-volatility variance $\\nu^2$ whose estimation appears to be poorly reliable\n(though used widely in the rough volatility literature), the estimation of the\nso-called \"intermittency coefficient\" $\\lambda^2$, which is the product of\n$\\nu^2$ and the Hurst exponent $H$, appears to be far more reliable leading to\nvalues that seem to be universal for respectively all individual stocks and all\nstock indices.\n"
    },
    {
        "paper_id": 2201.0979,
        "authors": "Marcell T. Kurbucz, P\\'eter P\\'osfay, Antal Jakov\\'ac",
        "title": "Linear Laws of Markov Chains with an Application for Anomaly Detection\n  in Bitcoin Prices",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  The goals of this paper are twofold: (1) to present a new method that is able\nto find linear laws governing the time evolution of Markov chains and (2) to\napply this method for anomaly detection in Bitcoin prices. To accomplish these\ngoals, first, the linear laws of Markov chains are derived by using the time\nembedding of their (categorical) autocorrelation function. Then, a binary\nseries is generated from the first difference of Bitcoin exchange rate (against\nthe United States Dollar). Finally, the minimum number of parameters describing\nthe linear laws of this series is identified through stepped time windows.\nBased on the results, linear laws typically became more complex (containing an\nadditional third parameter that indicates hidden Markov property) in two\nperiods: before the crash of cryptocurrency markets inducted by the COVID-19\npandemic (12 March 2020), and before the record-breaking surge in the price of\nBitcoin (Q4 2020 - Q1 2021). In addition, the locally high values of this third\nparameter are often related to short-term price peaks, which suggests price\nmanipulation.\n"
    },
    {
        "paper_id": 2201.09806,
        "authors": "Gennady Shkliarevsky",
        "title": "Infinite Growth: A Curse or a Blessing?",
        "comments": "43 pages",
        "journal-ref": null,
        "doi": "10.13140/RG.2.2.33330.89287",
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  The article discusses the controversy over infinite growth. It analyzes the\ntwo dominant perspectives on economic growth and finds them based on a limited\nand subjective view of reality. An examination of the principal aspects of\neconomic growth (economic activity, value, and value creation) helps to\nunderstand what fuels economic growth. The article also discusses the\ncorrelations between production, consumption, as well as resources and\npopulation dynamics. The article finds that infinite and exponential economic\ngrowth is essential for the survival of our civilization.\n"
    },
    {
        "paper_id": 2201.09876,
        "authors": "Thiemo Fetzer, Christopher Rauh",
        "title": "Pandemic Pressures and Public Health Care: Evidence from England",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  This paper documents that the COVID-19 pandemic induced pressures on the\nhealth care system have significant adverse knock-on effects on the\naccessibility and quality of non-COVID-19 care. We observe persistently\nworsened performance and longer waiting times in A&E; drastically limited\naccess to specialist care; notably delayed or inaccessible diagnostic services;\nacutely undermined access to and quality of cancer care. We find that providers\nunder COVID-19 pressures experience notably more excess deaths among non-COVID\nrelated hospital episodes such as, for example, for treatment of heart attacks.\nWe estimate there to be at least one such non-COVID-19 related excess death\namong patients being admitted to hospital for non-COVID-19 reasons for every 30\nCOVID-19 deaths that is caused by the disruption to the quality of care due to\nCOVID-19. In total, this amounts to 4,003 non COVID-19 excess deaths from March\n2020 to February 2021. Further, there are at least 32,189 missing cancer\npatients that should counterfactually have started receiving treatment which\nsuggests continued increased numbers of excess deaths in the future due to\ndelayed access to care in the past.\n"
    },
    {
        "paper_id": 2201.09878,
        "authors": "Agnieszka Kleszcz, Krzysztof Rusek",
        "title": "Has EU accession boosted patents performance in the EU-13? -- A critical\n  evaluation using causal impact analysis with Bayesian structural time-series\n  models",
        "comments": "This work has been submitted to the Central European Journal of\n  Economic Modelling and Econometrics and is under review process",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Nowadays innovation is one of the main determinants of economic development.\nPatents are a key measure of innovation output, as patent indicators reflect\nthe inventive performance of countries, technologies and firms. This paper\nprovides new insights on the causal effects of the enlargement of the European\nUnion (EU) by investigating the patents performance within the new EU member\nstates (EU-13). The empirical results based on data collected from the OECD\ndatabase from 1985-2017 and causal impact using a Bayesian structural\ntime-series model (proposed by Google) point towards a conclusion that joining\nthe EU has had a significant impact on patents performance in Romania, Estonia,\nPoland, Czech Republic, Croatia and Lithuania, although in the latter two\ncountries the impact was negative. For the rest of the EU-13 countries there is\nno significant effect on patent performance. Whether the EU accession effect is\nsignificant or not, the EU-13 are far behind the EU-15 (countries which entered\nthe EU before 2004) in terms of patent performance. The majority of patents\n(98.66\\%) are assigned to the EU-15, with just 1.34\\% of assignees belonging to\nthe EU-13.\n"
    },
    {
        "paper_id": 2201.09927,
        "authors": "Arega Getaneh Abate, Rossana Riccardi, Carlos Ruiz",
        "title": "Contract design in electricity markets with high penetration of\n  renewables: A two-stage approach",
        "comments": null,
        "journal-ref": "A. Getaneh, R. Riccardi and C. Ruiz. Contract design in\n  Electricity Markets with high renewables penetration: A two-stage approach.\n  Omega 111(2022) 102666",
        "doi": "10.1016/j.omega.2022.102666",
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  The interplay between risk aversion and financial derivatives has received\nincreasing attention since the advent of electricity market liberalization. One\nimportant challenge in this context is how to develop economically efficient\nand cost-effective models to integrate renewable energy sources (RES) in the\nelectricity market, which constitutes a relatively new and exciting field of\nresearch. This paper proposes a game-theoretical equilibrium model that\ncharacterizes the interactions between oligopolistic generators in a two-stage\nelectricity market under the presence of high RES penetration. Given\nconventional generators with generation cost uncertainty and renewable\ngenerators with intermittent and stochastic capacity, we consider a single\nfutures contract market that is cleared prior to a spot market where the energy\ndelivery takes place. We introduce physical and financial contracts to evaluate\ntheir performance assess their impact on the electricity market outcomes and\nexamine how these depend on the level of RES penetration. Since market\nparticipants are usually risk-averse, a coherent risk measure is introduced to\ndeal with both risk-neutral and risk-averse generators. We derive analytical\nrelationships between contracts, study the implications of uncertainties, test\nthe performance of the proposed equilibrium model and its main properties\nthrough numerical examples. Our results show that overall electricity prices,\ngeneration costs, profits, and quantities for conventional generators decrease,\nwhereas quantities and profits for RES generators increase with RES\npenetration. Hence, both physical and financial contracts efficiently mitigate\nthe impact of uncertainties and help the integration of RES into the\nelectricity system.\n"
    },
    {
        "paper_id": 2201.10115,
        "authors": "Suat Evren, Praneeth Vepakomma",
        "title": "Effects of Privacy-Inducing Noise on Welfare and Influence of Referendum\n  Systems",
        "comments": "22 pages, 2 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Social choice functions help aggregate individual preferences while\ndifferentially private mechanisms provide formal privacy guarantees to release\nanswers of queries operating on sensitive data. However, preserving\ndifferential privacy requires introducing noise to the system, and therefore\nmay lead to undesired byproducts. Does an increase in the level of differential\nprivacy for releasing the outputs of social choice functions increase or\ndecrease the level of influence and welfare, and at what rate? In this paper,\nwe mainly address this question in more precise terms in a referendum setting\nwith two candidates when the celebrated randomized response mechanism is used.\nWe show that there is an inversely-proportional relation between welfare and\nprivacy, and also influence and privacy.\n"
    },
    {
        "paper_id": 2201.10173,
        "authors": "Kyungsub Lee, Byoung Ki Seo",
        "title": "Modeling bid and ask price dynamics with an extended Hawkes process and\n  its empirical applications for high-frequency stock market data",
        "comments": null,
        "journal-ref": null,
        "doi": "10.1093/jjfinec/nbab029",
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  This study proposes a versatile model for the dynamics of the best bid and\nask prices using an extended Hawkes process. The model incorporates the zero\nintensities of the spread-narrowing processes at the minimum bid-ask spread,\nspread-dependent intensities, possible negative excitement, and nonnegative\nintensities. We apply the model to high-frequency best bid and ask price data\nfrom US stock markets. The empirical findings demonstrate a spread-narrowing\ntendency, excitations of the intensities caused by previous events, the impact\nof flash crashes, characteristic trends in fast trading over time, and the\ndifferent features of market participants in the various exchanges.\n"
    },
    {
        "paper_id": 2201.10304,
        "authors": "Anindya Goswami, Kedar Nath Mukherjee, Irvine Homi Patalwala, Sanjay\n  N. S",
        "title": "Regime recovery using implied volatility in Markov modulated market\n  model",
        "comments": "19 pages, 9 images",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  In the regime switching extension of Black-Scholes-Merton model of asset\nprice dynamics, one assumes that the volatility coefficient evolves as a hidden\npure jump process. Under the assumption of Markov regime switching, we have\nconsidered the locally risk minimizing price of European vanilla options. By\npretending these prices or their noisy versions as traded prices, we have first\ncomputed the implied volatility (IV) of the underlying asset. Then by\nperforming several numerical experiments we have investigated the dependence of\nIV on the time to maturity (TTM) and strike price of the vanilla options. We\nhave observed a clear dependence that is at par with the empirically observed\nstylized facts. Furthermore, we have experimentally validated that IV time\nseries, obtained from contracts with moneyness and TTM varying in particular\nnarrow ranges, can recover the transition instances of the hidden Markov chain.\nSuch regime recovery has also been proved in a theoretical setting. Moreover,\nthe novel scheme for computing option price is shown to be stable.\n"
    },
    {
        "paper_id": 2201.10351,
        "authors": "Stefan Vamosi and Michael Platzer and Thomas Reutterer",
        "title": "AI-based Re-identification of Behavioral Clickstream Data",
        "comments": "Submitted to the EMAC Conference 2022",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  AI-based face recognition, i.e., the re-identification of individuals within\nimages, is an already well established technology for video surveillance, for\nuser authentication, for tagging photos of friends, etc. This paper\ndemonstrates that similar techniques can be applied to successfully re-identify\nindividuals purely based on their behavioral patterns. In contrast to\nde-anonymization attacks based on record linkage, these methods do not require\nany overlap in data points between a released dataset and an identified\nauxiliary dataset. The mere resemblance of behavioral patterns between records\nis sufficient to correctly attribute behavioral data to identified individuals.\nFurther, we can demonstrate that data perturbation does not provide protection,\nunless a significant share of data utility is being destroyed. These findings\ncall for sincere cautions when sharing actual behavioral data with third\nparties, as modern-day privacy regulations, like the GDPR, define their scope\nbased on the ability to re-identify. This has also strong implications for the\nMarketing domain, when dealing with potentially re-identify-able data sources\nlike shopping behavior, clickstream data or cockies. We also demonstrate how\nsynthetic data can offer a viable alternative, that is shown to be resilient\nagainst our introduced AI-based re-identification attacks.\n"
    },
    {
        "paper_id": 2201.10391,
        "authors": "Henrique Guerreiro and Jo\\~ao Guerra",
        "title": "VIX pricing in the rBergomi model under a regime switching change of\n  measure",
        "comments": "34 pages, 9 figures",
        "journal-ref": "Quantitative Finance, 23(5):721-738, 2023",
        "doi": "10.1080/14697688.2023.2178321",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The rBergomi model under the physical measure consists of modeling the\nlog-variance as a truncated Brownian semi-stationary process. Then, a\ndeterministic change of measure is applied. The rBergomi model is able to\nreproduce observed market SP500 smiles with few parameters, but by virtue of\nthe deterministic change of measure, produces flat VIX smiles, in contrast to\nthe upward sloping smiles observed in the market. We use the exact solution for\na certain inhomogeneous fractional Ornstein-Uhlenbeck equation to build a\nregime switching stochastic change of measure for the rBergomi model that both\nyields upward slopping VIX smiles and is equipped with an efficient\nsemi-analytic Monte Carlo method to price VIX options. The model also allows an\napproximation of the VIX, which leads to a significant reduction of the\ncomputational cost of pricing VIX options and futures. A variance reduction\ntechnique based on the underlying continuous time Markov chain allows us to\nfurther reduce the computational cost. We verify the capabilities of our model\nby calibrating it to observed market smiles and discuss the results.\n"
    },
    {
        "paper_id": 2201.10454,
        "authors": "Marcin Pitera and Thorsten Schmidt",
        "title": "Estimating and backtesting risk under heavy tails",
        "comments": "Double submission. Please see arXiv:2010.09937",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  While the estimation of risk is an important question in the daily business\nof banking and insurance, many existing plug-in estimation procedures suffer\nfrom an unnecessary bias. This often leads to the underestimation of risk and\nnegatively impacts backtesting results, especially in small sample cases. In\nthis article we show that the link between estimation bias and backtesting can\nbe traced back to the dual relationship between risk measures and the\ncorresponding performance measures, and discuss this in reference to\nvalue-at-risk, expected shortfall and expectile value-at-risk.\n  Motivated by the consistent underestimation of risk by plug-in procedures, we\npropose a new algorithm for bias correction and show how to apply it for\ngeneralized Pareto distributions to the i.i.d.\\ setting and to a GARCH(1,1)\ntime series. In particular, we show that the application of our algorithm leads\nto gain in efficiency when heavy tails or heteroscedasticity exists in the\ndata.\n"
    },
    {
        "paper_id": 2201.10466,
        "authors": "Giuseppe Brandi and T. Di Matteo",
        "title": "Multiscaling and rough volatility: an empirical investigation",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  Pricing derivatives goes back to the acclaimed Black and Scholes model.\nHowever, such a modeling approach is known not to be able to reproduce some of\nthe financial stylized facts, including the dynamics of volatility. In the\nmathematical finance community, it has therefore emerged a new paradigm, named\nrough volatility modeling, that represents the volatility dynamics of financial\nassets as a fractional Brownian motion with Hurst exponent very small, which\nindeed produces rough paths. At the same time, prices' time series have been\nshown to be multiscaling, characterized by different Hurst scaling exponents.\nThis paper assesses the interplay, if present, between price multiscaling and\nvolatility roughness, defined as the (low) Hurst exponent of the volatility\nprocess. In particular, we perform extensive simulation experiments by using\none of the leading rough volatility models present in the literature, the rough\nBergomi model. A real data analysis is also carried out in order to test if the\nrough volatility model reproduces the same relationship. We find that the model\nis able to reproduce multiscaling features of the prices' time series when a\nlow value of the Hurst exponent is used but it fails to reproduce what the real\ndata say. Indeed, we find that the dependency between prices' multiscaling and\nthe Hurst exponent of the volatility process is diametrically opposite to what\nwe find in real data, namely a negative interplay between the two.\n"
    },
    {
        "paper_id": 2201.10524,
        "authors": "Maximilian G\\\"obel, Nuno Tavares",
        "title": "Zombie-Lending in the United States -- Prevalence versus Relevance",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Extraordinary fiscal and monetary interventions in response to the COVID-19\npandemic have revived concerns about zombie prevalence in advanced economies.\nWithin a sample of publicly listed U.S. companies, we find zombie prevalence\nand zombie-lending not to be a widespread phenomenon per se. Nevertheless, our\nresults reveal negative spillovers of zombie-lending on productivity,\ncapital-growth, and employment-growth of non-zombies as well as on overall\nbusiness dynamism. It is predominantly the class of healthy small- and\nmedium-sized companies that is sensitive to zombie-lending activities, with\nfinancial constraints further amplifying these effects.\n"
    },
    {
        "paper_id": 2201.10726,
        "authors": "B.N. Kausik",
        "title": "Income Inequality, Cause and Cure",
        "comments": null,
        "journal-ref": "Challenge 1-13 2022",
        "doi": "10.1080/05775132.2022.2046883",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We argue that the recent growth in income inequality is driven by disparate\ngrowth in investment income rather than by disparate growth in wages.\nSpecifically, we present evidence that real wages are flat across a range of\nprofessions, doctors, software engineers, auto mechanics and cashiers, while\nstock ownership favors higher education and income levels. Artificial\nIntelligence and automation allocate an increased share of job tasks towards\ncapital and away from labor. The rewards of automation accrue to capital, and\nare reflected in the growth of the stock market with several companies now\nvalued in the trillions. We propose a Deferred Investment Payroll plan to\nenable all workers to participate in the rewards of automation and analyze the\nperformance of such a plan.\n  JEL Classification: J31, J33, O33\n"
    },
    {
        "paper_id": 2201.10808,
        "authors": "Uwe Sunde, Dainis Zegners, Anthony Strittmatter",
        "title": "Speed, Quality, and the Optimal Timing of Complex Decisions: Field\n  Evidence",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  This paper presents an empirical investigation of the relation between\ndecision speed and decision quality for a real-world setting of\ncognitively-demanding decisions in which the timing of decisions is endogenous:\nprofessional chess. Move-by-move data provide exceptionally detailed and\nprecise information about decision times and decision quality, based on a\ncomparison of actual decisions to a computational benchmark of best moves\nconstructed using the artificial intelligence of a chess engine. The results\nreveal that faster decisions are associated with better performance. The\nfindings are consistent with the predictions of procedural decision models like\ndrift-diffusion-models in which decision makers sequentially acquire\ninformation about decision alternatives with uncertain valuations.\n"
    },
    {
        "paper_id": 2201.10846,
        "authors": "Jan Rosenzweig",
        "title": "Fat Tails and Optimal Liability Driven Portfolios",
        "comments": null,
        "journal-ref": "Risk June 2023",
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  We look at optimal liability-driven portfolios in a family of fat-tailed and\nextremal risk measures, especially in the context of pension fund and insurance\nfixed cashflow liability profiles, but also those arising in derivatives books\nsuch as delta one books or options books in the presence of stochastic\nvolatilities. In the extremal limit, we recover a new tail risk measure,\nExtreme Deviation (XD), an extremal risk measure significantly more sensitive\nto extremal returns than CVaR. Resulting optimal portfolios optimize the return\nper unit of XD, with portfolio weights consisting of a liability hedging\ncontribution, and a risk contribution seeking to generate positive\nrisk-adjusted return. The resulting allocations are analyzed qualitatively and\nquantitatively in a number of different limits.\n"
    },
    {
        "paper_id": 2201.10961,
        "authors": "Nael Alsaleh and Bilal Farooq",
        "title": "The Impact of COVID-19 Pandemic on Ridesourcing Services Differed\n  Between Small Towns and Large Cities",
        "comments": null,
        "journal-ref": "PLoS ONE 17(10): e0275714 (2022)",
        "doi": "10.1371/journal.pone.0275714",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The COVID-19 pandemic has significantly influenced all modes of\ntransportation. However, it is still unclear how the pandemic affected the\ndemand for ridesourcing services and whether these effects varied between small\ntowns and large cities. We analyzed over 220 million ride requests in the City\nof Chicago (population: 2.7 million), Illinois, and 52 thousand in the Town of\nInnisfil (population: 37 thousand), Ontario, to investigate the impact of the\nCOVID-19 pandemic on the ridesourcing demand in the two locations. Overall, the\npandemic resulted in fewer trips in areas with higher proportions of seniors\nand more trips to parks and green spaces. Ridesourcing demand was adversely\naffected by the stringency index and COVID-19-related variables, and positively\naffected by vaccination rates. However, compared to Innisfil, ridesourcing\nservices in Chicago experienced higher reductions in demand, were more affected\nby the number of hospitalizations and deaths, were less impacted by vaccination\nrates, and had lower recovery rates.\n"
    },
    {
        "paper_id": 2201.11047,
        "authors": "Lucas Finamor",
        "title": "Labor market conditions and college graduation: evidence from Brazil",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  College students graduating in a recession have been shown to face large and\npersistent negative effects on their earnings, health, and other outcomes. This\npaper investigates whether students delay graduation to avoid these effects.\nUsing data on the universe of students in higher education in Brazil and\nleveraging variation in labor market conditions across time, space, and chosen\nmajors, the paper finds that students in public institutions delay graduation\nto avoid entering depressed labor markets. A typical recession causes the\non-time graduation rate to fall by 6.5% in public universities and there is no\neffect on private institutions. The induced delaying increases average\ngraduation by 0.11 semesters, consistent with 1 out of 18 students delaying\ngraduation by one year in public universities. The delaying effect is larger\nfor students with higher scores, in higher-earnings majors, and from more\nadvantaged backgrounds. This has important implications for the distributional\nimpact of recessions.\n"
    },
    {
        "paper_id": 2201.11051,
        "authors": "Peng Li, Arim Park, Soohyun Cho, and Yao Zhao",
        "title": "Toward a More Populous Online Platform: The Economic Impacts of\n  Compensated Reviews",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Many companies nowadays offer compensation to online reviews (called\ncompensated reviews), expecting to increase the volume of their non-compensated\nreviews and overall rating. Does this strategy work? On what subjects or topics\ndoes this strategy work the best? These questions have still not been answered\nin the literature but draw substantial interest from the industry. In this\npaper, we study the effect of compensated reviews on non-compensated reviews by\nutilizing online reviews on 1,240 auto shipping companies over a ten-year\nperiod from a transportation website. Because some online reviews have missing\ninformation on their compensation status, we first develop a classification\nalgorithm to differentiate compensated reviews from non-compensated reviews by\nleveraging a machine learning-based identification process, drawing upon the\nunique features of the compensated reviews. From the classification results, we\nempirically investigate the effects of compensated reviews on non-compensated.\nOur results indicate that the number of compensated reviews does indeed\nincrease the number of non-compensated reviews. In addition, the ratings of\ncompensated reviews positively affect the ratings of non-compensated reviews.\nMoreover, if the compensated reviews feature the topic or subject of a car\nshipping function, the positive effect of compensated reviews on\nnon-compensated ones is the strongest. Besides methodological contributions in\ntext classification and empirical modeling, our study provides empirical\nevidence on how to prove the effectiveness of compensated online reviews in\nterms of improving the platform's overall online reviews and ratings. Also, it\nsuggests a guideline for utilizing compensated reviews to their full strength,\nthat is, with regard to featuring certain topics or subjects in these reviews\nto achieve the best outcome.\n"
    },
    {
        "paper_id": 2201.1107,
        "authors": "Andrea Berdondini",
        "title": "The theory of quantitative trading",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Abstract: This book consists of a selection of articles divided into three\nmain themes: Statistics, Quantitative Trading, Psychology. These three\narguments are indispensable for the development of a quantitative trading\nsystem. The order of the articles was chosen so as to constitute a single\nlogical reasoning that develops progressively.\n"
    },
    {
        "paper_id": 2201.11122,
        "authors": "Eric C.K. Cheung, Oscar Peralta, Jae-Kyung Woo",
        "title": "Multivariate matrix-exponential affine mixtures and their applications\n  in risk theory",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  In this paper, a class of multivariate matrix-exponential affine mixtures\nwith matrix-exponential marginals is proposed. The class is shown to possess\nvarious attractive properties such as closure under size-biased Esscher\ntransform, order statistics, residual lifetime and higher order equilibrium\ndistributions. This allows for explicit calculations of various actuarial\nquantities of interest. The results are applied in a wide range of actuarial\nproblems including multivariate risk measures, aggregate loss, large claims\nreinsurance, weighted premium calculations and risk capital allocation.\nFurthermore, a multiplicative background risk model with dependent risks is\nconsidered and its capital allocation rules are provided as well. We finalize\nby discussing a calibration scheme based on complete data and potential avenues\nof research.\n"
    },
    {
        "paper_id": 2201.11214,
        "authors": "Carla Zoe Cremer, Luke Kemp",
        "title": "Democratising Risk: In Search of a Methodology to Study Existential Risk",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  Studying potential global catastrophes is vital. The high stakes of\nexistential risk studies (ERS) necessitate serious scrutiny and\nself-reflection. We argue that existing approaches to studying existential risk\nare not yet fit for purpose, and perhaps even run the risk of increasing harm.\nWe highlight general challenges in ERS: accommodating value pluralism, crafting\nprecise definitions, developing comprehensive tools for risk assessment,\ndealing with uncertainty, and accounting for the dangers associated with taking\nexceptional actions to mitigate or prevent catastrophes. The most influential\nframework for ERS, the 'techno-utopian approach' (TUA), struggles with these\nissues and has a unique set of additional problems: it unnecessarily combines\nthe study of longtermism and longtermist ethics with the study of extinction,\nrelies on a non-representative moral worldview, uses ambiguous and inadequate\ndefinitions, fails to incorporate insights from risk assessment in relevant\nfields, chooses arbitrary categorisations of risk, and advocates for dangerous\nmitigation strategies. Its moral and empirical assumptions might be\nparticularly vulnerable to securitisation and misuse. We suggest several key\nimprovements: separating the study of extinction ethics (ethical implications\nof extinction) and existential ethics (the ethical implications of different\nsocietal forms), from the analysis of human extinction and global catastrophe;\ndrawing on the latest developments in risk assessment literature; diversifying\nthe field, and; democratising its policy recommendations.\n"
    },
    {
        "paper_id": 2201.11241,
        "authors": "Julio Guerrero and Giuseppe Orlando",
        "title": "Stochastic Local Volatility models and the Wei-Norman factorization\n  method",
        "comments": "25 latex pages, 16 figures. Accepted in Discrete and Continuous\n  Dynamical Systems Series S",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  In this paper, we show that a time-dependent local stochastic volatility\n(SLV) model can be reduced to a system of autonomous PDEs that can be solved\nusing the Heat kernel, by means of the Wei-Norman factorization method and Lie\nalgebraic techniques. Then, we compare the results of traditional Monte Carlo\nsimulations with the explicit solutions obtained by said techniques. This\napproach is new in the literature and, in addition to reducing a non-autonomous\nproblem into an autonomous one, allows for reduced time in numerical\ncomputations.\n"
    },
    {
        "paper_id": 2201.11394,
        "authors": "Koichi Miyamoto",
        "title": "Quantum algorithm for calculating risk contributions in a credit\n  portfolio",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Finance is one of the promising field for industrial application of quantum\ncomputing. In particular, quantum algorithms for calculation of risk measures\nsuch as the value at risk and the conditional value at risk of a credit\nportfolio have been proposed. In this paper, we focus on another problem in\ncredit risk management, calculation of risk contributions, which quantify the\nconcentration of the risk on subgroups in the portfolio. Based on the recent\nquantum algorithm for simultaneous estimation of multiple expected values, we\npropose the method for credit risk contribution calculation. We also evaluate\nthe query complexity of the proposed method and see that it scales as\n$\\widetilde{O}\\left(\\sqrt{N_{\\rm gr}}/\\epsilon\\right)$ on the subgroup number\n$N_{\\rm gr}$ and the accuracy $\\epsilon$, in contrast with the classical method\nwith $\\widetilde{O}\\left(\\log(N_{\\rm gr})/\\epsilon^2\\right)$ complexity. This\nmeans that, for calculation of risk contributions of finely divided subgroups,\nthe advantage of the quantum method is reduced compared with risk measure\ncalculation for the entire portfolio. Nevertheless, the quantum method can be\nadvantageous in high-accuracy calculation, and in fact yield less complexity\nthan the classical method in some practically plausible setting.\n"
    },
    {
        "paper_id": 2201.11441,
        "authors": "Raphael Koster, Jan Balaguer, Andrea Tacchetti, Ari Weinstein, Tina\n  Zhu, Oliver Hauser, Duncan Williams, Lucy Campbell-Gillingham, Phoebe\n  Thacker, Matthew Botvinick and Christopher Summerfield",
        "title": "Human-centered mechanism design with Democratic AI",
        "comments": "18 pages, 4 figures, 54 pages including supplemental materials",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Building artificial intelligence (AI) that aligns with human values is an\nunsolved problem. Here, we developed a human-in-the-loop research pipeline\ncalled Democratic AI, in which reinforcement learning is used to design a\nsocial mechanism that humans prefer by majority. A large group of humans played\nan online investment game that involved deciding whether to keep a monetary\nendowment or to share it with others for collective benefit. Shared revenue was\nreturned to players under two different redistribution mechanisms, one designed\nby the AI and the other by humans. The AI discovered a mechanism that redressed\ninitial wealth imbalance, sanctioned free riders, and successfully won the\nmajority vote. By optimizing for human preferences, Democratic AI may be a\npromising method for value-aligned policy innovation.\n"
    },
    {
        "paper_id": 2201.11507,
        "authors": "Igor Nesiolovskiy",
        "title": "Stock exchange shares ranking and binary-ternary compressive coding",
        "comments": "in Russian",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  This paper proposes a method for ranking the investment attractiveness of\nexchange-traded stocks where investment risk is not related to the volatility\nindicator but instead is related to the indicator of compression of the time\nseries of price changes. The article describes in detail the ranking algorithm,\nprovides an example of ranking the shares of all companies included in the Dow\nJones stock index. The paper additionally compares the results of ranking these\nstocks by volatility and compression and also shows the strengths of the second\nindicator, which is formed using the method of binary-ternary compression of\nhistorical financial data.\n"
    },
    {
        "paper_id": 2201.11787,
        "authors": "Ross Hyman and Nicolaus Tideman",
        "title": "A New Perspective on Impartial and Unbiased Apportionment",
        "comments": "23 pages, 2 figures, accepted for publication in the American\n  Mathematical Monthly",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  How to fairly apportion congressional seats to states has been debated for\ncenturies. We present an alternative perspective on apportionment, centered not\non states but \"families\" of state, sets of states with \"divisor-method\" quotas\nwith the same integer part. We develop ``impartial\" and ``unbiased\"\napportionment methods. Impartial methods apportion the same number of seats to\nfamilies of states containing the same total population, whether a family\nconsists of many small-population states or a few large-population states.\nUnbiased methods apportion seats so that if states are drawn repeatedly from\nthe same distribution, the expected number of seats apportioned to each family\nequals the expected divisor-method quota for that family.\n"
    },
    {
        "paper_id": 2201.1193,
        "authors": "Nicolas Lanchier and Stephanie Reed",
        "title": "Distribution of money on connected graphs with multiple banks",
        "comments": "22 pages, 2 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  This paper studies an interacting particle system of interest in econophysics\ninspired from a model introduced in the physics literature. The original model\nconsists of the customers of a single bank characterized by their capital, and\nthe discrete-time dynamics consists of monetary transactions in which a random\nindividual $x$ gives one coin to another random individual $y$, the transaction\nbeing canceled when $x$ is in debt and there is no more coins to borrow from\nthe bank. Using a combination of numerical simulations and heuristic arguments,\nphysicists conjectured that the distribution of money (the distribution of the\nnumber of coins owned by a given individual) at equilibrium converges to an\nasymmetric Laplace distribution in the large population/temperature limit. In\nthis paper, we prove and extend this conjecture to a more general model\nincluding multiple banks and interactions among customers across banks. More\nimportantly, our model assumes that customers are located on a general\nundirected connected graph (as opposed to the complete graph in the original\nmodel) where neighbors are interpreted as business partners, and transactions\noccur along the edges, thus modeling the flow of money across a social network.\nWe show the convergence to the asymmetric Laplace distribution in the large\npopulation/temperature limit for any graph, thus proving and extending the\nconjecture from the physicists, and derive an exact expression of the\ndistribution of money for all population sizes and money temperatures.\n"
    },
    {
        "paper_id": 2201.11962,
        "authors": "Seungki Min, Ciamac C. Moallemi, Costis Maglaras",
        "title": "Risk-Sensitive Optimal Execution via a Conditional Value-at-Risk\n  Objective",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  We consider a liquidation problem in which a risk-averse trader tries to\nliquidate a fixed quantity of an asset in the presence of market impact and\nrandom price fluctuations. The trader encounters a trade-off between the\ntransaction costs incurred due to market impact and the volatility risk of\nholding the position. Our formulation begins with a continuous-time and\ninfinite horizon variation of the seminal model of Almgren and Chriss (2000),\nbut we define as the objective the conditional value-at-risk (CVaR) of the\nimplementation shortfall, and allow for dynamic (adaptive) trading strategies.\nIn this setting, we are able to derive closed-form expressions for the optimal\nliquidation strategy and its value function.\n  Our results yield a number of important practical insights. We are able to\nquantify the benefit of adaptive policies over optimized static policies. The\nrelevant improvement depends only on the level of risk aversion: for moderate\nlevels of risk aversion, the optimal dynamic policy outperforms the optimal\nstatic policy by 5-15%, and outperforms the optimal volume weighted average\nprice (VWAP) policy by 15-25%. This improvement is achieved through dynamic\npolicies that exhibit \"aggressiveness-in-the-money\": trading is accelerated\nwhen price movements are favorable, and is slowed when price movements are\nunfavorable.\n  From a mathematical perspective, our analysis exploits the dual\nrepresentation of CVaR to convert the problem to a continuous-time, zero-sum\ngame. We leverage the idea of the state-space augmentation, and obtain a\npartial differential equation describing the optimal value function, which is\nseparable and a special instance of the Emden-Fowler equation. This leads to a\nclosed-form solution. As our problem is a special case of a\nlinear-quadratic-Gaussian control problem with a CVaR objective, these results\nmay be interesting in broader settings.\n"
    },
    {
        "paper_id": 2201.12263,
        "authors": "Krzysztof Rusek, Piotr Bory{\\l}o, Piotr Jaglarz, Fabien Geyer, Albert\n  Cabellos, Piotr Cho{\\l}da",
        "title": "RiskNet: Neural Risk Assessment in Networks of Unreliable Resources",
        "comments": "This paper is under consideration at Journal of Network and Systems\n  Management",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  We propose a graph neural network (GNN)-based method to predict the\ndistribution of penalties induced by outages in communication networks, where\nconnections are protected by resources shared between working and backup paths.\nThe GNN-based algorithm is trained only with random graphs generated with the\nBarab\\'asi-Albert model. Even though, the obtained test results show that we\ncan precisely model the penalties in a wide range of various existing\ntopologies. GNNs eliminate the need to simulate complex outage scenarios for\nthe network topologies under study. In practice, the whole design operation is\nlimited by 4ms on modern hardware. This way, we can gain as much as over 12,000\ntimes in the speed improvement.\n"
    },
    {
        "paper_id": 2201.12283,
        "authors": "Taylan Kabbani (1 and 2), Fatih Enes Usta (3) ((1) Ozyegin University,\n  (2) Huawei Turkey R&D Center, (3) Marmara University)",
        "title": "Predicting The Stock Trend Using News Sentiment Analysis and Technical\n  Indicators in Spark",
        "comments": "4 pages, 5 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
        "abstract": "  Predicting the stock market trend has always been challenging since its\nmovement is affected by many factors. Here, we approach the future trend\nprediction problem as a machine learning classification problem by creating\ntomorrow_trend feature as our label to be predicted. Different features are\ngiven to help the machine learning model predict the label of a given day;\nwhether it is an uptrend or downtrend, those features are technical indicators\ngenerated from the stock's price history. In addition, as financial news plays\na vital role in changing the investor's behavior, the overall sentiment score\non a given day is created from all news released on that day and added to the\nmodel as another feature. Three different machine learning models are tested in\nSpark (big-data computing platform), Logistic Regression, Random Forest, and\nGradient Boosting Machine. Random Forest was the best performing model with a\n63.58% test accuracy.\n"
    },
    {
        "paper_id": 2201.12286,
        "authors": "Ivan Letteri, Giuseppe Della Penna, Giovanni De Gasperis, Abeer Dyoub",
        "title": "A Stock Trading System for a Medium Volatile Asset using Multi Layer\n  Perceptron",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Stock market forecasting is a lucrative field of interest with promising\nprofits but not without its difficulties and for some people could be even\ncauses of failure. Financial markets by their nature are complex, non-linear\nand chaotic, which implies that accurately predicting the prices of assets that\nare part of it becomes very complicated. In this paper we propose a stock\ntrading system having as main core the feed-forward deep neural networks (DNN)\nto predict the price for the next 30 days of open market, of the shares issued\nby Abercrombie & Fitch Co. (ANF) in the stock market of the New York Stock\nExchange (NYSE).\n  The system we have elaborated calculates the most effective technical\nindicator, applying it to the predictions computed by the DNNs, for generating\ntrades. The results showed an increase in values such as Expectancy Ratio of\n2.112% of profitable trades with Sharpe, Sortino, and Calmar Ratios of 2.194,\n3.340, and 12.403 respectively. As a verification, we adopted a backtracking\nsimulation module in our system, which maps trades to actual test data\nconsisting of the last 30 days of open market on the ANF asset. Overall, the\nresults were promising bringing a total profit factor of 3.2% in just one month\nfrom a very modest budget of $100. This was possible because the system reduced\nthe number of trades by choosing the most effective and efficient trades,\nsaving on commissions and slippage costs.\n"
    },
    {
        "paper_id": 2201.12291,
        "authors": "Effat Ara Easmin Lucky, Md. Mahadi Hasan Sany, Mumenunnesa Keya, Md.\n  Moshiur Rahaman, Umme Habiba Happy, Sharun Akter Khushbu, Md. Arid Hasan",
        "title": "Simulating Using Deep Learning The World Trade Forecasting of\n  Export-Import Exchange Rate Convergence Factor During COVID-19",
        "comments": "Accepted in ICDLAIR 2021",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
        "abstract": "  By trade we usually mean the exchange of goods between states and countries.\nInternational trade acts as a barometer of the economic prosperity index and\nevery country is overly dependent on resources, so international trade is\nessential. Trade is significant to the global health crisis, saving lives and\nlivelihoods. By collecting the dataset called \"Effects of COVID19 on trade\"\nfrom the state website NZ Tatauranga Aotearoa, we have developed a sustainable\nprediction process on the effects of COVID-19 in world trade using a deep\nlearning model. In the research, we have given a 180-day trade forecast where\nthe ups and downs of daily imports and exports have been accurately predicted\nin the Covid-19 period. In order to fulfill this prediction, we have taken data\nfrom 1st January 2015 to 30th May 2021 for all countries, all commodities, and\nall transport systems and have recovered what the world trade situation will be\nin the next 180 days during the Covid-19 period. The deep learning method has\nreceived equal attention from both investors and researchers in the field of\nin-depth observation. This study predicts global trade using the Long-Short\nTerm Memory. Time series analysis can be useful to see how a given asset,\nsecurity, or economy changes over time. Time series analysis plays an important\nrole in past analysis to get different predictions of the future and it can be\nobserved that some factors affect a particular variable from period to period.\nThrough the time series it is possible to observe how various economic changes\nor trade effects change over time. By reviewing these changes, one can be aware\nof the steps to be taken in the future and a country can be more careful in\nterms of imports and exports accordingly. From our time series analysis, it can\nbe said that the LSTM model has given a very gracious thought of the future\nworld import and export situation in terms of trade.\n"
    },
    {
        "paper_id": 2201.12402,
        "authors": "Hui Xu, Yue Wu",
        "title": "The China Trade Shock and the ESG Performances of US firms",
        "comments": "53 pages; 11 tables; 3 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  How does import competition from China affect engagement on ESG initiatives\nby US corporates? On the one hand, reduced profitability due to import\ncompetition and lagging ESG performance of Chinese exporters can disincentivize\nUS firms to put more resources to ESG initiatives. On the other hand, the shift\nfrom labor-intensive production to capital/technology-intensive production\nalong with offshoring may improve the US company's ESG performance. Moreover,\nUS companies have incentives to actively pursue more ESG engagement to\ndifferentiate from Chinese imports. Exploiting a trade policy in which US\ncongress granted China the Permanent Normal Trade Relations and the resulting\nchange in expected tariff rates on Chinese imports, we find that greater import\ncompetition from China leads to an increase in the US company's ESG\nperformance. The improvement primarily stems from \"doing more positives\" and\nfrom more involvement on environmental initiatives. Indirect and direct\nevidence shows that the improvement is not driven by the change in production\nprocess or offshoring, but is consistent with product differentiation. Our\nresults suggest that the trade shock from China has significant impact on the\nUS company's ESG performance.\n"
    },
    {
        "paper_id": 2201.12618,
        "authors": "Gian Paolo Clemente and Rosanna Grassi and Giorgio Rizzini",
        "title": "The effect of the pandemic on complex socio-economic systems: community\n  detection induced by communicability",
        "comments": null,
        "journal-ref": "Soft Computing (2023)",
        "doi": "10.1007/s00500-023-09456-3",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The increasing complexity of interrelated systems has made the use of\nmultiplex networks an important tool for explaining the nature of relations\nbetween elements in the system. In this paper, we aim at investigating various\naspects of countries' behaviour during the coronavirus pandemic period. By\nmeans of a multiplex network we consider simultaneously stringency index\nvalues, COVID-19 infections and international trade data, in order to detect\nclusters of countries that showed a similar reaction to the pandemic. We\npropose a new methodological approach based on the Estrada communicability for\nidentifying communities on a multiplex network, based on a two-step\noptimization. At first, we determine the optimal inter-layer intensity between\nlevels by minimizing a distance function. Hence, the optimal inter-layer\nintensity is used to detect communities on each layer. Our findings show that\nthe community detection on this multiplex network has greater information power\nthan classical methods for single-layer networks. Our approach better reveals\nclusters on each layer with respect to the application of the same approach on\neach single-layer. Moreover, detected groups in the multiplex case benefit of a\nhigher cohesion, leading to identifying on each layer a lower number of\ncommunities with respect to the ones obtained in the single-layer cases.\n"
    },
    {
        "paper_id": 2201.12731,
        "authors": "Maxim Bichuch, Nils Detering",
        "title": "Optimal Support for Distressed Subsidiaries -- a Systemic Risk\n  Perspective",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We consider a network of bank holdings, where every holding has two\nsubsidiaries of different types. A subsidiary can trade with another holding's\nsubsidiary of the same type. Holdings support their subsidiaries up to a\ncertain level when they would otherwise fail to honor their financial\nobligations. We investigate the spread of contagion in this banking network\nwhen the number of bank holdings is large, and find the final number of\ndefaulted subsidiaries under different rules for the holding support. We also\nconsider resilience of this multilayered network to small shocks. Our work\nsheds light onto the role that holding structures can play in the amplification\nof financial stress. We find that depending on the capitalization of the\nnetwork, a holding structure can be beneficial as compared to smaller separated\nentities. In other instances, it can be harmful and actually increase\ncontagion. We illustrate our results in a numerical case study and also\ndetermine the optimal level of holding support from a regulator perspective.\n"
    },
    {
        "paper_id": 2201.12893,
        "authors": "Yulin Liu and Luyao Zhang",
        "title": "Cryptocurrency Valuation: An Explainable AI Approach",
        "comments": null,
        "journal-ref": null,
        "doi": "10.2139/ssrn.3657986",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Currently, there are no convincing proxies for the fundamentals of\ncryptocurrency assets. We propose a new market-to-fundamental ratio, the\nprice-to-utility (PU) ratio, utilizing unique blockchain accounting methods. We\nthen proxy various existing fundamental-to-market ratios by Bitcoin historical\ndata and find they have little predictive power for short-term bitcoin returns.\nHowever, PU ratio effectively predicts long-term bitcoin returns than\nalternative methods. Furthermore, we verify the explainability of PU ratio\nusing machine learning. Finally, we present an automated trading strategy\nadvised by the PU ratio that outperforms the conventional buy-and-hold and\nmarket-timing strategies. Our research contributes to explainable AI in finance\nfrom three facets: First, our market-to-fundamental ratio is based on classic\nmonetary theory and the unique UTXO model of Bitcoin accounting rather than ad\nhoc; Second, the empirical evidence testifies the buy-low and sell-high\nimplications of the ratio; Finally, we distribute the trading algorithms as\nopen-source software via Python Package Index for future research, which is\nexceptional in finance research.\n"
    },
    {
        "paper_id": 2201.12898,
        "authors": "Giuseppe C. Calafiore, Giulia Fracastoro, Anton V. Proskurnikov",
        "title": "Clearing Payments in Dynamic Financial Networks",
        "comments": null,
        "journal-ref": null,
        "doi": "10.1016/j.automatica.2023.111299",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper proposes a novel dynamical model for determining clearing payments\nin financial networks. We extend the classical Eisenberg-Noe model of financial\ncontagion to multiple time periods, allowing financial operations to continue\nafter possible initial pseudo defaults, thus permitting nodes to recover and\neventually fulfil their liabilities. Optimal clearing payments in our model are\ncomputed by solving a suitable linear program, both in the full matrix payments\ncase and in the pro-rata constrained case. We prove that the proposed model\nobeys the \\emph{priority of debt claims} requirement, that is, each node at\nevery step either pays its liabilities in full, or it pays out all its balance.\nIn the pro-rata case, the optimal dynamic clearing payments are unique, and can\nbe determined via a time-decoupled sequential optimization approach.\n"
    },
    {
        "paper_id": 2201.13094,
        "authors": "Beatrice Acciaio, Anastasis Kratsios, Gudmund Pammer",
        "title": "Designing Universal Causal Deep Learning Models: The Geometric\n  (Hyper)Transformer",
        "comments": "Main Body: 31 Pages, Proofs: 16 Pages, Figures: 13, Tables: 3",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Several problems in stochastic analysis are defined through their geometry,\nand preserving that geometric structure is essential to generating meaningful\npredictions. Nevertheless, how to design principled deep learning (DL) models\ncapable of encoding these geometric structures remains largely unknown. We\naddress this open problem by introducing a universal causal geometric DL\nframework in which the user specifies a suitable pair of metric spaces\n$\\mathscr{X}$ and $\\mathscr{Y}$ and our framework returns a DL model capable of\ncausally approximating any ``regular'' map sending time series in\n$\\mathscr{X}^{\\mathbb{Z}}$ to time series in $\\mathscr{Y}^{\\mathbb{Z}}$ while\nrespecting their forward flow of information throughout time. Suitable\ngeometries on $\\mathscr{Y}$ include various (adapted) Wasserstein spaces\narising in optimal stopping problems, a variety of statistical manifolds\ndescribing the conditional distribution of continuous-time finite state Markov\nchains, and all Fr\\'{e}chet spaces admitting a Schauder basis, e.g. as in\nclassical finance. Suitable spaces $\\mathscr{X}$ are compact subsets of any\nEuclidean space. Our results all quantitatively express the number of\nparameters needed for our DL model to achieve a given approximation error as a\nfunction of the target map's regularity and the geometric structure both of\n$\\mathscr{X}$ and of $\\mathscr{Y}$. Even when omitting any temporal structure,\nour universal approximation theorems are the first guarantees that H\\\"{o}lder\nfunctions, defined between such $\\mathscr{X}$ and $\\mathscr{Y}$ can be\napproximated by DL models.\n"
    },
    {
        "paper_id": 2201.13235,
        "authors": "Jiayue Xu",
        "title": "A hybrid deep learning approach for purchasing strategy of carbon\n  emission rights -- Based on Shanghai pilot market",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The price of carbon emission rights play a crucial role in carbon trading\nmarkets. Therefore, accurate prediction of the price is critical. Taking the\nShanghai pilot market as an example, this paper attempted to design a carbon\nemission purchasing strategy for enterprises, and establish a carbon emission\nprice prediction model to help them reduce the purchasing cost. To make\npredictions more precise, we built a hybrid deep learning model by embedding\nGeneralized Autoregressive Conditional Heteroskedastic (GARCH) into the Gate\nRecurrent Unit (GRU) model, and compared the performance with those of other\nmodels. Then, based on the Iceberg Order Theory and the predicted price, we\nproposed the purchasing strategy of carbon emission rights. As a result, the\nprediction errors of the GARCH-GRU model with a 5-day sliding time window were\nthe minimum values of all six models. And in the simulation, the purchasing\nstrategy based on the GARCH-GRU model was executed with the least cost as well.\nThe carbon emission purchasing strategy constructed by the hybrid deep learning\nmethod can accurately send out timing signals, and help enterprises reduce the\npurchasing cost of carbon emission permits.\n"
    },
    {
        "paper_id": 2201.13325,
        "authors": "William Schueller, Christian Diem, Melanie Hinterplattner, Johannes\n  Stangl, Beate Conrady, Markus Gerschberger and Stefan Thurner",
        "title": "Propagation of disruptions in supply networks of essential goods: A\n  population-centered perspective of systemic risk",
        "comments": "*The authors acknowledge the equal contributions of WS and CD",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The Covid-19 pandemic drastically emphasized the fragility of national and\ninternational supply networks (SNs),leading to significant supply shortages of\nessential goods for people, such as food and medical equipment. Severe\ndisruptions that propagate along complex SNs can expose the population of\nentire regions or even countries to these risks. A lack of both, data and\nquantitative methodology, has hitherto hindered us to empirically quantify the\nvulnerability of the population to disruptions. Here we develop a data-driven\nsimulation methodology to locally quantify actual supply losses for the\npopulation that result from the cascading of supply disruptions. We demonstrate\nthe method on a large food SN of a European country including 22,938 business\npremises, 44,355 supply links and 116 local administrative districts. We rank\nthe business premises with respect to their criticality for the districts'\npopulation with the proposed systemic risk index, SRIcrit, to identify around\n30 premises that -- in case of their failure -- are expected to cause critical\nsupply shortages in sizable fractions of the population. The new methodology is\nimmediately policy relevant as a fact-driven and generalizable crisis\nmanagement tool. This work represents a starting point for quantitatively\nstudying SN disruptions focused on the well-being of the population.\n"
    },
    {
        "paper_id": 2201.13416,
        "authors": "Carlo Campajola and Marco D'Errico and Claudio J. Tessone",
        "title": "MicroVelocity: rethinking the Velocity of Money for digital currencies",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  We propose a novel framework to analyse the velocity of money in terms of the\ncontribution (MicroVelocity) of each individual agent, and to uncover the\ndistributional determinants of aggregate velocity. Leveraging on complete\npublicly available transactions data stored in blockchains from four\ncryptocurrencies, we empirically find that MicroVelocity i) is very\nheterogeneously distributed and ii) strongly correlates with agents' wealth. We\nfurther document the emergence of high-velocity intermediaries, thereby\nchallenging the idea that these systems are fully decentralised. Further, our\nframework and results provide policy insights for the development and analysis\nof digital currencies.\n"
    },
    {
        "paper_id": 2202.00006,
        "authors": "Aneel Bhusal",
        "title": "Impact of Information and Communication Technology on Individual\n  Well-being",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper investigates the impact of information and communication\ntechnology (ICT) adoption on individual well-being.\n"
    },
    {
        "paper_id": 2202.00007,
        "authors": "Aneel Bhusal and Madhu Sudan Gautam",
        "title": "Impact of Gold Prices on Stock Exchange: An Empirical Case Study of\n  Nepal",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The purpose of this study is to examine the long-run relationship between\ngold prices and Nepal Stock Exchange (NEPSE).\n"
    },
    {
        "paper_id": 2202.00044,
        "authors": "Chad Brown, Jeronimo Carballo, Alessandro Peri",
        "title": "Bankruptcy Shocks and Legal Labor Markets: Evidence from the Court\n  Competition Era",
        "comments": "57 pages, 7 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We study how Chapter 11 bankruptcies affect local legal labor markets. We\ndocument that bankruptcy shocks increase county legal employment and\ncorroborate this finding by exploiting a stipulation of the law known as Forum\nShopping during the Court Competition Era (1991-1996). We quantify losses to\nlocal communities from firms forum shopping away from their local area as\nfollows. First, we calculate the unrealized potential employment gains implied\nby our reduced-form results. Second, we structurally estimate a model of legal\nlabor markets and quantify welfare losses. We uncover meaningful costs to local\ncommunities from lax bankruptcy venue laws.\n"
    },
    {
        "paper_id": 2202.00108,
        "authors": "Elena G. Demidova (Starooskolsky Technological Institute)",
        "title": "The regulation methods of fiscal risk in the framework of the\n  implementation of entrepreneurship support",
        "comments": "8 pages, in Russian. Keywords: financing, financial risks regulation,\n  entrepreneurship",
        "journal-ref": "Finance and credit. - 2017. - Vol. 23, No. 36. - pp. 2189 - 2196",
        "doi": "10.24891/fc.23.36.2189",
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  In this article, I consider the issues of financial resource shortage, the\nlimited possibility of attracting bank loans, and business risk. All these\nproblems constrain the development of entrepreneurship. Objectives I aim to\ndetermine a scientifically based financial mechanism of financing the priority\ndirections of territories' development. In this study, I develop tools of\nfinancing the priority directions of the municipal economy. The proposed\nfinancial scheme allows to expand the volume of financing and ensure the access\nof businesses to financial support. The article proposes concrete financing\nmechanisms for investment with minimal risk for the budget and preferential\nconditions for business.\n"
    },
    {
        "paper_id": 2202.00109,
        "authors": "Adel Daoud, Felipe Jordan, Makkunda Sharma, Fredrik Johansson, Devdatt\n  Dubhashi, Sourabh Paul, and Subhashis Banerjee",
        "title": "Measuring poverty in India with machine learning and remote sensing",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  In this paper, we use deep learning to estimate living conditions in India.\nWe use both census and surveys to train the models. Our procedure achieves\ncomparable results to those found in the literature, but for a wide range of\noutcomes.\n"
    },
    {
        "paper_id": 2202.00124,
        "authors": "Marika Ormotsadze",
        "title": "Problems of Tax Administration and its Impact on Budget Revenues",
        "comments": "in Georgian language",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  The topic under study is of crucial importance, especially for developing\ncountries. The aim of the present paper is to study the problems in revenue\nadministration in terms of tax revenue in Georgia and analyze foreign\nexperience in that respect. The main question arises here - What kind of tax\nrates should be implemented to be able to perform both functions of the fiscal\nand stimulating one. Liberal method of revenue seems an attractive one for\ntaxpayers. According to the economic situation in Georgia, the best solution is\nto use the liberal method. This will help business to develop and people to\nfind jobs. Taxation system will also benefit from that. Tax rate in Georgia\namounts to 15% and is the same for everyone, regardless the size of the\nbusiness. The taxation system is regarded to be proportional. As for the\nAmerican and European countries, taxes there are progressive. I think the same\npractice should be implemented in Georgia, and not only in case of taxation.\n"
    },
    {
        "paper_id": 2202.00125,
        "authors": "Bent Flyvbjerg",
        "title": "Top Ten Behavioral Biases in Project Management: An Overview",
        "comments": null,
        "journal-ref": "Project Management Journal 52(6), 2021",
        "doi": "10.1177/87569728211049046",
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Behavioral science has witnessed an explosion in the number of biases\nidentified by behavioral scientists, to more than 200 at present. This article\nidentifies the 10 most important behavioral biases for project management.\nFirst, we argue it is a mistake to equate behavioral bias with cognitive bias,\nas is common. Cognitive bias is half the story; political bias the other half.\nSecond, we list the top 10 behavioral biases in project management: (1)\nstrategic misrepresentation, (2) optimism bias, (3) uniqueness bias, (4) the\nplanning fallacy, (5) overconfidence bias, (6) hindsight bias, (7) availability\nbias, (8) the base rate fallacy, (9) anchoring, and (10) escalation of\ncommitment. Each bias is defined, and its impacts on project management are\nexplained, with examples. Third, base rate neglect is identified as a primary\nreason that projects underperform. This is supported by presentation of the\nmost comprehensive set of base rates that exist in project management\nscholarship, from 2,062 projects. Finally, recent findings of power law\noutcomes in project performance are identified as a possible first stage in\ndiscovering a general theory of project management, with more fundamental and\nmore scientific explanations of project outcomes than found in conventional\ntheory.\n"
    },
    {
        "paper_id": 2202.00127,
        "authors": "Wolfgang Kuhle",
        "title": "On Market Design and Latency Arbitrage",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  We argue that contemporary stock market designs are, due to traders'\ninability to fully express their preferences over the execution times of their\norders, prone to latency arbitrage. In turn, we propose a new order type which\nallows traders to specify the time at which their orders are executed after\nreaching the exchange. Using this order type, traders can synchronize order\nexecutions across different exchanges, such that high-frequency traders, even\nif they operate at the speed of light, can no-longer engage in latency\narbitrage.\n"
    },
    {
        "paper_id": 2202.00297,
        "authors": "Anton J. Heckens and Thomas Guhr",
        "title": "New Collectivity Measures for Financial Covariances and Correlations",
        "comments": "32 pages, 21 figures, 3 tables",
        "journal-ref": "Physica A: Statistical Mechanics and its Applications 604, 127704\n  (2022)",
        "doi": "10.1016/j.physa.2022.127704",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Complex systems are usually non-stationary and their dynamics is often\ndominated by collective effects. Collectivity, defined as coherent motion of\nthe whole system or of some of its parts, manifests itself in the\ntime-dependent structures of covariance and correlation matrices. The largest\neigenvalue corresponds to the collective motion of the system as a whole, while\nthe other large, isolated, eigenvalues indicate collectivity in parts of the\nsystem. In the case of finance, these are industrial sectors. By removing the\ncollective motion of the system as a whole, the latter effects are much better\nrevealed. We measure a remaining collectivity to which we refer as average\nsector collectivity. We identify collective signals around the Lehman Brothers\ncrash and after the dot-com bubble burst. For the Lehman Brothers crash, we\nfind a potential precursor. We analyze 213 US stocks over a period of more than\n30 years from 1990 to 2021. We plot the average sector collectivity versus the\ncollectivity corresponding to the largest eigenvalue to study the whole market\ntrajectory in a two dimensional space spanned by both collectivities.\nTherefore, we capture the average sector collectivity in a much more precise\nway. Additionally, we observe that larger values in the average sector\ncollectivity are often accompanied by trend shifts in the mean covariances and\nmean correlations. As of 2015/2016 the collectivity in the US stock markets\nchanged fundamentally.\n"
    },
    {
        "paper_id": 2202.00409,
        "authors": "Matthew Smith and Yasaman Sarabi",
        "title": "Mapping intra firm trade in the automotive sector: a network approach",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Intra-firm trade describes the trade between affiliated firms and is\nincreasingly important as global production is fragmented. However, statistics\nand data on global intra-firm trade patterns are widely unavailable. This study\nproposes a novel multilevel approach combining firm and country level data to\nconstruct a set of country intra-firm trade networks for various segments of\nthe automotive production chain. A multilevel network is constructed with a\nnetwork of international trade at the macro level, a firm ownership network at\nthe micro level and a firm-country affiliation network linking the two, at the\nmeso level. A motif detection approach is used to filter these networks to\nextract potential intra-firm trade ties between countries, where the motif (or\nsubstructure) is two countries linked by trade, each affiliated with a firm,\nand these two firms linked by ownership. The motif detection is used to extract\npotential country level intra-firm trade ties. An Exponential Random Graph\nModel (ERGM) is applied to the country level intra-firm trade networks, one for\neach segment of the automotive production chain, to inform on the determinants\nof intra-firm trade at the country level.\n"
    },
    {
        "paper_id": 2202.00556,
        "authors": "Denis S. Gusev, Elena G. Demidova, Olga A. Novikova (Starooskolsky\n  Technological Institute)",
        "title": "Building a Dynamic System of Advanced Risk Management and Risk\n  Assessment of the Company",
        "comments": "10 pages, in Russian, 2 tab., 1 picture. Keywords: risks,\n  identification,quantification. Economic Science and practice in the\n  conditions of instability of the external environment and the expansion of\n  digital globalization: International Scientific and Practical Conference ESPE\n  2021, May 20-21, 2021, Moscow, Russia",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  The purpose of the research presented in this article is to develop a dynamic\nsystem for forecasting and minimizing the risks of an industrial company based\non their quantitative assessment. The article considers the conceptual\napparatus of the essential content of risk management of an industrial\nenterprise, reviews the theoretical aspects of risk management systems and the\nmost significant risk management methods from a practical point of view. The\nmethodological apparatus of qualitative and quantitative analysis and risk\nassessment has been expanded on the basis of some conditionality of risk\nclassification features identified and a systematic approach to the\nclassification of risks of an industrial enterprise has been proposed, taking\ninto account the dynamics of their impact on the object, the stages of building\na dynamic risk management system are given. The article substantiates the need\nto supplement the dynamic risk management system of industrial enterprises with\nmethods of qualitative and quantitative risk assessment in order to form\neffective risk management strategies\n"
    },
    {
        "paper_id": 2202.00631,
        "authors": "Sohom Ghosh, Sudip Kumar Naskar",
        "title": "FiNCAT: Financial Numeral Claim Analysis Tool",
        "comments": "3 pages, 2 figures, 1 table",
        "journal-ref": null,
        "doi": "10.1145/3487553.3524635",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  While making investment decisions by reading financial documents, investors\nneed to differentiate between in-claim and outof-claim numerals. In this paper,\nwe present a tool which does it automatically. It extracts context embeddings\nof the numerals using one of the transformer based pre-trained language model\ncalled BERT. After this, it uses a Logistic Regression based model to detect\nwhether the numerals is in-claim or out-of-claim. We use FinNum-3 (English)\ndataset to train our model. After conducting rigorous experiments we achieve a\nMacro F1 score of 0.8223 on the validation set. We have open-sourced this tool\nand it can be accessed from\nhttps://github.com/sohomghosh/FiNCAT_Financial_Numeral_Claim_Analysis_Tool\n"
    },
    {
        "paper_id": 2202.00662,
        "authors": "Yichen Feng, Jean-Pierre Fouque, Ruimeng Hu, Tomoyuki Ichiba",
        "title": "Systemic Risk Models for Disjoint and Overlapping Groups with\n  Equilibrium Strategies",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  We analyze the systemic risk for disjoint and overlapping groups (e.g.,\ncentral clearing counterparties (CCP)) by proposing new models with realistic\ngame features. Specifically, we generalize the systemic risk measure proposed\nin [F. Biagini, J.-P. Fouque, M. Frittelli, and T. Meyer-Brandis, Finance and\nStochastics, 24(2020), 513--564] by allowing individual banks to choose their\npreferred groups instead of being assigned to certain groups. We introduce the\nconcept of Nash equilibrium for these new models, and analyze the optimal\nsolution under Gaussian distribution of the risk factor. We also provide an\nexplicit solution for the risk allocation of the individual banks, and study\nthe existence and uniqueness of Nash equilibrium both theoretically and\nnumerically. The developed numerical algorithm can simulate scenarios of\nequilibrium, and we apply it to study the bank-CCP structure with real data and\nshow the validity of the proposed model.\n"
    },
    {
        "paper_id": 2202.00713,
        "authors": "Robert Moffitt, John Abowd, Christopher Bollinger, Michael Carr,\n  Charles Hokayem, Kevin McKinney, Emily Wiemers, Sisi Zhang and James Ziliak",
        "title": "Reconciling Trends in U.S. Male Earnings Volatility: Results from Survey\n  and Administrative Data",
        "comments": "Version submitted to JBES in January of 2022",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  There is a large literature on earnings and income volatility in labor\neconomics, household finance, and macroeconomics. One strand of that literature\nhas studied whether individual earnings volatility has risen or fallen in the\nU.S. over the last several decades. There are strong disagreements in the\nempirical literature on this important question, with some studies showing\nupward trends, some showing downward trends, and some showing no trends. Some\nstudies have suggested that the differences are the result of using flawed\nsurvey data instead of more accurate administrative data. This paper summarizes\nthe results of a project attempting to reconcile these findings with four\ndifferent data sets and six different data series--three survey and three\nadministrative data series, including two which match survey respondent data to\ntheir administrative data. Using common specifications, measures of volatility,\nand other treatments of the data, four of the six data series show a lack of\nany significant long-term trend in male earnings volatility over the last\n20-to-30+ years when differences across the data sets are properly accounted\nfor. A fifth data series (the PSID) shows a positive net trend but small in\nmagnitude. A sixth, administrative, data set, available only since 1998, shows\nno net trend 1998-2011 and only a small decline thereafter. Many of the\nremaining differences across data series can be explained by differences in\ntheir cross-sectional distribution of earnings, particularly differences in the\nsize of the lower tail. We conclude that the data sets we have analyzed, which\ninclude many of the most important available, show little evidence of any\nsignificant trend in male earnings volatility since the mid-1980s.\n"
    },
    {
        "paper_id": 2202.00785,
        "authors": "Yen Thuan Trinh, Bernard Hanzon",
        "title": "Option Pricing and CVA Calculations using the Monte Carlo-Tree (MC-Tree)\n  Method",
        "comments": "34 pages, 4 figures, 11 tables. This paper is submitted",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  The binomial tree method and the Monte Carlo (MC) method are popular methods\nfor solving option pricing problems. However in both methods there is a\ntrade-off between accuracy and speed of computation, both of which are\nimportant in applications. We introduce a new method, the MC-Tree method, that\ncombines the MC method with the binomial tree method. It employs a mixing\ndistribution on the tree parameters, which are restricted to give prescribed\nmean and variance. For the family of mixing densities proposed here, the\ncorresponding compound densities of the tree outcomes at final time are\nobtained. Ideally the compound density would be (after a logarithmic\ntransformation of the asset prices) Gaussian. Using the fact that in general,\nwhen mean and variance are prescribed, the maximum entropy distribution is\nGaussian, we look for mixing densities for which the corresponding compound\ndensity has high entropy level. The compound densities that we obtain are not\nexactly Gaussian, but have entropy values close to the maximum possible\nGaussian entropy. Furthermore we introduce techniques to correct for the\ndeviation from the ideal Gaussian pricing measure. One of these (distribution\ncorrection technique) ensures that expectations calculated with the method are\ntaken with respect to the desired Gaussian measure. The other one\n(bias-correction technique) ensures that the probability distributions used are\nrisk-neutral in each of the trees. Apart from option pricing, we apply our\ntechniques to develop an algorithm for calculation of the Credit Valuation\nAdjustment (CVA) to the price of an American option. Numerical examples of the\nworkings of the MC-Tree approach are provided, which show good performance in\nterms of accuracy and computational speed.\n"
    },
    {
        "paper_id": 2202.00831,
        "authors": "Takanobu Mizuta, Isao Yagi and Kosei Takashima",
        "title": "Instability of financial markets by optimizing investment strategies\n  investigated by an agent-based model",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Most finance studies are discussed on the basis of several hypotheses, for\nexample, investors rationally optimize their investment strategies. However,\nthe hypotheses themselves are sometimes criticized. Market impacts, where\ntrades of investors can impact and change market prices, making optimization\nimpossible. In this study, we built an artificial market model by adding\ntechnical analysis strategy agents searching one optimized parameter to a whole\nsimulation run to the prior model and investigated whether investors' inability\nto accurately estimate market impacts in their optimizations leads to\noptimization instability. In our results, the parameter of investment strategy\nnever converged to a specific value but continued to change. This means that\neven if all other traders are fixed, only one investor will use backtesting to\noptimize his/her strategy, which leads to the time evolution of market prices\nbecoming unstable. Optimization instability is one level higher than\n\"non-equilibrium of market prices.\" Therefore, the time evolution of market\nprices produced by investment strategies having such unstable parameters is\nhighly unlikely to be predicted and have stable laws written by equations. This\nnature makes us suspect that financial markets include the principle of natural\nuniformity and indicates the difficulty of building an equation model\nexplaining the time evolution of prices.\n"
    },
    {
        "paper_id": 2202.00839,
        "authors": "Dami\\'an Vergara",
        "title": "Minimum Wages and Optimal Redistribution",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper analyzes whether a minimum wage should be used for redistribution\non top of taxes and transfers. I characterize optimal redistribution for a\ngovernment with three policy instruments -- labor income taxes and transfers,\ncorporate income taxes, and a minimum wage -- using an empirically grounded\nmodel of the labor market with positive firm profits. A minimum wage can\nincrease social welfare when it increases the average post-tax wages of\nlow-skill labor market participants and when corporate profit incidence is\nlarge. When chosen together with taxes, the minimum wage can help the\ngovernment redistribute efficiently to low-skill workers by preventing firms\nfrom capturing low-wage income subsidies such as the EITC and from enjoying\nhigh profits that cannot be redistributed via corporate taxes due to capital\nmobility in unaffected industries. Event studies show that the average US\nstate-level minimum wage reform over the last two decades increased average\npost-tax wages of low-skilled labor market participants and reduced corporate\nprofits in affected industries, namely low-skill labor-intensive services. A\nsufficient statistics analysis implies that US minimum wages typically remain\nbelow their optimum under the current tax and transfer system.\n"
    },
    {
        "paper_id": 2202.00871,
        "authors": "Jose Blanchet and Fernando Hernandez and Viet Anh Nguyen and Markus\n  Pelger and Xuhui Zhang",
        "title": "Bayesian Imputation with Optimal Look-Ahead-Bias and Variance Tradeoff",
        "comments": "This work merges and supersedes arXiv:2102.12736",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Missing time-series data is a prevalent problem in many prescriptive\nanalytics models in operations management, healthcare and finance. Imputation\nmethods for time-series data are usually applied to the full panel data with\nthe purpose of training a prescriptive model for a downstream out-of-sample\ntask. For example, the imputation of missing asset returns may be applied\nbefore estimating an optimal portfolio allocation. However, this practice can\nresult in a look-ahead-bias in the future performance of the downstream task,\nand there is an inherent trade-off between the look-ahead-bias of using the\nentire data set for imputation and the larger variance of using only the\ntraining portion of the data set for imputation. By connecting layers of\ninformation revealed in time, we propose a Bayesian consensus posterior that\nfuses an arbitrary number of posteriors to optimize the variance and\nlook-ahead-bias trade-off in the imputation. We derive tractable two-step\noptimization procedures for finding the optimal consensus posterior, with\nKullback-Leibler divergence and Wasserstein distance as the dissimilarity\nmeasure between posterior distributions. We demonstrate in simulations and in\nan empirical study the benefit of our imputation mechanism for portfolio\nallocation with missing returns.\n"
    },
    {
        "paper_id": 2202.00877,
        "authors": "B. Cooper Boniece, Jos\\'e E. Figueroa-L\\'opez, and Yuchen Han",
        "title": "Efficient Volatility Estimation for L\\'evy Processes with Jumps of\n  Unbounded Variation",
        "comments": "40 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  Statistical inference for stochastic processes based on high-frequency\nobservations has been an active research area for more than a decade. One of\nthe most well-known and widely studied problems is that of estimation of the\nquadratic variation of the continuous component of an It\\^o semimartingale with\njumps. Several rate- and variance-efficient estimators have been proposed in\nthe literature when the jump component is of bounded variation. However, to\ndate, very few methods can deal with jumps of unbounded variation. By\ndeveloping new high-order expansions of the truncated moments of a L\\'evy\nprocess, we construct a new rate- and variance-efficient estimator for a class\nof L\\'evy processes of unbounded variation, whose small jumps behave like those\nof a stable L\\'evy process with Blumenthal-Getoor index less than $8/5$. The\nproposed method is based on a two-step debiasing procedure for the truncated\nrealized quadratic variation of the process. Our Monte Carlo experiments\nindicate that the method outperforms other efficient alternatives in the\nliterature in the setting covered by our theoretical framework.\n"
    },
    {
        "paper_id": 2202.00917,
        "authors": "Thitithep Sitthiyot, Kanyarat Holasut",
        "title": "A quantitative method for benchmarking fair income distribution",
        "comments": "version 1, 56 pages, 7 figures, 6 tables",
        "journal-ref": "Heliyon (2022)",
        "doi": "10.1016/j.heliyon.2022.e10511",
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Concern about income inequality has become prominent in public discourse\naround the world. However, studies in behavioral economics and psychology have\nconsistently shown that people prefer not equal but fair income distributions.\nThus, finding a benchmark that could be used to measure fair income\ndistribution across countries is a theoretical and practical challenge. Here a\nmethod for benchmarking fair income distribution is introduced. The benchmark\nis constructed based on the concepts of procedural justice, distributive\njustice, and authority's power in professional sports where it is widely agreed\nas an international norm that the allocations of athlete's salary are outcomes\nof fair rules, individual and/or team performance, and luck in line with\nno-envy principle of fair allocation. Using the World Bank data, this study\ndemonstrates how the benchmark could be used to quantitatively gauge whether,\nfor a given value of the Gini index, the income shares by quintile of a country\nare the fair shares or not, and if not, what fair income shares by quintile of\nthat country should be. Knowing this could be useful for those involved in\nsetting targets for the Gini index and the fair income shares that are\nappropriate for the context of each country before formulating policies toward\nachieving the Sustainable Development Goal 10 and other SDGs.\n"
    },
    {
        "paper_id": 2202.00929,
        "authors": "Claudio Fontana, Zorana Grbac and Thorsten Schmidt",
        "title": "Term structure modelling with overnight rates beyond stochastic\n  continuity",
        "comments": "27 pages, 2 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Overnight rates, such as the SOFR (Secured Overnight Financing Rate) in the\nUS, are central to the current reform of interest rate benchmarks. A striking\nfeature of overnight rates is the presence of jumps and spikes occurring at\npredetermined dates due to monetary policy interventions and liquidity\nconstraints. This corresponds to stochastic discontinuities (i.e.,\ndiscontinuities occurring at ex-ante known points in time) in their dynamics.\nIn this work, we propose a term structure modelling framework based on\novernight rates and characterize absence of arbitrage in a generalised\nHeath-Jarrow-Morton (HJM) setting. We extend the classical short-rate approach\nto accommodate stochastic discontinuities, developing a tractable setup driven\nby affine semimartingales. In this context, we show that simple specifications\nallow to capture stylized facts of the jump behavior of overnight rates. In a\nGaussian setting, we provide explicit valuation formulas for bonds and caplets.\nFurthermore, we investigate hedging in the sense of local risk-minimization\nwhen the underlying term structures feature stochastic discontinuities.\n"
    },
    {
        "paper_id": 2202.00941,
        "authors": "Selim Amrouni, Aymeric Moulin, Tucker Balch",
        "title": "CTMSTOU driven markets: simulated environment for regime-awareness in\n  trading policies",
        "comments": "fix typo in title",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Market regimes is a popular topic in quantitative finance even though there\nis little consensus on the details of how they should be defined. They arise as\na feature both in financial market prediction problems and financial market\ntask performing problems.\n  In this work we use discrete event time multi-agent market simulation to\nfreely experiment in a reproducible and understandable environment where\nregimes can be explicitly switched and enforced.\n  We introduce a novel stochastic process to model the fundamental value\nperceived by market participants: Continuous-Time Markov Switching Trending\nOrnstein-Uhlenbeck (CTMSTOU), which facilitates the study of trading policies\nin regime switching markets.\n  We define the notion of regime-awareness for a trading agent as well and\nillustrate its importance through the study of different order placement\nstrategies in the context of order execution problems.\n"
    },
    {
        "paper_id": 2202.01027,
        "authors": "Jori Hoencamp, Shashi Jain, Drona Kandhai",
        "title": "A semi-static replication approach to efficient hedging and pricing of\n  callable IR derivatives",
        "comments": "47 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  We present a semi-static hedging algorithm for callable interest rate\nderivatives under an affine, multi-factor term-structure model. With a\ntraditional dynamic hedge, the replication portfolio needs to be updated\ncontinuously through time as the market moves. In contrast, we propose a\nsemi-static hedge that needs rebalancing on just a finite number of instances.\nWe show, taking as an example Bermudan swaptions, that callable interest rate\nderivatives can be replicated with an options portfolio written on a basket of\ndiscount bonds. The static portfolio composition is obtained by regressing the\ntarget option's value using an interpretable, artificial neural network.\nLeveraging on the approximation power of neural networks, we prove that the\nhedging error can be arbitrarily small for a sufficiently large replication\nportfolio. A direct, a lower bound, and an upper bound estimator for the\nrisk-neutral Bermudan swaption price is inferred from the hedging algorithm.\nAdditionally, closed-form error margins to the price statistics are determined.\nWe practically demonstrate the hedging and pricing performance through several\nnumerical experiments.\n"
    },
    {
        "paper_id": 2202.01043,
        "authors": "Matthew F. Tomlinson, David Greenwood, Marcin Mucha-Kruczynski",
        "title": "2T-POT Hawkes model for left- and right-tail conditional quantile\n  forecasts of financial log-returns: out-of-sample comparison of conditional\n  EVT models",
        "comments": "Main paper: 25 pages, 7 figures, 6 tables. Supplementary material: 12\n  pages, 14 figures",
        "journal-ref": "Int. J. Forecast., 40(1), 324-347 (2024)",
        "doi": "10.1016/j.ijforecast.2023.03.003",
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Conditional extreme value theory (EVT) methods promise enhanced forecasting\nof the extreme tail events that often dominate systemic risk. We present an\nimproved two-tailed peaks-over-threshold (2T-POT) Hawkes model that is adapted\nfor conditional quantile forecasting in both the left and right tails of a\nunivariate time series. This is applied to the daily log-returns of six large\ncap indices. We also take the unique step of fitting the model at multiple\nexceedance thresholds (from the 1.25% to 25.00% mirrored quantiles).\nQuantitatively similar asymmetries in Hawkes parameters are found across all\nsix indices, adding further empirical support to a temporal leverage effect in\nfinancial price time series in which the impact of losses is not only larger\nbut also more immediate. Out-of-sample backtests find that our 2T-POT Hawkes\nmodel is more reliably accurate than the GARCH-EVT model when forecasting\n(mirrored) value-at-risk and expected shortfall at the 5% coverage level and\nbelow. This suggests that asymmetric Hawkes-type arrival dynamics are a better\napproximation of the true data generating process for extreme daily log-returns\nthan GARCH-type conditional volatility; our 2T-POT Hawkes model therefore\npresents a better performing alternative for financial risk modelling.\n"
    },
    {
        "paper_id": 2202.0108,
        "authors": "Riccardo Di Clemente, Bal\\'azs Lengyel, Lars F. Andersson and Rikard\n  Eriksson",
        "title": "Understanding European Integration with Bipartite Networks of\n  Comparative Advantage",
        "comments": "17 pages, 4 figures, 6 tables",
        "journal-ref": "PNAS Nexus, Volume 1, Issue 5, November 2022, pgac262",
        "doi": "10.1093/pnasnexus/pgac262",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Core objectives of European common market integration are convergence and\neconomic growth, but these are hampered by redundancy, and value chain\nasymmetries. The challenge is how to harmonize labor division to reach global\ncompetitiveness, meanwhile bridging productivity differences across the EU. We\ndevelop a bipartite network approach to trace pairwise co-specialization, by\napplying the Revealed Comparative Advantage method, within and between EU15 and\nCentral and Eastern European (CEE). This approach assesses redundancies and\ndivision of labor in the EU at the level of industries and countries. We find\nsignificant co-specialization among CEE countries but a diverging\nspecialization between EU15 and CEE. Productivity increases in those CEE\nindustries that have co-specialized with other CEE countries after EU\naccession, while co-specialization across CEE and EU15 countries is less\nrelated to productivity growth. These results show that a division of sectoral\nspecialization can lead to productivity convergence between EU15 and CEE\ncountries.\n"
    },
    {
        "paper_id": 2202.01423,
        "authors": "Takanobu Mizuta",
        "title": "Do new investment strategies take existing strategies' returns -- An\n  investigation into agent-based models",
        "comments": "The 8th International Conference on Behavioral, Economic, and\n  Socio-Cultural Computing (BESC 2021). arXiv admin note: substantial text\n  overlap with arXiv:2202.00831",
        "journal-ref": null,
        "doi": "10.1109/BESC53957.2021.9635097",
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Commodity trading advisors (CTAs), who mainly trade commodity futures, showed\ngood returns in the 2000s. However, since the 2010's, they have not performed\nvery well. One possible reason of this phenomenon is the emergence of\nshort-term reversal traders (STRTs) who prey on CTAs for profit. In this study,\nI built an artificial market model by adding a CTA agent (CTAA) and STRT agent\n(STRTA) to a prior model and investigated whether emerging STRTAs led to a\ndecrease in CTAA revenue to determine whether STRTs prey on CTAs for profit. To\nthe contrary, my results showed that a CTAA and STRTA are more likely to trade\nand earn more when both exist. Therefore, it is possible that they have a\nmutually beneficial relationship.\n"
    },
    {
        "paper_id": 2202.0172,
        "authors": "Juan Ignacio Pe\\~na, and Rosa Rodriguez",
        "title": "Are EU Climate and Energy Package 20-20-20 targets achievable and\n  compatible? Evidence from the impact of renewables on electricity prices",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper studies the realizability and compatibility of the three CEP2020\ntargets, focusing on electricity prices. We study the impact of renewables and\nother fundamental determinants on wholesale and household retail electricity\nprices in ten EU countries from 2008 to 2016. Increases in production from\nrenewables decrease wholesale electricity prices in all countries. As decreases\nin prices should promote consumption, an apparent contradiction emerges between\nthe target of an increase in renewables and the target of a reduction in\nconsumption. However, the impact of renewables on the non-energy part of\nhousehold wholesale electricity prices is positive in six countries. Therefore,\ndecreases in wholesale prices, that may compromise the CEP2020 target of\ndecrease in consumption, do not necessarily translate into lower household\nretail prices.\n"
    },
    {
        "paper_id": 2202.01732,
        "authors": "Juan Ignacio Pe\\~na, Rosa Rodriguez, and Silvia Mayoral",
        "title": "Tail Risk of Electricity Futures",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper compares the in-sample and out-of-sample performance of several\nmodels for computing the tail risk of one-month and one-year electricity\nfutures contracts traded in the NordPool, French, German, and Spanish markets\nin 2008-2017. As measures of tail risk, we use the one-day-ahead Value-at-Risk\n(VaR) and the Expected Shortfall (ES). With VaR, the AR (1)-GARCH (1,1) model\nwith Student-t distribution is the best-performing specification with 88% cases\nin which the Fisher test accepts the model, with a success rate of 94% in the\nleft tail and of 81% in the right tail. The model passes the test of model\nadequacy in the 100% of the cases in the NordPool and German markets, but only\nin the 88% and 63% of the cases in the Spanish and French markets. With ES,\nthis model passes the test of model adequacy in 100% of cases in all markets.\nHistorical Simulation and Quantile Regression-based approaches misestimate tail\nrisks. The right-hand tail of the returns is more difficult to model than the\nleft-hand tail and therefore financial regulators and the administrators of\nfutures markets should take these results into account when setting additional\nregulatory capital requirements and margin account regulations to short\npositions.\n"
    },
    {
        "paper_id": 2202.01737,
        "authors": "Juan Ignacio Pe\\~na and Rosa Rodriguez",
        "title": "Time-zero Efficiency of European Power Derivatives Markets",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We study time-zero efficiency of electricity derivatives markets. By\ntime-zero efficiency is meant a sequence of prices of derivatives contracts\nhaving the same underlying asset but different times to maturity which implies\nthat prices comply with a set of efficiency conditions that prevent profitable\ntime-zero arbitrage opportunities. We investigate whether statistical tests,\nbased on the law of one price, and trading rules, based on price differentials\nand no-arbitrage violations, are useful for assessing time-zero efficiency. We\napply tests and trading rules to daily data of three European power markets:\nGermany, France and Spain. In the case of the German market, after considering\nliquidity availability and transaction costs, results are not inconsistent with\ntime-zero efficiency. However, in the case of the French and Spanish markets,\nlimitations in liquidity and representativeness are challenges that prevent\ndefinite conclusions. Liquidity in French and Spanish markets should improve by\nusing pricing and marketing incentives. These incentives should attract more\nparticipants into the electricity derivatives exchanges and should encourage\nthem to settle OTC trades in clearinghouses. Publication of statistics on\nprices, volumes and open interest per type of participant should be promoted.\n"
    },
    {
        "paper_id": 2202.01743,
        "authors": "Juan Ignacio Pe\\~na and Rosa Rodriguez",
        "title": "Default Supply Auctions in Electricity Markets: Challenges and Proposals",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper studies premiums got by winning bidders in default supply\nauctions, and speculation and hedging activities in power derivatives markets\nin dates near auctions. Data includes fifty-six auction prices from 2007 to\n2013, those of CESUR in the Spanish OMEL electricity market, and those of Basic\nGeneration Service auctions (PJM-BGS) in New Jersey's PJM market. Winning\nbidders got an average ex-post yearly forward premium of 7% (CESUR) and 38%\n(PJM-BGS). The premium using an index of futures prices is 1.08% (CESUR) and\n24% (PJM-BGS). Ex-post forward premium is negatively related to the number of\nbidders and spot price volatility. In CESUR, hedging-driven trading in power\nderivatives markets predominates around auction dates, but in PJM-BGS,\nspeculation-driven trading prevails.\n"
    },
    {
        "paper_id": 2202.01804,
        "authors": "Dario Laudati, Manuel S. Mariani, Luciano Pietronero, Andrea Zaccaria",
        "title": "The different structure of economic ecosystems at the scales of\n  companies and countries",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  A key element to understand complex systems is the relationship between the\nspatial scale of investigation and the structure of the interrelation among its\nelements. When it comes to economic systems, it is now well-known that the\ncountry-product bipartite network exhibits a nested structure, which is the\nfoundation of different algorithms that have been used to scientifically\ninvestigate countries' development and forecast national economic growth.\nChanging the subject from countries to companies, a significantly different\nscenario emerges. Through the analysis of a unique dataset of Italian firms'\nexports and a worldwide dataset comprising countries' exports, here we find\nthat, while a globally nested structure is observed at the country level, a\nlocal, in-block nested structure emerges at the level of firms. Remarkably,\nthis in-block nestedness is statistically significant with respect to suitable\nnull models and the algorithmic partitions of products into blocks have a high\ncorrespondence with exogenous product classifications. These findings lay a\nsolid foundation for developing a scientific approach based on the physics of\ncomplex systems to the analysis of companies, which has been lacking until now.\n"
    },
    {
        "paper_id": 2202.01844,
        "authors": "Martin Gonzalez-Rozada and Hernan Ruffo",
        "title": "The welfare effects of unemployment insurance in Argentina. New\n  estimates using changes in the schedule of transfers",
        "comments": "34 pages, 12 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Unemployment insurance transfers should balance the provision of consumption\nto the unemployed with the disincentive effects on the search behavior.\nDeveloping countries face the additional challenge of informality. Workers can\nchoose to hide their employment state and labor income in informal jobs, an\nadditional form of moral hazard. To provide evidence about the effects of this\npolicy in a country affected by informality we exploit kinks in the schedule of\ntransfers in Argentina. Our results suggest that higher benefits induce\nmoderate behavioral responses in job-finding rates and increase re-employment\nwages. We use a sufficient statistics formula from a model with random wage\noffers and we calibrate it with our estimates. We show that welfare could rise\nsubstantially if benefits were increased in Argentina. Importantly, our\nconclusion is relevant for the median eligible worker that is strongly affected\nby informality.\n"
    },
    {
        "paper_id": 2202.02197,
        "authors": "Carlo Drago and Andrea Scozzari",
        "title": "Evaluating conditional covariance estimates via a new targeting approach\n  and a networks-based analysis",
        "comments": "23 pages, 18 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Modeling and forecasting of dynamically varying covariances have received\nmuch attention in the literature. The two most widely used conditional\ncovariances and correlations models are BEKK and DCC. In this paper, we advance\na new method to introduce targeting in both models to estimate matrices\nassociated with financial time series. Our approach is based on specific groups\nof highly correlated assets in a financial market, and these relationships\nremain unaltered over time. Based on the estimated parameters, we evaluate our\ntargeting method on simulated series by referring to two well-known loss\nfunctions introduced in the literature and Network analysis. We find all the\nmaximal cliques in correlation graphs to evaluate the effectiveness of our\nmethod. Results from an empirical case study are encouraging, mainly when the\nnumber of assets is not large.\n"
    },
    {
        "paper_id": 2202.02254,
        "authors": "Sergio Mayordomo, Maria Rodriguez-Moreno, Juan Ignacio Pe\\~na",
        "title": "Derivatives Holdings and Systemic Risk in the U.S. Banking Sector",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Foreign exchange and credit derivatives increase the bank's contributions to\nsystemic risk. Interest rate derivatives decrease it. The proportion of\nnon-performing loans over total loans and the leverage ratio have stronger\nimpact on systemic risk than derivatives holdings.\n"
    },
    {
        "paper_id": 2202.02263,
        "authors": "Wan-Chien Chiua, Juan Ignacio Pe\\~na, and Chih-Wei Wang",
        "title": "Industry Characteristics and Financial Risk Spillovers",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper proposes a new measure of tail risk spillover. The empirical\napplication provides evidence of significant volatility and tail risk\nspillovers from the financial sector to many real economy sectors in the U.S.\neconomy in the period from 2001 to 2011. These spillovers increase in crisis\nperiods. The conditional coexceedance in a given sector is positively related\nto its amount of debt financing, and negatively related to its relative\nvaluation and investment. Real economy sectors which require substantial\nexternal financing, and whose value and investment activity are relatively\nlower, are prime candidates for depreciation in the wake of crisis in the\nfinancial sector.\n"
    },
    {
        "paper_id": 2202.02268,
        "authors": "Stefan Pasch, Daniel Ehnes",
        "title": "StonkBERT: Can Language Models Predict Medium-Run Stock Price Movements?",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  To answer this question, we fine-tune transformer-based language models,\nincluding BERT, on different sources of company-related text data for a\nclassification task to predict the one-year stock price performance. We use\nthree different types of text data: News articles, blogs, and annual reports.\nThis allows us to analyze to what extent the performance of language models is\ndependent on the type of the underlying document. StonkBERT, our\ntransformer-based stock performance classifier, shows substantial improvement\nin predictive accuracy compared to traditional language models. The highest\nperformance was achieved with news articles as text source. Performance\nsimulations indicate that these improvements in classification accuracy also\ntranslate into above-average stock market returns.\n"
    },
    {
        "paper_id": 2202.02273,
        "authors": "Sergio Mayordomo, Juan Ignacio Pe\\~na, Eduardo S. Schwartz",
        "title": "Are all Credit Default Swap Databases equal?",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We compare the five major sources of corporate Credit Default Swap prices:\nGFI, Fenics, Reuters, CMA, and Markit, using the most liquid single name 5-year\nCDS in the iTraxx and CDX indexes from 2004 to 2010. Deviations from the common\ntrend among prices in the different databases are not random but are explained\nby idiosyncratic factors, financing costs, global risk, and other trading\nfactors. The CMA quotes lead the price discovery process. Moreover, we find\nthat there is not a full agreement among databases in the results of the price\ndiscovery analysis between stock and CDS returns.\n"
    },
    {
        "paper_id": 2202.02276,
        "authors": "Wan-Chien Chiu, Juan Ignacio Pe\\~na, Chih-Wei Wang",
        "title": "Measuring Systemic Risk: Common Factor Exposures and Tail Dependence\n  Effects",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We model systemic risk using a common factor that accounts for market-wide\nshocks and a tail dependence factor that accounts for linkages among extreme\nstock returns. Specifically, our theoretical model allows for firm-specific\nimpacts of infrequent and extreme events. Using data on the four sectors of the\nU.S. financial industry from 1996 to 2011, we uncover two key empirical\nfindings. First, disregarding the effect of the tail dependence factor leads to\na downward bias in the measurement of systemic risk, especially during weak\neconomic times. Second, when these measures serve as leading indicators of the\nSt. Louis Fed Financial Stress Index, measures that include a tail dependence\nfactor offer better forecasting ability than measures based on a common factor\nonly.\n"
    },
    {
        "paper_id": 2202.0228,
        "authors": "Sergio Mayordomo, Mar\\'ia Rodriguez-Moreno, Juan Ignacio Pe\\~na",
        "title": "Portfolio Choice with Indivisible and Illiquid Housing Assets: The Case\n  of Spain",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper studies the investment decision of the Spanish households using a\nunique data set, the Spanish Survey of Household Finance (EFF). We propose a\ntheoretical model in which households, given a fixed investment in housing,\nallocate their net wealth across bank time deposits, stocks, and mortgage.\nBesides considering housing as an indivisible and illiquid asset that restricts\nthe portfolio choice decision, we take into account the financial constraints\nthat households face when they apply for external funding. For every\nrepresentative household in the EFF we solve this theoretical problem and\nobtain the theoretically optimal portfolio that is compared with households'\nactual choices. We find that households significantly underinvest in stocks and\ndeposits while the optimal and actual mortgage investments are alike.\nConsidering the three types of financial assets at once, we find that the\nhouseholds headed by highly financially sophisticated, older, retired, richer,\nand unconstrained persons are the ones investing more efficiently.\n"
    },
    {
        "paper_id": 2202.023,
        "authors": "Chung-Han Hsieh",
        "title": "On Robust Optimal Linear Feedback Stock Trading",
        "comments": "Submitted for possible publication",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  The take-off point for this paper is the Simultaneous Long-Short (SLS)\ncontrol class, which is known to guarantee the so-called robust positive\nexpectation (RPE) property. That is, the expected cumulative trading gain-loss\nfunction is guaranteed to be positive for a broad class of stock price\nprocesses. This fact attracts many new extensions and ramifications to the SLS\ntheory. However, it is arguable that \"systematic\" way to select an optimal\ndecision variable that is robust in the RPE sense is still unresolved. To this\nend, we propose a modified SLS control structure, which we call the {double\nlinear feedback control scheme}, that allows us to solve the issue above for\nstock price processes involving independent returns. In this paper, we go\nbeyond the existing literature by not only deriving explicit expressions for\nthe expected value and variance of cumulative gain-loss function but also\nproving various theoretical results regarding {robust positive expected growth}\nand {monotonicity}. Subsequently, we propose a new {robust optimal gain\nselection problem} that seeks a solution maximizing the expected trading\ngain-loss subject to the prespecified standard deviation {and} RPE constraints.\nUnder some mild conditions, we show that the optimal solution exists and is\nunique. Moreover, a simple graphical approach that allows one to systematically\ndetermine the optimal solution is also proposed. Finally, some numerical and\nempirical studies using historical price data are also provided to support our\ntheory.\n"
    },
    {
        "paper_id": 2202.02367,
        "authors": "Angela Acocella, Chris Caplice, Yossi Sheffi",
        "title": "The end of 'set it and forget it' pricing? Opportunities for\n  market-based freight contracts",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  In the for-hire truckload market, firms often experience unexpected\ntransportation cost increases due to contracted transportation service provider\n(carrier) load rejections. The dominant procurement strategy results in\nlong-term, fixed-price contracts that become obsolete as transportation\nproviders' networks change and freight markets fluctuate between times of over\nand under supply. We build behavioral models of the contracted carrier's load\nacceptance decision under two distinct freight market conditions based on\nempirical load transaction data. With the results, we quantify carriers'\nlikelihood of sticking to the contract as their best known alternative priced\nload options increase and become more attractive; in other words, carriers'\ncontract price stickiness. Finally, we explore carriers' contract price\nstickiness for different lane, freight, and carrier segments and offer insights\nfor shippers to identify where they can expect to see substantial improvement\nin contracted carrier load acceptance as they consider alternative,\nmarket-based pricing strategies.\n"
    },
    {
        "paper_id": 2202.02488,
        "authors": "Hasanjan Sayit",
        "title": "A discussion of stochastic dominance and mean-risk optimal portfolio\n  problems based on mean-variance-mixture models",
        "comments": "The paper contains some errors and need major revision, I will\n  resubmit after major revise the paper",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  The classical Markowitz mean-variance model uses variance as a risk measure\nand calculates frontier portfolios in closed form by using standard\noptimization techniques. For general mean-risk models such closed form optimal\nportfolios are difficult to obtain. In this note, we obtain closed form\nexpression for frontier portfolios under mean-risk criteria when risk is\nmodelled by any finite law-invariant convex measures of risk and when return\nvectors follow the class of normal mean-variance mixture (NMVM) distributions.\nTo achieve this goal, we first present necessary as well as sufficient\nconditions for stochastic dominance within the class of one dimensional NMVM\nmodels and then we apply them to portfolio optimization problems. Our main\nresult in this paper states that when return vectors follow the class of NMVM\ndistributions the associated mean-risk frontier portfolios can be obtained by\noptimizing a Markowitz mean-variance model with an appropriately adjusted\nreturn vector.\n"
    },
    {
        "paper_id": 2202.02579,
        "authors": "William A. Masters, Amelia B. Finaret, Steven A. Block",
        "title": "The economics of malnutrition: Dietary transition and food system\n  transformation",
        "comments": "Preprint of Chapter 6 in the Handbook of Agricultural Economics, vol.\n  6, edited by C.B. Barrett and D.R. Just (Amsterdam: Elsevier, forthcoming\n  2022). Last revised January 31, 2022. 97 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Rapid increases in food supplies have reduced global hunger, while rising\nburdens of diet-related disease have made poor diet quality the leading cause\nof death and disability around the world. Today's \"double burden\" of\nundernourishment in utero and early childhood then undesired weight gain and\nobesity later in life is accompanied by a third less visible burden of\nmicronutrient imbalances. The triple burden of undernutrition, obesity, and\nunbalanced micronutrients that underlies many diet-related diseases such as\ndiabetes, hypertension and other cardiometabolic disorders often coexist in the\nsame person, household and community. All kinds of deprivation are closely\nlinked to food insecurity and poverty, but income growth does not always\nimprove diet quality in part because consumers cannot directly or immediately\nobserve the health consequences of their food options, especially for newly\nintroduced or reformulated items. Even after direct experience and\nepidemiological evidence reveals relative risks of dietary patterns and\nnutritional exposures, many consumers may not consume a healthy diet because\nfood choice is driven by other factors. This chapter reviews the evidence on\ndietary transition and food system transformation during economic development,\ndrawing implications for how research and practice in agricultural economics\ncan improve nutritional outcomes.\n"
    },
    {
        "paper_id": 2202.02723,
        "authors": "Jaydip Sen, Saikat Mondal, Sidra Mehtab",
        "title": "Portfolio Optimization on NIFTY Thematic Sector Stocks Using an LSTM\n  Model",
        "comments": "The is the preprint version of our published paper listed in the IEEE\n  Xplore. The final paper is published in the Proceedings of the IEEE\n  International Conference on Data Analytics for Business and Industry, pp.\n  364-369, Bahrain, October 25-26, 2021. The preprint consists of 6 pages and\n  it contains 10 figures and 16 tables",
        "journal-ref": null,
        "doi": "10.1109/ICDABI53623.2021.9655886",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Portfolio optimization has been a broad and intense area of interest for\nquantitative and statistical finance researchers and financial analysts. It is\na challenging task to design a portfolio of stocks to arrive at the optimized\nvalues of the return and risk. This paper presents an algorithmic approach for\ndesigning optimum risk and eigen portfolios for five thematic sectors of the\nNSE of India. The prices of the stocks are extracted from the web from Jan 1,\n2016, to Dec 31, 2020. Optimum risk and eigen portfolios for each sector are\ndesigned based on ten critical stocks from the sector. An LSTM model is\ndesigned for predicting future stock prices. Seven months after the portfolios\nwere formed, on Aug 3, 2021, the actual returns of the portfolios are compared\nwith the LSTM-predicted returns. The predicted and the actual returns indicate\na very high-level accuracy of the LSTM model.\n"
    },
    {
        "paper_id": 2202.02728,
        "authors": "Jaydip Sen, Sidra Mehtab, Abhishek Dutta, Saikat Mondal",
        "title": "Hierarchical Risk Parity and Minimum Variance Portfolio Design on NIFTY\n  50 Stocks",
        "comments": "The is the preprint version of our published paper listed in the IEEE\n  Xplore. The final paper is published in the Proceedings of the IEEE\n  International Conference on Decision Aid Sciences and Applications, pp.\n  668-675, December 7-8, 2021, Bahrain. The preprint consists of 8 pages and it\n  contains 32 figures and 9 tables",
        "journal-ref": null,
        "doi": "10.1109/DASA53625.2021.9681925",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Portfolio design and optimization have been always an area of research that\nhas attracted a lot of attention from researchers from the finance domain.\nDesigning an optimum portfolio is a complex task since it involves accurate\nforecasting of future stock returns and risks and making a suitable tradeoff\nbetween them. This paper proposes a systematic approach to designing portfolios\nusing two algorithms, the critical line algorithm, and the hierarchical risk\nparity algorithm on eight sectors of the Indian stock market. While the\nportfolios are designed using the stock price data from Jan 1, 2016, to Dec 31,\n2020, they are tested on the data from Jan 1, 2021, to Aug 26, 2021. The\nbacktesting results of the portfolios indicate while the performance of the CLA\nalgorithm is superior on the training data, the HRP algorithm has outperformed\nthe CLA algorithm on the test data.\n"
    },
    {
        "paper_id": 2202.02787,
        "authors": "Lorenzo Fant, Onofrio Mazzarisi, Emanuele Panizon, Jacopo Grilli",
        "title": "Stable cooperation emerges in stochastic multiplicative growth",
        "comments": "7 pages, 3 figures",
        "journal-ref": null,
        "doi": "10.1103/PhysRevE.108.L012401",
        "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
        "abstract": "  Understanding the evolutionary stability of cooperation is a central problem\nin biology, sociology, and economics. There exist only a few known mechanisms\nthat guarantee the existence of cooperation and its robustness to cheating.\nHere, we introduce a new mechanism for the emergence of cooperation in the\npresence of fluctuations. We consider agents whose wealth change stochastically\nin a multiplicative fashion. Each agent can share part of her wealth as public\ngood, which is equally distributed among all the agents. We show that, when\nagents operate with long time-horizons, cooperation produce an advantage at the\nindividual level, as it effectively screens agents from the deleterious effect\nof environmental fluctuations.\n"
    },
    {
        "paper_id": 2202.02872,
        "authors": "Michael Curry, Tuomas Sandholm, John Dickerson",
        "title": "Differentiable Economics for Randomized Affine Maximizer Auctions",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  A recent approach to automated mechanism design, differentiable economics,\nrepresents auctions by rich function approximators and optimizes their\nperformance by gradient descent. The ideal auction architecture for\ndifferentiable economics would be perfectly strategyproof, support multiple\nbidders and items, and be rich enough to represent the optimal (i.e.\nrevenue-maximizing) mechanism. So far, such an architecture does not exist.\nThere are single-bidder approaches (MenuNet, RochetNet) which are always\nstrategyproof and can represent optimal mechanisms. RegretNet is multi-bidder\nand can approximate any mechanism, but is only approximately strategyproof. We\npresent an architecture that supports multiple bidders and is perfectly\nstrategyproof, but cannot necessarily represent the optimal mechanism. This\narchitecture is the classic affine maximizer auction (AMA), modified to offer\nlotteries. By using the gradient-based optimization tools of differentiable\neconomics, we can now train lottery AMAs, competing with or outperforming prior\napproaches in revenue.\n"
    },
    {
        "paper_id": 2202.02994,
        "authors": "Hayden Brown",
        "title": "Withdrawal Success Estimation",
        "comments": "24 pages, 8 figures",
        "journal-ref": null,
        "doi": "10.1142/S0219024923500140",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Given a geometric Levy alpha-stable wealth process, a log-Levy alpha-stable\nlower bound is constructed for the terminal wealth of a regular investing\nschedule. Using a transformation, the lower bound is applied to a schedule of\nwithdrawals occurring after an initial investment. As a result, an upper bound\nis described on the probability to complete a given schedule of withdrawals.\nFor withdrawals of a constant amount at equidistant times, necessary conditions\nare given on the initial investment and parameters of the wealth process such\nthat $k$ withdrawals can be made with 95% confidence. When the initial\ninvestment is in the S&P Composite Index and $2\\leq k\\leq 16$, then the initial\ninvestment must be at least $k$ times the amount of each withdrawal.\n"
    },
    {
        "paper_id": 2202.03081,
        "authors": "Voraprapa Nakavachara and Kanis Saengchote",
        "title": "Is Metaverse LAND a good investment? It depends on your unit of account!",
        "comments": "12 pages, 5 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  The Sandbox metaverse LAND non-fungible token (NFT) prices increased by than\n300 times (in USD) between December 2019 and January 2022, but when measured in\nits native utility token (SAND), the increase is only 3 times. Depending on how\nprices are denominated, investment returns and effective transaction prices\nvary. We analyze more than 71,000 transactions and find that users are willing\nto pay 3-4% more when transactions are settled in SAND, and 30% less when\nsettled in wETH (a smart contract version of ETH) when compared to ETH, so unit\nof account matters. Our results contribute to the discussions of\nblockchain-based, virtual economy management and the digitalization of money\n(Brunnermeier et al., 2019).\n"
    },
    {
        "paper_id": 2202.03146,
        "authors": "Shi Bo",
        "title": "Application of K-means Clustering Algorithm in Evaluation and\n  Statistical Analysis of Internet Financial Transaction Data",
        "comments": "Thank you for reading my comment here. Need a major revision, and\n  there are some errors in description of dataset and abstract",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  The purpose is to promote the orderly development of China's Internet\nfinancial transactions and minimize default and delinquency in Internet\nfinancial transactions. Based on the typical big data algorithm (K-means\nalgorithm), this paper discusses the concepts of the K-means algorithm and\nInternet financial transactions, as well as the significance of big data\nalgorithms for Internet financial transaction data evaluation and statistical\nanalysis. Meanwhile, the existing Internet financial transaction systems are\nreviewed, and their deficiencies are summarized, based on which relevant\ncountermeasures and suggestions are put forward. At the same time, the K-means\nclustering algorithm is applied to evaluate financial transaction data, finding\nthat it can improve the accuracy of data and reduce the error by 40%. But when\nthe number of clusters is 7, the output result distribution interval of the\nK-means clustering algorithm is 4 days, and when the number of clusters is 10,\nthe output result distribution interval of the K-means clustering algorithm is\n6 days, indicating that the convergence effect of this algorithm is relatively\ngood. Additionally, many small and micro individuals still hold a negative\nattitude towards the innovation and adjustment of Internet financial\ntransactions, indicating that the construction of China's Internet financial\ntransaction system needs further optimization. The satisfaction of most small\nand micro individuals with innovation and adjustment also shows that the\nproposed Internet financial transaction adjustment measures are feasible, can\nprovide references for relevant Internet financial transactions, and\ncontributes to the development of Internet financial transactions in China.\n"
    },
    {
        "paper_id": 2202.03156,
        "authors": "Ogulcan E. Orsel, Sasha S. Yamada",
        "title": "Comparative Study of Machine Learning Models for Stock Price Prediction",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this work, we apply machine learning techniques to historical stock prices\nto forecast future prices. To achieve this, we use recursive approaches that\nare appropriate for handling time series data. In particular, we apply a linear\nKalman filter and different varieties of long short-term memory (LSTM)\narchitectures to historical stock prices over a 10-year range (1/1/2011 -\n1/1/2021). We quantify the results of these models by computing the error of\nthe predicted values versus the historical values of each stock. We find that\nof the algorithms we investigated, a simple linear Kalman filter can predict\nthe next-day value of stocks with low-volatility (e.g., Microsoft) surprisingly\nwell. However, in the case of high-volatility stocks (e.g., Tesla) the more\ncomplex LSTM algorithms significantly outperform the Kalman filter. Our results\nshow that we can classify different types of stocks and then train an LSTM for\neach stock type. This method could be used to automate portfolio generation for\na target return rate.\n"
    },
    {
        "paper_id": 2202.03158,
        "authors": "Jia Wang, Hongwei Zhu, Jiancheng Shen, Yu Cao, Benyuan Liu",
        "title": "Dual-CLVSA: a Novel Deep Learning Approach to Predict Financial Markets\n  with Sentiment Measurements",
        "comments": "8 pages, 2021 20th IEEE International Conference on Machine Learning\n  and Applications (ICMLA). IEEE, 2021",
        "journal-ref": null,
        "doi": "10.1109/ICMLA52953.2021.00062",
        "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
        "abstract": "  It is a challenging task to predict financial markets. The complexity of this\ntask is mainly due to the interaction between financial markets and market\nparticipants, who are not able to keep rational all the time, and often\naffected by emotions such as fear and ecstasy. Based on the state-of-the-art\napproach particularly for financial market predictions, a hybrid convolutional\nLSTM Based variational sequence-to-sequence model with attention (CLVSA), we\npropose a novel deep learning approach, named dual-CLVSA, to predict financial\nmarket movement with both trading data and the corresponding social sentiment\nmeasurements, each through a separate sequence-to-sequence channel. We evaluate\nthe performance of our approach with backtesting on historical trading data of\nSPDR SP 500 Trust ETF over eight years. The experiment results show that\ndual-CLVSA can effectively fuse the two types of data, and verify that\nsentiment measurements are not only informative for financial market\npredictions, but they also contain extra profitable features to boost the\nperformance of our predicting system.\n"
    },
    {
        "paper_id": 2202.03198,
        "authors": "MohammadReza Zahedian, Mahsa Bagherikalhor, Andrey Trufanov, G. Reza\n  Jafari",
        "title": "Financial Crisis in the Framework of Non-zero Temperature Balance Theory",
        "comments": "10 pages, 6 figures",
        "journal-ref": null,
        "doi": "10.1371/journal.pone.0279089",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Financial crises are known as crashes that result in a sudden loss of value\nof financial assets in large part and they continue to occur from time to time\nsurprisingly. In order to discover features of the financial network, the\npairwise interaction of stocks has been considered in many research, but the\nexistence of the strong correlation of stocks and their collective behavior in\ncrisis made us address higher-order interactions. Hence, in this study, we\ninvestigate financial networks by triplet interaction in the framework of\nbalance theory. Due to detecting the contribution of higher-order interactions\nin understanding the complex behavior of stocks we take the advantage of the\norders parameters of the higher-order interactions. Looking at real data of\nfinancial market obtained from $S\\&P500$ through the lens of balance theory for\nthe quest of network structure in different periods of time near and far from\ncrisis reveals the existence of a structural difference of the network that\ncorresponds to different periods of time. Here, we address two well-known\ncrises the Great regression (2008) and the Covid-19 recession (2020). Results\nshow an ordered structure forms on-crisis in the financial network while stocks\nbehave independently far from a crisis. The formation of the ordered structure\nof stocks in crisis makes the network resistant against disorder. The\nresistance of the ordered structure against applying a disorder (temperature)\ncan measure the crisis strength and determine the temperature at which the\nnetwork transits. There is a critical temperature, $T_{c}$, in the language of\nstatistical mechanics and mean-field approach which above, the ordered\nstructure destroys abruptly and a first-order phase transition occurs. The\nstronger the crisis, the higher the critical temperature.\n"
    },
    {
        "paper_id": 2202.03248,
        "authors": "Dorinel Bastide (UEVE, LaMME), St\\'ephane Cr\\'epey (LPSM (UMR\\_8001),\n  UFR 929), Samuel Drapeau (SAIF), Mekonnen Tadese",
        "title": "Derivatives Risks as Costs in a One-Period Network Model",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We present a one-period XVA model encompassing bilateral and centrally\ncleared trading in a unified framework with explicit formulas for most\nquantities at hand. We illustrate possible uses of this framework for running\nstress test exercises on a financial network from a clearing member's\nperspective or for optimizing the porting of the portfolio of a defaulted\nclearing member.\n"
    },
    {
        "paper_id": 2202.03406,
        "authors": "Marius Hofert, Avinash Prasad, Mu Zhu",
        "title": "Dependence model assessment and selection with DecoupleNets",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Neural networks are suggested for learning a map from $d$-dimensional samples\nwith any underlying dependence structure to multivariate uniformity in $d'$\ndimensions. This map, termed DecoupleNet, is used for dependence model\nassessment and selection. If the data-generating dependence model was known,\nand if it was among the few analytically tractable ones, one such\ntransformation for $d'=d$ is Rosenblatt's transform. DecoupleNets have multiple\nadvantages. For example, they only require an available sample and are\napplicable to $d'<d$, in particular $d'=2$. This allows for simpler model\nassessment and selection, both numerically and, because $d'=2$, especially\ngraphically. A graphical assessment method has the advantage of being able to\nidentify why, or in which region of the domain, a candidate model does not\nprovide an adequate fit, thus leading to model selection in particular regions\nof interest or improved model building strategies in such regions. Through\nsimulation studies with data from various copulas, the feasibility and validity\nof this novel DecoupleNet approach is demonstrated. Applications to real world\ndata illustrate its usefulness for model assessment and selection.\n"
    },
    {
        "paper_id": 2202.03413,
        "authors": "Robert A. Moffitt and Matthew V. Zahn",
        "title": "The Marginal Labor Supply Disincentives of Welfare: Evidence from\n  Administrative Barriers to Participation",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Existing research on the static effects of the manipulation of welfare\nprogram benefit parameters on labor supply has allowed only restrictive forms\nof heterogeneity in preferences. Yet preference heterogeneity implies that the\nmarginal effects on labor supply of welfare expansions and contractions may\ndiffer in different time periods with different populations and which sweep out\ndifferent portions of the distribution of preferences. A new examination of the\nheavily studied AFDC program uses variation in state-level administrative\nbarriers to entering the program in the late 1980s and early 1990s to estimate\nthe marginal labor supply effects of changes in program participation induced\nby that variation. The estimates are obtained from a theory-consistent reduced\nform model which allows for a nonparametric specification of how changes in\nwelfare program participation affect labor supply on the margin. Estimates\nusing a form of local instrumental variables show that the marginal treatment\neffects are quadratic, rising and then falling as participation rates rise\n(i.e., becoming more negative then less negative on hours of work). The average\nwork disincentive is not large but that masks some margins where effects are\nclose to zero and some which are sizable. Traditional IV which estimates a\nweighted average of marginal effects gives a misleading picture of marginal\nresponses. A counterfactual exercise which applies the estimates to three\nhistorical reform periods in 1967, 1981, and 1996 when the program tax rate was\nsignificantly altered shows that marginal labor supply responses differed in\neach period because of differences in the level of participation in the period\nand the composition of who was on the program.\n"
    },
    {
        "paper_id": 2202.03602,
        "authors": "Sourav Sinha",
        "title": "US Salary History Bans -- Strategic Disclosure by Job Applicants and the\n  Gender Pay Gap",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  I study the effects of US salary history bans which restrict employers from\ninquiring about job applicants' pay history during the hiring process, but\nallow candidates to voluntarily share information. Using a\ndifference-in-differences design, I show that these policies narrowed the\ngender pay gap significantly by 2 p.p., driven almost entirely by an increase\nin female earnings. The bans were also successful in weakening the\nauto-correlation between current and future earnings, especially among\njob-changers. I provide novel evidence showing that when employers could no\nlonger nudge candidates for information, the likelihood of voluntarily\ndisclosing salary history decreased among job applicants and by 2 p.p. more\namong women. I then develop a salary negotiation model with asymmetric\ninformation, where I allow job applicants to choose whether to reveal pay\nhistory, and use this framework to explain my empirical findings on disclosure\nbehavior and gender pay gap.\n"
    },
    {
        "paper_id": 2202.03682,
        "authors": "Pramod Kumar Sur",
        "title": "The Legacy of Authoritarianism in a Democracy",
        "comments": "55 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  Recent democratic backsliding and the rise of authoritarian regimes worldwide\nhave rekindled interest in understanding the causes and consequences of such\nauthoritarian rule in democracies. In this paper, I study the long-run\npolitical consequences of authoritarianism in the world's largest democracy.\nExploiting the unexpected timing of the authoritarian rule imposed in India in\nthe 1970s and using a difference-in-difference (DID), triple difference (DDD),\nand a regression discontinuity design (RDD) estimation approach, I document a\nsharp decline in the then-dominant incumbent, the Indian National Congress\nparty's political dominance in subsequent years. I also present evidence that\nthe decline in political dominance was not at the expense of a lower voter\nturnout rate. Instead, a sharp rise in the number of opposition candidates\ncontesting elections in subsequent years played an important role. Finally, I\nexamine the enduring consequences, revealing that confidence in politicians\nremains low in states where the draconian policy was high.\n"
    },
    {
        "paper_id": 2202.03806,
        "authors": "Jinghan Tian, Jianhua Wang",
        "title": "Cuierzhuang Phenomenon: A model of rural industrialization in north\n  China",
        "comments": "13 pages,3 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
        "abstract": "  Cuierzhuang Phenomenon (or Cuierzhuang Model) is a regional development\nphenomenon or rural revitalization model driven by ICT in the information era,\ncharacterized by the storage and transportation, processing, packaging and\nonline sales of agricultural products, as well as online and offline\ncoordination, long-distance and cross-regional economic cooperation, ethnic\nblending, equality, and mutual benefit. Unlike the Wenzhou Model, South Jiangsu\nModel, and Pearl River Model in the 1980s and 1990s, the Cuierzhuang Model is\nnot only a rural revitalization brought about by the industrialization and\nmodernization of northern rural areas with the characteristics of industrial\ndevelopment in the information age, but also an innovative regional economic\ncooperation and development model with folk nature, spontaneous formation,\nequality, and mutual benefit. Taking southern Xinjiang as the production base,\nXinjiang jujubes from Hotan and Ruoqiang are continuously transported to\nCuierzhuang, Cangzhou City, Hebei Province, where they are transferred,\ncleaned, dried and packaged, and finally sold all over the country. With red\ndates as a link, the eastern town of Cuierzhuang, which is more than 4,000\nkilometers apart, connected with Xinjiang in the western region. Along the\nancient Silk Road, the farthest route can reach as far as Kashgar through the\nsouthern Xinjiang route. Then, how did this long-distance and cross-regional\neconomic cooperation channel form, what are the regional economics or economic\ngeography principles of Cuierzhuang attracting Xinjiang jujube, and the\nchallenges and opportunities faced by Cuierzhuang phenomenon, etc. A\npreliminary economic analysis has been carried out in this paper.\n"
    },
    {
        "paper_id": 2202.03858,
        "authors": "Chung-Han Hsieh",
        "title": "On Solving Robust Log-Optimal Portfolio: A Supporting Hyperplane\n  Approximation Approach",
        "comments": "submitted for possible publication",
        "journal-ref": "European Journal of Operational Research, 2023",
        "doi": "10.1016/j.ejor.2023.09.040",
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  A {log-optimal} portfolio is any portfolio that maximizes the expected\nlogarithmic growth (ELG) of an investor's wealth. This maximization problem\ntypically assumes that the information of the true distribution of returns is\nknown to the trader in advance. However, in practice, the return distributions\nare indeed {ambiguous}; i.e., the true distribution is unknown to the trader or\nit is partially known at best. To this end, a {distributional robust\nlog-optimal portfolio problem} formulation arises naturally. While the problem\nformulation takes into account the ambiguity on return distributions, the\nproblem needs not to be tractable in general. To address this, in this paper,\nwe propose a {supporting hyperplane approximation} approach that allows us to\nreformulate a class of distributional robust log-optimal portfolio problems\ninto a linear program, which can be solved very efficiently. Our framework is\nflexible enough to allow {transaction costs}, {leverage and shorting},\n{survival trades}, and {diversification considerations}. In addition, given an\nacceptable approximation error, an efficient algorithm for rapidly calculating\nthe optimal number of hyperplanes is provided. Some empirical studies using\nhistorical stock price data are also provided to support our theory.\n"
    },
    {
        "paper_id": 2202.03874,
        "authors": "Yu Zhao, Shaopeng Wei, Yu Guo, Qing Yang, Xingyan Chen, Qing Li,\n  Fuzhen Zhuang, Ji Liu, Gang Kou",
        "title": "Combining Intra-Risk and Contagion Risk for Enterprise Bankruptcy\n  Prediction Using Graph Neural Networks",
        "comments": "12 pages, 8 figures",
        "journal-ref": "Information Sciences, 659(2024)1-17",
        "doi": "10.1016/j.ins.2023.120081",
        "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
        "abstract": "  Predicting the bankruptcy risk of small and medium-sized enterprises (SMEs)\nis an important step for financial institutions when making decisions about\nloans. Existing studies in both finance and AI research fields, however, tend\nto only consider either the intra-risk or contagion risk of enterprises,\nignoring their interactions and combinatorial effects. This study for the first\ntime considers both types of risk and their joint effects in bankruptcy\nprediction. Specifically, we first propose an enterprise intra-risk encoder\nbased on statistically significant enterprise risk indicators for its\nintra-risk learning. Then, we propose an enterprise contagion risk encoder\nbased on enterprise relation information from an enterprise knowledge graph for\nits contagion risk embedding. In particular, the contagion risk encoder\nincludes both the newly proposed Hyper-Graph Neural Networks and Heterogeneous\nGraph Neural Networks, which can model contagion risk in two different aspects,\ni.e. common risk factors based on hyperedges and direct diffusion risk from\nneighbors, respectively. To evaluate the model, we collect real-world\nmulti-sources data on SMEs and build a novel benchmark dataset called SMEsD. We\nprovide open access to the dataset, which is expected to further promote\nresearch on financial risk analysis. Experiments on SMEsD against twelve\nstate-of-the-art baselines demonstrate the effectiveness of the proposed model\nfor bankruptcy prediction.\n"
    },
    {
        "paper_id": 2202.04131,
        "authors": "Luis Aguiar, Christian Peukert, Maximilian Sch\\\"afer, Hannes Ullrich",
        "title": "Facebook Shadow Profiles",
        "comments": "13 pages, 5 figures, 4 tables",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  We quantify Facebook's ability to build shadow profiles by tracking\nindividuals across the web, irrespective of whether they are users of the\nsocial network. For a representative sample of US Internet users, we find that\nFacebook is able to track about 40 percent of the browsing time of both users\nand non-users of Facebook, including on privacy-sensitive domains and across\nuser demographics. We show that the collected browsing data can produce\naccurate predictions of personal information that is valuable for advertisers,\nsuch as age or gender. Because Facebook users reveal their demographic\ninformation to the platform, and because the browsing behavior of users and\nnon-users of Facebook overlaps, users impose a data externality on non-users by\nallowing Facebook to infer their personal information.\n"
    },
    {
        "paper_id": 2202.04174,
        "authors": "Shomak Chakrabarti, Ilia Krasikov, Rohit Lamba",
        "title": "Behavioral epidemiology: An economic model to evaluate optimal policy in\n  the midst of a pandemic",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  This paper combines a canonical epidemiology model of disease dynamics with\ngovernment policy of lockdown and testing, and agents' decision to social\ndistance in order to avoid getting infected. The model is calibrated with data\non deaths and testing outcomes in the Unites States. It is shown that an\nintermediate but prolonged lockdown is socially optimal when both mortality and\nGDP are taken into account. This is because the government wants the economy to\nkeep producing some output and the slack in reducing infection is picked up by\nsocial distancing agents. Social distancing best responds to the optimal\ngovernment policy to keep the effective reproductive number at one and avoid\nmultiple waves through the pandemic. Calibration shows testing to have been\neffective, but it could have been even more instrumental if it had been\naggressively pursued from the beginning of the pandemic. Not having any\nlockdown or shutting down social distancing would have had extreme\nconsequences. Greater centralized control on social activities would have\nmitigated further the spread of the pandemic.\n"
    },
    {
        "paper_id": 2202.0422,
        "authors": "Xiang Gao, Cody Hyndman, Traian A. Pirvu, Petar Jevti\\'c",
        "title": "Optimal annuitization post-retirement with labor income",
        "comments": "28 pages, 9 figures, 1 table",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Evidence shows that the labor participation rate of retirement age cohorts is\nnon-negligible, and it is a widespread phenomenon globally. In the United\nStates, the labor force participation rate for workers age 75 and older is\nprojected to be over 10 percent by 2026 as reported by the Bureau of Labor\nStatistics. The prevalence of post-retirement work changes existing\nconsiderations of optimal annuitization, a research question further\ncomplicated by novel factors such as post-retirement labor rates, wage rates,\nand capacity or willingness to work. To our knowledge, this poses a practical\nand theoretical problem not previously investigated in actuarial literature. In\nthis paper, we study the problem of post-retirement annuitization with extra\nlabor income in the framework of stochastic control, optimal stopping, and\nexpected utility maximization. The utility functions are of the Cobb-Douglas\ntype. The martingale methodology and duality techniques are employed to obtain\nclosed-form solutions for the dual and primal problems. The effect of labor\nincome is investigated by exploiting the explicit solutions and Monte-Carlo\nsimulation. The latter reveals that the optimal annuitization time is strongly\nlinear with respect to the initial wealth, with or without labor income. When\nit comes to optimal annuitization, we find that the wage and labor rates may\nplay opposite roles. However, their impact is mediated by the leverage ratio.\n"
    },
    {
        "paper_id": 2202.04591,
        "authors": "Leonardo Gasparini and Germ\\'an Reyes",
        "title": "Are Fairness Perceptions Shaped by Income Inequality? Evidence from\n  Latin America",
        "comments": "JEL Codes: D31, D63, D83. Keywords: Distributive Justice, Fairness,\n  Income Inequality, Latin America, Perceptions",
        "journal-ref": "Journal of Economic Inequality, October, 2022",
        "doi": "10.1007/s10888-022-09526-w",
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  A common assumption in the literature is that the level of income inequality\nshapes individuals' beliefs about whether the income distribution is fair\n(``fairness views,'' for short). However, individuals do not directly observe\nincome inequality (which often leads to large misperceptions), nor do they\nconsider all inequities to be unfair. In this paper, we empirically assess the\nlink between objective measures of income inequality and fairness views in a\ncontext of high but decreasing income inequality. We combine opinion poll data\nwith harmonized data from household surveys of 18 Latin American countries from\n1997--2015. We report three main findings. First, we find a strong and\nstatistically significant relationship between income inequality and unfairness\nviews across countries and over time. Unfairness views evolved in the same\ndirection as income inequality for 17 out of the 18 countries in our sample.\nSecond, individuals who are older, unemployed, and left-wing are, on average,\nmore likely to perceive the income distribution as very unfair. Third, fairness\nviews and income inequality have predictive power for individuals'\nself-reported propensity to mobilize and protest independent of each other,\nsuggesting that these two variables capture different channels through which\nchanges in the income distribution can affect social unrest.\n"
    },
    {
        "paper_id": 2202.04811,
        "authors": "Jonatan A. Lassa, Gisela Emanuela Nappoe, Susilo Budhi Sulistyo",
        "title": "Creating an institutional ecosystem for cash transfer programming:\n  Lessons from post-disaster governance in Indonesia",
        "comments": "21p",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Humanitarian and disaster management actors have increasingly adopted cash\ntransfer to reduce the sufferings and vulnerability of the survivors. Case\ntransfers have also been used as a critical instrument in the current COVID-19\npandemic. Unfortunately, academic work on humanitarian and disaster-cash\ntransfer related issues remains limited. This article explores how NGOs and\ngovernments implement humanitarian cash transfer in a post-disaster setting\nusing an exploratory research strategy. It asks What are institutional\nconstraints and opportunities faced by humanitarian emergency responders in\nensuring an effective humanitarian cash transfer and how humanitarian actors\naddress such institutional conditions. We introduced a new conceptual\nframework, namely humanitarian and disaster management ecosystem for cash\ntransfer. This framework allows non-governmental actors to restore complex\nrelations between the state, disaster survivors or citizen, local market\neconomy and civil society. Mixed methods and multistage research strategy were\nused to collect and analyze primary and secondary data. The findings suggest\nthat implementing cash transfers in the context of post tsunamigenic\nearthquakes and liquefaction hazards, NGOs must co-create an ecosystem of\nresponse that not only aimed at restoring peoples access to cash and basic\nneeds but first they must restore relations between the states and their\ncitizen while linking the at-risk communities with the private sectors to\njump-starting local livelihoods and market economy.\n"
    },
    {
        "paper_id": 2202.04931,
        "authors": "Carl Bonander, Mats Ekman, Niklas Jakobsson",
        "title": "Vaccination nudges: A study of pre-booked COVID-19 vaccinations in\n  Sweden",
        "comments": "30 pages, 10 figures, 12 tables",
        "journal-ref": "Social Science & Medicine, vol. 309, 2022, 115248",
        "doi": "10.1016/j.socscimed.2022.115248",
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  A nudge changes people's actions without removing their options or altering\ntheir incentives. During the COVID-19 vaccine rollout, the Swedish Region of\nUppsala sent letters with pre-booked appointments to inhabitants aged 16-17\ninstead of opening up manual appointment booking. Using regional and municipal\nvaccination data, we document a higher vaccine uptake among 16- to 17-year-olds\nin Uppsala compared to untreated control regions (constructed using the\nsynthetic control method as well as neighboring municipalities). The results\nhighlight pre-booked appointments as a strategy for increasing vaccination\nrates in populations with low perceived risk.\n"
    },
    {
        "paper_id": 2202.0522,
        "authors": "Jeffrey D. Michler, Anna Josephson, Talip Kilic, Siobhan Murray",
        "title": "Privacy Protection, Measurement Error, and the Integration of Remote\n  Sensing and Socioeconomic Survey Data",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  When publishing socioeconomic survey data, survey programs implement a\nvariety of statistical methods designed to preserve privacy but which come at\nthe cost of distorting the data. We explore the extent to which spatial\nanonymization methods to preserve privacy in the large-scale surveys supported\nby the World Bank Living Standards Measurement Study - Integrated Surveys on\nAgriculture (LSMS-ISA) introduce measurement error in econometric estimates\nwhen that survey data is integrated with remote sensing weather data. Guided by\na pre-analysis plan, we produce 90 linked weather-household datasets that vary\nby the spatial anonymization method and the remote sensing weather product. By\nvarying the data along with the econometric model we quantify the magnitude and\nsignificance of measurement error coming from the loss of accuracy that results\nfrom protect privacy measures. We find that spatial anonymization techniques\ncurrently in general use have, on average, limited to no impact on estimates of\nthe relationship between weather and agricultural productivity. However, the\ndegree to which spatial anonymization introduces mismeasurement is a function\nof which remote sensing weather product is used in the analysis. We conclude\nthat care must be taken in choosing a remote sensing weather product when\nlooking to integrate it with publicly available survey data.\n"
    },
    {
        "paper_id": 2202.05326,
        "authors": "Georgios I. Papayiannis",
        "title": "Robust Policy Selection and Harvest Risk Quantification for Natural\n  Resources Management under Model Uncertainty",
        "comments": "15 pages",
        "journal-ref": "Journal of Dynamics and Games, 2022, 9.2: 203-217",
        "doi": "10.3934/jdg.2022004",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this work the problem of optimal harvesting policy selection for natural\nresources management under model uncertainty is investigated. Under the\nframework of the neoclassical growth model dynamics, the associated optimal\ncontrol problem is investigated by introducing the concept of model uncertainty\non the initial conditions of the operational procedure. At this stage, the\nnotion of convex risk measures, and in particular the class of Fr\\'echet risk\nmeasures, is employed in order to quantify the total operational and marginal\nrisk, whereas simultaneously obtaining robust to model uncertainty harvesting\nstrategies.\n"
    },
    {
        "paper_id": 2202.05374,
        "authors": "Zachariah Sinkala, Vajira Manathunga, Bichaka Fayissa",
        "title": "An Epidemic Compartment Model for Economic Policy Directions for\n  Managing Future Pandemic",
        "comments": "42 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  In this research, we develop a framework to analyze the interaction between\nthe economy and the Covid-19 pandemic using an extension of SIR epidemic model.\nAt the outset, we assume there are two health related investments including\ngeneral medical expenditures and the other for a direct investment for\ncontrolling the pandemic. We incorporate the learning dynamics associated with\nthe management of the virus into our model. Given that the labor force in a\nsociety depends on the state of the epidemic, we allow birth, death, and\nvaccination to occur in our model and assume labor force consists of the\nsusceptible, vaccinated, and recovered individuals. We also assume parameters\nin our epidemic compartmental model depend on investment amount for directly\ncontrolling the epidemic, the health stock of individual representative agents\nin the society, and the knowledge or learning about the epidemic in the\ncommunity. By controlling consumption, the general medical expenditure, and the\ndirect investment of funds for controlling the epidemic, we optimize the\nutility realized by the representative individuals because of consumption. This\nproblem is nontrivial since the disease dynamics results in a non-convex\noptimization problem.\n"
    },
    {
        "paper_id": 2202.05671,
        "authors": "Mark Mink and Frans J. de Weert",
        "title": "Black-Scholes-Merton Option Pricing Revisited: Did we Find a Fatal Flaw?",
        "comments": "18 pages, no figures, thorough revision with new results and\n  references",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  The option pricing formula of Black and Scholes (1973) hinges on the\ncontinuous-time self-financing condition, which is a special case of the\ncontinuous-time budget equation of Merton (1971). The self-financing condition\nis believed to formalize the economic concept of portfolio rebalancing without\ninflows or outflows of external funds, but was never formally derived in\ncontinuous time. As a much bigger issue, however, we discover a timing mistake\nin the model of Merton (1971) and show that his self-financing condition is\nmisspecified both in discrete and continuous time. Our results invalidate\nseminal contributions to the literature, including the budget equation of\nMerton (1971), the option pricing formula of Black and Scholes (1973), the\ncontinuous trading model of Harrison and Pliska (1981), and the binomial option\npricing model of Cox, Ross and Rubinstein (1979). We also show that Black and\nScholes (1973) and Harrison and Pliska (1981) implicitly assumed their\nreplication result.\n"
    },
    {
        "paper_id": 2202.05674,
        "authors": "George Sullivan and Luke Burns",
        "title": "Cashing Out: Assessing the risk of localised financial exclusion as the\n  UK moves towards a cashless society",
        "comments": "60 pages, 10 figures, 14 maps, 10 tables",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Whilst academic, commercial and policy literature on financial exclusion is\nextensive and wide-ranging, there have been very few attempts to quantify and\nmeasure localised financial exclusion anywhere in the world. This is a subject\nof growing importance in modern UK society with the withdrawal of cash\ninfrastructure and a shift towards online banking. This research develops a\ncomposite indicator using a wide-range of input variables, including the\nlocations of existing cash infrastructure, various demographic factors (such as\nincome and housing tenure) and other freely available lifestyle data to\nidentify areas at greatest risk of financial exclusion, thereby aiding\norganisations to develop intervention strategies to tackle the problem. The\nindicator illustrates that whilst there is no apparent correlation between\nfinancial exclusion and deprivation, pockets of extreme financial exclusion are\ngenerally found in deprived communities, and affluent, suburban areas tend to\nscore consistently more favourably and consequently carry less risk. The\nattributing causes vary, from a lack of infrastructure, to low car\navailability, but income levels have a pronounced influence. Three policy\nproposals are put forward, including offering banking services at PayPoint\noutlets, and converting cash machines to cash recyclers, but improving digital\nadoption was found to be the most effective intervention, provided that it is\nimplemented by community organisations. Policies purely targeting\ninfrastructure provision or addressing social exclusion are unlikely to be\neffective, as community-based initiatives coupled with wider reforms to the\nfinancial system are needed.\n"
    },
    {
        "paper_id": 2202.05702,
        "authors": "Yuxuan Huang, Luiz Fernando Capretz, Danny Ho",
        "title": "Machine Learning for Stock Prediction Based on Fundamental Analysis",
        "comments": "10 pages. IEEE Symposium Series on Computational Intelligence,\n  Orlando, Florida,USA, December 2021",
        "journal-ref": null,
        "doi": "10.1109/SSCI50451.2021.9660134",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Application of machine learning for stock prediction is attracting a lot of\nattention in recent years. A large amount of research has been conducted in\nthis area and multiple existing results have shown that machine learning\nmethods could be successfully used toward stock predicting using stocks\nhistorical data. Most of these existing approaches have focused on short term\nprediction using stocks historical price and technical indicators. In this\npaper, we prepared 22 years worth of stock quarterly financial data and\ninvestigated three machine learning algorithms: Feed-forward Neural Network\n(FNN), Random Forest (RF) and Adaptive Neural Fuzzy Inference System (ANFIS)\nfor stock prediction based on fundamental analysis. In addition, we applied RF\nbased feature selection and bootstrap aggregation in order to improve model\nperformance and aggregate predictions from different models. Our results show\nthat RF model achieves the best prediction results, and feature selection is\nable to improve test performance of FNN and ANFIS. Moreover, the aggregated\nmodel outperforms all baseline models as well as the benchmark DJIA index by an\nacceptable margin for the test period. Our findings demonstrate that machine\nlearning models could be used to aid fundamental analysts with decision-making\nregarding stock investment.\n"
    },
    {
        "paper_id": 2202.05743,
        "authors": "Edmond Berisha, Ram Sewak Dubey, Orkideh Gharehgozli",
        "title": "Inflation and income inequality: Does the level of income inequality\n  matter?",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In the recent times of global Covid pandemic, the Federal Reserve has raised\nthe concerns of upsurges in prices. Given the complexity of interaction between\ninflation and inequality, we examine whether the impact of inflation on\ninequality differs among distinct levels of income inequality across the US\nstates. Results reveal that there is a negative contemporaneous effect of\ninflation on the inequality which becomes stronger with higher levels of income\ninequality. However, over a one year period, we find higher inflation rate to\nfurther increase income inequality only when income inequality is initially\nrelatively low.\n"
    },
    {
        "paper_id": 2202.05779,
        "authors": "Agostino Capponi, Ruizhe Jia, Ye Wang",
        "title": "The Evolution of Blockchain: from Lit to Dark",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
        "abstract": "  Transactions submitted through the blockchain peer-to-peer (P2P) network may\nleak out exploitable information. We study the economic incentives behind the\nadoption of blockchain dark venues, where users' transactions are observable\nonly by miners on these venues. We show that miners may not fully adopt dark\nvenues to preserve rents extracted from arbitrageurs, hence creating execution\nrisk for users. The dark venue neither eliminates frontrunning risk nor reduces\ntransaction costs. It strictly increases the payoff of miners, weakly increases\nthe payoff of users, and weakly reduces arbitrageurs' profits. We provide\nempirical support for our main implications, and show that they are\neconomically significant. A 1% increase in the probability of being frontrun\nraises users' adoption rate of the dark venue by 0.6%. Arbitrageurs'\ncost-to-revenue ratio increases by a third with a dark venue.\n"
    },
    {
        "paper_id": 2202.05885,
        "authors": "Hong Chen and Murray Zed Frank",
        "title": "Equilibrium Defaultable Corporate Debt and Investment",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  In dynamic capital structure models with an investor break-even condition,\nthe firm's Bellman equation may not generate a contraction mapping, so the\nstandard existence and uniqueness conditions do not apply. First, we provide an\nexample showing the problem in a classical trade-off model. The firm can issue\none-period defaultable debt, invest in capital and pay a dividend. If the firm\ncannot meet the required debt payment, it is liquidated. Second, we show how to\nuse a dual to the original problem and a change of measure, such that existence\nand uniqueness can be proved. In the unique Markov-perfect equilibrium, firm\ndecisions reflect state-dependent capital and debt targets. Our approach may be\nuseful for other dynamic firm models that have an investor break-even\ncondition.\n"
    },
    {
        "paper_id": 2202.06177,
        "authors": "P. Carr, A. Itkin, D. Muravey",
        "title": "Semi-analytical pricing of barrier options in the time-dependent Heston\n  model",
        "comments": "29 pages, 5 figures, 4 tables",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  We develop the general integral transforms (GIT) method for pricing barrier\noptions in the time-dependent Heston model (also with a time-dependent barrier)\nwhere the option price is represented in a semi-analytical form as a\ntwo-dimensional integral. This integral depends on yet unknown function\n$\\Phi(t,v)$ which is the gradient of the solution at the moving boundary $S =\nL(t)$ and solves a linear mixed Volterra-Fredholm equation of the second kind\nalso derived in the paper. Thus, we generalize the one-dimensional GIT method,\ndeveloped in (Itkin, Lipton, Muravey, Generalized integral transforms in\nmathematical finance, WS, 2021) and the corresponding papers, to the\ntwo-dimensional case. In other words, we show that the GIT method can be\nextended to stochastic volatility models (two drivers with inhomogeneous\ncorrelation). As such, this 2D approach naturally inherits all advantages of\nthe corresponding 1D methods, in particular, their speed and accuracy. This\nresult is new and has various applications not just in finance but also in\nphysics. Numerical examples illustrate high speed and accuracy of the method as\ncompared with the finite-difference approach.\n"
    },
    {
        "paper_id": 2202.06534,
        "authors": "Romain Blanchard, Laurence Carassus",
        "title": "Super-replication prices with multiple-priors in discrete time",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In the frictionless discrete time financial market of Bouchard and Nutz\n(2015), we propose a full characterization of the quasi-sure super-replication\nprice: as the supremum of the mono-prior super-replication prices, through an\nextreme prior and through martingale measures.\n"
    },
    {
        "paper_id": 2202.06555,
        "authors": "Aryan Eftekhari, Simon Scheidegger",
        "title": "High-Dimensional Dynamic Stochastic Model Representation",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-sa/4.0/",
        "abstract": "  We propose a scalable method for computing global solutions of nonlinear,\nhigh-dimensional dynamic stochastic economic models. First, within a time\niteration framework, we approximate economic policy functions using an\nadaptive, high-dimensional model representation scheme, combined with adaptive\nsparse grids to address the ubiquitous challenge of the curse of\ndimensionality. Moreover, the adaptivity within the individual component\nfunctions increases sparsity since grid points are added only where they are\nmost needed, that is, in regions with steep gradients or at\nnondifferentiabilities. Second, we introduce a performant vectorization scheme\nfor the interpolation compute kernel. Third, the algorithm is hybrid\nparallelized, leveraging both distributed- and shared-memory architectures. We\nobserve significant speedups over the state-of-the-art techniques, and almost\nideal strong scaling up to at least $1,000$ compute nodes of a Cray XC$50$\nsystem at the Swiss National Supercomputing Center. Finally, to demonstrate our\nmethod's broad applicability, we compute global solutions to two variates of a\nhigh-dimensional international real business cycle model up to $300$ continuous\nstate variables. In addition, we highlight a complementary advantage of the\nframework, which allows for a priori analysis of the model complexity.\n"
    },
    {
        "paper_id": 2202.06637,
        "authors": "Ziheng Wang and Justin Sirignano",
        "title": "Continuous-time stochastic gradient descent for optimizing over the\n  stationary distribution of stochastic differential equations",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We develop a new continuous-time stochastic gradient descent method for\noptimizing over the stationary distribution of stochastic differential equation\n(SDE) models. The algorithm continuously updates the SDE model's parameters\nusing an estimate for the gradient of the stationary distribution. The gradient\nestimate is simultaneously updated using forward propagation of the SDE state\nderivatives, asymptotically converging to the direction of steepest descent. We\nrigorously prove convergence of the online forward propagation algorithm for\nlinear SDE models (i.e., the multi-dimensional Ornstein-Uhlenbeck process) and\npresent its numerical results for nonlinear examples. The proof requires\nanalysis of the fluctuations of the parameter evolution around the direction of\nsteepest descent. Bounds on the fluctuations are challenging to obtain due to\nthe online nature of the algorithm (e.g., the stationary distribution will\ncontinuously change as the parameters change). We prove bounds for the\nsolutions of a new class of Poisson partial differential equations (PDEs),\nwhich are then used to analyze the parameter fluctuations in the algorithm. Our\nalgorithm is applicable to a range of mathematical finance applications\ninvolving statistical calibration of SDE models and stochastic optimal control\nfor long time horizons where ergodicity of the data and stochastic process is a\nsuitable modeling framework. Numerical examples explore these potential\napplications, including learning a neural network control for high-dimensional\noptimal control of SDEs and training stochastic point process models of limit\norder book events.\n"
    },
    {
        "paper_id": 2202.06666,
        "authors": "Taras Bodnar and Nestor Parolya and Erik Thors\\'en",
        "title": "Two is better than one: Regularized shrinkage of large minimum variance\n  portfolio",
        "comments": "25 pages, 5 figures, 3 tables",
        "journal-ref": "Journal of Machine Learning Research, 25(173), 2024, p. 1-32",
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper we construct a shrinkage estimator of the global minimum\nvariance (GMV) portfolio by a combination of two techniques: Tikhonov\nregularization and direct shrinkage of portfolio weights. More specifically, we\nemploy a double shrinkage approach, where the covariance matrix and portfolio\nweights are shrunk simultaneously. The ridge parameter controls the stability\nof the covariance matrix, while the portfolio shrinkage intensity shrinks the\nregularized portfolio weights to a predefined target. Both parameters\nsimultaneously minimize with probability one the out-of-sample variance as the\nnumber of assets $p$ and the sample size $n$ tend to infinity, while their\nratio $p/n$ tends to a constant $c>0$. This method can also be seen as the\noptimal combination of the well-established linear shrinkage approach of Ledoit\nand Wolf (2004, JMVA) and the shrinkage of the portfolio weights by Bodnar et\nal. (2018, EJOR). No specific distribution is assumed for the asset returns\nexcept of the assumption of finite $4+\\varepsilon$ moments. The performance of\nthe double shrinkage estimator is investigated via extensive simulation and\nempirical studies. The suggested method significantly outperforms its\npredecessor (without regularization) and the nonlinear shrinkage approach in\nterms of the out-of-sample variance, Sharpe ratio and other empirical measures\nin the majority of scenarios. Moreover, it obeys the most stable portfolio\nweights with uniformly smallest turnover.\n"
    },
    {
        "paper_id": 2202.06782,
        "authors": "Jack S. Baker and Santosh Kumar Radha",
        "title": "Wasserstein Solution Quality and the Quantum Approximate Optimization\n  Algorithm: A Portfolio Optimization Case Study",
        "comments": "21 pages and 11 Figures in main article, 8 pages, 5 Figures and 3\n  tables in Supplemental Material",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Optimizing of a portfolio of financial assets is a critical industrial\nproblem which can be approximately solved using algorithms suitable for quantum\nprocessing units (QPUs). We benchmark the success of this approach using the\nQuantum Approximate Optimization Algorithm (QAOA); an algorithm targeting\ngate-model QPUs. Our focus is on the quality of solutions achieved as\ndetermined by the Normalized and Complementary Wasserstein Distance, $\\eta$,\nwhich we present in a manner to expose the QAOA as a transporter of\nprobability. Using $\\eta$ as an application specific benchmark of performance,\nwe measure it on selection of QPUs as a function of QAOA circuit depth $p$. At\n$n = 2$ (2 qubits) we find peak solution quality at $p=5$ for most systems and\nfor $n = 3$ this peak is at $p=4$ on a trapped ion QPU. Increasing solution\nquality with $p$ is also observed using variants of the more general Quantum\nAlternating Operator Ans\\\"{a}tz at $p=2$ for $n = 2$ and $3$ which has not been\npreviously reported. In identical measurements, $\\eta$ is observed to be\nvariable at a level exceeding the noise produced from the finite number of\nshots. This suggests that variability itself should be regarded as a QPU\nperformance benchmark for given applications. While studying the ideal\nexecution of QAOA, we find that $p=1$ solution quality degrades when the\nportfolio budget $B$ approaches $n/2$ and increases when $B \\approx 1$ or\n$n-1$. This trend directly corresponds to the binomial coefficient $nCB$ and is\nconnected with the recently reported phenomenon of reachability deficits.\nDerivative-requiring and derivative-free classical optimizers are benchmarked\non the basis of the achieved $\\eta$ beyond $p=1$ to find that derivative-free\noptimizers are generally more effective for the given computational resources,\nproblem sizes and circuit depths.\n"
    },
    {
        "paper_id": 2202.07128,
        "authors": "Hengameh Fakhravar and Hesamoddin Tahami",
        "title": "International Co-Branding and Firms Finance Performance",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Co-branding has become a widely used marketing strategy, yet little attention\nhas been paid to its impact on a firm's stock value. Prior literature has shown\nthat using a co-branding strategy properly helps firms leverage the brand value\nand equity. We discussed the theoretical foundations and main accomplishments\nof prior studies. We developed a conceptual framework and hypothesis to close\nthe existing research gap in the topic of interest. We argued that co-branding\nevent announcement generates positive abnormal returns in the stock market.\nFurthermore, we investigated the moderating impact of co-branding structure on\nthe relation between co-branding event announcements and abnormal returns. We\nclaimed that higher co-branding integration, greater co-branding exclusivity,\nand longer co-branding duration generate a greater positive abnormal return for\nthe partnering firms.\n"
    },
    {
        "paper_id": 2202.07148,
        "authors": "Samuel N. Cohen and Christoph Reisinger and Sheng Wang",
        "title": "Estimating risks of option books using neural-SDE market models",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper, we examine the capacity of an arbitrage-free neural-SDE market\nmodel to produce realistic scenarios for the joint dynamics of multiple\nEuropean options on a single underlying. We subsequently demonstrate its use as\na risk simulation engine for option portfolios. Through backtesting analysis,\nwe show that our models are more computationally efficient and accurate for\nevaluating the Value-at-Risk (VaR) of option portfolios, with better coverage\nperformance and less procyclicality than standard filtered historical\nsimulation approaches.\n"
    },
    {
        "paper_id": 2202.07174,
        "authors": "Michael V. Klibanov, Aleksander A. Shananin, Kirill V. Golubnichiy,\n  Sergey M. Kravchenko",
        "title": "Forecasting Stock Options Prices via the Solution of an Ill-Posed\n  Problem for the Black-Scholes Equation",
        "comments": null,
        "journal-ref": null,
        "doi": "10.1088/1361-6420/ac91ec",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In the previous paper (Inverse Problems, 32, 015010, 2016), a new heuristic\nmathematical model was proposed for accurate forecasting of prices of stock\noptions for 1-2 trading days ahead of the present one. This new technique uses\nthe Black-Scholes equation supplied by new intervals for the underlying stock\nand new initial and boundary conditions for option prices. The Black-Scholes\nequation was solved in the positive direction of the time variable, This\nill-posed initial boundary value problem was solved by the so-called\nQuasi-Reversibility Method (QRM). This approach with an added trading strategy\nwas tested on the market data for 368 stock options and good forecasting\nresults were demonstrated. In the current paper, we use the geometric Brownian\nmotion to provide an explanation of that effectivity using computationally\nsimulated data for European call options. We also provide a convergence\nanalysis for QRM. The key tool of that analysis is a Carleman estimate.\n"
    },
    {
        "paper_id": 2202.07269,
        "authors": "Philine Widmer, Sergio Galletta, and Elliott Ash",
        "title": "Media Slant is Contagious",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  This paper examines the diffusion of media slant, specifically how partisan\ncontent from national cable news affects local newspapers in the U.S.,\n2005-2008. We use a text-based measure of cable news slant trained on content\nfrom Fox News Channel (FNC), CNN, and MSNBC to analyze how local newspapers\nadopt FNC's slant over CNN/MSNBC's. Our findings show that local news becomes\nmore similar to FNC content in response to an exogenous increase in local FNC\nviewership. This shift is not limited to borrowing from cable news, but rather,\nlocal newspapers' own content changes. Further, cable TV slant polarizes local\nnews content.\n"
    },
    {
        "paper_id": 2202.073,
        "authors": "YinYin Yu, Guillaume Saint-Jacques",
        "title": "Choosing an algorithmic fairness metric for an online marketplace:\n  Detecting and quantifying algorithmic bias on LinkedIn",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  In this paper, we derive an algorithmic fairness metric from the fairness\nnotion of equal opportunity for equally qualified candidates for recommendation\nalgorithms commonly used by two-sided marketplaces. We borrow from the economic\nliterature on discrimination to arrive at a test for detecting bias that is\nsolely attributable to the algorithm, as opposed to other sources such as\nsocietal inequality or human bias on the part of platform users. We use the\nproposed method to measure and quantify algorithmic bias with respect to gender\nof two algorithms used by LinkedIn, a popular online platform used by job\nseekers and employers. Moreover, we introduce a framework and the rationale for\ndistinguishing algorithmic bias from human bias, both of which can potentially\nexist on a two-sided platform where algorithms make recommendations to human\nusers. Finally, we discuss the shortcomings of a few other common algorithmic\nfairness metrics and why they do not capture the fairness notion of equal\nopportunity for equally qualified candidates.\n"
    },
    {
        "paper_id": 2202.07378,
        "authors": "Kathrin Hellmuth and Christian Klingenberg",
        "title": "Computing Black Scholes with Uncertain Volatility-A Machine Learning\n  Approach",
        "comments": null,
        "journal-ref": "Mathematics 10, no.3: 489 (2022)",
        "doi": "10.3390/math10030489",
        "license": "http://creativecommons.org/licenses/by-sa/4.0/",
        "abstract": "  In financial mathematics, it is a typical approach to approximate financial\nmarkets operating in discrete time by continuous-time models such as the Black\nScholes model. Fitting this model gives rise to difficulties due to the\ndiscrete nature of market data. We thus model the pricing process of financial\nderivatives by the Black Scholes equation, where the volatility is a function\nof a finite number of random variables. This reflects an influence of uncertain\nfactors when determining volatility. The aim is to quantify the effect of this\nuncertainty when computing the price of derivatives. Our underlying method is\nthe generalized Polynomial Chaos (gPC) method in order to numerically compute\nthe uncertainty of the solution by the stochastic Galerkin approach and a\nfinite difference method. We present an efficient numerical variation of this\nmethod, which is based on a machine learning technique, the so-called\nBi-Fidelity approach. This is illustrated with numerical examples.\n"
    },
    {
        "paper_id": 2202.07442,
        "authors": "Akhil Rao, Giacomo Rondina",
        "title": "The Economics of Orbit Use: Open Access, External Costs, and Runaway\n  Debris Growth",
        "comments": "Revised draft. PDF includes appendix",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-sa/4.0/",
        "abstract": "  We present a dynamic physico-economic model of Earth orbit use with\nendogenous satellite collision risk to study conditions under which\ndebris-producing collisions between orbiting bodies result in debris growth\nthat may render Earth's orbits unusable, an outcome known as Kessler Syndrome.\nWe characterize the dynamics of objects in orbit under open access as well as\nwhen external costs -- the impact of an additional satellite launch on the\ncollision risk faced by all satellites -- are internalized, and we show that\nKessler Syndrome can emerge in both cases. Finally, we show that once the\neconomic incentives of satellite launching are modeled, for Kessler Syndrome to\nemerge, autocatalytic debris growth is essential. In our main calibration,\nKessler Syndrome can emerge anytime between the year 2040 and the year 2184,\nwith the precise date being very sensitive to the calibration of autocatalytic\ndebris growth parameters.\n"
    },
    {
        "paper_id": 2202.07478,
        "authors": "Fay\\c{c}al Drissi",
        "title": "Solvability of Differential Riccati Equations and Applications to\n  Algorithmic Trading with Signals",
        "comments": "30 pages, published",
        "journal-ref": "Applied Mathematical Finance, 29:6, 457-493, DOI:\n  10.1080/1350486X.2023.2241130",
        "doi": "10.1080/1350486X.2023.2241130",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We study a differential Riccati equation (DRE) with indefinite matrix\ncoefficients, which arises in a wide class of practical problems. We show that\nthe DRE solves an associated control problem, which is key to provide existence\nand uniqueness of a solution. As an application, we solve two algorithmic\ntrading problems in which the agent adopts a constant absolute risk-aversion\n(CARA) utility function, and where the optimal strategies use signals and past\nobservations of prices to improve their performance. First, we derive a\nmulti-asset market making strategy in over-the-counter markets, where the\nmarket maker uses an external trading venue to hedge risk. Second, we derive an\noptimal trading strategy that uses prices and signals to learn the drift in the\nasset prices.\n"
    },
    {
        "paper_id": 2202.07542,
        "authors": "Frido Rolloos",
        "title": "The ATM implied volatility slope, the (dual) volatility swap, and the\n  (dual) zero vanna implied volatility",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Exact relationships between the short time-to-maturity ATM implied volatility\nslope, the (dual) volatility swap, and the (dual) zero vanna implied volatility\nare given.\n"
    },
    {
        "paper_id": 2202.07564,
        "authors": "Barry Eichengreen and Ganesh Viswanath-Natraj",
        "title": "Stablecoins and Central Bank Digital Currencies: Policy and Regulatory\n  Challenges",
        "comments": null,
        "journal-ref": "Asian Economic Papers (2022): 1-18",
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Stablecoins and central bank digital currencies are on the horizon in Asia,\nand in some cases have already arrived. This paper provides new analysis and a\ncritique of the use case for both forms of digital currency. It provides\ntime-varying estimates of devaluation risk for the leading stablecoin, Tether,\nusing data from the futures market. It describes the formidable obstacles to\nwidespread use of central bank digital currencies in cross-border transactions,\nthe context in which their utility is arguably greatest. The bottom line is\nthat significant uncertainties continue to dog the region's digital currency\ninitiatives.\n"
    },
    {
        "paper_id": 2202.07609,
        "authors": "Dominic A. Smith and Sergio Ocampo",
        "title": "The Evolution of U.S. Retail Concentration",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Increases in national concentration have been a salient feature of industry\ndynamics in the U.S. and have contributed to concerns about increasing market\npower. Yet, local trends may be more informative about market power,\nparticularly in the retail sector where consumers have traditionally shopped at\nnearby stores. We find that local concentration has increased almost in\nparallel with national concentration using novel Census data on product-level\nrevenue for all U.S. retail stores between 1992 and 2012. The increases in\nconcentration are broad based, affecting most markets, products, and retail\nindustries. We show that the expansion of multi-market firms into new markets\nexplains most of the increase in national retail concentration, with\nconsolidation via increases in local market shares increasing in importance\nbetween 1997 and 2007, and single-market firms playing a negligible role.\nFinally, we find that increases in local concentration can explain one-quarter\nto one-third of the observed rise in retail gross margins.\n"
    },
    {
        "paper_id": 2202.0761,
        "authors": "Martin Herdegen and Nazem Khan",
        "title": "$\\rho$-arbitrage and $\\rho$-consistent pricing for star-shaped risk\n  measures",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper revisits mean-risk portfolio selection in a one-period financial\nmarket, where risk is quantified by a star-shaped risk measure $\\rho$. We make\nthree contributions. First, we introduce the new axiom of sensitivity to large\nexpected losses and show that it is key to ensure the existence of optimal\nportfolios. Second, we give primal and dual characterisations of (strong)\n$\\rho$-arbitrage. Finally, we use our conditions for the absence of (strong)\n$\\rho$-arbitrage to explicitly derive the (strong) $\\rho$-consistent price\ninterval for an external financial contract.\n"
    },
    {
        "paper_id": 2202.07689,
        "authors": "Chris Kenyon and Mourad Berrahoui and Andrea Macrina",
        "title": "Transparency principle for carbon emissions drives sustainable finance",
        "comments": "33 pages, 2 figures, 5 tables",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Alignment of financial market incentives and carbon emissions disincentives\nis key to limiting global warming. Regulators and standards bodies have made a\nstart by requiring some carbon-related disclosures and proposing others. Here\nwe go further and propose a Carbon Equivalence Principle: all financial\nproducts shall contain a description of the equivalent carbon flows from\ngreenhouse gases that the products enable, as well as their existing\ndescription in terms of cash flows. This description of the carbon flows\nenabled by the project shall be compatible with existing bank systems that\ntrack cashflows so that carbon flows have equal standing to cash flows. We\ndemonstrate that this transparency alone can align incentives by applying it to\nproject finance examples for power generation and by following through the\nfinancial analysis. The financial requirements to offset costs of carbon flows\nenabled in the future radically change project costs, and risk that assets\nbecome stranded, thus further increasing costs. This observation holds\nwhichever partner in the project bears the enabled-carbon costs. Mitigating\nthese risks requires project re-structuring to include negative emissions\ntechnologies. We also consider that sequestered carbon needs to remain\nsequestered permanently, e.g., for at least one hundred years. We introduce\nmixed financial-physical solutions to minimise this permanence cost, and price\nto them. This complements previous insurance-based proposals with lesser scope.\nFor financial viability we introduce project designs that are financially\nnet-zero, and as a consequence are carbon negative. Thus we see that adoption\nof the Carbon Equivalence Principle for financial products aligns incentives,\nrequires product redesign, and is simply good financial management driving\nsustainability.\n"
    },
    {
        "paper_id": 2202.07734,
        "authors": "Af\\c{s}ar Onat Ayd{\\i}nhan, Xiaoyue Li, John M. Mulvey",
        "title": "Solving Multi-Period Financial Planning Models: Combining Monte Carlo\n  Tree Search and Neural Networks",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  This paper introduces the MCTS algorithm to the financial world and focuses\non solving significant multi-period financial planning models by combining a\nMonte Carlo Tree Search algorithm with a deep neural network. The MCTS provides\nan advanced start for the neural network so that the combined method\noutperforms either approach alone, yielding competitive results. Several\ninnovations improve the computations, including a variant of the upper\nconfidence bound applied to trees (UTC) and a special lookup search. We compare\nthe two-step algorithm with employing dynamic programs/neural networks. Both\napproaches solve regime switching models with 50-time steps and transaction\ncosts with twelve asset categories. Heretofore, these problems have been\noutside the range of solvable optimization models via traditional algorithms.\n"
    },
    {
        "paper_id": 2202.07742,
        "authors": "Teng Andrea Xu, Jiahua Xu",
        "title": "A Short Survey on Business Models of Decentralized Finance (DeFi)\n  Protocols",
        "comments": null,
        "journal-ref": "FC 2022 Workshops, LNCS 13412",
        "doi": "10.1007/978-3-031-32415-4_13",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Decentralized Finance (DeFi) services are moving traditional financial\noperations to the Internet of Value (IOV) by exploiting smart contracts,\ndistributed ledgers, and clever heterogeneous transactions among different\nprotocols. The exponential increase of the Total Value Locked (TVL) in DeFi\nforeshadows a bright future for automated money transfers in a plethora of\nservices. In this short survey paper, we describe the business model for\ndifferent DeFi domains - namely, Protocols for Loanable Funds (PLFs),\nDecentralized Exchanges (DEXs), and Yield Aggregators. We claim that the\ncurrent state of the literature is still unclear how to value thousands of\ndifferent competitors (tokens) in DeFi. With this work, we abstract the general\nbusiness model for different DeFi domains and compare them. Finally, we provide\nopen research challenges that will involve heterogeneous domains such as\neconomics, finance, and computer science.\n"
    },
    {
        "paper_id": 2202.07771,
        "authors": "Kristof Wiedermann",
        "title": "An SMP-Based Algorithm for Solving the Constrained Utility Maximization\n  Problem via Deep Learning",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We consider the utility maximization problem under convex constraints with\nregard to theoretical results which allow the formulation of algorithmic\nsolvers which make use of deep learning techniques. In particular for the case\nof random coefficients, we prove a stochastic maximum principle (SMP), which\nalso holds for utility functions $U$ with $\\mathrm{id}_{\\mathbb{R}^{+}} \\cdot\nU'$ being not necessarily nonincreasing, like the power utility functions,\nthereby generalizing the SMP proved by Li and Zheng (2018). We use this SMP\ntogether with the strong duality property for defining a new algorithm, which\nwe call deep primal SMP algorithm. Numerical examples illustrate the\neffectiveness of the proposed algorithm - in particular for higher-dimensional\nproblems and problems with random coefficients, which are either path dependent\nor satisfy their own SDEs. Moreover, our numerical experiments for constrained\nproblems show that the novel deep primal SMP algorithm overcomes the deep SMP\nalgorithm's (see Davey and Zheng (2021)) weakness of erroneously producing the\nvalue of the corresponding unconstrained problem. Furthermore, in contrast to\nthe deep controlled 2BSDE algorithm from Davey and Zheng (2021), this algorithm\nis also applicable to problems with path dependent coefficients. As the deep\nprimal SMP algorithm even yields the most accurate results in many of our\nstudied problems, we can highly recommend its usage. Moreover, we propose a\nlearning procedure based on epochs which improved the results of our algorithm\neven further. Implementing a semi-recurrent network architecture for the\ncontrol process turned out to be also a valuable advancement.\n"
    },
    {
        "paper_id": 2202.07849,
        "authors": "Alexander Lipton and Artur Sepp",
        "title": "Toward an efficient hybrid method for pricing barrier options on assets\n  with stochastic volatility",
        "comments": "33 pages, 11 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  We combine the one-dimensional Monte Carlo simulation and the semi-analytical\none-dimensional heat potential method to design an efficient technique for\npricing barrier options on assets with correlated stochastic volatility. Our\napproach to barrier options valuation utilizes two loops. First we run the\nouter loop by generating volatility paths via the Monte Carlo method. Second,\nwe condition the price dynamics on a given volatility path and apply the method\nof heat potentials to solve the conditional problem in closed-form in the inner\nloop. We illustrate the accuracy and efficacy of our semi-analytical approach\nby comparing it with the two-dimensional Monte Carlo simulation and a hybrid\nmethod, which combines the finite-difference technique for the inner loop and\nthe Monte Carlo simulation for the outer loop. We apply our method for\ncomputation of state probabilities (Green function), survival probabilities,\nand values of call options with barriers. Our approach provides better accuracy\nand is orders of magnitude faster than the existing methods. s a by-product of\nour analysis, we generalize Willard's (1997) conditioning formula for valuation\nof path-independent options to path-dependent options and derive a novel\nexpression for the joint probability density for the value of drifted Brownian\nmotion and its running minimum.\n"
    },
    {
        "paper_id": 2202.07863,
        "authors": "David Ubilava, Justin V. Hastings, Kadir Atalay",
        "title": "Agricultural Windfalls and the Seasonality of Political Violence in\n  Africa",
        "comments": null,
        "journal-ref": "American Journal of Agricultural Economics 105 (2023) 1309-1332",
        "doi": "10.1111/ajae.12364",
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  When the prices of cereal grains rise, social unrest and conflict become\nlikely. In rural areas, the predation motives of perpetrators can explain the\npositive relationship between prices and conflict. Predation happens at places\nand in periods where and when spoils to be appropriated are available. In\npredominantly agrarian societies, such opportune times align with the harvest\nseason. Does the seasonality of agricultural income lead to the seasonality of\nconflict? We address this question by analyzing over 55 thousand incidents\ninvolving violence against civilians staged by paramilitary groups across\nAfrica during the 1997-2020 period. We investigate the crop year pattern of\nviolence in response to agricultural income shocks via changes in international\ncereal prices. We find that a year-on-year one standard deviation annual growth\nof the price of the major cereal grain results in a harvest-time spike in\nviolence by militias in a one-degree cell where this cereal grain is grown.\nThis translates to a nearly ten percent increase in violence during the early\npostharvest season. We observe no such change in violence by state forces or\nrebel groups--the other two notable actors. By further investigating the\nmechanisms, we show that the violence by militias is amplified after plausibly\nrich harvest seasons when the value of spoils to be appropriated is higher. By\nfocusing on harvest-related seasonality of conflict, as well as actors more\nlikely to be involved in violence against civilians, we contribute to the\ngrowing literature on the economic causes of conflict in predominantly agrarian\nsocieties.\n"
    },
    {
        "paper_id": 2202.08102,
        "authors": "Raffaele Mosca",
        "title": "A note on hospital financing: local financing vs. central financing",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  This note tries to study how hospital behaviors, with reference to\ninterhospital collaboration or competition, could be affected by hospital\nfinancing systems. For that this note simulates two scenarios which start with\nthe following baseline scenario: a State, with a set of hospitals, each with\nall types of wards at a basic level. The evolution of this baseline scenario\nconsists in the evolution of hospitals, that is, in the possibility of\nhospitals to make some of their wards excel. The State has a budget, for the\nevolution of this baseline scenario, which can be used by two financing\nsystems: either by a \"local financing\", i.e., by splitting the budget among the\nhospitals so that each hospital is managing its own portion of the budget by\npursuing the individual benefit, or by a \"central financing\", i.e., by not\nsplitting the budget among the hospitals so that the State is the sole manager\nof the budget, by pursuing the benefit of the whole community. The conclusions\nseem to be that: in the local financing system hospitals tend to diversify\ntheir excellences, while in the central financing system the State tends to\ncreate poles of excellence.\n"
    },
    {
        "paper_id": 2202.08148,
        "authors": "Matt Davison, Marcos Escobar-Anel, and Yichen Zhu",
        "title": "Optimal market completion through financial derivatives with\n  applications to volatility risk",
        "comments": "16 pages, 5 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  This paper investigates the optimal choices of financial derivatives to\ncomplete a financial market in the framework of stochastic volatility (SV)\nmodels. We introduce an efficient and accurate simulation-based method,\napplicable to generalized diffusion models, to approximate the optimal\nderivatives-based portfolio strategy. We build upon the double optimization\napproach (i.e. expected utility maximization and risk exposure minimization)\nproposed in Escobar-Anel et al. (2022); demonstrating that strangle options are\nthe best choices for market completion within equity options. Furthermore, we\nexplore the benefit of using volatility index derivatives and conclude that\nthey could be more convenient substitutes when only long-term maturity equity\noptions are available.\n"
    },
    {
        "paper_id": 2202.08564,
        "authors": "Dimitrios Tsiotas",
        "title": "A 3D index for measuring economic resilience with application to the\n  modern international and global financial crises",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The study and measurement of economic resilience is ruled by high level of\ncomplexity related to the diverse structure, functionality, spatiality, and\ndynamics describing economic systems. Towards serving the demand of\nintegration, this paper develops a three-dimensional index, capturing\nengineering, ecological, and evolutionary aspects of economic resilience that\nare considered separately in the current literature. The proposed index is\ncomputed on GDP data of worldwide countries, for the period 1960-2020,\nconcerning 14 crises considered as shocks, and was found well defined in a\nconceptual context of its components. Its application on real-world data allows\nintroducing a novel classification of countries in terms of economic\nresilience, and reveals geographical patterns and structural determinants of\nthis attribute. Impressively enough, economic resilience appears positively\nrelated to major productivity coefficients, gravitationally driven, and\ndepended on agricultural specialization, with high structural heterogeneity in\nthe low class. Also, the analysis fills the literature gap by shaping the\nworldwide map of economic resilience, revealing geographical duality and\ncentrifugal patterns in its geographical distribution, a relationship between\ndiachronically good performance in economic resilience and geographical\ndistance from the shocks origin, and a continent differentiation expressed by\nthe specialization of America in engineering resilience, Africa and Asia in\necological and evolutionary resilience, and a relative lag of Europe and\nOceania. Finally, the analysis provides insights into the effect of the 2008 on\nthe globe and supports a further research hypothesis that political instability\nis a main determinant of low economic resilience, addressing avenues of further\nresearch.\n"
    },
    {
        "paper_id": 2202.0859,
        "authors": "Siddhartha Paul Tiwari",
        "title": "Emerging trends in soybean industry",
        "comments": null,
        "journal-ref": "Soybean Research 15.1 (2017): 1-17",
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-sa/4.0/",
        "abstract": "  Soybean is the most globalized, traded and processed crop commodity. USA,\nArgentina and Brazil continue to be the top three producers and exporters of\nsoybean and soymeal. Indian soyindustry has also made a mark in the national\nand global arena. While soymeal, soyoil, lecithin and other soy-derivatives\nstand to be driven up by commerce, the soyfoods for human health and nutrition\nneed to be further promoted. The changing habitat of commerce in soyderivatives\nnecessitates a shift in strategy, technological tools and policy environment to\nmake Indian soybean industry continue to thrive in the new industrial era.\nTerms of trade for soyfarming and soy-industry could be further improved.\nPresent trends, volatilities, slowdowns, challenges faced and associated\ndesiderata are accordingly spelt out in the present article.\n"
    },
    {
        "paper_id": 2202.08822,
        "authors": "Fabian Schueler, Dimitri Petrik",
        "title": "Objectives of platform research: A co-citation and systematic literature\n  review analysis",
        "comments": "ZfbF Sonderheft 75/20 (Schmalenbach Business Review)",
        "journal-ref": "Management Digitaler Plattformen pp 1-33 (2021)",
        "doi": "10.1007/978-3-658-31118-6_1",
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Business economics research on digital platforms often overlooks existing\nknowledge from other fields of research leading to conceptual ambiguity and\ninconsistent findings. To reduce these restrictions and foster the utilization\nof the extensive body of literature, we apply a mixed methods design to\nsummarize the key findings of scientific platform research. Our bibliometric\nanalysis identifies 14 platform-related research fields. Conducting a\nsystematic qualitative content analysis, we identify three primary research\nobjectives related to platform ecosystems: (1) general literature defining and\nunifying research on platforms; (2) exploitation of platform and ecosystem\nstrategies; (3) improvement of platforms and ecosystems. Finally, we discuss\nthe identified insights from a business economics perspective and present\npromising future research directions that could enhance business economics and\nmanagement research on digital platforms and platform ecosystems.\n"
    },
    {
        "paper_id": 2202.08921,
        "authors": "Alejandro Rodriguez Dominguez",
        "title": "Portfolio Optimization based on Neural Networks Sensitivities from\n  Assets Dynamics respect Common Drivers",
        "comments": "33 pages, 14figures, Preprint",
        "journal-ref": "Machine Learning with Applications, Volume 11, 2023, 100447, ISSN\n  2666-8270",
        "doi": "10.1016/j.mlwa.2022.100447",
        "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
        "abstract": "  We present a framework for modeling asset and portfolio dynamics,\nincorporating this information into portfolio optimization. For this framework,\nwe introduce the Commonality Principle, providing a solution for the optimal\nselection of portfolio drivers as the common drivers. Portfolio constituent\ndynamics are modeled by Partial Differential Equations, and solutions\napproximated with neural networks. Sensitivities with respect to the common\ndrivers are obtained via Automatic Adjoint Differentiation. Information on\nasset dynamics is incorporated via sensitivities into portfolio optimization.\nPortfolio constituents are embedded into the space of sensitivities with\nrespect to their common drivers, and a distance matrix in this space called the\nSensitivity matrix is used to solve the convex optimization for\ndiversification. The sensitivity matrix measures the similarity of the\nprojections of portfolio constituents on a vector space formed by common\ndrivers' returns and is used to optimize for diversification on both\nidiosyncratic and systematic risks while adding directionality and future\nbehavior information via returns dynamics. For portfolio optimization, we\nperform hierarchical clustering on the sensitivity matrix. To the best of the\nauthor's knowledge, this is the first time that sensitivities' dynamics\napproximated with neural networks have been used for portfolio optimization.\nSecondly, that hierarchical clustering on a matrix of sensitivities is used to\nsolve the convex optimization problem and incorporate the hierarchical\ninformation of these sensitivities. Thirdly, public and listed variables can be\nused to obtain maximum idiosyncratic and systematic diversification by means of\nthe sensitivity space with respect to optimal portfolio drivers. We reach\nover-performance in many experiments with respect to all other out-of-sample\nmethods for different markets and real datasets.\n"
    },
    {
        "paper_id": 2202.08962,
        "authors": "Chao Zhang, Yihuang Zhang, Mihai Cucuringu, Zhongmin Qian",
        "title": "Volatility forecasting with machine learning and intraday commonality",
        "comments": "40 pages, 12 figures, 6 tables; to appear in Journal of Financial\n  Econometrics",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  We apply machine learning models to forecast intraday realized volatility\n(RV), by exploiting commonality in intraday volatility via pooling stock data\ntogether, and by incorporating a proxy for the market volatility. Neural\nnetworks dominate linear regressions and tree-based models in terms of\nperformance, due to their ability to uncover and model complex latent\ninteractions among variables. Our findings remain robust when we apply trained\nmodels to new stocks that have not been included in the training set, thus\nproviding new empirical evidence for a universal volatility mechanism among\nstocks. Finally, we propose a new approach to forecasting one-day-ahead RVs\nusing past intraday RVs as predictors, and highlight interesting time-of-day\neffects that aid the forecasting mechanism. The results demonstrate that the\nproposed methodology yields superior out-of-sample forecasts over a strong set\nof traditional baselines that only rely on past daily RVs.\n"
    },
    {
        "paper_id": 2202.08966,
        "authors": "Hugo Schnoering and Hugo Inzirillo",
        "title": "Constructing a NFT Price Index and Applications",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-sa/4.0/",
        "abstract": "  We are witnessing the emergence of a new digital art market, the art market\n3.0. Blockchain technology has taken on a new sector which is still not well\nknown, Non-Fungible tokens (NFT). In this paper we propose a new methodology to\nbuild a NFT Price Index that represents this new market on the whole. In\naddition, this index will allow us to have a look on the dynamics and\nperformances of NFT markets, and to diagnose them.\n"
    },
    {
        "paper_id": 2202.08967,
        "authors": "Zeyd Boukhers and Azeddine Bouabdallah and Matthias Lohr and Jan\n  J\\\"urjens",
        "title": "Ensemble and Multimodal Approach for Forecasting Cryptocurrency Price",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  Since the birth of Bitcoin in 2009, cryptocurrencies have emerged to become a\nglobal phenomenon and an important decentralized financial asset. Due to this\ndecentralization, the value of these digital currencies against fiat currencies\nis highly volatile over time. Therefore, forecasting the crypto-fiat currency\nexchange rate is an extremely challenging task. For reliable forecasting, this\npaper proposes a multimodal AdaBoost-LSTM ensemble approach that employs all\nmodalities which derive price fluctuation such as social media sentiments,\nsearch volumes, blockchain information, and trading data. To better support\ninvestment decision making, the approach forecasts also the fluctuation\ndistribution. The conducted extensive experiments demonstrated the\neffectiveness of relying on multimodalities instead of only trading data.\nFurther experiments demonstrate the outperformance of the proposed approach\ncompared to existing tools and methods with a 19.29% improvement.\n"
    },
    {
        "paper_id": 2202.08968,
        "authors": "Rian Dolphin, Barry Smyth, Ruihai Dong",
        "title": "Stock Embeddings: Learning Distributed Representations for Financial\n  Assets",
        "comments": "Currently under review. 9 pages, 4 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Identifying meaningful relationships between the price movements of financial\nassets is a challenging but important problem in a variety of financial\napplications. However with recent research, particularly those using machine\nlearning and deep learning techniques, focused mostly on price forecasting, the\nliterature investigating the modelling of asset correlations has lagged\nsomewhat. To address this, inspired by recent successes in natural language\nprocessing, we propose a neural model for training stock embeddings, which\nharnesses the dynamics of historical returns data in order to learn the nuanced\nrelationships that exist between financial assets. We describe our approach in\ndetail and discuss a number of ways that it can be used in the financial\ndomain. Furthermore, we present the evaluation results to demonstrate the\nutility of this approach, compared to several important benchmarks, in two\nreal-world financial analytics tasks.\n"
    },
    {
        "paper_id": 2202.09116,
        "authors": "Claudio Fontana",
        "title": "Caplet pricing in affine models for alternative risk-free rates",
        "comments": "19 pages (a shortened version is forthcoming in SIAM J. Financ.\n  Math.)",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Alternative risk-free rates (RFRs) play a central role in the reform of\ninterest rate benchmarks. We study a model for RFRs driven by a general affine\nprocess. Under minimal assumptions, we derive explicit valuation formulas for\nforward-looking and backward-looking caplets/floorlets, term-basis caplets as\nwell as 1-month and 3-month RFR futures contracts.\n"
    },
    {
        "paper_id": 2202.09323,
        "authors": "Victor Olkhov",
        "title": "Market-Based Price Autocorrelation",
        "comments": "13 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper assumes that the randomness of market trade values and volumes\ndetermines the properties of stochastic market prices. We derive the direct\ndependence of the first two price statistical moments and price volatility on\nstatistical moments, volatilities, and correlations of market trade values and\nvolumes. That helps describe the dependence of market-based price\nautocorrelation between times t and t-{\\tau} on statistical moments and\ncorrelations between trade values and volumes. That highlights the impact of\nthe randomness of the size of market deals on price statistical moments and\nautocorrelation. Statistical moments and correlations of market trade values\nand volumes are assessed by conventional frequency-based probabilities. The\ndistinctions between market-based price autocorrelation and autocorrelation\nthat are assessed by the frequency-based probability analysis of price time\nseries reveal the different approaches to the definitions of price\nprobabilities. To forecast market-based price autocorrelation, one should\npredict the statistical moments and correlations of trade values and volumes.\n"
    },
    {
        "paper_id": 2202.09359,
        "authors": "Gurjeet Singh",
        "title": "Machine Learning Models in Stock Market Prediction",
        "comments": null,
        "journal-ref": null,
        "doi": "10.35940/ijitee.C9733.0111322",
        "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
        "abstract": "  The paper focuses on predicting the Nifty 50 Index by using 8 Supervised\nMachine Learning Models. The techniques used for empirical study are Adaptive\nBoost (AdaBoost), k-Nearest Neighbors (kNN), Linear Regression (LR), Artificial\nNeural Network (ANN), Random Forest (RF), Stochastic Gradient Descent (SGD),\nSupport Vector Machine (SVM) and Decision Trees (DT). Experiments are based on\nhistorical data of Nifty 50 Index of Indian Stock Market from 22nd April, 1996\nto 16th April, 2021, which is time series data of around 25 years. During the\nperiod there were 6220 trading days excluding all the non trading days. The\nentire trading dataset was divided into 4 subsets of different size-25% of\nentire data, 50% of entire data, 75% of entire data and entire data. Each\nsubset was further divided into 2 parts-training data and testing data. After\napplying 3 tests- Test on Training Data, Test on Testing Data and Cross\nValidation Test on each subset, the prediction performance of the used models\nwere compared and after comparison, very interesting results were found. The\nevaluation results indicate that Adaptive Boost, k- Nearest Neighbors, Random\nForest and Decision Trees under performed with increase in the size of data\nset. Linear Regression and Artificial Neural Network shown almost similar\nprediction results among all the models but Artificial Neural Network took more\ntime in training and validating the model. Thereafter Support Vector Machine\nperformed better among rest of the models but with increase in the size of data\nset, Stochastic Gradient Descent performed better than Support Vector Machine.\n"
    },
    {
        "paper_id": 2202.0948,
        "authors": "Mukund Sundararajan (Google), Walid Krichene (Google Research)",
        "title": "Reciprocity in Machine Learning",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Machine learning is pervasive. It powers recommender systems such as Spotify,\nInstagram and YouTube, and health-care systems via models that predict sleep\npatterns, or the risk of disease. Individuals contribute data to these models\nand benefit from them. Are these contributions (outflows of influence) and\nbenefits (inflows of influence) reciprocal? We propose measures of outflows,\ninflows and reciprocity building on previously proposed measures of training\ndata influence. Our initial theoretical and empirical results indicate that\nunder certain distributional assumptions, some classes of models are\napproximately reciprocal. We conclude with several open directions.\n"
    },
    {
        "paper_id": 2202.0977,
        "authors": "Matyas Barczy, Fanni K. Ned\\'enyi, L\\'aszl\\'o S\\\"ut\\H{o}",
        "title": "Probability equivalent level of Value at Risk and higher-order Expected\n  Shortfalls",
        "comments": "45 pages",
        "journal-ref": "Insurance: Mathematics and Economics 108, (2023), 107-128",
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We investigate the probability equivalent level of Value at Risk and\n$n^{\\mathrm{th}}$-order Expected Shortfall (called PELVE_n), which can be\nconsidered as a variant of the notion of the probability equivalent level of\nValue at Risk and Expected Shortfall (called PELVE) due to Li and Wang (2022).\nWe study the finiteness, uniqueness and several properties of PELVE_n, we\ncalculate PELVE_n of some notable distributions, PELVE_2 of a random variable\nhaving generalized Pareto excess distribution, and we describe the asymptotic\nbehaviour of PELVE_2 of regularly varying distributions as the level tends to\n$0$. Some properties of $n^{\\mathrm{th}}$-order Expected Shortfall are also\ninvestigated. Among others, it turns out that the Gini Shortfall at some level\n$p\\in[0,1)$ corresponding to a (loading) parameter $\\lambda\\geq 0$ is the\nlinear combination of the Expected Shortfall at level $p$ and the\n$2^{\\mathrm{nd}}$-order Expected Shortfall at level $p$ with coefficients\n$1-2\\lambda$ and $2\\lambda$, respectively.\n"
    },
    {
        "paper_id": 2202.0978,
        "authors": "Sebastian Cassel",
        "title": "Fast high-dimensional integration using tensor networks",
        "comments": "8 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The design and application of regression-free tensor network representations\nfor integration is presented. Tensor network methods are demonstrated to\noutperform Monte Carlo for test problems, and exponential convergence is shown\nto be achievable for a non-analytic integrand.\n"
    },
    {
        "paper_id": 2202.09845,
        "authors": "Toshiko Matsui, Ali Al-Ali, William J. Knottenbelt",
        "title": "On the Dynamics of Solid, Liquid and Digital Gold Futures",
        "comments": "5 pages (incl. references)",
        "journal-ref": "2022 IEEE International Conference on Blockchain and\n  Cryptocurrency (ICBC)",
        "doi": "10.1109/ICBC54727.2022.9805528",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper examines the determinants of the volatility of futures prices and\nbasis for three commodities: gold, oil and bitcoin -- often dubbed solid,\nliquid and digital gold -- by using contract-by-contract analysis which has\nbeen previously applied to crude oil futures volatility investigations. By\nextracting the spot and futures daily prices as well as the maturity, trading\nvolume and open interest data for the three assets from 18th December 2017 to\n30th November 2021, we find a positive and significant role for trading volume\nand a possible negative influence of open interest, when significant, in\nshaping the volatility in all three assets, supporting earlier findings in the\ncontext of oil futures. Additionally, we find maturity has a relatively\npositive significance for bitcoin and oil futures price volatility.\nFurthermore, our analysis demonstrates that maturity affects the basis of\nbitcoin and gold positively -- confirming the general theory that the basis\nconverges to zero as maturity nears for bitcoin and gold -- while oil is\naffected in both directions.\n"
    },
    {
        "paper_id": 2202.09939,
        "authors": "Yusuke Uchiyama and Kei Nakagawa",
        "title": "Schr\\\"{o}dinger Risk Diversification Portfolio",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  The mean-variance portfolio that considers the trade-off between expected\nreturn and risk has been widely used in the problem of asset allocation for\nmulti-asset portfolios. However, since it is difficult to estimate the expected\nreturn and the out-of-sample performance of the mean-variance portfolio is\npoor, risk-based portfolio construction methods focusing only on risk have been\nproposed, and are attracting attention mainly in practice. In terms of risk,\nasset fluctuations that make up the portfolio are thought to have common\nfactors behind them, and principal component analysis, which is a dimension\nreduction method, is applied to extract the factors. In this study, we propose\nthe Schr\\\"{o}dinger risk diversification portfolio as a factor risk\ndiversifying portfolio using Schr\\\"{o}dinger principal component analysis that\napplies the Schr\\\"{o}dinger equation in quantum mechanics. The Schr\\\"{o}dinger\nprincipal component analysis can accurately estimate the factors even if the\nsample points are unequally spaced or in a small number, thus we can make\nefficient risk diversification. The proposed method was verified to outperform\nthe conventional risk parity and other risk diversification portfolio\nconstructions.\n"
    },
    {
        "paper_id": 2202.0997,
        "authors": "Lin Han, Ivor Cribben and Stefan Trueck",
        "title": "Extremal Dependence in Australian Electricity Markets",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Electricity markets are significantly more volatile than other comparable\nfinancial or commodity markets. Extreme price outcomes and their transmission\nbetween regions pose significant risks for market participants. We examine the\ndependence between extreme spot price outcomes in the Australian National\nElectricity Market (NEM). We investigate extremal dependence both in a\nunivariate and multivariate setting, applying the extremogram developed by\nDavis and Mikosch (2009) and Davis et al. (2011, 2012). We measure the\npersistence of extreme prices within individual regional markets and the\ntransmission of extreme prices across different regions. With both 5-minute and\n30-minute price data, we find that extreme prices are more persistent in the\nmarket with a higher share of intermittent renewable energy. We also find that\npersistence of extreme prices is more prevalent in more concentrated markets.\nWe also show significant extremal price dependence between different regions,\nwhich is typically stronger between physically interconnected markets. The\ndependence structure of extreme prices shows asymmetric and time-dependent\npatterns. Applying the extremograms, we further show the effectiveness of the\nAustralian Energy Market Commission's 2016 rebidding rule with respect to\nreducing the share of isolated price spikes that are often considered as an\nindication of strategic bidding. Our results provide important information for\nhedging decisions of market participants and for policy makers who aim to\nreduce market volatility and extreme price outcomes through effective\nregulations which guide the trading behaviour of market participants as well as\nimproved network interconnections.\n"
    },
    {
        "paper_id": 2202.10122,
        "authors": "Jan Balaguer, Raphael Koster, Ari Weinstein, Lucy Campbell-Gillingham,\n  Christopher Summerfield, Matthew Botvinick, Andrea Tacchetti",
        "title": "HCMD-zero: Learning Value Aligned Mechanisms from Data",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Artificial learning agents are mediating a larger and larger number of\ninteractions among humans, firms, and organizations, and the intersection\nbetween mechanism design and machine learning has been heavily investigated in\nrecent years. However, mechanism design methods often make strong assumptions\non how participants behave (e.g. rationality), on the kind of knowledge\ndesigners have access to a priori (e.g. access to strong baseline mechanisms),\nor on what the goal of the mechanism should be (e.g. total welfare). Here we\nintroduce HCMD-zero, a general purpose method to construct mechanisms making\nnone of these three assumptions. HCMD-zero learns to mediate interactions among\nparticipants and adjusts the mechanism parameters to make itself more likely to\nbe preferred by participants. It does so by remaining engaged in an electoral\ncontest with copies of itself, thereby accessing direct feedback from\nparticipants. We test our method on a stylized resource allocation game that\nhighlights the tension between productivity, equality and the temptation to\nfree ride. HCMD-zero produces a mechanism that is preferred by human\nparticipants over a strong baseline, it does so automatically, without\nrequiring prior knowledge, and using human behavioral trajectories sparingly\nand effectively. Our analysis shows HCMD-zero consistently makes the mechanism\npolicy more and more likely to be preferred by human participants over the\ncourse of training, and that it results in a mechanism with an interpretable\nand intuitive policy.\n"
    },
    {
        "paper_id": 2202.10135,
        "authors": "Jan Balaguer, Raphael Koster, Christopher Summerfield, Andrea\n  Tacchetti",
        "title": "The Good Shepherd: An Oracle Agent for Mechanism Design",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  From social networks to traffic routing, artificial learning agents are\nplaying a central role in modern institutions. We must therefore understand how\nto leverage these systems to foster outcomes and behaviors that align with our\nown values and aspirations. While multiagent learning has received considerable\nattention in recent years, artificial agents have been primarily evaluated when\ninteracting with fixed, non-learning co-players. While this evaluation scheme\nhas merit, it fails to capture the dynamics faced by institutions that must\ndeal with adaptive and continually learning constituents. Here we address this\nlimitation, and construct agents (\"mechanisms\") that perform well when\nevaluated over the learning trajectory of their adaptive co-players\n(\"participants\"). The algorithm we propose consists of two nested learning\nloops: an inner loop where participants learn to best respond to fixed\nmechanisms; and an outer loop where the mechanism agent updates its policy\nbased on experience. We report the performance of our mechanism agents when\npaired with both artificial learning agents and humans as co-players. Our\nresults show that our mechanisms are able to shepherd the participants\nstrategies towards favorable outcomes, indicating a path for modern\ninstitutions to effectively and automatically influence the strategies and\nbehaviors of their constituents.\n"
    },
    {
        "paper_id": 2202.10189,
        "authors": "Pavel V. Shevchenko (1), Jiwook Jang (1), Matteo Malavasi (1), Gareth\n  W. Peters (2), Georgy Sofronov (3) and Stefan Tr\\\"uck (1) ((1) Department of\n  Actuarial Studies and Business Analytics, Macquarie Business School,\n  Macquarie University, Sydney, Australia, (2) Department of Statistics and\n  Applied Probability, College of Letters and Science, University of California\n  Santa Barbara, Santa Barbara, California USA, (3) School of Mathematical and\n  Physical Sciences, Faculty of Science and Engineering, Macquarie University,\n  Sydney, Australia)",
        "title": "The Nature of Losses from Cyber-Related Events: Risk Categories and\n  Business Sectors",
        "comments": "24 pages, 10 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this study we examine the nature of losses from cyber related events\nacross different risk categories and business sectors. Using a leading industry\ndataset of cyber events, we evaluate the relationship between the frequency and\nseverity of individual cyber-related events and the number of affected records.\nWe find that the frequency of reported cyber related events has substantially\nincreased between 2008 and 2016. Furthermore, the frequency and severity of\nlosses depend on the business sector and type of cyber threat: the most\nsignificant cyber loss event categories, by number of events, were related to\ndata breaches and the unauthorized disclosure of data, while cyber extortion,\nphishing, spoofing and other social engineering practices showed substantial\ngrowth rates. Interestingly, we do not find a distinct pattern between the\nfrequency of events, the loss severity, and the number of affected records as\noften alluded to in the literature. We also analyse the severity distribution\nof cyber related events across all risk categories and business sectors. This\nanalysis reveals that cyber risks are heavy-tailed, i.e., cyber risk events\nhave a higher probability to produce extreme losses than events whose severity\nfollows an exponential distribution. Furthermore, we find that the frequency\nand severity of cyber related losses exhibits a very dynamic and time varying\nnature.\n"
    },
    {
        "paper_id": 2202.10265,
        "authors": "Bernhard K. Meister, Henry C. W. Price",
        "title": "Yields: The Galapagos Syndrome Of Cryptofinance",
        "comments": "15 pages, 8 figures, book chapter",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  In this chapter structures that generate yield in cryptofinance will be\nanalyzed and related to leverage. While the majority of crypto-assets do not\nhave intrinsic yields in and of themselves, similar to cash holdings of fiat\ncurrency, revolutionary innovation based on smart contracts, which enable\ndecentralised finance, does generate return. Examples include lending or\nproviding liquidity to an automated market maker on a decentralised exchange,\nas well as performing block formation in a proof of stake blockchain. On\ncentralised exchanges, perpetual and finite duration futures can trade at a\npremium or discount to the spot market for extended periods with one side of\nthe transaction earning a yield. Disparities in yield exist between products\nand venues as a result of market segmentation and risk profile differences.\nCryptofinance was initially shunned by legacy finance and developed\nindependently. This led to curious and imaginative adaptions, reminiscent of\nDarwin's finches, including stable coins for dollar transfers, perpetuals for\nleverage, and a new class of exchanges for trading and investment.\n"
    },
    {
        "paper_id": 2202.1034,
        "authors": "Bernhard K. Meister, Henry C.W. Price",
        "title": "Darwin Among the Cryptocurrencies",
        "comments": "7 pages, 3 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  The paper highlights some commonalities between the development of\ncryptocurrencies and the evolution of ecosystems. Concepts from evolutionary\nfinance embedded in toy models consistent with stylized facts are employed to\nunderstand what survival of the fittest means in cryptofinance. Stylized facts\nfor ownership, trading volume and market capitalization of cryptocurrencies are\nselectively presented in terms of scaling laws.\n"
    },
    {
        "paper_id": 2202.10344,
        "authors": "Sonia Oreffice and Dario Sansone",
        "title": "Commuting to work and gender-conforming social norms: evidence from\n  same-sex couples",
        "comments": "arXiv admin note: text overlap with arXiv:2107.06210",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We analyze work commute time by sexual orientation of partnered or married\nindividuals, using the American Community Survey 2008-2019. Women in same-sex\ncouples have a longer commute to work than working women in different-sex\ncouples, whereas the commute to work of men in same-sex couples is shorter than\nthe one of working men in different-sex couples, also after controlling for\ndemographic characteristics, partner characteristics, location, fertility, and\nmarital status. These differences are particularly stark among married couples\nwith children: on average, about 3 minutes more one-way to work for married\nmothers in same-sex couples, and almost 2 minutes less for married fathers in\nsame-sex couples, than their corresponding working parents in different-sex\ncouples. These gaps among men and women amount to 50 percent, and 100 percent,\nrespectively, of the gender commuting gap estimated in the literature.\nWithin-couple gaps in commuting time are also significantly smaller in same-sex\ncouples. We interpret these differences as evidence that it is\ngender-conforming social norms boosted by parenthood that lead women in\ndifferent-sex couples to specialize into jobs with a shorter commute while\ntheir male partners or spouses hold jobs with a longer commute.\n"
    },
    {
        "paper_id": 2202.10347,
        "authors": "Giovanni Abramo, Francesca Apponi, Ciriaco Andrea D'Angelo",
        "title": "The geographic proximity effect on domestic cross-sector vis-a-vis\n  intra-sector research collaborations",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Geographic proximity is acknowledged to be a key factor in research\ncollaborations. Specifically, it can work as a possible substitute for\ninstitutional proximity. The present study investigates the relevance of the\n\"proximity\" effect for different types of national research collaborations. We\napply a bibliometric approach based on the Italian 2010-2017 scientific\nproduction indexed in the Web of Science. On such dataset, we apply statistical\ntools for analyzing if and to what extent geographical distance between\nco-authors in the byline of a publication varies across collaboration types,\nscientific disciplines, and along time. Results can inform policies aimed at\neffectively stimulating cross-sector collaborations, and also bear direct\npractical implications for research performance assessments.\n"
    },
    {
        "paper_id": 2202.10391,
        "authors": "Erhan Bayraktar, Tao Chen",
        "title": "Nonparametric Adaptive Robust Control Under Model Uncertainty",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We consider a discrete time stochastic Markovian control problem under model\nuncertainty. Such uncertainty not only comes from the fact that the true\nprobability law of the underlying stochastic process is unknown, but the\nparametric family of probability distributions which the true law belongs to is\nalso unknown. We propose a nonparametric adaptive robust control methodology to\ndeal with such problem. Our approach hinges on the following building concepts:\nfirst, using the adaptive robust paradigm to incorporate online learning and\nuncertainty reduction into the robust control problem; second, learning the\nunknown probability law through the empirical distribution, and representing\nuncertainty reduction in terms of a sequence of Wasserstein balls around the\nempirical distribution; third, using Lagrangian duality to convert the\noptimization over Wasserstein balls to a scalar optimization problem, and\nadopting a machine learning technique to achieve efficient computation of the\noptimal control. We illustrate our methodology by considering a utility\nmaximization problem. Numerical comparisons show that the nonparametric\nadaptive robust control approach is preferable to the traditional robust\nframeworks.\n"
    },
    {
        "paper_id": 2202.10413,
        "authors": "Florian Bourgey, Stefano De Marco, and Emmanuel Gobet",
        "title": "Weak approximations and VIX option price expansions in forward variance\n  curve models",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We provide explicit approximation formulas for VIX futures and options in\nforward variance models, with particular emphasis on the family of so-called\nBergomi models: the one-factor Bergomi model [Bergomi, Smile dynamics II, Risk,\n2005], the rough Bergomi model [Bayer, Friz, and Gatheral, Pricing under rough\nvolatility, Quantitative Finance, 16(6):887-904, 2016], and an enhanced version\nof the rough model that can generate realistic positive skew for VIX smiles --\nintroduced simultaneously by De Marco [Bachelier World Congress, 2018] and\nGuyon [Bachelier World Congress, 2018] on the lines of [Bergomi, Smile dynamics\nIII, Risk, 2008], that we refer to as 'mixed rough Bergomi model'. Following\nthe methodology set up in [Gobet and Miri, Weak approximation of averaged\ndiffusion processes. Stochastic Process.\\ Appl., 124(1):475-504, 2014], we\nderive weak approximations for the law of the VIX, leading to option price\napproximations under the form of explicit combinations of Black-Scholes prices\nand greeks. As new contributions, we cope with the fractional integration\nkernel appearing in rough models and treat the case of non-smooth payoffs, so\nto encompass VIX futures, call and put options. We stress that our approach\ndoes not rely on small-time asymptotics nor small-parameter (such as small\nvolatility-of-volatility) asymptotics, and can therefore be applied to any\noption maturity and a wide range of parameter configurations. Our results are\nillustrated by several numerical experiments and calibration tests to VIX\nmarket data.\n"
    },
    {
        "paper_id": 2202.10414,
        "authors": "Felix Dammann and Giorgio Ferrari",
        "title": "Optimal Execution with Multiplicative Price Impact and Incomplete\n  Information on the Return",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We study an optimal liquidation problem with multiplicative price impact in\nwhich the trend of the asset's price is an unobservable Bernoulli random\nvariable. The investor aims at selling over an infinite time-horizon a fixed\namount of assets in order to maximize a net expected profit functional, and\nlump-sum as well as singularly continuous actions are allowed. Our mathematical\nmodelling leads to a singular stochastic control problem featuring a\nfinite-fuel constraint and partial observation. We provide the complete\nanalysis of an equivalent three-dimensional degenerate problem under full\ninformation, whose state process is composed of the asset's price dynamics, the\namount of available assets in the portfolio, and the investor's belief about\nthe true value of the asset's trend. The optimal execution rule and the\nproblem's value function are expressed in terms of the solution to a truly\ntwo-dimensional optimal stopping problem, whose associated belief-dependent\nfree boundary $b$ triggers the investor's optimal selling rule. The curve $b$\nis uniquely determined through a nonlinear integral equation, for which we\nderive a numerical solution through an application of the Monte-Carlo method.\nThis allows us to understand the sensitivity of the problem's solution with\nrespect to the relevant model's parameters as well as the value of information\nin our model.\n"
    },
    {
        "paper_id": 2202.10588,
        "authors": "Gareth W. Peters (1), Matteo Malavasi (2), Georgy Sofronov (3), Pavel\n  V. Shevchenko (2), Stefan Tr\\\"uck (2) and Jiwook Jang (2) ((1) Statistics &\n  Applied Probability, University of California Santa Barbara, (2) Actuarial\n  Studies and Business Analytics, Macquarie University, Australia, (3)\n  Mathematical and Physical Sciences, Macquarie University, Australia)",
        "title": "Cyber Loss Model Risk Translates to Premium Mispricing and Risk\n  Sensitivity",
        "comments": "30 pages, 34 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We focus on model risk and risk sensitivity when addressing the insurability\nof cyber risk. The standard statistical approaches to assessment of\ninsurability and potential mispricing are enhanced in several aspects involving\nconsideration of model risk. Model risk can arise from model uncertainty, and\nparameters uncertainty. We demonstrate how to quantify the effect of model risk\nin this analysis by incorporating various robust estimators for key model\nparameter estimates that apply in both marginal and joint cyber risk loss\nprocess modelling. We contrast these robust techniques with standard methods\npreviously used in studying insurabilty of cyber risk. This allows us to\naccurately assess the critical impact that robust estimation can have on tail\nindex estimation for heavy tailed loss models, as well as the effect of robust\ndependence analysis when quantifying joint loss models and insurance portfolio\ndiversification. We argue that the choice of such methods is akin to a form of\nmodel risk and we study the risk sensitivity that arise from choices relating\nto the class of robust estimation adopted and the impact of the settings\nassociated with such methods on key actuarial tasks such as premium calculation\nin cyber insurance. Through this analysis we are able to address the question\nthat, to the best of our knowledge, no other study has investigated in the\ncontext of cyber risk: is model risk present in cyber risk data, and how does\nis it translate into premium mispricing? We believe our findings should\ncomplement existing studies seeking to explore insurability of cyber losses. In\norder to ensure our findings are based on realistic industry informed loss\ndata, we have utilised one of the leading industry cyber loss datasets obtained\nfrom Advisen, which represents a comprehensive data set on cyber monetary\nlosses, from which we form our analysis and conclusions.\n"
    },
    {
        "paper_id": 2202.10623,
        "authors": "Nick James and Max Menzies and Georg A. Gottwald",
        "title": "On financial market correlation structures and diversification benefits\n  across and within equity sectors",
        "comments": "Accepted manuscript. Minor edits and typos fixed since v1. Equal\n  contribution",
        "journal-ref": "Physica A: Statistical Mechanics and its Applications 604 (2022)\n  127682",
        "doi": "10.1016/j.physa.2022.127682",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We study how to assess the potential benefit of diversifying an equity\nportfolio by investing within and across equity sectors. We analyse 20 years of\nUS stock price data, which includes the global financial crisis (GFC) and the\nCOVID-19 market crash, as well as periods of financial stability, to determine\nthe `all weather' nature of equity portfolios. We establish that one may use\nthe leading eigenvalue of the cross-correlation matrix of log returns as well\nas graph-theoretic diagnostics such as modularity to quantify the collective\nbehaviour of the market or a subset of it. We confirm that financial crises are\ncharacterised by a high degree of collective behaviour of equities, whereas\nperiods of financial stability exhibit less collective behaviour. We argue that\nduring times of increased collective behaviour, risk reduction via sector-based\nportfolio diversification is ineffective. Using the degree of collectivity as a\nproxy for the benefit of diversification, we perform an extensive sampling of\nequity portfolios to confirm the old financial adage that 30-40 stocks provide\nsufficient diversification. Using hierarchical clustering, we discover a `best\nvalue' equity portfolio for diversification consisting of 36 equities sampled\nuniformly from 9 sectors. We further show that it is typically more beneficial\nto diversify across sectors rather than within. Our findings have implications\nfor cost-conscious retail investors seeking broad diversification across equity\nmarkets.\n"
    },
    {
        "paper_id": 2202.10685,
        "authors": "Patricio Dom\\'inguez, Nicol\\'as Grau, Dami\\'an Vergara",
        "title": "Discrimination Against Immigrants in the Criminal Justice System:\n  Evidence from Pretrial Detentions",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper tests for discrimination against immigrant defendants in the\ncriminal justice system in Chile using a decade of nationwide administrative\nrecords on pretrial detentions. Observational benchmark regressions show that\nimmigrant defendants are 8.6 percentage points less likely to be released\npretrial relative to Chilean defendants with similar proxies for pretrial\nmisconduct potential. Diagnostics for omitted variable bias -- including a\nnovel test to assess the quality of the proxy vector based on comparisons of\npretrial misconduct rates among released defendants -- suggest that the\ndiscrimination estimates are not driven by omitted variable bias and that, if\nanything, failing to fully account for differences in misconduct potential\nleads to an underestimation of discrimination. Our estimates suggest that\ndiscrimination stems from an informational problem because judges do not\nobserve criminal records in origin countries, with stereotypes and taste-based\ndiscrimination playing a role in the problem's resolution. We find that\ndiscrimination is especially large for drug offenses and that discrimination\nincreased after a recent immigration wave.\n"
    },
    {
        "paper_id": 2202.10721,
        "authors": "Benjamin Bruder, Nazar Kostyuchyk, Thierry Roncalli",
        "title": "Risk Parity Portfolios with Skewness Risk: An Application to Factor\n  Investing and Alternative Risk Premia",
        "comments": "48 pages, 23 figures, 21 tables",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  This article develops a model that takes into account skewness risk in risk\nparity portfolios. In this framework, asset returns are viewed as stochastic\nprocesses with jumps or random variables generated by a Gaussian mixture\ndistribution. This dual representation allows us to show that skewness and jump\nrisks are equivalent. As the mixture representation is simple, we obtain\nanalytical formulas for computing asset risk contributions of a given\nportfolio. Therefore, we define risk budgeting portfolios and derive existence\nand uniqueness conditions. We then apply our model to the\nequity/bond/volatility asset mix policy. When assets exhibit jump risks like\nthe short volatility strategy, we show that skewness-based risk parity\nportfolios produce better allocation than volatility-based risk parity\nportfolios. Finally, we illustrate how this model is suitable to manage the\nskewness risk of long-only equity factor portfolios and to allocate between\nalternative risk premia.\n"
    },
    {
        "paper_id": 2202.1076,
        "authors": "Alhonita Yatie (BSE)",
        "title": "Crypto-assets better safe-havens than Gold during Covid-19: The case of\n  European indices",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  As the first crisis faced by Crypto-assets, Covid-19 updated the debate about\ntheir safehaven properties. Our paper tries to analyze the safe-haven\nproperties of Crypto-assets and Gold for European assets. We find that Gold has\nnot been more efficient than Cryptoassets (Tether, Cardano and Dogecoin) as\nsafe-haven during the market crash due to Covid-19 in March 2020. We also found\nthat during the study period Bitcoin, Ethereum, Litecoin and Ripple were just\ndiversifiers for the European indices. Finally, Tether, Cardano and Dogecoin\nshowed hedging properties like Gold before and after the market crash.\n"
    },
    {
        "paper_id": 2202.10817,
        "authors": "Nikan Firoozye, Vincent Tan, Stefan Zohren",
        "title": "Canonical Portfolios: Optimal Asset and Signal Combination",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  This paper presents a novel framework for analyzing the optimal asset and\nsignal combination problem. Our approach builds upon the dynamic portfolio\nselection problem introduced by Brandt and Santa-Clara (2006) and consists of\ntwo stages. First, we reformulate their original investment problem into a\ntractable one that allows us to derive a closed-form expression for the optimal\nportfolio policy that is scalable to large cross-sectional financial\napplications. Second, we recast the problem of selecting a portfolio of\ncorrelated assets and signals into selecting a set of uncorrelated managed\nportfolios through the lens of Canonical Correlation Analysis of Hotelling\n(1936). The new investment environment of uncorrelated managed portfolios\noffers unique economic insights into the joint correlation structure of our\noptimal portfolio policy. We also operationalize our theoretical framework to\nbridge the gap between theory and practice, showcasing the improved performance\nof our proposed method over natural competing benchmarks.\n"
    },
    {
        "paper_id": 2202.10834,
        "authors": "Olli Palm\\'en",
        "title": "Macroeconomic Effect of Uncertainty and Financial Shocks: a non-Gaussian\n  VAR approach",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  The Great Recession highlighted the role of financial and uncertainty shocks\nas drivers of business cycle fluctuations. However, the fact that uncertainty\nshocks may affect economic activity by tightening financial conditions makes\nempirically distinguishing these shocks difficult. This paper examines the\nmacroeconomic effects of the financial and uncertainty shocks in the United\nStates in an SVAR model that exploits the non-normalities of the time series to\nidentify the uncertainty and the financial shock. The results show that\nmacroeconomic uncertainty and financial shocks seem to affect business cycles\nindependently as well as through dynamic interaction. Uncertainty shocks appear\nto tighten financial conditions, whereas there appears to be no causal\nrelationship between financial conditions and uncertainty. Moreover, the\nresults suggest that uncertainty shocks may have persistent effects on output\nand investment that last beyond the business cycle.\n"
    },
    {
        "paper_id": 2202.10918,
        "authors": "Zhengkun Li",
        "title": "Honour Thesis: A Joint Value at Risk and Expected Shortfall Combination\n  Framework and its Applications in the Cryptocurrency Market",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Value at risk and expected shortfall are increasingly popular tail risk\nmeasures in the financial risk management field. Both academia and financial\ninstitutions are working to improve tail risk forecasts in order to meet the\nrequirements of the Basel Capital Accord; it states that one purpose of risk\nmanagement and measuring risk accuracy is, since extreme movements cannot\nalways be avoided, financial institutions can prepare for these extreme returns\nby capital allocation, and putting aside the appropriate amount of capital so\nas to avoid default in times of extreme price or index movements. Forecast\ncombination has drawn much attention, as a combined forecast can outperform the\nindividual forecasts under certain conditions. We propose two methodology, one\nis a semiparametric combination framework that can jointly produce combined\nvalue at risk and expected shortfall forecasts, another one is a parametric\nregression framework named as Quantile-ES regression that can produce combined\nexpected shortfall forecasts. The favourability of the semiparametric\ncombination framework has been presented via an empirical study - application\nin cryptocurrency markets with high-frequency data where the necessity of risk\nmanagement application increases as the cryptocurrency market becomes more\npopular and mature. Additionally, the general framework of the parametric\nQuantile-ES regression has been presented via a simulation study, whereas it\nstill need to be improved in the future. The contributions of this work include\nbut are not limited to the enabling of the combination of expected shortfall\nforecasts and the application of risk management procedures in the\ncryptocurrency market with high-frequency data.\n"
    },
    {
        "paper_id": 2202.1106,
        "authors": "Giuseppe Genovese and Ashkan Nikeghbali and Nicola Serra and Gabriele\n  Visentin",
        "title": "Universal approximation of credit portfolio losses using Restricted\n  Boltzmann Machines",
        "comments": "39 pages, 27 figures; changed dataset, added new results, updated\n  affiliation",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-sa/4.0/",
        "abstract": "  We introduce a new portfolio credit risk model based on Restricted Boltzmann\nMachines (RBMs), which are stochastic neural networks capable of universal\napproximation of loss distributions. We test the model on an empirical dataset\nof default probabilities of 1'012 US companies and we show that it outperforms\ncommonly used parametric factor copula models -- such as the Gaussian or the t\nfactor copula models -- across several credit risk management tasks. In\nparticular, the model leads to better fits for the empirical loss distribution\nand more accurate risk measure estimations. We introduce an importance sampling\nprocedure which allows risk measures to be estimated at high confidence levels\nin a computationally efficient way and which is a substantial improvement over\nthe Monte Carlo techniques currently available for copula models. Furthermore,\nthe statistical factors extracted by the model admit an interpretation in terms\nof the underlying portfolio sector structure and provide practitioners with\nquantitative tools for the management of concentration risk. Finally, we show\nhow to use the model for stress testing by estimating stressed risk measures\n(e.g. stressed VaR) under various macroeconomic stress test scenarios, such as\nthose specified by the FRB's Dodd-Frank Act stress test.\n"
    },
    {
        "paper_id": 2202.11183,
        "authors": "John Stachurski",
        "title": "Systemic Risk in Financial Systems: Properties of Equilibria",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Eisenberg and Noe (2001) analyze systemic risk for financial institutions\nlinked by a network of liabilities. They show that the solution to their model\nis unique when the financial system is satisfies a regularity condition\ninvolving risk orbits. We show that this condition is not needed: a unique\nsolution always exists.\n"
    },
    {
        "paper_id": 2202.11285,
        "authors": "Zexuan Yin and Paolo Barucca",
        "title": "Neural Generalised AutoRegressive Conditional Heteroskedasticity",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  We propose Neural GARCH, a class of methods to model conditional\nheteroskedasticity in financial time series. Neural GARCH is a neural network\nadaptation of the GARCH 1,1 model in the univariate case, and the diagonal BEKK\n1,1 model in the multivariate case. We allow the coefficients of a GARCH model\nto be time varying in order to reflect the constantly changing dynamics of\nfinancial markets. The time varying coefficients are parameterised by a\nrecurrent neural network that is trained with stochastic gradient variational\nBayes. We propose two variants of our model, one with normal innovations and\nthe other with Students t innovations. We test our models on a wide range of\nunivariate and multivariate financial time series, and we find that the Neural\nStudents t model consistently outperforms the others.\n"
    },
    {
        "paper_id": 2202.11309,
        "authors": "Jun Lu",
        "title": "Exploring Classic Quantitative Strategies",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  The goal of this paper is to debunk and dispel the magic behind the black-box\nquantitative strategies. It aims to build a solid foundation on how and why the\ntechniques work. This manuscript crystallizes this knowledge by deriving from\nsimple intuitions, the mathematics behind the strategies. This tutorial doesn't\nshy away from addressing both the formal and informal aspects of quantitative\nstrategies. By doing so, it hopes to provide readers with a deeper\nunderstanding of these techniques as well as the when, the how and the why of\napplying these techniques. The strategies are presented in terms of both\nS\\&P500 and SH510300 data sets. However, the results from the tests are just\nexamples of how the methods work; no claim is made on the suggestion of real\nmarket positions.\n"
    },
    {
        "paper_id": 2202.11314,
        "authors": "Ludovic Tangpi and Xuchen Zhou",
        "title": "Optimal Investment in a Large Population of Competitive and\n  Heterogeneous Agents",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  This paper studies a stochastic utility maximization game under relative\nperformance concerns in finite agent and infinite agent settings, where a\ncontinuum of agents interact through a graphon (see definition below). We\nconsider an incomplete market model in which agents have CARA utilities, and we\nobtain characterizations of Nash equilibria in both the finite agent and\ngraphon paradigms. Under modest assumptions on the denseness of the interaction\ngraph among the agents, we establish convergence results for the Nash\nequilibria and optimal utilities of the finite player problem to the infinite\nplayer problem. This result is achieved as an application of a general backward\npropagation of chaos type result for systems of interacting forward-backward\nstochastic differential equations, where the interaction is heterogeneous and\nthrough the control processes, and the generator is of quadratic growth. In\naddition, characterizing the graphon game gives rise to a novel form of\ninfinite dimensional forward-backward stochastic differential equation of\nMckean-Vlasov type, for which we provide well-posedness results. An interesting\nconsequence of our result is the computation of the competition indifference\ncapital, i.e., the capital making an investor indifferent between whether or\nnot to compete.\n"
    },
    {
        "paper_id": 2202.11416,
        "authors": "David Evangelista, Yuri Saporito and Yuri Thamsten",
        "title": "Price formation in financial markets: a game-theoretic perspective",
        "comments": "Keywords: Price Formation; Optimal Trading; Mean-field Games; Finite\n  Population Games",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We propose two novel frameworks to study the price formation of an asset\nnegotiated in an order book. Specifically, we develop a game-theoretic model in\nmany-person games and mean-field games, considering costs stemming from limited\nliquidity. We derive analytical formulas for the formed price in terms of the\nrealized order flow. We also identify appropriate conditions that ensure the\nconvergence of the price we find in the finite population game to that of its\nmean-field counterpart. We numerically assess our results with a large\nexperiment using high-frequency data from ten stocks listed in the NASDAQ, a\nstock listed in B3 in Brazil, and a cryptocurrency listed in Binance.\n"
    },
    {
        "paper_id": 2202.11581,
        "authors": "German Rodikov, Nino Antulov-Fantulin",
        "title": "Can LSTM outperform volatility-econometric models?",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Volatility prediction for financial assets is one of the essential questions\nfor understanding financial risks and quadratic price variation. However,\nalthough many novel deep learning models were recently proposed, they still\nhave a \"hard time\" surpassing strong econometric volatility models. Why is this\nthe case? The volatility prediction task is of non-trivial complexity due to\nnoise, market microstructure, heteroscedasticity, exogenous and asymmetric\neffect of news, and the presence of different time scales, among others. In\nthis paper, we analyze the class of long short-term memory (LSTM) recurrent\nneural networks for the task of volatility prediction and compare it with\nstrong volatility-econometric models.\n"
    },
    {
        "paper_id": 2202.11606,
        "authors": "Fred Espen Benth, Nils Detering, Luca Galimberti",
        "title": "Pricing options on flow forwards by neural networks in Hilbert space",
        "comments": "21 pages, 5 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We propose a new methodology for pricing options on flow forwards by applying\ninfinite-dimensional neural networks. We recast the pricing problem as an\noptimization problem in a Hilbert space of real-valued function on the positive\nreal line, which is the state space for the term structure dynamics. This\noptimization problem is solved by facilitating a novel feedforward neural\nnetwork architecture designed for approximating continuous functions on the\nstate space. The proposed neural net is built upon the basis of the Hilbert\nspace. We provide an extensive case study that shows excellent numerical\nefficiency, with superior performance over that of a classical neural net\ntrained on sampling the term structure curves.\n"
    },
    {
        "paper_id": 2202.11785,
        "authors": "Stephanie von Hinke (School of Economics, University of Bristol,\n  Erasmus School of Economics, Erasmus University Rotterdam, Institute for\n  Fiscal Studies), Emil N. S{\\o}rensen (School of Economics, University of\n  Bristol)",
        "title": "The Long-Term Effects of Early-Life Pollution Exposure: Evidence from\n  the London Smog",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  This paper uses a large UK cohort to investigate the impact of early-life\npollution exposure on individuals' human capital and health outcomes in older\nage. We compare individuals who were exposed to the London smog in December\n1952 whilst in utero or in infancy to those born after the smog and those born\nat the same time but in unaffected areas. We find that those exposed to the\nsmog have substantially lower fluid intelligence and worse respiratory health,\nwith some evidence of a reduction in years of schooling.\n"
    },
    {
        "paper_id": 2202.12067,
        "authors": "Hediye Yarahmadi and Abbas Ali Saberi",
        "title": "A 2D Levy-flight model for the complex dynamics of real-life financial\n  markets",
        "comments": "5 figures, 9 pages",
        "journal-ref": "Chaos 32, 033113 (2022)",
        "doi": "10.1063/5.0082926",
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  We report on the emergence of scaling laws in the temporal evolution of the\ndaily closing values of the S\\&P 500 index prices and its modeling based on the\nL\\'evy flights in two dimensions (2D). The efficacy of our proposed model is\nverified and validated by using the extreme value statistics in random matrix\ntheory. We find that the random evolution of each pair of stocks in a 2D price\nspace is a scale-invariant complex trajectory whose tortuosity is governed by a\n$2/3$ geometric law between the gyration radius $R_g(t)$ and the total length\n$\\ell(t)$ of the path, i.e., $R_g(t)\\sim\\ell(t)^{2/3}$. We construct a Wishart\nmatrix containing all stocks up to a specific variable period and look at its\nspectral properties over 30 years. In contrast to the standard random matrix\ntheory, we find that the distribution of eigenvalues has a power-law tail with\na decreasing exponent over time -- a quantitative indicator of the temporal\ncorrelations. We find that the time evolution of the distance of a 2D L\\'evy\nflights with index $\\alpha=3/2$ from origin generates the same empirical\nspectral properties. The statistics of the largest eigenvalues of the model and\nthe observations are in perfect agreement.\n"
    },
    {
        "paper_id": 2202.12137,
        "authors": "Tommaso Mariotti, Fabrizio Lillo, Giacomo Toscano",
        "title": "From Zero-Intelligence to Queue-Reactive: Limit Order Book modeling for\n  high-frequency volatility estimation and optimal execution",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The estimation of the volatility with high-frequency data is plagued by the\npresence of microstructure noise, which leads to biased measures. Alternative\nestimators have been developed and tested either on specific structures of the\nnoise or by the speed of convergence to their asymptotic distributions.\nGatheral and Oomen (2010) proposed to use the Zero-Intelligence model of the\nlimit order book to test the finite-sample performance of several estimators of\nthe integrated variance. Building on this approach, in this paper we introduce\nthree main innovations: (i) we use as data-generating process the\nQueue-Reactive model of the limit order book (Huang et al. (2015)), which -\ncompared to the Zero-Intelligence model - generates more realistic\nmicrostructure dynamics, as shown here by using an Hausman test; (ii) we\nconsider not only estimators of the integrated volatility but also of the spot\nvolatility; (iii) we show the relevance of the estimator in the prediction of\nthe variance of the cost of a simulated VWAP execution. Overall we find that,\nfor the integrated volatility, the pre-averaging estimator optimizes the\nestimation bias, while the unified and the alternation estimator lead to\noptimal mean squared error values. Instead, in the case of the spot volatility,\nthe Fourier estimator yields the optimal accuracy, both in terms of bias and\nmean squared error. The latter estimator leads also to the optimal prediction\nof the cost variance of a VWAP execution.\n"
    },
    {
        "paper_id": 2202.12186,
        "authors": "Gabriel Borrageiro",
        "title": "Sequential asset ranking in nonstationary time series",
        "comments": null,
        "journal-ref": null,
        "doi": "10.1145/3533271.3561666",
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  We create a ranking algorithm, the naive Bayes asset ranker. Our algorithm\ncomputes the posterior probability that individual assets will be ranked higher\nthan other portfolio constituents. Unlike earlier algorithms, such as the\nweighted majority, our algorithm allows poor-performing experts to have\nincreased weight when they start performing well. We outperform the long-only\nholding of the S&P 500 index and a regress-then-rank baseline.\n"
    },
    {
        "paper_id": 2202.12292,
        "authors": "Dan Levin, Luyao Zhang",
        "title": "Bridging Level-K to Nash Equilibrium",
        "comments": "Keywords: Nash equilibrium, Level-K, Bayesian Nash Equilibrium,\n  Sub-game Perfect Bayesian Nash Equilibrium, Bounded rationality, psychology,\n  behavioral economics, false consensus effects, centipede Game, the 11-20\n  money request game, Common Value Auction, experienced and inexperienced\n  bidders, learning in games, equilibrium solution concepts, strategic\n  thinking, chess players",
        "journal-ref": null,
        "doi": "10.1162/rest_a_00990",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We introduce NLK, a model that connects the Nash equilibrium (NE) and\nLevel-K. It allows a player in a game to believe that her opponent may be\neither less or as sophisticated as, she is, a view supported in psychology. We\napply NLK to data from five published papers on static, dynamic, and auction\ngames. NLK provides different predictions than those of the NE and Level-K;\nmoreover, a simple version of NLK explains the experimental data better in many\ncases, with the same or lower number of parameters. We discuss extensions to\ngames with more than two players and heterogeneous beliefs.\n"
    },
    {
        "paper_id": 2202.12339,
        "authors": "Guglielmo Briscese, Maddalena Grignani, Stephen Stapleton",
        "title": "Crises and Political Polarization: Towards a Better Understanding of the\n  Timing and Impact of Shocks and Media",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  We investigate how crises alter societies by analyzing the timing and\nchannels of change using a longitudinal multi-wave survey of a representative\nsample of Americans throughout 2020. This methodology allows us to overcome\nsome of the limitations of previous studies and uncover novel insights: (1)\nindividuals with a negative personal experience during a crisis become more\npro-welfare spending, in particular for policies they perceive will benefit\nthem personally, and they become less trusting of institutions; (2) indirect\nshocks or the mere exposure to the crisis doesn't have a similar effect; (3)\npolicy preferences and institutional trust can change quickly after a negative\nexperience; and (4) consuming partisan media can mitigate or exacerbate these\neffects by distorting perceptions of reality. In an experiment, we find that\nexposing individuals to the same information can recalibrate distorted\nperceptions with lasting effects. Using a machine learning model to test for\nheterogeneous treatment effects, we find a negative personal experience did not\nmake individuals more responsive to the information treatment, suggesting that\nlived and perceived experiences play an equally important role in changing\npreferences during a crisis.\n"
    },
    {
        "paper_id": 2202.12405,
        "authors": "Naroa Coretti Sanchez, Luis Alonso Pastor, Kent Larson",
        "title": "Can autonomy make bicycle-sharing systems more sustainable?\n  Environmental impact analysis of an emerging mobility technology",
        "comments": null,
        "journal-ref": "Transportation Research Part D: Transport and Environment, 2022",
        "doi": "10.1016/j.trd.2022.103489",
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  Autonomous bicycles have recently been proposed as a new and more efficient\napproach to bicycle-sharing systems (BSS), but the corresponding environmental\nimplications remain unresearched. Conducting environmental impact assessments\nat an early technological stage is critical to influencing the design and,\nultimately, environmental impacts of a system. Consequently, this paper aims to\nassess the environmental impact of autonomous shared bikes compared with\ncurrent station-based and dockless systems under different sets of modeling\nhypotheses and mode-shift scenarios. The results indicate that autonomy could\nreduce the environmental impact per passenger kilometer traveled of current\nstation-based and dockless BSS by 33.1 % and 58.0 %. The sensitivity analysis\nshows that the environmental impact of autonomous shared bicycles will mainly\ndepend on vehicle usage rates and the need for infrastructure. Finally, this\nstudy highlights the importance of targeting the mode replacement from more\npolluting modes, especially as traditional mobility modes decarbonize and\nbecome more efficient.\n"
    },
    {
        "paper_id": 2202.12548,
        "authors": "Malte Jansen, Philipp Beiter, Iegor Riepin, Felix Muesgens, Victor\n  Juarez Guajardo-Fajardo, Iain Staffell, Bernard Bulder, Lena Kitzing",
        "title": "Policy choices and outcomes for offshore wind auctions globally",
        "comments": "55 pages, 6 figures",
        "journal-ref": null,
        "doi": "10.1016/j.enpol.2022.113000",
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Offshore wind energy is rapidly expanding, facilitated largely through\nauctions run by governments. We provide a detailed quantified overview of\napplied auction schemes, including geographical spread, volumes, results, and\ndesign specifications. Our comprehensive global dataset reveals heterogeneous\ndesigns. Although most remuneration designs provide some form of revenue\nstabilisation, their specific instrument choices vary and include feed-in\ntariffs, one-sided and two-sided contracts for difference, mandated power\npurchase agreements, and mandated renewable energy certificates. We review the\nschemes used in all eight major offshore wind jurisdictions across Europe,\nAsia, and North America and evaluate bids in their jurisdictional context. We\nanalyse cost competitiveness, likelihood of timely construction, occurrence of\nstrategic bidding, and identify jurisdictional aspects that might have\ninfluenced auction results. We find that auctions are embedded within their\nrespective regulatory and market design context, and are remarkably diverse,\nthough with regional similarities. Auctions in each jurisdiction have evolved\nand tend to become more exposed to market price risks over time. Less mature\nmarkets are more prone to make use of lower-risk designs. Still, some form of\nrevenue stabilisation is employed for all auctioned offshore wind energy farms\nanalysed here, regardless of the specific policy choices. Our data confirm a\ncoincidence of declining costs and growing diffusion of auction regimes.\n"
    },
    {
        "paper_id": 2202.12695,
        "authors": "Stefan Griller, Florian Huber, Michael Pfarrhofer",
        "title": "Measuring Shocks to Central Bank Independence using Legal Rulings",
        "comments": "JEL: E43, E44, E58, K00; keywords: local projection, euro area,\n  financial markets, mixture model, factor model, identification through\n  heteroskedasticity",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  We investigate the consequences of legal rulings on the conduct of monetary\npolicy. Several unconventional monetary policy measures of the European Central\nBank have come under scrutiny before national courts and the European Court of\nJustice. These lawsuits have the potential to severely impact the scope and\nflexibility of central bank policies, and central bank independence in a wide\nsense, with important consequences for the real and financial economy. Since\nthe number of relevant legal challenges is small, we develop an econometric\napproach that searches for minimum variance regimes which we use to isolate and\nmeasure the effects of these events. Our results suggest that legal rulings\naddressing central bank policies have a powerful effect on financial markets.\nExpansionary shocks ease financial conditions along various dimensions, and\ninflation swap reactions suggest inflationary pressures with stronger effects\nin the short term.\n"
    },
    {
        "paper_id": 2202.12721,
        "authors": "Marika Khozrevanidze",
        "title": "The impact of accumulative pension policy on welfare of individuals",
        "comments": "in Georgian language",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  Many countries around the world have had to carry out radical reforms\nperiodically in their pension systems. Global experience shows that it is\nimportant to optimize the costs of pension and social security systems in order\nto ensure a decent old age in addition to reducing the pressure on budgetary\nresources. By Georgia is changing demographic situation, special attention is\npaid to proper functioning of the pension policy. The pension reform carried\nout in Georgia in 2019 caused a difference of opinion among experts. This issue\nin today is conditions does not lose relevance. The presented thesis discusses\nthe impact of the mandatory funded pension system on the well-being of people.\nThesis includes the following issues: peculiarities of the formation of pension\nsystems in Georgia. It is presented a small historical excursion in terms of\nthe development of pension systems. In addition, are discussed the\ninternational experience of pension systems and comparative analysis in\nrelation to Georgia. This paper specifically focuses on the essence of the\nmandatory funded pension system and assesses the current situation in terms of\ninvestment potential of the resource accumulated in the pension fund. In\nconclusion, are presented the challenges of this system and the ways of\nperfection.\n"
    },
    {
        "paper_id": 2202.12745,
        "authors": "Jingtang Ma, Zhengyang Lu, Zhenyu Cui",
        "title": "Delta family approach for the stochastic control problems of utility\n  maximization",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  In this paper, we propose a new approach for stochastic control problems\narising from utility maximization. The main idea is to directly start from the\ndynamical programming equation and compute the conditional expectation using a\nnovel representation of the conditional density function through the Dirac\nDelta function and the corresponding series representation. We obtain an\nexplicit series representation of the value function, whose coefficients are\nexpressed through integration of the value function at a later time point\nagainst a chosen basis function. Thus we are able to set up a recursive\nintegration time-stepping scheme to compute the optimal value function given\nthe known terminal condition, e.g. utility function. Due to tensor\ndecomposition property of the Dirac Delta function in high dimensions, it is\nstraightforward to extend our approach to solving high-dimensional stochastic\ncontrol problems. The backward recursive nature of the method also allows for\nsolving stochastic control and stopping problems, i.e. mixed control problems.\nWe illustrate the method through solving some two-dimensional stochastic\ncontrol (and stopping) problems, including the case under the classical and\nrough Heston stochastic volatility models, and stochastic local volatility\nmodels such as the stochastic alpha beta rho (SABR) model.\n"
    },
    {
        "paper_id": 2202.12811,
        "authors": "Santiago Camara",
        "title": "Does an increase in the cost of imported inputs hurt exports? Evidence\n  from firms' network of foreign suppliers",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/publicdomain/zero/1.0/",
        "abstract": "  This paper examines the relationship between changes in the cost of imported\ninputs and export performance using a novel dataset from Argentina which\nidentifies domestic firms' network of foreign suppliers. To guide my empirical\nstrategy, I construct a heterogeneous firm model subject to quality choice and\nfrictions in the market for foreign supplier. The model predicts that the\nimpact of an increase in the cost of imported inputs to be increasing in the\nadjustments costs of supplier linkages and in the quality of the product\nexported. I take the model to the data by constructing firm-specific shocks\nusing a shift-share analysis which exploits firms' lagged exposure to foreign\nsuppliers and finely defined import price shifts. Evidence suggests the\npresence of significant adjustment cost in firms' foreign supplier linkages and\nstrong complementarities between imported inputs and export performance,\nparticularly of high-quality products.\n"
    },
    {
        "paper_id": 2202.12946,
        "authors": "Puneet Pasricha, Dharmaraja Selvamuthu and Selvaraju Natarajan",
        "title": "A contagion process with self-exciting jumps in credit risk applications",
        "comments": "20 pages, 3 figures, 4 tables",
        "journal-ref": "Stochastics: An International Journal Of Probability And\n  Stochastic Processes, 2022",
        "doi": "10.1080/17442508.2022.2041641",
        "license": "http://creativecommons.org/publicdomain/zero/1.0/",
        "abstract": "  The modeling of the probability of joint default or total number of defaults\namong the firms is one of the crucial problems to mitigate the credit risk\nsince the default correlations significantly affect the portfolio loss\ndistribution and hence play a significant role in allocating capital for\nsolvency purposes. In this article, we derive a closed-form expression for the\nprobability of default of a single firm and the probability of the total number\nof defaults by any time $t$ in a homogeneous portfolio of firms. We use a\ncontagion process to model the arrival of credit events that causes the default\nand develop a framework that allows firms to have resistance against default\nunlike the standard intensity-based models. We assume the point process driving\nthe credit events to be composed of a systematic and an idiosyncratic\ncomponent, whose intensities are independently specified by a mean-reverting\naffine jump-diffusion process with self-exciting jumps. The proposed framework\nis competent of capturing the feedback effect, an empirically observed\nphenomenon in the default events. We further demonstrate how the proposed\nframework can be used to price synthetic collateralized debt obligation (CDO)\nand obtain a closed-form solution for tranche spread. Finally, we present the\nsensitivity analysis to demonstrate the effect of different parameters\ngoverning the contagion effect on the spread of tranches and the expected loss\nof the CDO.\n"
    },
    {
        "paper_id": 2202.13276,
        "authors": "Diego Rold\\'an, Ang\\'elica Abad Cisneros, Francisco Rold\\'an-Ar\\'auz,\n  Samantha Leta Angamarca, Anah\\'i Ram\\'irez Zambrano",
        "title": "An analysis of indifference curves and areas from a human nutrition\n  perspective",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  Through documentary research and interviews with nutrition experts, we found\nthat all nutrients have two thresholds, the Recommended Daily Allowance (RDA)\nand the Tolerable Upper Intake Level (UL). Intake less than the RDA or more\nthan the UL negatively affects health. Intake quantities of nutrients within\nthese limits covers 100% of the objective physiological needs without negative\nrepercussions. These characteristics, and others, are common knowledge among\nnutrition experts; however, these are not adequately reflected in the\nmicroeconomics models that study these needs. We conclude that the generalized\npresence of these thresholds determines the existence of significant\nindifference areas that should be added to the microeconomics models of the\nindifference curves, thus improving the modelling of reality.\n"
    },
    {
        "paper_id": 2202.13317,
        "authors": "Davide Fiaschi, Cristina Tealdi",
        "title": "The attachment of adult women to the Italian labour market in the shadow\n  of COVID-19",
        "comments": "52 pages, 21 figures, 7 tables",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We investigate the attachment to the labour market of women in their 30s, who\nare combining career and family choices, through their reactions to an\nexogenous, and potentially symmetric shock, such as the COVID-19 pandemic. We\nfind that in Italy a large number of females with small children, living in the\nNorth, left permanent (and temporary) employment and became inactive in 2020.\nDespite the short period of observation after the burst of the pandemic, the\nidentified impacts appear large and persistent, particularly with respect to\nthe males of the same age. We argue that this evidence is ascribable to\nspecific regional socio-cultural factors, which foreshadow a potential\nlong-term detrimental impact on female labour force participation.\n"
    },
    {
        "paper_id": 2202.13417,
        "authors": "Pier Luigi Sacco, Alex Arenas, Manlio De Domenico",
        "title": "The political economy of big data leaks: Uncovering the skeleton of tax\n  evasion",
        "comments": "15 pages, 2 figures",
        "journal-ref": null,
        "doi": "10.1016/j.chaos.2023.113182",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  After the leak of 11.5 million documents from the Panamanian corporation\nMossack Fonseca, an intricate network of offshore business entities has been\nrevealed. The emerging picture is that of legal entities, either individuals or\ncompanies, involved in offshore activities and transactions with several tax\nhavens simultaneously which establish, indirectly, an effective network of\ncountries acting on tax evasion. The analysis of this network quantitatively\nuncovers a strongly connected core (a rich-club) of countries whose indirect\ninteractions, mediated by legal entities, form the skeleton for tax evasion\nworldwide. Intriguingly, the rich-club mainly consists of well-known tax havens\nsuch as British Virgin Islands and Hong Kong, and major global powers such as\nChina, Russia, United Kingdom and United States of America. The analysis\nprovides a new way to rank tax havens because of the role they play in this\nnetwork, and the results call for an international coordination on taxation\npolicies that take into account the complex interconnected structure of tax\nevaders in a globalized economy.\n"
    },
    {
        "paper_id": 2202.1354,
        "authors": "Daniel Bj\\\"orkegren, Joshua Blumenstock, Omowunmi Folajimi-Senjobi,\n  Jacqueline Mauro, and Suraj R. Nair",
        "title": "Instant Loans Can Lift Subjective Well-Being: A Randomized Evaluation of\n  Digital Credit in Nigeria",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Digital loans have exploded in popularity across low and middle income\ncountries, providing short term, high interest credit via mobile phones. This\npaper reports the results of a randomized evaluation of a digital loan product\nin Nigeria. Being randomly approved for digital credit (irrespective of credit\nscore) substantially increases subjective well-being after an average of three\nmonths. For those who are approved, being randomly offered larger loans has an\ninsignificant effect. Neither treatment significantly impacts other measures of\nwelfare. We rule out large short-term impacts either positive or negative: on\nincome and expenditures, resilience, and women's economic empowerment.\n"
    },
    {
        "paper_id": 2202.13695,
        "authors": "Alex A.T. Rathke",
        "title": "Taxpayer deductions and the endogenous probability of tax penalisation",
        "comments": "Keywords: probability of tax penalisation; tax deductions; expected\n  tax penalty",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  We propose a parametric specification of the probability of tax penalisation\nfaced by a taxpayer, based on the amount of deduction chosen by her to reduce\ntotal taxation. Comparative analyses lead to a closed-form solution for the\noptimum tax deduction, and provide the maximising conditions with respect to\nthe probability parameters.\n"
    },
    {
        "paper_id": 2202.13713,
        "authors": "Ion Santra",
        "title": "Effect of tax dynamics on linearly growing processes under stochastic\n  resetting: a possible economic model",
        "comments": "4 figures",
        "journal-ref": "EPL 137, 52001, (2022)",
        "doi": "10.1209/0295-5075/ac5e53",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We study a system of $N$ agents, whose wealth grows linearly, under the\neffect of stochastic resetting and interacting via a tax-like dynamics -- all\nagents donate a part of their wealth, which is, in turn, redistributed equally\namong all others. This mimics a socio-economic scenario where people have fixed\nincomes, suffer individual economic setbacks, and pay taxes to the state. The\nsystem always reaches a stationary state, which shows a trivial exponential\nwealth distribution in the absence of tax dynamics. The introduction of the tax\ndynamics leads to several interesting features in the stationary wealth\ndistribution. In particular, we analytically find that an increase in taxation\nfor a homogeneous system (where all agents are alike) results in a transition\nfrom a society where agents are most likely poor to another where rich agents\nare more common. We also study inhomogeneous systems, where the growth rates of\nthe agents are chosen from a distribution, and the taxation is proportional to\nthe individual growth rates. We find an optimal taxation, which produces a\ncomplete economic equality (average wealth is independent of the individual\ngrowth rates), beyond which there is a reverse disparity, where agents with low\ngrowth rates are more likely to be rich. We consider three income distributions\nobserved in the real world and show that they exhibit the same qualitative\nfeatures. Our analytical results are in the $N\\to\\infty$ limit and backed by\nnumerical simulations.\n"
    },
    {
        "paper_id": 2202.1393,
        "authors": "Giulio Caldarelli",
        "title": "Formalizing Oracle Trust Models for blockchain-based business\n  applications. An example from the supply chain sector",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Blockchain technology truly opened the gate to a wave of unparalleled\ninnovations; however, despite the rapidly growing load of hype, the integration\ninto the business, apart from a few applications, seems to be coming at a\nslower rate. One reason for that delay may be the need in the real-world\napplications for the so-called trust model. Trust models are rarely mentioned\nin blockchain application proposals despite their importance, which creates\nskepticism about their successful developments. To promote trust model\nimplementation and help practitioners in its redaction, this article provides\nan outline of what a trust model is, why it is essential, and an example of how\nit is elaborated. The discussed example comes from a case study of a dairy\ncompany that implemented blockchain for the traceability of its products.\nDespite being tailored on a traceability project, the redaction and elements of\nthe trust model, with few adjustments, could be easily readapted for other\napplications.\n"
    },
    {
        "paper_id": 2202.13996,
        "authors": "Magnus Wiese, Phillip Murray",
        "title": "Risk-Neutral Market Simulation",
        "comments": null,
        "journal-ref": "AAAI 2022 Workshop on AI in Financial Services: Adaptiveness,\n  Resilience & Governance",
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We develop a risk-neutral spot and equity option market simulator for a\nsingle underlying, under which the joint market process is a martingale. We\nleverage an efficient low-dimensional representation of the market which\npreserves no static arbitrage, and employ neural spline flows to simulate\nsamples which are free from conditional drifts and are highly realistic in the\nsense that among all possible risk-neutral simulators, the obtained\nrisk-neutral simulator is the closest to the historical data with respect to\nthe Kullback-Leibler divergence. Numerical experiments demonstrate the\neffectiveness and highlight both drift removal and fidelity of the calibrated\nsimulator.\n"
    },
    {
        "paper_id": 2203.00148,
        "authors": "Jaehyuk Choi and Rong Chen",
        "title": "Improved iterative methods for solving risk parity portfolio",
        "comments": null,
        "journal-ref": "Journal of Derivatives and Quantitative Studies, 30(2):114-124,\n  2022",
        "doi": "10.1108/JDQS-12-2021-0031",
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Risk parity, also known as equal risk contribution, has recently gained\nincreasing attention as a portfolio allocation method. However, solving\nportfolio weights must resort to numerical methods as the analytic solution is\nnot available. This study improves two existing iterative methods: the cyclical\ncoordinate descent (CCD) and Newton methods. We enhance the CCD method by\nsimplifying the formulation using a correlation matrix and imposing an\nadditional rescaling step. We also suggest an improved initial guess inspired\nby the CCD method for the Newton method. Numerical experiments show that the\nimproved CCD method performs the best and is approximately three times faster\nthan the original CCD method, saving more than 40% of the iterations.\n"
    },
    {
        "paper_id": 2203.00184,
        "authors": "Benjamin Avanzi and Mark Lavender and Greg Taylor and Bernard Wong",
        "title": "On the impact of outliers in loss reserving",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The sensitivity of loss reserving techniques to outliers in the data or\ndeviations from model assumptions is a well known challenge. It has been shown\nthat the popular chain-ladder reserving approach is at significant risk to such\naberrant observations in that reserve estimates can be significantly shifted in\nthe presence of even one outlier. As a consequence the chain-ladder reserving\ntechnique is non-robust. In this paper we investigate the sensitivity of\nreserves and mean squared errors of prediction under Mack's Model (Mack, 1993).\nThis is done through the derivation of impact functions which are calculated by\ntaking the first derivative of the relevant statistic of interest with respect\nto an observation. We also provide and discuss the impact functions for\nquantiles when total reserves are assumed to be lognormally distributed.\nAdditionally, comparisons are made between the impact functions for individual\naccident year reserves under Mack's Model and the Bornhuetter-Ferguson\nmethodology. It is shown that the impact of incremental claims on these\nstatistics of interest varies widely throughout a loss triangle and is heavily\ndependent on other cells in the triangle.\n  Results are illustrated using data from a Belgian non-life insurer.\n"
    },
    {
        "paper_id": 2203.00427,
        "authors": "Philippe Cotte, Pierre Lagier, Vincent Margot, Christophe Geissler",
        "title": "Making use of supercomputers in financial machine learning",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This article is the result of a collaboration between Fujitsu and Advestis.\nThis collaboration aims at refactoring and running an algorithm based on\nsystematic exploration producing investment recommendations on a\nhigh-performance computer of the Fugaku, to see whether a very high number of\ncores could allow for a deeper exploration of the data compared to a cloud\nmachine, hopefully resulting in better predictions. We found that an increase\nin the number of explored rules results in a net increase in the predictive\nperformance of the final ruleset. Also, in the particular case of this study,\nwe found that using more than around 40 cores does not bring a significant\ncomputation time gain. However, the origin of this limitation is explained by a\nthreshold-based search heuristic used to prune the search space. We have\nevidence that for similar data sets with less restrictive thresholds, the\nnumber of cores actually used could very well be much higher, allowing\nparallelization to have a much greater effect.\n"
    },
    {
        "paper_id": 2203.0046,
        "authors": "Tobias Fissler and Silvana M. Pesenti",
        "title": "Sensitivity Measures Based on Scoring Functions",
        "comments": null,
        "journal-ref": "European Journal of Operational Research, Volume 307, Issue 3,\n  2023, Pages 1408-1423",
        "doi": "10.1016/j.ejor.2022.10.002",
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  We propose a holistic framework for constructing sensitivity measures for any\nelicitable functional $T$ of a response variable. The sensitivity measures,\ntermed score-based sensitivities, are constructed via scoring functions that\nare (strictly) consistent for $T$. These score-based sensitivities quantify the\nrelative improvement in predictive accuracy when available information, e.g.,\nfrom explanatory variables, is used ideally. We establish intuitive and\ndesirable properties of these sensitivities and discuss advantageous choices of\nscoring functions leading to scale-invariant sensitivities.\n  Since elicitable functionals typically possess rich classes of (strictly)\nconsistent scoring functions, we demonstrate how Murphy diagrams can provide a\npicture of all score-based sensitivity measures. We discuss the family of\nscore-based sensitivities for the mean functional (of which the Sobol indices\nare a special case) and risk functionals such as Value-at-Risk, and the pair\nValue-at-Risk and Expected Shortfall. The sensitivity measures are illustrated\nusing numerous examples, including the Ishigami--Homma test function. In a\nsimulation study, estimation of score-based sensitivities for a non-linear\ninsurance portfolio is performed using neural nets.\n"
    },
    {
        "paper_id": 2203.00487,
        "authors": "Matz Dahlberg, Johan Egebark, G\\\"ulay \\\"Ozcan, Ulrika Vikman",
        "title": "Labor Market Integration of Refugees: RCT Evidence from an Early\n  Intervention Program in Sweden",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This study uses a randomized control trial to evaluate a new program for\nincreased labor market integration of refugees. The program introduces highly\nintensive assistance immediately after the residence permit is granted. The\nearly intervention strategy contrasts previous integration policies, which\ntypically constitute low-intensive help over long periods of time. We find\npositive effects on employment of the program. The magnitude of the effect is\nsubstantial, corresponding to around 15 percentage points. Our cost estimates\nsuggest that the new policy is less expensive than comparable labor market\nprograms used in the past.\n"
    },
    {
        "paper_id": 2203.00618,
        "authors": "Pier Luigi Sacco, Alex Arenas, Manlio De Domenico",
        "title": "The resilience of the multirelational structure of geopolitical treaties\n  is critically linked to past colonial world order and offshore fiscal havens",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The governance of the political and economic world order builds on a complex\narchitecture of international treaties at various geographical scales. In a\nhistorical phase of high institutional turbulence, assessing the stability of\nsuch architecture with respect to the unilateral defection of single countries\nand to the breakdown of single treaties is important. We carry out this\nanalysis on the whole global architecture and find that the countries with the\nhighest disruption potential are not major world powers (with the exception of\nGermany and France) but mostly medium-small and micro-countries. Political\nstability is highly dependent on many former colonial overseas territories that\nare today part of the global network of fiscal havens, as well as on emerging\neconomies, mostly from South-East Asia. Economic stability depends on medium\nsized European and African countries. However, single global treaties have\nsurprisingly less disruptive potential, with the major exception of the WTO.\nThese apparently counter-intuitive results highlight the importance to a\nnonlinear approach to international relations where the complex multilayered\narchitecture of global governance is analyzed using advanced network science\ntechniques. Our results suggest that the potential fragility of the world order\nseem to be much more directly related to global inequality and fiscal injustice\nthan it is commonly believed, and that the legacy of the colonial world order\nis still very strong in the current international relations scenario. In\nparticular, vested interests related to tax avoidance seem to have a structural\nrole in the political architecture of global governance\n"
    },
    {
        "paper_id": 2203.00684,
        "authors": "Guido Th\\\"ommes, Martin Oliver Sailer, Nicolas Bonnet, Alex Carlton,\n  Juan J. Abellan, Veronique Robert",
        "title": "Awareness and use of quantitative decision-making methods in\n  pharmaceutical development",
        "comments": "14 pages, 4 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  The pharmaceutical industry has experienced increasing costs and sustained\nhigh attrition rates in drug development over the last years. One proposal that\naddresses this challenge from a statistical perspective is the use of\nquantitative decision-making (QDM) methods to support a data-driven, objective\nappraisal of the evidence that forms the basis of decisions at different\ndevelopment levels. Growing awareness among statistical leaders in the industry\nhas led to the creation of the European EFSPI/PSI special interest group (ESIG)\non quantitative decision making to share experiences, collect best practices,\nand promote the use of QDM. In this paper, we introduce key components of QDM\nand present examples of QDM methods on trial, program, and portfolio level. The\nESIG created a questionnaire to learn how and to what extent QDM methods are\ncurrently used in the different development phases. We present the main\nquestionnaire findings, and we show where QDM is already used today but also\nwhere areas for future improvement can be identified. In particular,\nstatisticians should increase their visibility, involvement, and leadership in\ncross-functional decision-making.\n"
    },
    {
        "paper_id": 2203.00729,
        "authors": "Pietro Biroli and Titus J. Galama and Stephanie von Hinke and Hans van\n  Kippersluis and Cornelius A. Rietveld and Kevin Thom",
        "title": "The Economics and Econometrics of Gene-Environment Interplay",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  Economists and social scientists have debated the relative importance of\nnature (one's genes) and nurture (one's environment) for decades, if not\ncenturies. This debate can now be informed by the ready availability of genetic\ndata in a growing number of social science datasets. This paper explores the\npotential uses of genetic data in economics, with a focus on estimating the\ninterplay between nature (genes) and nurture (environment). We discuss how\neconomists can benefit from incorporating genetic data into their analyses even\nwhen they do not have a direct interest in estimating genetic effects. We argue\nthat gene--environment (GxE) studies can be instrumental for (i) testing\neconomic theory, (ii) uncovering economic or behavioral mechanisms, and (iii)\nanalyzing treatment effect heterogeneity, thereby improving the understanding\nof how (policy) interventions affect population subgroups. We introduce the\nreader to essential genetic terminology, develop a conceptual economic model to\ninterpret gene-environment interplay, and provide practical guidance to\nempirical researchers.\n"
    },
    {
        "paper_id": 2203.00839,
        "authors": "Baishuai Zuo, Chuancun Yin",
        "title": "Multivariate doubly truncated moments for generalized skew-elliptical\n  distributions with application to multivariate tail conditional risk measures",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper, we focus on multivariate doubly truncated first two moments of\ngeneralized skew-elliptical (GSE) distributions and derive explicit expressions\nfor them.\n  It includes many useful distributions, for examples, generalized skew-normal\n(GSN), generalized skew-Laplace (GSLa), generalized skew-logistic (GSLo) and\ngeneralized skew student-$t$ (GSSt) distributions, all as special cases. We\nalso give formulas of multivariate doubly truncated expectation and covariance\nfor GSE distributions. As applications, we show the results of multivariate\ntail conditional expectation (MTCE) and multivariate tail covariance (MTCov)\nfor GSE distributions.\n"
    },
    {
        "paper_id": 2203.01068,
        "authors": "Ola Hall, Mattias Ohlsson and Thortseinn R\\\"ognvaldsson",
        "title": "Satellite Image and Machine Learning based Knowledge Extraction in the\n  Poverty and Welfare Domain",
        "comments": "17 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
        "abstract": "  Recent advances in artificial intelligence and machine learning have created\na step change in how to measure human development indicators, in particular\nasset based poverty. The combination of satellite imagery and machine learning\nhas the capability to estimate poverty at a level similar to what is achieved\nwith workhorse methods such as face-to-face interviews and household surveys.\nAn increasingly important issue beyond static estimations is whether this\ntechnology can contribute to scientific discovery and consequently new\nknowledge in the poverty and welfare domain. A foundation for achieving\nscientific insights is domain knowledge, which in turn translates into\nexplainability and scientific consistency. We review the literature focusing on\nthree core elements relevant in this context: transparency, interpretability,\nand explainability and investigate how they relates to the poverty, machine\nlearning and satellite imagery nexus. Our review of the field shows that the\nstatus of the three core elements of explainable machine learning\n(transparency, interpretability and domain knowledge) is varied and does not\ncompletely fulfill the requirements set up for scientific insights and\ndiscoveries. We argue that explainability is essential to support wider\ndissemination and acceptance of this research, and explainability means more\nthan just interpretability.\n"
    },
    {
        "paper_id": 2203.01091,
        "authors": "Baishuai Zuo, Chuancun Yin",
        "title": "Doubly truncated moment risk measures for elliptical distributions",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper, we define doubly truncated moment (DTM), doubly truncated\nskewness (DTS) and kurtosis (DTK). We derive DTM formulae for elliptical\nfamily, with emphasis on normal, student-$t$, logistic, Laplace and Pearson\ntype VII distributions. We also present explicit formulas of the DTE (doubly\ntruncated expectation), DTV (doubly truncated variance), DTS and DTK for those\ndistributions. As illustrative example, DTEs, DTVs, DTSs and DTKs of three\nindustry segments' (Banks, Insurance, Financial and Credit Service) stock\nreturn in London stock exchange are discussed.\n"
    },
    {
        "paper_id": 2203.0116,
        "authors": "Christian Bayer, Denis Belomestny, Oleg Butkovsky, John Schoenmakers",
        "title": "A Reproducing Kernel Hilbert Space approach to singular local stochastic\n  volatility McKean-Vlasov models",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Motivated by the challenges related to the calibration of financial models,\nwe consider the problem of numerically solving a singular McKean-Vlasov\nequation $$ d X_t= \\sigma(t,X_t) X_t \\frac{\\sqrt v_t}{\\sqrt {E[v_t|X_t]}}dW_t,\n$$ where $W$ is a Brownian motion and $v$ is an adapted diffusion process. This\nequation can be considered as a singular local stochastic volatility model.\nWhilst such models are quite popular among practitioners, unfortunately, its\nwell-posedness has not been fully understood yet and, in general, is possibly\nnot guaranteed at all. We develop a novel regularization approach based on the\nreproducing kernel Hilbert space (RKHS) technique and show that the regularized\nmodel is well-posed. Furthermore, we prove propagation of chaos. We demonstrate\nnumerically that a thus regularized model is able to perfectly replicate option\nprices due to typical local volatility models. Our results are also applicable\nto more general McKean--Vlasov equations.\n"
    },
    {
        "paper_id": 2203.01235,
        "authors": "Gennady Shkliarevsky",
        "title": "Is Our Research Productivity In Decline? A New Approach in Resolving the\n  Controversy",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  This contribution examines the current controversy over research\nproductivity. There are two sides in this controversy. Using extensive data\nfrom several industries and areas of research, one side argues that research\nproductivity is currently in decline. The other side disputes this conclusion.\nIt contends that the data used in making this argument are selective and\nlimited; they do not reflect the overall state of research. The conclusion that\nfollows from this critique is that the indicators of research productivity we\ncurrently use are not reliable and do not warrant a definitive answer to the\nproblem.\n  The article agrees that we need a new set of indicators in assessing research\nproductivity. It proposes that we should look at global indicators related to\nknowledge production in general, rather than look at selective data that are\ninevitably limited in their scope. The article argues that the process of\ncreation plays the essential role in knowledge production. Therefore, the\nperspective that uses the process of creation as its central organizing\nprinciple offers a unique and global view on the production of knowledge and\nmakes a definitive resolution of the controversy possible. The article also\noutlines some steps for improving research productivity and realizing the full\npotential of the human capacity to produce knowledge.\n  Key words: Research productivity, knowledge growth, the process of creation,\nlevels of organization, equilibration and the production of disequilibrium.\n"
    },
    {
        "paper_id": 2203.01326,
        "authors": "Jaydip Sen, Sidra Mehtab, Abhishek Dutta, Saikat Mondal",
        "title": "Precise Stock Price Prediction for Optimized Portfolio Design Using an\n  LSTM Model",
        "comments": "This is the accepted version of our paper in the IEEE 19th OITS\n  International Conference on Information Technology (OCIT 21). The final\n  version is available in the IEEE Xplore. The paper consists of 6 pages and it\n  includes 9 figures and 20 tables. arXiv admin note: substantial text overlap\n  with arXiv:2202.02723, arXiv:2111.04709",
        "journal-ref": null,
        "doi": "10.1109/OCIT53463.2021.00050",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Accurate prediction of future prices of stocks is a difficult task to\nperform. Even more challenging is to design an optimized portfolio of stocks\nwith the identification of proper weights of allocation to achieve the\noptimized values of return and risk. We present optimized portfolios based on\nthe seven sectors of the Indian economy. The past prices of the stocks are\nextracted from the web from January 1, 2016, to December 31, 2020. Optimum\nportfolios are designed on the selected seven sectors. An LSTM regression model\nis also designed for predicting future stock prices. Five months after the\nconstruction of the portfolios, i.e., on June 1, 2021, the actual and predicted\nreturns and risks of each portfolio are computed. The predicted and the actual\nreturns indicate the very high accuracy of the LSTM model.\n"
    },
    {
        "paper_id": 2203.01477,
        "authors": "J Ceasar Aguma",
        "title": "A Matching Mechanism for Provision of Housing to the Marginalized",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  During this pandemic, there have been unprecedented community and local\ngovernment efforts to slow down the spread of the coronavirus, and also to\nprotect our local economies. One such effort is California's project Roomkey\nthat provided emergency housing to over 2000 vulnerable persons but fell short\nof the set goal of 15,000. It is projected that the homelessness problem will\nonly get worse after the pandemic. With that in mind, we borrow from efforts\nlike project Roomkey and suggest a solution that looks to improve upon these\nefforts to efficiently assign housing to the unhoused in our communities. The\npandemic, together with the project Roomkey, shed light on the underlying\nsupply demand mismatch that presents an opportunity for a matching mechanism\nsolution to assigning housing options to the unhoused in a way that maximizes\nsocial welfare and minimizes susceptibility to strategic manipulation.\nAdditionally, we argue that this automated solution would cut down on the\namount of funding and personnel required for the assignment of housing to\nunhoused persons. Our solution is not intended to replace current solutions to\nhomeless housing assignments but rather improve upon them. We can not postpone\na proper solution to homelessness anymore, the time is now as the need for an\nefficient solution is most dire.\n"
    },
    {
        "paper_id": 2203.01614,
        "authors": "Ivar Ekeland, Wolfram Schlenker, Peter Tankov and Brian Wright",
        "title": "Optimal Exploration of an Exhaustible Resource with Stochastic\n  Discoveries",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The standard Hotelling model assumes that the stock of an exhaustible\nresource is known. We expand on the model by Arrow and Chang that introduced\nstochastic discoveries and for the first time completely solve such a model\nusing impulse control. The model has two state variables: the \"proven\" reserves\nas well as a finite unexplored area available for exploration with constant\nmarginal cost, resulting in a Poisson process of new discoveries. We prove that\na frontier of critical levels of \"proven\" reserves exists, above which\nexploration is stopped, and below which it happens at infinite speed. This\nfrontier is increasing in the explored area, and higher \"proven\" reserve levels\nalong this critical threshold are indicative of more scarcity, not less. In\nthis stochastic generalization of Hotelling's rule, the expected shadow price\nof reserves rises at the rate of interest across exploratory episodes. However,\nthe actual trajectories of prices realized prior to exhaustion of the\nexploratory area may jump up or down upon exploration. Conditional on\nnon-exhaustion, expected price arises at a rate bounded above by the rate of\ninterest, consistent with most empirical tests based on observed price\nhistories.\n"
    },
    {
        "paper_id": 2203.01664,
        "authors": "Rama Cont, Mihai Cucuringu, Renyuan Xu, Chao Zhang",
        "title": "Tail-GAN: Learning to Simulate Tail Risk Scenarios",
        "comments": "39 pages, 17 figures, 11 tables. An earlier version of this paper\n  circulated under the title \"TailGAN: Nonparametric Scenario Generation for\n  Tail Risk Estimation\". First draft: March 2022",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  The estimation of loss distributions for dynamic portfolios requires the\nsimulation of scenarios representing realistic joint dynamics of their\ncomponents, with particular importance devoted to the simulation of tail risk\nscenarios. We propose a novel data-driven approach that utilizes Generative\nAdversarial Network (GAN) architecture and exploits the joint elicitability\nproperty of Value-at-Risk (VaR) and Expected Shortfall (ES). Our proposed\napproach is capable of learning to simulate price scenarios that preserve tail\nrisk features for benchmark trading strategies, including consistent statistics\nsuch as VaR and ES.\n  We prove a universal approximation theorem for our generator for a broad\nclass of risk measures. In addition, we show that the training of the GAN may\nbe formulated as a max-min game, leading to a more effective approach for\ntraining. Our numerical experiments show that, in contrast to other data-driven\nscenario generators, our proposed scenario simulation method correctly captures\ntail risk for both static and dynamic portfolios.\n"
    },
    {
        "paper_id": 2203.01729,
        "authors": "Darren Shannon, Grigorios Fountas",
        "title": "Amending the Heston Stochastic Volatility Model to Forecast Local Motor\n  Vehicle Crash Rates: A Case Study of Washington, D.C",
        "comments": "26 pages, 5 tables, 7 figures",
        "journal-ref": null,
        "doi": "10.1016/j.trip.2022.100576",
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Modelling crash rates in an urban area requires a swathe of data regarding\nhistorical and prevailing traffic volumes and crash events and characteristics.\nProvided that the traffic volume of urban networks is largely defined by\ntypical work and school commute patterns, crash rates can be determined with a\nreasonable degree of accuracy. However, this process becomes more complicated\nfor an area that is frequently subject to peaks and troughs in traffic volume\nand crash events owing to exogenous events (for example, extreme weather)\nrather than typical commute patterns. One such area that is particularly\nexposed to exogenous events is Washington, DC, which has seen a large rise in\ncrash events between 2009 and 2020. In this study, we adopt a forecasting model\nthat embeds heterogeneity and temporal instability in its estimates in order to\nimprove upon forecasting models currently used in transportation and road\nsafety research. Specifically, we introduce a stochastic volatility model that\naims to capture the nuances associated with crash rates in Washington, DC. We\ndetermine that this model can outperform conventional forecasting models, but\nit does not perform well in light of the unique travel patterns exhibited\nthroughout the COVID-19 pandemic. Nevertheless, its adaptability to the\nidiosyncrasies of Washington, DC crash rates demonstrates its ability to\naccurately simulate localised crash rates processes, which can be further\nadapted in public policy contexts to form road safety targets.\n"
    },
    {
        "paper_id": 2203.01738,
        "authors": "Javad T. Firouzjaee and Pouriya Khaliliyan",
        "title": "Machine learning model to project the impact of Ukraine crisis",
        "comments": "9 pages, 7 figures, 2 tables",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-sa/4.0/",
        "abstract": "  Russia's attack on Ukraine on Thursday 24 February 2022 hitched financial\nmarkets and the increased geopolitical crisis. In this paper, we select some\nmain economic indexes, such as Gold, Oil (WTI), NDAQ, and known currency which\nare involved in this crisis and try to find the quantitative effect of this war\non them. To quantify the war effect, we use the correlation feature and the\nrelationships between these economic indices, create datasets, and compare the\nresults of forecasts with real data. To study war effects, we use Machine\nLearning Linear Regression. We carry on empirical experiments and perform on\nthese economic indices datasets to evaluate and predict this war tolls and its\neffects on main economics indexes.\n"
    },
    {
        "paper_id": 2203.01778,
        "authors": "Melissa Newham, Marica Valente",
        "title": "The Cost of Influence: How Gifts to Physicians Shape Prescriptions and\n  Drug Costs",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  This paper studies how gifts - monetary or in-kind payments - from drug firms\nto physicians in the US affect prescriptions and drug costs. We estimate\nheterogeneous treatment effects by combining physician-level data on\nantidiabetic prescriptions and payments with causal inference and machine\nlearning methods. We find that payments cause physicians to prescribe more\nbrand drugs, resulting in a cost increase of $30 per dollar received. Responses\ndiffer widely across physicians, and are primarily explained by variation in\npatients' out-of-pocket costs. A gift ban is estimated to decrease drug costs\nby 3-4%. Taken together, these novel findings reveal how payments shape\nprescription choices and drive up costs.\n"
    },
    {
        "paper_id": 2203.02089,
        "authors": "Olukunle O. Owolabi, Kathryn Lawson, Sanhita Sengupta, Yingsi Huang,\n  Lan Wang, Chaopeng Shen, Mila Getmansky Sherman, Deborah A. Sunter",
        "title": "A Robust Statistical Analysis of the Role of Hydropower on the System\n  Electricity Price and Price Volatility",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Hydroelectric power (hydropower) is unique in that it can function as both a\nconventional source of electricity and as backup storage (pumped hydroelectric\nstorage) for providing energy in times of high demand on the grid. This study\nexamines the impact of hydropower on system electricity price and price\nvolatility in the region served by the New England Independent System Operator\n(ISONE) from 2014 - 2020. We perform a robust holistic analysis of the mean and\nquantile effects, as well as the marginal contributing effects of hydropower in\nthe presence of solar and wind resources. First, the price data is adjusted for\ndeterministic temporal trends, correcting for seasonal, weekend, and diurnal\neffects that may obscure actual representative trends in the data. Using\nmultiple linear regression and quantile regression, we observe that hydropower\ncontributes to a reduction in the system electricity price and price\nvolatility. While hydropower has a weak impact on decreasing price and\nvolatility at the mean, it has greater impact at extreme quantiles (> 70th\npercentile). At these higher percentiles, we find that hydropower provides a\nstabilizing effect on price volatility in the presence of volatile resources\nsuch as wind. We conclude with a discussion of the observed relationship\nbetween hydropower and system electricity price and volatility.\n"
    },
    {
        "paper_id": 2203.0213,
        "authors": "Lei Dong, Rui Du, Yu Liu",
        "title": "Mapping evolving population geography in China",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  China's demographic changes have important global economic and geopolitical\nimplications. Yet, our understanding of such transitions at the micro-spatial\nscale remains limited due to spatial inconsistency of the census data caused by\nadministrative boundary adjustments. To fill this gap, we manually collected\nand built a population census panel from 2010 to 2020 at both the county and\nprefectural-city levels. We show that the massive internal migration drives\nChina's increasing population concentration and regional disparity, resulting\nin severe population aging in shrinking cities and increasing gender imbalance\nin growing cities.\n"
    },
    {
        "paper_id": 2203.02198,
        "authors": "Rita Dias Pereira, Pietro Biroli, Titus Galama, Stephanie von Hinke,\n  Hans van Kippersluis, Cornelius A. Rietveld, Kevin Thom",
        "title": "Gene-Environment Interplay in the Social Sciences",
        "comments": "Newest version before the publication of the article",
        "journal-ref": "Oxford Research Encyclopedia of Economics and Finance. Oxford\n  University Press. 2022",
        "doi": "10.1093/acrefore/9780190625979.013.804",
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  Nature (one's genes) and nurture (one's environment) jointly contribute to\nthe formation and evolution of health and human capital over the life cycle.\nThis complex interplay between genes and environment can be estimated and\nquantified using genetic information readily available in a growing number of\nsocial science data sets. Using genetic data to improve our understanding of\nindividual decision making, inequality, and to guide public policy is possible\nand promising, but requires a grounding in essential genetic terminology,\nknowledge of the literature in economics and social-science genetics, and a\ncareful discussion of the policy implications and prospects of the use of\ngenetic data in the social sciences and economics.\n"
    },
    {
        "paper_id": 2203.02323,
        "authors": "Fei Gao, Shuaiqiang Liu, Cornelis W. Oosterlee, Nico M. Temme",
        "title": "Solution of integrals with fractional Brownian motion for different\n  Hurst indices",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper, we will evaluate integrals that define the conditional\nexpectation, variance and characteristic function of stochastic processes with\nrespect to fractional Brownian motion (fBm) for all relevant Hurst indices,\ni.e. $H \\in (0,1)$. The fractional Ornstein-Uhlenbeck (fOU) process, for\nexample, gives rise to highly nontrivial integration formulas that need careful\nanalysis when considering the whole range of Hurst indices. We will show that\nthe classical technique of analytic continuation, from complex analysis,\nprovides a way of extending the domain of validity of an integral, from\n$H\\in(1/2,1)$, to the larger domain, $H\\in(0,1)$. Numerical experiments for\ndifferent Hurst indices confirm the robustness and efficiency of the integral\nformulations presented here. Moreover, we provide accurate and highly efficient\nfinancial option pricing results for processes that are related to the fOU\nprocess, with the help of Fourier cosine expansions.\n"
    },
    {
        "paper_id": 2203.02576,
        "authors": "Bernardo Alves Furtado and Gustavo Onofre Andre\\~ao",
        "title": "Machine Learning Simulates Agent-Based Model Towards Policy",
        "comments": "32 pages, 9 tables, 3 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Public Policies are not intrinsically positive or negative. Rather, policies\nprovide varying levels of effects across different recipients.\nMethodologically, computational modeling enables the application of multiple\ninfluences on empirical data, thus allowing for heterogeneous response to\npolicies. We use a random forest machine learning algorithm to emulate an\nagent-based model (ABM) and evaluate competing policies across 46 Metropolitan\nRegions (MRs) in Brazil. In doing so, we use input parameters and output\nindicators of 11,076 actual simulation runs and one million emulated runs. As a\nresult, we obtain the optimal (and non-optimal) performance of each region over\nthe policies. Optimum is defined as a combination of GDP production and the\nGini coefficient inequality indicator for the full ensemble of Metropolitan\nRegions. Results suggest that MRs already have embedded structures that favor\noptimal or non-optimal results, but they also illustrate which policy is more\nbeneficial to each place. In addition to providing MR-specific policies'\nresults, the use of machine learning to simulate an ABM reduces the\ncomputational burden, whereas allowing for a much larger variation among model\nparameters. The coherence of results within the context of larger\nuncertainty--vis-\\`a-vis those of the original ABM--reinforces robustness of\nthe model. At the same time the exercise indicates which parameters should\npolicymakers intervene on, in order to work towards precise policy optimal\ninstruments.\n"
    },
    {
        "paper_id": 2203.02599,
        "authors": "Yuanying Guan, Zhanyi Jiao and Ruodu Wang",
        "title": "A reverse ES (CVaR) optimization formula",
        "comments": "23 pages, 15 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
        "abstract": "  The celebrated Expected Shortfall (ES) optimization formula implies that ES\nat a fixed probability level is the minimum of a linear real function plus a\nscaled mean excess function. We establish a reverse ES optimization formula,\nwhich says that a mean excess function at any fixed threshold is the maximum of\nan ES curve minus a linear function. Despite being a simple result, this\nformula reveals elegant symmetries between the mean excess function and the ES\ncurve, as well as their optimizers. The reverse ES optimization formula is\nclosely related to the Fenchel-Legendre transforms, and our formulas are\ngeneralized from ES to optimized certainty equivalents, a popular class of\nconvex risk measures. We analyze worst-case values of the mean excess function\nunder two popular settings of model uncertainty to illustrate the usefulness of\nthe reverse ES optimization formula, and this is further demonstrated with an\napplication using insurance datasets.\n"
    },
    {
        "paper_id": 2203.02625,
        "authors": "Bradley J. Larsen, Timothy J. Ryan, Steven Greene, Marc J.\n  Hetherington, Rahsaan Maxwell, and Steven Tadelis",
        "title": "Using Donald Trump's COVID-19 Vaccine Endorsement to Give Public Health\n  a Shot in the Arm: A Large-Scale Ad Experiment",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  We report a large-scale randomized controlled trial designed to assess\nwhether the partisan cue of a pro-vaccine message from Donald Trump would\ninduce Americans to get COVID-19 vaccines. Our study involved presenting a\n27-second advertisement to millions of U.S. YouTube users in October 2021.\nResults indicate that the campaign increased the number of vaccines in the\naverage treated county by 103. Spread across 1,014 treated counties, the total\neffect of the campaign was an estimated increase of 104,036 vaccines. The\ncampaign was cost-effective: with an overall budget of about \\$100,000, the\ncost to obtain an additional vaccine was about \\$1 or less.\n"
    },
    {
        "paper_id": 2203.02756,
        "authors": "Fabian Braesemann, Max Schuler",
        "title": "Data Science vs Putin: How much does each of us pay for Putin's war?",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Putin's Ukraine war has caused gas prices to skyrocket. Because of Europe's\ndependence on Russian gas supplies, we all pay significantly more for heating,\ninvoluntarily helping to fund Russia's war against Ukraine. Based on an\nanalysis of real-time gas price data, we present a calculation that estimates\nevery household's financial contribution for heating paid to Russian gas\nsuppliers daily at current prices - six euros per household and day. We show\nways everyone can save energy and help reduce the dependency on Russian gas\nsupply.\n"
    },
    {
        "paper_id": 2203.02804,
        "authors": "Michael Kastoryano and Nicola Pancotti",
        "title": "A highly efficient tensor network algorithm for multi-asset Fourier\n  options pricing",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Risk assessment and in particular derivatives pricing is one of the core\nareas in computational finance and accounts for a sizeable fraction of the\nglobal computing resources of the financial industry. We outline a\nquantum-inspired algorithm for multi-asset options pricing. The algorithm is\nbased on tensor networks, which have allowed for major conceptual and numerical\nbreakthroughs in quantum many body physics and quantum computation. In the\nproof-of-concept example explored, the tensor network approach yields several\norders of magnitude speedup over vanilla Monte Carlo simulations. We take this\nas good evidence that the use of tensor network methods holds great promise for\nalleviating the computation burden of risk evaluation in the financial and\nother industries, thus potentially lowering the carbon footprint these\nsimulations incur today.\n"
    },
    {
        "paper_id": 2203.02834,
        "authors": "Fabian Scheller, Stefan Wald, Hendrik Kondziella, Philipp Andreas\n  Gunkel, Thomas Bruckner, Dogan Keles",
        "title": "Future role and economic benefits of hydrogen and synthetic energy\n  carriers in Germany: a systematic review of long-term energy scenarios",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  Determining the development of Germany's energy system by taking the energy\ntransition objectives into account is the subject of a series of studies. Since\ntheir assumptions and results play a significant role in the political energy\ndebate for understanding the role of hydrogen and synthetic energy carriers, a\nbetter discussion is needed. This article provides a comparative assessment of\npublished transition pathways for Germany to assess the role and advantages of\nhydrogen and synthetic energy carriers. Twelve energy studies were selected and\n37 scenarios for the years 2030 and 2050 were evaluated. Despite the\nvariations, the two carriers will play an important future role. While their\ndeployment is expected to have only started by 2030 with a mean demand of 91\nTWh/a (4% of the final energy demand) in Germany, they will be an essential\npart by 2050 with a mean demand of 480 TWh/a (24% of the final energy demand).\nA moderately positive correlation (0.53) between the decarbonisation targets\nand the share of hydrogen-based carriers in final energy demand underlines the\nrelevance for reaching the climate targets. Additionally, value creation\neffects of about 5 bn EUR/a in 2030 can be expected for hydrogen-based\ncarriers. By 2050, these effects will increase to almost 16 bn EUR/a. Hydrogen\nis expected to be mainly produced domestically while synthetic fuels are\nprojected to be mostly imported. Despite of all the advantages, the\nconstruction of the facilities is associated with high costs which should be\nnot neglected in the discussion.\n"
    },
    {
        "paper_id": 2203.02943,
        "authors": "Christian Bayer, Masaaki Fukasawa, Shonosuke Nakahara",
        "title": "On the weak convergence rate in the discretization of rough volatility\n  models",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We study the weak convergence rate in the discretization of rough volatility\nmodels. After showing a lower bound $2H$ under a general model, where $H$ is\nthe Hurst index of the volatility process, we give a sharper bound $H + 1/2$\nunder a linear model.\n"
    },
    {
        "paper_id": 2203.03003,
        "authors": "Raad Khraishi and Ramin Okhrati",
        "title": "Offline Deep Reinforcement Learning for Dynamic Pricing of Consumer\n  Credit",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We introduce a method for pricing consumer credit using recent advances in\noffline deep reinforcement learning. This approach relies on a static dataset\nand requires no assumptions on the functional form of demand. Using both real\nand synthetic data on consumer credit applications, we demonstrate that our\napproach using the conservative Q-Learning algorithm is capable of learning an\neffective personalized pricing policy without any online interaction or price\nexperimentation.\n"
    },
    {
        "paper_id": 2203.03069,
        "authors": "Scott W. Hegerty",
        "title": "Banking Deserts,\" City Size, and Socioeconomic Characteristics in Medium\n  and Large U.S. Cities",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  A lack of financial access, which is often an issue in many central-city U.S.\nneighborhoods, can be linked to higher interest rates as well as negative\nhealth and psychological outcomes. A number of analyses of \"banking deserts\"\nhave also found these areas to be poorer and less White than other parts of the\ncity. While previous research has examined specific cities, or has classified\nareas by population densities, no study to date has examined a large set of\nindividual cities. This study looks at 319 U.S. cities with populations greater\nthan 100,000 and isolates areas with fewer than 0.318 banks per square mile\nbased on distances from block-group centroids. The relative shares of these\n\"deserts\" appears to be independent of city population across the sample, and\nthere is little relationship between these shares and socioeconomic variables\nsuch as the poverty rate or the percentage of Black residents. One plausible\nexplanation is that only a subset of many cities' poorest, least White block\ngroups can be classified as banking deserts; nearby block groups with similar\nsocioeconomic characteristics are therefore non-deserts. Outside of the\nNortheast, non-desert areas tend to be poorer than deserts, suggesting that\nincome- and bank-poor neighborhoods might not be as prevalent as is commonly\nassumed.\n"
    },
    {
        "paper_id": 2203.03179,
        "authors": "Ariel Neufeld, Julian Sester, Daiying Yin",
        "title": "Detecting data-driven robust statistical arbitrage strategies with deep\n  neural networks",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We present an approach, based on deep neural networks, that allows\nidentifying robust statistical arbitrage strategies in financial markets.\nRobust statistical arbitrage strategies refer to trading strategies that enable\nprofitable trading under model ambiguity. The presented novel methodology\nallows to consider a large amount of underlying securities simultaneously and\ndoes not depend on the identification of cointegrated pairs of assets, hence it\nis applicable on high-dimensional financial markets or in markets where\nclassical pairs trading approaches fail. Moreover, we provide a method to build\nan ambiguity set of admissible probability measures that can be derived from\nobserved market data. Thus, the approach can be considered as being model-free\nand entirely data-driven. We showcase the applicability of our method by\nproviding empirical investigations with highly profitable trading performances\neven in 50 dimensions, during financial crises, and when the cointegration\nrelationship between asset pairs stops to persist.\n"
    },
    {
        "paper_id": 2203.03565,
        "authors": "Edmond Berisha, Ram Sewak Dubey, Eric Olson",
        "title": "Monetary policy and the racial wage gap",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper aims to clarify the relationship between monetary policy shocks\nand wage inequality. We emphasize the relevance of within and between wage\ngroup inequalities in explaining total wage inequality in the United States.\nRelying on the quarterly data for the period 2000-2020, our analysis shows that\nracial disparities explain 12\\% of observed total wage inequality.\nSubsequently, we examine the role of monetary policy in wage inequality. We do\nnot find compelling evidence that shows that monetary policy plays a role in\nexacerbating the racial wage gap. However, there is evidence that accommodative\nmonetary policy plays a role in magnifying between group wage inequalities but\nthe impact occurs after 2008.\n"
    },
    {
        "paper_id": 2203.03874,
        "authors": "Benjamin Avanzi and Mark Lavender and Greg Taylor and Bernard Wong",
        "title": "Detection and treatment of outliers for multivariate robust loss\n  reserving",
        "comments": null,
        "journal-ref": null,
        "doi": "10.1017/S1748499523000155",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Traditional techniques for calculating outstanding claim liabilities such as\nthe chain ladder are notoriously at risk of being distorted by outliers in past\nclaims data. Unfortunately, the literature in robust methods of reserving is\nscant, with notable exceptions such as Verdonck and Debruyne (2011) and\nVerdonck and Van Wouwe (2011). In this paper, we put forward two alternative\nrobust bivariate chain-ladder techniques to extend the approach of Verdonck and\nVan Wouwe (2011). The first technique is based on Adjusted Outlyingness (Hubert\nand Van der Veeken, 2008) and explicitly incorporates skewness into the\nanalysis whilst providing a unique measure of outlyingness for each\nobservation. The second technique is based on bagdistance (Hubert et al., 2016)\nwhich is derived from the bagplot however is able to provide a unique measure\nof outlyingness and a means to adjust outlying observations based on this\nmeasure.\n  Furthermore, we extend our robust bivariate chain-ladder approach to an\nN-dimensional framework. The implementation of the methods, especially beyond\nbivariate, is not trivial. This is illustrated on a trivariate data set from\nAustralian general insurers, and results under the different outlier detection\nand treatment mechanisms are compared.\n"
    },
    {
        "paper_id": 2203.03991,
        "authors": "Yuanrong Wang, Tomaso Aste",
        "title": "Sparsification and Filtering for Spatial-temporal GNN in Multivariate\n  Time-series",
        "comments": "7 pages, 1 figure, 3tables",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-sa/4.0/",
        "abstract": "  We propose an end-to-end architecture for multivariate time-series prediction\nthat integrates a spatial-temporal graph neural network with a matrix filtering\nmodule. This module generates filtered (inverse) correlation graphs from\nmultivariate time series before inputting them into a GNN. In contrast with\nexisting sparsification methods adopted in graph neural network, our model\nexplicitly leverage time-series filtering to overcome the low signal-to-noise\nratio typical of complex systems data. We present a set of experiments, where\nwe predict future sales from a synthetic time-series sales dataset. The\nproposed spatial-temporal graph neural network displays superior performances\nwith respect to baseline approaches, with no graphical information, and with\nfully connected, disconnected graphs and unfiltered graphs.\n"
    },
    {
        "paper_id": 2203.04001,
        "authors": "Edoardo Gallo, Yohanes E. Riyanto, Nilanjan Roy, Tat-How Teh",
        "title": "Cooperation and punishment mechanisms in uncertain and dynamic networks",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  This paper examines experimentally how reputational uncertainty and the rate\nof change of the social environment determine cooperation. Reputational\nuncertainty significantly decreases cooperation, while a fast-changing social\nenvironment only causes a second-order qualitative increase in cooperation. At\nthe individual level, reputational uncertainty induces more leniency and\nforgiveness in imposing network punishment through the link proposal and\nremoval processes, inhibiting the formation of cooperative clusters. However,\nthis effect is significant only in the fast-changing environment and not in the\nslow-changing environment. A substitution pattern between network punishment\nand action punishment (retaliatory defection) explains this discrepancy across\nthe two social environments.\n"
    },
    {
        "paper_id": 2203.04053,
        "authors": "Yevhen Havrylenko, Maria Hinken, Rudi Zagst",
        "title": "Risk sharing in equity-linked insurance products: Stackelberg\n  equilibrium between an insurer and a reinsurer",
        "comments": "39 pages",
        "journal-ref": null,
        "doi": "10.1017/asb.2023.32",
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  We study the optimal investment-reinsurance problem in the context of\nequity-linked insurance products. Such products often have a capital guarantee,\nwhich can motivate insurers to purchase reinsurance. Since a reinsurance\ncontract implies an interaction between the insurer and the reinsurer, we model\nthe optimization problem as a Stackelberg game. The reinsurer is the leader in\nthe game and maximizes its expected utility by selecting its optimal investment\nstrategy and a safety loading in the reinsurance contract it offers to the\ninsurer. The reinsurer can assess how the insurer will rationally react on each\naction of the reinsurer. The insurance company is the follower and maximizes\nits expected utility by choosing its investment strategy and the amount of\nreinsurance the company purchases at the price offered by the reinsurer. In\nthis game, we derive the Stackelberg equilibrium for general utility functions.\nFor power utility functions, we calculate the equilibrium explicitly and find\nthat the reinsurer selects the largest reinsurance premium such that the\ninsurer may still buy the maximal amount of reinsurance. Since in the\nequilibrium the insurer is indifferent in the amount of reinsurance, in\npractice, the reinsurer should consider charging a smaller reinsurance premium\nthan the equilibrium one. Therefore, we propose several criteria for choosing\nsuch a discount rate and investigate its wealth-equivalent impact on the\nexpected utility of each party.\n"
    },
    {
        "paper_id": 2203.04101,
        "authors": "Daniel L. Bennett, Christopher J. Boudreaux, and Boris N. Nikolaev",
        "title": "Populist Discourse and Entrepreneurship: The Role of Political Ideology\n  and Institutions",
        "comments": "55 pages, 5 Tables, 3 Figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Using institutional economic theory as our guiding framework, we develop a\nmodel to describe how populist discourse by a nation's political leader\ninfluences entrepreneurship. We hypothesize that populist discourse reduces\nentrepreneurship by creating regime uncertainty concerning the future stability\nof the institutional environment, resulting in entrepreneurs anticipating\nhigher future transaction costs. Our model highlights two important factors\nthat moderate the relationship. First, is the strength of political checks and\nbalances, which we hypothesize weakens the negative relationship between\npopulist discourse and entrepreneurship by providing entrepreneurs with greater\nconfidence that the actions of a populist will be constrained. Second, the\npolitical ideology of the leader moderates the relationship between populist\ndiscourse and entrepreneurship. The anti-capitalistic rhetoric of left-wing\npopulism will create greater regime uncertainty than right-wing populism, which\nis often accompanied by rhetoric critical of free trade and foreigners but also\nsupportive of business interests. The effect of centrist populism, which is\naccompanied by a mix of contradictory and often moderate ideas that make it\ndifficult to discern future transaction costs, will have a weaker negative\neffect on entrepreneurship than either left-wing or right-wing populism. We\nempirically test our model using a multi-level design and a dataset comprised\nof more than 780,000 individuals in 33 countries over the period 2002-2016. Our\nanalysis largely supports our theory regarding the moderating role of ideology.\nStill, surprisingly, our findings suggest that the negative effect of populism\non entrepreneurship is greater in nations with stronger checks and balances.\n"
    },
    {
        "paper_id": 2203.04579,
        "authors": "Federico Cornalba, Constantin Disselkamp, Davide Scassola, Christopher\n  Helf",
        "title": "Multi-Objective reward generalization: Improving performance of Deep\n  Reinforcement Learning for applications in single-asset trading",
        "comments": "13 pages, 20 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We investigate the potential of Multi-Objective, Deep Reinforcement Learning\nfor stock and cryptocurrency single-asset trading: in particular, we consider a\nMulti-Objective algorithm which generalizes the reward functions and discount\nfactor (i.e., these components are not specified a priori, but incorporated in\nthe learning process). Firstly, using several important assets (cryptocurrency\npairs BTCUSD, ETHUSDT, XRPUSDT, and stock indexes AAPL, SPY, NIFTY50), we\nverify the reward generalization property of the proposed Multi-Objective\nalgorithm, and provide preliminary statistical evidence showing increased\npredictive stability over the corresponding Single-Objective strategy.\nSecondly, we show that the Multi-Objective algorithm has a clear edge over the\ncorresponding Single-Objective strategy when the reward mechanism is sparse\n(i.e., when non-null feedback is infrequent over time). Finally, we discuss the\ngeneralization properties with respect to the discount factor. The entirety of\nour code is provided in open source format.\n"
    },
    {
        "paper_id": 2203.04878,
        "authors": "L. Mungu\\'ia, J. C. Escalante and E. Robles Belmont",
        "title": "The management of scientific and technological infrastructures: the case\n  of the Mexican National Laboratories",
        "comments": "24 pages, 3 figures, 5 tables, research article",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-sa/4.0/",
        "abstract": "  The effectiveness of research units is assessed on the basis of their\nperformance in relation to scientific, technological and innovation production,\nthe quality of their results and their contribution to the solution of\nscientific and social problems. This paper examines the management practices\nemployed in some Mexican National Laboratories in order to identify those that\ncould explain their effectiveness in meeting their objectives. The results of\nother works that propose common elements among laboratories with outstanding\nperformance are used and verified directly in the field. Considering the\ninherent complexity of each field of knowledge and the socio-spatial\ncharacteristics in which the laboratories operate, we report which management\npractices are relevant for their effectiveness, how they contribute to their\nconsolidation as fundamental scientific and technological infrastructures, how\nthese can be translated into indicators that support the evaluation of their\nperformance, and still pending.\n"
    },
    {
        "paper_id": 2203.04924,
        "authors": "Jeong Yu Han, Patrick Rebentrost",
        "title": "Quantum advantage for multi-option portfolio pricing and valuation\n  adjustments",
        "comments": "42 pages, 1 figure, based on a BComp Dissertation at National\n  University of Singapore",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  A critical problem in the financial world deals with the management of risk,\nfrom regulatory risk to portfolio risk. Many such problems involve the analysis\nof securities modelled by complex dynamics that cannot be captured\nanalytically, and hence rely on numerical techniques that simulate the\nstochastic nature of the underlying variables. These techniques may be\ncomputationally difficult or demanding. Hence, improving these methods offers a\nvariety of opportunities for quantum algorithms. In this work, we study the\nproblem of Credit Valuation Adjustments (CVAs) which have significant\nimportance in the valuation of derivative portfolios. We propose quantum\nalgorithms that accelerate statistical sampling processes to approximate the\nCVA under different measures of dispersion, using known techniques in Quantum\nMonte Carlo (QMC) and analyse the conditions under which we may employ these\ntechniques.\n"
    },
    {
        "paper_id": 2203.05139,
        "authors": "Benjamin Avanzi and Ping Chen and Lars Frederik Brandt Henriksen and\n  Bernard Wong",
        "title": "On the surplus management of funds with assets and liabilities in\n  presence of solvency requirements",
        "comments": null,
        "journal-ref": null,
        "doi": "10.1080/03461238.2022.2116725",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper we consider a company whose assets and liabilities evolve\naccording to a correlated bivariate geometric Brownian motion, such as in\nGerber and Shiu (2003). We determine what dividend strategy maximises the\nexpected present value of dividends until ruin in two cases: (i) when\nshareholders won't cover surplus shortfalls and a solvency constraint (as in\nPaulsen, 2003) is consequently imposed, and (ii) when shareholders are always\nto fund any capital deficiency with capital (asset) injections. In the latter\ncase, ruin will never occur and the objective is to maximise the difference\nbetween dividends and capital injections.\n  Developing and using appropriate verification lemmas, we show that the\noptimal dividend strategy is, in both cases, of barrier type. Both value\nfunctions are derived in closed form. Furthermore, the barrier is defined on\nthe ratio of assets to liabilities, which mimics some of the dividend\nstrategies that can be observed in practice by insurance companies. Existence\nand uniqueness of the optimal strategies are shown. Results are illustrated.\n"
    },
    {
        "paper_id": 2203.05545,
        "authors": "Matthew Lorig, Natchanon Suaysom",
        "title": "Optimal times to buy and sell a home",
        "comments": "21 pages, 5 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We consider a financial market in which the risk-free rate of interest is\nmodeled as a Markov diffusion. We suppose that home prices are set by a\nrepresentative home-buyer, who can afford to pay only a fixed cash-flow per\nunit time for housing. The cash-flow is a fraction of the representative\nhome-buyer's salary, which grows at a rate that is proportional to the\nrisk-free rate of interest. As a result, in the long-run, higher interest rates\nlead to faster growth of home prices. The representative home-buyer finances\nthe purchase of a home by taking out a mortgage. The mortgage rate paid by the\nhome-buyer is fixed at the time of purchase and equal to the risk-free rate of\ninterest plus a positive constant. As the home-buyer can only afford to pay a\nfixed cash-flow per unit time, a higher mortgage rate limits the size of the\nloan the home-buyer can take out. As a result, the short-term effect of higher\ninterest rates is to lower the value of homes. In this setting, we consider an\ninvestor who wishes to buy and then sell a home in order to maximize his\ndiscounted expected profit. This leads to a nested optimal stopping problem. We\nuse a nonnegative concave majorant approach to derive the investor's optimal\nbuying and selling strategies. Additionally, we provide a detailed analytic and\nnumerical study of the case in which the risk-free rate of interest is modeled\nby a Cox-Ingersoll-Ross (CIR) process. We also examine, in the case of CIR\ninterest rates, the expected time that the investor waits before buying and\nthen selling a home when following the optimal strategies.\n"
    },
    {
        "paper_id": 2203.05593,
        "authors": "Mario Bossler, Martin Popp",
        "title": "Labor Demand on a Tight Leash",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  We develop a labor demand model that encompasses pre-match hiring cost\narising from tight labor markets. Through the lens of the model, we study the\neffect of labor market tightness on firms' labor demand by applying novel\nshift-share instruments to the universe of German firms. In line with theory,\nwe find that a doubling in tightness reduces firms' employment by 5 percent.\nTaking into account the resulting search externalities, the wage elasticity of\nfirms' labor demand reduces from -0.7 to -0.5 through reallocation effects. In\nlight of our results, pre-match hiring cost amount to 40 percent of annual wage\npayments.\n"
    },
    {
        "paper_id": 2203.05595,
        "authors": "Harshil Sahai and Michael Bailey",
        "title": "Social Networks and Spatial Mobility: Evidence from Facebook in India",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  This paper studies the role of social networks in spatial mobility across\nIndia. Using aggregated and de-identified data from the world's largest online\nsocial network, we (i) document new descriptive findings on the structure of\nsocial networks and spatial mobility in India; (ii) quantify the effects of\nsocial networks on annual migration choice; and (iii) embed these estimates in\na spatial equilibrium model to study the wage implications of increasing social\nconnectedness. Across millions of individuals, we find that multiple measures\nof social capital are concentrated among the rich and educated and among\nmigrants. Across destinations, both mobility patterns and social networks are\nconcentrated toward richer areas. A model of migration suggests individuals are\nindifferent between a 10% increase in destination wages and a 12-16% increase\nin destination social networks. Accounting for networks reduces the\nmigration-distance relationship by 19%. In equilibrium, equalizing social\nnetworks across locations improves average wages by 3% (24% for the bottom\nwage-quartile), a larger impact than removing the marginal cost of distance. We\nfind evidence of an economic support mechanism, with destination economic\nimprovements reducing the migration-network elasticity. We also find suggestive\nevidence for an emotional support mechanism from qualitative surveys among\nFacebook users. Difference-in-difference estimates suggest college attendance\ndelivers a 20% increase in network size and diversity. Taken together, our data\nsuggest that - by reducing effective moving costs - increasing social\nconnectedness across space may have considerable economic gains.\n"
    },
    {
        "paper_id": 2203.05603,
        "authors": "Miguel A. Ruiz-Ortiz, Jos\\'e Carlos G\\'omez-Larra\\~naga and Jes\\'us\n  Rodr\\'iguez-Viorato",
        "title": "A persistent-homology-based turbulence index & some applications of TDA\n  on financial markets",
        "comments": "Code and data are found in this repository:\n  https://github.com/miguelruor/TDA_financial_markets",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Topological Data Analysis (TDA) is a modern approach to Data Analysis\nfocusing on the topological features of data; it has been widely studied in\nrecent years and used extensively in Biology, Physics, and many other areas.\nHowever, financial markets have been studied slightly through TDA. Here we\npresent a quick review of some recent applications of TDA on financial markets,\nincluding applications in the early detection of turbulence periods in\nfinancial markets and how TDA can help to get new insights while investing.\nAlso, we propose a new turbulence index based on persistent homology -- the\nfundamental tool for TDA -- that seems to capture critical transitions in\nfinancial data; we tested our index with different financial time series\n(S&P500, Russel 2000, S&P/BMV IPC and Nikkei 225) and crash events (Black\nMonday crash, dot-com crash, 2007-08 crash and COVID-19 crash). Furthermore, we\ninclude an introduction to persistent homology so the reader can understand\nthis paper without knowing TDA.\n"
    },
    {
        "paper_id": 2203.05673,
        "authors": "Mufhumudzi Muthivhi, Terence L. van Zyl",
        "title": "Fusion of Sentiment and Asset Price Predictions for Portfolio\n  Optimization",
        "comments": "9 pages, 4 figures, 6 tables",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  The fusion of public sentiment data in the form of text with stock price\nprediction is a topic of increasing interest within the financial community.\nHowever, the research literature seldom explores the application of investor\nsentiment in the Portfolio Selection problem. This paper aims to unpack and\ndevelop an enhanced understanding of the sentiment aware portfolio selection\nproblem. To this end, the study uses a Semantic Attention Model to predict\nsentiment towards an asset. We select the optimal portfolio through a\nsentiment-aware Long Short Term Memory (LSTM) recurrent neural network for\nprice prediction and a mean-variance strategy. Our sentiment portfolio\nstrategies achieved on average a significant increase in revenue above the\nnon-sentiment aware models. However, the results show that our strategy does\nnot outperform traditional portfolio allocation strategies from a stability\nperspective. We argue that an improved fusion of sentiment prediction with a\ncombination of price prediction and portfolio optimization leads to an enhanced\nportfolio selection strategy.\n"
    },
    {
        "paper_id": 2203.05719,
        "authors": "Hyong Chol O, Dae Song Choe, Gyong-Dok Rim",
        "title": "Analytical Pricing of 2 Factor Structural PDE model for a Puttable Bond\n  with Credit Risk",
        "comments": "12 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  In this paper is proposed a 2 factor structural PDE model of pricing puttable\nbond with credit risk and derived the analytical pricing formula. To this end,\nfirst, a 2 factor structural (PDE) model of pricing zero coupon bond with\ncredit risk is provided, the analytical pricing formula is derived under some\nconditions for default boundary and default recovery, and the strict\nmonotonicity of the bond price function with respect to the firm value variable\nis proved. Then a (2 factor) pricing model of the option on zero coupon bond\nwith credit risk is provided and under some condition on the exercise price its\nanalytical pricing formula is derived by transforming the 2 factor model into a\nterminal boundary value problem for Black-Scholes equation with time dependent\ncoefficient using zero coupon bond as numeraire. Using it, we provide the\npricing formulae of the puttable and callable bonds with credit risk.\n"
    },
    {
        "paper_id": 2203.05726,
        "authors": "Hyong-Chol O, Tae-Song Choe",
        "title": "General properties of the Solutions to Moving Boundary Problems for\n  Black-Sholes Equations",
        "comments": "22 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  We study general properties such as the solution representation of a moving\nboundary value problem of the Black-Scholes equation, its min-max estimation,\nlower and upper gradient estimates, and strict monotonicity with respect to the\nspatial variables of the solution. These results are used in the study of a\nstructural model of pricing puttable bond with credit risk. We first prove the\nsolution representation of a special fixed boundary value problem of the\nBlack-Scholes equation, the min-max estimate, the lower and upper gradient\nestimates, and the strict monotonicity with respect to the spatial variables of\nthe solution. Then, these results are applied to give the solution\nrepresentation of a moving boundary value problem of the Black-Scholes equation\nwith moving boundary in the form of an exponential function, the min-max\nestimate, the lower and upper gradient estimates, and the strict monotonicity\nresults on the spatial variables of the solution. Finally, we illustrate how\nthese results can be used in the derivation of analytical pricing formulae and\nfinancial analysis of price functions of puttable bonds with credit risk\n(corporate bonds with one early redemption date). Our results can be used for\nthe derivation and analysis of the analytical pricing formulae of the\none-factor structural model of a more general puttable bonds with credit risk\n(corporate bond with several early redemption dates).\n"
    },
    {
        "paper_id": 2203.05762,
        "authors": "Sudo Yi and Deok-Sun Lee",
        "title": "Structure of international trade hypergraphs",
        "comments": "13 pages, 8 figures",
        "journal-ref": "J. Stat. Mech. (2022) 103402",
        "doi": "10.1088/1742-5468/ac946f",
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  We study the structure of the international trade hypergraph consisting of\ntriangular hyperedges representing the exporter-importer-product relationship.\nMeasuring the mean hyperdegree of the adjacent vertices, we first find its\nbehaviors different from those in the pairwise networks and explain the origin\nby tracing the relation between the hyperdegree and the pairwise degree. To\ninterpret the observed hyperdegree correlation properties in the context of\ntrade strategies, we decompose the correlation into two components by\nidentifying one with the background correlation remnant even in the exponential\nrandom hypergraphs preserving the given empirical hyperdegree sequence. The\nother component characterizes the net correlation and reveals the bias of the\nexporters of low hyperdegree towards the importers of high hyperdegree and the\nproducts of low hyperdegree, which information is not readily accessible in the\npairwise networks. Our study demonstrates the power of the hypergraph approach\nin the study of real-world complex systems and offers a theoretical framework.\n"
    },
    {
        "paper_id": 2203.06537,
        "authors": "Cameron Fen",
        "title": "Fast Simulation-Based Bayesian Estimation of Heterogeneous and\n  Representative Agent Models using Normalizing Flow Neural Networks",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  This paper proposes a simulation-based deep learning Bayesian procedure for\nthe estimation of macroeconomic models. This approach is able to derive\nposteriors even when the likelihood function is not tractable. Because the\nlikelihood is not needed for Bayesian estimation, filtering is also not needed.\nThis allows Bayesian estimation of HANK models with upwards of 800 latent\nstates as well as estimation of representative agent models that are solved\nwith methods that don't yield a likelihood--for example, projection and value\nfunction iteration approaches. I demonstrate the validity of the approach by\nestimating a 10 parameter HANK model solved via the Reiter method that\ngenerates 812 covariates per time step, where 810 are latent variables, showing\nthis can handle a large latent space without model reduction. I also estimate\nthe algorithm with an 11-parameter model solved via value function iteration,\nwhich cannot be estimated with Metropolis-Hastings or even conventional maximum\nlikelihood estimators. In addition, I show the posteriors estimated on\nSmets-Wouters 2007 are higher quality and faster using simulation-based\ninference compared to Metropolis-Hastings. This approach helps address the\ncomputational expense of Metropolis-Hastings and allows solution methods which\ndon't yield a tractable likelihood to be estimated.\n"
    },
    {
        "paper_id": 2203.06539,
        "authors": "Mike Ludkovski",
        "title": "Regression Monte Carlo for Impulse Control",
        "comments": "18 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  I develop a numerical algorithm for stochastic impulse control in the spirit\nof Regression Monte Carlo for optimal stopping. The approach consists in\ngenerating statistical surrogates (aka functional approximators) for the\ncontinuation function. The surrogates are recursively trained by empirical\nregression over simulated state trajectories. In parallel, the same surrogates\nare used to learn the intervention function characterizing the optimal impulse\namounts. I discuss appropriate surrogate types for this task, as well as the\nchoice of training sets. Case studies from forest rotation and irreversible\ninvestment illustrate the numerical scheme and highlight its flexibility and\nextensibility. Implementation in \\texttt{R} is provided as a publicly available\npackage posted on GitHub.\n"
    },
    {
        "paper_id": 2203.0654,
        "authors": "Cameron Fen and Samir Undavia",
        "title": "Improving Macroeconomic Model Validity and Forecasting Performance with\n  Pooled Country Data using Structural, Reduced Form, and Neural Network Model",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  We show that pooling countries across a panel dimension to macroeconomic data\ncan improve by a statistically significant margin the generalization ability of\nstructural, reduced form, and machine learning (ML) methods to produce\nstate-of-the-art results. Using GDP forecasts evaluated on an out-of-sample\ntest set, this procedure reduces root mean squared error by 12\\% across\nhorizons and models for certain reduced-form models and by 24\\% across horizons\nfor dynamic structural general equilibrium models. Removing US data from the\ntraining set and forecasting out-of-sample country-wise, we show that\nreduced-form and structural models are more policy-invariant when trained on\npooled data, and outperform a baseline that uses US data only. Given the\ncomparative advantage of ML models in a data-rich regime, we demonstrate that\nour recurrent neural network model and automated ML approach outperform all\ntested baseline economic models. Robustness checks indicate that our\noutperformance is reproducible, numerically stable, and generalizable across\nmodels.\n"
    },
    {
        "paper_id": 2203.06848,
        "authors": "Md Rashidul Hasan, Muntasir A Kabir, Rezoan A Shuvro, and Pankaz Das",
        "title": "A Comparative Study on Forecasting of Retail Sales",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Predicting product sales of large retail companies is a challenging task\nconsidering volatile nature of trends, seasonalities, events as well as unknown\nfactors such as market competitions, change in customer's preferences, or\nunforeseen events, e.g., COVID-19 outbreak. In this paper, we benchmark\nforecasting models on historical sales data from Walmart to predict their\nfuture sales. We provide a comprehensive theoretical overview and analysis of\nthe state-of-the-art timeseries forecasting models. Then, we apply these models\non the forecasting challenge dataset (M5 forecasting by Kaggle). Specifically,\nwe use a traditional model, namely, ARIMA (Autoregressive Integrated Moving\nAverage), and recently developed advanced models e.g., Prophet model developed\nby Facebook, light gradient boosting machine (LightGBM) model developed by\nMicrosoft and benchmark their performances. Results suggest that ARIMA model\noutperforms the Facebook Prophet and LightGBM model while the LightGBM model\nachieves huge computational gain for the large dataset with negligible\ncompromise in the prediction accuracy.\n"
    },
    {
        "paper_id": 2203.06865,
        "authors": "Nelson Vadori",
        "title": "Calibration of Derivative Pricing Models: a Multi-Agent Reinforcement\n  Learning Perspective",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  One of the most fundamental questions in quantitative finance is the\nexistence of continuous-time diffusion models that fit market prices of a given\nset of options. Traditionally, one employs a mix of intuition, theoretical and\nempirical analysis to find models that achieve exact or approximate fits. Our\ncontribution is to show how a suitable game theoretical formulation of this\nproblem can help solve this question by leveraging existing developments in\nmodern deep multi-agent reinforcement learning to search in the space of\nstochastic processes. Our experiments show that we are able to learn local\nvolatility, as well as path-dependence required in the volatility process to\nminimize the price of a Bermudan option. Our algorithm can be seen as a\nparticle method \\textit{\\`{a} la} Guyon \\textit{et} Henry-Labordere where\nparticles, instead of being designed to ensure $\\sigma_{loc}(t,S_t)^2 =\n\\mathbb{E}[\\sigma_t^2|S_t]$, are learning RL-driven agents cooperating towards\nmore general calibration targets.\n"
    },
    {
        "paper_id": 2203.07145,
        "authors": "Jonas Crevecoeur, Katrien Antonio, Stijn Desmedt, Alexandre Masquelein",
        "title": "Bridging the gap between pricing and reserving with an occurrence and\n  development model for non-life insurance claims",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Due to the presence of reporting and settlement delay, claim data sets\ncollected by non-life insurance companies are typically incomplete, facing\nright censored claim count and claim severity observations. Current practice in\nnon-life insurance pricing tackles these right censored data via a two-step\nprocedure. First, best estimates are computed for the number of claims that\noccurred in past exposure periods and the ultimate claim severities, using the\nincomplete, historical claim data. Second, pricing actuaries build predictive\nmodels to estimate technical, pure premiums for new contracts by treating these\nbest estimates as actual observed outcomes, hereby neglecting their inherent\nuncertainty. We propose an alternative approach that brings valuable insights\nfor both non-life pricing as well as reserving. As such we effectively bridge\nthese two key actuarial tasks that have traditionally been discussed in silos.\nHereto we develop a granular occurrence and development model for non-life\nclaims that tackles reserving and at the same time resolves the inconsistency\nin traditional pricing techniques between actual observations and imputed best\nestimates. We illustrate our proposed model on an insurance as well as a\nreinsurance portfolio. The advantages of our proposed strategy are most\ncompelling in the reinsurance illustration where large uncertainties in the\nbest estimates originate from long reporting and settlement delays, low claim\nfrequencies and heavy (even extreme) claim sizes.\n"
    },
    {
        "paper_id": 2203.07282,
        "authors": "Santiago Camara",
        "title": "Granular Linkages, Supplier Cost Shocks & Export Performance",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/publicdomain/zero/1.0/",
        "abstract": "  This paper presents evidence on the granular nature of firms' network of\nforeign suppliers and studies its implications for the impact of supplier\nshocks on domestic firms' performance. To demonstrate this, I use customs level\ninformation on transactions between Argentinean firms and foreign firms. I\nhighlight two novel stylized facts: (i) the distribution of domestic firms'\nnumber of foreign suppliers is highly skewed with the median firm reporting\nlinkages with only two, (ii) firms focus imported value on one top-supplier,\neven when controlling for firm size. Motivated by these facts I construct a\ntheoretical framework of heterogeneous firms subject to search frictions in the\nmarket for foreign suppliers. Through a calibration exercise I study the\nframework's predictions and test them in the data using a shift-share\nidentification strategy. Results present evidence of significant frictions in\nthe market for foreign suppliers and strong import-export complementarities.\n"
    },
    {
        "paper_id": 2203.07458,
        "authors": "Marco Di Francesco and Kevin Kamm",
        "title": "On the deterministic-shift extended CIR model in a negative interest\n  rate framework",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  In this paper, we propose a new exogenous model to address the problem of\nnegative interest rates that preserves the analytical tractability of the\noriginal Cox-Ingersoll-Ross (CIR) model with a perfect fit to the observed\nterm-structure. We use the difference of two independent CIR processes and\napply the deterministic-shift extension technique. To allow for a fast\ncalibration to the market swaption surface, we apply the Gram-Charlier\nexpansion to calculate the swaption prices in our model. We run several\nnumerical tests to demonstrate the strengths of this model by using Monte-Carlo\ntechniques. In particular, the model produces close Bermudan swaption prices\ncompared to Bloomberg's Hull-White one-factor model. Moreover, it finds\nconstant maturity swap (CMS) rates very close to Bloomberg's CMS rates.\n"
    },
    {
        "paper_id": 2203.0755,
        "authors": "Igor Halperin",
        "title": "Phases of MANES: Multi-Asset Non-Equilibrium Skew Model of a Strongly\n  Non-Linear Market with Phase Transitions",
        "comments": "38 pages, 11 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper presents an analytically tractable and practically-oriented model\nof non-linear dynamics of a multi-asset market in the limit of a large number\nof assets. The asset price dynamics are driven by money flows into the market\nfrom external investors, and their price impact. This leads to a model of a\nmarket as an ensemble of interacting non-linear oscillators with the Langevin\ndynamics. In a homogeneous portfolio approximation, the mean field treatment of\nthe resulting Langevin dynamics produces the McKean-Vlasov equation as a\ndynamic equation for market returns. Due to the strong non-linearity of the\nMcKean-Vlasov equation, the resulting dynamics give rise to ergodicity breaking\nand first- or second-order phase transitions under variations of model\nparameters. Using a tractable potential of the Non-Equilibrium Skew (NES) model\npreviously suggested by the author for a single-stock case, the new Multi-Asset\nNES (MANES) model enables an analytically tractable framework for a multi-asset\nmarket. The equilibrium expected market log-return is obtained as a\nself-consistent mean field of the McKean-Vlasov equation, and derived in closed\nform in terms of parameters that are inferred from market prices of S&P 500\nindex options. The model is able to accurately fit the market data for either a\nbenign or distressed market environments, while using only a single volatility\nparameter.\n"
    },
    {
        "paper_id": 2203.0766,
        "authors": "Eiji Yamamura, Youki Koska, Yoshiro Tsutsui, Fumio Ohtake",
        "title": "Effect of the COVID-19 vaccine on preventive behaviors: Evidence from\n  Japan",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Vaccination against the coronavirus disease 2019 (COVID-19) is a key measure\nto reduce the probability of getting infected with the disease. Accordingly,\nthis might significantly change an individuals perception and decision-making\nin daily life. For instance, it is predicted that with widespread vaccination,\nindividuals will exhibit less rigid preventive behaviors, such as staying at\nhome, frequently washing hands, and wearing a mask. We observed the same\nindividuals on a monthly basis for 18 months, from March 2020 (the early stage\nof the COVID-19 pandemic) to September 2021, in Japan to independently\nconstruct large sample panel data (N=54,007). Using the data, we compare the\nindividuals preventive behaviors before and after they got vaccinated;\nadditionally, we compare their behaviors with those individuals who did not get\nvaccinated. Furthermore, we compare the effect of vaccination on the\nindividuals less than or equal to 40 years of age with those greater than 40\nyears old. The major findings determined after controlling for individual\ncharacteristics using the fixed effects model and various factors are as\nfollows. First, as opposed to the prediction, based on the whole sample, the\nvaccinated people were observed to stay at home and did not change their habits\nof frequently washing hands and wearing a mask. Second, using the sub-sample of\nindividuals aged equal to or below 40, we find that the vaccinated people are\nmore likely to go out. Third, the results obtained using a sample comprising\npeople aged over 40 are similar to those obtained using the whole sample.\nPreventive behaviors are affecting oneself and generating externalities on\nothers during this pandemic. Informal social norms motivate people to increase\nor maintain preventive behaviors even after being vaccinated in societies where\nsuch behaviors are not enforced.\n"
    },
    {
        "paper_id": 2203.07663,
        "authors": "Eiji Yamamura, Youki Kosaka, Yoshiro Tsutsui, Fumio Ohtake",
        "title": "Gender differences of the effect of vaccination on perceptions of\n  COVID-19 and mental health in Japan",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Vaccination has been promoted to mitigate the spread of the coronavirus\ndisease 2019 (COVID-19). Vaccination is expected to reduce the probability of\nand alleviate the seriousness of COVID-19 infection. Accordingly, this might\nsignificantly change an individuals subjective well-being and mental health.\nHowever, it is unknown how vaccinated people perceive the effectiveness of\nCOVID-19 and how their subjective well-being and mental health change after\nvaccination. We thus observed the same individuals on a monthly basis from\nMarch 2020 to September 2021 in all parts of Japan. Then, large sample panel\ndata (N=54,007) were independently constructed. Using the data, we compared the\nindividuals perceptions of COVID-19, subjective well-being, and mental health\nbefore and after vaccination. Furthermore, we compared the effect of\nvaccination on the perceptions of COVID-19 and mental health for females and\nmales. We used the fixed-effects model to control for individual time-invariant\ncharacteristics. The major findings were as follows: First, the vaccinated\npeople perceived the probability of getting infected and the seriousness of\nCOVID-19 to be lower than before vaccination. This was observed not only when\nwe used the whole sample, but also when we used sub-samples. Second, using the\nwhole sample, subjective well-being and mental health improved. The same\nresults were also observed using the sub-sample of females, whereas the\nimprovements were not observed using a sub-sample of males.\n"
    },
    {
        "paper_id": 2203.07774,
        "authors": "Jan Arvid Berg, Robin Fritsch, Lioba Heimbach, Roger Wattenhofer",
        "title": "An Empirical Study of Market Inefficiencies in Uniswap and SushiSwap",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Decentralized exchanges are revolutionizing finance. With their ever-growing\nincrease in popularity, a natural question that begs to be asked is: how\nefficient are these new markets?\n  We find that nearly 30% of analyzed trades are executed at an unfavorable\nrate. Additionally, we observe that, especially during the DeFi summer in 2020,\nprice inaccuracies across the market plagued DEXes. Uniswap and SushiSwap,\nhowever, quickly adapt to their increased volumes. We see an increase in market\nefficiency with time during the observation period. Nonetheless, the DEXes\nstill struggle to track the reference market when cryptocurrency prices are\nhighly volatile. During such periods of high volatility, we observe the market\nbecoming less efficient - manifested by an increased prevalence in cyclic\narbitrage opportunities.\n"
    },
    {
        "paper_id": 2203.07865,
        "authors": "Guillaume Coqueret",
        "title": "Characteristics-driven returns in equilibrium",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  We reverse-engineer the equilibrium construction process of asset prices in\norder to obtain returns which depend on firm characteristics, possibly in a\nlinear fashion. One key requirement is that agents must have demands that rely\nseparately on firm characteristics and on the log-price of assets. Market\nclearing via exogenous (non-factor driven) supply, combined with linear demands\nin characteristics, yields the sought form. The coefficients in the resulting\nlinear expressions are scaled net aggregate demands for characteristics, as\nwell as their variations, and both can be jointly estimated via panel\nregressions. Conditions underpinning asset pricing anomalies are derived and\nunderline the theoretical importance of the links between characteristics.\nEmpirically, when the number of characteristics is small, the value and\nmomentum anomalies are mostly driven by firm-specific fixed-effects, i.e.,\nlatent demands, which highlights the shortcomings of low-dimensional models.\n"
    },
    {
        "paper_id": 2203.0794,
        "authors": "Anantya Bhatnagar, Dimitri D. Vvedensky",
        "title": "Quantum effects in an expanded Black-Scholes model",
        "comments": "11 pages, 3 figures (Supplement - 26 pages, 20 figures)",
        "journal-ref": "Eur. Phys. J. B, 95:138 (2022)",
        "doi": "10.1140/epjb/s10051-022-00402-0",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The limitations of the classical Black-Scholes model are examined by\ncomparing calculated and actual historical prices of European call options on\nstocks from several sectors of the S&P 500. Persistent differences between the\ntwo prices point to an expanded model proposed by Segal and Segal (1998) in\nwhich information not simultaneously observable or actionable with public\ninformation can be represented by an additional pseudo-Wiener process. A real\nlinear combination of the original and added processes leads to a commutation\nrelation analogous to that between a boson field and its canonical momentum in\nquantum field theory. The resulting pricing formula for a European call option\nreplaces the classical volatility with the norm of a complex quantity, whose\nimaginary part is shown to compensate for the disparity between prices obtained\nfrom the classical Black-Scholes model and actual prices of the test call\noptions. This provides market evidence for the influence of a non-classical\nprocess on the price of a security based on non-commuting operators.\n"
    },
    {
        "paper_id": 2203.08143,
        "authors": "Ishu Gupta and Tarun Kumar Madan and Sukhman Singh and Ashutosh Kumar\n  Singh",
        "title": "HiSA-SMFM: Historical and Sentiment Analysis based Stock Market\n  Forecasting Model",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  One of the pillars to build a country's economy is the stock market. Over the\nyears, people are investing in stock markets to earn as much profit as possible\nfrom the amount of money that they possess. Hence, it is vital to have a\nprediction model which can accurately predict future stock prices. With the\nhelp of machine learning, it is not an impossible task as the various machine\nlearning techniques if modeled properly may be able to provide the best\nprediction values. This would enable the investors to decide whether to buy,\nsell or hold the share. The aim of this paper is to predict the future of the\nfinancial stocks of a company with improved accuracy. In this paper, we have\nproposed the use of historical as well as sentiment data to efficiently predict\nstock prices by applying LSTM. It has been found by analyzing the existing\nresearch in the area of sentiment analysis that there is a strong correlation\nbetween the movement of stock prices and the publication of news articles.\nTherefore, in this paper, we have integrated these factors to predict the stock\nprices more accurately.\n"
    },
    {
        "paper_id": 2203.08144,
        "authors": "Pok Wah Chan",
        "title": "DeepTrust: A Reliable Financial Knowledge Retrieval Framework For\n  Explaining Extreme Pricing Anomalies",
        "comments": "72 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Extreme pricing anomalies may occur unexpectedly without a trivial cause, and\nequity traders typically experience a meticulous process to source disparate\ninformation and analyze its reliability before integrating it into the trusted\nknowledge base. We introduce DeepTrust, a reliable financial knowledge\nretrieval framework on Twitter to explain extreme price moves at speed, while\nensuring data veracity using state-of-the-art NLP techniques. Our proposed\nframework consists of three modules, specialized for anomaly detection,\ninformation retrieval and reliability assessment. The workflow starts with\nidentifying anomalous asset price changes using machine learning models trained\nwith historical pricing data, and retrieving correlated unstructured data from\nTwitter using enhanced queries with dynamic search conditions. DeepTrust\nextrapolates information reliability from tweet features, traces of generative\nlanguage model, argumentation structure, subjectivity and sentiment signals,\nand refine a concise collection of credible tweets for market insights. The\nframework is evaluated on two self-annotated financial anomalies, i.e., Twitter\nand Facebook stock price on 29 and 30 April 2021. The optimal setup outperforms\nthe baseline classifier by 7.75% and 15.77% on F0.5-scores, and 10.55% and\n18.88% on precision, respectively, proving its capability in screening\nunreliable information precisely. At the same time, information retrieval and\nreliability assessment modules are analyzed individually on their effectiveness\nand causes of limitations, with identified subjective and objective factors\nthat influence the performance. As a collaborative project with Refinitiv, this\nframework paves a promising path towards building a scalable commercial\nsolution that assists traders to reach investment decisions on pricing\nanomalies with authenticated knowledge from social media platforms in\nreal-time.\n"
    },
    {
        "paper_id": 2203.08196,
        "authors": "Michael Samet, Christian Bayer, Chiheb Ben Hammouda, Antonis\n  Papapantoleon, Ra\\'ul Tempone",
        "title": "Optimal Damping with Hierarchical Adaptive Quadrature for Efficient\n  Fourier Pricing of Multi-Asset Options in L\\'evy Models",
        "comments": null,
        "journal-ref": null,
        "doi": "10.21314/JCF.2023.012",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Efficiently pricing multi-asset options is a challenging problem in\nquantitative finance. When the characteristic function is available,\nFourier-based methods are competitive compared to alternative techniques\nbecause the integrand in the frequency space often has a higher regularity than\nthat in the physical space. However, when designing a numerical quadrature\nmethod for most Fourier pricing approaches, two key aspects affecting the\nnumerical complexity should be carefully considered: (i) the choice of damping\nparameters that ensure integrability and control the regularity class of the\nintegrand and (ii) the effective treatment of high dimensionality. We propose\nan efficient numerical method for pricing European multi-asset options based on\ntwo complementary ideas to address these challenges. First, we smooth the\nFourier integrand via an optimized choice of the damping parameters based on a\nproposed optimization rule. Second, we employ sparsification and\ndimension-adaptivity techniques to accelerate the convergence of the quadrature\nin high dimensions. The extensive numerical study on basket and rainbow options\nunder the multivariate geometric Brownian motion and some L\\'evy models\ndemonstrates the advantages of adaptivity and the damping rule on the numerical\ncomplexity of quadrature methods. Moreover, for the tested two-asset examples,\nthe proposed approach outperforms the COS method in terms of computational\ntime. Finally, we show significant speed-up compared to the Monte Carlo method\nfor up to six dimensions.\n"
    },
    {
        "paper_id": 2203.08223,
        "authors": "Hamdi Ra\\\"issi",
        "title": "On the dependence structure of the trade/no trade sequence of illiquid\n  assets",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  In this paper, we propose to consider the dependence structure of the\ntrade/no trade categorical sequence of individual illiquid stocks returns. The\nframework considered here is wide as constant and time-varying zero returns\nprobability are allowed. The ability of our approach in highlighting illiquid\nstock's features is underlined for a variety of situations. More specifically,\nwe show that long-run effects for the trade/no trade categorical sequence may\nbe spuriously detected in presence of a non-constant zero returns probability.\nMonte Carlo experiments, and the analysis of stocks taken from the Chilean\nfinancial market, illustrate the usefulness of the tools developed in the\npaper.\n"
    },
    {
        "paper_id": 2203.08224,
        "authors": "Konstantin G\\\"orgen, Jonas Meirer, Melanie Schienle",
        "title": "Predicting Value at Risk for Cryptocurrencies With Generalized Random\n  Forests",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  We study the prediction of Value at Risk (VaR) for cryptocurrencies. In\ncontrast to classic assets, returns of cryptocurrencies are often highly\nvolatile and characterized by large fluctuations around single events.\nAnalyzing a comprehensive set of 105 major cryptocurrencies, we show that\nGeneralized Random Forests (GRF) (Athey et al., 2019) adapted to quantile\nprediction have superior performance over other established methods such as\nquantile regression, GARCH-type and CAViaR models. This advantage is especially\npronounced in unstable times and for classes of highly-volatile\ncryptocurrencies. Furthermore, we identify important predictors during such\ntimes and show their influence on forecasting over time. Moreover, a\ncomprehensive simulation study also indicates that the GRF methodology is at\nleast on par with existing methods in VaR predictions for standard types of\nfinancial returns and clearly superior in the cryptocurrency setup.\n"
    },
    {
        "paper_id": 2203.08635,
        "authors": "Tobias Fissler and Hajo Holzmann",
        "title": "Measurability of functionals and of ideal point forecasts",
        "comments": "13 pages",
        "journal-ref": "Electronic Journal of Statistics 2022, Vol. 16, No. 2, 5019-5034",
        "doi": "10.1214/22-EJS2062",
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  The ideal probabilistic forecast for a random variable $Y$ based on an\ninformation set $\\mathcal{F}$ is the conditional distribution of $Y$ given\n$\\mathcal{F}$. In the context of point forecasts aiming to specify a functional\n$T$ such as the mean, a quantile or a risk measure, the ideal point forecast is\nthe respective functional applied to the conditional distribution. This paper\nprovides a theoretical justification why this ideal forecast is actually a\nforecast, that is, an $\\mathcal{F}$-measurable random variable. To that end,\nthe appropriate notion of measurability of $T$ is clarified and this\nmeasurability is established for a large class of practically relevant\nfunctionals, including elicitable ones. More generally, the measurability of\n$T$ implies the measurability of any point forecast which arises by applying\n$T$ to a probabilistic forecast. Similar measurability results are established\nfor proper scoring rules, the main tool to evaluate the predictive accuracy of\nprobabilistic forecasts.\n"
    },
    {
        "paper_id": 2203.08677,
        "authors": "Martin Friesen and Peng Jin",
        "title": "Volterra square-root process: Stationarity and regularity of the law",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The Volterra square-root process on $\\mathbb{R}_+^m$ is an affine Volterra\nprocess with continuous sample paths. Under a suitable integrability condition\non the resolvent of the second kind associated with the Volterra convolution\nkernel, we establish the existence of limiting distributions. In contrast to\nthe classical square-root diffusion process, here the limiting distributions\nmay depend on the initial state of the process. Our result shows that the\nnon-uniqueness of limiting distributions is closely related to the\nintegrability of the Volterra convolution kernel. Using an extension of the\nexponential-affine transformation formula we also give the construction of\nstationary processes associated with the limiting distributions. Finally, we\nprove that the time marginals as well as the limiting distributions, when\nrestricted to the interior of the state space $\\mathbb{R}_{+}^m$, are\nabsolutely continuous with respect to the Lebesgue measure and their densities\nbelong to some weighted Besov space of type $B_{1,\\infty}^{\\lambda}$.\n"
    },
    {
        "paper_id": 2203.08794,
        "authors": "Fabien Le Floc'h",
        "title": "Double sweep LU decomposition for American options under negative rates",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  The classic Brennan-Schwartz algorithm to solve the linear complementary\nproblem, which arises from the finite difference discretization of the partial\ndifferential equation related to American option pricing does not lead to the\nexact solution under negative interest rates. This is due to the two exercise\nboundaries which may appear under negative interest rate, while the algorithm\nwas proven to lead to the exact solution in the case of a single exercise\nboundary only. This paper explains that two sweeps of the Brennan-Schwartz\nalgorithm in two directions is enough to recover the exact solution.\n"
    },
    {
        "paper_id": 2203.08859,
        "authors": "Geoff Lindsell",
        "title": "Convergence of Optimal Expected Utility for a Sequence of Discrete-Time\n  Markets in Initially Enlarged Filtrations",
        "comments": "arXiv admin note: text overlap with arXiv:1907.11424 by other authors",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  In this paper, we extend Kreps' conjecture that optimal expected utility in\nthe classic Black-Scholes-Merton (BSM) economy is the limit of optimal expected\nutility for a sequence of discrete-time economies in initially enlarged\nfiltrations converge to the BSM economy in an initially enlarged filtration in\na \"strong\" sense. The n-th discrete-time economy is generated by a scaled\nn-step random walk, based on an unscaled random variable with mean 0, variance\n1, and bounded support. Moreover, the informed insider knows each functional\ngenerating the enlarged filtrations path-by-path. We confirm Kreps' conjecture\nin initially enlarged filtrations when the consumer's utility function U has\nasymptotic elasticity strictly less than one.\n"
    },
    {
        "paper_id": 2203.08933,
        "authors": "Tuheen Ahmmed, Afsoon Alidadi, Zichao Zhang, Aizaz U. Chaudhry, Halim\n  Yanikomeroglu",
        "title": "The Digital Divide in Canada and the Role of LEO Satellites in Bridging\n  the Gap",
        "comments": "Accepted for publication in IEEE Communications Magazine, Total 7\n  pages, 5 figures, 1 table",
        "journal-ref": null,
        "doi": "10.13140/RG.2.2.18223.20648",
        "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
        "abstract": "  Overcoming the digital divide in rural and remote areas has always been a big\nchallenge for Canada with its huge geographical area. In 2016, the Canadian\nRadio-television and Telecommunications Commission announced broadband Internet\nas a basic service available for all Canadians. However, approximately one\nmillion Canadians still do not have access to broadband services as of 2020.\nThe COVID-19 pandemic has made the situation more challenging, as social,\neconomic, and educational activities have increasingly been transferred online.\nThe condition is more unfavorable for Indigenous communities. A key challenge\nin deploying rural and remote broadband Internet is to plan and implement\nhigh-capacity backbones, which are now available only in denser urban areas.\nFor any Internet provider, it is almost impossible to make a viable business\nproposal in these areas. For example, the vast land of the Northwest\nTerritories, Yukon, and Nunavuts diverse geographical features present\nobstacles for broadband infrastructure. In this paper, we investigate the\ndigital divide in Canada with a focus on rural and remote areas. In so doing,\nwe highlight two potential solutions using low Earth orbit (LEO) constellations\nto deliver broadband Internet in rural and remote areas to address the access\ninequality and the digital divide. The first solution involves integrating LEO\nconstellations as a backbone for the existing 4G/5G telecommunications network.\nThis solution uses satellites in a LEO constellation to provide a backhaul\nnetwork connecting the 4G/5G access network to its core network. The 3rd\nGeneration Partnership Project already specifies how to integrate LEO satellite\nnetworks into the 4G/5G network, and the Canadian satellite operator Telesat\nhas already showcased this solution with one terrestrial operator, TIM Brasil,\nin their 4G network.\n"
    },
    {
        "paper_id": 2203.09015,
        "authors": "Archil Gulisashvili",
        "title": "Multivariate Stochastic Volatility Models and Large Deviation Principles",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We establish a comprehensive sample path large deviation principle (LDP) for\nlog-processes associated with multivariate time-inhomogeneous stochastic\nvolatility models. Examples of models for which the new LDP holds include\nGaussian models, non-Gaussian fractional models, mixed models, models with\nreflection, and models in which the volatility process is a solution to a\nVolterra type stochastic integral equation. The LDP for log-processes is used\nto obtain large deviation style asymptotic formulas for the distribution\nfunction of the first exit time of a log-process from an open set and for the\nprice of a multidimensional binary barrier option. We also prove a sample path\nLDP for solutions to Volterra type stochastic integral equations with\npredictable coefficients depending on auxiliary stochastic processes.\n"
    },
    {
        "paper_id": 2203.09118,
        "authors": "Ehsan Valavi, Joel Hestness, Newsha Ardalani, Marco Iansiti",
        "title": "Time and the Value of Data",
        "comments": "43 Pages, 8 Figures, Harvard Business School Working Paper 21-016",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Managers often believe that collecting more data will continually improve the\naccuracy of their machine learning models. However, we argue in this paper that\nwhen data lose relevance over time, it may be optimal to collect a limited\namount of recent data instead of keeping around an infinite supply of older\n(less relevant) data. In addition, we argue that increasing the stock of data\nby including older datasets may, in fact, damage the model's accuracy.\nExpectedly, the model's accuracy improves by increasing the flow of data\n(defined as data collection rate); however, it requires other tradeoffs in\nterms of refreshing or retraining machine learning models more frequently.\n  Using these results, we investigate how the business value created by machine\nlearning models scales with data and when the stock of data establishes a\nsustainable competitive advantage. We argue that data's time-dependency weakens\nthe barrier to entry that the stock of data creates. As a result, a competing\nfirm equipped with a limited (yet sufficient) amount of recent data can develop\nmore accurate models. This result, coupled with the fact that older datasets\nmay deteriorate models' accuracy, suggests that created business value doesn't\nscale with the stock of available data unless the firm offloads less relevant\ndata from its data repository. Consequently, a firm's growth policy should\nincorporate a balance between the stock of historical data and the flow of new\ndata.\n  We complement our theoretical results with an experiment. In the experiment,\nwe empirically measure the loss in the accuracy of a next word prediction model\ntrained on datasets from various time periods. Our empirical measurements\nconfirm the economic significance of the value decline over time. For example,\n100MB of text data, after seven years, becomes as valuable as 50MB of current\ndata for the next word prediction task.\n"
    },
    {
        "paper_id": 2203.09128,
        "authors": "Ehsan Valavi, Joel Hestness, Marco Iansiti, Newsha Ardalani, Feng Zhu,\n  Karim R. Lakhani",
        "title": "Time Dependency, Data Flow, and Competitive Advantage",
        "comments": "24 Pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Data is fundamental to machine learning-based products and services and is\nconsidered strategic due to its externalities for businesses, governments,\nnon-profits, and more generally for society. It is renowned that the value of\norganizations (businesses, government agencies and programs, and even\nindustries) scales with the volume of available data. What is often less\nappreciated is that the data value in making useful organizational predictions\nwill range widely and is prominently a function of data characteristics and\nunderlying algorithms.\n  In this research, our goal is to study how the value of data changes over\ntime and how this change varies across contexts and business areas (e.g. next\nword prediction in the context of history, sports, politics). We focus on data\nfrom Reddit.com and compare the value's time-dependency across various Reddit\ntopics (Subreddits). We make this comparison by measuring the rate at which\nuser-generated text data loses its relevance to the algorithmic prediction of\nconversations. We show that different subreddits have different rates of\nrelevance decline over time.\n  Relating the text topics to various business areas of interest, we argue that\ncompeting in a business area in which data value decays rapidly alters\nstrategies to acquire competitive advantage. When data value decays rapidly,\naccess to a continuous flow of data will be more valuable than access to a\nfixed stock of data. In this kind of setting, improving user engagement and\nincreasing user-base help creating and maintaining a competitive advantage.\n"
    },
    {
        "paper_id": 2203.09157,
        "authors": "Dar\\'io Blanco-Fern\\'andez, Stephan Leitner, Alexandra Rausch",
        "title": "Dynamic groups in complex task environments: To change or not to change\n  a winning team?",
        "comments": "22 pages, 6 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Organisations rely upon group formation to solve complex tasks, and groups\noften adapt to the demands of the task they face by changing their composition\nperiodically. Previous research comes to ambiguous results regarding the\neffects of group adaptation on task performance. This paper aims to understand\nthe impact of group adaptation, defined as a process of periodically changing a\ngroup's composition, on complex task performance and considers the moderating\nrole of individual learning and task complexity in this relationship. We base\nour analyses on an agent-based model of adaptive groups in a complex task\nenvironment based on the NK-framework. The results indicate that reorganising\nwell-performing groups might be beneficial, but only if individual learning is\nrestricted. However, there are also cases in which group adaptation might\nunfold adverse effects. We provide extensive analyses that shed additional\nlight on and, thereby, help explain the ambiguous results of previous research.\n"
    },
    {
        "paper_id": 2203.09162,
        "authors": "Dario Blanco-Fernandez, Stephan Leitner, Alexandra Rausch",
        "title": "Interactions between the individual and the group level in\n  organizations: The case of learning and autonomous group adaptation",
        "comments": "47 pages, 9 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Previous research on organizations often focuses on either the individual,\nteam, or organizational level. There is a lack of multidimensional research on\nemergent phenomena and interactions between the mechanisms at different levels.\nThis paper takes a multifaceted perspective on individual learning and\nautonomous group formation and adaptation. To analyze interactions between the\ntwo levels, we introduce an agent-based model that captures an organization\nwith a population of heterogeneous agents who learn and are limited in their\nrationality. To solve a task, agents form a group that can be adapted from time\nto time. We explore organizations that promote learning and group adaptation\neither simultaneously or sequentially and analyze the interactions between the\nactivities and the effects on performance. We observe underproportional\ninteractions when tasks are interdependent and show that pushing learning and\ngroup adaptation too far might backfire and decrease performance significantly.\n"
    },
    {
        "paper_id": 2203.09177,
        "authors": "Fuzhou Gong, Ting Wang",
        "title": "The Variable Volatility Elasticity Model from Commodity Markets",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper, we propose and study a novel continuous-time model, based on\nthe well-known constant elasticity of variance (CEV) model, to describe the\nasset price process. The basic idea is that the volatility elasticity of the\nCEV model can not be treated as a constant from the perspective of stochastic\nanalysis. To address this issue, we deduce the price process of assets from the\nperspective of volatility elasticity, propose the constant volatility\nelasticity (CVE) model, and further derive a more general variable volatility\nelasticity (VVE) model. Moreover, our model can describe the positive\ncorrelation between volatility and asset prices existing in the commodity\nmarkets, while CEV model can only describe the negative correlation. Through\nthe empirical research on the financial market, many assets, especially\ncommodities, often show this positive correlation phenomenon in some time\nperiods, which shows that our model has strong practical application value.\nFinally, we provide the explicit pricing formula of European options based on\nour model. This formula has an elegant form convenient to calculate, which is\nsimilarly to the renowned Black-Scholes formula and of great significance to\nthe research of derivatives market.\n"
    },
    {
        "paper_id": 2203.09298,
        "authors": "Paul Gassiat",
        "title": "Weak error rates of numerical schemes for rough volatility",
        "comments": "20 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Simulation of rough volatility models involves discretization of stochastic\nintegrals where the integrand is a function of a (correlated) fractional\nBrownian motion of Hurst index $H \\in (0,1/2)$. We obtain results on the rate\nof convergence for the weak error of such approximations, in the special cases\nwhen either the integrand is the fBm itself, or the test function is cubic. Our\nresult states that the convergence is of order $(3H+ \\frac{1}{2}) \\wedge 1$ for\nexact left-point discretization, and of order $H+\\frac{1}{2}$ for the hybrid\nscheme with well-chosen weights.\n"
    },
    {
        "paper_id": 2203.09548,
        "authors": "Manuel Alberto M. Ferreira",
        "title": "Maintenance Problem of Insufficiently Financed Pension Funds -- A\n  Stochastic Approach",
        "comments": "15 pages and no figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  The generic case of pensions fund that it is not sufficiently auto financed\nand it is thoroughly maintained with an external financing effort is considered\nin this chapter. To represent the unrestricted reserves value process of this\nkind of funds, a time homogeneous diffusion stochastic process with finite\nexpected time to ruin is proposed. Then it is projected a financial tool that\nregenerates the diffusion at some level with positive value every time the\ndiffusion hits a barrier placed at the origin. So, the financing effort can be\nmodeled as a renewal-reward process if the regeneration level is preserved\nconstant. The perpetual maintenance cost expected values and the finite time\nmaintenance cost evaluations are studied. An application of this approach when\nthe unrestricted reserves value process behaves as a generalized Brownian\nmotion process is presented.\n"
    },
    {
        "paper_id": 2203.09612,
        "authors": "Ziteng Cheng and Sebastian Jaimungal",
        "title": "Risk-Averse Markov Decision Processes through a Distributional Lens",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  By adopting a distributional viewpoint on law-invariant convex risk measures,\nwe construct dynamics risk measures (DRMs) at the distributional level. We then\napply these DRMs to investigate Markov decision processes, incorporating latent\ncosts, random actions, and weakly continuous transition kernels. Furthermore,\nthe proposed DRMs allow risk aversion to change dynamically. Under mild\nassumptions, we derive a dynamic programming principle and show the existence\nof an optimal policy in both finite and infinite time horizons. Moreover, we\nprovide a sufficient condition for the optimality of deterministic actions. For\nillustration, we conclude the paper with examples from optimal liquidation with\nlimit order books and autonomous driving.\n"
    },
    {
        "paper_id": 2203.10465,
        "authors": "Wai Weng Lo, Gayan K. Kulatilleke, Mohanad Sarhan, Siamak Layeghy,\n  Marius Portmann",
        "title": "Inspection-L: Self-Supervised GNN Node Embeddings for Money Laundering\n  Detection in Bitcoin",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Criminals have become increasingly experienced in using cryptocurrencies,\nsuch as Bitcoin, for money laundering. The use of cryptocurrencies can hide\ncriminal identities and transfer hundreds of millions of dollars of dirty funds\nthrough their criminal digital wallets. However, this is considered a paradox\nbecause cryptocurrencies are goldmines for open-source intelligence, giving law\nenforcement agencies more power when conducting forensic analyses. This paper\nproposed Inspection-L, a graph neural network (GNN) framework based on a\nself-supervised Deep Graph Infomax (DGI) and Graph Isomorphism Network (GIN),\nwith supervised learning algorithms, namely Random Forest (RF), to detect\nillicit transactions for anti-money laundering (AML). To the best of our\nknowledge, our proposal is the first to apply self-supervised GNNs to the\nproblem of AML in Bitcoin. The proposed method was evaluated on the Elliptic\ndataset and shows that our approach outperforms the state-of-the-art in terms\nof key classification metrics, which demonstrates the potential of\nself-supervised GNN in the detection of illicit cryptocurrency transactions.\n"
    },
    {
        "paper_id": 2203.10571,
        "authors": "Bingyan Han",
        "title": "Distributionally robust risk evaluation with a causality constraint and\n  structural information",
        "comments": "Major revision. Add a new Section on regularized NNs",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This work studies the distributionally robust evaluation of expected values\nover temporal data. A set of alternative measures is characterized by the\ncausal optimal transport. We prove the strong duality and recast the causality\nconstraint as minimization over an infinite-dimensional test function space. We\napproximate test functions by neural networks and prove the sample complexity\nwith Rademacher complexity. An example is given to validate the feasibility of\ntechnical assumptions. Moreover, when structural information is available to\nfurther restrict the ambiguity set, we prove the dual formulation and provide\nefficient optimization methods. Our framework outperforms the classic\ncounterparts in the distributionally robust portfolio selection problem. The\nconnection with the naive strategy is also investigated numerically.\n"
    },
    {
        "paper_id": 2203.1068,
        "authors": "Yue He and Reiichiro Kawai and Yasutaka Shimizu and Kazutoshi Yamazaki",
        "title": "The Gerber-Shiu discounted penalty function: A review from practical\n  perspectives",
        "comments": "32 pages",
        "journal-ref": "Insurance: Mathematics and Economics Volume 109, March 2023, Pages\n  1-28",
        "doi": "10.1016/j.insmatheco.2022.12.003",
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  The Gerber-Shiu function provides a unified framework for the evaluation of a\nvariety of risk quantities. Ever since its establishment, it has attracted\nconstantly increasing interests in actuarial science, whereas the conventional\nresearch has been focused on finding analytical or semi-analytical solutions,\neither of which is rarely available, except for limited classes of penalty\nfunctions on rather simple risk models. In contrast to its great generality,\nthe Gerber-Shiu function does not seem sufficiently prevalent in practice,\nlargely due to a variety of difficulties in numerical approximation and\nstatistical inference. To enhance research activities on such implementation\naspects, we provide a comprehensive review of existing formulations and\nunderlying surplus processes, as well as an extensive survey of analytical,\nsemi-analytical and asymptotic methods for the Gerber-Shiu function, which\naltogether shed fresh light on its numerical methods and statistical inference\nfor further developments. On the basis of an ambitious collection of 235\nreferences, the present survey can serve as an insightful guidebook to model\nand method selection from practical perspectives as well.\n"
    },
    {
        "paper_id": 2203.10777,
        "authors": "Martin Waltz and Abhay Kumar Singh and Ostap Okhrin",
        "title": "Vulnerability-CoVaR: Investigating the Crypto-market",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  This paper proposes an important extension to Conditional Value-at-Risk\n(CoVaR), the popular systemic risk measure, and investigates its properties on\nthe cryptocurrency market. The proposed Vulnerability-CoVaR (VCoVaR) is defined\nas the Value-at-Risk (VaR) of a financial system or institution, given that at\nleast one other institution is equal or below its VaR. The VCoVaR relaxes\nnormality assumptions and is estimated via copula. While important theoretical\nfindings of the measure are detailed, the empirical study analyzes how\ndifferent distressing events of the cryptocurrencies impact the risk level of\neach other. The results show that Litecoin displays the largest impact on\nBitcoin and that each cryptocurrency is significantly affected if an event of\njoint distress among the remaining market participants occurs. The VCoVaR is\nshown to capture domino effects better than other CoVaR extensions.\n"
    },
    {
        "paper_id": 2203.11072,
        "authors": "Tahir Choulli and Ferdoos Alharbi",
        "title": "Representation for martingales living after a random time with\n  applications",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  Our financial setting consists of a market model with two flows of\ninformation. The smallest flow F is the \"public\" flow of information which is\navailable to all agents, while the larger flow G has additional information\nabout the occurrence of a random time T. This random time can model the default\ntime in credit risk or death time in life insurance. Hence the filtration G is\nthe progressive enlargement of F with T. In this framework, under some mild\nassumptions on the pair (F, T), we describe explicitly how G-local martingales\ncan be represented in terms of F-local martingale and parameters of T. This\nrepresentation complements Choulli, Daveloose and Vanmaele\n\\cite{ChoulliDavelooseVanmaele} to the case when martingales live \"after T\".\nThe application of these results to the explicit parametrization of all\ndeflators under G is fully elaborated. The results are illustrated on the case\nof jump-diffusion model and the discrete-time market model.\n"
    },
    {
        "paper_id": 2203.11091,
        "authors": "Alireza Jafari and Saman Haratizadeh",
        "title": "GCNET: graph-based prediction of stock price movement using graph\n  convolutional network",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  The importance of considering related stocks data for the prediction of stock\nprice movement has been shown in many studies, however, advanced graphical\ntechniques for modeling, embedding and analyzing the behavior of interrelated\nstocks have not been widely exploited for the prediction of stocks price\nmovements yet. The main challenges in this domain are to find a way for\nmodeling the existing relations among an arbitrary set of stocks and to exploit\nsuch a model for improving the prediction performance for those stocks. The\nmost of existing methods in this domain rely on basic graph-analysis\ntechniques, with limited prediction power, and suffer from a lack of generality\nand flexibility. In this paper, we introduce a novel framework, called GCNET\nthat models the relations among an arbitrary set of stocks as a graph structure\ncalled influence network and uses a set of history-based prediction models to\ninfer plausible initial labels for a subset of the stock nodes in the graph.\nFinally, GCNET uses the Graph Convolutional Network algorithm to analyze this\npartially labeled graph and predicts the next price direction of movement for\neach stock in the graph. GCNET is a general prediction framework that can be\napplied for the prediction of the price fluctuations of interacting stocks\nbased on their historical data. Our experiments and evaluations on a set of\nstocks from the NASDAQ index demonstrate that GCNET significantly improves the\nperformance of SOTA in terms of accuracy and MCC measures.\n"
    },
    {
        "paper_id": 2203.11318,
        "authors": "Ruan Pretorius and Terence van Zyl",
        "title": "Deep Reinforcement Learning and Convex Mean-Variance Optimisation for\n  Portfolio Management",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-sa/4.0/",
        "abstract": "  Traditional portfolio management methods can incorporate specific investor\npreferences but rely on accurate forecasts of asset returns and covariances.\nReinforcement learning (RL) methods do not rely on these explicit forecasts and\nare better suited for multi-stage decision processes. To address limitations of\nthe evaluated research, experiments were conducted on three markets in\ndifferent economies with different overall trends. By incorporating specific\ninvestor preferences into our RL models' reward functions, a more comprehensive\ncomparison could be made to traditional methods in risk-return space.\nTransaction costs were also modelled more realistically by including nonlinear\nchanges introduced by market volatility and trading volume. The results of this\nstudy suggest that there can be an advantage to using RL methods compared to\ntraditional convex mean-variance optimisation methods under certain market\nconditions. Our RL models could significantly outperform traditional\nsingle-period optimisation (SPO) and multi-period optimisation (MPO) models in\nupward trending markets, but only up to specific risk limits. In sideways\ntrending markets, the performance of SPO and MPO models can be closely matched\nby our RL models for the majority of the excess risk range tested. The specific\nmarket conditions under which these models could outperform each other\nhighlight the importance of a more comprehensive comparison of Pareto optimal\nfrontiers in risk-return space. These frontiers give investors a more granular\nview of which models might provide better performance for their specific risk\ntolerance or return targets.\n"
    },
    {
        "paper_id": 2203.11352,
        "authors": "Neelesh Tiruviluamala, Alexander Port, Erik Lewis",
        "title": "A General Framework for Impermanent Loss in Automated Market Makers",
        "comments": "24 pages, 5 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  We provide a framework for analyzing impermanent loss for general Automated\nMarket Makers (AMMs) and show that Geometric Mean Market Makers (G3Ms) are in a\nrigorous sense the simplest class of AMMs from an impermanent loss viewpoint.\nIn this context, it becomes clear why automated market makers like Curve\n([Ego19]) require more parameters in order to specify impermanent loss. We\nsuggest the proper parameter space on which impermanent loss should be\nconsidered and prove results that help in understanding the impermanent loss\ncharacteristics of different AMMs.\n"
    },
    {
        "paper_id": 2203.11972,
        "authors": "Thomas J. Sargent and John Stachurski",
        "title": "Economic Networks: Theory and Computation",
        "comments": "Textbook homepage is https://networks.quantecon.org/",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  This textbook is an introduction to economic networks, intended for students\nand researchers in the fields of economics and applied mathematics. The\ntextbook emphasizes quantitative modeling, with the main underlying tools being\ngraph theory, linear algebra, fixed point theory and programming. The text is\nsuitable for a one-semester course, taught either to advanced undergraduate\nstudents who are comfortable with linear algebra or to beginning graduate\nstudents.\n"
    },
    {
        "paper_id": 2203.12123,
        "authors": "Alexander Port, Neelesh Tiruviluamala",
        "title": "Mixing Constant Sum and Constant Product Market Makers",
        "comments": "23 pages, 10 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Two popular forms of automated market makers are constant sum and constant\nproduct (CSMM and CPMM respectively). Each has its advantages and\ndisadvantages: CSMMs have stable exchange rates but are vulnerable to arbitrage\nand can sometimes fail to provide liquidity, while a CPMM can have large\nimpermanent loss due to exchange rate changes but are always able to provide\nliquidity to participants.\n  A significant amount of work has been done in order to get the best of both\nconstant sum and constant product characteristics. Perhaps most the relevant to\nthis paper is Stableswap, which has an \"amplification coefficient\" parameter\ncontrolling the balance between the two types of behavior [Ego19]. Alternative\napproaches, such as in [AEC21], involve constructing AMMs using portfolio value\nfunctions. However, there is still much work to be done on these fronts. This\npaper presents multiple novel methods for mixing market makers and demonstrates\nnew tools for designing markets with specific features.\n"
    },
    {
        "paper_id": 2203.12173,
        "authors": "Carlos G\\'oes and Eddy Bekkers",
        "title": "The Impact of Geopolitical Conflicts on Trade, Growth, and Innovation",
        "comments": "54 pages, 14 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Geopolitical conflicts have increasingly been a driver of trade policy. We\nstudy the potential effects of global and persistent geopolitical conflicts on\ntrade, technological innovation, and economic growth. In conventional trade\nmodels the welfare costs of such conflicts are modest. We build a multi-sector\nmulti-region general equilibrium model with dynamic sector-specific knowledge\ndiffusion, which magnifies welfare losses of trade conflicts. Idea diffusion is\nmediated by the input-output structure of production, such that both sector\ncost shares and import trade shares characterize the source distribution of\nideas. Using this framework, we explore the potential impact of a \"decoupling\nof the global economy,\" a hypothetical scenario under which technology systems\nwould diverge in the global economy. We divide the global economy into two\ngeopolitical blocs -- East and West -- based on foreign policy similarity and\nmodel decoupling through an increase in iceberg trade costs (full decoupling)\nor tariffs (tariff decoupling). Results yield three main insights. First, the\nprojected welfare losses for the global economy of a decoupling scenario can be\ndrastic, as large as 15% in some regions and are largest in the lower income\nregions as they would benefit less from technology spillovers from richer\nareas. Second, the described size and pattern of welfare effects are specific\nto the model with diffusion of ideas. Without diffusion of ideas the size and\nvariation across regions of the welfare losses would be substantially smaller.\nThird, a multi-sector framework exacerbates diffusion inefficiencies induced by\ntrade costs relative to a single-sector one.\n"
    },
    {
        "paper_id": 2203.12228,
        "authors": "Yunyun Wang, Tatsushi Oka, Dan Zhu",
        "title": "Bivariate Distribution Regression with Application to Insurance Data",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
        "abstract": "  Understanding variable dependence, particularly eliciting their statistical\nproperties given a set of covariates, provides the mathematical foundation in\npractical operations management such as risk analysis and decision-making given\nobserved circumstances. This article presents an estimation method for modeling\nthe conditional joint distribution of bivariate outcomes based on the\ndistribution regression and factorization methods. This method is considered\nsemiparametric in that it allows for flexible modeling of both the marginal and\njoint distributions conditional on covariates without imposing global\nparametric assumptions across the entire distribution. In contrast to existing\nparametric approaches, our method can accommodate discrete, continuous, or\nmixed variables, and provides a simple yet effective way to capture\ndistributional dependence structures between bivariate outcomes and covariates.\nVarious simulation results confirm that our method can perform similarly or\nbetter in finite samples compared to the alternative methods. In an application\nto the study of a motor third-party liability insurance portfolio, the proposed\nmethod effectively estimates risk measures such as the conditional\nValue-at-Risk and Expected Shortfall. This result suggests that this\nsemiparametric approach can serve as an alternative in insurance risk\nmanagement.\n"
    },
    {
        "paper_id": 2203.12331,
        "authors": "Emanuele Amodio, Michele Battisti, Antonio Francesco Gravina, Andrea\n  Mario Lavezzi, Giuseppe Maggio",
        "title": "School-age Vaccination, School Openings and Covid-19 diffusion",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Do school openings trigger Covid-19 diffusion when school-age vaccination is\navailable? We investigate this question using a unique geo-referenced high\nfrequency database on school openings, vaccinations, and Covid-19 cases from\nthe Italian region of Sicily. The analysis focuses on the change of Covid-19\ndiffusion after school opening in a homogeneous geographical territory. The\nidentification of causal effects derives from a comparison of the change in\ncases before and after school opening in 2020/21, when vaccination was not\navailable, and in 2021/22, when the vaccination campaign targeted individuals\nof age 12-19 and above 19. The results indicate that, while school opening\ndetermined an increase in the growth rate of Covid-19 cases in 2020/2021, this\neffect has been substantially reduced by school-age vaccination in 2021/2022.\nIn particular, we find that an increase of approximately 10% in the vaccination\nrate of school-age population reduces the growth rate of Covid-19 cases after\nschool opening by approximately 1.4%. In addition, a counterfactual simulation\nsuggests that a permanent no vaccination scenario would have implied an\nincrease of 19% in ICU beds occupancy.\n"
    },
    {
        "paper_id": 2203.12395,
        "authors": "Dadasaheb G. Godase, P. R. Sheshagiri Rao, Anil Gore",
        "title": "Favorit: farmers volatility risk treatment",
        "comments": "13, 2",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
        "abstract": "  This paper seeks to develop a strategy based on analytics, for an individual\nIndian farmer to tackle market price fluctuations. The idea is to select a\nmonth (or a week or a day) on which to take the produce to market for a good\nreturn on the sale. The choice is based on the history of price data and\nassociated variability. Market-wise price data for the last decade or so are\nused. These ideas are applied to three vegetable crops, tomato, onion, and\ncoriander for some markets in the state of Maharashtra. It is proposed that\nsimilar work should be done crop-wise and market-wise for different parts of\nIndia by local academic groups from any of the subjects such as statistics,\nanalytics, data science, agriculture, business management, commerce, and\neconomics. The objective is to mitigate the adverse impact of price fluctuation\non farmers.\n"
    },
    {
        "paper_id": 2203.12456,
        "authors": "Jun Lu, Shao Yi",
        "title": "Reducing overestimating and underestimating volatility via the augmented\n  blending-ARCH model",
        "comments": null,
        "journal-ref": "Applied Economics and Finance 9 (2), 48-59, 2022",
        "doi": "10.11114/aef.v9i2.5507",
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  SVR-GARCH model tends to \"backward eavesdrop\" when forecasting the financial\ntime series volatility in which case it tends to simply produce the prediction\nby deviating the previous volatility. Though the SVR-GARCH model has achieved\ngood performance in terms of various performance measurements, trading\nopportunities, peak or trough behaviors in the time series are all hampered by\nunderestimating or overestimating the volatility. We propose a blending ARCH\n(BARCH) and an augmented BARCH (aBARCH) model to overcome this kind of problem\nand make the prediction towards better peak or trough behaviors. The method is\nillustrated using real data sets including SH300 and S&P500. The empirical\nresults obtained suggest that the augmented and blending models improve the\nvolatility forecasting ability.\n"
    },
    {
        "paper_id": 2203.12457,
        "authors": "Yiyang Zheng",
        "title": "Neural Network and Order Flow, Technical Analysis: Predicting short-term\n  direction of futures contract",
        "comments": "13 pages, 6 figures",
        "journal-ref": null,
        "doi": "10.36227/techrxiv.19154276.v1",
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  Predictions of short-term directional movement of the futures contract can be\nchallenging as its pricing is often based on multiple complex dynamic\nconditions. This work presents a method for predicting the short-term\ndirectional movement of an underlying futures contract. We engineered a set of\nfeatures from technical analysis, order flow, and order-book data. Then,\nTabnet, a deep learning neural network, is trained using these features. We\ntrain our model on the Silver Futures Contract listed on Shanghai Futures\nExchange and achieve an accuracy of 0.601 on predicting the directional change\nduring the selected period.\n"
    },
    {
        "paper_id": 2203.1246,
        "authors": "Sourav Medya, Mohammad Rasoolinejad, Yang Yang, Brian Uzzi",
        "title": "An Exploratory Study of Stock Price Movements from Earnings Calls",
        "comments": "To appear as a full paper in The Web Conference (WWW), 2022",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Financial market analysis has focused primarily on extracting signals from\naccounting, stock price, and other numerical hard data reported in P&L\nstatements or earnings per share reports. Yet, it is well-known that the\ndecision-makers routinely use soft text-based documents that interpret the hard\ndata they narrate. Recent advances in computational methods for analyzing\nunstructured and soft text-based data at scale offer possibilities for\nunderstanding financial market behavior that could improve investments and\nmarket equity. A critical and ubiquitous form of soft data are earnings calls.\nEarnings calls are periodic (often quarterly) statements usually by CEOs who\nattempt to influence investors' expectations of a company's past and future\nperformance. Here, we study the statistical relationship between earnings\ncalls, company sales, stock performance, and analysts' recommendations. Our\nstudy covers a decade of observations with approximately 100,000 transcripts of\nearnings calls from 6,300 public companies from January 2010 to December 2019.\nIn this study, we report three novel findings. First, the buy, sell and hold\nrecommendations from professional analysts made prior to the earnings have low\ncorrelation with stock price movements after the earnings call. Second, using\nour graph neural network based method that processes the semantic features of\nearnings calls, we reliably and accurately predict stock price movements in\nfive major areas of the economy. Third, the semantic features of transcripts\nare more predictive of stock price movements than sales and earnings per share,\ni.e., traditional hard data in most of the cases.\n"
    },
    {
        "paper_id": 2203.12587,
        "authors": "Kensuke Ito, Kyohei Shibano, Gento Mogi",
        "title": "Bubble Prediction of Non-Fungible Tokens (NFTs): An Empirical\n  Investigation",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Our study empirically predicts the bubble of non-fungible tokens (NFTs):\ntransferable and unique digital assets on public blockchains. This topic is\nimportant because, despite their strong market growth in 2021, NFTs on a\nproject basis have not been investigated in terms of bubble prediction.\nSpecifically, we applied the logarithmic periodic power law (LPPL) model to\ntime-series price data associated with four major NFT projects. The results\nindicate that, as of December 20, 2021, (i) NFTs, in general, are in a small\nbubble (a price decline is predicted), (ii) the Decentraland project is in a\nmedium bubble (a price decline is predicted), and (iii) the Ethereum Name\nService and ArtBlocks projects are in a small negative bubble (a price increase\nis predicted). A future work will involve a prediction refinement considering\nthe heterogeneity of NFTs, comparison with other methods, and the use of more\nenriched data.\n"
    },
    {
        "paper_id": 2203.126,
        "authors": "Marcelo de A. Borges, Guido L. de S. Filho, Cicero Inacio da Silva,\n  Anderson M. P. Barros, Raul V. B. J. Britto, Nivaldo M. de C. Junior and\n  Daniel F. L. de Souza",
        "title": "Standing Forest Coin (SFC)",
        "comments": "in Portuguese",
        "journal-ref": null,
        "doi": "10.13140/RG.2.2.17415.47520",
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  This article describes a proposal to create a digital currency that allows\nthe decentralized collection of resources directed to initiatives and\nactivities that aim to protect the Brazilian Amazon ecosystem by using\nblockchain and digital contracts. In addition to the digital currency, the goal\nis to design a smart contract based in oracles to ensure credibility and\nsecurity for investors and donors of financial resources invested in projects\nwithin the Standing Forest Coin (SFC - standingforest.org).\n"
    },
    {
        "paper_id": 2203.12603,
        "authors": "Zhou Tianbao, Li Xinghao, Zhao Junguang",
        "title": "Solar Term Anomaly in China Stock Market: Evidence from Shanghai Index",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  This paper investigates the solar term effect in China stock market as a\nsupplementary to the existing literature of calender effect. Based on a\nregression framework, this paper verifies the existence of solar term effect in\nShanghai Index in multiple dimensions: inter-solar-term analysis, full sample\nanalysis at mean level and risk level as well as the turn of solar term effect.\nSeveral solar terms have been found to cause significant positive and negative\nvalue to the return such as solar term 1,3 and 4. and bring high volatility\nsuch as solar term 8, 11 and 14. The result is reliable and robust under the\nExtreme Bound Analysis and various assumptions of errors distribution in IGARCH\nmodel. These findings give readers a new perspective to view calender effect\nunder the influence of traditional Chinese culture that solar terms affect the\nmarket through affecting investors mood, expectation, enthusiasm, etc. which is\na good evidence to the Culture bonus hypothesis proposed by Chen and Chien and\nthe possible influence by the Chinese culture in other Asian markets.\n"
    },
    {
        "paper_id": 2203.12605,
        "authors": "I Mart\\'in-de-Santos",
        "title": "Grandes fraudes y gobiernos corporativos en la Econom\\'ia desde mediados\n  del siglo XX",
        "comments": "12 pages, in Spanish",
        "journal-ref": null,
        "doi": "10.5281/zenodo.5070443",
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  The international financial system is currently not yet prepared to face a\nforeseeable crisis mainly motivated by the dichotomy between the real economy\nand the virtual economy. Skepticism is widespread even when it comes to\ninvestments in sustainable economy. The concentration of capital in a few\npersons is one of the greatest risks for the possible reiteration of economic\ncrises. The benevolent sentences of the courts to some of the fraudsters do not\ncontribute to dispelling the ghost of fraud nor to the disappearance of tax\nhavens. From the diachronic perspective, it is observed that economic crises\nare increasingly frequent and incidents always in the financial field; which\nforces us to rethink an economic model on an international scale in which there\nis a greater weight of the economic policy of governments over the power of\nmultinational companies in the context of globalization. In the context of\nCorporate Social Responsibility, Corporate Governance is listed as one of the\nfundamental levers to curb large business fraud, but its efficiency seems\ninsufficient due to the lack of international regulations and the ignorance of\nhidden forces in what has been known as fiscal and financial engineering. The\napplication of liberal policies in an unorthodox way is causing real social\ngaps in the distribution of income and is undermining the current capitalist\nsystem. The need to implement corporate governments is recommended as one of\nthe essential formulas for sustaining the international economic system.\n"
    },
    {
        "paper_id": 2203.12606,
        "authors": "Varun Shukla, Manoj Kumar Misra, Atul Chaturvedi",
        "title": "Journey of Cryptocurrency in India In View of Financial Budget 2022-23",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Recently, Indian Finance minister Nirmala Sitharaman announced in Union\nbudget 2022-23 that Indian government will put 30% tax (the highest tax slab in\nIndia) on income generated from cryptocurrencies. Big financial institutions,\nexperts and academicians have different opinions in this regard. They claim\nthat it would be the end of cryptocurrency market in India or it would be\npossible that RBI (Reserve Bank of India) may launch its own crypto or digital\ncurrency. So in this context, in this article, the journey and future aspects\nof cryptocurrency in India are discussed and we hope that it will be a\nreference for further research and discussion in this area.\n"
    },
    {
        "paper_id": 2203.12607,
        "authors": "Marcin Makowski, Edward W. Piotrowski, Piotr Fr\\k{a}ckiewicz, Marek\n  Szopa",
        "title": "Transactional Interpretation for the Principle of Minimum Fisher\n  Information",
        "comments": "13 pages, 1 figure",
        "journal-ref": "Entropy 2021, 23(11), 1464;",
        "doi": "10.3390/e23111464",
        "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
        "abstract": "  The principle of minimum Fisher information states that in the set of\nacceptable probability distributions characterizing the given system, it is\nbest done by the one that minimizes the corresponding Fisher information. This\nprinciple can be applied to transaction processes, the dynamics of which can be\ninterpreted as the market tendency to minimize the information revealed about\nitself. More information involves higher costs (information is physical). The\nstarting point for our considerations is a description of the market derived\nfrom the assumption of minimum Fisher information for a strategy with a fixed\nfinancial risk. Strategies of this type that minimize Fisher information\noverlap with the well-known eigenstates of a the quantum harmonic oscillator.\nThe analytical extension of this field of strategy to the complex vector space\n(traditional for quantum mechanics) suggests the study of the interference of\nthe oscillator eigenstates in terms of their minimization of Fisher\ninformation. It is revealed that the minimum value of Fisher information of the\nsuperposition of the two strategies being the ground state and the second\nexcited state of the oscillator, has Fisher information less than the ground\nstate of the oscillator. Similarly, less information is obtained for the system\nof strategies (the oscillator eigenstates) randomized by the Gibbs\ndistribution. We distinguish two different views on the description of Fisher\ninformation. One of them, the classical, is based on the value of Fisher\ninformation. The second, we call it transactional, expresses Fisher information\nfrom the perspective of the constant risk of market strategies. The orders of\nthe market strategies derived from these two descriptions are different. From a\nmarket standpoint, minimizing Fisher information is equivalent to minimizing\nrisk.\n"
    },
    {
        "paper_id": 2203.12611,
        "authors": "E\\'oin Flaherty, Constantin Gurdgiev, Ronan Lyons, Emer \\'O Siochr\\'u\n  and James Pike",
        "title": "Submission to the Commission on Taxation and Welfare on introducing a\n  site value tax",
        "comments": "36 pages, 19 figures. Paper prepared as a submission to the Irish\n  Commission on Taxation and Welfare (sitting in 2021-2022). Further\n  information on the Commission can be found here:\n  https://www.gov.ie/en/campaigns/92902-commission-on-taxation-and-welfare/",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This submission to the Irish Commission on Taxation and Welfare advocates the\nintroduction of a site value tax in Ireland. Ireland has high and volatile\nproperty prices, constraining social and economic development. Site values are\nthe main driver of these phenomena. Taxing site values would reduce both the\nlevel and volatility of property prices, and thus help to alleviate these\nproblems. Site value tax has many other beneficial features. For example, it\ncaptures price gains due to the community and government rather than owners'\nefforts and thus diminishes the incentive to buy land for speculative reasons.\nSite value tax can be used to finance infrastructural investments, help\nfacilitate site assembly for development and as a support for the maintenance\nof protected structures. Site value tax is also a tax on wealth.\n"
    },
    {
        "paper_id": 2203.12657,
        "authors": "Gurjeet Singh, Pankaj Nagar",
        "title": "A Case Study on Nutek India Limited, Regarding Deep Fall in Share Price",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-sa/4.0/",
        "abstract": "  Manipulating the security price is an act of artificially inflating or\ndeflating the price of a security. Generally, manipulation is defined as a\nseries of transactions designed to raise or lower a price of a security or to\ngive the appearance of trading for the purpose of inducing others to buy or\nsell. In essence, a manipulation is intentional interference with the free\nforces of supply and demand. In this paper we have tried to study the reasons\nbehind drastic fall in share price of Nutek India Limited.\n"
    },
    {
        "paper_id": 2203.12842,
        "authors": "Ranik Raaen Wahlstr{\\o}m",
        "title": "Financial statements of companies in Norway",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  This document details a dataset that contains all unconsolidated annual\nfinancial statements of the universe of Norwegian private and public limited\nliability companies. It also includes all financial statements of other company\ntypes reported to the Norwegian authorities.\n"
    },
    {
        "paper_id": 2203.12858,
        "authors": "Katsunobu Okamoto, Takuji Takemoto, Yoshimi Kawamoto, Sachiyo Kamimura",
        "title": "The differences in SAPA Needs by Route, Traffic Volume and after\n  COVID-19",
        "comments": "36pages, 22tables, 21Figure",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This study aims to identify the differences in SAPA user interest due to each\nroute, traffic volumes, and after COVID-19 by the daily feedback from them and\nto help develop SAPA plans. Food was the most common opinion. However, for the\nroute, some showed interest in other options. For the traffic volume, the\ndifference of interest was also shown in some heavy traffic areas, On the other\nhand, the changes in customer needs after the COVID-19 disaster were less\nchanged.\n"
    },
    {
        "paper_id": 2203.12968,
        "authors": "Luca Verginer, Federica Parisi, Jeroen van Lidth de Jeude and Massimo\n  Riccaboni",
        "title": "The Impact of Acquisitions in the Biotechnology Sector on R&D\n  Productivity",
        "comments": "26 pages, 9 tables, 3 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This study examines the effects of acquisitions on the retention and R&D\nproductivity of inventors in the biotech sector, using data from 15,318\ninventors involved in 1,375 acquisitions between 1990 and 2010. We employ a\nstaggered difference-in-differences approach and find that acquisitions lead to\na 13.5% decrease in inventor retention and a 35% drop in citation-weighted\npatent productivity post-acquisition. The productivity decline is more severe\nfor inventors who remain with the acquiring firm, particularly for those whose\nexpertise is closely tied to the target company. However, older inventors and\nthose whose expertise aligns with the acquiring company's existing R&D\nportfolio tend to retain higher productivity levels after the acquisition.\n"
    },
    {
        "paper_id": 2203.13001,
        "authors": "Karim Amzile, Rajaa Amzile",
        "title": "The application of techniques derived from artificial intelligence to\n  the prediction of the solvency of bank customers: case of the application of\n  the cart type decision tree (dt)",
        "comments": "10 pages, 2 Figures, 6 tables",
        "journal-ref": null,
        "doi": "10.5121/csit.2022.120503",
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  In this study we applied the CART-type Decision Tree (DT-CART) method derived\nfrom artificial intelligence technique to the prediction of the solvency of\nbank customers, for this we used historical data of bank customers. However we\nhave adopted the process of Data Mining techniques, for this purpose we started\nwith a data preprocessing in which we clean the data and we deleted all rows\nwith outliers or missing values as well as rows with empty columns, then we\nfixed the variable to be explained (dependent or Target) and we also thought to\neliminate all explanatory (independent) variables that are not significant\nusing univariate analysis as well as the correlation matrix, then we applied\nour CART decision tree method using the SPSS tool. After completing our process\nof building our model (AD-CART), we started the process of evaluating and\ntesting the performance of our model, by which we found that the accuracy and\nprecision of our model is 71%, so we calculated the error ratios, and we found\nthat the error rate equal to 29%, this allowed us to conclude that our model at\na fairly good level in terms of precision, predictability and very precisely in\npredicting the solvency of our banking customers.\n"
    },
    {
        "paper_id": 2203.13053,
        "authors": "Bastien Baldacci, Philippe Bergault, Dylan Possama\\\"i",
        "title": "A mean-field game of market-making against strategic traders",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  We design a market-making model \\`a la Avellaneda-Stoikov in which the\nmarket-takers act strategically, in the sense that they design their trading\nstrategy based on an exogenous trading signal. The market-maker chooses her\nquotes based on the average market-takers' behaviour, modelled through a\nmean-field interaction. We derive, up to the resolution of a coupled\nHJB--Fokker--Planck system, the optimal controls of the market-maker and the\nrepresentative market-taker. This approach is flexible enough to incorporate\ndifferent behaviours for the market-takers and takes into account the impact of\ntheir strategies on the price process.\n"
    },
    {
        "paper_id": 2203.13056,
        "authors": "Furkan Sezer, Ceyhun Eksin",
        "title": "Information Preferences of Individual Agents in\n  Linear-Quadratic-Gaussian Network Games",
        "comments": "6 pages, 4 figures",
        "journal-ref": null,
        "doi": "10.1109/LCSYS.2022.3183539",
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  We consider linear-quadratic-Gaussian (LQG) network games in which agents\nhave quadratic payoffs that depend on their individual and neighbors' actions,\nand an unknown payoff-relevant state. An information designer determines the\nfidelity of information revealed to the agents about the payoff state to\nmaximize the social welfare. Prior results show that full information\ndisclosure is optimal under certain assumptions on the payoffs, i.e., it is\nbeneficial for the average individual. In this paper, we provide conditions\nbased on the strength of the dependence of payoffs on neighbors' actions, i.e.,\ncompetition, under which a rational agent is expected to benefit, i.e., receive\nhigher payoffs, from full information disclosure. We find that all agents\nbenefit from information disclosure for the star network structure when the\ngame is symmetric and submodular or supermodular. We also identify that the\ncentral agent benefits more than a peripheral agent from full information\ndisclosure unless the competition is strong and the number of peripheral agents\nis small enough. Despite the fact that all agents expect to benefit from\ninformation disclosure ex-ante, a central agent can be worse-off from\ninformation disclosure in many realizations of the payoff state under strong\ncompetition, indicating that a risk-averse central agent can prefer\nuninformative signals ex-ante.\n"
    },
    {
        "paper_id": 2203.1374,
        "authors": "Karoline Bax, Emanuele Taufer, Sandra Paterlini",
        "title": "A generalized precision matrix for t-Student distributions in portfolio\n  optimization",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The Markowitz model is still the cornerstone of modern portfolio theory. In\nparticular, when focusing on the minimum-variance portfolio, the covariance\nmatrix or better its inverse, the so-called precision matrix, is the only input\nrequired. So far, most scholars worked on improving the estimation of the\ninput, however little attention has been given to the limitations of the\ninverse covariance matrix when capturing the dependence structure in a\nnon-Gaussian setting. While the precision matrix allows to correctly understand\nthe conditional dependence structure of random vectors in a Gaussian setting,\nthe inverse of the covariance matrix might not necessarily result in a reliable\nsource of information when Gaussianity fails. In this paper, exploiting the\nlocal dependence function, different definitions of the generalized precision\nmatrix (GPM), which holds for a general class of distributions, are provided.\nIn particular, we focus on the multivariate t-Student distribution and point\nout that the interaction in random vectors does not depend only on the inverse\nof the covariance matrix, but also on additional elements. We test the\nperformance of the proposed GPM using a minimum-variance portfolio set-up by\nconsidering S\\&P 100 and Fama and French industry data. We show that portfolios\nrelying on the GPM often generate statistically significant lower out-of-sample\nvariances than state-of-art methods.\n"
    },
    {
        "paper_id": 2203.13766,
        "authors": "Daniele Bufalo, Michele Bufalo, Francesco Cesarone, Giuseppe Orlando",
        "title": "Straightening skewed markets with an index tracking optimizationless\n  portfolio",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Among professionals and academics alike, it is well known that active\nportfolio management is unable to provide additional risk-adjusted returns\nrelative to their benchmarks. For this reason, passive wealth management has\nemerged in recent decades to offer returns close to benchmarks at a lower cost.\nIn this article, we first refine the existing results on the theoretical\nproperties of oblique Brownian motion. Then, assuming that the returns follow\nskew geometric Brownian motions and that they are correlated, we describe some\nstatistical properties for the \\emph{ex-post}, the \\emph{ex-ante} tracking\nerrors, and the forecasted tracking portfolio. To this end, we develop an\ninnovative statistical methodology, based on a benchmark-asset principal\ncomponent factorization, to determine a tracking portfolio that replicates the\nperformance of a benchmark by investing in a subset of the investable universe.\nThis strategy, named hybrid Principal Component Analysis (hPCA), is applied\nboth on normal and skew distributions. In the case of skew-normal returns, we\npropose a framework for calibrating the model parameters, based on the maximum\nlikelihood estimation method. For testing and validation, we compare four\nalternative models for index tracking. The first two are based on the hPCA when\nreturns are assumed to be normal or skew-normal. The third model adopts a\nstandard optimization-based approach and the last one is used in the financial\nsector by some practitioners. For validation and testing, we present a thorough\ncomparison of these strategies on real-world data, both in terms of performance\nand computational efficiency. A noticeable result is that, not only, the\nsuggested lean PCA-based portfolio selection approach compares well versus\ncumbersome algorithms for optimization-based portfolios, but, also, it could\nprovide a better service to the asset management industry.\n"
    },
    {
        "paper_id": 2203.1379,
        "authors": "Ilaria Gianstefani, Luigi Longo, Massimo Riccaboni",
        "title": "The echo chamber effect resounds on financial markets: a social media\n  alert system for meme stocks",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  The short squeeze of Gamestop (GME) has revealed to the world how retail\ninvestors pooling through social media can severely impact financial markets.\nIn this paper, we devise an early warning signal to detect suspicious users'\nsocial network activity, which might affect the financial market stability. We\napply our approach to the subreddit r/WallStreetBets, selecting two meme stocks\n(GME and AMC) and two non-meme stocks (AAPL and MSFT) as case studies. The\nalert system is structured in two stpng; the first one is based on\nextraordinary activity on the social network, while the second aims at\nidentifying whether the movement seeks to coordinate the users to a bulk\naction. We run an event study analysis to see the reaction of the financial\nmarkets when the alert system catches social network turmoil. A regression\nanalysis witnesses the discrepancy between the meme and non-meme stocks in how\nthe social networks might affect the trend on the financial market.\n"
    },
    {
        "paper_id": 2203.1382,
        "authors": "Rama Cont and Purba Das",
        "title": "Rough volatility: fact or artefact?",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We investigate the statistical evidence for the use of `rough' fractional\nprocesses with Hurst exponent $H< 0.5$ for the modeling of volatility of\nfinancial assets, using a model-free approach. We introduce a non-parametric\nmethod for estimating the roughness of a function based on discrete sample,\nusing the concept of normalized $p$-th variation along a sequence of\npartitions. We investigate the finite sample performance of our estimator for\nmeasuring the roughness of sample paths of stochastic processes using detailed\nnumerical experiments based on sample paths of fractional Brownian motion and\nother fractional processes. We then apply this method to estimate the roughness\nof realized volatility signals based on high-frequency observations. Detailed\nnumerical experiments based on stochastic volatility models show that, even\nwhen the instantaneous volatility has diffusive dynamics with the same\nroughness as Brownian motion, the realized volatility exhibits rough behaviour\ncorresponding to a Hurst exponent significantly smaller than $0.5$. Comparison\nof roughness estimates for realized and instantaneous volatility in fractional\nvolatility models with different values of Hurst exponent shows that,\nirrespective of the roughness of the spot volatility process, realized\nvolatility always exhibits `rough' behaviour with an apparent Hurst index\n$\\hat{H}<0.5$. These results suggest that the origin of the roughness observed\nin realized volatility time-series lies in the microstructure noise rather than\nthe volatility process itself.\n"
    },
    {
        "paper_id": 2203.13991,
        "authors": "Ning Qi, Lin Cheng, Yuxiang Wan, Yingrui Zhuang, and Zeyu Liu",
        "title": "Risk Assessment with Generic Energy Storage under Exogenous and\n  Endogenous Uncertainty",
        "comments": "PES GM2022-Exogenous and Endogenous Uncertainty",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Current risk assessment ignores the stochastic nature of energy storage\navailability itself and thus lead to potential risk during operation. This\npaper proposes the redefinition of generic energy storage (GES) that is allowed\nto offer probabilistic reserve. A data-driven unified model with exogenous and\nendogenous uncertainty (EXU & EDU) description is presented for four typical\ntypes of GES. Moreover, risk indices are proposed to assess the impact of\noverlooking (EXU & EDU) of GES. Comparative results between EXU & EDU are\nillustrated in distribution system with day-ahead chance-constrained\noptimization (CCO) and more severe risks are observed for the latter, which\nindicate that system operator (SO) should adopt novel strategies for EDU\nuncertainty.\n"
    },
    {
        "paper_id": 2203.13999,
        "authors": "Xin Zhang",
        "title": "Distributional Robust Portfolio Construction based on Investor Aversion",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  In behavioral finance, aversion affects investors' judgment of future\nuncertainty when profit and loss occur. Considering investors' aversion to loss\nand risk, and the ambiguous uncertainty characterizing asset returns, we\nconstruct a distributional robust portfolio model (DRP) under the condition\nthat the distribution of risky asset returns is unknown. Specifically, our\nobjective is to find an optimal portfolio of assets that maximizes the\nworst-case utility level on the Wasserstein ball, which is centered on the\nempirical distribution of sample returns and the radius of the ball quantifies\nthe investor's ambiguity level. The model is also formulated as a mixed-integer\nquadratic programming problem with cardinality constraints. In addition, we\npropose a hybrid algorithm to improve the efficiency of the solution and make\nit more suitable for large-scale problems. The distributional robust portfolio\nmodel considering aversion is empirically tested for superior performance in\nasset allocation, and we also compare common asset allocation strategies to\nfurther enhance the credibility of the portfolio.\n"
    },
    {
        "paper_id": 2203.14254,
        "authors": "Jan Schulz and Daniel M. Mayerhoffer and Anna Gebhard",
        "title": "A Network-Based Explanation of Inequality Perceptions",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  Across income groups and countries, individual citizens perceive economic\ninequality spectacularly wrong. These misperceptions have far-reaching\nconsequences, as it is perceived inequality, not actualinequality informing\nredistributive preferences. The prevalence of this phenomenon is independent of\nsocial class and welfare regime, which suggests the existence of a common\nmechanism behind public perceptions. The literature has identified several\nstylised facts on how individual perceptions respond to actual inequality and\nhow these biases vary systematically along the income distribution. We propose\na network-based explanation of perceived inequality building on recent advances\nin random geometric graph theory. The generating mechanism can replicate all of\nforementioned stylised facts simultaneously. It also produces social networks\nthat exhibit salient features of real-world networks; namely, they cannot be\nstatistically distinguished from small-world networks, testifying to the\nrobustness of our approach. Our results, therefore, suggest that homophilic\nsegregation is a promising candidate to explain inequality perceptions with\nstrong implications for theories of consumption and voting behaviour.\n"
    },
    {
        "paper_id": 2203.14255,
        "authors": "Ravi Kashyap",
        "title": "Are Instrumental Variables Really That Instrumental? Endogeneity\n  Resolution in Regression Models for Comparative Studies",
        "comments": "Statistica Sinica, 32 (2022), 645-651",
        "journal-ref": null,
        "doi": "10.5705/ss.202019.0459",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We provide a justification for why, and when, endogeneity will not cause bias\nin the interpretation of the coefficients in a regression model. This technique\ncan be a viable alternative to, or even used alongside, the instrumental\nvariable method. We show that when performing any comparative study, it is\npossible to measure the true change in the coefficients under a broad set of\nconditions. Our results hold, as long as the product of the covariance\nstructure between the explanatory variables and the covariance between the\nerror term and the explanatory variables are equal, within the same system at\ndifferent time periods or across multiple systems at the same point in time.\n"
    },
    {
        "paper_id": 2203.14259,
        "authors": "Jan Schulz and Daniel M. Mayerhoffer",
        "title": "A Network Approach to Consumption",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  The nexus between debt and inequality has attracted considerable scholarly\nattention in the wake of the global financial crisis. One prominent candidate\nto explain the striking co-evolution of income inequality and private debt in\nthis period has been the theory of upward-looking consumption externalities\nleading to expenditure cascades. We propose a parsimonious model of\nupward-looking consumption at the micro level mediated by perception networks\nwith empirically plausible topologies. This allows us to make sense of the\nambiguous empirical literature on the relevance of this channel. Up to our\nknowledge, our approach is the first to make the reference group to which\nconspicuous consumption relates explicit. Our model, based purely on current\nincome, replicates the major stylised facts regarding micro consumption\nbehaviour and is thus observationally equivalent to the workhorse permanent\nincome hypothesis, without facing its dual problem of `excess smoothness' and\n`excess sensitivity'. We also demonstrate that the network topology and\nsegregation has a significant effect on consumption patterns which has so far\nbeen neglected.\n"
    },
    {
        "paper_id": 2203.14282,
        "authors": "Brendon McConnell",
        "title": "Racial Sentencing Disparities and Differential Progression Through the\n  Criminal Justice System: Evidence From Linked Federal and State Court Data",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  Several key actors -- police, prosecutors, judges -- can alter the course of\nindividuals passing through the multi-staged criminal justice system. I use\nlinked arrest-sentencing data for federal courts from 1994-2010 to examine the\nrole that earlier stages play when estimating Black-white sentencing gaps. I\nfind no evidence of sample selection at play in the federal setting, suggesting\nfederal judges are largely responsible for racial sentencing disparities. In\ncontrast, I document substantial sample selection bias in two different state\ncourts systems. Estimates of racial and ethnic sentencing gaps that ignore\nselection underestimate the true disparities by 15% and 13% respectively.\n"
    },
    {
        "paper_id": 2203.14601,
        "authors": "Xiaotong Sun",
        "title": "Bribes to Miners: Evidence from Ethereum",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In blockchain, bribery is an inevitable problem since users with various\ngoals can bribe miners by transferring cryptoassets. To alleviate the negative\neffects of such collusion, Ethereum blockchain implemented new transaction fee\nmechanism in the London Fork, which was deployed on August 5th, 2021. In this\npaper, we first filter potential bribery by scanning Ethereum transactions, and\nthe potential bribers and bribees are centralized in a small group. Then we\nconstruct bribing proxies to measure the active level of bribery and then\ninvestigate the effects of bribery. Consequently, bribery can influence both\nEthereum and other mainstream blockchains, in aspects of underlying\ncryptocurrency, transaction statistics, and network adoption. Moreover, the\nLondon Fork shows complicated effects on relationship between bribery and\nblockchain factors. Besides, bribery in Ethereum relates to stock markets,\ne.g., S&P 500 and Nasdaq, implying implicit interlinks between blockchain and\ntraditional finance.\n"
    },
    {
        "paper_id": 2203.14751,
        "authors": "Isaiah Hull and Anna Grodecka-Messi",
        "title": "Measuring the Impact of Taxes and Public Services on Property Values: A\n  Double Machine Learning Approach",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  How do property prices respond to changes in local taxes and local public\nservices? Attempts to measure this, starting with Oates (1969), have suffered\nfrom a lack of local public service controls. Recent work attempts to overcome\nsuch data limitations through the use of quasi-experimental methods. We revisit\nthis fundamental problem, but adopt a different empirical strategy that pairs\nthe double machine learning estimator of Chernozhukov et al. (2018) with a\nnovel dataset of 947 time-varying local characteristic and public service\ncontrols for all municipalities in Sweden over the 2010-2016 period. We find\nthat properly controlling for local public service and characteristic controls\nmore than doubles the estimated impact of local income taxes on house prices.\nWe also exploit the unique features of our dataset to demonstrate that tax\ncapitalization is stronger in areas with greater municipal competition,\nproviding support for a core implication of the Tiebout hypothesis. Finally, we\nmeasure the impact of public services, education, and crime on house prices and\nthe effect of local taxes on migration.\n"
    },
    {
        "paper_id": 2203.14904,
        "authors": "Nigel Adams, Adriano Augusto, Michael Davern, Marcello La Rosa",
        "title": "Why Do Banks Find Business Process Compliance So Challenging? An\n  Australian Case Study",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Banks play an intrinsic role in any modern economy, recycling capital from\nsavers to borrowers. They are heavily regulated and there have been a\nsignificant number of well publicized compliance failings in recent years. This\nis despite Business Process Compliance (BPC) being both a well researched\ndomain in academia and one where significant progress has been made. This study\nseeks to determine why Australian banks find BPC so challenging. We interviewed\n22 senior managers from a range of functions within the four major Australian\nbanks to identify the key challenges. Not every process in every bank is facing\nthe same issues, but in processes where a bank is particularly challenged to\nmeet its compliance requirements, the same themes emerge. The compliance\nrequirement load they bear is excessive, dynamic and complex. Fulfilling these\nrequirements relies on impenetrable spaghetti processes, and the case for\nsustainable change remains elusive, locking banks into a fail-fix cycle that\nincreases the underlying complexity. This paper proposes a conceptual framework\nthat identifies and aggregates the challenges, and a circuit-breaker approach\nas an \"off ramp\" to the fail-fix cycle.\n"
    },
    {
        "paper_id": 2203.15009,
        "authors": "Jase Clarkson, Mihai Cucuringu, Andrew Elliott, Gesine Reinert",
        "title": "DAMNETS: A Deep Autoregressive Model for Generating Markovian Network\n  Time Series",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Generative models for network time series (also known as dynamic graphs) have\ntremendous potential in fields such as epidemiology, biology and economics,\nwhere complex graph-based dynamics are core objects of study. Designing\nflexible and scalable generative models is a very challenging task due to the\nhigh dimensionality of the data, as well as the need to represent temporal\ndependencies and marginal network structure. Here we introduce DAMNETS, a\nscalable deep generative model for network time series. DAMNETS outperforms\ncompeting methods on all of our measures of sample quality, over both real and\nsynthetic data sets.\n"
    },
    {
        "paper_id": 2203.1547,
        "authors": "Deborah Sulem, Henry Kenlay, Mihai Cucuringu, Xiaowen Dong",
        "title": "Graph similarity learning for change-point detection in dynamic networks",
        "comments": "33 pages, 21 figures, 5 tables",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Dynamic networks are ubiquitous for modelling sequential graph-structured\ndata, e.g., brain connectome, population flows and messages exchanges. In this\nwork, we consider dynamic networks that are temporal sequences of graph\nsnapshots, and aim at detecting abrupt changes in their structure. This task is\noften termed network change-point detection and has numerous applications, such\nas fraud detection or physical motion monitoring. Leveraging a graph neural\nnetwork model, we design a method to perform online network change-point\ndetection that can adapt to the specific network domain and localise changes\nwith no delay. The main novelty of our method is to use a siamese graph neural\nnetwork architecture for learning a data-driven graph similarity function,\nwhich allows to effectively compare the current graph and its recent history.\nImportantly, our method does not require prior knowledge on the network\ngenerative distribution and is agnostic to the type of change-points; moreover,\nit can be applied to a large variety of networks, that include for instance\nedge weights and node attributes. We show on synthetic and real data that our\nmethod enjoys a number of benefits: it is able to learn an adequate graph\nsimilarity function for performing online network change-point detection in\ndiverse types of change-point settings, and requires a shorter data history to\ndetect changes than most existing state-of-the-art baselines.\n"
    },
    {
        "paper_id": 2203.15716,
        "authors": "Martin Vesel\\'y",
        "title": "Application of Quantum Computers in Foreign Exchange Reserves Management",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The main purpose of this article is to evaluate possible applications of\nquantum computers in foreign exchange reserves management. The capabilities of\nquantum computers are demonstrated by means of risk measurement using the\nquantum Monte Carlo method and portfolio optimization using a linear equations\nsystem solver (the Harrow-Hassidim-Lloyd algorithm) and quadratic unconstrained\nbinary optimization (the quantum approximate optimization algorithm). All\ndemonstrations are carried out on the cloud-based IBM Quantum(TM) platform.\nDespite the fact that real-world applications are impossible under the current\nstate of development of quantum computers, it is proven that in principle it\nwill be possible to apply such computers in FX reserves management in the\nfuture. In addition, the article serves as an introduction to quantum computing\nfor the staff of central banks and financial market supervisory authorities.\n"
    },
    {
        "paper_id": 2203.15911,
        "authors": "Nick James, Max Menzies, Kevin Chin",
        "title": "Economic state classification and portfolio optimisation with\n  application to stagflationary environments",
        "comments": "Accepted manuscript. Minor edits since v3",
        "journal-ref": "Chaos, Solitons & Fractals 164 (2022) 112664",
        "doi": "10.1016/j.chaos.2022.112664",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Motivated by the current fears of a potentially stagflationary global\neconomic environment, this paper uses new and recently introduced mathematical\ntechniques to study multivariate time series pertaining to country inflation\n(CPI), economic growth (GDP) and equity index behaviours. We begin by assessing\nthe temporal evolution among various economic phenomena, and complement this\nanalysis with `economic driver analysis,' where we decouple country economic\ntrajectories and determine what is most important in their association. Next,\nwe study the temporal self-similarity of global inflation, growth and equity\nindex returns to identify the most anomalous historic periods, and windows in\nthe past that are most similar to current market dynamics. We then introduce a\nnew algorithm to construct economic state classifications and compute an\neconomic state integral, where countries are determined to belong in one of\nfour candidate states based on their inflation and growth behaviours. Finally,\nwe implement a decade-by-decade portfolio optimisation to determine which\nequity indices and portfolio assets have been most beneficial in maximising\nportfolio risk-adjusted returns in various market conditions. This could be of\ngreat interest to those looking for asset allocation guidance in the current\nperiod of high economic uncertainty.\n"
    },
    {
        "paper_id": 2203.15929,
        "authors": "Kun Zhang, Ben Mingbin Feng, Guangwu Liu, Shiyu Wang",
        "title": "Sample Recycling for Nested Simulation with Application in Portfolio\n  Risk Measurement",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Nested simulation is a natural approach to tackle nested estimation problems\nin operations research and financial engineering. The outer-level simulation\ngenerates outer scenarios and the inner-level simulations are run in each outer\nscenario to estimate the corresponding conditional expectation. The resulting\nsample of conditional expectations is then used to estimate different risk\nmeasures of interest. Despite its flexibility, nested simulation is notorious\nfor its heavy computational burden. We introduce a novel simulation procedure\nthat reuses inner simulation outputs to improve efficiency and accuracy in\nsolving nested estimation problems. We analyze the convergence rates of the\nbias, variance, and MSE of the resulting estimator. In addition, central limit\ntheorems and variance estimators are presented, which lead to asymptotically\nvalid confidence intervals for the nested risk measure of interest. We conduct\nnumerical studies on two financial risk measurement problems. Our numerical\nstudies show consistent results with the asymptotic analysis and show that the\nproposed approach outperforms the standard nested simulation and a state-of-art\nregression approach for nested estimation problems.\n"
    },
    {
        "paper_id": 2203.16108,
        "authors": "Benjamin Avanzi and Hayden Lau and Mogens Steffensen",
        "title": "Optimal reinsurance design under solvency constraints",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We consider the optimal risk transfer from an insurance company to a\nreinsurer. The problem formulation considered in this paper is closely\nconnected to the optimal portfolio problem in finance, with some crucial\ndistinctions. In particular, the insurance company's surplus is here (as is\nroutinely the case) approximated by a Brownian motion, as opposed to the\ngeometric Brownian motion used to model assets in finance. Furthermore, risk\nexposure is dialled \"down\" via reinsurance, rather than \"up\" via risky\ninvestments. This leads to interesting qualitative differences in the optimal\ndesigns.\n  In this paper, using the martingale method, we derive the optimal design as a\nfunction of proportional, non-cheap reinsurance design that maximises the\nquadratic utility of the terminal value of the insurance surplus. We also\nconsider several realistic constraints on the terminal value: a strict lower\nboundary, the probability (Value at Risk) constraint, and the expected\nshortfall (conditional Value at Risk) constraints under the $\\mathbb{P}$ and\n$\\mathbb{Q}$ measures, respectively. In all cases, the optimal reinsurance\ndesigns boil down to a combination of proportional protection and option-like\nprotection (stop-loss) of the residual proportion with various deductibles.\nProportions and deductibles are set such that the initial capital is fully\nallocated. Comparison of the optimal designs with the optimal portfolios in\nfinance is particularly interesting. Results are illustrated.\n"
    },
    {
        "paper_id": 2203.16316,
        "authors": "\\\"Onder Nomaler and Bart Verspagen",
        "title": "Some New Views on Product Space and Related Diversification",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We aim to contribute to the literature on product space and diversification\nby proposing a number of extensions of the current literature: (1) we propose\nthat the alternative but related idea of a country space also has empirical and\ntheoretical appeal; (2) we argue that the loss of comparative advantage should\nbe an integral part of (testing the empirical relevance of) the product space\nidea; (3) we propose several new indicators for measuring relatedness in\nproduct space; and (4) we propose a non-parametric statistical test based on\nbootstrapping to test the empirical relevance of the product space idea.\n"
    },
    {
        "paper_id": 2203.16405,
        "authors": "Peter Hull, Michal Koles\\'ar, Christopher Walters",
        "title": "Labour by Design: Contributions of David Card, Joshua Angrist, and Guido\n  Imbens",
        "comments": null,
        "journal-ref": "Scandinavian Journal of Economics, Volume 12, Issue 3, July 2022,\n  pages 603-645",
        "doi": "10.1111/sjoe.12505",
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  The 2021 Sveriges Riksbank Prize in Economic Sciences in Memory of Alfred\nNobel was awarded to David Card \"for his empirical contributions to labour\neconomics\" and to Joshua Angrist and Guido Imbens \"for their methodological\ncontributions to the analysis of causal relationships.\" We survey these\ncontributions of the three laureates, and discuss how their empirical and\nmethodological insights transformed the modern practice of applied\nmicroeconomics. By emphasizing research design and formalizing the causal\ncontent of different econometric procedures, the laureates shed new light on\nkey questions in labour economics and advanced a robust toolkit for empirical\nanalyses across many fields.\n"
    },
    {
        "paper_id": 2203.16516,
        "authors": "Ankit Singhal, Sarmad Hanif, Bishnu Bhattarai, Fernando B. dos Reis,\n  Hayden Reeve, and Robert Pratt",
        "title": "Designing a Transactive Electric Vehicle Agent with Customer's\n  Participation Preference",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  The proliferation of electric vehicles (EVs) and their inherent flexibility\nin charging timings make them an asset to improve grid performance. In contrast\nto direct control by a utility or autonomous price-based charging, the\ntransactive control framework not only provides benefits to both grid and\ncustomers but also ensures customer autonomy. In this work, we design a\ntransactive electric vehicle (TEV) agent that incorporates the EV owner's\nwillingness to trade-off between savings and amenity in form of a slider, where\nthe EV owner's amenity is characterized as vehicle readiness. Further, a\nprivacy-preserving bidding formulation is proposed that also represents the\ncustomer's transactive preference. A transactive market mechanism is discussed\nthat integrates the TEV Agents into the local retail market and reconciles with\nthe current day-ahead and real-time market structure. It is demonstrated that\nthe proposed slider is able to provide a preferred trade-off between savings\nand amenity to individual customers. At the same time, the market mechanism is\nshown to successfully reduce both peak prices and peak demand. A comparative\ninvestigation of V1G and V2G technologies with respect to the battery prices is\nalso discussed.\n"
    },
    {
        "paper_id": 2203.16612,
        "authors": "Xiaotong Sun, Charalampos Stasinakis, Georigios Sermpinis",
        "title": "Decentralization illusion in Decentralized Finance: Evidence from\n  tokenized voting in MakerDAO polls",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Decentralized Autonomous Organization (DAO) is very popular in Decentralized\nFinance (DeFi) applications as it provides a decentralized governance solution\nthrough blockchain. We analyze the governance characteristics in the Maker\nprotocol, its stablecoin DAI and governance token Maker (MKR). To achieve that,\nwe establish several measurements of centralized governance. Our empirical\nanalysis investigates the effect of centralized governance over a series of\nfactors related to MKR and DAI, such as financial, transaction, network and\ntwitter sentiment indicators. Our results show that governance centralization\ninfluences both the Maker protocol, and the distribution of voting power\nmatters. The main implication of this study is that centralized governance in\nMakerDAO very much exists, while DeFi investors face a trade-off between\ndecentralization and performance of a DeFi protocol. This further contributes\nto the contemporary debate on whether DeFi can be truly decentralized.\ncentralized governance in MakerDAO very much exists, while DeFi investors face\na trade-off between efficiency and decentralization. This further contributes\nto the contemporary debate on whether DeFi can be truly decentralized.\n"
    },
    {
        "paper_id": 2203.17101,
        "authors": "Emily Hannum, Xiaoying Liu, Fan Wang",
        "title": "Estimating the Effects of Educational System Consolidation: The Case of\n  China's Rural School Closure Initiative",
        "comments": null,
        "journal-ref": "Economic Development and Cultural Change 70, no. 1 (October 1,\n  2021): 485-528",
        "doi": "10.1086/711654",
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Global trends of fertility decline, population aging, and rural outmigration\nare creating pressures to consolidate school systems, with the rationale that\neconomies of scale will enable higher quality education to be delivered in an\nefficient manner, despite longer travel distances for students. Yet, few\nstudies have considered the implications of system consolidation for\neducational access and inequality, outside of the context of developed\ncountries. We estimate the impact of educational infrastructure consolidation\non educational attainment using the case of China's rural primary school\nclosure policies in the early 2000s. We use data from a large household survey\ncovering 728 villages in 7 provinces, and exploit variation in villages' year\nof school closure and children's ages at closure to identify the causal impact\nof school closure. For girls exposed to closure during their primary school\nages, we find an average decrease of 0.60 years of schooling by 2011, when\nchildren's mean age was 17 years old. Negative effects strengthen with time\nsince closure. For boys, there is no corresponding significant effect.\nDifferent effects by gender may be related to greater sensitivity of girls'\nenrollment to distance and greater responsiveness of boys' enrollment to\nquality.\n"
    },
    {
        "paper_id": 2204.00052,
        "authors": "Sergio Correia, Stephan Luck",
        "title": "Digitizing Historical Balance Sheet Data: A Practitioner's Guide",
        "comments": "For associated Github repository, see\n  https://github.com/sergiocorreia/quipucamayoc/",
        "journal-ref": null,
        "doi": "10.1016/j.eeh.2022.101475",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper discusses how to successfully digitize large-scale historical\nmicro-data by augmenting optical character recognition (OCR) engines with pre-\nand post-processing methods. Although OCR software has improved dramatically in\nrecent years due to improvements in machine learning, off-the-shelf OCR\napplications still present high error rates which limit their applications for\naccurate extraction of structured information. Complementing OCR with\nadditional methods can however dramatically increase its success rate, making\nit a powerful and cost-efficient tool for economic historians. This paper\nshowcases these methods and explains why they are useful. We apply them against\ntwo large balance sheet datasets and introduce quipucamayoc, a Python package\ncontaining these methods in a unified framework.\n"
    },
    {
        "paper_id": 2204.00059,
        "authors": "Luke Snow and Shashwat Jain and Vikram Krishnamurthy",
        "title": "Lyapunov based Stochastic Stability of Human-Machine Interaction: A\n  Quantum Decision System Approach",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In mathematical psychology, decision makers are modeled using the Lindbladian\nequations from quantum mechanics to capture important human-centric features\nsuch as order effects and violation of the sure thing principle. We consider\nhuman-machine interaction involving a quantum decision maker (human) and a\ncontroller (machine). Given a sequence of human decisions over time, how can\nthe controller dynamically provide input messages to adapt these decisions so\nas to converge to a specific decision? We show via novel stochastic Lyapunov\narguments how the Lindbladian dynamics of the quantum decision maker can be\ncontrolled to converge to a specific decision asymptotically. Our methodology\nyields a useful mathematical framework for human-sensor decision making. The\nstochastic Lyapunov results are also of independent interest as they generalize\nrecent results in the literature.\n"
    },
    {
        "paper_id": 2204.00189,
        "authors": "Jung-In Yeon and Sojung Hwang and Bogang Jun",
        "title": "The spillover effect of neighboring port on regional industrial\n  diversification and regional economic resilience",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  We examine the spillover effect of neighboring ports on regional industrial\ndiversification and their economic resilience using the export data of South\nKorea from 2006 to 2020. First, we build two distinct product spaces of ports\nand port regions, and provide direct estimates of the role of neighboring ports\nas spillover channels spatially linked. This is in contrast to the previous\nliterature that mainly regarded ports as transport infrastructure per se.\nSecond, we confirm that the knowledge spillover effect from neighboring ports\nhad a non-negligible role in sustaining regional economies during the recovery\nafter the economic crisis but its power has weakened recently due to a loosened\nglobal value chain.\n"
    },
    {
        "paper_id": 2204.00204,
        "authors": "JunTao Duan, Ionel Popescu",
        "title": "LoCoV: low dimension covariance voting algorithm for portfolio\n  optimization",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  Minimum-variance portfolio optimizations rely on accurate covariance\nestimator to obtain optimal portfolios. However, it usually suffers from large\nerror from sample covariance matrix when the sample size $n$ is not\nsignificantly larger than the number of assets $p$. We analyze the random\nmatrix aspects of portfolio optimization and identify the order of errors in\nsample optimal portfolio weight and show portfolio risk are underestimated when\nusing samples. We also provide LoCoV (low dimension covariance voting)\nalgorithm to reduce error inherited from random samples. From various\nexperiments, LoCoV is shown to outperform the classical method by a large\nmargin.\n"
    },
    {
        "paper_id": 2204.00219,
        "authors": "Xiaoying Liu, Jere R. Behrman, Emily Hannum, Fan Wang, Qingguo Zhao",
        "title": "Same environment, stratified impacts? Air pollution, extreme\n  temperatures, and birth weight in south China",
        "comments": null,
        "journal-ref": "Social Science Research, February, 102691 (2022)",
        "doi": "10.1016/j.ssresearch.2021.102691",
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  This paper investigates whether associations between birth weight and\nprenatal ambient environmental conditions--pollution and extreme\ntemperatures--differ by 1) maternal education; 2) children's innate health; and\n3) interactions between these two. We link birth records from Guangzhou, China,\nduring a period of high pollution, to ambient air pollution (PM10 and a\ncomposite measure) and extreme temperature data. We first use mean regressions\nto test whether, overall, maternal education is an \"effect modifier\" in the\nrelationships between ambient air pollution, extreme temperature, and birth\nweight. We then use conditional quantile regressions to test for effect\nheterogeneity according to the unobserved innate vulnerability of babies after\nconditioning on other confounders. Results show that 1) the negative\nassociation between ambient exposures and birth weight is twice as large at\nlower conditional quantiles of birth weights as at the median; 2) the\nprotection associated with college-educated mothers with respect to pollution\nand extreme heat is heterogeneous and potentially substantial: between 0.02 and\n0.34 standard deviations of birth weights, depending on the conditional\nquantiles; 3) this protection is amplified under more extreme ambient\nconditions and for infants with greater unobserved innate vulnerabilities.\n"
    },
    {
        "paper_id": 2204.00251,
        "authors": "Florentina \\c{S}oiman (CASC, CNRS - UMR3571), Guillaume Dumas (CNRS -\n  UMR3571), Sonia Jimenez-Garces (CERAG)",
        "title": "The return of (I)DeFiX",
        "comments": null,
        "journal-ref": null,
        "doi": "10.1016/j.intfin.2023.101786",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Decentralized Finance (DeFi) is a nascent set of financial services, using\ntokens, smart contracts, and blockchain technology as financial instruments. We\ninvestigate four possible drivers of DeFi returns: exposure to cryptocurrency\nmarket, the network effect, the investor's attention, and the valuation ratio.\nAs DeFi tokens are distinct from classical cryptocurrencies, we design a new\ndedicated market index, denoted DeFiX. First, we show that DeFi tokens returns\nare driven by the investor's attention on technical terms such as\n\"decentralized finance\" or \"DeFi\", and are exposed to their own network\nvariables and cryptocurrency market. We construct a valuation ratio for the\nDeFi market by dividing the Total Value Locked (TVL) by the Market\nCapitalization (MC). Our findings do not support the TVL/MC predictive power\nassumption. Overall, our empirical study shows that the impact of the\ncryptocurrency market on DeFi returns is stronger than any other considered\ndriver and provides superior explanatory power.\n"
    },
    {
        "paper_id": 2204.00312,
        "authors": "Arianna Mingone",
        "title": "No arbitrage global parametrization for the eSSVI volatility surface",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The article describes a global and arbitrage-free parametrization of the\neSSVI surfaces introduced by Hendriks and Martini in 2019. A robust calibration\nof such surfaces has already been proposed by the quantitative research team at\nZeliade in 2019, but it is sequential in expiries and lacks of a global view on\nthe surface. The alternative calibration suggested in this article is faster\nand always guarantees an arbitrage-free fit of market data.\n"
    },
    {
        "paper_id": 2204.00419,
        "authors": "Jean-Philippe Deranty, Thomas Corbin",
        "title": "Artificial Intelligence and work: a critical review of recent research\n  from the social sciences",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  This review seeks to present a comprehensive picture of recent discussions in\nthe social sciences of the anticipated impact of AI on the world of work.\nIssues covered include technological unemployment, algorithmic management,\nplatform work an the politics of AI work. The review identifies the major\ndisciplinary and methodological perspectives on AI's impact on work, and the\nobstacles they face in making predictions. Two parameters influencing the\ndevelopment and deployment of AI in the economy are highlighted, the capitalist\nimperative and nationalistic pressures.\n"
    },
    {
        "paper_id": 2204.0053,
        "authors": "Zongxia Liang, Xiaodong Luo, Fengyi Yuan",
        "title": "Consumption-investment decisions with endogenous reference point and\n  drawdown constraint",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We propose a consumption-investment decision model where past consumption\npeak $h$ plays a crucial role. There are two important consumption levels: the\nlowest constrained level and a reference level, at which the risk aversion in\nterms of consumption rate is changed. We solve this stochastic control problem\nand derive the value function, optimal consumption plan, and optimal investment\nstrategy in semi-explicit forms. We find five important thresholds of wealth,\nall as functions of $h$, and most of them are nonlinear functions. As can be\nseen from numerical results and theoretical analysis, this intuitive and simple\nmodel has significant economic implications, and there are at least three\nimportant predictions: the marginal propensity to consume out of wealth is\ngenerally decreasing but can be increasing for intermediate wealth levels, and\nit jumps inversely proportional to the risk aversion at the reference point;\nthe implied relative risk aversion is roughly a smile in wealth; the welfare of\nthe poor is more vulnerable to wealth shocks than the wealthy. Moreover,\nlocally changing the risk aversion influences the optimal strategies globally,\nrevealing some risk allocation behaviors.\n"
    },
    {
        "paper_id": 2204.00582,
        "authors": "Upasak Das, Gindo Tampubolon",
        "title": "Female Agency and its Implications on Mental and Physical Health:\n  Evidence from the city of Dhaka",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  Women agency defined as the ability to conceive of purposeful plan and to\ncarry out action consistent with such a plan can play an important role in\ndetermining health status. Using data from female respondents conducted in a\nsurvey in the city of Dhaka in Bangladesh, this paper explores how women agency\nrelates to their physical and mental health. The findings indicate women with\nhigh agency to experience significantly lesser mental distress on average.\nCounterintuitively, these women are more likely to report poor physical health.\nAs an explanation, we propose purposeful action among women with high agency as\na potential reason, wherein they conceive purpose in the future and formulate\naction that is feasible today. Hence, these women prefer to report illness and\nget the required treatment to ensure better future health. This illuminates our\nunderstanding of sustainable development and emphasises the critical role of\nwomen agency for sustainable human development.\n"
    },
    {
        "paper_id": 2204.00713,
        "authors": "Suguru Otani",
        "title": "Individual Rationality Conditions of Identifying Matching Costs in\n  Transferable Utility Matching Games",
        "comments": "9 pages with appendix",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  As the widely applied method for measuring matching assortativeness in a\ntransferable utility matching game, a matching maximum score estimation is\nproposed by \\cite{fox2010qe}. This article reveals that combining unmatched\nagents, transfers, and individual rationality conditions with sufficiently\nlarge penalty terms makes it possible to identify the coefficient parameter of\na single common constant, i.e., matching costs in the market.\n"
    },
    {
        "paper_id": 2204.00785,
        "authors": "Tao Xu",
        "title": "Rural Pension System and Farmers' Participation in Residents' Social\n  Insurance",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  As the ageing population and childlessness are increasing in rural China,\nsocial pensions will become the mainstream choice for farmers, and the level of\nsocial pensions must be supported by better social insurance. The paper\ncompares the history of rural pension insurance system, outlines the current\nsituation and problems, analyses China Family Panel Studies data and explores\nthe key factors influencing farmers' participation through an empirical\napproach. The paper shows that residents' social pension insurance is facing\nproblems in the rural areas such as low level of protection and weak management\ncapacity, which have contributed to the under-insured rate, and finds that\nthere is a significant impact on farmers' participation in insurance from\npersonal characteristics factors such as gender, age, health and (family)\nfinancial factors such as savings, personal income, intergenerational mobility\nof funds. And use of the Internet can help farmers enroll in pension insurance.\nThe paper argues for the need to continue to implement the rural revitalisation\nstrategy, with the government as the lead and the market as the support, in a\nconcerted effort to improve the protection and popularity of rural pension\ninsurance.\n"
    },
    {
        "paper_id": 2204.00872,
        "authors": "Julia Nasiadka and Weronika Nitka and Rafa{\\l} Weron",
        "title": "Calibration window selection based on change-point detection for\n  forecasting electricity prices",
        "comments": "Forthcoming in: Proceedings of the International Conference on\n  Computational Science (ICCS) 2022, London, UK",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We employ a recently proposed change-point detection algorithm, the\nNarrowest-Over-Threshold (NOT) method, to select subperiods of past\nobservations that are similar to the currently recorded values. Then,\ncontrarily to the traditional time series approach in which the most recent\n$\\tau$ observations are taken as the calibration sample, we estimate\nautoregressive models only for data in these subperiods. We illustrate our\napproach using a challenging dataset - day-ahead electricity prices in the\nGerman EPEX SPOT market - and observe a significant improvement in forecasting\naccuracy compared to commonly used approaches, including the Autoregressive\nHybrid Nearest Neighbors (ARHNN) method.\n"
    },
    {
        "paper_id": 2204.00883,
        "authors": "Arkadiusz J\\k{e}drzejewski, Jesus Lago, Grzegorz Marcjasz, Rafa{\\l}\n  Weron",
        "title": "Electricity Price Forecasting: The Dawn of Machine Learning",
        "comments": "Forthcoming in: IEEE Power & Energy Magazine, May/June 2022",
        "journal-ref": "IEEE Power & Energy Magazine 20(3) (2022) 24-31",
        "doi": "10.1109/MPE.2022.3150809",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Electricity price forecasting (EPF) is a branch of forecasting on the\ninterface of electrical engineering, statistics, computer science, and finance,\nwhich focuses on predicting prices in wholesale electricity markets for a whole\nspectrum of horizons. These range from a few minutes (real-time/intraday\nauctions and continuous trading), through days (day-ahead auctions), to weeks,\nmonths or even years (exchange and over-the-counter traded futures and forward\ncontracts). Over the last 25 years, various methods and computational tools\nhave been applied to intraday and day-ahead EPF. Until the early 2010s, the\nfield was dominated by relatively small linear regression models and\n(artificial) neural networks, typically with no more than two dozen inputs. As\ntime passed, more data and more computational power became available. The\nmodels grew larger to the extent where expert knowledge was no longer enough to\nmanage the complex structures. This, in turn, led to the introduction of\nmachine learning (ML) techniques in this rapidly developing and fascinating\narea. Here, we provide an overview of the main trends and EPF models as of\n2022.\n"
    },
    {
        "paper_id": 2204.00894,
        "authors": "Duha Altindag, Samuel Cole, R. Alan Seals Jr",
        "title": "The Price of COVID-19 Risk in a Public University",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We study the allocation of and compensation for occupational COVID-19 risk at\nAuburn University, a large public university in the U.S. In Spring 2021,\napproximately half of the face-to-face classes had enrollments above the legal\ncapacity allowed by a public health order, which followed CDC social distancing\nguidelines. We find lower-ranked graduate student teaching assistants and\nadjunct instructors were systematically recruited to deliver riskier classes.\nUsing an IV strategy in which teaching risk is shifted by classroom features\n(geometry and furniture), we show instructors who taught at least one risky\nclass earned $7,400 more than those who did not.\n"
    },
    {
        "paper_id": 2204.01071,
        "authors": "Jonathan Ansari, Eva L\\\"utkebohmert, Ariel Neufeld, Julian Sester",
        "title": "Improved Robust Price Bounds for Multi-Asset Derivatives under\n  Market-Implied Dependence Information",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We show how inter-asset dependence information derived from market prices of\noptions can lead to improved model-free price bounds for multi-asset\nderivatives. Depending on the type of the traded option, we either extract\ncorrelation information or we derive restrictions on the set of admissible\ncopulas that capture the inter-asset dependencies. To compute the resultant\nprice bounds for some multi-asset options of interest, we apply a modified\nmartingale optimal transport approach. Several examples based on simulated and\nreal market data illustrate the improvement of the obtained price bounds and\nthus provide evidence for the relevance and tractability of our approach.\n"
    },
    {
        "paper_id": 2204.01196,
        "authors": "Emily Hannum, Fan Wang",
        "title": "Fewer, better pathways for all? Intersectional impacts of rural school\n  consolidation in China's minority regions",
        "comments": null,
        "journal-ref": "World Development 151 (March): 105734 (2022)",
        "doi": "10.1016/j.worlddev.2021.105734",
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Primary school consolidation--the closure of small community schools or their\nmergers into larger, better-resourced schools--is emerging as a significant\npolicy response to changing demographics in middle income countries with large\nrural populations. In China, large-scale consolidation took place in the early\n21st century. Because officially-recognized minority populations\ndisproportionately reside in rural and remote areas, minority students were\namong those at elevated risk of experiencing school consolidation. We analyze\nheterogeneous effects of consolidation on educational attainment and reported\nnational language ability in China by exploiting variations in closure timing\nacross villages and cohorts captured in a 2011 survey of provinces and\nautonomous regions with substantial minority populations. We consider\nheterogeneous treatment effects across groups defined at the intersections of\nminority status, gender, and community ethnic composition and socioeconomic\nstatus. Compared to villages with schools, villages whose schools had closed\nreported that the schools students now attended were better resourced, less\nlikely to offer minority language of instruction, more likely to have Han\nteachers, farther away, and more likely to require boarding. Much more than Han\nyouth, ethnic minority youth were negatively affected by closure, in terms of\nits impact on both educational attainment and written Mandarin facility.\nHowever, significant penalties accruing to minority youth occurred only in the\npoorest villages. Penalties were generally heavier for girls, but in the most\nethnically segregated minority villages, boys from minority families were\nhighly vulnerable to closure effects on attainment and written Mandarin\nfacility. Results show that intersections of minority status, gender, and\ncommunity characteristics can delineate significant heterogeneities in policy\nimpacts.\n"
    },
    {
        "paper_id": 2204.01284,
        "authors": "Maria Logvaneva, Mikhail Tselishchev",
        "title": "On a Stochastic Model of Diversification",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We propose a definition of diversification as a binary relationship between\nfinancial portfolios. According to it, a convex linear combination of several\nrisk positions with some weights is considered to be less risky than the\nprobabilistic mixture of the same risk positions with the same weights. It\nturns out to be that the proposed partial ordering coincides with the\nwell-known second order stochastic dominance, but allows to take a look at it\nfrom another perspective.\n"
    },
    {
        "paper_id": 2204.01296,
        "authors": "Kerstin H\\\"otte and Melline Somers and Angelos Theodorakopoulos",
        "title": "Technology and jobs: A systematic literature review",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Does technological change destroy or create jobs? New technologies may\nreplace human workers, but can simultaneously create jobs if workers are needed\nto use these technologies or if new economic activities emerge. Furthermore,\ntechnology-driven productivity growth may increase disposable income,\nstimulating a demand-induced expansion of employment. To synthesize the\nexisting knowledge on this question, we systematically review the empirical\nliterature on the past four decades of technological change and its impact on\nemployment, distinguishing between five broad technology categories (ICT,\nRobots, Innovation, TFP-style, Other). Overall, we find across studies that the\nlabor-displacing effect of technology appears to be more than offset by\ncompensating mechanisms that create or reinstate labor. This holds for most\ntypes of technology, suggesting that previous anxieties over widespread\ntechnology-driven unemployment lack an empirical base, at least so far.\nNevertheless, low-skill, production, and manufacturing workers have been\nadversely affected by technological change, and effective up- and reskilling\nstrategies should remain at the forefront of policy making along with targeted\nsocial support systems.\n"
    },
    {
        "paper_id": 2204.01535,
        "authors": "Kilian Wenker",
        "title": "Retail Central Bank Digital Currencies (CBDC), Disintermediation and\n  Financial Privacy: The Case of the Bahamian Sand Dollar",
        "comments": "30 pages, 5 figures, 2 tables",
        "journal-ref": "FinTech, Volume 1, Issue 4, 1040026, 2022",
        "doi": "10.3390/fintech1040026",
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  The fast-growing, market-driven demand for cryptocurrencies worries central\nbanks, as their monetary policy could be completely undermined. Central bank\ndigital currencies (CBDCs) could offer a solution, yet our understanding of\ntheir design and consequences is in its infancy. This non-technical paper\nexamines how The Bahamas has designed the Sand Dollar, the first real-world\ninstance of a retail CBDC. It contrasts the Sand Dollar with definition-based\nspecifications. I then develop a scenario analysis to illustrate commercial\nbank risks. In this process, the central bank becomes a deposit monopolist,\nleading to high funding risks, disintermediation risks, and solvency risks for\nthe com-mercial banking sector. I argue that restrictions and caps will be the\nnew specifications of a regulatory framework for CBDCs if disintermediation in\nthe banking sector is to be prevented. I identify the anonymity of CBDCs as a\ncomparative disadvantage that will affect their adoption. These findings\nprovide insight into governance problems facing central banks, and coherently\nlead to the design of the Sand Dollar. I conclude by suggesting that combating\ncryptocurrencies is a task that cannot be solved by a CBDC.\n"
    },
    {
        "paper_id": 2204.0185,
        "authors": "Jaydip Sen, Saikat Mondal, Gourab Nath",
        "title": "Robust Portfolio Design and Stock Price Prediction Using an Optimized\n  LSTM Model",
        "comments": "This is the accepted version of our paper in the IEEE 18th India\n  Council International Conference (INDICON 21). The final version was\n  published in the proceedings of the IEEE INDOCON'21 which is available in\n  IEEE Xplore. The conference was organized during December 19-21, 2021, in\n  Guwahati, India. The paper consists of 6 pages and it contains 7 figures and\n  13 tables. arXiv admin note: text overlap with arXiv:2202.02723",
        "journal-ref": null,
        "doi": "10.1109/INDICON52576.2021.9691583",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Accurate prediction of future prices of stocks is a difficult task to\nperform. Even more challenging is to design an optimized portfolio with weights\nallocated to the stocks in a way that optimizes its return and the risk. This\npaper presents a systematic approach towards building two types of portfolios,\noptimum risk, and eigen, for four critical economic sectors of India. The\nprices of the stocks are extracted from the web from Jan 1, 2016, to Dec 31,\n2020. Sector-wise portfolios are built based on their ten most significant\nstocks. An LSTM model is also designed for predicting future stock prices. Six\nmonths after the construction of the portfolios, i.e., on Jul 1, 2021, the\nactual returns and the LSTM-predicted returns for the portfolios are computed.\nA comparison of the predicted and the actual returns indicate a high accuracy\nlevel of the LSTM model.\n"
    },
    {
        "paper_id": 2204.01933,
        "authors": "Fan Wang, Esteban Puentes, Jere R. Behrman, Fl\\'avio Cunha",
        "title": "You are what your parents expect: Height and local reference points",
        "comments": null,
        "journal-ref": "Journal of Econometrics, April 1, 2022",
        "doi": "10.1016/j.jeconom.2021.09.020",
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Recent estimates are that about 150 million children under five years of age\nare stunted, with substantial negative consequences for their schooling,\ncognitive skills, health, and economic productivity. Therefore, understanding\nwhat determines such growth retardation is significant for designing public\npolicies that aim to address this issue. We build a model for nutritional\nchoices and health with reference-dependent preferences. Parents care about the\nhealth of their children relative to some reference population. In our\nempirical model, we use height as the health outcome that parents target.\nReference height is an equilibrium object determined by earlier cohorts'\nparents' nutritional choices in the same village. We explore the exogenous\nvariation in reference height produced by a protein-supplementation experiment\nin Guatemala to estimate our model's parameters. We use our model to decompose\nthe impact of the protein intervention on height into price and reference-point\neffects. We find that the changes in reference points account for 65% of the\nheight difference between two-year-old children in experimental and control\nvillages in the sixth annual cohort born after the initiation of the\nintervention.\n"
    },
    {
        "paper_id": 2204.02038,
        "authors": "\\'Eric Herbert and and Gael Giraud and Aur\\'elie Louis-Napol\\'eon and\n  Christophe Goupil",
        "title": "Macroeconomic Dynamics in a finite world: the Thermodynamic Potential\n  Approach",
        "comments": "30 pages, 10 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  This paper presents a conceptual model describing the medium and long-term\nco-evolution of natural and socio-economic subsystems of Earth. An economy is\nviewed as an out-of-equilibrium dissipative structure that can only be\nmaintained with a flow of energy and matter. The distinctive approach\nemphasized here consists in capturing the economic impact of natural ecosystems\nbeing depleted and destroyed by human activities via a pinch of thermodynamic\npotentials. This viewpoint allows: (i) the full-blown integration of a limited\nquantity of primary resources into a non-linear macrodynamics that is\nstock-flow consistent both in terms of matter-energy as well as economic\ntransactions; (ii) the inclusion of natural and forced recycling; (iii) the\ninclusion of a friction term which reflects the impossibility of producing\ngoods and services in high metabolising intensity without exuding energy and\nmatter wastes; (iv) the computation of the anthropically produced entropy as a\nfunction of intensity and friction. Analysis and numerical computations confirm\nthe role played by intensity and friction as key factors for sustainability.\nOur approach is flexible enough to allow for various economic models to be\nembedded into our thermodynamic framework.\n"
    },
    {
        "paper_id": 2204.02376,
        "authors": "Florian Bourgey, Stefano De Marco, Peter K. Friz, Paolo Pigato",
        "title": "Local volatility under rough volatility",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/publicdomain/zero/1.0/",
        "abstract": "  Several asymptotic results for the implied volatility generated by a rough\nvolatility model have been obtained in recent years (notably in the\nsmall-maturity regime), providing a better understanding of the shapes of the\nvolatility surface induced by rough volatility models, and supporting their\ncalibration power to S&P500 option data. Rough volatility models also generate\na local volatility surface, via the so-called Markovian projection of the\nstochastic volatility. We complement the existing results on the implied\nvolatility by studying the asymptotic behavior of the local volatility surface\ngenerated by a class of rough stochastic volatility models, encompassing the\nrough Bergomi model. Notably, we observe that the celebrated \"1/2 skew rule\"\nlinking the short-term at-the-money skew of the implied volatility to the\nshort-term at-the-money skew of the local volatility, a consequence of the\ncelebrated \"harmonic mean formula\" of [Berestycki, Busca, and Florent, QF\n2002], is replaced by a new rule: the ratio of the at-the-money implied and\nlocal volatility skews tends to the constant 1/(H + 3/2) (as opposed to the\nconstant 1/2), where H is the regularity index of the underlying instantaneous\nvolatility process.\n"
    },
    {
        "paper_id": 2204.02542,
        "authors": "Esteban Puentes, Fan Wang, Jere R. Behrman, Fl\\'avio Cunha, John\n  Hoddinott, John A. Maluccio, Linda S. Adair, Judith B. Borja, Reynaldo\n  Martorell, Aryeh D. Stein",
        "title": "Early life height and weight production functions with endogenous energy\n  and protein inputs",
        "comments": null,
        "journal-ref": "Economics & Human Biology 22 (September 1, 2016): 65-81",
        "doi": "10.1016/j.ehb.2016.03.002",
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  We examine effects of protein and energy intakes on height and weight growth\nfor children between 6 and 24 months old in Guatemala and the Philippines.\nUsing instrumental variables to control for endogeneity and estimating multiple\nspecifications, we find that protein intake plays an important and positive\nrole in height and weight growth in the 6-24 month period. Energy from other\nmacronutrients, however, does not have a robust relation with these two\nanthropometric measures. Our estimates indicate that in contexts with\nsubstantial child undernutrition, increases in protein-rich food intake in the\nfirst 24 months can have important growth effects, which previous studies\nindicate are related significantly to a range of outcomes over the life cycle.\n"
    },
    {
        "paper_id": 2204.02623,
        "authors": "Zhuangwei Shi, Yang Hu, Guangliang Mo, Jian Wu",
        "title": "Attention-based CNN-LSTM and XGBoost hybrid model for stock prediction",
        "comments": "arXiv admin note: text overlap with arXiv:2202.13800",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Stock market plays an important role in the economic development. Due to the\ncomplex volatility of the stock market, the research and prediction on the\nchange of the stock price, can avoid the risk for the investors. The\ntraditional time series model ARIMA can not describe the nonlinearity, and can\nnot achieve satisfactory results in the stock prediction. As neural networks\nare with strong nonlinear generalization ability, this paper proposes an\nattention-based CNN-LSTM and XGBoost hybrid model to predict the stock price.\nThe model constructed in this paper integrates the time series model, the\nConvolutional Neural Networks with Attention mechanism, the Long Short-Term\nMemory network, and XGBoost regressor in a non-linear relationship, and\nimproves the prediction accuracy. The model can fully mine the historical\ninformation of the stock market in multiple periods. The stock data is first\npreprocessed through ARIMA. Then, the deep learning architecture formed in\npretraining-finetuning framework is adopted. The pre-training model is the\nAttention-based CNN-LSTM model based on sequence-to-sequence framework. The\nmodel first uses convolution to extract the deep features of the original stock\ndata, and then uses the Long Short-Term Memory networks to mine the long-term\ntime series features. Finally, the XGBoost model is adopted for fine-tuning.\nThe results show that the hybrid model is more effective and the prediction\naccuracy is relatively high, which can help investors or institutions to make\ndecisions and achieve the purpose of expanding return and avoiding risk. Source\ncode is available at\nhttps://github.com/zshicode/Attention-CLX-stock-prediction.\n"
    },
    {
        "paper_id": 2204.0268,
        "authors": "T. van der Zwaard, L.A. Grzelak, C.W. Oosterlee",
        "title": "Relevance of Wrong-Way Risk in Funding Valuation Adjustments",
        "comments": null,
        "journal-ref": null,
        "doi": "10.1016/j.frl.2022.103091",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In March 2020, the world was thrown into financial distress. This manifested\nitself in increased uncertainty in the financial markets. Many interest rates\ncollapsed, and funding spreads surged significantly, which increased due to the\nmarket turmoil. In light of these events, it is essential to understand and\nmodel Wrong-Way Risk (WWR) in a Funding Valuation Adjustment (FVA) context. WWR\nmay currently be absent from FVA calculations in banks' Valuation Adjustment\n(xVA) engines. However, in this letter, we demonstrate that WWR effects are\nnon-negligible in FVA modelling from a risk-management perspective. We look at\nthe impact of various modelling choices, such as including the default times of\nthe relevant parties, as well as stochastic and deterministic funding spreads.\nA case study is presented for interest rate derivatives.\n"
    },
    {
        "paper_id": 2204.02682,
        "authors": "James B. Glattfelder and Anton Golub",
        "title": "Bridging the Gap: Decoding the Intrinsic Nature of Time in Market Data",
        "comments": "15 pages, 5 figures, and 2 tables",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
        "abstract": "  Intrinsic time is an example of an event-based conception of time, used to\nanalyze financial time series. Here, for the first time, we reveal the\nconnection between intrinsic time and physical time. In detail, we present an\nanalytic relationship which links the two different time paradigms. Central to\nthis discovery are the emergence of scaling laws. Indeed, a novel empirical\nscaling law is presented, relating to the variability of what is know as\novershoots in the intrinsic time framework. To evaluate the validity of the\ntheoretically derived expressions, three time series are analyzed; in detail,\nBrownian motion and two tick-by-tick empirical currency market data sets (one\ncrypto and one fiat). Finally, the time series analyzed in physical time can be\ndecomposed into their liquidity and volatility components, both only visible in\nintrinsic time, further highlighting the utility of this temporal kinship.\n"
    },
    {
        "paper_id": 2204.02757,
        "authors": "Bruno Spilak and Wolfgang Karl H\\\"ardle",
        "title": "Risk budget portfolios with convex Non-negative Matrix Factorization",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  We propose a portfolio allocation method based on risk factor budgeting using\nconvex Nonnegative Matrix Factorization (NMF). Unlike classical factor\nanalysis, PCA, or ICA, NMF ensures positive factor loadings to obtain\ninterpretable long-only portfolios. As the NMF factors represent separate\nsources of risk, they have a quasi-diagonal correlation matrix, promoting\ndiversified portfolio allocations. We evaluate our method in the context of\nvolatility targeting on two long-only global portfolios of cryptocurrencies and\ntraditional assets. Our method outperforms classical portfolio allocations\nregarding diversification and presents a better risk profile than hierarchical\nrisk parity (HRP). We assess the robustness of our findings using Monte Carlo\nsimulation.\n"
    },
    {
        "paper_id": 2204.02891,
        "authors": "Xianfei Hui, Baiqing Sun, Indranil SenGupta, Yan Zhou and Hui Jiang",
        "title": "Stochastic volatility modeling of high-frequency CSI 300 index and\n  dynamic jump prediction driven by machine learning",
        "comments": null,
        "journal-ref": "Electronic Research Archive, 2023",
        "doi": "10.3934/era.2023070",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper models stochastic process of price time series of CSI 300 index in\nChinese financial market, analyzes volatility characteristics of intraday\nhigh-frequency price data. In the new generalized Barndorff-Nielsen and\nShephard model, the lag caused by asynchrony of market information is\nconsidered, and the problem of lack of long-term dependence is solved. To speed\nup the valuation process, several machine learning and deep learning algorithms\nare used to estimate parameter and evaluate forecast results. Tracking\nhistorical jumps of different magnitudes offers promising avenues for\nsimulating dynamic price processes and predicting future jumps. Numerical\nresults show that the deterministic component of stochastic volatility\nprocesses would always be captured over short and longer-term windows. Research\nfinding could be suitable for influence investors and regulators interested in\npredicting market dynamics based on realized volatility.\n"
    },
    {
        "paper_id": 2204.03055,
        "authors": "Paras Bhatt, Claire Vishik, Govind Hariharan, H. Raghav Rao",
        "title": "To Participate Or Not To Participate: An Investigation Of Strategic\n  Participation In Standards",
        "comments": "https://www.researchgate.net/publication/359415466_To_Participate_Or_Not_To_Participate_An_Investigation_Of_Strategic_Participation_In_Standards",
        "journal-ref": "11th International Conference on Standardisation and Innovation in\n  Information Technology (SIIT) - The Past, Present and Future of ICT\n  Standardisation, Sep, 2021",
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  Essential functionality in the ICT (Information and Communication Technology)\nspace draws from standards such as HTTP (IETF RFC 2616, Bluetooth (IEEE 802.15)\nand various telecommunication standards (4G, 5G). They have fuelled rapid\ngrowth of ICT sector in the last decades by ensuring interoperability and\nconsistency in computing environment. Research shows that firms that backed ICT\nstandards and participated in standards development, have emerged as industry\ninnovators. Standards development thus clearly has benefits for participating\ncompanies as well as technology development and innovation in general. However,\nsignificant costs are also associated with development of standards and need to\nbe better understood to support investment in standardization necessary for\ntodays ICT environment. We present a conceptual model that considers the\npotential for market innovation across a standards lifecycle and efficiency\nfrom standardization work, to build a forward-looking decision model that can\nguide an organizations standards development activities. We investigate and\nformalize motivations that drive firms to participate in standardization,\nspecifically, changes in market innovation. Our model can serve as a strategic\ndecision framework to drive assessments of a firms participation in standards\ndevelopment. We test our model with a use case on an established access control\napproach that was standardized more than two decades ago, Role Based Access\nControl (RBAC) using historical data. The investigation of the case study shows\nthat change in market innovation is a significant indicator of success in\nstandards development and are viable criteria to model a firms decision to\nparticipate (or not to participate) in a specific area of standardization.\n"
    },
    {
        "paper_id": 2204.03285,
        "authors": "Rutger van der Spek and Alexis Derumigny",
        "title": "Fast estimation of Kendall's Tau and conditional Kendall's Tau matrices\n  under structural assumptions",
        "comments": "61 pages, 13 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Kendall's tau and conditional Kendall's tau matrices are multivariate\n(conditional) dependence measures between the components of a random vector.\nFor large dimensions, available estimators are computationally expensive and\ncan be improved by averaging. Under structural assumptions on the underlying\nKendall's tau and conditional Kendall's tau matrices, we introduce new\nestimators that have a significantly reduced computational cost while keeping a\nsimilar error level. In the unconditional setting we assume that, up to\nreordering, the underlying Kendall's tau matrix is block-structured with\nconstant values in each of the off-diagonal blocks. Consequences on the\nunderlying correlation matrix are then discussed. The estimators take advantage\nof this block structure by averaging over (part of) the pairwise estimates in\neach of the off-diagonal blocks. Derived explicit variance expressions show\ntheir improved efficiency. In the conditional setting, the conditional\nKendall's tau matrix is assumed to have a constant block structure,\nindependently of the conditioning variable. Conditional Kendall's tau matrix\nestimators are constructed similarly as in the unconditional case by averaging\nover (part of) the pairwise conditional Kendall's tau estimators. We establish\ntheir joint asymptotic normality, and show that the asymptotic variance is\nreduced compared to the naive estimators. Then, we perform a simulation study\nwhich displays the improved performance of both the unconditional and\nconditional estimators. Finally, the estimators are used for estimating the\nvalue at risk of a large stock portfolio; backtesting illustrates the obtained\nimprovements compared to the previous estimators.\n"
    },
    {
        "paper_id": 2204.03318,
        "authors": "Johan Gars, Daniel Spiro, Henrik Wachtmeister",
        "title": "What is the effect of EU's fuel-tax cuts on Russia's oil income?",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Following the oil-price surge in the wake of Russia's invasion of Ukraine,\nmany countries in the EU are cutting taxes on petrol and diesel. Using standard\ntheory and empirical estimates, we assess how such tax cuts influence the oil\nincome in Russia. We find that a tax cut of 20 euro cents per liter increase\nRussia's oil profits by around 11 million Euros per day in the short run and\nlong run. This is equivalent to 4100 million Euros in a year, 0.3% of Russia's\nGDP or 7% of its military spending. We show that a cash transfer to EU\ncitizens, with an equivalent fiscal burden as the tax cut, reduces these side\neffects to a fraction.\n"
    },
    {
        "paper_id": 2204.0345,
        "authors": "Danae Arroyos-Calvera and Nattavudh Powdthavee",
        "title": "Reputation as insurance: how reputation moderates public backlash\n  following a company's decision to profiteer",
        "comments": "42 pages, 5 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  We examine whether a company's corporate reputation gained from their CSR\nactivities and a company leader's reputation, one that is unrelated to his or\nher business acumen, can impact economic action fairness appraisals. We provide\nexperimental evidence that good corporate reputation causally buffers\nindividuals' negative fairness judgment following the firm's decision to\nprofiteer from an increase in the demand. Bad corporate reputation does not\nmake the decision to profiteer as any less acceptable. However, there is\nevidence that individuals judge as more unfair an ill-reputed firm's decision\nto raise their product's price to protect against losses. Thus, our results\nhighlight the importance of a good reputation in protecting a firm against\nsevere negative judgments from making an economic decision that the public\ndeems unfair.\n"
    },
    {
        "paper_id": 2204.0376,
        "authors": "Peter Lerner",
        "title": "The market drives ETFs or ETFs the market: causality without Granger",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-sa/4.0/",
        "abstract": "  This paper develops a deep learning-based econometric methodology to\ndetermine the causality of the financial time series. This method is applied to\nthe imbalances in daily transactions in individual stocks, as well as the ETFs\nreported to SEC with a nanosecond time stamp. Based on our method, we conclude\nthat transaction imbalances of ETFs alone are more informative than the\ntransaction imbalances in the entire market. Characteristically, a sheer number\nof imbalance messages related to the individual stocks dominates the imbalance\nmessages due to the ETF in the proportion of 8:1.\n"
    },
    {
        "paper_id": 2204.03798,
        "authors": "Ferdoos Alharbi and Tahir Choulli",
        "title": "Log-optimal portfolio after a random time: Existence, description and\n  sensitivity analysis",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper, we consider an informational market model with two flows of\ninformations. The smallest flow F, which is available to all agents, is the\nfiltration of the initial market model(S,F,P), where S is the assets' prices\nand P is a probability measure. The largest flow G contains additional\ninformation about the occurrence of a random time T. This setting covers credit\nrisk theory where T models the default time of a firm, and life insurance where\nT represents the death time of an insured. For the model (S-S^T,G,P), we\naddress the log-optimal portfolio problem in many aspects. In particular, we\nanswer the following questions and beyond: 1) What are the necessary and\nsufficient conditions for the existence of log-optimal portfolio of the model\nunder consideration? 2) what are the various type of risks induced by T that\naffect this portfolio and how? 3) What are the factors that completely describe\nthe sensitivity of the log-portfolio to the parameters of T? The answers to\nthese questions and other related discussions definitely complement the work of\nChoulli and Yansori [12] which deals with the stopped model (S^T,G).\n"
    },
    {
        "paper_id": 2204.03799,
        "authors": "Vegard M. Nygaard, Bent E. S{\\o}rensen, Fan Wang",
        "title": "Optimal allocations to heterogeneous agents with an application to\n  stimulus checks",
        "comments": null,
        "journal-ref": "Journal of Economic Dynamics and Control 138 (May 1, 2022): 104352",
        "doi": "10.1016/j.jedc.2022.104352",
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  A planner allocates discrete transfers of size $D_g$ to $N$ heterogeneous\ngroups labeled $g$ and has CES preferences over the resulting outcomes,\n$H_g(D_g)$. We derive a closed-form solution for optimally allocating a fixed\nbudget subject to group-specific inequality constraints under the assumption\nthat increments in the $H_g$ functions are non-increasing. We illustrate our\nmethod by studying allocations of \"support checks\" from the U.S. government to\nhouseholds during both the Great Recession and the COVID-19 pandemic. We\ncompare the actual allocations to optimal ones under alternative constraints,\nassuming the government focused on stimulating aggregate consumption during the\n2008--2009 crisis and focused on welfare during the 2020--2021 crisis. The\ninputs for this analysis are obtained from versions of a life-cycle model with\nheterogeneous households, which predicts household-type-specific consumption\nand welfare responses to tax rebates and cash transfers.\n"
    },
    {
        "paper_id": 2204.03948,
        "authors": "Elissaios Pappyrakis, Osiris Jorge Parcero",
        "title": "The Psychology of Mineral Wealth: Empirical Evidence from Kazakhstan",
        "comments": "26 pages, 7 tables",
        "journal-ref": "Resources Policy, vol.77, 102706 (2022)",
        "doi": "10.1016/j.resourpol.2022.102706",
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  Despite rapidly-expanding academic and policy interest in the links between\nnatural resource wealth and development failures (commonly referred to as the\nresource curse) little attention has been devoted to the psychology behind the\nphenomenon. Rent-seeking and excessive reliance on mineral revenues can be\nattributed largely to social psychology. Mineral booms (whether due to the\ndiscovery of mineral reserves or to the drastic rise in commodity prices) start\nas positive income shocks that can subsequently evolve into influential and\nexpectation-changing public and media narratives; these lead consecutively to\nunrealistic demands that favor immediate consumption of accrued mineral\nrevenues and to the postponement of productive investment. To our knowledge,\nthis paper is the first empirical analysis that tests hypotheses regarding the\npsychological underpinnings of resource mismanagement in mineral-rich states.\nOur study relies on an extensive personal survey (of 1977 respondents) carried\nout in Almaty, Kazakhstan, between May and August 2018. We find empirical\nsupport for a positive link between exposure to news and inflated expectations\nregarding mineral availability, as well as evidence that the latter can\ngenerate preferences for excessive consumption, and hence, rent-seeking.\n"
    },
    {
        "paper_id": 2204.04396,
        "authors": "Jules R. Siedenburg",
        "title": "Local Knowledge and Natural Resource Management in a Peasant Farming\n  Community Facing Rapid Change: A Critical Examination",
        "comments": "31 pages including 12 figures and tables, based on PhD research\n  conducted at Queen Elizabeth House, Oxford University",
        "journal-ref": "QEH Working Paper 166, 2008, 1-31",
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Environmental degradation is a major global problem. Its impacts are not just\nenvironmental, but also economic, with degradation recognised as a key cause of\nreduced agricultural productivity and rural poverty in the developing world.\nThe degradation literature typically emphasises common property or open access\nnatural resources, and how perverse incentives or missing institutions lead\noptimising private actors to degrade them. By contrast, the present paper\nconsiders degradation occurring on private farms in peasant communities. This\nis a critical yet delicate issue, given the poverty of such areas and questions\nabout the role of farmers in either degrading or regenerating rural lands. The\npaper examines natural resource management by peasant farmers in Tanzania. Its\nkey concern is how the local knowledge informing their management decisions\nadapts to challenges associated with environmental degradation and market\nliberalisation. Given their poverty, this question could have direct\nimplications for the capacity of households to meet their livelihood needs.\nBased on fresh empirical data, the paper finds that differential farmer\nknowledge helps explain the large differences in how households respond to the\ndegradation challenge. The implication is that some farmers adapt more\neffectively to emerging challenges than others, despite all being rational,\noptimising agents who follow the strategies they deem best. The paper thus\nprovides a critique of local knowledge, implying that some farmers experience\nadaptation slippages while others race ahead with effective adaptations. The\npaper speaks to the chronic poverty that plagues many rural communities in the\ndeveloping world. It helps explain the failure of proven sustainable\nagriculture technologies to disseminate readily beyond early innovators. Its\nkey policy implication is to inform improved capacity building for such\ncommunities.\n"
    },
    {
        "paper_id": 2204.04701,
        "authors": "Jo Blanden, Matthias Doepke and Jan Stuhler",
        "title": "Educational Inequality",
        "comments": "This chapter has been prepared for the Handbook of the Economics of\n  Education, Volume 6",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
        "abstract": "  This chapter provides new evidence on educational inequality and reviews the\nliterature on the causes and consequences of unequal education. We document\nlarge achievement gaps between children from different socio-economic\nbackgrounds, show how patterns of educational inequality vary across countries,\ntime, and generations, and establish a link between educational inequality and\nsocial mobility. We interpret this evidence from the perspective of economic\nmodels of skill acquisition and investment in human capital. The models account\nfor different channels underlying unequal education and highlight how\nendogenous responses in parents' and children's educational investments\ngenerate a close link between economic inequality and educational inequality.\nGiven concerns over the extended school closures during the Covid-19 pandemic,\nwe also summarize early evidence on the impact of the pandemic on children's\neducation and on possible long-run repercussions for educational inequality.\n"
    },
    {
        "paper_id": 2204.04794,
        "authors": "Neji Saidi",
        "title": "Willingness to pay, surplus and Insurance policy under dual theory",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper, we aims to state some proprieties of willingness to pay (WTP)\nfor partial risk reduction and links with insurance within the dual theory of\ndecision. In the case of partial reduction, we get as Langlais (2005) that a\nrisk-averse decision maker (DM) can have a willingness to pay small than this\nof a neutral one. By decomposition the WTP as Courbage and al (2008), we get\nthat a strong averse DM is willing to give more for a reduction of a high\nprobability portion rather than a low probability one. The main result is that\nin the dual theory, reducing probability of risk and supply insurance can be\ncomplementary if the surplus is increasing in risk reduction.\n"
    },
    {
        "paper_id": 2204.05105,
        "authors": "Fujun Hou",
        "title": "Describing Sen's Transitivity Condition in Inequalities and Equations",
        "comments": "The practical significance of our results lies in the testing of\n  Sen's transitivity condition with a computer. This is particularly true when,\n  as presented in Theorem 1\", the condition is constructed based on 0-1\n  matrices",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  In social choice theory, Sen's value restriction condition is a sufficiency\ncondition restricted to individuals' ordinal preferences so as to obtain a\ntransitive social preference under the majority decision rule. In this article,\nSen's transitivity condition is described by use of inequality and equation.\nFirst, for a triple of alternatives, an individual's preference is represented\nby a preference map, whose entries are sets containing the ranking position or\npositions derived from the individual's preference over that triple of those\nalternatives. Second, by using the union operation of sets and the cardinality\nconcept, Sen's transitivity condition is described by inequalities. Finally, by\nusing the membership function of sets, Sen's transitivity condition is further\ndescribed by equations.\n"
    },
    {
        "paper_id": 2204.05199,
        "authors": "Shao Ying-Hui and Liu Ying-Lin and Yang Yan-Hong",
        "title": "The short-term effect of COVID-19 pandemic on China's crude oil futures\n  market: A study based on multifractal analysis",
        "comments": "11 pages,3 figures",
        "journal-ref": null,
        "doi": "10.1142/S0219477523400011",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The ongoing COVID-19 shocked financial markets globally, including China's\ncrude oil future market, which is the third most traded crude oil futures after\nWTI and Brent. As China's first crude oil futures accessible to foreign\ninvestors, the Shanghai crude oil futures (SC) have attracted significant\ninterest since launch at the Shanghai International Energy Exchange. The impact\nof COVID-19 on the new crude oil futures is an important issue for investors\nand policy makers. Therefore this paper studies the short-term influence of\nCOVID-19 pandemic on SC via multifractal analysis. We compare market efficiency\nof SC before and during the pandemic with the multifractal detrended\nfluctuation analysis and other commonly-used random walk tests. Then we\ngenerate shuffled and surrogate data to investigate the components of\nmultifractal nature in SC. And we examine cross-correlations between SC returns\nand other financial assets returns as well as SC trading volume changes by the\nmultifractal detrended cross-correlation analysis. The results show that market\nefficiency of SC and its cross-correlations with other assets increase\nsignificantly after the outbreak of COVID-19. Besides that, the sources of its\nmultifractal nature have changed since the pandemic. The findings provide\nevidence for the short-term impacts of COVID-19 on SC. The results may have\nimportant implications for assets allocation, investment strategies and risk\nmonitoring.\n"
    },
    {
        "paper_id": 2204.05204,
        "authors": "Jos\\'e Brito, Andrei Goloubentsev, Evgeny Goncharov",
        "title": "Automatic Adjoint Differentiation for special functions involving\n  expectations",
        "comments": "16 pages, 1 figure, v2: added acknowledgement",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  We explain how to compute gradients of functions of the form $G = \\frac{1}{2}\n\\sum_{i=1}^{m} (E y_i - C_i)^2$, which often appear in the calibration of\nstochastic models, using Automatic Adjoint Differentiation and parallelization.\nWe expand on the work of arXiv:1901.04200 and give faster and easier to\nimplement approaches. We also provide an implementation of our methods and\napply the technique to calibrate European options.\n"
    },
    {
        "paper_id": 2204.05238,
        "authors": "Guillermo Angeris, Tarun Chitra, Alex Evans, Stephen Boyd",
        "title": "Optimal Routing for Constant Function Market Makers",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We consider the problem of optimally executing an order involving multiple\ncrypto-assets, sometimes called tokens, on a network of multiple constant\nfunction market makers (CFMMs). When we ignore the fixed cost associated with\nexecuting an order on a CFMM, this optimal routing problem can be cast as a\nconvex optimization problem, which is computationally tractable. When we\ninclude the fixed costs, the optimal routing problem is a mixed-integer convex\nproblem, which can be solved using (sometimes slow) global optimization\nmethods, or approximately solved using various heuristics based on convex\noptimization. The optimal routing problem includes as a special case the\nproblem of identifying an arbitrage present in a network of CFMMs, or\ncertifying that none exists.\n"
    },
    {
        "paper_id": 2204.05403,
        "authors": "Guillermo Alonso Alvarez, Sergey Nadtochiy, and Kevin Webster",
        "title": "Optimal brokerage contracts in Almgren-Chriss model with multiple\n  clients",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  This paper constructs optimal brokerage contracts for multiple\n(heterogeneous) clients trading a single asset whose price follows the\nAlmgren-Chriss model. The distinctive features of this work are as follows: (i)\nthe reservation values of the clients are determined endogenously, and (ii) the\nbroker is allowed to not offer a contract to some of the potential clients,\nthus choosing her portfolio of clients strategically. We find a computationally\ntractable characterization of the optimal portfolios of clients (up to a\ndigital optimization problem, which can be solved efficiently if the number of\npotential clients is small) and conduct numerical experiments which illustrate\nhow these portfolios, as well as the equilibrium profits of all market\nparticipants, depend on the price impact coefficients.\n"
    },
    {
        "paper_id": 2204.055,
        "authors": "Javier Espinosa-Brito and Carlos Yevenes-Ortega and Gonzalo\n  Franetovic-Guzman and Diana Ochoa-Diaz",
        "title": "Impact of an Employment Policy on Companies' Expectations Fulfilment",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  We study the effect of Chile's Employment Protection Law (Ley de Protecci\\'on\ndel Empleo, EPL), a law which allowed temporal suspensions of job contracts in\nexceptional circumstances during the COVID-19 pandemic, on the fulfillment of\nfirms' expectations regarding layoffs. We use monthly surveys directed at a\nrepresentative group of firms in the national territory. This panel data allows\nto follow firms through time and analyze the match between their expectations\nand the actual realization to model their expectation fulfilment. We model the\nprobability of expectation fulfilment through a logit model that allows for\nmoderation effects. Results suggest that for those firms that expected to fire\nworkers, for the firms that used the EPL, the odds they finally ended up with a\njob separation are 50% of the odds for those that did not used the EPL. Small\nfirms increase their probability of expectation fulfilment in 11.9% when using\nthe EPL compared to large firms if they declared they were expecting to fire\nworkers.\n"
    },
    {
        "paper_id": 2204.05507,
        "authors": "Chinmay Maheshwari and Kshitij Kulkarni and Manxi Wu and Shankar\n  Sastry",
        "title": "Inducing Social Optimality in Games via Adaptive Incentive Design",
        "comments": "20 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  How can a social planner adaptively incentivize selfish agents who are\nlearning in a strategic environment to induce a socially optimal outcome in the\nlong run? We propose a two-timescale learning dynamics to answer this question\nin both atomic and non-atomic games. In our learning dynamics, players adopt a\nclass of learning rules to update their strategies at a faster timescale, while\na social planner updates the incentive mechanism at a slower timescale. In\nparticular, the update of the incentive mechanism is based on each player's\nexternality, which is evaluated as the difference between the player's marginal\ncost and the society's marginal cost in each time step. We show that any fixed\npoint of our learning dynamics corresponds to the optimal incentive mechanism\nsuch that the corresponding Nash equilibrium also achieves social optimality.\nWe also provide sufficient conditions for the learning dynamics to converge to\na fixed point so that the adaptive incentive mechanism eventually induces a\nsocially optimal outcome. Finally, we demonstrate that the sufficient\nconditions for convergence are satisfied in a variety of games, including (i)\natomic networked quadratic aggregative games, (ii) atomic Cournot competition,\nand (iii) non-atomic network routing games.\n"
    },
    {
        "paper_id": 2204.0568,
        "authors": "Philippe Casgrain, Martin Larsson, Johanna Ziegel",
        "title": "Anytime-valid sequential testing for elicitable functionals via\n  supermartingales",
        "comments": "36 pages, 3 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  We design sequential tests for a large class of nonparametric null hypotheses\nbased on elicitable and identifiable functionals. Such functionals are defined\nin terms of scoring functions and identification functions, which are ideal\nbuilding blocks for constructing nonnegative supermartingales under the null.\nThis in turn yields sequential tests via Ville's inequality. Using regret\nbounds from Online Convex Optimization, we obtain rigorous guarantees on the\nasymptotic power of the tests for a wide range of alternative hypotheses. Our\nresults allow for bounded and unbounded data distributions, assuming that a\nsub-$\\psi$ tail bound is satisfied.\n"
    },
    {
        "paper_id": 2204.05749,
        "authors": "Cornelius A. Rietveld and Pankaj C. Patel",
        "title": "A critical assessment of the National Entrepreneurship Context Index of\n  the Global Entrepreneurship Monitor",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  Data collected through the National Expert Survey (NES) of the Global\nEntrepreneurship Monitor (GEM) are widely used to assess the quality and impact\nof national entrepreneurial ecosystems. By focusing on the measurement of the\nNational Entrepreneurship Context Index (NECI), we argue and show that the\nsubjective nature of the responses of the national experts precludes meaningful\ncross-country analyses and cross-country rankings. Moreover, we show that the\nlimited precision of the NECI severely constraints the longitudinal assessment\nof within-country trends. We provide recommendations for the current use of\nNECI data and suggestions for future NES data collections.\n"
    },
    {
        "paper_id": 2204.05781,
        "authors": "Duygu Ider, Stefan Lessmann",
        "title": "Forecasting Cryptocurrency Returns from Sentiment Signals: An Analysis\n  of BERT Classifiers and Weak Supervision",
        "comments": "29 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  Anticipating price developments in financial markets is a topic of continued\ninterest in forecasting. Funneled by advancements in deep learning and natural\nlanguage processing (NLP) together with the availability of vast amounts of\ntextual data in form of news articles, social media postings, etc., an\nincreasing number of studies incorporate text-based predictors in forecasting\nmodels. We contribute to this literature by introducing weak learning, a\nrecently proposed NLP approach to address the problem that text data is\nunlabeled. Without a dependent variable, it is not possible to finetune\npretrained NLP models on a custom corpus. We confirm that finetuning using weak\nlabels enhances the predictive value of text-based features and raises forecast\naccuracy in the context of predicting cryptocurrency returns. More\nfundamentally, the modeling paradigm we present, weak labeling domain-specific\ntext and finetuning pretrained NLP models, is universally applicable in\n(financial) forecasting and unlocks new ways to leverage text data.\n"
    },
    {
        "paper_id": 2204.05783,
        "authors": "Narayana Darapaneni, Anwesh Reddy Paduri, Himank Sharma, Milind\n  Manjrekar, Nutan Hindlekar, Pranali Bhagat, Usha Aiyer, and Yogesh Agarwal",
        "title": "Stock Price Prediction using Sentiment Analysis and Deep Learning for\n  Indian Markets",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  Stock market prediction has been an active area of research for a\nconsiderable period. Arrival of computing, followed by Machine Learning has\nupgraded the speed of research as well as opened new avenues. As part of this\nresearch study, we aimed to predict the future stock movement of shares using\nthe historical prices aided with availability of sentiment data. Two models\nwere used as part of the exercise, LSTM was the first model with historical\nprices as the independent variable. Sentiment Analysis captured using Intensity\nAnalyzer was used as the major parameter for Random Forest Model used for the\nsecond part, some macro parameters like Gold, Oil prices, USD exchange rate and\nIndian Govt. Securities yields were also added to the model for improved\naccuracy of the model. As the end product, prices of 4 stocks viz. Reliance,\nHDFC Bank, TCS and SBI were predicted using the aforementioned two models. The\nresults were evaluated using RMSE metric.\n"
    },
    {
        "paper_id": 2204.05806,
        "authors": "Zexuan Yin, Paolo Barucca",
        "title": "Variational Heteroscedastic Volatility Model",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  We propose Variational Heteroscedastic Volatility Model (VHVM) -- an\nend-to-end neural network architecture capable of modelling heteroscedastic\nbehaviour in multivariate financial time series. VHVM leverages recent advances\nin several areas of deep learning, namely sequential modelling and\nrepresentation learning, to model complex temporal dynamics between different\nasset returns. At its core, VHVM consists of a variational autoencoder to\ncapture relationships between assets, and a recurrent neural network to model\nthe time-evolution of these dependencies. The outputs of VHVM are time-varying\nconditional volatilities in the form of covariance matrices. We demonstrate the\neffectiveness of VHVM against existing methods such as Generalised\nAutoRegressive Conditional Heteroscedasticity (GARCH) and Stochastic Volatility\n(SV) models on a wide range of multivariate foreign currency (FX) datasets.\n"
    },
    {
        "paper_id": 2204.05926,
        "authors": "Lotfi Boudabsa and Damir Filipovi\\'c",
        "title": "Ensemble learning for portfolio valuation and risk management",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We introduce an ensemble learning method for dynamic portfolio valuation and\nrisk management building on regression trees. We learn the dynamic value\nprocess of a derivative portfolio from a finite sample of its cumulative cash\nflow. The estimator is given in closed form. The method is fast and accurate,\nand scales well with sample size and path space dimension. The method can also\nbe applied to Bermudan style options. Numerical experiments show good results\nin moderate dimension problems.\n"
    },
    {
        "paper_id": 2204.05979,
        "authors": "Francois Mercier, Makesh Narsimhan",
        "title": "Discovering material information using hierarchical Reformer model on\n  financial regulatory filings",
        "comments": "KDD ML in Finance workshop 2021",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Most applications of machine learning for finance are related to forecasting\ntasks for investment decisions. Instead, we aim to promote a better\nunderstanding of financial markets with machine learning techniques. Leveraging\nthe tremendous progress in deep learning models for natural language\nprocessing, we construct a hierarchical Reformer ([15]) model capable of\nprocessing a large document level dataset, SEDAR, from canadian financial\nregulatory filings. Using this model, we show that it is possible to predict\ntrade volume changes using regulatory filings. We adapt the pretraining task of\nHiBERT ([36]) to obtain good sentence level representations using a large\nunlabelled document dataset. Finetuning the model to successfully predict trade\nvolume changes indicates that the model captures a view from financial markets\nand processing regulatory filings is beneficial. Analyzing the attention\npatterns of our model reveals that it is able to detect some indications of\nmaterial information without explicit training, which is highly relevant for\ninvestors and also for the market surveillance mandate of financial regulators.\n"
    },
    {
        "paper_id": 2204.06109,
        "authors": "Sebastian Baran, Przemys{\\l}aw Rola",
        "title": "Prediction of motor insurance claims occurrence as an imbalanced machine\n  learning problem",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  The insurance industry, with its large datasets, is a natural place to use\nbig data solutions. However it must be stressed, that significant number of\napplications for machine learning in insurance industry, like fraud detection\nor claim prediction, deals with the problem of machine learning on an\nimbalanced data set. This is due to the fact that frauds or claims are rare\nevents when compared with the entire population of drivers. The problem of\nimbalanced learning is often hard to overcome. Therefore, the main goal of this\nwork is to present and apply various methods of dealing with an imbalanced\ndataset in the context of claim occurrence prediction in car insurance. In\naddition, the above techniques are used to compare the results of machine\nlearning algorithms in the context of claim occurrence prediction in car\ninsurance. Our study covers the following techniques: logistic-regression,\ndecision tree, random forest, xgBoost, feed-forward network. The problem is the\nclassification one.\n"
    },
    {
        "paper_id": 2204.06545,
        "authors": "Nils K\\\"orber, Maximilian R\\\"ohrig and Andreas Ulbig",
        "title": "A stakeholder-oriented multi-criteria optimization model for decentral\n  multi-energy systems",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The decarbonization of municipal and district energy systems requires\neconomic and ecologic efficient transformation strategies in a wide spectrum of\ntechnical options. Especially under the consideration of multi-energy systems,\nwhich connect energy domains such as heat and electricity supply, expansion and\noperational planning of so-called decentral multi-energy systems (DMES) holds a\nmultiplicity of complexities. This motivates the use of optimization problems,\nwhich reach their limitations with regard to computational feasibility in\ncombination with the required level of detail. With an increased focus on DMES\nimplementation, this problem is aggravated since, moving away from the\ntraditional system perspective, a user-centered, market-integrated perspective\nis assumed. Besides technical concepts it requires the consideration of market\nregimes, e.g. self-consumption and the broader energy sharing. This highlights\nthe need for DMES optimization models which cover a microeconomic perspective\nunder consideration of detailed technical options and energy regulation, in\norder to understand mutual technical and socio-economic and -ecologic\ninteractions of energy policies. In this context we present a\nstakeholder-oriented multi-criteria optimization model for DMES, which\naddresses technical aspects, as well as market and services coverage towards a\nreal-world implementation. The current work bridges a gap between the required\nmodelling level of detail and computational feasibility of DMES expansion and\noperation optimization. Model detail is achieved by the application of a hybrid\ncombination of mathematical methods in a nested multi-level decomposition\napproach, including a Genetic Algorithm, Benders Decomposition and Lagrange\nRelaxation. This also allows for distributed computation on multi-node high\nperformance computer clusters.\n"
    },
    {
        "paper_id": 2204.06588,
        "authors": "Priyank Lathwal, Parth Vaishnav, M. Granger Morgan",
        "title": "Environmental injustice in America: Racial disparities in exposure to\n  air pollution health damages from freight trucking",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
        "abstract": "  PM2.5 produced by freight trucks has adverse impacts on human health.\nHowever, it is unknown to what extent freight trucking affects communities of\ncolor and the total public health burden arising from the sector. Based on\nspatially resolved US federal government data, we explore the geographic\ndistribution of freight trucking emissions and demonstrate that Black and\nHispanic populations are more likely to be exposed to elevated emissions from\nfreight trucks. Our results indicate that freight trucks contribute ~10% of NOx\nand ~12% of CO2 emissions from all sources in the continental US. The annual\ncosts to human health and the environment due to NOx, PM2.5, SO2, and CO2 from\nfreight trucking in the US are estimated respectively to be $11B, $5.5B, $110M,\nand $30B. Overall, the sector is responsible for nearly two-fifths (~$47B out\nof $120B) of all transportation-related public health damages.\n"
    },
    {
        "paper_id": 2204.06692,
        "authors": "Xinyu Wang, Liang Zhao, Ning Zhang, Liu Feng, Haibo Lin",
        "title": "Stability of China's Stock Market: Measure and Forecast by Ricci\n  Curvature on Network",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  The systemic stability of a stock market is one of the core issues in the\nfinancial field. The market can be regarded as a complex network whose nodes\nare stocks connected by edges that signify their correlation strength. Since\nthe market is a strongly nonlinear system, it is difficult to measure the\nmacroscopic stability and depict market fluctuations in time. In this paper, we\nuse a geometric measure derived from discrete Ricci curvature to capture the\nhigher-order nonlinear architecture of financial networks. In order to confirm\nthe effectiveness of our method, we use it to analyze the CSI 300 constituents\nof China's stock market from 2005--2020 and the systemic stability of the\nmarket is quantified through the network's Ricci type curvatures. Furthermore,\nwe use a hybrid model to analyze the curvature time series and predict the\nfuture trends of the market accurately. As far as we know, this is the first\npaper to apply Ricci curvature to forecast the systemic stability of domestic\nstock market, and our results show that Ricci curvature has good explanatory\npower for the market stability and can be a good indicator to judge the future\nrisk and volatility of the domestic market.\n"
    },
    {
        "paper_id": 2204.06848,
        "authors": "Jozef Barunik and Lubos Hanus",
        "title": "Learning Probability Distributions in Macroeconomics and Finance",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  We propose a deep learning approach to probabilistic forecasting of\nmacroeconomic and financial time series. Being able to learn complex patterns\nfrom a data rich environment, our approach is useful for a decision making that\ndepends on uncertainty of large number of economic outcomes. Specifically, it\nis informative to agents facing asymmetric dependence of their loss on outcomes\nfrom possibly non-Gaussian and non-linear variables. We show the usefulness of\nthe proposed approach on the two distinct datasets where a machine learns the\npattern from data. First, we construct macroeconomic fan charts that reflect\ninformation from high-dimensional data set. Second, we illustrate gains in\nprediction of stock return distributions which are heavy tailed, asymmetric and\nsuffer from low signal-to-noise ratio.\n"
    },
    {
        "paper_id": 2204.06943,
        "authors": "Peter Reinhard Hansen and Chen Tong",
        "title": "Option Pricing with Time-Varying Volatility Risk Aversion",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  We introduce a novel pricing kernel with time-varying variance risk aversion\nthat yields closed-form expressions for the VIX. We also obtain closed-form\nexpressions for option prices with a novel approximation method. The model can\nexplain the observed time-variation in the shape of the pricing kernel. We\nestimate the model with S&P 500 returns and option prices and find that\ntime-variation in volatility risk aversion brings a substantial reduction in\nderivative pricing errors. The variance risk ratio emerges as a fundamental\nvariable and we show that it is closely related to economic fundamentals and\nkey measures of sentiment and uncertainty.\n"
    },
    {
        "paper_id": 2204.06967,
        "authors": "Jiafeng Chen, Edward Glaeser, David Wessel",
        "title": "JUE Insight: The (Non-)Effect of Opportunity Zones on Housing Prices",
        "comments": "To appear in Journal of Urban Economics",
        "journal-ref": null,
        "doi": "10.1016/j.jue.2022.103451",
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  Will the Opportunity Zones (OZ) program, America's largest new place-based\npolicy in decades, generate neighborhood change? We compare single-family\nhousing price growth in OZs with price growth in areas that were eligible but\nnot included in the program. We also compare OZs to their nearest geographic\nneighbors. Our most credible estimates rule out price impacts greater than 0.5\npercentage points with 95% confidence, suggesting that, so far, home buyers\ndon't believe that this subsidy will generate major neighborhood change. OZ\nstatus reduces prices in areas with little employment, perhaps because buyers\nthink that subsidizing new investment will increase housing supply. Mixed\nevidence suggests that OZs may have increased residential permitting.\n"
    },
    {
        "paper_id": 2204.07115,
        "authors": "Christa Cuchiero, Guido Gazzani, Irene Klein",
        "title": "Risk measures under model uncertainty: a Bayesian viewpoint",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  We introduce two kinds of risk measures with respect to some reference\nprobability measure, which both allow for a certain order structure and\ndomination property. Analyzing their relation to each other leads to the\nquestion when a certain minimax inequality is actually an equality. We then\nprovide conditions under which the corresponding robust risk measures, being\ndefined as the supremum over all risk measures induced by a set of probability\nmeasures, can be represented classically in terms of one single probability\nmeasure. We focus in particular on the mixture probability measure obtained via\nmixing over a set of probability measures using some prior, which represents\nfor instance the regulator's beliefs. The classical representation in terms of\nthe mixture probability measure can then be interpreted as a Bayesian approach\nto robust risk measures.\n"
    },
    {
        "paper_id": 2204.07134,
        "authors": "Alessio Brini, Gabriele Tedeschi, Daniele Tantari",
        "title": "Reinforcement Learning Policy Recommendation for Interbank Network\n  Stability",
        "comments": "63 pages, 24 figures. Submitted the revised version that is going to\n  be published on the Journal of Financial Stability",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper, we analyze the effect of a policy recommendation on the\nperformance of an artificial interbank market. Financial institutions stipulate\nlending agreements following a public recommendation and their individual\ninformation. The former is modeled by a reinforcement learning optimal policy\nthat maximizes the system's fitness and gathers information on the economic\nenvironment. The policy recommendation directs economic actors to create credit\nrelationships through the optimal choice between a low interest rate or a high\nliquidity supply. The latter, based on the agents' balance sheet, allows\ndetermining the liquidity supply and interest rate that the banks optimally\noffer their clients within the market. Thanks to the combination between the\npublic and the private signal, financial institutions create or cut their\ncredit connections over time via a preferential attachment evolving procedure\nable to generate a dynamic network. Our results show that the emergence of a\ncore-periphery interbank network, combined with a certain level of homogeneity\nin the size of lenders and borrowers, is essential to ensure the system's\nresilience. Moreover, the optimal policy recommendation obtained through\nreinforcement learning is crucial in mitigating systemic risk.\n"
    },
    {
        "paper_id": 2204.07506,
        "authors": "Victor Olkhov",
        "title": "Price and Payoff Autocorrelations in a Multi-Period Consumption-Based\n  Asset Pricing Model",
        "comments": "10 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper highlights the hidden dependence of the basic pricing equation of\na multi-period consumption-based asset pricing model on price and payoff\nautocorrelations. We obtain the approximations of the basic pricing equation\nthat describe the mean price \"to-day,\" mean payoff \"next-day,\" price and payoff\nvolatilities, and price and payoff autocorrelations. The deep conjunction of\nthe consumption-based model with other versions of asset pricing, such as\nICAPM, APM, etc. (Cochrane, 2001), emphasizes that our results are valid for\nother pricing models.\n"
    },
    {
        "paper_id": 2204.07888,
        "authors": "Eiji Yamamura, Ryohei Hayashi",
        "title": "AI, Ageing and Brain-Work Productivity: Technological Change in\n  Professional Japanese Chess",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Using Japanese professional chess (Shogi) players records in the novel\nsetting, this paper examines how and the extent to which the emergence of\ntechnological changes influences the ageing and innate ability of players\nwinning probability. We gathered games of professional Shogi players from 1968\nto 2019.\n  The major findings are: (1) diffusion of artificial intelligence (AI) reduces\ninnate ability, which reduces the performance gap among same-age players; (2)\nplayers winning rates declined consistently from 20 years and as they get\nolder; (3) AI accelerated the ageing declination of the probability of winning,\nwhich increased the performance gap among different aged players; (4) the\neffects of AI on the ageing declination and the probability of winning are\nobserved for high innate skill players but not for low innate skill ones. This\nimplies that the diffusion of AI hastens players retirement from active play,\nespecially for those with high innate abilities. Thus, AI is a substitute for\ninnate ability in brain-work productivity.\n"
    },
    {
        "paper_id": 2204.07891,
        "authors": "Eiji Yamamura",
        "title": "Bad Weather, Social Network, and Internal Migration; Case of Japanese\n  Sumo Wrestlers 1946-1985",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Post-World War II , there was massive internal migration from rural to urban\nareas in Japan. The location of Sumo stables was concentrated in Tokyo. Hence,\nsupply of Sumo wrestlers from rural areas to Tokyo was considered as migration.\nUsing a panel dataset covering forty years, specifically 1946-1985, this study\ninvestigates how weather conditions and social networks influenced the labor\nsupply of Sumo wrestlers. Major findings are; (1) inclemency of the weather in\nlocal areas increased supply of Sumo wrestlers in the period 1946-1965, (2) the\neffect of the bad weather conditions is greater in the locality where large\nnumber of Sumo wrestlers were supplied in the pre-war period, (3) neither the\noccurrence of bad weather conditions nor their interactions with sumo-wrestlers\ninfluenced the supply of Sumo wrestlers in the period 1966-1985. These findings\nimply that the negative shock of bad weather conditions on agriculture in the\nrural areas incentivized young individuals to be apprenticed in Sumo stables in\nTokyo. Additionally, in such situations, the social networks within Sumo\nwrestler communities from the same locality are important. However, once the\nshare of workers in agricultural sectors became very low, this mechanism did\nnot work.\n"
    },
    {
        "paper_id": 2204.07914,
        "authors": "Takuji Arai and Masahiko Takenaka",
        "title": "Constrained optimal stopping under a regime-switching model",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  We investigate an optimal stopping problem for the expected value of a\ndiscounted payoff on a regime-switching geometric Brownian motion under two\nconstraints on the possible stopping times: only at exogenous random times and\nonly during a specific regime. The main objectives are to show that an optimal\nstopping time exists as a threshold type under some boundary conditions and to\nderive expressions of the value functions and the optimal threshold. To this\nend, we solve the corresponding variational inequality and show that its\nsolution coincides with the value functions. Some numerical results are also\nintroduced. Furthermore, we investigate some asymptotic behaviors.\n"
    },
    {
        "paper_id": 2204.07989,
        "authors": "M.V. Pomazanov",
        "title": "Second-order accuracy metrics for scoring models and their practical use",
        "comments": "14 pages, 7 figure",
        "journal-ref": "Procedia Computer Science, Volume 214, 2022, Pages 565-572, ISSN\n  1877-0509",
        "doi": "10.1016/j.procs.2022.11.213",
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  The paper proposes new second-order accuracy metrics for scoring or rating\nmodels, which show the target preference of the model, it is better to diagnose\ngood objects or better to diagnose bad ones for a constant generally accepted\npredictive power determined by the first order metric that is known as the Gini\nindex. There are two metrics, they have both an integral representation and a\nnumerical one. The numerical representation of metrics is of two types, the\nfirst of which is based on binary events to evaluate the model, the second on\nthe default probability given by the model. Comparison of the results of\ncalculating the metrics allows you to validate the calibration settings of the\nscoring or rating model and reveals its distortions. The article provides\nexamples of calculating second-order accuracy metrics for ratings of several\nrating agencies, as well as for the well known approach to calibration based on\nvan der Burg's ROC curves.\n"
    },
    {
        "paper_id": 2204.08289,
        "authors": "Souhir Ben Amor, Heni Boubaker, Lotfi Belkacem",
        "title": "A Dual Generalized Long Memory Modelling for Forecasting Electricity\n  Spot Price: Neural Network and Wavelet Estimate",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  In this paper, dual generalized long memory modelling has been proposed to\npredict the electricity spot price. First, we focus on modelling the\nconditional mean of the series so we adopt a generalized fractional k-factor\nGegenbauer process ( k-factor GARMA). Secondly, the residual from the k-factor\nGARMA model has been used as a proxy for the conditional variance; these\nresiduals were predicted using two different approaches. In the first approach,\na local linear wavelet neural network model (LLWNN) has developed to predict\nthe conditional variance using two different learning algorithms, so we\nestimate the hybrid k- factor GARMA-LLWNN based backpropagation (BP) algorithm\nand based particle swarm optimization (PSO) algorithm. In the second approach,\nthe Gegenbauer generalized autoregressive conditional heteroscedasticity\nprocess (G-GARCH) has been adopted, and the parameters of the k-factor GARMAG-\nGARCH model have been estimated using the wavelet methodology based on the\ndiscrete wavelet packet transform (DWPT) approach. To illustrate the usefulness\nof our methodology, we carry out an empirical application using the hourly\nreturns of electricity prices from the Nord Pool market. The empirical results\nhave shown that the k-factor GARMA-G-GARCH model has the best prediction\naccuracy in terms of forecasting criteria, and find that this is more\nappropriate for forecasts.\n"
    },
    {
        "paper_id": 2204.0834,
        "authors": "Xiangtai Zuo",
        "title": "Research on the accumulation effect model of technological innovation in\n  textile industry based on chaos theory",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Technological innovation is one of the most important variables in the\nevolution of the textile industry system. As the innovation process changes, so\ndoes the degree of technological diffusion and the state of competitive\nequilibrium in the textile industry system, and this leads to fluctuations in\nthe economic growth of the industry system. The fluctuations resulting from the\nrole of innovation are complex, irregular and imperfectly cyclical. The study\nof the chaos model of the accumulation of innovation in the evolution of the\ntextile industry can help to provide theoretical guidance for technological\ninnovation in the textile industry, and can help to provide suggestions for the\ninteraction between the government and the textile enterprises themselves. It\nis found that reasonable government regulation parameters contribute to the\naccelerated accumulation of innovation in the textile industry.\n"
    },
    {
        "paper_id": 2204.08581,
        "authors": "Tao Chen and Mike Ludkovski and Moritz Vo{\\ss}",
        "title": "On Parametric Optimal Execution and Machine Learning Surrogates",
        "comments": "33 pages, 8 figures. Github repo at\n  https://github.com/moritz-voss/Parametric_Optimal_Execution_ML",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We investigate optimal order execution problems in discrete time with\ninstantaneous price impact and stochastic resilience. First, in the setting of\nlinear transient price impact we derive a closed-form recursion for the optimal\nstrategy, extending the deterministic results from Obizhaeva and Wang (J\nFinancial Markets, 2013). Second, we develop a numerical algorithm based on\ndynamic programming and deep learning for the case of nonlinear transient price\nimpact as proposed by Bouchaud et al. (Quant. Finance, 2004). Specifically, we\nutilize an actor-critic framework that constructs two neural-network (NN)\nsurrogates for the value function and the feedback control. The flexible\nscalability of NN functional approximators enables parametric learning, i.e.,\nincorporating several model or market parameters as part of the input space.\nPrecise calibration of price impact, resilience, etc., is known to be extremely\nchallenging and hence it is critical to understand sensitivity of the execution\npolicy to these parameters. Our NN learner organically scales across multiple\ninput dimensions and is shown to accurately approximate optimal strategies\nacross a wide range of parameter configurations. We provide a fully\nreproducible Jupyter Notebook with our NN implementation, which is of\nindependent pedagogical interest, demonstrating the ease of use of NN\nsurrogates in (parametric) stochastic control problems.\n"
    },
    {
        "paper_id": 2204.08876,
        "authors": "Hanzhe Li",
        "title": "Transparency and Policymaking with Endogenous Information Provision",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  How does the politician's reputation concern affect information provision\nwhen the information is endogenously provided by a biased lobbyist? I develop a\nmodel to study this problem and show that the answer depends on the\ntransparency design. When the lobbyist's preference is publicly known, the\npolitician's reputation concern induces the lobbyist to provide more\ninformation. When the lobbyist's preference is unknown, the politician's\nreputation concern may induce the lobbyist to provide less information. One\nimplication of the result is that given transparent preferences, the\ntransparency of decision consequences can impede information provision by\nmoderating the politician's reputational incentive.\n"
    },
    {
        "paper_id": 2204.08882,
        "authors": "Hirbod Assa and Liyuan Lin and Ruodu Wang",
        "title": "Calibrating distribution models from PELVE",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The Value-at-Risk (VaR) and the Expected Shortfall (ES) are the two most\npopular risk measures in banking and insurance regulation. To bridge between\nthe two regulatory risk measures, the Probability Equivalent Level of VaR-ES\n(PELVE) was recently proposed to convert a level of VaR to that of ES. It is\nstraightforward to compute the value of PELVE for a given distribution model.\nIn this paper, we study the converse problem of PELVE calibration, that is, to\nfind a distribution model that yields a given PELVE, which may either be\nobtained from data or from expert opinion. We discuss separately the cases when\none-point, two-point, n-point and curve constraints are given. In the most\ncomplicated case of a curve constraint, we convert the calibration problem to\nthat of an advanced differential equation. We apply the model calibration\ntechniques to estimation and simulation for datasets used in insurance. We\nfurther study some technical properties of PELVE by offering a few new results\non monotonicity and convergence.\n"
    },
    {
        "paper_id": 2204.0945,
        "authors": "Pedro Forquesato",
        "title": "Who Benefits from Political Connections in Brazilian Municipalities",
        "comments": "23 pages, 9 pages appendix, 3 figures (main text), 3 tables (main\n  text)",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  A main issue in improving public sector efficiency is to understand to what\nextent public appointments are based on worker capability, instead of being\nused to reward political supporters (patronage). I contribute to a recent\nliterature documenting patronage in public sector employment by establishing\nwhat type of workers benefit the most from political connections. Under the\n(empirically supported) assumption that in close elections the result of the\nelection is as good as random, I estimate a causal forest to identify\nheterogeneity in the conditional average treatment effect of being affiliated\nto the party of the winning mayor. Contrary to previous literature, for most\npositions we find positive selection on education, but a negative selection on\n(estimated) ability. Overall, unemployed workers or low tenure employees that\nare newly affiliated to the winning candidate's party benefit the most from\npolitical connections, suggesting that those are used for patronage.\n"
    },
    {
        "paper_id": 2204.09544,
        "authors": "Ji Liu, Zheng Xu, Yanmei Zhang, Wei Dai, Hao Wu, and Shiping Chen",
        "title": "Digging into Primary Financial Market: Challenges and Opportunities of\n  Adopting Blockchain",
        "comments": "11 pages and 7 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Since the emergence of blockchain technology, its application in the\nfinancial market has always been an area of focus and exploration by all\nparties. With the characteristics of anonymity, trust, tamper-proof, etc.,\nblockchain technology can effectively solve some problems faced by the\nfinancial market, such as trust issues and information asymmetry issues. To\ndeeply understand the application scenarios of blockchain in the financial\nmarket, the issue of securities issuance and trading in the primary market is a\nproblem that must be studied clearly. We conducted an empirical study to\ninvestigate the main difficulties faced by primary market participants in their\nbusiness practices and the potential challenges of the deepening application of\nblockchain technology in the primary market. We adopted a hybrid method\ncombining interviews (qualitative methods) and surveys (quantitative methods)\nto conduct this research in two stages. In the first stage, we interview 15\nmajor primary market participants with different backgrounds and expertise. In\nthe second phase, we conducted a verification survey of 54 primary market\npractitioners to confirm various insights from the interviews, including\nchallenges and desired improvements. Our interviews and survey results revealed\nseveral significant challenges facing blockchain applications in the primary\nmarket: complex due diligence, mismatch, and difficult monitoring. On this\nbasis, we believe that our future research can focus on some aspects of these\nchallenges.\n"
    },
    {
        "paper_id": 2204.09568,
        "authors": "Souhir Ben Amor, Heni Boubaker, Lotfi Belkacem",
        "title": "Predictive Accuracy of a Hybrid Generalized Long Memory Model for Short\n  Term Electricity Price Forecasting",
        "comments": "arXiv admin note: text overlap with arXiv:2204.08289",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Accurate electricity price forecasting is the main management goal for market\nparticipants since it represents the fundamental basis to maximize the profits\nfor market players. However, electricity is a non-storable commodity and the\nelectricity prices are affected by some social and natural factors that make\nthe price forecasting a challenging task. This study investigates the\npredictive performance of a new hybrid model based on the Generalized long\nmemory autoregressive model (k-factor GARMA), the Gegenbauer Generalized\nAutoregressive Conditional Heteroscedasticity(G-GARCH) process, Wavelet\ndecomposition, and Local Linear Wavelet Neural Network (LLWNN) optimized using\ntwo different learning algorithms; the Backpropagation algorithm (BP) and the\nParticle Swarm optimization algorithm (PSO). The performance of the proposed\nmodel is evaluated using data from Nord Pool Electricity markets. Moreover, it\nis compared with some other parametric and non-parametric models in order to\nprove its robustness. The empirical results prove that the proposed method\nperforms well than other competing techniques.\n"
    },
    {
        "paper_id": 2204.10026,
        "authors": "Franco D. Albareti, Thomas Ankenbrand, Denis Bieri, Esther H\\\"anggi,\n  Damian L\\\"otscher, Stefan Stettler, Marcel Sch\\\"ongens",
        "title": "A Structured Survey of Quantum Computing for the Financial Industry",
        "comments": "IEEE format, 19 pages, 7 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Quantum computers can solve specific problems that are not feasible on\n\"classical\" hardware. Harvesting the speed-up provided by quantum computers\ntherefore has the potential to change any industry which uses computation,\nincluding finance. First quantum applications for the financial industry\ninvolving optimization, simulation, and machine learning problems have already\nbeen proposed and applied to use cases such as portfolio management, risk\nmanagement, and pricing derivatives. This survey reviews platforms, algorithms,\nmethodologies, and use cases of quantum computing for various applications in\nfinance in a structured way. It is aimed at people working in the financial\nindustry and serves to gain an overview of the current development and\ncapabilities and understand the potential of quantum computing in the financial\nindustry.\n"
    },
    {
        "paper_id": 2204.10103,
        "authors": "Giacomo Giorgio, Barbara Pacchiarotti and Paolo Pigato",
        "title": "Short-time asymptotics for non self-similar stochastic volatility models",
        "comments": "25 pages, 4 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We provide a short-time large deviation principle (LDP) for stochastic\nvolatility models, where the volatility is expressed as a function of a\nVolterra process. This LDP does not require strict self-similarity assumptions\non the Volterra process. For this reason, we are able to apply such an LDP to\ntwo notable examples of non self-similar rough volatility models: models where\nthe volatility is given as a function of a log-modulated fractional Brownian\nmotion [Bayer et al., Log-modulated rough stochastic volatility models. SIAM J.\nFinanc. Math, 2021, 12(3), 1257-1284], and models where it is given as a\nfunction of a fractional Ornstein-Uhlenbeck (fOU) process [Gatheral et al.,\nVolatility is rough. Quant. Finance, 2018, 18(6), 933-949]. In both cases we\nderive consequences for short-maturity European option prices, implied\nvolatility surfaces and implied volatility skew. In the fOU case we also\ndiscuss moderate deviations pricing and simulation results.\n"
    },
    {
        "paper_id": 2204.10177,
        "authors": "Rudy Morel, Gaspar Rochette, Roberto Leonarduzzi, Jean-Philippe\n  Bouchaud, St\\'ephane Mallat",
        "title": "Scale Dependencies and Self-Similar Models with Wavelet Scattering\n  Spectra",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  We introduce the wavelet scattering spectra which provide non-Gaussian models\nof time-series having stationary increments. A complex wavelet transform\ncomputes signal variations at each scale. Dependencies across scales are\ncaptured by the joint correlation across time and scales of wavelet\ncoefficients and their modulus. This correlation matrix is nearly diagonalized\nby a second wavelet transform, which defines the scattering spectra. We show\nthat this vector of moments characterizes a wide range of non-Gaussian\nproperties of multi-scale processes. We prove that self-similar processes have\nscattering spectra which are scale invariant. This property can be tested\nstatistically on a single realization and defines a class of wide-sense\nself-similar processes. We build maximum entropy models conditioned by\nscattering spectra coefficients, and generate new time-series with a\nmicrocanonical sampling algorithm. Applications are shown for highly\nnon-Gaussian financial and turbulence time-series.\n"
    },
    {
        "paper_id": 2204.10243,
        "authors": "Amin Mekacher, Alberto Bracci, Matthieu Nadini, Mauro Martino, Laura\n  Alessandretti, Luca Maria Aiello, Andrea Baronchelli",
        "title": "Heterogeneous rarity patterns drive price dynamics in NFT collections",
        "comments": null,
        "journal-ref": "Scientific reports, Volume 12, Issue 1, August 2022",
        "doi": "10.1038/s41598-022-17922-5",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We quantify Non Fungible Token (NFT) rarity and investigate how it impacts\nmarket behaviour by analysing a dataset of 3.7M transactions collected between\nJanuary 2018 and June 2022, involving 1.4M NFTs distributed across 410\ncollections. First, we consider the rarity of an NFT based on the set of\nhuman-readable attributes it possesses and show that most collections present\nheterogeneous rarity patterns, with few rare NFTs and a large number of more\ncommon ones. Then, we analyze market performance and show that, on average,\nrarer NFTs: (i) sell for higher prices, (ii) are traded less frequently, (iii)\nguarantee higher returns on investment (ROIs), and (iv) are less risky, i.e.,\nless prone to yield negative returns. We anticipate that these findings will be\nof interest to researchers as well as NFT creators, collectors, and traders.\n"
    },
    {
        "paper_id": 2204.10275,
        "authors": "Andrew Y. Chen",
        "title": "Do t-Statistic Hurdles Need to be Raised?",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Many scholars have called for raising statistical hurdles to guard against\nfalse discoveries in academic publications. I show these calls may be difficult\nto justify empirically. Published data exhibit bias: results that fail to meet\nexisting hurdles are often unobserved. These unobserved results must be\nextrapolated, which can lead to weak identification of revised hurdles. In\ncontrast, statistics that can target only published findings (e.g. empirical\nBayes shrinkage and the FDR) can be strongly identified, as data on published\nfindings is plentiful. I demonstrate these results theoretically and in an\nempirical analysis of the cross-sectional return predictability literature.\n"
    },
    {
        "paper_id": 2204.10304,
        "authors": "Kerstin H\\\"otte, Taheya Tarannum, Vilhelm Verendel, Lauren Bennett",
        "title": "Measuring artificial intelligence: a systematic assessment and\n  implications for governance",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Governing artificial intelligence (AI) is high on the political agenda, but\nit is still not clear how to define and measure it. We compare four approaches\nto identifying AI patented inventions that reflect different ways of\nunderstanding AI with divergent definitions. Using US patents from 1990-2019,\nwe assess the extent to which each approach qualifies AI as a general purpose\ntechnology (GPT) and study patterns of concentration, which both are criteria\nrelevant for regulation. The four approaches overlap on only 1.37% of patents\nand vary in scale, accounting for shares that range from 3-17% of all US\npatents in 2019. The smallest set of AI patents in our sample, identified by\nthe latest AI keywords, is most GPT-like with high levels of growth and\ngenerality. All four approaches show AI inventions to be concentrated in few\nfirms, confirming worries about competition. Our results suggest that\nregulation may not be straightforward, as the identification of AI inventions\nultimately depends on how AI is defined.\n"
    },
    {
        "paper_id": 2204.10692,
        "authors": "Hossein Nohrouzian and Anatoliy Malyarenko and Ying Ni",
        "title": "Constructing Trinomial Models Based on Cubature Method on Wiener Space:\n  Applications to Pricing Financial Derivatives",
        "comments": "20 pages, 7 figures, 6 tables",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  This contribution deals with an extension to our developed novel cubature\nmethods of degrees 5 on Wiener space. In our previous studies, we have shown\nthat the cubature formula is exact for all multiple Stratonovich integrals up\nto dimension equal to the degree. In fact, cubature method reduces solving a\nstochastic differential equation to solving a finite set of ordinary\ndifferential equations. Now, we apply the above methods to construct trinomial\nmodels and to price different financial derivatives. We will compare our\nnumerical solutions with the Black's and Black--Scholes models' analytical\nsolutions. The constructed model has practical usage in pricing American-style\nderivatives and can be extended to more sophisticated stochastic market models.\n"
    },
    {
        "paper_id": 2204.1082,
        "authors": "Henrika Langen and Martin Huber",
        "title": "How causal machine learning can leverage marketing strategies: Assessing\n  and improving the performance of a coupon campaign",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  We apply causal machine learning algorithms to assess the causal effect of a\nmarketing intervention, namely a coupon campaign, on the sales of a retailer.\nBesides assessing the average impacts of different types of coupons, we also\ninvestigate the heterogeneity of causal effects across different subgroups of\ncustomers, e.g., between clients with relatively high vs. low prior purchases.\nFinally, we use optimal policy learning to determine (in a data-driven way)\nwhich customer groups should be targeted by the coupon campaign in order to\nmaximize the marketing intervention's effectiveness in terms of sales. We find\nthat only two out of the five coupon categories examined, namely coupons\napplicable to the product categories of drugstore items and other food, have a\nstatistically significant positive effect on retailer sales. The assessment of\ngroup average treatment effects reveals substantial differences in the impact\nof coupon provision across customer groups, particularly across customer groups\nas defined by prior purchases at the store, with drugstore coupons being\nparticularly effective among customers with high prior purchases and other food\ncoupons among customers with low prior purchases. Our study provides a use case\nfor the application of causal machine learning in business analytics to\nevaluate the causal impact of specific firm policies (like marketing campaigns)\nfor decision support.\n"
    },
    {
        "paper_id": 2204.10944,
        "authors": "Geoff Boeing, William Riggs",
        "title": "Converting One-Way Streets to Two-Way Streets to Improve Transportation\n  Network Efficiency and Reduce Vehicle Distance Traveled",
        "comments": null,
        "journal-ref": "Journal of Planning Education and Research, 2022",
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Planning scholars have identified economic, safety, and social benefits of\nconverting one-way streets to two-way. Less is known about how conversions\ncould impact vehicular distances traveled - of growing relevance in an era of\nfleet automation, electrification, and ride-hailing. We simulate such a\nconversion in San Francisco, California. We find that its current street\nnetwork's average intra-city trip is about 1.7% longer than it would be with\nall two-way streets, corresponding to 27 million kilometers of annual surplus\ntravel. As transportation technologies evolve, planners must consider different\nfacets of network efficiency to align local policy and street design with\nsustainability and other societal goals.\n"
    },
    {
        "paper_id": 2204.10971,
        "authors": "Yizhe Xu, Tom H. Greene, Adam P. Bress, Brandon K. Bellows, Yue Zhang,\n  Zugui Zhang, Paul Kolm, William S.Weintraub, Andrew S. Moran, Jincheng Shen",
        "title": "An Efficient Approach for Optimizing the Cost-effective Individualized\n  Treatment Rule Using Conditional Random Forest",
        "comments": "Submitted to Statistical Methods in Medical Research",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Evidence from observational studies has become increasingly important for\nsupporting healthcare policy making via cost-effectiveness (CE) analyses.\nSimilar as in comparative effectiveness studies, health economic evaluations\nthat consider subject-level heterogeneity produce individualized treatment\nrules (ITRs) that are often more cost-effective than one-size-fits-all\ntreatment. Thus, it is of great interest to develop statistical tools for\nlearning such a cost-effective ITR (CE-ITR) under the causal inference\nframework that allows proper handling of potential confounding and can be\napplied to both trials and observational studies. In this paper, we use the\nconcept of net-monetary-benefit (NMB) to assess the trade-off between health\nbenefits and related costs. We estimate CE-ITR as a function of patients'\ncharacteristics that, when implemented, optimizes the allocation of limited\nhealthcare resources by maximizing health gains while minimizing\ntreatment-related costs. We employ the conditional random forest approach and\nidentify the optimal CE-ITR using NMB-based classification algorithms, where\ntwo partitioned estimators are proposed for the subject-specific weights to\neffectively incorporate information from censored individuals. We conduct\nsimulation studies to evaluate the performance of our proposals. We apply our\ntop-performing algorithm to the NIH-funded Systolic Blood Pressure Intervention\nTrial (SPRINT) to illustrate the CE gains of assigning customized intensive\nblood pressure therapy.\n"
    },
    {
        "paper_id": 2204.11069,
        "authors": "Ibrahim Ekren and Brad Mostowski and Gordan \\v{Z}itkovi\\'c",
        "title": "Kyle's Model with Stochastic Liquidity",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
        "abstract": "  We construct an equilibrium for the continuous time Kyle's model with\nstochastic liquidity, a general distribution of the fundamental price, and\ncorrelated stock and volatility dynamics. For distributions with positive\nsupport, our equilibrium allows us to study the impact of the stochastic\nvolatility of noise trading on the volatility of the asset. In particular, when\nthe fundamental price is log-normally distributed, informed trading forces the\nlog-return up to maturity to be Gaussian for any choice of noise-trading\nvolatility even though the price process itself comes with stochastic\nvolatility. Surprisingly, we find that in equilibrium both Kyle's Lambda and\nits inverse (the market depth) are submartingales.\n"
    },
    {
        "paper_id": 2204.11088,
        "authors": "Victor Ushahemba Ijirshar",
        "title": "Trade Facilitation and Economic Growth Among Middle-Income Countries",
        "comments": null,
        "journal-ref": "Journal of Economic and Social Research, 9(1), 1-40. 2022",
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  This study examined the relationship between trade facilitation and economic\ngrowth among the middle-income countries from 2010 to 2020 using 94 countries\nmade up of 48 lower-middle-income countries and 46 upper-middle-income\ncountries. The study utilized both difference and system Generalised Method of\nMoments (GMM) since the cross-sections (N) were greater than the periods (T).\nThe study found that container port traffic, quality of trade and\ntransport-related infrastructure have a strong influence on imports and exports\nof goods and national income while trade tariff hurts the growth of the\ncountries. The study also found that most of the trade facilitation indicators\nindicated a weak positive influence on trade flows and economic growth. Based\non these findings, the study recommends that reforms aimed at significantly\nlowering the costs of trading across borders among middle-income countries\nshould be highly prioritized in policy formulations, with a focus on the export\nside by reducing at-the-border documentation, time, and real costs of trading\nacross borders while the international organizations should continue to report\nthe set of Trade Facilitation Indicators (TFIs) that identify areas for action\nand enable the potential impact of reforms to be assessed.\n"
    },
    {
        "paper_id": 2204.11107,
        "authors": "Michael Darlin, Georgios Palaiokrassas, Leandros Tassiulas",
        "title": "Debt-Financed Collateral and Stability Risks in the DeFi Ecosystem",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The rise of Decentralized Finance (\"DeFi\") on the Ethereum blockchain has\nenabled the creation of lending platforms, which serve as marketplaces to lend\nand borrow digital currencies. We first categorize the activity of lending\nplatforms within a standard regulatory framework. We then employ a novel\ngrouping and classification algorithm to calculate the percentage of fund flows\ninto DeFi lending platforms that can be attributed to debt created elsewhere in\nthe system (\"debt-financed collateral\"). Based on our results, we conclude that\nthe wide-spread use of stablecoins as debt-financed collateral increases\nfinancial stability risks in the DeFi ecosystem.\n"
    },
    {
        "paper_id": 2204.11203,
        "authors": "Reza Hosseini, Samin Tajik, Zahra Koohi Lai, Tayeb Jamali, Emmanuel\n  Haven, G. Reza Jafari",
        "title": "Quantum Bohmian Inspired Potential to Model Non-Gaussian Events and the\n  Application in Financial Markets",
        "comments": "7 pages, 2 figures",
        "journal-ref": null,
        "doi": "10.3390/e25071061",
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  We have implemented quantum modeling mainly based on Bohmian Mechanics to\nstudy time series that contain strong coupling between their events. We firstly\npropose how compared to normal densities, our target time series seem to be\nassociated with a higher number of rare events, and Gaussian statistics tend to\nunderestimate these events' frequency drastically. To this end, we suggest that\nby imposing Gaussian densities to the natural processes, one will seriously\nneglect the existence of extreme events in many circumstances. The central\nquestion of our study concerns the consideration of the effects of these rare\nevents in the corresponding probability densities and studying their role from\nthe point of view of quantum measurements. To model the non-Gaussian behavior\nof these time-series, we utilize the multifractal random walk (MRW) approach\nand control the non-Gaussianity parameter $\\lambda$ accordingly. Using the\nframework of quantum mechanics, we then examine the role of $\\lambda$ in\nquantum potentials derived for these time series. Our Bohmian quantum analysis\nshows that the derived potential takes some negative values in high frequencies\n(its mean values), then substantially increases, and the value drops again for\nthe rare events. We thus conclude that these events could generate a potential\nbarrier that the system, lingering in a non-Gaussian high-frequency region,\nencounters, and their role becomes more prominent when it comes to transversing\nthis barrier. In this study, as an example of the application of quantum\npotential outside of the micro-world, we compute the quantum potentials for the\nS\\&P financial market time series to verify the presence of rare events in the\nnon-Gaussian densities for this real data and remark the deviation from the\nGaussian case.\n"
    },
    {
        "paper_id": 2204.11438,
        "authors": "Takaaki Koike and Liyuan Lin and Ruodu Wang",
        "title": "Joint mixability and notions of negative dependence",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  A joint mix is a random vector with a constant component-wise sum. The\ndependence structure of a joint mix minimizes some common objectives such as\nthe variance of the component-wise sum, and it is regarded as a concept of\nextremal negative dependence. In this paper, we explore the connection between\nthe joint mix structure and popular notions of negative dependence in\nstatistics, such as negative correlation dependence, negative orthant\ndependence and negative association. A joint mix is not always negatively\ndependent in any of the above senses, but some natural classes of joint mixes\nare. We derive various necessary and sufficient conditions for a joint mix to\nbe negatively dependent, and study the compatibility of these notions. For\nidentical marginal distributions, we show that a negatively dependent joint mix\nsolves a multi-marginal optimal transport problem for quadratic cost under a\nnovel setting of uncertainty. Analysis of this optimal transport problem with\nheterogeneous marginals reveals a trade-off between negative dependence and the\njoint mix structure.\n"
    },
    {
        "paper_id": 2204.11451,
        "authors": "Tien Mai and Arunesh Sinha",
        "title": "Safe Delivery of Critical Services in Areas with Volatile Security\n  Situation via a Stackelberg Game Approach",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
        "abstract": "  Vaccine delivery in under-resourced locations with security risks is not just\nchallenging but also life threatening. The current COVID pandemic and the need\nto vaccinate have added even more urgency to this issue. Motivated by this\nproblem, we propose a general framework to set-up limited temporary\n(vaccination) centers that balance physical security and desired (vaccine)\nservice coverage with limited resources. We set-up the problem as a Stackelberg\ngame between the centers operator (defender) and an adversary, where the set of\ncenters is not fixed a priori but is part of the decision output. This results\nin a mixed combinatorial and continuous optimization problem. As part of our\nscalable approximation of this problem, we provide a fundamental contribution\nby identifying general duality conditions of switching max and min when both\ndiscrete and continuous variables are involved. We perform detailed experiments\nto show that the solution proposed is scalable in practice.\n"
    },
    {
        "paper_id": 2204.11554,
        "authors": "Elisa Al\\`os, Fabio Antonelli, Alessandro Ramponi, Sergio Scarlatti",
        "title": "CVA in fractional and rough volatility models",
        "comments": "29 pages, 7 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  In this work we present a general representation formula for the price of a\nvulnerable European option, and the related CVA in stochastic (either rough or\nnot) volatility models for the underlying's price, when admitting correlation\nwith the default event. We specialize it for some volatility models and we\nprovide price approximations, based on the representation formula. We study\nnumerically their accuracy, comparing the results with Monte Carlo simulations,\nand we run a theoretical study of the error. We also introduce a seminal study\nof roughness influence on the claim's price.\n"
    },
    {
        "paper_id": 2204.11585,
        "authors": "Jiamin Yu",
        "title": "Will claim history become a deprecated rating factor? An optimal design\n  method for the real-time road risk model",
        "comments": "8 pages, 7 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
        "abstract": "  With the popularity of Telematics and Self-driving, more and more rating\nfactors, such as mileage, route, driving behavior, etc., are introduced into\nactuarial models. There are quite a few doubts and disputes on the rationality\nand accuracy of the selection of rating variables, but it does not involve the\nwidely accepted historical claim records. Recently, Tesla Insurance released a\nnew generation of Safety Score-based insurance, irrespective of accident\nhistory. Forward-looking experts and scholars began to discuss whether claim\nhistory will disappear in the future auto insurance rate-making system.\nTherefore, this paper proposes a new risk variable elimination method as well\nas a real-time road risk model design framework and concludes that claim\nhistory will be regarded as a \"noise\" factor and deprecated in the\nPay-How-You-Drive model.\n"
    },
    {
        "paper_id": 2204.11735,
        "authors": "Katarzyna Maciejowska, Bartosz Uniejewski, Rafa{\\l} Weron",
        "title": "Forecasting Electricity Prices",
        "comments": "Forthcoming in the Oxford Research Encyclopedia of Economics and\n  Finance (https://oxfordre.com/economics)",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Forecasting electricity prices is a challenging task and an active area of\nresearch since the 1990s and the deregulation of the traditionally monopolistic\nand government-controlled power sectors. Although it aims at predicting both\nspot and forward prices, the vast majority of research is focused on short-term\nhorizons which exhibit dynamics unlike in any other market. The reason is that\npower system stability calls for a constant balance between production and\nconsumption, while being weather (both demand and supply) and business activity\n(demand only) dependent. The recent market innovations do not help in this\nrespect. The rapid expansion of intermittent renewable energy sources is not\noffset by the costly increase of electricity storage capacities and\nmodernization of the grid infrastructure. On the methodological side, this\nleads to three visible trends in electricity price forecasting research as of\n2022. Firstly, there is a slow, but more noticeable with every year, tendency\nto consider not only point but also probabilistic (interval, density) or even\npath (also called ensemble) forecasts. Secondly, there is a clear shift from\nthe relatively parsimonious econometric (or statistical) models towards more\ncomplex and harder to comprehend, but more versatile and eventually more\naccurate statistical/machine learning approaches. Thirdly, statistical error\nmeasures are nowadays regarded as only the first evaluation step. Since they\nmay not necessarily reflect the economic value of reducing prediction errors,\nmore and more often, they are complemented by case studies comparing profits\nfrom scheduling or trading strategies based on price forecasts obtained from\ndifferent models.\n"
    },
    {
        "paper_id": 2204.11849,
        "authors": "Zheng Zhang, Yingsheng Ji, Jiachen Shen, Xi Zhang, Guangwen Yang",
        "title": "Heterogeneous Information Network based Default Analysis on Banking\n  Micro and Small Enterprise Users",
        "comments": "Corrected typos",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Risk assessment is a substantial problem for financial institutions that has\nbeen extensively studied both for its methodological richness and its various\npractical applications. With the expansion of inclusive finance, recent\nattentions are paid to micro and small-sized enterprises (MSEs). Compared with\nlarge companies, MSEs present a higher exposure rate to default owing to their\ninsecure financial stability. Conventional efforts learn classifiers from\nhistorical data with elaborate feature engineering. However, the main obstacle\nfor MSEs involves severe deficiency in credit-related information, which may\ndegrade the performance of prediction. Besides, financial activities have\ndiverse explicit and implicit relations, which have not been fully exploited\nfor risk judgement in commercial banks. In particular, the observations on real\ndata show that various relationships between company users have additional\npower in financial risk analysis. In this paper, we consider a graph of banking\ndata, and propose a novel HIDAM model for the purpose. Specifically, we attempt\nto incorporate heterogeneous information network with rich attributes on\nmulti-typed nodes and links for modeling the scenario of business banking\nservice. To enhance feature representation of MSEs, we extract interactive\ninformation through meta-paths and fully exploit path information. Furthermore,\nwe devise a hierarchical attention mechanism respectively to learn the\nimportance of contents inside each meta-path and the importance of different\nmetapahs. Experimental results verify that HIDAM outperforms state-of-the-art\ncompetitors on real-world banking data.\n"
    },
    {
        "paper_id": 2204.1225,
        "authors": "Marcel Nutz, Johannes Wiesel, Long Zhao",
        "title": "Martingale Schr\\\"odinger Bridges and Optimal Semistatic Portfolios",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In a two-period financial market where a stock is traded dynamically and\nEuropean options at maturity are traded statically, we study the so-called\nmartingale Schr\\\"odinger bridge Q*; that is, the minimal-entropy martingale\nmeasure among all models calibrated to option prices. This minimization is\nshown to be in duality with an exponential utility maximization over semistatic\nportfolios. Under a technical condition on the physical measure P, we show that\nan optimal portfolio exists and provides an explicit solution for Q*. This\nresult overcomes the remarkable issue of non-closedness of semistatic\nstrategies discovered by Acciaio, Larsson and Schachermayer. Specifically, we\nexhibit a dense subset of calibrated martingale measures with particular\nproperties to show that the portfolio in question has a well-defined and\nintegrable option position.\n"
    },
    {
        "paper_id": 2204.12251,
        "authors": "Marcel Nutz, Johannes Wiesel, Long Zhao",
        "title": "Limits of Semistatic Trading Strategies",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We show that pointwise limits of semistatic trading strategies in discrete\ntime are again semistatic strategies. The analysis is carried out in full\ngenerality for a two-period model, and under a probabilistic condition for\nmulti-period, multi-stock models. Our result contrasts with a counterexample of\nAcciaio, Larsson and Schachermayer, and shows that their observation is due to\na failure of integrability rather than instability of the semistatic form.\nMathematically, our results relate to the decomposability of functions as\nstudied in the context of Schr\\\"odinger bridges.\n"
    },
    {
        "paper_id": 2204.12359,
        "authors": "Elena Derby, Lucas Goodman, Kathleen Mackie, and Jacob Mortenson",
        "title": "Changes in Retirement Savings During the COVID Pandemic",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/publicdomain/zero/1.0/",
        "abstract": "  This paper documents changes in retirement saving patterns at the onset of\nthe COVID-19 pandemic. We construct a large panel of U.S. tax data, including\ntens of millions of person-year observations, and measure retirement savings\ncontributions and withdrawals. We use these data to document several important\nchanges in retirement savings patterns during the pandemic relative to the\nyears preceding the pandemic or the Great Recession. First, unlike during the\nGreat Recession, contributions to retirement savings vehicles did not\nmeaningfully decline. Second, driven by the suspension of required minimum\ndistribution rules, IRA withdrawals substantially declined in 2020 for those\nolder than age 72. Third, potentially driven by partial suspension of the early\nwithdrawal penalty, employer-plan withdrawals increased for those under age 60.\n"
    },
    {
        "paper_id": 2204.12374,
        "authors": "Fan Wang",
        "title": "An empirical equilibrium model of formal and informal credit markets in\n  developing countries",
        "comments": null,
        "journal-ref": "Review of Economic Dynamics, September 17, 2021",
        "doi": "10.1016/j.red.2021.09.001",
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  I develop and estimate a dynamic equilibrium model of risky entrepreneurs'\nborrowing and savings decisions incorporating both formal and local-informal\ncredit markets. Households have access to an exogenous formal credit market and\nto an informal credit market in which the interest rate is endogenously\ndetermined by the local demand and supply of credit. I estimate the model via\nSimulated Maximum Likelihood using Thai village data during an episode of\nformal credit market expansion. My estimates suggest that a 49 percent\nreduction in fixed costs increased the proportion of households borrowing\nformally by 36 percent, and that a doubling of the collateralized borrowing\nlimits lowered informal interest rates by 24 percent. I find that more\nproductive households benefited from the policies that expanded borrowing\naccess, but less productive households lost in terms of welfare due to\ndiminished savings opportunities. Gains are overall smaller than would be\npredicted by models that do not consider the informal credit market.\n"
    },
    {
        "paper_id": 2204.12657,
        "authors": "Xianfei Hui, Baiqing Sun, Hui Jiang, Yan Zhou",
        "title": "Modeling dynamic volatility under uncertain environment with fuzziness\n  and randomness",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The problem related to predicting dynamic volatility in financial market\nplays a crucial role in many contexts. We build a new generalized\nBarndorff-Nielsen and Shephard (BN-S) model suitable for uncertain environment\nwith fuzziness and randomness. This new model considers the delay phenomenon\nbetween price fluctuation and volatility changes, solves the problem of the\nlack of long-range dependence of classic models. Through the experiment of Dow\nJones futures price, we find that compared with the classical model, this\nmethod effectively combines the uncertain environmental characteristics, which\nmakes the prediction of dynamic volatility has more ideal performance.\n"
    },
    {
        "paper_id": 2204.12766,
        "authors": "Theis Bathke and Marcus Christiansen",
        "title": "Two-dimensional forward and backward transition rates",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Forward transition rates were originally introduced with the aim to evaluate\nlife insurance liabilities market-consistently. While this idea turned out to\nhave its limitations, recent literature repurposes forward transition rates as\na tool for avoiding Markov assumptions in the calculation of life insurance\nreserves. While life insurance reserves are some form of conditional\nfirst-order moments, the calculation of conditional second-order moments needs\nan extension of the forward transition rate concept from one dimension to two\ndimensions. Two-dimensional forward transition rates are also needed for the\ncalculation of path-dependent life insurance cash-flows as they occur upon\ncontract modifications. Forward transition rates are designed for doing\nprospective calculations, and by a time-symmetric definition of so-called\nbackward transition rates one can do retrospective calculations.\n"
    },
    {
        "paper_id": 2204.12865,
        "authors": "Lei Dong, Paolo Santi, Yu Liu, Siqi Zheng, Carlo Ratti",
        "title": "The universality in urban commuting across and within cities",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Commuting is a key mechanism that governs the dynamics of cities. Despite its\nimportance, very little is known of the properties and mechanisms underlying\nthis crucial urban process. Here, we capitalize on $\\sim$ 50 million\nindividuals' smartphone data from 234 Chinese cities to show that urban\ncommuting obeys remarkable regularities. These regularities can be generalized\nas two laws: (i) the scale-invariance of the average commuting distance across\ncities, which is a long-awaited validation of Marchetti's constant conjecture,\nand (ii) a universal inverted U-shape of the commuting distance as a function\nof the distance from the city centre within cities, indicating that the city\ncentre's attraction is bounded. Motivated by such empirical findings, we\ndevelop a simple urban growth model that connects individual-level mobility\nchoices with macroscopic urban spatial structure and faithfully explains both\ncommuting laws. Our results further show that the scale-invariants of human\nmobility will ultimately lead to the polycentric transition in cities, which\ncould be used to better inform urban development strategies.\n"
    },
    {
        "paper_id": 2204.12914,
        "authors": "Linwei Li, Paul-Amaury Matt, Christian Heumann",
        "title": "Forecasting foreign exchange rates with regression networks tuned by\n  Bayesian optimization",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The article is concerned with the problem of multi-step financial time series\nforecasting of Foreign Exchange (FX) rates. To address this problem, we\nintroduce a regression network termed RegPred Net. The exchange rate to\nforecast is treated as a stochastic process. It is assumed to follow a\ngeneralization of Brownian motion and the mean-reverting process referred to as\nthe generalized Ornstein-Uhlenbeck (OU) process, with time-dependent\ncoefficients. Using past observed values of the input time series, these\ncoefficients can be regressed online by the cells of the first half of the\nnetwork (Reg). The regressed coefficients depend only on - but are very\nsensitive to - a small number of hyperparameters required to be set by a global\noptimization procedure for which, Bayesian optimization is an adequate\nheuristic. Thanks to its multi-layered architecture, the second half of the\nregression network (Pred) can project time-dependent values for the OU process\ncoefficients and generate realistic trajectories of the time series.\nPredictions can be easily derived in the form of expected values estimated by\naveraging values obtained by Monte Carlo simulation. The forecasting accuracy\non a 100 days horizon is evaluated for several of the most important FX rates\nsuch as EUR/USD, EUR/CNY, and EUR/GBP. Our experimental results show that the\nRegPred Net significantly outperforms ARMA, ARIMA, LSTMs, and Autoencoder-LSTM\nmodels in terms of metrics measuring the absolute error (RMSE) and correlation\nbetween predicted and actual values (Pearson R, R-squared, MDA). Compared to\nblack-box deep learning models such as LSTM, RegPred Net has better\ninterpretability, simpler structure, and fewer parameters.\n"
    },
    {
        "paper_id": 2204.12928,
        "authors": "Anton Kolonin, Ali Raheman, Mukul Vishwas, Ikram Ansari, Juan Pinzon,\n  Alice Ho",
        "title": "Causal Analysis of Generic Time Series Data Applied for Market\n  Prediction",
        "comments": "10 pages, 4 figures, submitted to Artificial General Intelligence\n  2022 conference",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We explore the applicability of the causal analysis based on temporally\nshifted (lagged) Pearson correlation applied to diverse time series of\ndifferent natures in context of the problem of financial market prediction.\nTheoretical discussion is followed by description of the practical approach for\nspecific environment of time series data with diverse nature and sparsity, as\napplied for environments of financial markets. The data involves various\nfinancial metrics computable from raw market data such as real-time trades and\nsnapshots of the limit order book as well as metrics determined upon social\nmedia news streams such as sentiment and different cognitive distortions. The\napproach is backed up with presentation of algorithmic framework for data\nacquisition and analysis, concluded with experimental results, and summary\npointing out at the possibility to discriminate causal connections between\ndifferent sorts of real field market data with further discussion on present\nissues and possible directions of the following work.\n"
    },
    {
        "paper_id": 2204.12929,
        "authors": "Sihao Hu, Zhen Zhang, Shengliang Lu, Bingsheng He, Zhao Li",
        "title": "Sequence-Based Target Coin Prediction for Cryptocurrency Pump-and-Dump",
        "comments": "SIGMOD conference 2023",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  With the proliferation of pump-and-dump schemes (P&Ds) in the cryptocurrency\nmarket, it becomes imperative to detect such fraudulent activities in advance\nto alert potentially susceptible investors. In this paper, we focus on\npredicting the pump probability of all coins listed in the target exchange\nbefore a scheduled pump time, which we refer to as the target coin prediction\ntask. Firstly, we conduct a comprehensive study of the latest 709 P&D events\norganized in Telegram from Jan. 2019 to Jan. 2022. Our empirical analysis\nreveals some interesting patterns of P&Ds, such as that pumped coins exhibit\nintra-channel homogeneity and inter-channel heterogeneity. Here channel refers\na form of group in Telegram that is frequently used to coordinate P&D events.\nThis observation inspires us to develop a novel sequence-based neural network,\ndubbed SNN, which encodes a channel's P&D event history into a sequence\nrepresentation via the positional attention mechanism to enhance the prediction\naccuracy. Positional attention helps to extract useful information and\nalleviates noise, especially when the sequence length is long. Extensive\nexperiments verify the effectiveness and generalizability of proposed methods.\nAdditionally, we release the code and P&D dataset on GitHub:\nhttps://github.com/Bayi-Hu/Pump-and-Dump-Detection-on-Cryptocurrency, and\nregularly update the dataset.\n"
    },
    {
        "paper_id": 2204.12932,
        "authors": "Shrey Jain, Camille Bruckmann, Chase McDougall",
        "title": "NFT Appraisal Prediction: Utilizing Search Trends, Public Market Data,\n  Linear Regression and Recurrent Neural Networks",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  In this paper we investigate the correlation between NFT valuations and\nvarious features from three primary categories: public market data, NFT\nmetadata, and social trends data.\n"
    },
    {
        "paper_id": 2204.12933,
        "authors": "Huiling Yuan and Guodong Li and Junhui Wang",
        "title": "High-Frequency-Based Volatility Model with Network Structure",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  This paper introduces one new multivariate volatility model that can\naccommodate an appropriately defined network structure based on low-frequency\nand high-frequency data. The model reduces the number of unknown parameters and\nthe computational complexity substantially. The model parameterization and\niterative multistep-ahead forecasts are discussed and the targeting\nreparameterization is also presented. Quasi-likelihood functions for parameter\nestimation are proposed and their asymptotic properties are established. A\nseries of simulation experiments are carried out to assess the performance of\nthe estimation in finite samples. An empirical example is demonstrated that the\nproposed model outperforms the network GARCH model, with the gains being\nparticularly significant at short forecast horizons.\n"
    },
    {
        "paper_id": 2204.13102,
        "authors": "John E. Marthinsen and Steven R. Gordon",
        "title": "The Price and Cost of Bitcoin",
        "comments": "18 pages, 3 tables, 5 figures Accepted by the Quarterly Review of\n  Economics and Finance",
        "journal-ref": null,
        "doi": "10.1016/j.qref.2022.04.003",
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  Explaining changes in bitcoin's price and predicting its future have been the\nfoci of many research studies. In contrast, far less attention has been paid to\nthe relationship between bitcoin's mining costs and its price. One popular\nnotion is the cost of bitcoin creation provides a support level below which\nthis cryptocurrency's price should never fall because if it did, mining would\nbecome unprofitable and threaten the maintenance of bitcoin's public ledger.\nOther research has used mining costs to explain or forecast bitcoin's price\nmovements. Competing econometric analyses have debunked this idea, showing that\nchanges in mining costs follow changes in bitcoin's price rather than preceding\nthem, but the reason for this behavior remains unexplained in these analyses.\nThis research aims to employ economic theory to explain why econometric studies\nhave failed to predict bitcoin prices and why mining costs follow movements in\nbitcoin prices rather than precede them. We do so by explaining the chain of\ncausality connecting a bitcoin's price to its mining costs.\n"
    },
    {
        "paper_id": 2204.13265,
        "authors": "Ali Raheman, Anton Kolonin, Alexey Glushchenko, Arseniy Fokin, Ikram\n  Ansari",
        "title": "Adaptive Multi-Strategy Market-Making Agent For Volatile Markets",
        "comments": "10 pages, 1 figure, submitted to conference - Artificial General\n  Intelligence 2022",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Crypto-currency market uncertainty drives the need to find adaptive solutions\nto maximise gain or at least to avoid loss throughout the periods of trading\nactivity. Given the high dimensionality and complexity of the state-action\nspace in this domain, it can be treated as a \"Narrow AGI\" problem with the\nscope of goals and environments bound to financial markets. Adaptive\nMulti-Strategy Agent approach for market-making introduces a new solution to\nmaximise positive \"alpha\" in long-term handling limit order book (LOB)\npositions by using multiple sub-agents implementing different strategies with a\ndynamic selection of these agents based on changing market conditions. AMSA\nprovides no specific strategy of its own while being responsible for segmenting\nthe periods of market-making activity into smaller execution sub-periods,\nperforming internal backtesting on historical data on each of the sub-periods,\ndoing sub- agent performance evaluation and re-selection of them at the end of\neach sub- period, and collecting returns and losses incrementally. With this\napproach, the return becomes a function of hyper-parameters such as market data\ngranularity (refresh rate), the execution sub-period duration, number of active\nsub-agents, and their individual strategies. Sub-agent selection for the next\ntrading sub-period is made based on return/loss and alpha values obtained\nduring internal backtesting as well as real trading. Experiments with the AMSA\nhave been performed under different market conditions relying on historical\ndata and proved a high probability of positive alpha throughout the periods of\ntrading activity in the case of properly selected hyper-parameters.\n"
    },
    {
        "paper_id": 2204.13338,
        "authors": "Masanori Hirano, Hiroki Sakaji, Kiyoshi Izumi",
        "title": "Policy Gradient Stock GAN for Realistic Discrete Order Data Generation\n  in Financial Markets",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This study proposes a new generative adversarial network (GAN) for generating\nrealistic orders in financial markets. In some previous works, GANs for\nfinancial markets generated fake orders in continuous spaces because of GAN\narchitectures' learning limitations. However, in reality, the orders are\ndiscrete, such as order prices, which has minimum order price unit, or order\ntypes. Thus, we change the generation method to place the generated fake orders\ninto discrete spaces in this study. Because this change disabled the ordinary\nGAN learning algorithm, this study employed a policy gradient, frequently used\nin reinforcement learning, for the learning algorithm. Through our experiments,\nwe show that our proposed model outperforms previous models in generated order\ndistribution. As an additional benefit of introducing the policy gradient, the\nentropy of the generated policy can be used to check GAN's learning status. In\nthe future, higher performance GANs, better evaluation methods, or the\napplications of our GANs can be addressed.\n"
    },
    {
        "paper_id": 2204.13385,
        "authors": "Gour Sundar Mitra Thakur, Rupak Bhattacharyya, Seema Sarkar (Mondal)",
        "title": "Fuzzy Expert System for Stock Portfolio Selection: An Application to\n  Bombay Stock Exchange",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Selection of proper stocks, before allocating investment ratios, is always a\ncrucial task for the investors. Presence of many influencing factors in stock\nperformance have motivated researchers to adopt various Artificial Intelligence\n(AI) techniques to make this challenging task easier. In this paper a novel\nfuzzy expert system model is proposed to evaluate and rank the stocks under\nBombay Stock Exchange (BSE). Dempster-Shafer (DS) evidence theory is used for\nthe first time to automatically generate the consequents of the fuzzy rule base\nto reduce the effort in knowledge base development of the expert system. Later\na portfolio optimization model is constructed where the objective function is\nconsidered as the ratio of the difference of fuzzy portfolio return and the\nrisk free return to the weighted mean semi-variance of the assets that has been\nused. The model is solved by applying Ant Colony Optimization (ACO) algorithm\nby giving preference to the top ranked stocks. The performance of the model\nproved to be satisfactory for short-term investment period when compared with\nthe recent performance of the stocks.\n"
    },
    {
        "paper_id": 2204.13398,
        "authors": "Charles Shaw",
        "title": "Portfolio Diversification Revisited",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  We relax a number of assumptions in Alexeev and Tapon (2012) in order to\naccount for non-normally distributed, skewed, multi-regime, and leptokurtic\nasset return distributions. We calibrate a Markov-modulated Levy process model\nto equity market data to demonstrate the merits of our approach, and show that\nthe calibrated models do a good job of matching the empirical moments. Finally,\nwe argue that much of the related literature on portfolio diversification\nrelies on assumptions that are in tension with certain observable regularities\nand which, if ignored, may lead to underestimation of risk.\n"
    },
    {
        "paper_id": 2204.13481,
        "authors": "Job Boerma, Aleh Tsyvinski, Alexander P. Zimin",
        "title": "Bunching and Taxing Multidimensional Skills",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We characterize optimal policies in a multidimensional nonlinear taxation\nmodel with bunching. We develop an empirically relevant model with cognitive\nand manual skills, firm heterogeneity, and labor market sorting. The analysis\nof optimal policy is based on two main results. We first derive an optimality\ncondition - a general ABC formula - that states that the entire schedule of\nbenefits of taxes second order stochastically dominates the entire schedule of\ntax distortions. Second, we use Legendre transforms to represent our problem as\na linear program. This linearization allows us to solve the model\nquantitatively and to precisely characterize the regions and patterns of\nbunching. At an optimum, 9.8 percent of workers is bunched both locally and\nnonlocally. We introduce two notions of bunching - blunt bunching and targeted\nbunching. Blunt bunching constitutes 30 percent of all bunching, occurs at the\nlowest regions of cognitive and manual skills, and lumps the allocations of\nthese workers resulting in a significant distortion. Targeted bunching\nconstitutes 70 percent of all bunching and recognizes the workers' comparative\nadvantage. The planner separates workers on their dominant skill and bunches\nthem on their weaker skill, thus mitigating distortions along the dominant\nskill dimension. Tax wedges are particularly high for low skilled workers who\nare bluntly bunched and are also high along the dimension of comparative\ndisadvantage for somewhat more skilled workers who are targetedly bunched.\n"
    },
    {
        "paper_id": 2204.13587,
        "authors": "Alexander Brunhuemer, Lukas Larcher, Philipp Seidl, Sascha Desmettre,\n  Johannes Kofler and Gerhard Larcher",
        "title": "Supervised machine learning classification for short straddles on the\n  S&P500",
        "comments": "25 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
        "abstract": "  In this working paper we present our current progress in the training of\nmachine learning models to execute short option strategies on the S&P500. As a\nfirst step, this paper is breaking this problem down to a supervised\nclassification task to decide if a short straddle on the S&P500 should be\nexecuted or not on a daily basis. We describe our used framework and present an\noverview over our evaluation metrics on different classification models. In\nthis preliminary work, using standard machine learning techniques and without\nhyperparameter search, we find no statistically significant outperformance to a\nsimple \"trade always\" strategy, but gain additional insights on how we could\nproceed in further experiments.\n"
    },
    {
        "paper_id": 2204.13664,
        "authors": "Thomas Meissner, Xavier Gassmann, Corinne Faure, Joachim Schleich",
        "title": "Individual characteristics associated with risk and time preferences: A\n  multi country representative survey",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  This paper empirically analyzes how individual characteristics are associated\nwith risk aversion, loss aversion, time discounting, and present bias. To this\nend, we conduct a large-scale demographically representative survey across\neight European countries. We elicit preferences using incentivized multiple\nprice lists and jointly estimate preference parameters to account for their\nstructural dependencies. Our findings suggest that preferences are linked to a\nvariety of individual characteristics such as age, gender, and income as well\nas some personal values. We also report evidence on the relationship between\ncognitive ability and preferences. Incentivization, stake size, and the order\nof presentation of binary choices matter, underlining the importance of\ncontrolling for these factors when eliciting economic preferences.\n"
    },
    {
        "paper_id": 2204.14232,
        "authors": "Guillaume Lambert, Jesper Kristensen",
        "title": "Panoptic: the perpetual, oracle-free options protocol",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/publicdomain/zero/1.0/",
        "abstract": "  Panoptic is the perpetual, oracle-free, instant-settlement options trading\nprotocol on the Ethereum blockchain. Panoptic enables the permissionless\ntrading of options on top of any asset pool in the Uniswap v3 ecosystem and\nseeks to develop a trustless, permissionless, and composable options product,\ni.e., do for decentralized options markets what x*y=k automated market maker\nprotocols did for spot trading.\n"
    },
    {
        "paper_id": 2205.00055,
        "authors": "Salome O. Ighomereho, Afolabi A. Ojo, Samuel O. Omoyele and Samuel O.\n  Olabode",
        "title": "From Service Quality to E-Service Quality: Measurement, Dimensions and\n  Model",
        "comments": null,
        "journal-ref": "Journal of Management Information and Decision Sciences 25 (2022)\n  1-15",
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  With the global increase in online services, there is a paradigm shift from\nservice quality to e-service quality. In order to sustain this strategic\nchange, there is need to measure and evaluate the quality of e-services.\nConsequently, the paper seeks to determine the relevant e-service quality\ndimensions for e-channels. The aim is to generate a concise set of dimensions\nthat managers can use to measure e-service quality. The paper proposed an\ne-service quality model comprising seven e-service quality dimensions (website\nappearance, ease of use, reliability, security, personalisation, fulfilment and\nresponsiveness) and overall e-service quality. The study employed a\ncross-sectional research design and quantitative research approach. The data\nwere collected via a questionnaire from 400 e-channel users in Lagos State,\nNigeria. However, 318 copies of the questionnaire were found useful. The data\nwere analysed using mean, frequency, percentages, correlation and multiple\nregression analysis. The results revealed that the relevant e-service quality\ndimensions influencing overall e-service quality are reliability, security,\nfulfilment, ease of use and responsiveness. These e-service quality dimensions\nare expected to provide information for managers to evaluate and improve their\ne-channel service delivery.\n"
    },
    {
        "paper_id": 2205.00104,
        "authors": "Claudiu Vinte and Marcel Ausloos",
        "title": "The Cross-Sectional Intrinsic Entropy. A Comprehensive Stock Market\n  Volatility Estimator",
        "comments": "10 Tables, 20 Figures, 42 references, 32 pages",
        "journal-ref": "Entropy 2022, 24, 623",
        "doi": "10.3390/e24050623",
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  To take into account the temporal dimension of uncertainty in stock markets,\nthis paper introduces a cross-sectional estimation of stock market volatility\nbased on the intrinsic entropy model. The proposed cross-sectional intrinsic\nentropy (CSIE) is defined and computed as a daily volatility estimate for the\nentire market, grounded on the daily traded prices: open, high, low, and close\nprices (OHLC), along with the daily traded volume for all symbols listed on The\nNew York Stock Exchange (NYSE) and The National Association of Securities\nDealers Automated Quotations (NASDAQ). We perform a comparative analysis\nbetween the time series obtained from the CSIE and the historical volatility as\nprovided by the estimators: close-to-close, Parkinson, Garman-Klass,\nRogers-Satchell, Yang-Zhang, and intrinsic entropy (IE), defined and computed\nfrom historical OHLC daily prices of the Standard & Poor's 500 index (S&P500),\nDow Jones Industrial Average (DJIA), and the NASDAQ Composite index,\nrespectively, for various time intervals. Our study uses approximately 6000 day\nreference points, starting on 1 Jan. 2001, until 23 Jan. 2022, for both the\nNYSE and the NASDAQ. We found that the CSIE market volatility estimator is\nconsistently at least 10 times more sensitive to market changes, compared to\nthe volatility estimate captured through the market indices. Furthermore, beta\nvalues confirm a consistently lower volatility risk for market indices overall,\nbetween 50% and 90% lower, compared to the volatility risk of the entire market\nin various time intervals and rolling windows.\n"
    },
    {
        "paper_id": 2205.00279,
        "authors": "Toshiyuki Nakayama and Stefan Tappe",
        "title": "Distance between closed sets and the solutions to stochastic partial\n  differential equations",
        "comments": "34 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The goal of this paper is to clarify when the solutions to stochastic partial\ndifferential equations stay close to a given subset of the state space for\nstarting points which are close as well. This includes results for\ndeterministic partial differential equations. As an example, we will consider\nthe situation where the subset is a finite dimensional submanifold with\nboundary. We also discuss applications to mathematical finance, namely the\nmodeling of the evolution of interest rate curves.\n"
    },
    {
        "paper_id": 2205.00335,
        "authors": "Mohammadreza Mahmoudi",
        "title": "Evaluating the Impact of Bitcoin on International Asset Allocation using\n  Mean-Variance, Conditional Value-at-Risk (CVaR), and Markov Regime Switching\n  Approaches",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper aims to analyze the effect of Bitcoin on portfolio optimization\nusing mean-variance, conditional value-at-risk (CVaR), and Markov regime\nswitching approaches. I assessed each approach and developed the next based on\nthe prior approach's weaknesses until I ended with a high level of confidence\nin the final approach. Though the results of mean-variance and CVaR frameworks\nindicate that Bitcoin improves the diversification of a well-diversified\ninternational portfolio, they assume that assets' returns are developed\nlinearly and normally distributed. However, the Bitcoin return does not have\nboth of these characteristics. Due to this, I developed a Markov regime\nswitching approach to analyze the effect of Bitcoin on an international\nportfolio performance. The results show that there are two regimes based on the\nassets' returns: 1- bear state, where returns have low means and high\nvolatility, 2- bull state, where returns have high means and low volatility.\n"
    },
    {
        "paper_id": 2205.00378,
        "authors": "Philip T. Metzger, Greg W. Autry",
        "title": "The Cost of Lunar Landing Pads with a Trade Study of Construction\n  Methods",
        "comments": "50 pages, 15 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This study estimates the cost of building lunar landing pads and examines\nwhether any construction methods are economically superior to others. Some\nproposed methods require large amounts of mass transported from the Earth,\nothers require high energy consumption on the lunar surface, and others have a\nlong construction time. Each of these factors contributes direct and indirect\ncosts to lunar activities. The most important economic variables turn out to be\nthe transportation cost to the lunar surface and the magnitude of the program\ndelay cost imposed by a construction method. The cost of a landing pad depends\nsensitively on the optimization of the mass and speed of the construction\nequipment, so a minimum-cost set of equipment exists for each construction\nmethod within a specified economic scenario. Several scenarios have been\nanalyzed across a range of transportation costs with both high and low program\ndelay costs. It is found that microwave sintering is currently the most\nfavorable method to build the inner, high temperature zone of a lunar landing\npad, although other methods are within the range of uncertainty. The most\nfavorable method to build the outer, low temperature zone of the landing pad is\nalso sintering when transportation costs are high, but it switches to polymer\ninfusion when transportation costs drop below about \\$110K/kg to the lunar\nsurface. It is estimated that the Artemis Basecamp could build a landing pad\nwith a budgeted line-item cost of \\$229M assuming that transportation costs\nwill be reduced modestly from the current rate \\$1M/kg to the lunar surface to\n\\$300K/kg. A landing pad drops to \\$130M when the transportation cost drops\nfurther to \\$100K/kg, or to \\$47M if transportation costs fall below \\$10K/kg.\nUltimately, landing pads can be built around the Moon at very low cost, due to\neconomies of scale.\n"
    },
    {
        "paper_id": 2205.00383,
        "authors": "Zhe Fei, Weixuan Xia",
        "title": "Regulating stochastic clocks",
        "comments": "Reformatted; 55 pages, 6 tables, 9 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Stochastic clocks represent a class of time change methods for incorporating\ntrading activity into continuous-time financial models, with the ability to\ndeal with typical asymmetrical and tail risks in financial returns. In this\npaper we propose a significant improvement of stochastic clocks for the same\nobjective but without decreasing the number of trades or changing the trading\nintensity. Our methodology targets any L\\'{e}vy subordinator, or more generally\nany process of nonnegative independent increments, and is based on various\nchoices of regulating kernels motivated from repeated averaging. By way of a\nhyperparameter linked to the degree of regulation, arbitrarily large skewness\nand excess kurtosis of returns can be easily achieved. Generic-time Laplace\ntransforms, characterizing triplets, and cumulants of the regulated clocks and\nsubsequent mixed models are analyzed, serving purposes ranging from statistical\nestimation and option price calibration to simulation techniques. Under\nspecified jump--diffusion processes and tempered stable processes, a robust\nmoment-based estimation procedure with profile likelihood is developed and a\ncomprehensive empirical study involving S\\&P500 and Bitcoin daily returns is\nconducted to demonstrate a series of desirable effects of the proposed methods.\n"
    },
    {
        "paper_id": 2205.00494,
        "authors": "Francesco Cordoni and Fabrizio Lillo",
        "title": "Transient impact from the Nash equilibrium of a permanent market impact\n  game",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  A large body of empirical literature has shown that market impact of\nfinancial prices is transient. However, from a theoretical standpoint, the\norigin of this temporary nature is still unclear. We show that an implied\ntransient impact arises from the Nash equilibrium between a directional trader\nand one arbitrageur in a market impact game with fixed and permanent impact.\nThe implied impact is the one that can be empirically inferred from the\ndirectional trader's trading profile and price reaction to order flow.\nSpecifically, we propose two approaches to derive the functional form of the\ndecay kernel of the Transient Impact Model, one of the most popular empirical\nmodels for transient impact, from the behaviour of the directional trader at\nthe Nash equilibrium. The first is based on the relationship between past order\nflow and future price change, while in the second we solve an inverse optimal\nexecution problem. We show that in the first approach the implied kernel is\nunique, while in the second case infinite solutions exist and a linear kernel\ncan always be inferred.\n"
    },
    {
        "paper_id": 2205.00497,
        "authors": "Samuel Cole, Zachary Cowell, John M. Nunley, R. Alan Seals Jr",
        "title": "The Distribution of Occupational Tasks in the United States:\n  Implications for a Diverse and Aging Population",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We document the age-race-gender intersectionality in the distribution of\noccupational tasks in the United States. We also investigate how the task\ncontent of work changed from the early-2000s to the late-2010s for different\nage-race/ethnicity-gender groups. Using the Occupation Information Network\n(O*NET) and pooled cross-sectional data from the American Community Survey\n(ACS) we examine how the tasks that workers perform vary with age and over\ntime. We find that White men transition to occupations high in non-routine\ncognitive tasks early in their careers, whereas Hispanic and Black men work\nmostly in physically demanding jobs over their entire working lives. Routine\nmanual tasks increased dramatically for 55-67 year-old workers, except for\nAsian men and women. Policymakers will soon be challenged by financial stress\non entitlement programs, reforms could have disproportionate effects on gender\nand racial/ethnic groups due to inequality in the distribution of occupational\ntasks.\n"
    },
    {
        "paper_id": 2205.00573,
        "authors": "Jiling Cao, Jeong-Hoon Kim, Xi Li and Wenjun Zhang",
        "title": "Pricing Path-dependent Options under Stochastic Volatility via Mellin\n  Transform",
        "comments": "19 pages with 4 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  In this paper, we derive closed-form formulas of first-order approximation\nfor down-and-out barrier and floating strike lookback put option prices under a\nstochastic volatility model, by using an asymptotic approach. To find the\nexplicit closed-form formulas for the zero-order term and the first-order\ncorrection term, we use Mellin transform. We also conduct a sensitivity\nanalysis on these formulas, and compare the option prices calculated by them\nwith those generated by Monte-Carlo simulation.\n"
    },
    {
        "paper_id": 2205.00586,
        "authors": "A.H. Nzokem and V.T. Montshiwa",
        "title": "Fitting Generalized Tempered Stable distribution: Fractional Fourier\n  Transform (FRFT) Approach",
        "comments": "17 pages, 10 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  The paper investigates the rich class of Generalized Tempered Stable\ndistribution, an alternative to Normal distribution and the $\\alpha$-Stable\ndistribution for modelling asset return and many physical and economic systems.\nFirstly, we explore some important properties of the Generalized Tempered\nStable (GTS) distribution. The theoretical tools developed are used to perform\nempirical analysis. The GTS distribution is fitted using S&P 500, SPY ETF and\nBitcoin BTC. The Fractional Fourier Transform (FRFT) technique evaluates the\nprobability density function and its derivatives in the maximum likelihood\nprocedure. Based on the results from the statistical inference and the\nKolmogorov-Smirnov (K-S) goodness-of-fit, the GTS distribution fits the\nunderlying distribution of the SPY ETF return. The right side of the Bitcoin\nBTC return, and the left side of the S&P 500 return underlying distributions\nfit the Tempered Stable distribution; while the left side of the Bitcoin BTC\nreturn and the right side of the S&P 500 return underlying distributions are\nmodelled by the compound Poisson process\n"
    },
    {
        "paper_id": 2205.00605,
        "authors": "Udai Nagpal, Krishan Nagpal",
        "title": "Cluster-based Regression using Variational Inference and Applications in\n  Financial Forecasting",
        "comments": "Added comparison to regression without clusters and clearer\n  description of theoretical contribution",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
        "abstract": "  This paper describes an approach to simultaneously identify clusters and\nestimate cluster-specific regression parameters from the given data. Such an\napproach can be useful in learning the relationship between input and output\nwhen the regression parameters for estimating output are different in different\nregions of the input space. Variational Inference (VI), a machine learning\napproach to obtain posterior probability densities using optimization\ntechniques, is used to identify clusters of explanatory variables and\nregression parameters for each cluster. From these results, one can obtain both\nthe expected value and the full distribution of predicted output. Other\nadvantages of the proposed approach include the elegant theoretical solution\nand clear interpretability of results. The proposed approach is well-suited for\nfinancial forecasting where markets have different regimes (or clusters) with\ndifferent patterns and correlations of market changes in each regime. In\nfinancial applications, knowledge about such clusters can provide useful\ninsights about portfolio performance and identify the relative importance of\nvariables in different market regimes. An illustrative example of predicting\none-day S&P change is considered to illustrate the approach and compare the\nperformance of the proposed approach with standard regression without clusters.\nDue to the broad applicability of the problem, its elegant theoretical\nsolution, and the computational efficiency of the proposed algorithm, the\napproach may be useful in a number of areas extending beyond the financial\ndomain.\n"
    },
    {
        "paper_id": 2205.00634,
        "authors": "Emmanuel Coffie",
        "title": "Numerical Method for Highly Non-linear Mean-reverting Asset Price Model\n  with CEV-type Process",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  It is well documented from various empirical studies that the volatility\nprocess of an asset price dynamics is stochastic. This phenomenon called for a\nnew approach to describing the random evolution of volatility through time with\nstochastic models. In this paper, we propose a mean-reverting theta-rho model\nfor asset price dynamics where the volatility diffusion factor of this model\nfollows a highly non-linear CEV-type process. Since this model lacks a\nclosed-form formula, we construct a new truncated EM method to study it\nnumerically under the Khasminskii-type condition. We justify that the truncated\nEM solutions can be used to evaluate a path-dependent financial product.\n"
    },
    {
        "paper_id": 2205.00666,
        "authors": "Yoshua Bengio, Prateek Gupta, Dylan Radovic, Maarten Scholl, Andrew\n  Williams, Christian Schroeder de Witt, Tianyu Zhang, Yang Zhang",
        "title": "(Private)-Retroactive Carbon Pricing [(P)ReCaP]: A Market-based Approach\n  for Climate Finance and Risk Assessment",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Insufficient Social Cost of Carbon (SCC) estimation methods and short-term\ndecision-making horizons have hindered the ability of carbon emitters to\nproperly correct for the negative externalities of climate change, as well as\nthe capacity of nations to balance economic and climate policy. To overcome\nthese limitations, we introduce Retrospective Social Cost of Carbon Updating\n(ReSCCU), a novel mechanism that corrects for these limitations as empirically\nmeasured evidence is collected. To implement ReSCCU in the context of carbon\ntaxation, we propose Retroactive Carbon Pricing (ReCaP), a market mechanism in\nwhich polluters offload the payment of ReSCCU adjustments to insurers. To\nalleviate systematic risks and minimize government involvement, we introduce\nthe Private ReCaP (PReCaP) prediction market, which could see real-world\nimplementation based on the engagement of a few high net-worth individuals or\nindependent institutions.\n"
    },
    {
        "paper_id": 2205.00752,
        "authors": "Florian Peters, Doris Neuberger, Oliver Reinhardt, Adelinde Uhrmacher",
        "title": "A basic macroeconomic agent-based model for analyzing monetary regime\n  shifts",
        "comments": "31 pages, 13 Figures, 5 Tables",
        "journal-ref": null,
        "doi": "10.1371/journal.pone.0277615",
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  In macroeconomics, an emerging discussion of alternative monetary systems\naddresses the dimensions of systemic risk in advanced financial systems.\nMonetary regime changes with the aim of achieving a more sustainable financial\nsystem have already been discussed in several European parliaments and were the\nsubject of a referendum in Switzerland. However, their effectiveness and\nefficacy concerning macro-financial stability are not well-known. This paper\nintroduces a macroeconomic agent-based model (MABM) in a novel simulation\nenvironment to simulate the current monetary system, which may serve as a basis\nto implement and analyze monetary regime shifts. In this context, the monetary\nsystem affects the lending potential of banks and might impact the dynamics of\nfinancial crises. MABMs are predestined to replicate emergent financial crisis\ndynamics, analyze institutional changes within a financial system, and thus\nmeasure macro-financial stability. The used simulation environment makes the\nmodel more accessible and facilitates exploring the impact of different\nhypotheses and mechanisms in a less complex way. The model replicates a wide\nrange of stylized economic facts, including simplifying assumptions to reduce\nmodel complexity.\n"
    },
    {
        "paper_id": 2205.00957,
        "authors": "Stefan Rass, Sandra K\\\"onig, Stefan Schauer",
        "title": "Decisions with Uncertain Consequences -- A Total Ordering on\n  Loss-Distributions",
        "comments": "preprint of a correction to the article with the same name, published\n  with PLOS ONE, and currently under review",
        "journal-ref": null,
        "doi": "10.1371/journal.pone.0168583",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Decisions are often based on imprecise, uncertain or vague information.\nLikewise, the consequences of an action are often equally unpredictable, thus\nputting the decision maker into a twofold jeopardy. Assuming that the effects\nof an action can be modeled by a random variable, then the decision problem\nboils down to comparing different effects (random variables) by comparing their\ndistribution functions. Although the full space of probability distributions\ncannot be ordered, a properly restricted subset of distributions can be totally\nordered in a practically meaningful way. We call these loss-distributions,\nsince they provide a substitute for the concept of loss-functions in decision\ntheory. This article introduces the theory behind the necessary restrictions\nand the hereby constructible total ordering on random loss variables, which\nenables decisions under uncertainty of consequences. Using data obtained from\nsimulations, we demonstrate the practical applicability of our approach.\n"
    },
    {
        "paper_id": 2205.00974,
        "authors": "Panpan Li, Shengbo Gong, Shaocong Xu, Jiajun Zhou, Yu Shanqing, Qi\n  Xuan",
        "title": "Cross Cryptocurrency Relationship Mining for Bitcoin Price Prediction",
        "comments": "14 pages, 4 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Blockchain finance has become a part of the world financial system, most\ntypically manifested in the attention to the price of Bitcoin. However, a great\ndeal of work is still limited to using technical indicators to capture Bitcoin\nprice fluctuation, with little consideration of historical relationships and\ninteractions between related cryptocurrencies. In this work, we propose a\ngeneric Cross-Cryptocurrency Relationship Mining module, named C2RM, which can\neffectively capture the synchronous and asynchronous impact factors between\nBitcoin and related Altcoins. Specifically, we utilize the Dynamic Time Warping\nalgorithm to extract the lead-lag relationship, yielding Lead-lag Variance\nKernel, which will be used for aggregating the information of Altcoins to form\nrelational impact factors. Comprehensive experimental results demonstrate that\nour C2RM can help existing price prediction methods achieve significant\nperformance improvement, suggesting the effectiveness of Cross-Cryptocurrency\ninteractions on benefitting Bitcoin price prediction.\n"
    },
    {
        "paper_id": 2205.00975,
        "authors": "Katarzyna Maciejowska",
        "title": "A portfolio management of a small RES utility with a Structural Vector\n  Autoregressive model of German electricity markets",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  The changes in electricity markets expose RES producers and electricity\ntraders to various risks, among which the price and the volume risk play a very\nimportant role. In this research, a portfolio building strategies are\npresented, which allow to dynamically choose a proportion of electricity traded\nin different electricity markets (day-ahead and intraday) and hence to optimize\nthe behavior of an utility. Two types of approaches are considered: simple,\nassuming that the proportions are fixed, and data driven, which allows for\nthier fluctuation. In order to explore the market information, Structural\nVector Autoregressive (SVAR) model is applied, which allows to estimate the\nrelationship between variables of interest and to simulate their future\ndistribution. The presented methods are evaluated with data coming from German\nelectricity market. The results indicate that data driven trading strategies\nallow to increase the utility revenue and at the same time reduce the trading\nrisk, measured by the predictability of the next day income and the revenue\nValue at Risk. It turns out that the approach based on Sharp Ratio provides the\nmost robust results.\n"
    },
    {
        "paper_id": 2205.00993,
        "authors": "Sebastian G. Huayamares, Melissa P. Lokugamage, Alejandro J. Da Silva\n  Sanchez, James E. Dahlman",
        "title": "A systematic analysis of biotech startups that went public in the first\n  half of 2021",
        "comments": "14 Pages main text, 6 Pages of Supplementary Info , 6 Figures, 4\n  Supplementary Figures",
        "journal-ref": "2022 Current Research in Biotechnology",
        "doi": "10.1016/j.crbiot.2022.09.004",
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Biotechnologies are being commercialized at historic rates. In 2020, 74\nbiotech startups went public through an Initial Public Offering (IPO), and 60\nwent through the IPO process in the first six months of 2021. However, the\ntraits associated with biotech startups obtaining recent IPOs have not been\nreported. Here we build a database of biotechs that underwent an IPO in the\nfirst half of 2021. By analyzing leadership, technological focus, clinical\ntrials, and financing, we found that advanced degrees among the leadership,\nclinical trials, and intellectual property are important factors for biotech\nstartups. The data also suggest that large private rounds can decrease\ntime-to-IPO and affect post-IPO stock performance. Notably, these traits were\noften exhibited by the 138 biotech IPOs in 2018-2019, suggesting 2021 data were\nnot driven by COVID.\n"
    },
    {
        "paper_id": 2205.01012,
        "authors": "Jean-Philippe Bouchaud, Iacopo Mastromatteo, Marc Potters, Konstantin\n  Tikhonov",
        "title": "Excess Out-of-Sample Risk and Fleeting Modes",
        "comments": "9 pages, 5 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Using Random Matrix Theory, we propose a universal and versatile tool to\nreveal the existence of \"fleeting modes\", i.e. portfolios that carry\nstatistically significant excess risk, signalling ex-post a change in the\ncorrelation structure in the underlying asset space. Our proposed test is\nfurthermore independent of the \"true\" (but unknown) underlying correlation\nstructure. We show empirically that such fleeting modes exist both in futures\nmarkets and in equity markets. We proposed a metric to quantify the alignment\nbetween known factors and fleeting modes and identify momentum as a source of\nexcess risk in the equity space.\n"
    },
    {
        "paper_id": 2205.01094,
        "authors": "Yong Xie, Dakuo Wang, Pin-Yu Chen, Jinjun Xiong, Sijia Liu, Sanmi\n  Koyejo",
        "title": "A Word is Worth A Thousand Dollars: Adversarial Attack on Tweets Fools\n  Stock Predictions",
        "comments": "NAACL short paper, github: https://github.com/yonxie/AdvFinTweet",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-sa/4.0/",
        "abstract": "  More and more investors and machine learning models rely on social media\n(e.g., Twitter and Reddit) to gather real-time information and sentiment to\npredict stock price movements. Although text-based models are known to be\nvulnerable to adversarial attacks, whether stock prediction models have similar\nvulnerability is underexplored. In this paper, we experiment with a variety of\nadversarial attack configurations to fool three stock prediction victim models.\nWe address the task of adversarial generation by solving combinatorial\noptimization problems with semantics and budget constraints. Our results show\nthat the proposed attack method can achieve consistent success rates and cause\nsignificant monetary loss in trading simulation by simply concatenating a\nperturbed but semantically similar tweet.\n"
    },
    {
        "paper_id": 2205.01175,
        "authors": "Toni M. Whited",
        "title": "Integrating Structural and Reduced-Form Methods in Empirical Finance",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  I discuss various ways in which inference based on the estimation of the\nparameters of statistical models (reduced-form estimation) can be combined with\ninference based on the estimation of the parameters of economic models\n(structural estimation). I discuss five basic categories of integration:\ndirectly combining the two methods, using statistical models to simplify\nstructural estimation, using structural estimation to extend the validity of\nreduced-form results, using reduced-form techniques to assess the external\nvalidity of structural estimations, and using structural estimation as a sample\nselection remedy. I illustrate each of these methods with examples from\ncorporate finance, banking, and personal finance.\n"
    },
    {
        "paper_id": 2205.01216,
        "authors": "Ashley Nunes, Chung Yi See, Lucas Woodley, Nicole A. Divers, and\n  Audrey L. Cui",
        "title": "Estimating beneficiaries of the child tax credit: past, present, and\n  future",
        "comments": "64 pages, 7 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Government efforts to address child poverty commonly encompass economic\nassistance programs that bolster household income. The Child Tax Credit (CTC)\nis the most prominent example of this. Introduced by the United States Congress\nin 1997, the program endeavors to help working parents via income\nstabilization. Our work examines the extent to which the CTC has done so. Our\nstudy, which documents clear, consistent, and compelling evidence of gender\ninequity in benefits realization, yields four key findings. First, stringent\nrequisite income thresholds disproportionally disadvantage single mothers, a\nreflection of the high concentration of this demographic in lower segments of\nthe income distribution. Second, married parents and, to a lesser extent,\nsingle fathers, are the primary beneficiaries of the CTC program when benefits\nare structured as credits rather than refunds. Third, making program benefits\nmore generous disproportionally reduces how many single mothers, relative to\nmarried parents and single fathers, can claim this benefit. Fourth and finally,\nincreasing credit refundability can mitigate gender differences in relief\neligibility, although doing so imposes externalities of its own. Our findings\ncan inform public policy discourse surrounding the efficacy of programs like\nthe CTC and the effectiveness of programs aimed at alleviating child poverty.\n"
    },
    {
        "paper_id": 2205.01255,
        "authors": "Timothy Ludlow, Jonas Fooken, Christiern Rose, Kam Tang",
        "title": "Incorporating Financial Hardship in Measuring the Mental Health Impact\n  of Housing Stress",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Housing expenditure tends to be sticky and costly to adjust, and makes up a\nlarge proportion of household expenditure. Additionally, the loss of housing\ncan have catastrophic consequences. These specific features of housing\nexpenditure imply that housing stress could cause negative mental health\nimpacts. This research investigates the effects of housing stress on mental\nhealth, contributing to the literature by nesting housing stress within a\nmeasure of financial hardship, thus improving robustness to omitted variables\nand creating a natural comparison group for matching. Fixed effects (FE)\nregressions and a difference-in-differences (DID) methodology are estimated\nutilising data from the Household Income and Labour Dynamics in Australia\n(HILDA) Survey. The results show that renters who are in housing stress have a\nsignificant decline in self-reported mental health, with those in prior\nfinancial hardship being more severely affected. In contrast, there is little\nto no evidence of housing stress impacting on owners with a mortgage. The\nresults also suggest that the mental health impact of housing stress is more\nimportant than some, but not all, aspects of financial hardship.\n"
    },
    {
        "paper_id": 2205.01317,
        "authors": "Vishnu Baburajan, Jo\\~ao de Abreu e Silva, Francisco Camara Pereira",
        "title": "Open vs Closed-ended questions in attitudinal surveys -- comparing,\n  combining, and interpreting using natural language processing",
        "comments": null,
        "journal-ref": null,
        "doi": "10.1016/j.trc.2022.103589",
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  To improve the traveling experience, researchers have been analyzing the role\nof attitudes in travel behavior modeling. Although most researchers use\nclosed-ended surveys, the appropriate method to measure attitudes is debatable.\nTopic Modeling could significantly reduce the time to extract information from\nopen-ended responses and eliminate subjective bias, thereby alleviating analyst\nconcerns. Our research uses Topic Modeling to extract information from\nopen-ended questions and compare its performance with closed-ended responses.\nFurthermore, some respondents might prefer answering questions using their\npreferred questionnaire type. So, we propose a modeling framework that allows\nrespondents to use their preferred questionnaire type to answer the survey and\nenable analysts to use the modeling frameworks of their choice to predict\nbehavior. We demonstrate this using a dataset collected from the USA that\nmeasures the intention to use Autonomous Vehicles for commute trips.\nRespondents were presented with alternative questionnaire versions (open- and\nclosed- ended). Since our objective was also to compare the performance of\nalternative questionnaire versions, the survey was designed to eliminate\ninfluences resulting from statements, behavioral framework, and the choice\nexperiment. Results indicate the suitability of using Topic Modeling to extract\ninformation from open-ended responses; however, the models estimated using the\nclosed-ended questions perform better compared to them. Besides, the proposed\nmodel performs better compared to the models used currently. Furthermore, our\nproposed framework will allow respondents to choose the questionnaire type to\nanswer, which could be particularly beneficial to them when using voice-based\nsurveys.\n"
    },
    {
        "paper_id": 2205.0137,
        "authors": "Claudiu Vinte, Marcel Ausloos and Titus Felix Furtuna",
        "title": "A Volatility Estimator of Stock Market Indices Based on the Intrinsic\n  Entropy Model",
        "comments": null,
        "journal-ref": "Entropy 2021, 23(4), 484 (This article belongs to the Special\n  Issue Information Theory and Economic Network)",
        "doi": "10.3390/e23040484",
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Grasping the historical volatility of stock market indices and accurately\nestimating are two of the major focuses of those involved in the financial\nsecurities industry and derivative instruments pricing. This paper presents the\nresults of employing the intrinsic entropy model as a substitute for estimating\nthe volatility of stock market indices. Diverging from the widely used\nvolatility models that take into account only the elements related to the\ntraded prices, namely the open, high, low, and close prices of a trading day\n(OHLC), the intrinsic entropy model takes into account the traded volumes\nduring the considered time frame as well. We adjust the intraday intrinsic\nentropy model that we introduced earlier for exchange-traded securities in\norder to connect daily OHLC prices with the ratio of the corresponding daily\nvolume to the overall volume traded in the considered period. The intrinsic\nentropy model conceptualizes this ratio as entropic probability or market\ncredence assigned to the corresponding price level. The intrinsic entropy is\ncomputed using historical daily data for traded market indices (S&P 500, Dow\n30, NYSE Composite, NASDAQ Composite, Nikkei 225, and Hang Seng Index). We\ncompare the results produced by the intrinsic entropy model with the volatility\nestimates obtained for the same data sets using widely employed industry\nvolatility estimators. The intrinsic entropy model proves to consistently\ndeliver reliable estimates for various time frames while showing peculiarly\nhigh values for the coefficient of variation, with the estimates falling in a\nsignificantly lower interval range compared with those provided by the other\nadvanced volatility estimators.\n"
    },
    {
        "paper_id": 2205.01386,
        "authors": "Claudiu Vinte, Ion Smeureanu, Titus-Felix Furtuna and Marcel Ausloos",
        "title": "An Intrinsic Entropy Model for Exchange-Traded Securities",
        "comments": null,
        "journal-ref": "Entropy 2019, 21(12), 1173 (This article belongs to the Special\n  Issue Disordered Systems, Fractals and Chaos)",
        "doi": "10.3390/e21121173",
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  This article introduces an intrinsic entropy model that can be used as an\nindicator to gauge investor interest in a given exchange-traded security, along\nwith the state of the general market corroborated by individual security trade\ndata. Although the syntagma of intrinsic entropy might sound somehow\npleonastic, since entropy itself characterizes the fundamentals of a system, we\nwould like to make a clear distinction between entropy models based on the\nvalues that a random variable may take and the model that we propose, which\nemploys actual stock exchange trading data. The model we propose for intrinsic\nentropy does not include any exogenous factor that could influence the level of\nentropy. The intrinsic entropy signals whether the market is inclined to buy\nthe security or rather to sell it. We further explore the usage of the\nintrinsic entropy model for algorithmic trading, in order to demonstrate the\nvalue of our model in assisting investors in the selection of the intraday\nstock portfolio, along with timely generated signals to support the buy / sell\ndecision making process. The test results provide empirical evidence that the\nproposed intrinsic entropy model can be used as an indicator to evaluate the\ndirection and intensity of intraday trading activity of an exchange-traded\nsecurity. The data used for the test consisted of historical intraday\ntransactions executed on The Bucharest Stock Exchange (BVB).\n"
    },
    {
        "paper_id": 2205.01436,
        "authors": "Javier L\\'opez Prol, Karl W. Steininger, Keith Williges, Wolf D.\n  Grossmann, Iris Grossmann",
        "title": "Potential gains of long-distance trade in electricity",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Electrification of all economic sectors and solar photovoltaics (PV) becoming\nthe lowest-cost electricity generation technology in ever more regions give\nrise to new potential gains of trade. We develop a stylized analytical model to\nminimize unit energy cost in autarky, open it to different trade\nconfigurations, and evaluate it empirically. We identify large potential gains\nfrom interhemispheric and global electricity trade by combining complementary\nseasonal and diurnal cycles. The corresponding high willingness to pay for\nlarge-scale transmission suggests far-reaching political economy and regulatory\nimplications.\n"
    },
    {
        "paper_id": 2205.01444,
        "authors": "Taras Bodnar, Vilhelm Niklasson and Erik Thors\\'en",
        "title": "Volatility Sensitive Bayesian Estimation of Portfolio VaR and CVaR",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper, a new way to integrate volatility information for estimating\nvalue at risk (VaR) and conditional value at risk (CVaR) of a portfolio is\nsuggested. The new method is developed from the perspective of Bayesian\nstatistics and it is based on the idea of volatility clustering. By specifying\nthe hyperparameters in a conjugate prior based on two different rolling window\nsizes, it is possible to quickly adapt to changes in volatility and\nautomatically specify the degree of certainty in the prior. This constitutes an\nadvantage in comparison to existing Bayesian methods that are less sensitive to\nsuch changes in volatilities and also usually lack standardized ways of\nexpressing the degree of belief. We illustrate our new approach using both\nsimulated and empirical data. Compared to some other well known homoscedastic\nand heteroscedastic models, the new method provides a good alternative for risk\nestimation, especially during turbulent periods where it can quickly adapt to\nchanging market conditions.\n"
    },
    {
        "paper_id": 2205.01524,
        "authors": "Margherita Doria, Elisa Luciano, Patrizia Semeraro",
        "title": "Machine learning techniques in joint default assessment",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper studies the consequences of capturing non-linear dependence among\nthe covariates that drive the default of different obligors and the overall\nriskiness of their credit portfolio. Joint default modeling is, without loss of\ngenerality, the classical Bernoulli mixture model. Using an application to a\ncredit card dataset we show that, even when Machine Learning techniques perform\nonly slightly better than Logistic Regression in classifying individual\ndefaults as a function of the covariates, they do outperform it at the\nportfolio level. This happens because they capture linear and non-linear\ndependence among the covariates, whereas Logistic Regression only captures\nlinear dependence. The ability of Machine Learning methods to capture\nnon-linear dependence among the covariates produces higher default correlation\ncompared with Logistic Regression. As a consequence, on our data, Logistic\nRegression underestimates the riskiness of the credit portfolio.\n"
    },
    {
        "paper_id": 2205.01639,
        "authors": "Nicole Koenigstein",
        "title": "Dynamic and Context-Dependent Stock Price Prediction Using Attention\n  Modules and News Sentiment",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The growth of machine-readable data in finance, such as alternative data,\nrequires new modeling techniques that can handle non-stationary and\nnon-parametric data. Due to the underlying causal dependence and the size and\ncomplexity of the data, we propose a new modeling approach for financial time\nseries data, the $\\alpha_{t}$-RIM (recurrent independent mechanism). This\narchitecture makes use of key-value attention to integrate top-down and\nbottom-up information in a context-dependent and dynamic way. To model the data\nin such a dynamic manner, the $\\alpha_{t}$-RIM utilizes an exponentially\nsmoothed recurrent neural network, which can model non-stationary times series\ndata, combined with a modular and independent recurrent structure. We apply our\napproach to the closing prices of three selected stocks of the S\\&P 500\nuniverse as well as their news sentiment score. The results suggest that the\n$\\alpha_{t}$-RIM is capable of reflecting the causal structure between stock\nprices and news sentiment, as well as the seasonality and trends. Consequently,\nthis modeling approach markedly improves the generalization performance, that\nis, the prediction of unseen data, and outperforms state-of-the-art networks\nsuch as long short-term memory models.\n"
    },
    {
        "paper_id": 2205.01957,
        "authors": "Charles Shaw, Silvio Vanadia",
        "title": "Utilitarianism on the front lines: COVID-19, public ethics, and the\n  \"hidden assumption\" problem",
        "comments": "Forthcoming in Ethics & Bioethics",
        "journal-ref": "Ethics and Bioethics (in Central Europe) 12 (1-2):60-78 (2022)",
        "doi": "10.2478/ebce-2022-0006",
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  How should we think of the preferences of citizens? Whereas self-optimal\npolicy is relatively straightforward to produce, socially optimal policy often\nrequires a more detailed examination. In this paper, we identify an issue that\nhas received far too little attention in welfarist modelling of public policy,\nwhich we name the \"hidden assumptions\" problem. Hidden assumptions can be\ndeceptive because they are not expressed explicitly and the social planner\n(e.g. a policy maker, a regulator, a legislator) may not give them the critical\nattention they need. We argue that ethical expertise has a direct role to play\nin public discourse because it is hard to adopt a position on major issues like\npublic health policy or healthcare prioritisation without making contentious\nassumptions about population ethics. We then postulate that ethicists are best\nsituated to critically evaluate these hidden assumptions, and can therefore\nplay a vital role in public policy debates.\n"
    },
    {
        "paper_id": 2205.02123,
        "authors": "Marita Kuhlmann",
        "title": "Eine empirische Analyse der Skalierung von Value-at-Risk Schaetzungen",
        "comments": "in German language",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  In practice, the value-at-risk (VaR) for a longer holding period is often\nscaled using the 'square root of time rule'. The VaR is determined for a\nshorter holding period and then scaled up according to the desired holding\nperiod. For example, the Basel rules allow banks to scale up the 1-day VaR by\nthe square root of ten to determine the 10-day VaR. It can be seen from the\nresults of this thesis that scaling can also provide good and accurate\nestimates of VaR. However, it is probably much more important to consider that,\ndepending on the methods or data set involved, there may also be significant\nconsequences for risk provisioning. Particularly, since scaling does not always\navoid the occurrence of losses that exceed the VaR estimate on a frequent basis\nover a period of time. Overall, the permission to use the square root of time\nrule in the regulatory framework should be reconsidered.\n"
    },
    {
        "paper_id": 2205.02164,
        "authors": "C\\'esar A. Hidalgo",
        "title": "The Policy Implications of Economic Complexity",
        "comments": null,
        "journal-ref": "Research Policy, 52(9), 104863 (2023)",
        "doi": "10.1016/j.respol.2023.104863",
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  In recent years economic complexity has grown into an active field of\nfundamental and applied research. Yet, despite important advances, the policy\nimplications of economic complexity remain unclear or misunderstood. Here I\norganize the policy implications of economic complexity in a framework grounded\non 4 Ws: what approaches, focused on identifying target activities and/or\nlocations; when approaches, focused on timing support for related and unrelated\nactivities; where approaches, focused on the geographic diffusion of knowledge;\nand who approaches, focused on the role played by agents of structural change.\nThe goal of this paper is to provide a framework that groups, organizes, and\nclarifies the policy implications of economic complexity to facilitate its\ncontinued use in regional and international development.\n"
    },
    {
        "paper_id": 2205.0231,
        "authors": "Michele Battisti, Andrea Mario Lavezzi, Roberto Musotto",
        "title": "Organizing Crime: an Empirical Analysis of the Sicilian Mafia",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this article we study the organizational structure of a large group of\nmembers of the Sicilian Mafia by means of social network analysis and an\neconometric analysis of link formation. Our mains results are the following. i)\nThe Mafia network is a small-world network adjusted by its criminal nature, and\nis strongly disassortative. ii) Mafia bosses are not always central in the\nnetwork. In particular, consistent with a prediction of Baccara and Bar-Isaac,\nwe identify a \"cell-dominated hierarchy\" in the network: a key member is not\ncentral, but is connected to a relative with a central position. iii) The\nprobability of link formation between two agents is higher if the two agents\nbelong to the same Mandamento, if they share a high number of similar tasks,\nwhile being a \"boss\" reduces the probability of link formation between them.\niv) The probability of link formation for an individual agent is higher if he\nis in charge of keeping connections outside his Mandamento, of collecting\nprotection money and or having a directive role, while age has modest role.\nThese results are interpreted in the light of the efficiency/security trade-off\nfaced by the Mafia and of its known hierarchical structure.\n"
    },
    {
        "paper_id": 2205.02581,
        "authors": "Jean-Baptiste Gaudemet, Jules Deschamps, Olivier Vinciguerra",
        "title": "A Stochastic Climate Model -- An approach to calibrate the\n  Climate-Extended Risk Model (CERM)",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-sa/4.0/",
        "abstract": "  The initial Climate-Extended Risk Model (CERM) addresses the estimate of\nclimate-related financial risk embedded within a bank loan portfolio, through a\nclimatic extension of the Basel II IRB model. It uses a Gaussian copula model\ncalibrated with non stationary macro-correlations in order to reflect the\nfuture evolution of climate-related financial risks. In this complementary\narticle, we propose a stochastic forward-looking methodology to calibrate\nclimate macro-correlation evolution from scientific climate data, for physical\nand transition efforts specifically. We assume a global physical and transition\nrisk, likened to persistent greenhouse gas (GHG) concentration in the\natmosphere. The economic risk is considered stationary and can therefore be\ncalibrated with a backward-looking methodology. We present 4 key principles to\nmodel the GDP and we propose to model the economic, physical and transition\neffort factors with three interdependent stochastic processes allowing for a\ncalibration with seven well defined parameters. These parameters can be\ncalibrated using public data. This new approach means not only to evaluate\nclimate risks without picking any specific scenario but also allows to fill the\ngap between current one year approach of regulatory and economic capital models\nand the necessarily long-term view of climate risks by designing a framework to\nevaluate the resulting credit loss on each step (typically yearly) of the\ntransition path. This new approach could prove instrumental in the 2022 context\nof central banks weighing the pros and cons of a climate capital charge.\n"
    },
    {
        "paper_id": 2205.028,
        "authors": "Viktor Stojkoski",
        "title": "Measures of physical mixing evaluate the economic mobility of the\n  typical individual",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Measures of economic mobility represent aggregate values for how individual\nwealth changes over time. As such, these measures may not describe the\nfeasibility of a typical individual to change their wealth. To address this\nlimitation, we introduce mixing, a concept from statistical physics, as a\nrelevant phenomenon for quantifying how individuals move across the wealth\ndistribution. We display the relationship between mixing and mobility both\ntheoretically and using data. By studying the properties of an established\nmodel of wealth dynamics, we show that some individuals can move across the\ndistribution when wealth is a non-mixing observable. Only in the mixing case\nevery individual is able to move across the whole wealth distribution. There is\nalso a direct equivalence between measures of mixing and the magnitude of the\nstandard measures of economic mobility, but the opposite is not true. We then\ndescribe an empirical method for estimating the mixing properties of wealth\ndynamics in practice. We use this method to present a pedagogical application\nusing the USA longitudinal data. This, approach, even though limited in data\navailability, leads to results suggesting that wealth in the USA is either\nnon-mixing or that it takes a very long time for the individuals to mix within\nthe distribution. These results showcase how mixing can be used in tandem with\nmeasures of mobility for drawing conclusions about the extent of mobility\nacross the whole distribution.\n"
    },
    {
        "paper_id": 2205.03087,
        "authors": "Pierre Gosselin (IF), A\\\"ileen Lotz, Marc Wambst (IRMA)",
        "title": "Financial Markets and the Real Economy: A Statistical Field Perspective\n  on Capital Allocation and Accumulation",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper provides a general method to directly translate a classical\neconomic framework with a large number of agents into a field-formalism model.\nThis type of formalism allows the analytical treatment of economic models with\nan arbitrary number of agents, while preserving the system's interactions and\nmicroeconomic features of the individual level.We apply this methodology to\nmodel the interactions between financial markets and the real economy,\ndescribed in a classical framework of a large number of heterogeneous agents,\ninvestors and firms. Firms are spread among sectors but may shift between\nsectors to improve their returns. They compete by producing differentiated\ngoods and reward their investors by paying dividends and through their stocks'\nvaluation. Investors invest in firms and move along sectors based on firms'\nexpected long-run returns.The field-formalism model derived from this framework\nallows for collective states to emerge. We show that the number of firms in\neach sector depends on the aggregate financial capital invested in the sector\nand its firms' expected long-term returns. Capital accumulation in each sector\ndepends both on short-term returns and expected long-term returns relative to\nneighbouring sectors.For each sector, three patterns of accumulation emerge. In\nthe first pattern, the dividend component of short-term returns is determinant\nfor sectors with small number of firms and low capital. In the second pattern,\nboth short and long-term returns in the sector drive intermediate-to-high\ncapital. In the third pattern, higher expectations of long-term returns drive\nmassive inputs of capital.Instability in capital accumulation may arise among\nand within sectors. We therefore widen our approach and study the dynamics of\nthe collective configurations, in particular interactions between average\ncapital and expected long-term returns, and show that overall stability\ncrucially depends on the expectations' formation process.Expectations that are\nhighly reactive to capital variations stabilize high capital configurations,\nand drive low-to-moderate capital sectors towards zero or a higher level of\ncapital, depending on their initial capital. Inversely, low-to moderate capital\nconfigurations are stabilized by expectations moderately reactive to capital\nvariations, and drive high capital sectors towards more moderate level of\ncapital equilibria.Eventually, the combination of expectations both highly\nsensitive to exogenous conditions and highly reactive to variations in capital\nimply that large fluctuations of capital in the system, at the possible expense\nof the real economy.\n"
    },
    {
        "paper_id": 2205.03393,
        "authors": "Sarah A. Jacobson, Luyao Zhang, Jiasheng Zhu",
        "title": "The Right Tool for the Job: Matching Active Learning Techniques to\n  Learning Objectives",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Active learning comprises many varied techniques that engage students\nactively in the construction of their understanding. Because of this variation,\ndifferent active learning techniques may be best suited to achieving different\nlearning objectives. We study students' perceptions of a set of active learning\ntechniques (including a Python simulation and an interactive game) and some\ntraditional techniques (like lecture). We find that students felt they engaged\nfairly actively with all of the techniques, though more with those with a heavy\ngrade weight and some of the active learning techniques, and they reported\nenjoying the active learning techniques the most except for an assignment that\nrequired soliciting peer advice on a research idea. All of the techniques were\nrated as relatively effective for achieving each of six learning objectives,\nbut to varying extents. The most traditional techniques like exams were rated\nhighest for achieving an objective associated with lower order cognitive\nskills, remembering concepts. In contrast, some active learning techniques like\nclass presentations and the Python simulation were rated highest for achieving\nobjectives related to higher order cognitive skills, including learning to\nconduct research, though lectures also performed surprisingly well for these\nobjectives. Other technique-objective matches are intuitive; for example, the\ndebate is rated highly for understanding pros and cons of an issue, and small\ngroup discussion is rated highly for collaborative learning. Our results\nsupport the idea that different teaching techniques are best suited for\ndifferent outcomes, which implies that a mix of techniques may be optimal in\ncourse design.\n"
    },
    {
        "paper_id": 2205.03741,
        "authors": "Peter Friz, Jim Gatheral",
        "title": "Diamonds and forward variance models",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this non-technical introduction to diamond trees and forests, we focus on\ntheir application to computation in stochastic volatility models written in\nforward variance form, rough volatility models in particular.\n"
    },
    {
        "paper_id": 2205.03852,
        "authors": "Cyril Bachelard, Apostolos Chalkis, Vissarion Fisikopoulos, Elias\n  Tsigaridas",
        "title": "Randomized geometric tools for anomaly detection in stock markets",
        "comments": "39 pages, 14 figures, 9 tables",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/publicdomain/zero/1.0/",
        "abstract": "  We propose novel randomized geometric tools to detect low-volatility\nanomalies in stock markets; a principal problem in financial economics. Our\nmodeling of the (detection) problem results in sampling and estimating the\n(relative) volume of geodesically non-convex and non-connected spherical\npatches that arise by intersecting a non-standard simplex with a sphere. To\nsample, we introduce two novel Markov Chain Monte Carlo (MCMC) algorithms that\nexploit the geometry of the problem and employ state-of-the-art continuous\ngeometric random walks (such as Billiard walk and Hit-and-Run) adapted on\nspherical patches. To our knowledge, this is the first geometric formulation\nand MCMC-based analysis of the volatility puzzle in stock markets. We have\nimplemented our algorithms in C++ (along with an R interface) and we illustrate\nthe power of our approach by performing extensive experiments on real data. Our\nanalyses provide accurate detection and new insights into the distribution of\nportfolios' performance characteristics. Moreover, we use our tools to show\nthat classical methods for low-volatility anomaly detection in finance form bad\nproxies that could lead to misleading or inaccurate results.\n"
    },
    {
        "paper_id": 2205.03862,
        "authors": "Alessandro Ferrari",
        "title": "Inventories, Demand Shocks Propagation and Amplification in Supply\n  Chains",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  I study the role of industries' position in supply chains in shaping the\ntransmission of final demand shocks. First, I use a shift-share design based on\ndestination-specific final demand shocks and destination shares to show that\nshocks amplify upstream. Quantitatively, upstream industries respond to final\ndemand shocks up to three times as much as final goods producers. To organize\nthe reduced form results, I develop a tractable production network model with\ninventories and study how the properties of the network and the cyclicality of\ninventories interact to determine whether final demand shocks amplify or\ndissipate upstream. I test the mechanism by directly estimating the\nmodel-implied relationship between output growth and demand shocks, mediated by\nnetwork position and inventories. I find evidence of the role of inventories in\nexplaining heterogeneous output elasticities. Finally, I use the model to\nquantitatively study how inventories and network properties shape the\nvolatility of the economy.\n"
    },
    {
        "paper_id": 2205.03908,
        "authors": "Alessandro Ferrari and Francisco Queir\\'os",
        "title": "Firm Heterogeneity, Market Power and Macroeconomic Fragility",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We study how firm heterogeneity and market power affect macroeconomic\nfragility, defined as the probability of long slumps. We propose a theory in\nwhich the positive interaction between firm entry, competition and factor\nsupply can give rise to multiple steady-states. We show that when firms are\nhighly heterogeneous in terms of productivities, even small temporary shocks\ncan trigger firm exit and make the economy spiral in a competition-driven\npoverty trap. We calibrate our model to incorporate the well-documented trends\non rising firm heterogeneity in the US economy, and show that they\nsignificantly increase the likelihood and length of slow recoveries. We use our\nframework to study the 2008-09 recession and show that the model can\nrationalize the persistent deviation of output and most macroeconomic\naggregates from trend, including the behavior of net entry, markups and the\nlabor share. Post-crisis cross-industry data corroborates our proposed\nmechanism. We conclude by showing that firm subsidies can be powerful in\npreventing long slumps and can lead to welfare gains between 10% and 50%.\n"
    },
    {
        "paper_id": 2205.04256,
        "authors": "Luyao Zhang, Xinshi Ma, Yulin Liu",
        "title": "SoK: Blockchain Decentralization",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Blockchain introduces decentralized trust in peer-to-peer networks, advancing\nsecurity and democratizing systems. Yet, a unified definition for\ndecentralization remains elusive. Our Systematization of Knowledge (SoK) seeks\nto bridge this gap, emphasizing quantification and methodological coherence.\nWe've formulated a taxonomy defining blockchain decentralization across five\nfacets: consensus, network, governance, wealth, and transaction. Despite the\nprevalent focus on consensus decentralization, our novel index, based on\nShannon entropy, provides comprehensive insights. Moreover, we delve into\nalternative metrics like the Gini and Nakamoto Coefficients and the\nHerfindahl-Hirschman Index (HHI), supplemented by an open-source Python tool on\nGitHub. In terms of methodology, blockchain research has often bypassed\nstringent scientific methods. By employing descriptive, predictive, and causal\nmethods, our study showcases the potential of structured research in\nblockchain. Descriptively, we observe a trend of converging decentralization\nlevels over time. Examining DeFi platforms reveals exchange and lending\napplications as more decentralized than their payment and derivatives\ncounterparts. Predictively, there's a notable correlation between Ether's\nreturns and transaction decentralization in Ether-backed stablecoins. Causally,\nEthereum's transition to the EIP-1559 transaction fee model has a profound\nimpact on DeFi transaction decentralization. To conclude, our work outlines\ndirections for blockchain research, emphasizing the delicate balance among\ndecentralization facets, fostering long-term decentralization, and the ties\nbetween decentralization, security, privacy, and efficiency. We end by\nspotlighting challenges in grasping blockchain decentralization intricacies.\n"
    },
    {
        "paper_id": 2205.0429,
        "authors": "Huaxin Wang-Lu",
        "title": "Bitcoin Returns and Public Attention to COVID-19: Do Timing and\n  Individualism Matter?",
        "comments": "17 pages, 16 figures, 1 table",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The evolution of the pandemic and people's concern over it have an impact on\nthe Bitcoin market, while the extent of individualism could differentiate\ninvestor behaviors in the financial market during the pandemic. This paper\nexamines whether public attention to COVID-19 in individualistic countries\nversus collectivistic countries Granger causes Bitcoin returns between February\n11, 2020 and May 09, 2022. To this end, eight large economies with a\nindividualistic or collectivistic tradition are chosen for analyses. By using\nrolling and recursive-evolving algorithms, it accounts for the timing of\nCOVID-19 issues that vary by country and circumvents the potential estimation\nbias that a traditional Granger causality test may suffer due largely to\nGoogle's sampling variation for different time frames. In general,\ncollectivistic countries are found to have stronger causal impacts on Bitcoin\nreturns than individualistic countries.\n"
    },
    {
        "paper_id": 2205.04426,
        "authors": "Pavel Vashchenko and Alexei Verenikin and Anna Verenikina",
        "title": "Generalized modified principal components analysis of Russian\n  universities competitiveness",
        "comments": "10 pages",
        "journal-ref": "The 14 th International Days of Statistics and Economics, Prague,\n  September 10-12, 2020",
        "doi": "10.18267/pr.2020.los.223.0",
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  The article is devoted to the competitiveness analysis of Russian\ninstitutions of higher education in international and local markets. The\nmethodology of research is based on generalized modified principal component\nanalysis. Principal components analysis has proven its efficiency in business\nperformance assessment. We apply a modification of this methodology to\nconstruction of an aggregate index of university performance. The whole set of\nprincipal components with weighting coefficients equal to the proportions of\nthe corresponding explained variance are utilized as an aggregate measure of\nvarious aspects of higher education. This methodology allows to reveal the\nfactors which exert positive or negative influence on university\ncompetitiveness. We construct a kind of objective ranking of universities in\norder to estimate the current situation and prospects of higher education in\nRussia. It is applicable for evaluation of public policy in higher education,\nwhich, by inertia, aims to promote competition rather than cooperation among\nuniversities.\n"
    },
    {
        "paper_id": 2205.0452,
        "authors": "Dixon Domfeh, Arpita Chatterjee, and Matthew Dixon",
        "title": "A Unified Bayesian Framework for Pricing Catastrophe Bond Derivatives",
        "comments": "38 pages, 11 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Catastrophe (CAT) bond markets are incomplete and hence carry uncertainty in\ninstrument pricing. As such various pricing approaches have been proposed, but\nnone treat the uncertainty in catastrophe occurrences and interest rates in a\nsufficiently flexible and statistically reliable way within a unifying asset\npricing framework. Consequently, little is known empirically about the expected\nrisk-premia of CAT bonds. The primary contribution of this paper is to present\na unified Bayesian CAT bond pricing framework based on uncertainty\nquantification of catastrophes and interest rates. Our framework allows for\ncomplex beliefs about catastrophe risks to capture the distinct and common\npatterns in catastrophe occurrences, and when combined with stochastic interest\nrates, yields a unified asset pricing approach with informative expected risk\npremia. Specifically, using a modified collective risk model -- Dirichlet\nPrior-Hierarchical Bayesian Collective Risk Model (DP-HBCRM) framework -- we\nmodel catastrophe risk via a model-based clustering approach. Interest rate\nrisk is modeled as a CIR process under the Bayesian approach. As a consequence\nof casting CAT pricing models into our framework, we evaluate the price and\nexpected risk premia of various CAT bond contracts corresponding to clustering\nof catastrophe risk profiles. Numerical experiments show how these clusters\nreveal how CAT bond prices and expected risk premia relate to claim frequency\nand loss severity.\n"
    },
    {
        "paper_id": 2205.04563,
        "authors": "Eric Luxenberg and Stephen Boyd",
        "title": "Portfolio Construction with Gaussian Mixture Returns and Exponential\n  Utility via Convex Optimization",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We consider the problem of choosing an optimal portfolio, assuming the asset\nreturns have a Gaussian mixture (GM) distribution, with the objective of\nmaximizing expected exponential utility. In this paper we show that this\nproblem is convex, and readily solved exactly using domain-specific languages\nfor convex optimization, without the need for sampling or scenarios. We then\nshow how the closely related problem of minimizing entropic value at risk can\nalso be formulated as a convex optimization problem.\n"
    },
    {
        "paper_id": 2205.04595,
        "authors": "A. Max Reppen, H. Mete Soner, Valentin Tissot-Daguette",
        "title": "Neural Optimal Stopping Boundary",
        "comments": "23 pages, ( figures, 6 Tables",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  A method based on deep artificial neural networks and empirical risk\nminimization is developed to calculate the boundary separating the stopping and\ncontinuation regions in optimal stopping. The algorithm parameterizes the\nstopping boundary as the graph of a function and introduces relaxed stopping\nrules based on fuzzy boundaries to facilitate efficient optimization. Several\nfinancial instruments, some in high dimensions, are analyzed through this\nmethod, demonstrating its effectiveness. The existence of the stopping boundary\nis also proved under natural structural assumptions.\n"
    },
    {
        "paper_id": 2205.04604,
        "authors": "A. Max Reppen, H. Mete Soner, Valentin Tissot-Daguette",
        "title": "Deep Stochastic Optimization in Finance",
        "comments": "18 pages, 4 Figures, 4 Tables",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  This paper outlines, and through stylized examples evaluates a novel and\nhighly effective computational technique in quantitative finance. Empirical\nRisk Minimization (ERM) and neural networks are key to this approach. Powerful\nopen source optimization libraries allow for efficient implementations of this\nalgorithm making it viable in high-dimensional structures. The free-boundary\nproblems related to American and Bermudan options showcase both the power and\nthe potential difficulties that specific applications may face. The impact of\nthe size of the training data is studied in a simplified Merton type problem.\nThe classical option hedging problem exemplifies the need of market generators\nor large number of simulations.\n"
    },
    {
        "paper_id": 2205.04736,
        "authors": "Mike Ludkovski and Glen Swindle and Eric Grannan",
        "title": "Large Scale Probabilistic Simulation of Renewables Production",
        "comments": "24 pages, 14 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We develop a probabilistic framework for joint simulation of short-term\nelectricity generation from renewable assets. In this paper we describe a\nmethod for producing hourly day-ahead scenarios of generated power at\ngrid-scale across hundreds of assets. These scenarios are conditional on\nspecified forecasts and yield a full uncertainty quantification both at the\nmarginal asset-level and across asset collections. Our simulation pipeline\nfirst applies asset calibration to normalize hourly, daily and seasonal\ngeneration profiles, and to Gaussianize the forecast--actuals distribution. We\nthen develop a novel clustering approach to stably estimate the covariance\nmatrix across assets; clustering is done hierarchically to achieve scalability.\nAn extended case study using an ERCOT-like system with nearly 500 solar and\nwind farms is used for illustration.\n"
    },
    {
        "paper_id": 2205.04743,
        "authors": "Chenrui Zhang",
        "title": "Deep learning based Chinese text sentiment mining and stock market\n  correlation research",
        "comments": "16 pages;6 figures;7 tables",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  We explore how to crawl financial forum data such as stock bars and combine\nthem with deep learning models for sentiment analysis. In this paper, we will\nuse the BERT model to train against the financial corpus and predict the SZSE\nComponent Index, and find that applying the BERT model to the financial corpus\nthrough the maximum information coefficient comparison study. The obtained\nsentiment features will be able to reflect the fluctuations in the stock market\nand help to improve the prediction accuracy effectively. Meanwhile, this paper\ncombines deep learning with financial text, in further exploring the mechanism\nof investor sentiment on stock market through deep learning method, which will\nbe beneficial for national regulators and policy departments to develop more\nreasonable policy guidelines for maintaining the stability of stock market.\n"
    },
    {
        "paper_id": 2205.05133,
        "authors": "Geoff Lindsell",
        "title": "Convergence of the financial value of weak information for a sequence of\n  discrete-time markets",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  We examine weak anticipations in discrete-time and continuous-time financial\nmarkets consisting of one risk-free asset and multiple risky assets, defining a\nminimal probability measure associated with the anticipation that does not\ndepend on the choice of a utility function. We then define the financial value\nof weak information in the discrete-time economies and show that these values\nconverge to the financial value of weak information in the continuous-time\neconomy in the case of a complete market.\n"
    },
    {
        "paper_id": 2205.05205,
        "authors": "Akhil Rao, Francesca Letizia",
        "title": "An integrated debris environment assessment model",
        "comments": "In 8th European Conference on Space Debris Proceedings",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-sa/4.0/",
        "abstract": "  Launch behaviors are a key determinant of the orbital environment. Physical\nand economic forces such as fragmentations and changing launch costs, or\npolicies like post-mission disposal (PMD) compliance requirements, will alter\nthe relative attractiveness of different orbits and lead operators to adjust\ntheir launch behaviors. However, integrating models of adaptive launch behavior\nwith models of the debris environment remains an open challenge. We present a\nstatistical framework for integrating theoretically-grounded models of launch\nbehavior with evolutionary models of the low-Earth orbit (LEO) environment. We\nimplement this framework using data on satellite launches, the orbital\nenvironment, launch vehicle prices, sectoral revenues, and government budgets\nover 2007-2020. The data are combined with a multi-shell and multi-species\nParticle-in-a-Box (PIB) model of the debris environment and a two-stage\nbudgeting model of commercial, civil government, and defense decisions to\nallocate new launches across orbital shells. We demonstrate the framework's\ncapabilities in three counterfactual scenarios: unexpected fragmentation events\nin highly-used regions, a sharp decrease in the cost of accessing lower parts\nof LEO, and increasing compliance with 25-year PMD guidelines. Substitution\nacross orbits based on their evolving characteristics and the behavior of other\noperators induces notable changes in the debris environment relative to models\nwithout behavioral channels.\n"
    },
    {
        "paper_id": 2205.0524,
        "authors": "Geoff Boeing, Carl Higgs, Shiqin Liu, Billie Giles-Corti, James F\n  Sallis, Ester Cerin, Melanie Lowe, Deepti Adlakha, Erica Hinckson, Anne\n  Vernez Moudon, Deborah Salvo, Marc A Adams, Ligia Vizeu Barrozo, Tamara\n  Bozovic, Xavier Delcl\\`os-Ali\\'o, Jan Dygr\\'yn, Sara Ferguson, Klaus Gebel,\n  Thanh Phuong Ho, Poh-Chin Lai, Joan Carles Martori, Kornsupha Nitvimol, Ana\n  Queralt, Jennifer D Roberts, Garba H Sambo, Jasper Schipperijn, David Vale,\n  Nico Van de Weghe, Guillem Vich, Jonathan Arundel",
        "title": "Using Open Data and Open-Source Software to Develop Spatial Indicators\n  of Urban Design and Transport Features for Achieving Healthy and Sustainable\n  Cities",
        "comments": null,
        "journal-ref": "The Lancet Global Health 10 (6), 907-918 (2022)",
        "doi": "10.1016/S2214-109X(22)00072-9",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Benchmarking and monitoring urban design and transport features is critical\nto achieving local and international health and sustainability goals. However,\nmost urban indicator frameworks use coarse spatial scales that only allow\nbetween-city comparisons or require expensive, technical, local spatial\nanalyses for within-city comparisons. This study developed a reusable\nopen-source urban indicator computational framework using open data to enable\nconsistent local and global comparative analyses. We demonstrate this framework\nby calculating spatial indicators - for 25 diverse cities in 19 countries - of\nurban design and transport features that support health and sustainability. We\nlink these indicators to cities' policy contexts and identify populations\nliving above and below critical thresholds for physical activity through\nwalking. Efforts to broaden participation in crowdsourcing data and to\ncalculate globally consistent indicators are essential for planning\nevidence-informed urban interventions, monitoring policy impacts, and learning\nlessons from peer cities to achieve health, equity, and sustainability goals.\n"
    },
    {
        "paper_id": 2205.05444,
        "authors": "Manudeep Bhuller, Philipp Eisenhauer and Moritz Mendel",
        "title": "Sequential Choices, Option Values, and the Returns to Education",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  Using detailed Norwegian data on earnings, education and work histories, we\nestimate a dynamic structural model of education and sector choices that\ncaptures rich life-cycle patterns by ability. We validate the model against\nvariation in education choices induced by a compulsory schooling reform. Our\napproach allows us to estimate the ex-ante returns to different education\ntracks across the life-cycle by individual ability and quantify the\ncontribution of option values. We find substantial heterogeneity in returns and\nestablish crucial roles for option values and re-enrollment in determining\neducation choices and the impact of schooling policies.\n"
    },
    {
        "paper_id": 2205.05489,
        "authors": "Frido Rolloos",
        "title": "Hull and White and Al\\`os type formulas for barrier options in\n  stochastic volatility models with nonzero correlation",
        "comments": "There is an error in the proof(s) of the main result(s)",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Two novel closed-form formulas for the price of barrier options in stochastic\nvolatility models with zero interest rate and dividend yield but nonzero\ncorrelation between the asset and its instantaneous volatility are derived. The\nfirst is a Hull and White type formula, and the second is a decomposition\nformula similar in form to the Al\\`os decomposition for vanilla options. A\nmodel-free approximation is also given.\n"
    },
    {
        "paper_id": 2205.056,
        "authors": "Ziheng Chen",
        "title": "RLOP: RL Methods in Option Pricing from a Mathematical Perspective",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Abstract In this work, we build two environments, namely the modified QLBS\nand RLOP models, from a mathematics perspective which enables RL methods in\noption pricing through replicating by portfolio. We implement the environment\nspecifications (the source code can be found at\nhttps://github.com/owen8877/RLOP), the learning algorithm, and agent\nparametrization by a neural network. The learned optimal hedging strategy is\ncompared against the BS prediction. The effect of various factors is considered\nand studied based on how they affect the optimal price and position.\n"
    },
    {
        "paper_id": 2205.05614,
        "authors": "Jay Cao, Jacky Chen, Soroush Farghadani, John Hull, Zissis Poulos,\n  Zeyu Wang, Jun Yuan",
        "title": "Gamma and Vega Hedging Using Deep Distributional Reinforcement Learning",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  We show how D4PG can be used in conjunction with quantile regression to\ndevelop a hedging strategy for a trader responsible for derivatives that arrive\nstochastically and depend on a single underlying asset. We assume that the\ntrader makes the portfolio delta neutral at the end of each day by taking a\nposition in the underlying asset. We focus on how trades in the options can be\nused to manage gamma and vega. The option trades are subject to transaction\ncosts. We consider three different objective functions. We reach conclusions on\nhow the optimal hedging strategy depends on the trader's objective function,\nthe level of transaction costs, and the maturity of the options used for\nhedging. We also investigate the robustness of the hedging strategy to the\nprocess assumed for the underlying asset.\n"
    },
    {
        "paper_id": 2205.05719,
        "authors": "Chenrui Zhang, Xinyi Wu, Hailu Deng, Huiwei Zhang",
        "title": "A time-varying study of Chinese investor sentiment, stock market\n  liquidity and volatility: Based on deep learning BERT model and TVP-VAR model",
        "comments": "25 pages, 7 figures, 8 tables, Funded by the National Student\n  Innovation and Entrepreneurship Training Program (Project No. 202110561076)",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  Based on the commentary data of the Shenzhen Stock Index bar on the EastMoney\nwebsite from January 1, 2018 to December 31, 2019. This paper extracts the\nembedded investor sentiment by using a deep learning BERT model and\ninvestigates the time-varying linkage between investment sentiment, stock\nmarket liquidity and volatility using a TVP-VAR model. The results show that\nthe impact of investor sentiment on stock market liquidity and volatility is\nstronger. Although the inverse effect is relatively small, it is more\npronounced with the state of the stock market. In all cases, the response is\nmore pronounced in the short term than in the medium to long term, and the\nimpact is asymmetric, with shocks stronger when the market is in a downward\nspiral.\n"
    },
    {
        "paper_id": 2205.05978,
        "authors": "E. Ruben van Beesten, Ole Kristian \\r{A}dnanes, H\\r{a}kon Morken\n  Linde, Paolo Pisciella, Asgeir Tomasgard",
        "title": "Welfare compensation in international transmission expansion planning\n  under uncertainty",
        "comments": "18 pages, 10 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In transmission expansion planning, situations can arise in which an\nexpansion plan that is optimal for the system as a whole is detrimental to a\nspecific country in terms of its expected economic welfare. If this country is\none of the countries hosting the planned capacity expansion, it has the power\nto veto the plan and thus, undermine the system-wide social optimum. To solve\nthis issue, welfare compensation mechanisms may be constructed that compensate\nsuffering countries and make them willing to participate in the expansion plan.\nIn the literature, welfare compensation mechanisms have been developed that\nwork in expectation. However, in a stochastic setting, even if the welfare\neffect after compensation is positive in expectation, countries might still be\nhesitant to accept the risk that the actual, realized welfare effect may be\nnegative in some scenarios.\n  In this paper we analyze welfare compensation mechanisms in a stochastic\nsetting. We consider two existing mechanisms, lump-sum payments and purchase\npower agreements, and we develop two novel mechanisms, based on the flow\nthrough the new transmission line and its economic value. Using a case study of\nthe Northern European power market, we investigate how well these mechanisms\nsucceed in mitigating risk for the countries involved. Using a theoretically\nideal model-based mechanism, we show that there is a significant potential for\nmitigating risk through welfare compensation mechanisms. Out of the four\npractical mechanisms we consider, our results indicate that a mechanism based\non the economic value of the new transmission line is most promising.\n"
    },
    {
        "paper_id": 2205.05984,
        "authors": "Mikhail Pomazanov",
        "title": "Method of indirect estimation of default probability dynamics for\n  industry-target segments according to the data of Bank of Russia",
        "comments": "8 pages, 2 tables, reported in Analytics for Management and Economics\n  Conference. September-December 2020 Higher School of Economics National\n  Research University, Saint Petersburg, Russia",
        "journal-ref": "AMEC Proc., pp. 70-76 (2020)",
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  A direct method for calculating default rates by industry and target\ncorporate segments is not possible given the lack of statistical data. The\nproposed paper considers a model for filtering the dynamics of the probability\nof default of corporate companies and other borrowers based on indirect data on\nthe dynamics of overdue debt supplied by the Bank of Russia. The model is based\non the equation of the balance of total and overdue debts, the missing links of\nthe corresponding time series are built using the Hodrick_Prescott filtering\nmethod. In retail lending segments (mortgage, consumer lending), default\nstatistics are available and supplied by Credit Bureaus. The presented method\nis validated on this statistic. Over a historical limited period, validation\nhas shown that the result is trustworthy. The resulting default probability\nseries are exogenous variables for macro_economic modelling of sectoral credit\nrisks.\n"
    },
    {
        "paper_id": 2205.05985,
        "authors": "Martina Halouskov\\'a (1), Daniel Sta\\v{s}ek (1), Mat\\'u\\v{s} Horv\\'ath\n  (1) ((1) Department of Finance, The Faculty of Economics and Administration,\n  Masaryk University)",
        "title": "The role of investor attention in global asset price variation during\n  the invasion of Ukraine",
        "comments": "This paper includes 17 pages, 3 figures and 8 tables",
        "journal-ref": null,
        "doi": "10.1016/j.frl.2022.103292",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We study the impact of event-specific attention indices -- based on Google\nTrends -- in predictive price variation models before and during the Russian\ninvasion of Ukraine in February 2022. We extend our analyses to the importance\nof geographical proximity and economic openness to Russia within 51 global\nequity markets. Our results demonstrate that 36 countries show significant\nattention to the conflict at the onset of and during the invasion, which helps\npredict volatility. We find that the impact of attention is more significant in\ncountries with a higher degree of economic openness to Russia and those nearer\nto it.\n"
    },
    {
        "paper_id": 2205.06161,
        "authors": "Samuel Baker, Pietro Biroli, Hans van Kippersluis, Stephanie von Hinke",
        "title": "Beyond Barker: Infant Mortality at Birth and Ischaemic Heart Disease in\n  Older Age",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-sa/4.0/",
        "abstract": "  Adverse conditions in early life can have consequential impacts on\nindividuals' health in older age. In one of the first papers on this topic,\nBarker and Osmond 1986 show a strong positive relationship between infant\nmortality rates in the 1920s and ischaemic heart disease in the 1970s. We go\n'beyond Barker', first by showing that this relationship is robust to the\ninclusion of local geographic area fixed effects, but not family fixed effects.\nSecond, we explore whether the average effects conceal underlying\nheterogeneity: we examine if the infant mortality effect offsets or reinforces\none's genetic predisposition for heart disease. We find considerable\nheterogeneity that is robust to within-area as well as within-family analyses.\nOur findings show that the effects of one's early life environments mainly\naffect individuals with the highest genetic risk for developing heart disease.\nPut differently, in areas with the lowest infant mortality rates, the effect of\none's genetic predisposition effectively vanishes. These findings suggest that\nadvantageous environments can cushion one's genetic risk of developing heart\ndisease.\n"
    },
    {
        "paper_id": 2205.06338,
        "authors": "Connor Oxenhorn",
        "title": "A Multivariate Hawkes Process Model for Stablecoin-Cryptocurrency\n  Depegging Event Dynamics",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Stablecoins, digital assets pegged to a specific currency or commodity value,\nare heavily involved in transactions of major cryptocurrencies. The effects of\ndeviations from their desired fixed values (depeggings) on the cryptocurrencies\nfor which they are frequently used in transactions are therefore of interest to\nstudy. We propose a model for this phenomenon using a multivariate\nmutually-exciting Hawkes process, and present a numerical example applying this\nmodel to Tether (USDT) and Bitcoin (BTC).\n"
    },
    {
        "paper_id": 2205.06434,
        "authors": "Tian Chen, Ruyi Liu and Zhen Wu",
        "title": "Continuous-time mean-variance portfolio selection under non-Markovian\n  regime-switching model with random horizon",
        "comments": "22 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
        "abstract": "  In this paper, we consider a continuous-time mean-variance portfolio\nselection with regime-switching and random horizon. Unlike previous works, the\ndynamic of assets are described by non-Markovian regime-switching models in the\nsense that all the market parameters are predictable with respect to the\nfiltration generated jointly by Markov chain and Brownian motion. We formulate\nthis problem as a constrained stochastic linear-quadratic optimal control\nproblem. The Markov chain is assumed to be independent of the Brownian motion.\nSo the market is incomplete. We derive closed-form expressions for both the\noptimal portfolios and the efficient frontier. All the results are different\nfrom those in the problem with fixed time horizon.\n"
    },
    {
        "paper_id": 2205.06572,
        "authors": "David Winkelmann, Matthias Ulrich, Michael R\\\"omer, Roland Langrock,\n  and Hermann Jahnke",
        "title": "Dynamic Stochastic Inventory Management in E-Grocery Retailing",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  E-grocery retailing enables ordering products online to be delivered at a\nfuture time slot chosen by the customer. This emerging field of business\nprovides retailers with large and comprehensive new data sets, yet creates\nseveral challenges for the inventory management process. For example, the risk\nof a single item's stock-out leading to a complete cancellation of the shopping\nprocess is higher in e-grocery than in traditional store retailing. As a\nconsequence, retailers aim at very high service level targets to provide\nsatisfactory customer service and to ensure long-term business growth. When\ndetermining replenishment order quantities, it is of crucial importance to\nprecisely account for the full uncertainty in the inventory process. This\nrequires predictive and prescriptive analytics to (1) estimate suitable\nunderlying probability distributions to represent the uncertainty caused by\nnon-stationary customer demand, shelf lives, and supply, and to (2) integrate\nthose forecasts into a comprehensive multi-period optimisation framework. In\nthis paper, we model this stochastic dynamic problem by a sequential decision\nprocess that allows us to avoid simplifying assumptions commonly made in the\nliterature, such as the focus on a single demand period. As the resulting\nproblem will typically be analytically intractable, we propose a stochastic\nlookahead policy incorporating Monte Carlo techniques to fully propagate the\nassociated uncertainties in order to derive replenishment order quantities.\nThis policy naturally integrates probabilistic forecasts and allows us to\nexplicitly derive the value of accounting for probabilistic information\ncompared to myopic or deterministic approaches in a simulation-based setting.\nIn addition, we evaluate our policy in a case study based on real-world data\nwhere underlying probability distributions are estimated from historical data\nand explanatory variables.\n"
    },
    {
        "paper_id": 2205.06673,
        "authors": "Vishal Kuber, Divakar Yadav, Arun Kr Yadav",
        "title": "Univariate and Multivariate LSTM Model for Short-Term Stock Market\n  Prediction",
        "comments": "24 pages, 20 figures, 8 tables",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Designing robust and accurate prediction models has been a viable research\narea since a long time. While proponents of a well-functioning market\npredictors believe that it is difficult to accurately predict market prices but\nmany scholars disagree. Robust and accurate prediction systems will not only be\nhelpful to the businesses but also to the individuals in making their financial\ninvestments. This paper presents an LSTM model with two different input\napproaches for predicting the short-term stock prices of two Indian companies,\nReliance Industries and Infosys Ltd. Ten years of historic data (2012-2021) is\ntaken from the yahoo finance website to carry out analysis of proposed\napproaches. In the first approach, closing prices of two selected companies are\ndirectly applied on univariate LSTM model. For the approach second, technical\nindicators values are calculated from the closing prices and then collectively\napplied on Multivariate LSTM model. Short term market behaviour for upcoming\ndays is evaluated. Experimental outcomes revel that approach one is useful to\ndetermine the future trend but multivariate LSTM model with technical\nindicators found to be useful in accurately predicting the future price\nbehaviours.\n"
    },
    {
        "paper_id": 2205.06675,
        "authors": "Chenrui Zhang",
        "title": "Research on the correlation between text emotion mining and stock market\n  based on deep learning",
        "comments": "16 pages, in Chinese language, 6 figures, 7 tables",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  This paper discusses how to crawl the data of financial forums such as stock\nbar, and conduct emotional analysis combined with the in-depth learning model.\nThis paper will use the Bert model to train the financial corpus and predict\nthe Shenzhen stock index. Through the comparative study of the maximal\ninformation coefficient (MIC), it is found that the emotional characteristics\nobtained by applying the BERT model to the financial corpus can be reflected in\nthe fluctuation of the stock market, which is conducive to effectively improve\nthe prediction accuracy. At the same time, this paper combines in-depth\nlearning with financial texts to further explore the impact mechanism of\ninvestor sentiment on the stock market through in-depth learning, which will\nhelp the national regulatory authorities and policy departments to formulate\nmore reasonable policies and guidelines for maintaining the stability of the\nstock market.\n"
    },
    {
        "paper_id": 2205.06677,
        "authors": "Maryam Zamani, Sander Paekivi, Philipp Meyer, Holger Kantz",
        "title": "Collective behavior of stock prices in the time of crisis as a response\n  to the external stimulus",
        "comments": "9 pages, 6 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We analyze the interaction between stock prices of big companies in the USA\nand Germany using Granger Causality. We claim that the increase in pair-wise\nGranger causality interaction between prices in the times of crisis is the\nconsequence of simultaneous response of the markets to the outside events or\nexternal stimulus that is considered as a common driver to all the stocks, not\na result of real causal predictability between the prices themselves. An\nalternative approach through recurrence analysis in single stock price series\nsupports this claim. The observed patterns in the price of stocks are modelled\nby adding a multiplicative exogenous term as the representative for external\nfactors to the geometric Brownian motion model for stock prices. Altogether, we\ncan detect and model the effects of the Great Recession as a consequence of the\nmortgage crisis in 2007/2008 as well as the impacts of the Covid out-break in\nearly 2020\n"
    },
    {
        "paper_id": 2205.06744,
        "authors": "Petri P. Karenlampi",
        "title": "Two strategies for boreal forestry with goodwill in capitalization",
        "comments": "15 pages, 15 equations, 9 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  Two strategies for boreal forestry with goodwill in estate capitalization are\nintroduced. A strategy focusing on Real Estate (RE) is financially superior to\nTimber Sales (TS). The feasibility of the RE requires the presence of forest\nland end users in the real estate market, like insurance companies or\ninvestment trusts, and the periodic boundary condition does not apply.\nCommercial thinnings do not enter the RE strategy in a stand-level discussion.\nHowever, they may appear in estates with a variable age structure and enable an\nextension of stand rotation times.\n"
    },
    {
        "paper_id": 2205.07009,
        "authors": "Alessandro Ferrari, Anna Rogantini Picco",
        "title": "Risk Sharing and the Adoption of the Euro",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper empirically evaluates whether adopting a common currency has\nchanged the level of consumption smoothing of euro area member states. We\nconstruct a counterfactual dataset of macroeconomic variables through the\nsynthetic control method. We then use the output variance decomposition of\nAsdrubali, Sorensen and Yosha (1996) on both the actual and the synthetic data\nto study if there has been a change in risk sharing and through which channels.\nWe find that the euro adoption has reduced risk sharing and consumption\nsmoothing. We further show that this reduction is mainly driven by the\nperiphery countries of the euro area who have experienced a decrease in risk\nsharing through private credit.\n"
    },
    {
        "paper_id": 2205.07022,
        "authors": "German Rodikov and Nino Antulov-Fantulin",
        "title": "Volatility-inspired $\\sigma$-LSTM cell",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Volatility models of price fluctuations are well studied in the econometrics\nliterature, with more than 50 years of theoretical and empirical findings. The\nrecent advancements in neural networks (NN) in the deep learning field have\nnaturally offered novel econometric modeling tools. However, there is still a\nlack of explainability and stylized knowledge about volatility modeling with\nneural networks; the use of stylized facts could help improve the performance\nof the NN for the volatility prediction task. In this paper, we investigate how\nthe knowledge about the \"physics\" of the volatility process can be used as an\ninductive bias to design or constrain a cell state of long short-term memory\n(LSTM) for volatility forecasting. We introduce a new type of $\\sigma$-LSTM\ncell with a stochastic processing layer, design its learning mechanism and show\ngood out-of-sample forecasting performance.\n"
    },
    {
        "paper_id": 2205.07077,
        "authors": "Zachary Feinstein and Birgit Rudloff",
        "title": "Deep Learning the Efficient Frontier of Convex Vector Optimization\n  Problems",
        "comments": null,
        "journal-ref": null,
        "doi": "10.1007/s10898-024-01408-x",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper, we design a neural network architecture to approximate the\nweakly efficient frontier of convex vector optimization problems (CVOP)\nsatisfying Slater's condition. The proposed machine learning methodology\nprovides both an inner and outer approximation of the weakly efficient\nfrontier, as well as an upper bound to the error at each approximated efficient\npoint. In numerical case studies we demonstrate that the proposed algorithm is\neffectively able to approximate the true weakly efficient frontier of CVOPs.\nThis remains true even for large problems (i.e., many objectives, variables,\nand constraints) and thus overcoming the curse of dimensionality.\n"
    },
    {
        "paper_id": 2205.07101,
        "authors": "Philipp Ratz",
        "title": "Nonparametric Value-at-Risk via Sieve Estimation",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Artificial Neural Networks (ANN) have been employed for a range of modelling\nand prediction tasks using financial data. However, evidence on their\npredictive performance, especially for time-series data, has been mixed.\nWhereas some applications find that ANNs provide better forecasts than more\ntraditional estimation techniques, others find that they barely outperform\nbasic benchmarks. The present article aims to provide guidance as to when the\nuse of ANNs might result in better results in a general setting. We propose a\nflexible nonparametric model and extend existing theoretical results for the\nrate of convergence to include the popular Rectified Linear Unit (ReLU)\nactivation function and compare the rate to other nonparametric estimators.\nFinite sample properties are then studied with the help of Monte-Carlo\nsimulations to provide further guidance. An application to estimate the\nValue-at-Risk of portfolios of varying sizes is also considered to show the\npractical implications.\n"
    },
    {
        "paper_id": 2205.07256,
        "authors": "Victor Olkhov",
        "title": "Market-Based Asset Price Probability",
        "comments": "18 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We consider the randomness of market trade values and volumes as the origin\nof asset price stochasticity. We define the first four market-based price\nstatistical moments that depend on statistical moments and correlations of\nmarket trade values and volumes. Market-based price statistical moments\ncoincide with conventional frequency-based ones if all trade volumes are\nconstant during the time averaging interval. We present approximations of\nmarket-based price probability by a finite number of price statistical moments.\nWe consider the consequences of the use of market-based price statistical\nmoments for asset-pricing models and Value-at-Risk. We show that the use of\nvolume weighted average price results in zero price-volume correlations. We\nderive market-based correlations between price and squares of volume and\nbetween squares of price and volume. To forecast market-based price volatility\nat horizon T one should predict the first two statistical moments of market\ntrade values and volumes and their correlations at the same horizon T.\n"
    },
    {
        "paper_id": 2205.07334,
        "authors": "Eduardo Ramos-P\\'erez, Pablo J. Alonso-Gonz\\'alez and Jos\\'e Javier\n  N\\'u\\~nez-Vel\\'azquez",
        "title": "Mack-Net model: Blending Mack's model with Recurrent Neural Networks",
        "comments": null,
        "journal-ref": "Expert Systems with Applications. Volume 201, 1 September 2022,\n  117146",
        "doi": "10.1016/j.eswa.2022.117146",
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  In general insurance companies, a correct estimation of liabilities plays a\nkey role due to its impact on management and investing decisions. Since the\nFinancial Crisis of 2007-2008 and the strengthening of regulation, the focus is\nnot only on the total reserve but also on its variability, which is an\nindicator of the risk assumed by the company. Thus, measures that relate\nprofitability with risk are crucial in order to understand the financial\nposition of insurance firms. Taking advantage of the increasing computational\npower, this paper introduces a stochastic reserving model whose aim is to\nimprove the performance of the traditional Mack's reserving model by applying\nan ensemble of Recurrent Neural Networks. The results demonstrate that blending\ntraditional reserving models with deep and machine learning techniques leads to\na more accurate assessment of general insurance liabilities.\n"
    },
    {
        "paper_id": 2205.07385,
        "authors": "Emilio Said",
        "title": "Market Impact: Empirical Evidence, Theory and Practice",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We propose a theory of the market impact of metaorders based on a\ncoarse-grained approach where the microscopic details of supply and demand is\nreplaced by a single parameter $\\rho \\in [0,+\\infty]$ shaping the supply-demand\nequilibrium and the market impact process during the execution of the\nmetaorder. Our model provides an unified explanation of most of the empirical\nobservations that have been reported and establishes a strong connection\nbetween the excess volatility puzzle and the order-driven view of the markets\nthrough the square-root law.\n"
    },
    {
        "paper_id": 2205.07486,
        "authors": "Ratul Das Chaudhury, C. Matthew Leister, Birendra Rai",
        "title": "Influencing a Polarized and Connected Legislature",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
        "abstract": "  When can an interest group exploit polarization between political parties to\nits advantage? Building upon Battaglini and Patacchini (2018), we study a model\nwhere an interest group credibly promises payments to legislators conditional\non voting for its preferred policy. A legislator can be directly susceptible to\nother legislators and value voting like them. The overall pattern of\ninter-legislator susceptibility determines the relative influence of individual\nlegislators, and therefore the relative influence of the parties. We show that\nhigh levels of ideological or affective polarization are more likely to benefit\nthe interest group when the party ideologically aligned with the interest group\nis relatively more influential. However, ideological and affective polarization\noperate in different ways. The influence of legislators is independent of\nideological polarization. In contrast, affective polarization effectively\ncreates negative links between legislators across parties, and thus modifies\nthe relative influence of individual legislators and parties.\n"
    },
    {
        "paper_id": 2205.07563,
        "authors": "Aleksejus Kononovicius, Rytis Kazakevi\\v{c}ius, Bronislovas Kaulakys",
        "title": "Resemblance of the power-law scaling behavior of a non-Markovian and\n  nonlinear point processes",
        "comments": "10 pages, 4 figures",
        "journal-ref": null,
        "doi": "10.1016/j.chaos.2022.112508",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We analyze the statistical properties of a temporal point process driven by a\nconfined fractional Brownian motion. The event count distribution and power\nspectral density of this non--Markovian point process exhibit power--law\nscaling. We show that a nonlinear Markovian point process can reproduce the\nsame scaling behavior. This result indicates a possible link between\nnonlinearity and apparent non--Markovian behavior.\n"
    },
    {
        "paper_id": 2205.07677,
        "authors": "Giacomo Vaccario, Luca Verginer, Antonios Garas, Mario V. Tomasello\n  and Frank Schweitzer",
        "title": "Network embeddedness indicates the innovation potential of firms",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Firms' innovation potential depends on their position in the R&D network. But\ndetails on this relation remain unclear because measures to quantify network\nembeddedness have been controversially discussed. We propose and validate a new\nmeasure, coreness, obtained from the weighted k-core decomposition of the R&D\nnetwork. Using data on R&D alliances, we analyse the change of coreness for\n14,000 firms over 25 years and patenting activity. A regression analysis\ndemonstrates that coreness explains firms' R&D output by predicting future\npatenting.\n"
    },
    {
        "paper_id": 2205.07742,
        "authors": "George MacKerron and Nattavudh Powdthavee",
        "title": "Predicting Emotional Volatility Using 41,000 Participants in the United\n  Kingdom",
        "comments": "30 pages, 1 figure, 2 tables",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Emotional volatility is a human universal. Yet there has been no large-scale\nscientific study of predictors of that phenomenon. Building from previous\nworks, which had been ad hoc and based on tiny samples, this paper reports the\nfirst large-scale estimation of volatility in human emotional experiences. Our\nstudy draws from a large sample of intrapersonal variation in moment-to-moment\nhappiness from over three million observations by 41,023 UK individuals.\nHolding other things constant, we show that emotional volatility is highest\namong women with children, the separated, the poor, and the young. Women\nwithout children report substantially greater emotional volatility than men\nwith and without children. For any given rate of volatility, women with\nchildren also experience more frequent extreme emotional lows than any other\nsocio-demographic group. Our results, which are robust to different\nspecification tests, enable researchers and policymakers to quantify and\nprioritise different determinants of intrapersonal variability in human\nemotions.\n"
    },
    {
        "paper_id": 2205.08042,
        "authors": "Hirokuni Iiboshi and Daisuke Ozaki",
        "title": "The Impact of the Social Security Reforms on Welfare: Who benefits and\n  Who loses across Generations, Gender, and Employment Type?",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
        "abstract": "  We quantitatively explore the impact of social security reforms in Japan,\nwhich is facing rapid aging and the highest government debt among developed\ncountries, using an overlapping generations model with four types of agents\ndistinguished by gender and employment type. We find that introducing social\nsecurity reforms without extending the retirement age raises the welfare of\nfuture generations, while reforms with rising copayment rates for medical and\nlong-term care expenditures, in particular, significantly lowers the welfare of\nlow-income groups (females and part-timers) of the current retired and working\ngenerations. In contrast, reforms reducing the pension replacement rate lead to\na greater decline in the welfare of full-timers. The combination of these\nreforms and the extension of the retirement age is expected to improve the\nwelfare of the current working generations by 2--9 % over the level without\nreforms.\n"
    },
    {
        "paper_id": 2205.08104,
        "authors": "Fupeng Sun, Yanwei Sun, Chiwei Yan, Li Jin",
        "title": "Restricting Entries to All-Pay Contests",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We study an all-pay contest where players with low abilities are filtered\nprior to the round of competing for prizes. These are often practiced due to\nlimited resources or to enhance the competitiveness of the contest. We consider\na setting where the designer admits a certain number of top players into the\ncontest. The players admitted into the contest update their beliefs about their\nopponents based on the signal that their abilities are among the top. We find\nthat their posterior beliefs, even with IID priors, are correlated and depend\non players' private abilities, representing a unique feature of this game. We\nexplicitly characterize the symmetric and unique Bayesian equilibrium strategy.\nWe find that each admitted player's equilibrium effort is in general not\nmonotone with the number of admitted players. Despite this non-monotonicity,\nsurprisingly, all players exert their highest efforts when all players are\nadmitted. This result holds generally -- it is true under any ranking-based\nprize structure, ability distribution, and cost function. We also discuss a\ntwo-stage extension where players with top first-stage efforts can proceed to\nthe second stage competing for prizes.\n"
    },
    {
        "paper_id": 2205.08112,
        "authors": "Laurence Barry and Arthur Charpentier",
        "title": "The Fairness of Machine Learning in Insurance: New Rags for an Old Man?",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Since the beginning of their history, insurers have been known to use data to\nclassify and price risks. As such, they were confronted early on with the\nproblem of fairness and discrimination associated with data. This issue is\nbecoming increasingly important with access to more granular and behavioural\ndata, and is evolving to reflect current technologies and societal concerns. By\nlooking into earlier debates on discrimination, we show that some algorithmic\nbiases are a renewed version of older ones, while others show a reversal of the\nprevious order. Paradoxically, while the insurance practice has not deeply\nchanged nor are most of these biases new, the machine learning era still deeply\nshakes the conception of insurance fairness.\n"
    },
    {
        "paper_id": 2205.08435,
        "authors": "Wing Fung Chong, Runhuan Feng, Hins Hu, Linfeng Zhang",
        "title": "Cyber Risk Assessment for Capital Management",
        "comments": "This paper was first presented on July 5, 2021, at the 24th\n  International Congress on Insurance: Mathematics and Economics",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Cyber risk is an omnipresent risk in the increasingly digitized world that is\nknown to be difficult to manage. This paper proposes a two-pillar cyber risk\nmanagement framework to address such difficulty. The first pillar, cyber risk\nassessment, blends the frequency-severity model in insurance with the cascade\nmodel in cybersecurity, to capture the unique feature of cyber risk. The second\npillar, cyber capital management, provides informative decision-making on a\nbalanced cyber risk management strategy, which includes cybersecurity\ninvestments, insurance coverage, and reserves. This framework is demonstrated\nby a case study based on a historical cyber incident dataset, which shows that\na comprehensive cost-benefit analysis is necessary for a budget-constrained\ncompany with competing objectives for cyber risk management. Sensitivity\nanalysis also illustrates that the best strategy depends on various factors,\nsuch as the amount of cybersecurity investments and the effectiveness of\ncybersecurity controls.\n"
    },
    {
        "paper_id": 2205.08569,
        "authors": "Sharad Agarwal and Marie Vasek",
        "title": "Investigating the concentration of High Yield Investment Programs in the\n  United Kingdom",
        "comments": null,
        "journal-ref": null,
        "doi": "10.1109/EuroSPW55150.2022.00017",
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Ponzi schemes that offer absurdly high rates of return by relying on more and\nmore people paying into the scheme have been documented since at least the\nmid-1800s. Ponzi schemes have shifted online in the Internet age, and some are\nre-branded as HYIPs or High Yield Investment Programs. This paper focuses on\nunderstanding HYIPs' continuous presence and presents various possible reasons\nbehind their existence in today's world. A look into the countries where these\nschemes purport to exist, we find that 62.89% of all collected HYIPs claim to\nbe in the United Kingdom (UK), and a further 55.56% are officially registered\nin the UK as a 'limited company' with a registration number provided by the UK\nCompanies House, a UK agency that registers companies. We investigate other\nfactors influencing these schemes, including the HYIPs' social media platforms\nand payment processors. The lifetime of the HYIPs helps to understand the\nsuccess/failure of the investment schemes and helps indicate the schemes that\ncould attract more investors. Using Cox proportional regression analysis, we\nfind that having a valid UK address significantly affects the lifetime of an\nHYIP.\n"
    },
    {
        "paper_id": 2205.08584,
        "authors": "Kirby Nielsen and Luca Rigotti",
        "title": "Revealed Incomplete Preferences",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  We elicit incomplete preferences over monetary gambles with subjective\nuncertainty. Subjects rank gambles, and these rankings are used to estimate\npreferences; payments are based on estimated preferences. About 40\\% of\nsubjects express incompleteness, but they do so infrequently. Incompleteness is\nsimilar for individuals with precise and imprecise beliefs, and in an\nenvironment with objective uncertainty, suggesting that it results from\nimprecise tastes more than imprecise beliefs. When we force subjects to choose,\nwe observe more inconsistencies and preference reversals. Evidence suggests\nthere is incompleteness that is indirectly revealed -- in up to 98\\% of\nsubjects -- in addition to what we directly measure.\n"
    },
    {
        "paper_id": 2205.08614,
        "authors": "Abdelali Gabih, Hakam Kondakji, Ralf Wunderlich",
        "title": "Well Posedness of Utility Maximization Problems Under Partial\n  Information in a Market with Gaussian Drift",
        "comments": "24 pages, 1 figure",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper investigates well posedness of utility maximization problems for\nfinancial markets where stock returns depend on a hidden Gaussian mean\nreverting drift process. Since that process is potentially unbounded, well\nposedness cannot be guaranteed for utility functions which are not bounded from\nabove. For power utility with relative risk aversion smaller than that of\nlog-utility this leads to restrictions on the choice of model parameters such\nas the investment horizon and parameters controlling the variance of the asset\nprice and drift processes. We derive sufficient conditions to the model\nparameters leading to bounded maximum expected utility of terminal wealth for\nmodels with full and partial information.\n"
    },
    {
        "paper_id": 2205.08743,
        "authors": "Y. Zhang, Z. Jin, J. Wei, G. Yin",
        "title": "Mean-variance portfolio selection with dynamic attention behavior in a\n  hidden Markov model",
        "comments": "15 pages, 4 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper, we study closed-loop equilibrium strategies for mean-variance\nportfolio selection problem in a hidden Markov model with dynamic attention\nbehavior. In addition to the investment strategy, the investor's attention to\nnews is introduced as a control of the accuracy of the news signal process. The\nobjective is to find equilibrium strategies by numerically solving an extended\nHJB equation by using Markov chain approximation method. An iterative algorithm\nis constructed and its convergence is established. Numerical examples are also\nprovided to illustrate the results.\n"
    },
    {
        "paper_id": 2205.0885,
        "authors": "Carole Bernard, Silvana M. Pesenti, Steven Vanduffel",
        "title": "Robust Distortion Risk Measures",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
        "abstract": "  The robustness of risk measures to changes in underlying loss distributions\n(distributional uncertainty) is of crucial importance in making well-informed\ndecisions. In this paper, we quantify, for the class of distortion risk\nmeasures with an absolutely continuous distortion function, its robustness to\ndistributional uncertainty by deriving its largest (smallest) value when the\nunderlying loss distribution has a known mean and variance and, furthermore,\nlies within a ball - specified through the Wasserstein distance - around a\nreference distribution. We employ the technique of isotonic projections to\nprovide for these distortion risk measures a complete characterisation of sharp\nbounds on their value, and we obtain quasi-explicit bounds in the case of\nValue-at-Risk and Range-Value-at-Risk. We extend our results to account for\nuncertainty in the first two moments and provide applications to portfolio\noptimisation and to model risk assessment.\n"
    },
    {
        "paper_id": 2205.08874,
        "authors": "Eszter Moln\\'ar and D\\'enes Csala",
        "title": "Topology-dependence of propagation mechanisms in the production network",
        "comments": "11 pages, 3 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The topology of production networks determines the propagation mechanisms of\nlocal shocks and thus the co-movement of industries. As a result, we need a\nmore precisely defined production network to model economic growth accurately.\nIn this study, we analyse Leontief's input-output model from a network theory\nperspective, aiming to construct a production network in such a way that it\nallows the most accurate modelling of the propagation mechanisms of changes\nthat generate industry growth. We do this by revisiting a prevalent threshold\nin the literature that determines industry-industry interdependence. Our\nhypothesis is that changing the threshold changes the topological structure of\nthe network and the core industries to a large extent. This is significant,\nbecause if the production network topology is not precisely defined, the\nresulting internal propagation mechanisms will be distorted, and thus industry\ngrowth modelling will not be accurate. We prove our hypothesis by examining the\nnetwork topology, and centrality metrics under different thresholds on a\nnetwork derived from the US input-output accounts data for 2007 and 2012.\n"
    },
    {
        "paper_id": 2205.08879,
        "authors": "Giuseppe Calafiore, Giulia Fracastoro, and Anton V. Proskurnikov",
        "title": "Control of Dynamic Financial Networks (The Extended Version)",
        "comments": null,
        "journal-ref": "IEEE Control Systems Letters, 6, pp. 3206 - 3211, 2022",
        "doi": "10.1109/LCSYS.2022.3182847",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The current global financial system forms a highly interconnected network\nwhere a default in one of its nodes can propagate to many other nodes, causing\na catastrophic avalanche effect. In this paper we consider the problem of\nreducing the financial contagion by introducing some targeted interventions\nthat can mitigate the cascaded failure effects. We consider a multi-step\ndynamic model of clearing payments and introduce an external control term that\nrepresents corrective cash injections made by a ruling authority. The proposed\ncontrol model can be cast and efficiently solved as a linear program. We show\nvia numerical examples that the proposed approach can significantly reduce the\ndefault propagation by applying small targeted cash injections.\n"
    },
    {
        "paper_id": 2205.08904,
        "authors": "Lioba Heimbach, Eric Schertenleib and Roger Wattenhofer",
        "title": "Risks and Returns of Uniswap V3 Liquidity Providers",
        "comments": null,
        "journal-ref": null,
        "doi": "10.1145/3558535.3559772",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Trade execution on Decentralized Exchanges (DEXes) is automatic and does not\nrequire individual buy and sell orders to be matched. Instead, liquidity\naggregated in pools from individual liquidity providers enables trading between\ncryptocurrencies. The largest DEX measured by trading volume, Uniswap V3,\npromises a DEX design optimized for capital efficiency. However, Uniswap V3\nrequires far more decisions from liquidity providers than previous DEX designs.\n  In this work, we develop a theoretical model to illustrate the choices faced\nby Uniswap V3 liquidity providers and their implications. Our model suggests\nthat providing liquidity on Uniswap V3 is highly complex and requires many\nconsiderations from a user. Our supporting data analysis of the risks and\nreturns of real Uniswap V3 liquidity providers underlines that liquidity\nproviding in Uniswap V3 is incredibly complicated, and performances can vary\nwildly. While there are simple and profitable strategies for liquidity\nproviders in liquidity pools characterized by negligible price volatilities,\nthese strategies only yield modest returns. Instead, significant returns can\nonly be obtained by accepting increased financial risks and at the cost of\nactive management. Thus, providing liquidity has become a game reserved for\nsophisticated players with the introduction of Uniswap V3, where retail traders\ndo not stand a chance.\n"
    },
    {
        "paper_id": 2205.08913,
        "authors": "Dian Yu, Jianjun Gao, Weiping Wu, Zizhuo Wang",
        "title": "Price Interpretability of Prediction Markets: A Convergence Analysis",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Prediction markets are long known for prediction accuracy. This study\nsystematically explores the fundamental properties of prediction markets,\naddressing questions about their information aggregation process and the\nfactors contributing to their remarkable efficacy. We propose a novel\nmultivariate utility (MU) based mechanism that unifies several existing\nautomated market-making schemes. Using this mechanism, we establish the\nconvergence results for markets comprised of risk-averse traders who have\nheterogeneous beliefs and repeatedly interact with the market maker. We\ndemonstrate that the resulting limiting wealth distribution aligns with the\nPareto efficient frontier defined by the utilities of all market participants.\nWith the help of this result, we establish analytical and numerical results for\nthe limiting price in different market models. Specifically, we show that the\nlimiting price converges to the geometric mean of agent beliefs in exponential\nutility-based markets. In risk-measure-based markets, we construct a family of\nrisk measures that satisfy the convergence criteria and prove that the price\ncan converge to a unique level represented by the weighted power mean of agent\nbeliefs. In broader markets with Constant Relative Risk Aversion (CRRA)\nutilities, we reveal that the limiting price can be characterized by systems of\nequations that encapsulate agent beliefs, risk parameters, and wealth. Despite\nthe potential impact of traders' trading sequences on the limiting price, we\nestablish a price invariance result for markets with a large trader population.\nUsing this result, we propose an efficient approximation scheme for the\nlimiting price.\n"
    },
    {
        "paper_id": 2205.08936,
        "authors": "Junshu Jiang, Thomas Dierckx, Duxiang Xiao, Wim Schoutens",
        "title": "Market Making via Reinforcement Learning in China Commodity Market",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Market makers play an essential role in financial markets. A successful\nmarket maker should control inventory and adverse selection risks and provide\nliquidity to the market. As an important methodology in control problems,\nReinforcement Learning enjoys the advantage of data-driven and less rigid\nassumptions, receiving great attention in the market-making field since 2018.\nHowever, although the China Commodity market has the biggest trading volume on\nagricultural products, nonferrous metals, and some other sectors, the study of\napplying RL to Market Making in China market is still rare. In this thesis, we\ntry to fill the gap. Our contribution is threefold: We develop the Automatic\nTrading System and verify the feasibility of applying Reinforcement Learning in\nthe China Commodity market. Also, we probe the agent's behavior by analyzing\nhow it reacts to different environmental conditions.\n"
    },
    {
        "paper_id": 2205.08996,
        "authors": "Quang Dang Nguyen, Mikhail Prokopenko",
        "title": "A general framework for optimising cost-effectiveness of pandemic\n  response under partial intervention measures",
        "comments": null,
        "journal-ref": "Scientific Reports 12, 19482 (2022)",
        "doi": "10.1038/s41598-022-23668-x",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The COVID-19 pandemic created enormous public health and socioeconomic\nchallenges. The health effects of vaccination and non-pharmaceutical\ninterventions (NPIs) were often contrasted with significant social and economic\ncosts. We describe a general framework aimed to derive adaptive cost-effective\ninterventions, adequate for both recent and emerging pandemic threats. We also\nquantify the net health benefits and propose a reinforcement learning approach\nto optimise adaptive NPIs. The approach utilises an agent-based model\nsimulating pandemic responses in Australia, and accounts for a heterogeneous\npopulation with variable levels of compliance fluctuating over time and across\nindividuals. Our analysis shows that a significant net health benefit may be\nattained by adaptive NPIs formed by partial social distancing measures, coupled\nwith moderate levels of the society's willingness to pay for health gains\n(health losses averted). We demonstrate that a socially acceptable balance\nbetween health effects and incurred economic costs is achievable over a long\nterm, despite possible early setbacks.\n"
    },
    {
        "paper_id": 2205.09066,
        "authors": "Mario Kendziorski, Leonard G\\\"oke, Christian von Hirschhausen, Claudia\n  Kemfert, Elmar Zozmann",
        "title": "Centralized and decentral approaches to succeed the 100% energiewende in\n  Germany in the European context: A model-based analysis of generation,\n  network, and storage investments",
        "comments": null,
        "journal-ref": null,
        "doi": "10.1016/j.enpol.2022.113039",
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  In this paper, we explore centralized and more decentral approaches to\nsucceed the energiewende in Germany, in the European context. We use the AnyMOD\nframework to model a future renewable-based European energy system, based on a\ntechno-economic optimization, i.e. cost minimization with given demand,\nincluding both investment and the subsequent dispatch of capacity. The model\nincludes 29 regions for European countries, and 38 NUTS-2 regions in Germany.\nFirst the entire energy system on the European level is optimized. Based on\nthese results, the electricity system for the German regions is optimized to\nachieve great regional detail to analyse spatial effects. The model allows a\ncomparison between a stylized central scenario with high amounts of wind\noffshore deployed, and a decentral scenario using mainly the existing grid, and\nthus relying more on local capacities. The results reveal that the cost for the\nsecond optimization of these two scenarios are about the same: The central\nscenario is characterized by network expansion in order to transport the\nelectricity from the wind offshore sites, whereas the decentral scenario leads\nto more photovoltaic and battery deployment closer to the areas with a high\ndemand for energy. A scenarios with higher energy efficiency and lower demand\nprojections lead to a significant reduction of investment requirements, and to\ndifferent localizations thereof.\n"
    },
    {
        "paper_id": 2205.09179,
        "authors": "\\v{S}tefan Ly\\'ocsa, Tom\\'a\\v{s} Pl\\'ihal",
        "title": "Russia's Ruble during the onset of the Russian invasion of Ukraine in\n  early 2022: The role of implied volatility and attention",
        "comments": "To be published in Finance Research Letters",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  The onset of the Russo-Ukrainian crisis has led to the rapid depreciation of\nthe Russian ruble. In this study, we model intraday price fluctuations of the\nUSD/RUB and the EUR/RUB exchange rates from the $1^{st}$ of December 2021 to\nthe $7^{th}$ of March 2022. Our approach is novel in that instead of using\ndaily (low-frequency) measures of attention and investor's expectations, we use\nintraday (high-frequency) data: google searches and implied volatility to proxy\ninvestor's attention and expectations. We show that both approaches are useful\nin predicting intraday price fluctuations of the two exchange rates, although\nimplied volatility encompasses intraday attention.\n"
    },
    {
        "paper_id": 2205.09337,
        "authors": "Marc Andreas Schmitt",
        "title": "Deep Learning in Business Analytics: A Clash of Expectations and Reality",
        "comments": "Submitted to the International Journal of Information Management Data\n  Insights, 21 pages, 4 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  Our fast-paced digital economy shaped by global competition requires\nincreased data-driven decision-making based on artificial intelligence (AI) and\nmachine learning (ML). The benefits of deep learning (DL) are manifold, but it\ncomes with limitations that have - so far - interfered with widespread industry\nadoption. This paper explains why DL - despite its popularity - has\ndifficulties speeding up its adoption within business analytics. It is shown -\nby a mixture of content analysis and empirical study - that the adoption of\ndeep learning is not only affected by computational complexity, lacking big\ndata architecture, lack of transparency (black-box), and skill shortage, but\nalso by the fact that DL does not outperform traditional ML models in the case\nof structured datasets with fixed-length feature vectors. Deep learning should\nbe regarded as a powerful addition to the existing body of ML models instead of\na one size fits all solution.\n"
    },
    {
        "paper_id": 2205.09508,
        "authors": "Maysa M. Garcia de Macedo and Wyatt Clarke and Eli Lucherini and Tyler\n  Baldwin and Dilermando Queiroz Neto and Rogerio de Paula and Subhro Das",
        "title": "Practical Skills Demand Forecasting via Representation Learning of\n  Temporal Dynamics",
        "comments": "15 pages, 5th AAAI/ACM Conference on AI, Ethics, and Society",
        "journal-ref": null,
        "doi": "10.1145/3514094.3534183",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Rapid technological innovation threatens to leave much of the global\nworkforce behind. Today's economy juxtaposes white-hot demand for skilled labor\nagainst stagnant employment prospects for workers unprepared to participate in\na digital economy. It is a moment of peril and opportunity for every country,\nwith outcomes measured in long-term capital allocation and the life\nsatisfaction of billions of workers. To meet the moment, governments and\nmarkets must find ways to quicken the rate at which the supply of skills reacts\nto changes in demand. More fully and quickly understanding labor market\nintelligence is one route. In this work, we explore the utility of time series\nforecasts to enhance the value of skill demand data gathered from online job\nadvertisements. This paper presents a pipeline which makes one-shot multi-step\nforecasts into the future using a decade of monthly skill demand observations\nbased on a set of recurrent neural network methods. We compare the performance\nof a multivariate model versus a univariate one, analyze how correlation\nbetween skills can influence multivariate model results, and present\npredictions of demand for a selection of skills practiced by workers in the\ninformation technology industry.\n"
    },
    {
        "paper_id": 2205.09649,
        "authors": "Matthew Olckers, Alicia Vidler, Toby Walsh",
        "title": "What Type of Explanation Do Rejected Job Applicants Want? Implications\n  for Explainable AI",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Rejected job applicants seldom receive explanations from employers.\nTechniques from Explainable AI (XAI) could provide explanations at scale.\nAlthough XAI researchers have developed many different types of explanations,\nwe know little about the type of explanations job applicants want. We use a\nsurvey of recent job applicants to fill this gap. Our survey generates three\nmain insights. First, the current norm of, at most, generic feedback frustrates\napplicants. Second, applicants feel the employer has an obligation to provide\nan explanation. Third, job applicants want to know why they were unsuccessful\nand how to improve.\n"
    },
    {
        "paper_id": 2205.09815,
        "authors": "William Lefebvre and Gr\\'egoire Loeper and Huy\\^en Pham",
        "title": "Differential learning methods for solving fully nonlinear PDEs",
        "comments": "47 pages, 18 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  We propose machine learning methods for solving fully nonlinear partial\ndifferential equations (PDEs) with convex Hamiltonian. Our algorithms are\nconducted in two steps. First the PDE is rewritten in its dual stochastic\ncontrol representation form, and the corresponding optimal feedback control is\nestimated using a neural network. Next, three different methods are presented\nto approximate the associated value function, i.e., the solution of the initial\nPDE, on the entire space-time domain of interest. The proposed deep learning\nalgorithms rely on various loss functions obtained either from regression or\npathwise versions of the martingale representation and its differential\nrelation, and compute simultaneously the solution and its derivatives. Compared\nto existing methods, the addition of a differential loss function associated to\nthe gradient, and augmented training sets with Malliavin derivatives of the\nforward process, yields a better estimation of the PDE's solution derivatives,\nin particular of the second derivative, which is usually difficult to\napproximate. Furthermore, we leverage our methods to design algorithms for\nsolving families of PDEs when varying terminal condition (e.g. option payoff in\nthe context of mathematical finance) by means of the class of DeepOnet neural\nnetworks aiming to approximate functional operators. Numerical tests illustrate\nthe accuracy of our methods on the resolution of a fully nonlinear PDE\nassociated to the pricing of options with linear market impact, and on the\nMerton portfolio selection problem.\n"
    },
    {
        "paper_id": 2205.0989,
        "authors": "Estelle Sterrett, Waylon Jepsen, Evan Kim",
        "title": "Replicating Portfolios: Constructing Permissionless Derivatives",
        "comments": "18 pages, 4 Figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-sa/4.0/",
        "abstract": "  The current design space of derivatives in Decentralized Finance (DeFi)\nrelies heavily on oracle systems. Replicating market makers (RMMs) provide a\nmechanism for converting specific payoff functions to an associated Constant\nFunction Market Makers (CFMMs). We leverage RMMs to replicate the approximate\npayoff of a Black-Scholes covered call option. RMM-01 is the first\nimplementation of an on-chain expiring option mechanism that relies on\narbitrage rather than an external oracle for price. We provide frameworks for\nderivative instruments and structured products achievable on-chain without\nrelying on oracles. We construct long and binary options and briefly discuss\nperpetual covered call strategies commonly referred to as \"theta vaults.\"\nMoreover, we introduce a procedure to eliminate liquidation risk in lending\nmarkets. The results suggest that CFMMs are essential for structured product\ndesign with minimized trust dependencies.\n"
    },
    {
        "paper_id": 2205.102,
        "authors": "Christophe Hurlin, Christophe P\\'erignon, and S\\'ebastien Saurin",
        "title": "The Fairness of Credit Scoring Models",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  In credit markets, screening algorithms aim to discriminate between good-type\nand bad-type borrowers. However, when doing so, they can also discriminate\nbetween individuals sharing a protected attribute (e.g. gender, age, racial\norigin) and the rest of the population. This can be unintentional and originate\nfrom the training dataset or from the model itself. We show how to formally\ntest the algorithmic fairness of scoring models and how to identify the\nvariables responsible for any lack of fairness. We then use these variables to\noptimize the fairness-performance trade-off. Our framework provides guidance on\nhow algorithmic fairness can be monitored by lenders, controlled by their\nregulators, improved for the benefit of protected groups, while still\nmaintaining a high level of forecasting accuracy.\n"
    },
    {
        "paper_id": 2205.10535,
        "authors": "Marc Schmitt",
        "title": "Deep Learning vs. Gradient Boosting: Benchmarking state-of-the-art\n  machine learning algorithms for credit scoring",
        "comments": "Submitted for publication in International Journal of Information\n  Management",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  Artificial intelligence (AI) and machine learning (ML) have become vital to\nremain competitive for financial services companies around the globe. The two\nmodels currently competing for the pole position in credit risk management are\ndeep learning (DL) and gradient boosting machines (GBM). This paper benchmarked\nthose two algorithms in the context of credit scoring using three distinct\ndatasets with different features to account for the reality that model\nchoice/power is often dependent on the underlying characteristics of the\ndataset. The experiment has shown that GBM tends to be more powerful than DL\nand has also the advantage of speed due to lower computational requirements.\nThis makes GBM the winner and choice for credit scoring. However, it was also\nshown that the outperformance of GBM is not always guaranteed and ultimately\nthe concrete problem scenario or dataset will determine the final model choice.\nOverall, based on this study both algorithms can be considered state-of-the-art\nfor binary classification tasks on structured datasets, while GBM should be the\ngo-to solution for most problem scenarios due to easier use, significantly\nfaster training time, and superior accuracy.\n"
    },
    {
        "paper_id": 2205.1054,
        "authors": "Jaan Masso and Amaresh K Tiwari",
        "title": "Productivity Implications of R&D, Innovation, and Capital Accumulation\n  for Incumbents and Entrants: Perspectives from a Catching-up Economy",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  We study the productivity implications of R&D, capital accumulation, and\ninnovation output for entrants and incumbents in Estonia. First, in contrast to\ndeveloped economies, a small percentage of firm engage in formal R&D, but a\nmuch larger percentage innovate. Second, while we find no difference in the R&D\nelasticity of productivity for the entrants and incumbents, the impact of\ninnovation output - many of which are a result of 'doing, using and\ninteracting' (DUI) mode of innovation - is found to be higher for the entrants.\nEntrants who innovate are 21% to 30% more productive than entrants who do not;\nthe corresponding figures for the incumbents are 10% to 13%. Third, despite the\nadverse sectoral composition typical of catching-up economies, Estonian\nincumbents, who are the primary carriers of 'scientific and\ntechnologically-based innovative' (STI) activities, are comparable to their\ncounterparts in developed economies in translating STI activities into\nproductivity gains. Fourth, while embodied technological change through capital\naccumulation is found to be more effective in generating productivity growth\nthan R&D, the effectiveness is higher for firms engaging in R&D. Finally, our\nresults suggest that certain policy recommendations for spurring productivity\ngrowth in technologically advanced economies may not be applicable for\ncatching-up economies.\n"
    },
    {
        "paper_id": 2205.10665,
        "authors": "Jingwei Liu",
        "title": "European Power Option Pricing with Extended Vasic\\v{e}k Interest Rate\n  and Exponential Ornstein-Uhlenbeck Asset Process under Different Market\n  Assumptions",
        "comments": "27 pages , 1 figure",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We propose a general framework of European power option pricing under two\ndifferent market assumptions about extended Vasic\\v{e}k interest rate process\nand exponential Ornstein-Uhlenbeck asset process with continuous dividend as\nunderlying, in which the Brownian motions involved in Vasic\\v{e}k interest rate\nand exponential Ornstein-Uhlenbeck process are time-dependent correlated in\nequivalent martingale measure probability space or real-world probability space\nrespectively. We first develop European power option pricing in two types of\npayoffs with martingale method under the market assumption that Vasic\\v{e}k\ninterest rate and exponential Ornstein-Uhlenbeck process are correlated in\nequivalent martingale measure probability space. Then, we solve the European\npower option pricing under the market assumption that Vasic\\v{e}k interest rate\nand exponential Ornstein-Uhlenbeck process are correlated in real-world\nprobability by constructing a Girsannov transform to map real-world probability\nto risk-neutral equivalent martingale measure. Finally, the European power\noption pricing formulae are derived with numeraire change and T-forward measure\nunder the above two market assumptions in a uniform theoretical framework and\nclose formulae expression.\n"
    },
    {
        "paper_id": 2205.10865,
        "authors": "Daniel Guterding",
        "title": "Sparse modeling approach to the arbitrage-free interpolation of\n  plain-vanilla option prices and implied volatilities",
        "comments": null,
        "journal-ref": "Risks 11, 83 (2023)",
        "doi": "10.3390/risks11050083",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We present a method for the arbitrage-free interpolation of plain-vanilla\noption prices and implied volatilities, which is based on a system of integral\nequations that relates terminal density and option prices. Using a\ndiscretization of the terminal density, we write these integral equations as a\nsystem of linear equations. We show that the kernel matrix of this system is in\ngeneral ill-conditioned, so that it can not be solved for the discretized\ndensity using a naive approach. Instead, we construct a sparse model for the\nkernel matrix using the singular value decomposition (SVD), which allows us not\nonly to systematically improve the condition number of the kernel matrix, but\nalso determines the computational effort and accuracy of our method. In order\nto allow for the treatment of realistic inputs that may contain arbitrage, we\nreformulate the system of linear equations as an optimization problem, in which\nthe SVD-transformed density minimizes the error between the input prices and\nthe arbitrage-free prices generated by our method. To further stabilize the\nmethod in the presence of noisy input prices or arbitrage, we apply an\n$L_1$-regularization to the SVD-transformed density. Our approach, which is\ninspired by recent progress in theoretical physics, offers a flexible and\nefficient framework for the arbitrage-free interpolation of plain-vanilla\noption prices and implied volatilities, without the need to explicitly specify\na stochastic process, expansion basis functions or any other kind of model. We\ndemonstrate the capabilities of our method on a number of artificial and\nrealistic test cases.\n"
    },
    {
        "paper_id": 2205.11012,
        "authors": "Yasushi Ota, Yu Jiang and Daiki Maki",
        "title": "Parameters identification for an inverse problem arising from a binary\n  option using a Bayesian inference approach",
        "comments": "16 pages, 8 figures and 4 tables",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  No--arbitrage property provides a simple method for pricing financial\nderivatives. However, arbitrage opportunities exist among different markets in\nvarious fields, even for a very short time. By knowing that an arbitrage\nproperty exists, we can adopt a financial trading strategy. This paper\ninvestigates the inverse option problems (IOP) in the extended Black--Scholes\nmodel. We identify the model coefficients from the measured data and attempt to\nfind arbitrage opportunities in different financial markets using a Bayesian\ninference approach, which is presented as an IOP solution. The posterior\nprobability density function of the parameters is computed from the measured\ndata.The statistics of the unknown parameters are estimated by a Markov Chain\nMonte Carlo (MCMC) algorithm, which exploits the posterior state space. The\nefficient sampling strategy of the MCMC algorithm enables us to solve inverse\nproblems by the Bayesian inference technique. Our numerical results indicate\nthat the Bayesian inference approach can simultaneously estimate the unknown\ntrend and volatility coefficients from the measured data.\n"
    },
    {
        "paper_id": 2205.11122,
        "authors": "Y. Chang, C. Lizardi, R. Shah",
        "title": "Optimizing Returns Using the Hurst Exponent and Q Learning on Momentum\n  and Mean Reversion Strategies",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Momentum and mean reversion trading strategies have opposite characteristics.\nThe former is generally better with trending assets, and the latter is\ngenerally better with mean reverting assets. Using the Hurst exponent, which\nclassifies time series as trending or mean reverting, we attempt to trade with\neach strategy when it is advantageous to generate higher returns on average. We\nultimately find that trading with the Hurst exponent can achieve higher\nreturns, but it also comes at a higher risk. Finally, we consider limitations\nof our study and propose a method using Q-learning to improve our strategy and\nimplementation of individual algorithms.\n"
    },
    {
        "paper_id": 2205.11185,
        "authors": "Elisa Al\\`os, David Garc\\'ia-Lorite, Makar Pravosud",
        "title": "On the skew and curvature of implied and local volatilities",
        "comments": "13 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper, we study the relationship between the short-end of the local\nand the implied volatility surfaces. Our results, based on Malliavin calculus\ntechniques, recover the recent $\\frac{1}{H+3/2}$ rule (where $H$ denotes the\nHurst parameter of the volatility process) for rough volatilitites (see\nBourgey, De Marco, Friz, and Pigato (2022)), that states that the short-time\nskew slope of the at-the-money implied volatility is $\\frac{1}{H+3/2}$ the\ncorresponding slope for local volatilities. Moreover, we see that the\nat-the-money short-end curvature of the implied volatility can be written in\nterms of the short-end skew and curvature of the local volatility and\nviceversa, and that this relationship depends on $H$.\n"
    },
    {
        "paper_id": 2205.11439,
        "authors": "Micha{\\l} Narajewski",
        "title": "Probabilistic forecasting of German electricity imbalance prices",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The exponential growth of renewable energy capacity has brought much\nuncertainty to electricity prices and to electricity generation. To address\nthis challenge, the energy exchanges have been developing further trading\npossibilities, especially the intraday and balancing markets. For an energy\ntrader participating in both markets, the forecasting of imbalance prices is of\nparticular interest. Therefore, in this manuscript we conduct a very short-term\nprobabilistic forecasting of imbalance prices, contributing to the scarce\nliterature in this novel subject. The forecasting is performed 30 minutes\nbefore the delivery, so that the trader might still choose the trading place.\nThe distribution of the imbalance prices is modelled and forecasted using\nmethods well-known in the electricity price forecasting literature: lasso with\nbootstrap, gamlss, and probabilistic neural networks. The methods are compared\nwith a naive benchmark in a meaningful rolling window study. The results\nprovide evidence of the efficiency between the intraday and balancing markets\nas the sophisticated methods do not substantially overperform the intraday\ncontinuous price index. On the other hand, they significantly improve the\nempirical coverage. The analysis was conducted on the German market, however it\ncould be easily applied to any other market of similar structure.\n"
    },
    {
        "paper_id": 2205.11632,
        "authors": "Alexander M. Petersen",
        "title": "Evolution of biomedical innovation quantified via billions of distinct\n  article-level MeSH keyword combinations",
        "comments": "11 pages, 4 figures",
        "journal-ref": "Advances in Complex Systems 24, 2150016 (2022)",
        "doi": "10.1142/S0219525921500168",
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  We develop a systematic approach to measuring combinatorial innovation in the\nbiomedical sciences based upon the comprehensive ontology of Medical Subject\nHeadings (MeSH). This approach leverages an expert-defined knowledge ontology\nthat features both breadth (27,875 MeSH analyzed across 25 million articles\nindexed by PubMed from 1902 onwards) and depth (we differentiate between Major\nand Minor MeSH terms to identify differences in the knowledge network\nrepresentation constructed from primary research topics only). With this level\nof uniform resolution we differentiate between three different modes of\ninnovation contributing to the combinatorial knowledge network: (i) conceptual\ninnovation associated with the emergence of new concepts and entities (measured\nas the entry of new MeSH); and (ii) recombinant innovation, associated with the\nemergence of new combinations, which itself consists of two types: peripheral\n(i.e., combinations involving new knowledge) and core (combinations comprised\nof pre-existing knowledge only). Another relevant question we seek to address\nis whether examining triplet and quartet combinations, in addition to the more\ntraditional dyadic or pairwise combinations, provide evidence of any new\nphenomena associated with higher-order combinations. Analysis of the size,\ngrowth, and coverage of combinatorial innovation yield results that are largely\nindependent of the combination order, thereby suggesting that the common dyadic\napproach is sufficient to capture essential phenomena. Our main results are\ntwofold: (a) despite the persistent addition of new MeSH terms, the network is\ndensifying over time meaning that scholars are increasingly exploring and\nrealizing the vast space of all knowledge combinations; and (b) conceptual\ninnovation is increasingly concentrated within single research articles, a\nharbinger of the recent paradigm shift towards convergence science.\n"
    },
    {
        "paper_id": 2205.11834,
        "authors": "Claudio Albanese, Cyril B\\'en\\'ezet (LaMME, ENSIIE), St\\'ephane\n  Cr\\'epey (LPSM (UMR\\_8001), UPCit\\'e)",
        "title": "Hedging Valuation Adjustment and Model Risk",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The dynamic hedging theory only makes sense in the setup of one given model,\nwhereas the practice of dynamic hedging is just the opposite, with models\nfleeing after the data through daily recalibration. In this paper we revisit\nBurnett (2021) \\& Burnett and Williams (2021)'s notion of hedging valuation\nadjustment (HVA), originally intended to deal with dynamic hedging frictions,\nin the direction of model risk. We formalize and quantify Darwinian model risk\nas introduced in Albanese, Cr{\\'e}pey, and Iabichino (2021), in which traders\nselect models producing short to medium term gains at the cost of large but\ndistant losses. The corresponding HVA can be seen as the bridge between a\nglobal fair valuation model and the local models used by the different desks of\nthe bank. Importantly, model risk and dynamic hedging frictions indeed deserve\na reserve, but a risk-adjusted one, so not only an HVA, but also a contribution\nto the KVA of the bank. The orders of magnitude of the effects involved suggest\nthat bad models should not so much be managed via reserves, as excluded\naltogether. Model risk on CVA and FVA metrics is also considered.\n"
    },
    {
        "paper_id": 2205.12026,
        "authors": "\\.Ibrahim Halil Efendio\\u{g}ku",
        "title": "The impact of conspicuous consumption in social Media on purchasing\n  intentions",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/publicdomain/zero/1.0/",
        "abstract": "  With the rapid increase in the use of social media in the last decade, the\nconspicuous consumption lifestyle within society has been now transferred to\nthe social media. Along with the changing culture of consumption, the consumer\nwho witnesses such portrayals on social media aspires to and desires the same\nproducts and services. Having regard to this situation, this study examines the\nimpact of the conspicuous consumption trend in social media on purchasing\nintentions. Accordingly, the study aims to discover whether social media is\nbeing used as a conspicuous consumption channel and whether these conspicuous\nportrayals affect purchasing intentions\n"
    },
    {
        "paper_id": 2205.12043,
        "authors": "Jun Deng, Hua Zong and Yun Wang",
        "title": "Static Replication of Impermanent Loss for Concentrated Liquidity\n  Provision in Decentralised Markets",
        "comments": "12pages, 1 figure",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  This article analytically characterizes the impermanent loss of concentrated\nliquidity provision for automatic market makers in decentralised markets such\nas Uniswap. We propose two static replication formulas for the impermanent loss\nby a combination of European calls or puts with strike prices supported on the\nliquidity provision price interval. It facilitates liquidity providers to hedge\npermanent loss by trading crypto options in more liquid centralised exchanges\nsuch as Deribit. Numerical examples illustrate the astonishing accuracy of the\nstatic replication.\n"
    },
    {
        "paper_id": 2205.12242,
        "authors": "Hayden Brown",
        "title": "Fundamental Portfolio Outperforms the Market Portfolio",
        "comments": "25 pages, 2 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  There is substantial empirical evidence showing the fundamental portfolio\noutperforming the market portfolio. Here a theoretical foundation is laid that\nsupports this empirical research. Assuming stock prices revert around\nfundamental prices with sufficient strength and symmetry, the fundamental\nportfolio outperforms the market portfolio in expectation. If reversion toward\nthe fundamental price is not sufficiently strong, then the fundamental\nportfolio underperforms the market portfolio in expectation.\n"
    },
    {
        "paper_id": 2205.12746,
        "authors": "Raphael P. B. Piovezan, Pedro Paulo de Andrade Junior",
        "title": "Machine learning method for return direction forecasting of Exchange\n  Traded Funds using classification and regression models",
        "comments": "Co-author did not agree with publishing here",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
        "abstract": "  This article aims to propose and apply a machine learning method to analyze\nthe direction of returns from Exchange Traded Funds (ETFs) using the historical\nreturn data of its components, helping to make investment strategy decisions\nthrough a trading algorithm. In methodological terms, regression and\nclassification models were applied, using standard datasets from Brazilian and\nAmerican markets, in addition to algorithmic error metrics. In terms of\nresearch results, they were analyzed and compared to those of the Na\\\"ive\nforecast and the returns obtained by the buy & hold technique in the same\nperiod of time. In terms of risk and return, the models mostly performed better\nthan the control metrics, with emphasis on the linear regression model and the\nclassification models by logistic regression, support vector machine (using the\nLinearSVC model), Gaussian Naive Bayes and K-Nearest Neighbors, where in\ncertain datasets the returns exceeded by two times and the Sharpe ratio by up\nto four times those of the buy & hold control model.\n"
    },
    {
        "paper_id": 2205.12892,
        "authors": "Tim Sainburg",
        "title": "American postdoctoral salaries do not account for growing disparities in\n  cost of living",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  The National Institute of Health (NIH) sets postdoctoral (postdoc) trainee\nstipend levels that many American institutions and investigators use as a basis\nfor postdoc salaries. Although salary standards are held constant across\nuniversities, the cost of living in those universities' cities and towns vary\nwidely. Across non-postdoc jobs, more expensive cities pay workers higher wages\nthat scale with an increased cost of living. This work investigates the extent\nto which postdoc wages account for cost-of-living differences. More than 27,000\npostdoc salaries across all US universities are analyzed alongside measures of\nregional differences in cost of living. We find that postdoc salaries do not\naccount for cost-of-living differences, in contrast with the broader labor\nmarket in the same cities and towns. Despite a modest increase in income in\nhigh cost of living areas, real (cost of living adjusted) postdoc salaries\ndiffer by 29% ($15k 2021 USD) between the least and most expensive areas.\nCities that produce greater numbers of tenure-track faculty relative to\nstudents such as Boston, New York, and San Francisco are among the most\nimpacted by this pay disparity. The postdoc pay gap is growing and is\nwell-positioned to incur a greater financial burden on economically\ndisadvantaged groups and contribute to faculty hiring disparities in women and\nracial minorities.\n"
    },
    {
        "paper_id": 2205.13025,
        "authors": "Lyndon Moore, Gertjan Verdickt",
        "title": "Railroad Bailouts in the Great Depression",
        "comments": "41 pages, 4 figures, 11 tables",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  The Reconstruction Finance Corporation and Public Works Administration loaned\n50 U.S. railroads over $1.1 billion between 1932 and 1939. The government goal\nwas to decrease the likelihood of bond defaults and increase employment.\nBailouts had little effect on employment, instead they increased the average\nwage of their employees. Bailouts reduced leverage, but did not significantly\nimpact bond default. Overall, bailing out railroads had little effect on their\nstock prices, but resulted in an increase in their bond prices and reduced the\nlikelihood of ratings downgrades. We find some evidence that manufacturing\nfirms located close to railroads benefited from bailout spillovers.\n"
    },
    {
        "paper_id": 2205.13171,
        "authors": "Shayegheh Ashourizadeh and Mehrzad Saeedikiya",
        "title": "Immigrant and native export benefiting from business collaborations: a\n  global study",
        "comments": null,
        "journal-ref": null,
        "doi": "10.1504/EJIM.2020.10022504",
        "license": "http://creativecommons.org/publicdomain/zero/1.0/",
        "abstract": "  The authors hypothesised that export develops in the network of business\ncollaborations that are embedded in migration status. In that, collaborative\nnetworking positively affects export performance and immigrant entrepreneurs\nenjoy higher collaborative networking than native entrepreneurs due to their\nadvantage of being embedded in the home and the host country. Moreover, the\nadvantage of being an immigrant promotes the benefits of collaborative\nnetworking for export compared to those of native entrepreneurs. A total of\n47,200 entrepreneurs starting, running and owning firms in 71 countries were\nsurveyed by Global Entrepreneurship Monitor and analysed through the\nhierarchical linear modelling technique. Collaborative networking facilitated\nexport and migration status influenced entrepreneur networking, in that,\nimmigrant entrepreneurs had a higher level of collaborative networking than\nnative entrepreneurs. Consequently, immigrant entrepreneurs seemed to have\nbenefited from their network collaborations more than their native counterparts\ndid. This study sheds light on how immigrant entrepreneur network\ncollaborations can be effective for their exporting.\n"
    },
    {
        "paper_id": 2205.13186,
        "authors": "Michele Fioretti and Alessandro Iaria and Aljoscha Janssen and\n  Cl\\'ement Mazet-Sonilhac and Robert K. Perrons",
        "title": "Innovation Begets Innovation and Concentration: The Case of Upstream Oil\n  & Gas in the North Sea",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
        "abstract": "  We investigate the effect of technology adoption on competition by leveraging\na unique dataset on production, costs, and asset characteristics for North Sea\nupstream oil & gas companies. Relying on heterogeneity in the geological\nsuitability of fields and a landmark decision of the Norwegian Supreme Court\nthat increased the returns of capital investment in Norway relative to the UK,\nwe show that technology adoption increases market concentration. Firms with\nprior technology-specific know-how specialize more in fields suitable for the\nsame technology but also invest more in high-risk-high-return fields (e.g.,\nultra-deep recovery), diversifying their technology portfolio and ultimately\ngaining larger shares of the North Sea market. Our analyses illustrate how\ntechnology adoption can lead to market concentration both directly through\nspecialization and indirectly via experimentation.\n"
    },
    {
        "paper_id": 2205.13321,
        "authors": "Luis A. Souto Arias, Pasquale Cirillo and Cornelis W. Oosterlee",
        "title": "A new self-exciting jump-diffusion process for option pricing",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  We propose a new jump-diffusion process, the Heston-Queue-Hawkes (HQH) model,\ncombining the well-known Heston model and the recently introduced Queue-Hawkes\n(Q-Hawkes) jump process. Like the Hawkes process, the HQH model can capture the\neffects of self-excitation and contagion. However, since the characteristic\nfunction of the HQH process is known in closed-form, Fourier-based fast pricing\nalgorithms, like the COS method, can be fully exploited with this model.\nFurthermore, we show that by using partial integrals of the characteristic\nfunction, which are also explicitly known for the HQH process, we can reduce\nthe dimensionality of the COS method, and so its numerical complexity.\nNumerical results for European and Bermudan options show that the HQH model\noffers a wider range of volatility smiles compared to the Bates model, while\nits computational burden is considerably smaller than that of the Heston-Hawkes\n(HH) process.\n"
    },
    {
        "paper_id": 2205.13367,
        "authors": "\\.Ibrahim Halil Efendio\\u{g}lu, Adnan Talha Mutlu, Yakup Durmaz",
        "title": "The effect of the brand in the decision to purchase the mobile phone a\n  research on Y generation consumers",
        "comments": "14 pages, in Turkish language",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/publicdomain/zero/1.0/",
        "abstract": "  The aim of the study is to determine the effect of the brand on purchasing\ndecision on generation Y. For this purpose, a face-to-face survey was conducted\nwith 231 people in the Y age range who have purchased mobile phones in the last\nyear. The study was conducted with young academicians and university students\nworking in Harran University Vocational High Schools. The collected data were\nanalysed with AMOS and SPSS statistical package programs using structural\nequation modelling. According to the results of the research, the prestige,\nname and reliability of the brand have a significant impact on the purchase\ndecision of the Y generation. Price, advertising campaigns and technical\nservices of the brand also affect this decision less. On the other hand, the\nplace where mobile phone brands are produced and the importance they attach to\nsocial responsibility projects do not affect the purchasing decision.\n"
    },
    {
        "paper_id": 2205.13423,
        "authors": "Fengpei Li, Vitalii Ihnatiuk, Ryan Kinnear, Anderson Schneider, and\n  Yuriy Nevmyvaka",
        "title": "Do price trajectory data increase the efficiency of market impact\n  estimation?",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Market impact is an important problem faced by large institutional investor\nand active market participant. In this paper, we rigorously investigate whether\nprice trajectory data from the metaorder increases the efficiency of\nestimation, from an asymptotic view of statistical estimation. We show that,\nfor popular market impact models, estimation methods based on partial price\ntrajectory data, especially those containing early trade prices, can outperform\nestablished estimation methods (e.g., VWAP-based) asymptotically. We discuss\ntheoretical and empirical implications of such phenomenon, and how they could\nbe readily incorporated into practice.\n"
    },
    {
        "paper_id": 2205.13625,
        "authors": "Sandhya Devi and Sherman Page",
        "title": "Tsallis Relative entropy from asymmetric distributions as a risk measure\n  for financial portfolios",
        "comments": "31 pages, 12 figures and 2 tables. arXiv admin note: substantial text\n  overlap with arXiv:1901.04945",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  In an earlier study, we showed that Tsallis relative entropy (TRE), which is\nthe generalization of Kullback-Leibler relative entropy (KLRE) to non-extensive\nsystems, can be used as a possible risk measure in constructing risk optimal\nportfolios whose returns beat market returns. Over a long term (> 10 years),\nthe risk-return profiles from TRE as the risk measure show a more consistent\nbehavior than those from the commonly used risk measure 'beta' of the Capital\nAsset Pricing Model (CAPM). In these investigations, the model distributions\nderived from TRE are symmetric. However, observations show that distributions\nof the returns of financial markets and equities are in general asymmetric in\npositive and negative returns. In this work, we generalize TRE for the\nasymmetric case (ATRE) by considering the data distribution as a linear\ncombination of two independent normalized distributions - one for negative\nreturns and one for positive returns. Each of these two independent\ndistributions are half q-Gaussians with different non-extensivity parameter q\nand temperature parameter b. The risk-return (in excess of market returns)\npatterns are investigated using ATRE as the risk measure. The results are\ncompared with those from two other risk measures: TRE and the Tsallis relative\nentropy S- derived from the negative returns only. Tests on data, which include\nthe dot-com bubble, the 2008 crash, and COVID periods, for both long (20 years)\nand shorter terms (10 years), show that a linear fit can be obtained for the\nrisk-excess return profiles of all three risk measures. However, the fits for\nportfolios created during the chaotic market conditions (crashes) using S- as\nthe risk show a much higher slope pointing to higher returns for a given risk\nvalue. Further, in this case, the excess returns of even short-term portfolios\nremain positive irrespective of the market behavior.\n"
    },
    {
        "paper_id": 2205.13773,
        "authors": "Ramandeep Kaur Bagri, Yihsu Chen",
        "title": "Wildfire Modeling: Designing a Market to Restore Assets",
        "comments": "72 pages, 5 figures, unpublished results",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  In the past decade, summer wildfires have become the norm in California, and\nthe United States of America. These wildfires are caused due to variety of\nreasons. The state collects wildfire funds to help the impacted customers.\nHowever, the funds are eligible only under certain conditions and are collected\nuniformly throughout California. Therefore, the overall idea of this project is\nto look for quantitative results on how electrical corporations cause wildfires\nand how they can help to collect the wildfire funds or charge fairly to the\ncustomers to maximize the social impact. The research project aims to propose\nthe implication of wildfire risk associated with vegetation, and due to power\nlines and incorporate that in dollars. Therefore, the project helps to solve\nthe problem of collecting wildfire funds associated with each location and\nincorporate energy prices to charge their customers according to their wildfire\nrisk related to the location to maximize the social surplus for the society.\nThe thesis findings will help to calculate the risk premium involving wildfire\nrisk associated with the location and incorporate the risk into pricing. The\nresearch of this submitted proposal provides the potential contribution towards\ndetecting the utilities associated wildfire risk in the power lines, which can\nprevent wildfires by controlling the line flows of the system. Ultimately, the\ngoal of this proposal is a social benefit to save money for the electrical\ncorporations and their customers in California, who pay flat charges for\nWildfire Fund each month $0.00580/kWh (in dollars). Therefore, this proposal\nwill propose new method to collect wildfire fund with maximum customer surplus\nfor future generations.\n"
    },
    {
        "paper_id": 2205.13942,
        "authors": "Nicolas Boursin, Carl Remlinger, Joseph Mikael, Carol Anne Hargreaves",
        "title": "Deep Generators on Commodity Markets; application to Deep Hedging",
        "comments": "15 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Driven by the good results obtained in computer vision, deep generative\nmethods for time series have been the subject of particular attention in recent\nyears, particularly from the financial industry. In this article, we focus on\ncommodity markets and test four state-of-the-art generative methods, namely\nTime Series Generative Adversarial Network (GAN) Yoon et al. [2019], Causal\nOptimal Transport GAN Xu et al. [2020], Signature GAN Ni et al. [2020] and the\nconditional Euler generator Remlinger et al. [2021], are adapted and tested on\ncommodity time series. A first series of experiments deals with the joint\ngeneration of historical time series on commodities. A second set deals with\ndeep hedging of commodity options trained on he generated time series. This use\ncase illustrates a purely data-driven approach to risk hedging.\n"
    },
    {
        "paper_id": 2205.14146,
        "authors": "Masato Hisakado, Kodai Hattori, Shintaro Mori",
        "title": "Multi-Dimensional self-exciting NBD process and Default portfolios",
        "comments": "26 pages, 7 figures",
        "journal-ref": "The Review of Socionetwork Strategies, vol.16,(2022) 493-512",
        "doi": "10.1007/s12626-022-00122-y",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this study, we apply a multidimensional self-exciting negative binomial\ndistribution (SE-NBD) process to default portfolios with 13 sectors. The SE-NBD\nprocess is a Poisson process with a gamma-distributed intensity function. We\nextend the SE-NBD process to a multidimensional process. Using the\nmultidimensional SE-NBD process (MD-SE-NBD), we can estimate interactions\nbetween these 13 sectors as a network. By applying impact analysis, we can\nclassify upstream and downstream sectors. The upstream sectors are real-estate\nand financial institution (FI) sectors. From these upstream sectors, shock\nspreads to the downstream sectors. This is an amplifier of the shock. This is\nconsistent with the analysis of bubble bursts. We compare these results to the\nmultidimensional Hawkes process (MD-Hawkes) that has a zero-variance intensity\nfunction.\n"
    },
    {
        "paper_id": 2205.14186,
        "authors": "Lingxi Chen",
        "title": "The Effect of Increased Access to IVF on Women's Careers",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Motherhood is the main contributor to gender gaps in the labor market. IVF is\na method of assisted reproduction that can delay fertility, which results in\ndecreased motherhood income penalty. In this research, I estimate the effects\nof expanded access to in vitro fertilization (IVF) arising from state insurance\nmandates. I use a difference-in-differences model to estimate the effect of\nincreased IVF accessibility for delaying childbirth and decreasing the\nmotherhood income penalty. Using the fertility supplement dataset from the\nCurrent Population Survey (CPS), I estimate how outcomes change in states when\nthey implement their mandates compared to how outcomes change in states that\nare not changing their policies. The results indicate that IVF mandates\nincrease the probability of motherhood by 38 by 3.1 percentage points (p<0.01).\nHowever, the results provide no evidence that IVF insurance mandates impact\nwomen's earnings.\n"
    },
    {
        "paper_id": 2205.14387,
        "authors": "Akira Matsushita, Kei Ikegami, Kyohei Okumura, Yoji Tomita, Atsushi\n  Iwasaki",
        "title": "Regulating Matching Markets with Constraints: Data-driven Taxation",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  This paper develops a framework to conduct a counterfactual analysis to\nregulate matching markets with regional constraints that impose lower and upper\nbounds on the number of matches in each region. Our work is motivated by the\nJapan Residency Matching Program, in which the policymaker wants to guarantee\nthe least number of doctors working in rural regions to achieve the minimum\nstandard of service. Among the multiple possible policies that satisfy such\nconstraints, a policymaker wants to choose the best. To this end, we develop a\ndiscrete choice model approach that estimates the utility functions of agents\nfrom observed data and predicts agents' behavior under different counterfactual\npolicies. Our framework also allows the policymaker to design the\nwelfare-maximizing tax scheme, which outperforms the policy currently used in\npractice. Furthermore, a numerical experiment illustrates how our method works.\n"
    },
    {
        "paper_id": 2205.14517,
        "authors": "Chang Su, Wenbo Lyu, Yueting Liu",
        "title": "The Relationship between Digital RMB and Digital Economy in China",
        "comments": "C.S. and W.L. contributed equally to this work",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  By comparing the historical patterns of currency development, this paper\npointed out the inevitability of the development of digital currency and the\nrelationship between digital currency and the digital economy. With the example\nof China, this paper predicts the future development trend of digital currency.\nIn the context of the rapid development of private cryptocurrency, China\nlaunched the digital currency based on serving the digital economy and\ncommitted to the globalization of the digital renminbi (RMB) and the\nglobalization of the digital economy. The global economy in 2022 ushered in\nstagnation, and China treats digital fiat currency and the digital economy\ndevelopment as a breakthrough to pursue economic transformation and new growth.\nIt has become one of the forefront countries with numerous experiences that can\nbe learned by countries around the world.\n"
    },
    {
        "paper_id": 2205.14699,
        "authors": "Hugo Inzirillo and Stanislas de Quenetain",
        "title": "Managing Risk in DeFi Portfolios",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
        "abstract": "  Decentralized Finance (DeFi) is a new financial industry built on blockchain\ntechnologies. Decentralized financial services have consequently increased the\nability to lend, borrow, and invest in decentralized investment vehicles,\nallowing investors to bypass third party intermediaries. DeFi's promise is to\nreduce the cost of transaction and management fees whilst increasing trust\nbetween agents of the Financial Industry 3.0. This paper provides an overview\nof the different components of DeFi, as well as the risks involved in investing\nthrough these new vehicles. We will also propose an allocation methodology\nwhich will integrate and quantify these risks.\n"
    },
    {
        "paper_id": 2205.15056,
        "authors": "Huifang Huang, Ting Gao, Yi Gui, Jin Guo, Peng Zhang",
        "title": "Stock Trading Optimization through Model-based Reinforcement Learning\n  with Resistance Support Relative Strength",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Reinforcement learning (RL) is gaining attention by more and more researchers\nin quantitative finance as the agent-environment interaction framework is\naligned with decision making process in many business problems. Most of the\ncurrent financial applications using RL algorithms are based on model-free\nmethod, which still faces stability and adaptivity challenges. As lots of\ncutting-edge model-based reinforcement learning (MBRL) algorithms mature in\napplications such as video games or robotics, we design a new approach that\nleverages resistance and support (RS) level as regularization terms for action\nin MBRL, to improve the algorithm's efficiency and stability. From the\nexperiment results, we can see RS level, as a market timing technique, enhances\nthe performance of pure MBRL models in terms of various measurements and\nobtains better profit gain with less riskiness. Besides, our proposed method\neven resists big drop (less maximum drawdown) during COVID-19 pandemic period\nwhen the financial market got unpredictable crisis. Explanations on why control\nof resistance and support level can boost MBRL is also investigated through\nnumerical experiments, such as loss of actor-critic network and prediction\nerror of the transition dynamical model. It shows that RS indicators indeed\nhelp the MBRL algorithms to converge faster at early stage and obtain smaller\ncritic loss as training episodes increase.\n"
    },
    {
        "paper_id": 2205.1532,
        "authors": "Rohith Mahadevan, Sam Richard, Kishore Harshan Kumar, Jeevitha\n  Murugan, Santhosh Kannan, Saaisri, Tarun, Raja CSP Raman",
        "title": "Payday loans -- blessing or growth suppressor? Machine Learning Analysis",
        "comments": "8 pages, 8 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  The upsurge of real estate involves a variety of factors that have got\ninfluenced by many domains. Indeed, the unrecognized sector that would affect\nthe economy for which regulatory proposals are being drafted to keep this in\ncontrol is the payday loans. This research paper revolves around the impact of\npayday loans in the real estate market. The research paper draws a first-hand\nexperience of obtaining the index for the concentration of real estate in an\narea of reference by virtue of payday loans in Toronto, Ontario in particular,\nwhich sets out an ideology to create, evaluate and demonstrate the scenario\nthrough research analysis. The purpose of this indexing via payday loans is the\nbasic - debt: income ratio which states that when the income of the person\nbound to pay the interest of payday loans increases, his debt goes down\nmarginally which hence infers that the person invests in fixed assets like real\nestate which hikes up its growth.\n"
    },
    {
        "paper_id": 2205.15398,
        "authors": "\\.Ibrahim Halil Efendio\\u{g}lu",
        "title": "Can I invest in Metaverse? The effect of obtained information and\n  perceived risk on purchase intention by the perspective of the information\n  adoption model",
        "comments": "in Turkish language",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/publicdomain/zero/1.0/",
        "abstract": "  Metaverse is a virtual universe that combines the physical world and the\ndigital world. People can socialize, play games and even shop with their\navatars created in this virtual environment. Metaverse, which is growing very\nfast in terms of investment, is both a profitable and risky area for consumers.\nIn order to enter the Metaverse for investment purposes, it is necessary to do\na certain research and gather information. In this direction, the aim of the\nstudy is to determine the effect of the quality of the information obtained by\nthe consumers about the metaverse world, the reliability of the information and\nthe perceived risk, on the purchase intention from the point of view of the\ninformation adoption model. For the research, data were collected online from\n495 consumers who were interested in metaverse investment. AMOS and SPSS\npackage programs were used in the analysis. First, descriptive statistical\nanalyzes were made for the basic structure of the variables. Then the\nreliability and validity of the model were tested. Finally, the structural\nequation model was used to test the proposed model. According to the findings,\nthe reliability and quality of the information affect the purchase intention\npositively and significantly, while the perceived risk affects the purchase\nintention negatively and significantly.\n"
    },
    {
        "paper_id": 2205.15451,
        "authors": "Takuya Hara",
        "title": "Economics of 100% renewable power systems",
        "comments": "36 pages (Main text: 9 pages with 3 figures, Supplementary Materials:\n  27 pages with 10 figures and 2 tables)",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  Studies have evaluated the economic feasibility of 100% renewable power\nsystems using the optimization approach, but the mechanisms determining the\nresults remain poorly understood. Based on a simple but essential model, this\nstudy found that the bottleneck formed by the largest mismatch between demand\nand power generation profiles determines the optimal capacities of generation\nand storage and their trade-off relationship. Applying microeconomic theory,\nparticularly the duality of quantity and value, this study comprehensively\nquantified the relationships among the factor cost of technologies, their\noptimal capacities, and total system cost. Using actual profile data for\nmultiple years/regions in Japan, this study demonstrated that hybrid systems\ncomprising cost-competitive multiple renewable energy sources and different\ntypes of storage are critical for the economic feasibility of any profile.\n"
    },
    {
        "paper_id": 2205.15558,
        "authors": "Kiyoshi Kanazawa, Hideki Takayasu, Misako Takayasu",
        "title": "Exact solution to two-body financial dealer model: revisited from the\n  viewpoint of kinetic theory",
        "comments": "40 pages, 15 figures",
        "journal-ref": "J. Stat. Phys. 190, 8 (2023)",
        "doi": "10.1007/s10955-022-03022-1",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The two-body stochastic dealer model is revisited to provide an exact\nsolution to the average order-book profile using the kinetic approach. The\ndealer model is a microscopic financial model where individual traders make\ndecisions on limit-order prices stochastically and then reach agreements on\ntransactions. In the literature, this model was solved for several cases: an\nexact solution for two-body traders $N=2$ and a mean-field solution for many\ntraders $N\\gg 1$. Remarkably, while kinetic theory plays a significant role in\nthe mean-field analysis for $N\\gg 1$, its role is still elusive for the case of\n$N=2$. In this paper, we revisit the two-body dealer model $N=2$ to clarify the\nutility of the kinetic theory. We first derive the exact master-Liouville\nequations for the two-body dealer model by several methods. We next illustrate\nthe physical picture of the master-Liouville equation from the viewpoint of the\nprobability currents. The master-Liouville equations are then solved exactly to\nderive the order-book profile and the average transaction interval.\nFurthermore, we introduce a generalised two-body dealer model by incorporating\ninteraction between traders via the market midprice and exactly solve the model\nwithin the kinetic framework. We finally confirm our exact solution by\nnumerical simulations. This work provides a systematic mathematical basis for\nthe econophysics model by developing better mathematical intuition.\n"
    },
    {
        "paper_id": 2205.15576,
        "authors": "Marvin Nipper, Andreas Ostermaier and Jochen Theis",
        "title": "Mandatory Disclosure of Standardized Sustainability Metrics: The Case of\n  the EU Taxonomy Regulation",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Sustainability reporting enables investors to make informed decisions and is\nhoped to facilitate the transition to a green economy. The European Union's\ntaxonomy regulation enacts rules to discern sustainable activities and\ndetermine the resulting green revenue, whose disclosure is mandatory for many\ncompanies. In an experiment, we explore how this standardized metric is\nreceived by investors relative to a sustainability rating. We find that green\nrevenue affects the investment probability more than the rating if the two\nmetrics disagree. If they agree, a strong rating has an incremental effect on\nthe investment probability. The effects are robust to variation in investors'\nattitudes. Our findings imply that a mandatory standardized sustainability\nmetric is an effective means of channeling investment, which complements rather\nthan substitutes sustainability ratings.\n"
    },
    {
        "paper_id": 2205.15699,
        "authors": "Kevin Kamm and Michelle Muniz",
        "title": "A novel approach to rating transition modelling via Machine Learning and\n  SDEs on Lie groups",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  In this paper, we introduce a novel methodology to model rating transitions\nwith a stochastic process. To introduce stochastic processes, whose values are\nvalid rating matrices, we noticed the geometric properties of stochastic\nmatrices and its link to matrix Lie groups. We give a gentle introduction to\nthis topic and demonstrate how It\\^o-SDEs in R will generate the desired model\nfor rating transitions. To calibrate the rating model to historical data, we\nuse a Deep-Neural-Network (DNN) called TimeGAN to learn the features of a time\nseries of historical rating matrices. Then, we use this DNN to generate\nsynthetic rating transition matrices. Afterwards, we fit the moments of the\ngenerated rating matrices and the rating process at specific time points, which\nresults in a good fit. After calibration, we discuss the quality of the\ncalibrated rating transition process by examining some properties that a time\nseries of rating matrices should satisfy, and we will see that this geometric\napproach works very well.\n"
    },
    {
        "paper_id": 2205.15905,
        "authors": "Yang Shen and Bin Zou",
        "title": "Cone-constrained Monotone Mean-Variance Portfolio Selection Under\n  Diffusion Models",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  We consider monotone mean-variance (MMV) portfolio selection problems with a\nconic convex constraint under diffusion models, and their counterpart problems\nunder mean-variance (MV) preferences. We obtain the precommitted optimal\nstrategies to both problems in closed form and find that they coincide, without\nand with the presence of the conic constraint. This result generalizes the\nequivalence between MMV and MV preferences from non-constrained cases to a\nspecific constrained case. A comparison analysis reveals that the orthogonality\nproperty under the conic convex set is a key to ensuring the equivalence\nresult.\n"
    },
    {
        "paper_id": 2205.15991,
        "authors": "Samuel N. Cohen, Christoph Reisinger, Sheng Wang",
        "title": "Hedging option books using neural-SDE market models",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We study the capability of arbitrage-free neural-SDE market models to yield\neffective strategies for hedging options. In particular, we derive\nsensitivity-based and minimum-variance-based hedging strategies using these\nmodels and examine their performance when applied to various option portfolios\nusing real-world data. Through backtesting analysis over typical and stressed\nmarket periods, we show that neural-SDE market models achieve lower hedging\nerrors than Black--Scholes delta and delta-vega hedging consistently over time,\nand are less sensitive to the tenor choice of hedging instruments. In addition,\nhedging using market models leads to similar performance to hedging using\nHeston models, while the former tends to be more robust during stressed market\nperiods.\n"
    },
    {
        "paper_id": 2206.00004,
        "authors": "Johannes Konig, David I. Stern, Richard S.J. Tol",
        "title": "Confidence Intervals for Recursive Journal Impact Factors",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We compute confidence intervals for recursive impact factors, that take into\naccount that some citations are more prestigious than others, as well as for\nthe associated ranks of journals, applying the methods to the population of\neconomics journals. The Quarterly Journal of Economics is clearly the journal\nwith greatest impact, the confidence interval for its rank only includes one.\nBased on the simple bootstrap, the remainder of the Top5 journals are in the\ntop 6 together with the Journal of Finance, while the Xie et al. (2009), and\nMogstad et al. (2022) methods generally broaden estimated confidence intervals,\nparticularly for mid-ranking journals. All methods agree that most apparent\ndifferences in journal quality are, in fact, mostly insignificant.\n"
    },
    {
        "paper_id": 2206.00117,
        "authors": "Xiaoqing Wan, Nichole R. Lighthall",
        "title": "Disclosure of Investment Advisor and Broker-Dealer Relationships: Impact\n  on Comprehension and Decision Making",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Recently enacted regulations aimed to enhance retail investors' understanding\nabout different types of investment accounts. Toward this goal, the Securities\nand Exchange Commission (SEC) mandated that SEC-registered investment advisors\nand broker-dealers provide a brief relationship summary (Form CRS) to retail\ninvestors. The present study examines the impact of this regulation on\ninvestors and considers its market implications. The effects of Form CRS were\nevaluated based on three outcome variables: perceived helpfulness,\ncomprehension, and decision making. The study also examined whether personal\ncharacteristics, such as investment experience, influenced the disclosure's\nimpact on decision making. Results indicated that participants perceived the\ndisclosure as helpful and it significantly enhanced comprehension about the two\ntypes of investment accounts. Critically, participants also showed increased\npreference and choice for broker-dealers after the disclosure. Increased\npreference for broker-dealers was associated with greater investment\nexperience, greater comprehension gains, and access to more information from a\nlonger disclosure. These findings suggest that Form CRS may promote informed\ndecision making among retail investors while simultaneously increasing the\nselection of broker-dealer accounts.\n"
    },
    {
        "paper_id": 2206.00174,
        "authors": "Giotis Georgios",
        "title": "Preliminary Results on the Employment Effect of Tourism. A meta-analysis",
        "comments": "18 pages, 1 Figure, 4 Tables",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Unemployment is one of the most important issues in every country. Tourism\nindustry is a dynamic sector which is labor augmented and can create jobs,\nincrease consumption expenditures and offer employment opportunities. In the\nanalysis of this work, the empirical literature on this issue is presented,\nwhich indicates that tourism can play a vital and beneficial role to increase\nemployment. This paper uses meta-analysis techniques to investigate the effect\nof tourism on employment, and finds that the vast majority of the studies\nreveal a positive relationship. The mean effect of the 36 studies of the\nmeta-sample, using Partial Correlations, is 0.129. This effect is similar to\nthe regression results which is around 0.9, which is positive and statistically\nsignificant, clearly indicating that tourism is effective in the creation of\njobs and offers employment opportunities. Moreover, evidence of selection bias\nin the literature in favor of studies which produce positive estimates is\nfound, and once this bias is corrected the true effect is 0.85-0.97.\n"
    },
    {
        "paper_id": 2206.00189,
        "authors": "Peng Zhang, Yuwei Zhang, Nuo Xu",
        "title": "Measurement of carbon finance level and exploration of its influencing\n  factors",
        "comments": "8pages, 3 figures, 14 tables",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Faced with increasingly severe environmental problems, carbon trading markets\nand related financial activities aiming at limiting carbon dioxide emissions\nare booming. Considering the complexity and urgency of carbon market, it is\nnecessary to construct an effective evaluation index system. This paper\nselected carbon finance index as a composite indicator. Taking Beijing,\nShanghai, and Guangdong as examples, we adopted the classic method of multiple\ncriteria decision analysis (MCDA) to analyze the composite indicator. Potential\nimpact factors were screened extensively and calculated through normalization,\nweighting by coefficient of variation and different aggregation methods. Under\nthe measurement of Shannon-Spearman Measure, the method with the least loss of\ninformation was used to obtain the carbon finance index (CFI) of the pilot\nareas. Through panel model analysis, we found that company size, the number of\npatents per 10,000 people and the proportion of new energy generation were the\nfactors with significant influence. Based on the research, corresponding\nsuggestions were put forward for different market entities. Hopefully, this\nresearch will contribute to the steady development of the national carbon\nmarket.\n"
    },
    {
        "paper_id": 2206.00368,
        "authors": "Aurelio Patelli and Lorenzo Napolitano and Giulio Cimini and Emanuele\n  Pugliese and Andrea Gabrielli",
        "title": "The Evolution of Competitiveness across Economic, Innovation and\n  Knowledge production activities",
        "comments": null,
        "journal-ref": null,
        "doi": "10.1038/s41598-023-29979-x",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The evolution of economic and innovation systems at the national scale is\nshaped by a complex dynamics, the footprint of which is the nested structure of\nthe activities in which different countries are competitive. Nestedness is a\npersistent feature across multiple kinds (layers) of activities related to the\nproduction of knowledge and goods: scientific research, technological\ninnovation, industrial production and trade. We observe that in the layers of\ninnovation and trade the competitiveness of countries correlates unambiguously\nwith their diversification, while the science layer displays some peculiar\nfeature. The evolution of scientific domains leads to an increasingly modular\nstructure, in which the most developed nations become less competitive in the\nless advanced scientific domains, where they are replaced by the emerging\ncountries. This observation is in line with a capability-based view of the\nevolution of economic systems, but with a slight twist. Indeed, while the\naccumulation of specific know-how and skills is a fundamental step towards\ndevelopment, resource constraints force countries to acquire competitiveness in\nthe more complex research fields at the price of losing ground in more basic,\nalbeit less visible (or more crowded), fields. This tendency towards a\nrelatively specialized basket of capabilities leads to a trade-off between the\nneed to diversify in order to evolve and the need to allocate resources\nefficiently. Collaborative patterns among developed nations reduce the\nnecessity to be competitive in the less sophisticated fields, freeing resources\nfor the more complex domains.\n"
    },
    {
        "paper_id": 2206.00396,
        "authors": "Allister Loder, Fabienne Cantner, Lennart Adenaw, Markus Siewert,\n  Sebastian Goerg, Markus Lienkamp, Klaus Bogenberger",
        "title": "A nation-wide experiment: fuel tax cuts and almost free public transport\n  for three months in Germany -- Report 1 Study design, recruiting and\n  participation",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
        "abstract": "  In spring 2022, the German federal government agreed on a set of measures\nthat aim at reducing households' financial burden resulting from a recent price\nincrease, especially in energy and mobility. These measures include among\nothers, a nation-wide public transport ticket for 9 EUR per month and a fuel\ntax cut that reduces fuel prices by more than 15% . In transportation research\nthis is an almost unprecedented behavioral experiment. It allows to study not\nonly behavioral responses in mode choice and induced demand but also to assess\nthe effectiveness of transport policy instruments. We observe this natural\nexperiment with a three-wave survey and an app-based travel diary on a sample\nof hundreds of participants as well as an analysis of traffic counts. In this\nfirst report, we inform about the study design, recruiting and initial\nparticipation of study participants.\n"
    },
    {
        "paper_id": 2206.00397,
        "authors": "Michael Kitchener, Nandini Anantharama, Simon D. Angus, Paul A.\n  Raschky",
        "title": "Predicting Political Ideology from Digital Footprints",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper proposes a new method to predict individual political ideology\nfrom digital footprints on one of the world's largest online discussion forum.\nWe compiled a unique data set from the online discussion forum reddit that\ncontains information on the political ideology of around 91,000 users as well\nas records of their comment frequency and the comments' text corpus in over\n190,000 different subforums of interest. Applying a set of statistical learning\napproaches, we show that information about activity in non-political discussion\nforums alone, can very accurately predict a user's political ideology.\nDepending on the model, we are able to predict the economic dimension of\nideology with an accuracy of up to 90.63% and the social dimension with and\naccuracy of up to 82.02%. In comparison, using the textual features from actual\ncomments does not improve predictive accuracy. Our paper highlights the\nimportance of revealed digital behaviour to complement stated preferences from\ndigital communication when analysing human preferences and behaviour using\nonline data.\n"
    },
    {
        "paper_id": 2206.00509,
        "authors": "Duygu Buyukyazici, Francesco Serti",
        "title": "Religiosity and Innovation Attitudes: An Instrumental Variables Analysis",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Estimating the influence of religion on innovation is challenging because of\nboth complexness and endogeneity. In order to untangle these issues, we use\nseveral measures of religiosity, adopt an individual-level approach to\ninnovation and employ the instrumental variables method. We analyse the effect\nof religiosity on individual attitudes that are either favourable or\nunfavourable to innovation, presenting an individual's propensity to innovate.\nWe instrument one's religiosity with the average religiosity of people of the\nsame sex, age range, and religious affiliation who live in countries with the\nsame dominant religious denomination. The results strongly suggest that each\nmeasure of religiosity has a somewhat negative effect on innovation attitudes.\nThe diagnostic test results and sensitivity analyses support the main findings.\nWe propose three causality channels from religion to innovation: time\nallocation, the fear of uncertainty, and conventional roles reinforced by\nreligion.\n"
    },
    {
        "paper_id": 2206.00568,
        "authors": "Qiang Liu, Yingtao Luo, Shu Wu, Zhen Zhang, Xiangnan Yue, Hong Jin,\n  Liang Wang",
        "title": "RMT-Net: Reject-aware Multi-Task Network for Modeling\n  Missing-not-at-random Data in Financial Credit Scoring",
        "comments": "Accepted by IEEE TKDE",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  In financial credit scoring, loan applications may be approved or rejected.\nWe can only observe default/non-default labels for approved samples but have no\nobservations for rejected samples, which leads to missing-not-at-random\nselection bias. Machine learning models trained on such biased data are\ninevitably unreliable. In this work, we find that the default/non-default\nclassification task and the rejection/approval classification task are highly\ncorrelated, according to both real-world data study and theoretical analysis.\nConsequently, the learning of default/non-default can benefit from\nrejection/approval. Accordingly, we for the first time propose to model the\nbiased credit scoring data with Multi-Task Learning (MTL). Specifically, we\npropose a novel Reject-aware Multi-Task Network (RMT-Net), which learns the\ntask weights that control the information sharing from the rejection/approval\ntask to the default/non-default task by a gating network based on rejection\nprobabilities. RMT-Net leverages the relation between the two tasks that the\nlarger the rejection probability, the more the default/non-default task needs\nto learn from the rejection/approval task. Furthermore, we extend RMT-Net to\nRMT-Net++ for modeling scenarios with multiple rejection/approval strategies.\nExtensive experiments are conducted on several datasets, and strongly verifies\nthe effectiveness of RMT-Net on both approved and rejected samples. In\naddition, RMT-Net++ further improves RMT-Net's performances.\n"
    },
    {
        "paper_id": 2206.0064,
        "authors": "Ryo Sakai",
        "title": "The Evolution of Investor Activism in Japan",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Activist investors have gradually become a catalyst for change in Japanese\ncompanies. This study examines the impact of activist board representation on\nfirm performance in Japan. I focus on the only two Japanese companies with\nactivist board representation: Kawasaki Kisen Kaisha, Ltd. (\"Kawasaki\") and\nOlympus Corporation (\"Olympus\"). Overall, I document significant benefits from\nthe decision to engage with activists at these companies. The target companies\nexperience greater short- and long-term abnormal stock returns following the\nactivist engagement. Moreover, I show operational improvements as measured by\nreturn on assets and return on equity. Activist board members also associate\nwith important changes in payout policy that help explain the positive stock\nreturns. My findings support the notion that Japanese companies should consider\nengagements with activist investors to transform and improve their businesses.\nSuch interactions can lead to innovative and forward-thinking policies that\ncreate value for Japanese businesses and their stakeholders.\n"
    },
    {
        "paper_id": 2206.00648,
        "authors": "Yanzhao Zou, Dorien Herremans",
        "title": "PreBit -- A multimodal model with Twitter FinBERT embeddings for extreme\n  price movement prediction of Bitcoin",
        "comments": "21 pages, submitted preprint to Elsevier Expert Systems with\n  Applications",
        "journal-ref": "Expert Systems with Applications, 233, 120838 (2023)",
        "doi": "10.1016/j.eswa.2023.120838",
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  Bitcoin, with its ever-growing popularity, has demonstrated extreme price\nvolatility since its origin. This volatility, together with its decentralised\nnature, make Bitcoin highly subjective to speculative trading as compared to\nmore traditional assets. In this paper, we propose a multimodal model for\npredicting extreme price fluctuations. This model takes as input a variety of\ncorrelated assets, technical indicators, as well as Twitter content. In an\nin-depth study, we explore whether social media discussions from the general\npublic on Bitcoin have predictive power for extreme price movements. A dataset\nof 5,000 tweets per day containing the keyword `Bitcoin' was collected from\n2015 to 2021. This dataset, called PreBit, is made available online. In our\nhybrid model, we use sentence-level FinBERT embeddings, pretrained on financial\nlexicons, so as to capture the full contents of the tweets and feed it to the\nmodel in an understandable way. By combining these embeddings with a\nConvolutional Neural Network, we built a predictive model for significant\nmarket movements. The final multimodal ensemble model includes this NLP model\ntogether with a model based on candlestick data, technical indicators and\ncorrelated asset prices. In an ablation study, we explore the contribution of\nthe individual modalities. Finally, we propose and backtest a trading strategy\nbased on the predictions of our models with varying prediction threshold and\nshow that it can used to build a profitable trading strategy with a reduced\nrisk over a `hold' or moving average strategy.\n"
    },
    {
        "paper_id": 2206.00727,
        "authors": "Daniel Bj\\\"orkegren, Joshua E. Blumenstock, Samsun Knight",
        "title": "(Machine) Learning What Policies Value",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  When a policy prioritizes one person over another, is it because they benefit\nmore, or because they are preferred? This paper develops a method to uncover\nthe values consistent with observed allocation decisions. We use machine\nlearning methods to estimate how much each individual benefits from an\nintervention, and then reconcile its allocation with (i) the welfare weights\nassigned to different people; (ii) heterogeneous treatment effects of the\nintervention; and (iii) weights on different outcomes. We demonstrate this\napproach by analyzing Mexico's PROGRESA anti-poverty program. The analysis\nreveals that while the program prioritized certain subgroups -- such as\nindigenous households -- the fact that those groups benefited more implies that\nthey were in fact assigned a lower welfare weight. The PROGRESA case\nillustrates how the method makes it possible to audit existing policies, and to\ndesign future policies that better align with values.\n"
    },
    {
        "paper_id": 2206.00818,
        "authors": "Dominik Hartmann and Flavio L. Pinheiro",
        "title": "Economic complexity and inequality at the national and regional level",
        "comments": "26 pages, 2 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Recent studies have found evidence of a negative association between economic\ncomplexity and inequality at the country level. Moreover, evidence suggests\nthat sophisticated economies tend to outsource products that are less desirable\n(e.g. in terms of wage and inequality effects), and instead focus on complex\nproducts requiring networks of skilled labor and more inclusive institutions.\nYet the negative association between economic complexity and inequality on a\ncoarse scale could hide important dynamics at a fine-grained level. Complex\neconomic activities are difficult to develop and tend to concentrate spatially,\nleading to 'winner-take-most' effects that spur regional inequality in\ncountries. Large, complex cities tend to attract both high- and low-skills\nactivities and workers, and are also associated with higher levels of\nhierarchies, competition, and skill premiums. As a result, the association\nbetween complexity and inequality reverses at regional scales; in other words,\nmore complex regions tend to be more unequal. Ideas from polarization theories,\ninstitutional changes, and urban scaling literature can help to understand this\nparadox, while new methods from economic complexity and relatedness can help\nidentify inclusive growth constraints and opportunities.\n"
    },
    {
        "paper_id": 2206.00982,
        "authors": "Marfri Gambal, Aleksandre Asatiani, Julia Kotlarsky",
        "title": "Strategic Innovation Through Outsourcing: A Theoretical Review",
        "comments": null,
        "journal-ref": "The Journal of Strategic Information Systems, Volume 31, Issue 2,\n  2022,101718",
        "doi": "10.1016/j.jsis.2022.101718",
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Competition in the Information Technology Outsourcing (ITO) and Business\nProcess Outsourcing (BPO) industry is increasingly moving from being motivated\nby cost savings towards strategic benefits that service providers can offer to\ntheir clients. Innovation is one such benefit that is expected nowadays in\noutsourcing engagements. The rising importance of innovation has been noticed\nand acknowledged not only in the Information Systems (IS) literature, but also\nin other management streams such as innovation and strategy. However, to date,\nthese individual strands of research remain largely isolated from each other.\nOur theoretical review addresses this gap by consolidating and analyzing\nresearch on strategic innovation in the ITO and BPO context. The article set\nincludes 95 papers published between 1998 to 2020 in outlets from the IS and\nrelated management fields. We craft a four-phase framework that integrates\nprior insights about (1) the antecedents of the decision to pursue strategic\ninnovation in outsourcing settings; (2) arrangement options that facilitate\nstrategic innovation in outsourcing relationships; (3) the generation of\nstrategic innovations; and (4) realized strategic innovation outcomes, as\nassessed in the literature. We find that the research landscape to date is\nskewed, with many studies focusing on the first two phases. The last two phases\nremain relatively uncharted. We also discuss how innovation-oriented\noutsourcing insights compare with established research on cost-oriented\noutsourcing engagements. Finally, we offer directions for future research.\n"
    },
    {
        "paper_id": 2206.01064,
        "authors": "Man Yiu Tsang, Tony Sit, Hoi Ying Wong",
        "title": "Adaptive Robust Online Portfolio Selection",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The online portfolio selection (OLPS) problem differs from classical\nportfolio model problems, as it involves making sequential investment\ndecisions. Many OLPS strategies described in the literature capture market\nmovement based on various beliefs and are shown to be profitable. In this\npaper, we propose a robust optimization (RO)-based strategy that takes\ntransaction costs into account. Moreover, unlike existing studies that\ncalibrate model parameters from benchmark data sets, we develop a novel\nadaptive scheme that decides the parameters sequentially. With a wide range of\nparameters as input, our scheme captures market uptrend and protects against\nmarket downtrend while controlling trading frequency to avoid excessive\ntransaction costs. We numerically demonstrate the advantages of our adaptive\nscheme against several benchmarks under various settings. Our adaptive scheme\nmay also be useful in general sequential decision-making problems. Finally, we\ncompare the performance of our strategy with that of existing OLPS strategies\nusing both benchmark and newly collected data sets. Our strategy outperforms\nthese existing OLPS strategies in terms of cumulative returns and competitive\nSharpe ratios across diversified data sets, demonstrating its\nadaptability-driven superiority.\n"
    },
    {
        "paper_id": 2206.01114,
        "authors": "Germ\\'an Reyes",
        "title": "Coarse Wage-Setting and Behavioral Firms",
        "comments": "JEL Codes: D22, E24, D91",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  This paper shows that the bunching of wages at round numbers is partly driven\nby firm coarse wage-setting. Using data from over 200 million new hires in\nBrazil, I first establish that contracted salaries tend to cluster at round\nnumbers. Then, I show that firms that tend to hire workers at round-numbered\nsalaries have worse market outcomes. Next, I develop a wage-posting model in\nwhich optimization costs lead to the adoption of coarse rounded wages and\nprovide evidence supporting two model predictions using two research designs.\nFinally, I examine some consequences of coarse wage-setting for relevant\neconomic outcomes.\n"
    },
    {
        "paper_id": 2206.01468,
        "authors": "Francesco Cordoni",
        "title": "Multi-Asset Bubbles Equilibrium Price Dynamics",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The price-bubble and crash process formation is theoretically investigated in\na two-asset equilibrium model. Sufficient and necessary conditions are derived\nfor the existence of average equilibrium price dynamics of different\nagent-based models, where agents are distinguished in terms of factor and\ninvestment trading strategies. In line with experimental results, we show that\nassets with a positive average dividend, i.e., with a strictly declining\nfundamental value, display at the equilibrium price the typical hump-shaped\nbubble observed in experimental asset markets. Moreover, a misvaluation effect\nis observed in the asset with a constant fundamental value, triggered by the\nother asset that displays the price bubble shape when a sharp price decline is\nexhibited at the end of the market.\n"
    },
    {
        "paper_id": 2206.01562,
        "authors": "Toon Vanderschueren, Robert Boute, Tim Verdonck, Bart Baesens, Wouter\n  Verbeke",
        "title": "Prescriptive maintenance with causal machine learning",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Machine maintenance is a challenging operational problem, where the goal is\nto plan sufficient preventive maintenance to avoid machine failures and\noverhauls. Maintenance is often imperfect in reality and does not make the\nasset as good as new. Although a variety of imperfect maintenance policies have\nbeen proposed in the literature, these rely on strong assumptions regarding the\neffect of maintenance on the machine's condition, assuming the effect is (1)\ndeterministic or governed by a known probability distribution, and (2)\nmachine-independent. This work proposes to relax both assumptions by learning\nthe effect of maintenance conditional on a machine's characteristics from\nobservational data on similar machines using existing methodologies for causal\ninference. By predicting the maintenance effect, we can estimate the number of\noverhauls and failures for different levels of maintenance and, consequently,\noptimize the preventive maintenance frequency to minimize the total estimated\ncost. We validate our proposed approach using real-life data on more than 4,000\nmaintenance contracts from an industrial partner. Empirical results show that\nour novel, causal approach accurately predicts the maintenance effect and\nresults in individualized maintenance schedules that are more accurate and\ncost-effective than supervised or non-individualized approaches.\n"
    },
    {
        "paper_id": 2206.0184,
        "authors": "Jorge Carrera and Pablo de la Vega",
        "title": "The Effect of External Debt on Greenhouse Gas Emissions",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  We estimate the causal effect of external debt on greenhouse gas emissions in\na panel of 78 emerging market and developing economies over the 1990-2015\nperiod. Unlike previous literature, we use external instruments to address the\npotential endogeneity in the relationship between external debt and greenhouse\ngas emissions. Specifically, we use international liquidity shocks as\ninstrumental variables for external debt. We find that dealing with the\npotential endogeneity problem brings about a positive and statistically\nsignificant effect of external debt on greenhouse gas emissions: a 1 percentage\npoint (pp.) rise in external debt causes, on average, a 0.5% increase in\ngreenhouse gas emissions. One possible mechanism of action could be that, as\nexternal debt increases, governments are less able to enforce environmental\nregulations because their main priority is to increase the tax base to pay\nincreasing debt services or because they are captured by the private sector who\nowns that debt and prevented from tightening such regulations.\n"
    },
    {
        "paper_id": 2206.01878,
        "authors": "Yiling Lin, Carl Benedikt Frey, Lingfei Wu",
        "title": "Remote Collaboration Fuses Fewer Breakthrough Ideas",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Theories of innovation emphasize the role of social networks and teams as\nfacilitators of breakthrough discoveries. Around the world, scientists and\ninventors today are more plentiful and interconnected than ever before. But\nwhile there are more people making discoveries, and more ideas that can be\nreconfigured in novel ways, research suggests that new ideas are getting harder\nto find-contradicting recombinant growth theory. In this paper, we shed new\nlight on this apparent puzzle. Analyzing 20 million research articles and 4\nmillion patent applications across the globe over the past half-century, we\nbegin by documenting the rise of remote collaboration across cities,\nunderlining the growing interconnectedness of scientists and inventors\nglobally. We further show that across all fields, periods, and team sizes,\nresearchers in these remote teams are consistently less likely to make\nbreakthrough discoveries relative to their onsite counterparts. Creating a\ndataset that allows us to explore the division of labor in knowledge production\nwithin teams and across space, we find that among distributed team members,\ncollaboration centers on late-stage, technical tasks involving more codified\nknowledge. Yet they are less likely to join forces in conceptual tasks-such as\nconceiving new ideas and designing research-when knowledge is tacit. We\nconclude that despite striking improvements in digital technology in recent\nyears, remote teams are less likely to integrate the knowledge of their members\nto produce new, disruptive ideas.\n"
    },
    {
        "paper_id": 2206.02142,
        "authors": "Stephan Leitner",
        "title": "Collaborative search and autonomous task allocation in organizations of\n  learning agents",
        "comments": "12 pages, 4 figures, 1 table. arXiv admin note: text overlap with\n  arXiv:2105.04514",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper introduces a model of multi-unit organizations with either static\nstructures, i.e., they are designed top-down following classical approaches to\norganizational design, or dynamic structures, i.e., the structures emerge over\ntime from micro-level decisions. In the latter case, the units are capable of\nlearning about the technical interdependencies of the task they face, and they\nuse their knowledge by adapting the task allocation from time to time. In both\nstatic and dynamic organizations, searching for actions to increase the\nperformance can either be carried out individually or collaboratively. The\nresults indicate that (i) collaborative search processes can help overcome the\nadverse effects of inefficient task allocations as long as there is an internal\nfit with other organizational design elements, and (ii) for dynamic\norganizations, the emergent task allocation does not necessarily mirror the\ntechnical interdependencies of the task the organizations face, even though the\nsame (or even higher) performances are achieved.\n"
    },
    {
        "paper_id": 2206.02205,
        "authors": "R. Vilela Mendes",
        "title": "The fractional volatility model and rough volatility",
        "comments": "13 pages latex, 4 figures. arXiv admin note: text overlap with\n  arXiv:cond-mat/0404684",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  The question of the volatility roughness is interpreted in the framework of a\ndata-reconstructed fractional volatility model, where volatility is driven by\nfractional noise. Some examples are worked out and also, using Malliavin\ncalculus for fractional processes, an option pricing equation and its solution\nare obtained.\n"
    },
    {
        "paper_id": 2206.02227,
        "authors": "Wenpin Tang",
        "title": "Stability of shares in the Proof of Stake Protocol -- Concentration and\n  Phase Transitions",
        "comments": "30 pages, 11 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper is concerned with the stability of shares in a cryptocurrency\nwhere the new coins are issued according to the Proof of Stake protocol. We\nidentify large, medium and small investors under various rewarding schemes, and\nshow that the limiting behaviors of these investors are different -- for large\ninvestors their shares are stable, while for medium to small investors their\nshares may be volatile or even shrink to zero. For instance, with a geometric\nreward there is chaotic centralization, where all the shares will eventually\nconcentrate on one investor in a random manner. This leads to the phase\ntransition phenomenon, and the thresholds for stability are characterized. In\nresponse to the increasing activities in blockchain networks, we also propose\nand analyze a dynamical population model for the PoS protocol, which allows the\nnumber of investors to grow over the time. Numerical experiments are provided\nto corroborate our theory.\n"
    },
    {
        "paper_id": 2206.02448,
        "authors": "Mayara Moraes Monteiro, Carlos M. Lima Azevedo, Maria Kamargianni,\n  Yoram Shiftan, Ayelet Gal-Tzur, Sharon Shoshany Tavory, Constantinos\n  Antoniou, Guido Cantelmo",
        "title": "Car-Sharing Subscription Preferences: The Case of Copenhagen, Munich,\n  and Tel Aviv-Yafo",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  Car-sharing services have been providing short-term car access to their\nusers, contributing to sustainable urban mobility and generating positive\nsocietal and often environmental impacts. As car-sharing business models vary,\nit is important to understand what features drive the attraction and retention\nof its members in different contexts. For that, it is essential to examine\nindividuals preferences for subscriptions to different business models and what\nthey perceive as most relevant, as well as understand what could be attractive\nincentives. This study aims precisely to examine individuals preferences for\nthe subscription of different car-sharing services in different cities. We\ndesigned a stated preference experiment and collected data from three different\nurban car-sharing settings, namely Copenhagen, Munich, and Tel Aviv-Yafo. Then\na mixed logit model was estimated to uncover car-sharing plan subscription and\nincentives preferences. The results improve our understanding of how both the\nfeatures of the car-sharing business model and the provision of incentives can\nmaintain and attract members to the system. The achieved insights pave the road\nfor the actual design of car-sharing business models and incentives that can be\noffered by existing and future car-sharing companies in the studied or similar\ncities.\n"
    },
    {
        "paper_id": 2206.02582,
        "authors": "Aleksy Leeuwenkamp",
        "title": "Making heads or tails of systemic risk measures",
        "comments": "Revised version of the $\\Delta$-CoES paper, now with a better\n  estimator and clear theoretical results. Main body: 22 pages. Appendix\n  contains: proofs, explanation of the data cleaning/pre-processing procedure,\n  supplementary figures, tables and details of the software/computer setup",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  This paper shows that the CoVaR,$\\Delta$-CoVaR,CoES,$\\Delta$-CoES and MES\nsystemic risk measures can be represented in terms of the univariate risk\nmeasure evaluated at a quantile determined by the copula. The result is applied\nto derive empirically relevant properties of these measures concerning their\nsensitivity to power-law tails, outliers and their properties under\naggregation. Furthermore, a novel empirical estimator for the CoES is proposed.\nThe power-law result is applied to derive a novel empirical estimator for the\npower-law coefficient which depends on\n$\\Delta\\text{-CoVaR}/\\Delta\\text{-CoES}$. To show empirical performance\nsimulations and an application of the methods to a large dataset of financial\ninstitutions are used. This paper finds that the MES is not suitable for\nmeasuring extreme risks. Also, the ES-based measures are more sensitive to\npower-law tails and large losses. This makes these measures more useful for\nmeasuring network risk but less so for systemic risk. The robustness analysis\nalso shows that all $\\Delta$ measures can underestimate due to the occurrence\nof intermediate losses. Lastly, it is found that the power-law tail coefficient\nestimator can be used as an early-warning indicator of systemic risk.\n"
    },
    {
        "paper_id": 2206.02685,
        "authors": "A. Carbone and L. Ponta",
        "title": "Relative cluster entropy for power-law correlated sequences",
        "comments": null,
        "journal-ref": "SciPost Phys. 13, 076 (2022)",
        "doi": "10.21468/SciPostPhys.13.3.076",
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  We propose an information-theoretical measure, the \\textit{relative cluster\nentropy} $\\mathcal{D_{C}}[P \\| Q] $, to discriminate among cluster partitions\ncharacterised by probability distribution functions $P$ and $Q$. The measure is\nillustrated with the clusters generated by pairs of fractional Brownian motions\nwith Hurst exponents $H_1$ and $H_2$ respectively. For subdiffusive, normal and\nsuperdiffusive sequences, the relative entropy sensibly depends on the\ndifference between $H_1$ and $H_2$. By using the \\textit{minimum relative\nentropy} principle, cluster sequences characterized by different correlation\ndegrees are distinguished and the optimal Hurst exponent is selected. As a case\nstudy, real-world cluster partitions of market price series are compared to\nthose obtained from fully uncorrelated sequences (simple Browniam motions)\nassumed as a model. The \\textit{minimum relative cluster entropy} yields\noptimal Hurst exponents $H_1=0.55$, $H_1=0.57$, and $H_1=0.63$ respectively for\nthe prices of DJIA, S\\&P500, NASDAQ: a clear indication of non-markovianity.\nFinally, we derive the analytical expression of the relative cluster entropy\nand the outcomes are discussed for arbitrary pairs of power-laws probability\ndistribution functions of continuous random variables.\n"
    },
    {
        "paper_id": 2206.02719,
        "authors": "Nazrul Islam",
        "title": "Assessment of Consumers Awareness of Nutrition, Food Safety and Hygiene\n  in the Life Cycle for Sustainable Development of Bangladesh- A Study on\n  Sylhet City Corporation",
        "comments": "88 pages, SUST research project report",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  The objective of the study is to explore the knowledge of consumers on\nnutrition, food safety and hygiene of the sampled respondents in the Sylhet\nCity Corporation in Bangladesh in terms of different demographic\ncharacteristics. This study includes the different types of consumers in the\nlife cycle viz. baby, child, adolescent, young and old so that an overall\nawareness level can be measured in the urban area of Bangladesh. The study is\nconfined to SCC area in which all types of respondents has been included and\nfindings from this study will be used generally for Bangladesh in making policy\nIn conducting the study the population has been divided into six group as,\nBaby, child, adolescent, parental, unmarried adult young and married adult\nmatured. We find that the average score of awareness of food nutrition and\nhygiene of unmarried adult is higher than that of married adults. The study\nsuggested it is needed to increase awareness in of the parents for feeding\nbabies. The average awareness of parents to their childs eating behavior\nbetween 5 and 9 years is 3.36 out of 5. The awareness is around 67 percent so\nwe should be more careful in this regard. The average awareness adolescent food\nhabit is 1.89 on three points scales which about 63 percent only. Therefore,\nthe consciousness of adolescent has been to increase in taking food. The\naverage feeding styles of parents is 4.24 out of 5 to their children up to 9\nyears and in percentage it is 84 percent.\n"
    },
    {
        "paper_id": 2206.02722,
        "authors": "Nazrul Islam",
        "title": "Vicious Cycle of Poverty in Haor Region of Bangladesh- Impact of Formal\n  and Informal Credits",
        "comments": "146 pages, Grants for Advanced Research in Education (GARE) Program,\n  Ministry of Education, Government of Bangladesh",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  This research attempts to explore the key research questions about what are\nthe different microcredit programs in Haor area in Bangladesh? And do\nmicrocredit programs have a positive impact on livelihoods of the clients in\nterms of selected social indicators viz. income, consumption, assets, net\nworth, education, access to finance, social capacity, food security and\nhandling socks etc. in Haor area in Bangladesh? Utilizing\ndifference-in-difference and factor analysis, we explore the nature and terms\nof conditions of available formal and informal micro-creditss in Haor region of\nBangladesh; and investigate the impact of micro-creditss on the poverty\ncondition of Haor people in Bangladesh. The findings showed that total income\nof borrowers has been increased over non-borrowers (z=6.75) significantly.\nAmong the components of income, non-agricultural income has been increased\nsignificantly on the other hand income from labor sale has been decreased\nsignificantly. Total consumption expenditure with its heads of food and\nnon-food consumption of both formal borrowers and informal borrowers have been\nincreased over the period 2016-2019 significantly. Most of the key informants\nagreed that the findings are very much consistent with prevailing condition of\nmicro-credits in Haor region. However, some of them raised question about the\nimpacts of micro-credits. They argued that there is no straightforward positive\nimpact of micro-credits on poverty condition of the households.\n"
    },
    {
        "paper_id": 2206.02798,
        "authors": "Nazrul Islam",
        "title": "Impact of micro-credit on the livelihoods of clients -- A study on\n  Sunamganj District",
        "comments": "94 pages, University Research Centre, Shahjalal University of Science\n  and Technology. arXiv admin note: substantial text overlap with\n  arXiv:2206.02722",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  The objective of this paper is to assess the impact of micro credit on the\nlivelihoods of the clients in the haor area of Sunamganj district, Sylhet,\nBangladesh. The major findings of the study are that 66.2 percent respondents\nof borrowers and 98.7 non-borrowers are head of the family and an average 76.6\npercent and among the borrowers 32 percent is husband/wife while 1.3 percent of\nnon-borrowers and on average 22.2. In terms of sex 64.7 percent of borrowers\nand 92.5 percent of non-borrowers are male while 35.3 percent of borrowers and\n7.5 percent of non-borrowers are female. The impact of micro-credit in terms of\nformal and informal credit receiving households based on DID method showed that\ntotal income, total expenditure and investment have been increased 13.57\npercent, 10.39 percent and 26.17 percent. All the elements of total income have\nbeen increased except debt which has been decreased by 2.39 percent. But the\ndecrease in debt is the good sign of positive impact of debt. Consumption of\nfood has been increased but non-food has been decreased. All the elements of\ninvestment have been increased except some factors. The savings has been\ndecreased due excess increase in investment. The study suggested that for\nbreaking vicious cycle of poverty by micro-credit the duration of loans should\nbe at least five year and the volume of loans must be minimum 500,000 and\nrepayment should at not be less than monthly. The rate of interest should not\nbe more than 5 percent.\n"
    },
    {
        "paper_id": 2206.02854,
        "authors": "Davide Lauria, W. Brent Lindquist, Stefan Mittnik, Svetlozar T. Rachev",
        "title": "ESG-Valued Portfolio Optimization and Dynamic Asset Pricing",
        "comments": "Main article: 51 pages, 11 figures, 5 tables. Supplementary appendix:\n  17 pages, 11 figures, 6 tables",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  ESG ratings provide a quantitative measure for socially responsible\ninvestment. We present a unified framework for incorporating numeric ESG\nratings into dynamic pricing theory. Specifically, we introduce an ESG-valued\nreturn that is a linearly constrained transformation of financial return and\nESG score. This leads to a more complex portfolio optimization problem in a\nspace governed by reward, risk and ESG score. The framework preserves the\ntraditional risk aversion parameter and introduces an ESG affinity parameter.\nWe apply this framework to develop ESG-valued: portfolio optimization; capital\nmarket line; risk measures; option pricing; and the computation of shadow\nriskless rates.\n"
    },
    {
        "paper_id": 2206.03148,
        "authors": "Rossana Mastrandrea, Rob ter Burg, Yuli Shan, Klaus Hubacek, Franco\n  Ruzzenenti",
        "title": "Scaling laws in global corporations as a benchmarking approach to assess\n  environmental performance",
        "comments": "Another update copy was uploaded",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  The largest 6,529 international corporations are accountable for almost 30%\nof global CO2e emissions. A growing awareness of the role of the corporate\nworld in the path toward sustainability has led many shareholders and\nstakeholders to pursue increasingly stringent and ambitious environmental\ngoals. However, how to assess the corporate environmental performance\nobjectively and efficiently remains an open question. This study reveals\nunderlying dynamics and structures that can be used to construct a unified\nquantitative picture of the environmental impact of companies. This study shows\nthat the environmental impact (metabolism) of companies CO2e energy used, water\nwithdrawal and waste production, scales with their size according to a simple\npower law which is often sublinear, and can be used to derive a\nsector-specific, size-dependent benchmark to asses unambiguously a company's\nenvironmental performance. Enforcing such a benchmark would potentially result\nin a 15% emissions reduction, but a fair and effective environmental policy\nshould consider the size of the corporation and the super or sublinear nature\nof the scaling relationship\n"
    },
    {
        "paper_id": 2206.03246,
        "authors": "Damian Kisiel and Denise Gorse",
        "title": "Portfolio Transformer for Attention-Based Asset Allocation",
        "comments": "11 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Traditional approaches to financial asset allocation start with returns\nforecasting followed by an optimization stage that decides the optimal asset\nweights. Any errors made during the forecasting step reduce the accuracy of the\nasset weightings, and hence the profitability of the overall portfolio. The\nPortfolio Transformer (PT) network, introduced here, circumvents the need to\npredict asset returns and instead directly optimizes the Sharpe ratio, a\nrisk-adjusted performance metric widely used in practice. The PT is a novel\nend-to-end portfolio optimization framework, inspired by the numerous successes\nof attention mechanisms in natural language processing. With its full\nencoder-decoder architecture, specialized time encoding layers, and gating\ncomponents, the PT has a high capacity to learn long-term dependencies among\nportfolio assets and hence can adapt more quickly to changing market conditions\nsuch as the COVID-19 pandemic. To demonstrate its robustness, the PT is\ncompared against other algorithms, including the current LSTM-based state of\nthe art, on three different datasets, with results showing that it offers the\nbest risk-adjusted performance.\n"
    },
    {
        "paper_id": 2206.03278,
        "authors": "Stavros Stavroyiannis",
        "title": "Cointegration and ARDL specification between the Dubai crude oil and the\n  US natural gas market",
        "comments": "23 pages, 5 figures, 7 tables",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  This paper examines the relationship between the price of the Dubai crude oil\nand the price of the US natural gas using an updated monthly dataset from 1992\nto 2018, incorporating the latter events in the energy markets. After employing\na variety of unit root and cointegration tests, the long-run relationship is\nexamined via the autoregressive distributed lag (ARDL) cointegration technique,\nalong with the Toda-Yamamoto (1995) causality test. Our results indicate that\nthere is a long-run relationship with a unidirectional causality running from\nthe Dubai crude oil market to the US natural gas market. A variety of post\nspecification tests indicate that the selected ARDL model is well-specified,\nand the results of the Toda-Yamamoto approach via impulse response functions,\nforecast error variance decompositions, and historical decompositions with\ngeneralized weights, show that the Dubai crude oil price retains a positive\nrelationship and affects the US natural gas price.\n"
    },
    {
        "paper_id": 2206.03306,
        "authors": "Volha Lazuka",
        "title": "Household and individual economic responses to different health shocks:\n  The role of medical innovations",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  This study provides new evidence regarding the extent to which medical care\nmitigates the economic consequences of various health shocks for the individual\nand a wider family. To obtain causal effects, I focus on the role of medical\nscientific discoveries and leverage the longitudinal dimension of unique\nadministrative data for Sweden. The results indicate that medical innovations\nstrongly mitigate the negative economic consequences of a health shock for the\nindividual and create spillovers to relatives. Such mitigating effects are\nhighly heterogeneous across prognoses. These results suggest that medical\ninnovation substantially reduces the burden of welfare costs yet produces\nincome inequalities.\n"
    },
    {
        "paper_id": 2206.03386,
        "authors": "Antonio Briola, Tomaso Aste",
        "title": "Dependency structures in cryptocurrency market from high to low\n  frequency",
        "comments": "22 pages, 9 figures, 4 tables, 4 appendices",
        "journal-ref": "Entropy 2022, 24, 1548",
        "doi": "10.3390/e24111548",
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  We investigate logarithmic price returns cross-correlations at different time\nhorizons for a set of 25 liquid cryptocurrencies traded on the FTX digital\ncurrency exchange. We study how the structure of the Minimum Spanning Tree\n(MST) and the Triangulated Maximally Filtered Graph (TMFG) evolve from high (15\ns) to low (1 day) frequency time resolutions. For each horizon, we test the\nstability, statistical significance and economic meaningfulness of the\nnetworks. Results give a deep insight into the evolutionary process of the time\ndependent hierarchical organization of the system under analysis. A decrease in\ncorrelation between pairs of cryptocurrencies is observed for finer time\nsampling resolutions. A growing structure emerges for coarser ones,\nhighlighting multiple changes in the hierarchical reference role played by\nmainstream cryptocurrencies. This effect is studied both in its pairwise\nrealizations and intra-sector ones.\n"
    },
    {
        "paper_id": 2206.03524,
        "authors": "John M Abowd and Michael B Hawes",
        "title": "Confidentiality Protection in the 2020 US Census of Population and\n  Housing",
        "comments": "Version 2 corrects a few transcription errors in Tables 2, 3 and 5.\n  Version 3 adds final journal copy edits to the preprint",
        "journal-ref": "Annual Review of Statistics and Its Application 2023 10:1",
        "doi": "10.1146/annurev-statistics-010422-034226",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In an era where external data and computational capabilities far exceed\nstatistical agencies' own resources and capabilities, they face the renewed\nchallenge of protecting the confidentiality of underlying microdata when\npublishing statistics in very granular form and ensuring that these granular\ndata are used for statistical purposes only. Conventional statistical\ndisclosure limitation methods are too fragile to address this new challenge.\nThis article discusses the deployment of a differential privacy framework for\nthe 2020 US Census that was customized to protect confidentiality, particularly\nthe most detailed geographic and demographic categories, and deliver controlled\naccuracy across the full geographic hierarchy.\n"
    },
    {
        "paper_id": 2206.03608,
        "authors": "Bahman Angoshtari",
        "title": "Predictable Forward Performance Processes in Complete Markets",
        "comments": "37 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We establish existence of Predictable Forward Performance Processes (PFPPs)\nin complete markets, which has been previously shown only in the binomial\nsetting. Our market model can be a discrete-time or a continuous-time model,\nand the investment horizon can be finite or infinite. We show that the main\nstep in construction of PFPPs is solving a one-period problem involving an\nintegral equation, which is the counterpart of the functional equation found in\nthe binomial case. Although this integral equation has been partially studied\nin the existing literature, we provide a new solution method using the Fourier\ntransform for tempered distributions. We also provide closed-form solutions for\nPFPPs with inverse marginal functions that are completely monotonic and\nestablish uniqueness of PFPPs within this class. We apply our results to two\nspecial cases. The first one is the binomial market and is included to relate\nour work to the existing literature. The second example considers a generalized\nBlack-Scholes model which, to the best of our knowledge, is a new result.\n"
    },
    {
        "paper_id": 2206.03622,
        "authors": "Pawel Dlotko, Wanling Qiu, Simon Rudkin",
        "title": "Topological Data Analysis Ball Mapper for Finance",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Finance is heavily influenced by data-driven decision-making. Meanwhile, our\nability to comprehend the full informational content of data sets remains\nimpeded by the tools we apply in analysis, especially where the data is\nhigh-dimensional. Presenting the Topological Data Analysis Ball Mapper\nalgorithm this paper illuminates a new means of seeing the detail in data from\ndata shape. With comparisons to existing approaches and illustrative examples,\nthe value of the new tool is shown. Directions for employing Ball Mapper in\npractice are given and the benefits are reviewed.\n"
    },
    {
        "paper_id": 2206.03742,
        "authors": "Donghan Kim",
        "title": "Market-to-book Ratio in Stochastic Portfolio Theory",
        "comments": "3 Figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
        "abstract": "  We study market-to-book ratios of stocks in the context of Stochastic\nPortfolio Theory. Functionally generated portfolios that depend on auxiliary\neconomic variables other than relative capitalizations (\"sizes\") are developed\nin two ways, together with their relative returns with respect to the market.\nThis enables us to identify the value factor (i.e., market-to-book ratio) in\nreturns of such generated portfolios when the auxiliary variables are stocks'\nbook values. Examples of portfolios, as well as their empirical results, are\ngiven, with the evidence that, in addition to size, the value factor does\naffect the performance of the portfolio.\n"
    },
    {
        "paper_id": 2206.03772,
        "authors": "Julia Ackermann, Thomas Kruse, Mikhail Urusov",
        "title": "Reducing Obizhaeva-Wang type trade execution problems to LQ stochastic\n  control problems",
        "comments": "45 pages; to appear in Finance and Stochastics",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We start with a stochastic control problem where the control process is of\nfinite variation (possibly with jumps) and acts as integrator both in the state\ndynamics and in the target functional. Problems of such type arise in the\nstream of literature on optimal trade execution pioneered by Obizhaeva and Wang\n(models with finite resilience). We consider a general framework where the\nprice impact and the resilience are stochastic processes. Both are allowed to\nhave diffusive components. First we continuously extend the problem from\nprocesses of finite variation to progressively measurable processes. Then we\nreduce the extended problem to a linear quadratic (LQ) stochastic control\nproblem. Using the well developed theory on LQ problems we describe the\nsolution to the obtained LQ one and trace it back up to the solution to the\n(extended) initial trade execution problem. Finally, we illustrate our results\nby several examples. Among other things the examples show the Obizhaeva-Wang\nmodel with random (terminal and moving) targets, the necessity to extend the\ninitial trade execution problem to a reasonably large class of progressively\nmeasurable processes (even going beyond semimartingales) and the effects of\ndiffusive components in the price impact process and/or in the resilience\nprocess.\n"
    },
    {
        "paper_id": 2206.03786,
        "authors": "Ravshanbek Khodzhimatov and Stephan Leitner and Friederike Wall",
        "title": "Controlling replication via the belief system in multi-unit\n  organizations",
        "comments": "Submitted to Social Simulation Conference 2022. arXiv admin note:\n  text overlap with arXiv:2104.05993",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Multi-unit organizations such as retail chains are interested in the\ndiffusion of best practices throughout all divisions. However, the strict\nguidelines or incentive schemes may not always be effective in promoting the\nreplication of a practice. In this paper we analyze how the individual belief\nsystems, namely the desire of individuals to conform, may be used to spread\nknowledge between departments. We develop an agent-based simulation of an\norganization with different network structures between divisions through which\nthe knowledge is shared, and observe the resulting synchrony. We find that the\neffect of network structures on the diffusion of knowledge depends on the\ninterdependencies between divisions, and that peer-to-peer exchange of\ninformation is more effective in reaching synchrony than unilateral sharing of\nknowledge from one division. Moreover, we find that centralized network\nstructures lead to lower performance in organizations.\n"
    },
    {
        "paper_id": 2206.03847,
        "authors": "Christoph Carnehl and Satoshi Fukuda and Nenad Kos",
        "title": "Time-varying Cost of Distancing: Distancing Fatigue and Lockdowns",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We study a behavioral SIR model with time-varying costs of distancing. The\ntwo main causes of the variation in the cost of distancing we explore are\ndistancing fatigue and public policies (lockdowns). We show that for a second\nwave of an epidemic to arise, a steep increase in distancing cost is necessary.\nDistancing fatigue cannot increase the distancing cost sufficiently fast to\ncreate a second wave. However, public policies that discontinuously affect the\ndistancing cost can create a second wave. With that in mind, we characterize\nthe largest change in the distancing cost (due to, for example, lifting a\npublic policy) that will not cause a second wave. Finally, we provide a\nnumerical analysis of public policies under distancing fatigue and show that a\nstrict lockdown at the beginning of an epidemic (as, for example, recently in\nChina) can lead to unintended adverse consequences. When the policy is lifted\nthe disease spreads very fast due to the accumulated distancing fatigue of the\nindividuals causing high prevalence levels.\n"
    },
    {
        "paper_id": 2206.03896,
        "authors": "Boris David and Gilles Zumbach",
        "title": "Multivariate backtests and copulas for risk evaluation",
        "comments": "26 pages, 14 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Risk evaluation is a forecast, and its validity must be backtested.\nProbability distribution forecasts are used in this work and allow for more\npowerful validations compared to point forecasts. Our aim is to use bivariate\ncopulas in order to characterize the in-sample copulas and to validate\nout-of-sample a bivariate forecast. For both set-ups, probability integral\ntransforms (PIT) and Rosenblatt transforms are used to map the problem into an\nindependent copula. For this simple copula, statistical tests can be applied to\nvalidate the choice of the in-sample copula or the validity of the bivariate\nforecast. The salient results are that a Student copula describes well the\ndependencies between financial time series (regardless of the correlation), and\nthat the bivariate forecasts provided by a risk methodology based on historical\ninnovations performs correctly out-of-sample. A prerequisite is to remove the\nheteroskedasticity in order to have stationary time series, in this work a\nlong-memory ARCH volatility model is used.\n"
    },
    {
        "paper_id": 2206.03919,
        "authors": "Conner Mullally, Sarah Janzen, Nicholas Magnan, Shruti Sharma, Bhola\n  Shrestha",
        "title": "Can Mobile Technology Improve Female Entrepreneurship? Evidence from\n  Nepal",
        "comments": "There was an error in the code used to generate table 10 \"Comparing\n  outcomes with traditional and distance training, traditional trainees\". Terms\n  were accidentally omitted from an imputation step used to create the numbers\n  in columns two and three of the table",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Gender norms may constrain the ability of women to develop their\nentrepreneurial skills, particularly in rural areas. By bringing\nentrepreneurial training to women rather than requiring extended time away from\nhome, mobile technology could open doors that would otherwise be closed. We\nrandomly selected Nepali women to be trained as veterinary service providers\nknown as community animal health workers. Half of the selected candidates were\nrandomly assigned to a traditional training course requiring 35 consecutive\ndays away from home, and half were assigned to a hybrid distance learning\ncourse requiring two shorter stays plus a table-based curriculum to be\ncompleted at home. Distance learning strongly increases women's ability to\ncomplete training as compared to traditional training. Distance learning has a\nlarger effect than traditional training on boosting the number of livestock\nresponsibilities women carry out at home, while also raising aspirations. Both\ntraining types increase women's control over income. Our results indicate that\nif anything, distance learning produced more effective community animal health\nworkers.\n"
    },
    {
        "paper_id": 2206.04013,
        "authors": "Maksim Borisov, Valeria Kolycheva, Alexander Semenov and Dmitry\n  Grigoriev",
        "title": "The influence of color on prices of abstract paintings",
        "comments": "23 pages, 8 figures, 4 tables",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Determination of price of an artwork is a fundamental problem in cultural\neconomics. In this work we investigate what impact visual characteristics of a\npainting have on its price. We construct a number of visual features measuring\ncomplexity of the painting, its points of interest, segmentation-based\nfeatures, local color features, and features based on Itten and Kandinsky\ntheories, and utilize mixed-effects model to study impact of these features on\nthe painting price. We analyze the influence of the color on the example of the\nmost complex art style - abstractionism, by created Kandinsky, for which the\ncolor is the primary basis. We use Itten's theory - the most recognized color\ntheory in art history, from which the largest number of subtheories was born.\nFor this day it is taken as the base for teaching artists. We utilize novel\ndataset of 3885 paintings collected from Christie's and Sotheby's and find that\ncolor harmony has some explanatory power, color complexity metrics are\ninsignificant and color diversity explains price well.\n"
    },
    {
        "paper_id": 2206.04257,
        "authors": "Ji Hyung Lee, Yuya Sasaki, Alexis Akira Toda, Yulong Wang",
        "title": "Capital and Labor Income Pareto Exponents in the United States,\n  1916-2019",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Accurately estimating income Pareto exponents is challenging due to\nlimitations in data availability and the applicability of statistical methods.\nUsing tabulated summaries of incomes from tax authorities and a recent\nestimation method, we estimate income Pareto exponents in U.S. for 1916-2019.\nWe find that during the past three decades, the capital and labor income Pareto\nexponents have been stable at around 1.2 and 2. Our findings suggest that the\ntop tail income and wealth inequality is higher and wealthy agents have twice\nas large an impact on the aggregate economy than previously thought but there\nis no clear trend post-1985.\n"
    },
    {
        "paper_id": 2206.0435,
        "authors": "P Fogel, C Geissler, P Cotte, G Luta (GU)",
        "title": "Applying separative non-negative matrix factorization to extra-financial\n  data",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We present here an original application of the non-negative matrix\nfactorization (NMF) method, for the case of extra-financial data. These data\nare subject to high correlations between co-variables, as well as between\nobservations. NMF provides a much more relevant clustering of co-variables and\nobservations than a simple principal component analysis (PCA). In addition, we\nshow that an initial data separation step before applying NMF further improves\nthe quality of the clustering.\n"
    },
    {
        "paper_id": 2206.04424,
        "authors": "Xavier D'Haultf{\\oe}uille and Ao Wang and Philippe F\\'evrier and\n  Lionel Wilner",
        "title": "Estimating the Gains (and Losses) of Revenue Management",
        "comments": "79 pages (the appendix starts at p.42)",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  Despite the wide adoption of revenue management in many industries such as\nairline, railway, and hospitality, there is still scarce empirical evidence on\nthe gains or losses of such strategies compared to uniform pricing or fully\nflexible strategies. We quantify such gains and losses and identify their\nunderlying sources in the context of French railway transportation. The\nidentification of demand is complicated by censoring and the absence of\nexogenous price variations. We develop an original identification strategy\ncombining temporal variations in relative prices, consumers' rationality and\nweak optimality conditions on the firm's pricing strategy. Our results suggest\nsimilar or better performance of the actual revenue management compared to\noptimal uniform pricing, but also substantial losses of up to 16.7% compared to\nthe optimal pricing strategy. We also highlight the key role of revenue\nmanagement in acquiring information when demand is uncertain.\n"
    },
    {
        "paper_id": 2206.04634,
        "authors": "Robin Fritsch, Samuel K\\\"aser and Roger Wattenhofer",
        "title": "The Economics of Automated Market Makers",
        "comments": null,
        "journal-ref": "Proceedings of the 4th ACM Conference on Advances in Financial\n  Technologies (2022) 102-110",
        "doi": "10.1145/3558535.3559790",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper studies the question whether automated market maker protocols such\nas Uniswap can sustainably retain a portion of their trading fees for the\nprotocol. We approach the problem by modelling how to optimally choose a pool's\ntake rate, i.e\\ the fraction of fee revenue that remains with the protocol, in\norder to maximize the protocol's revenue. The model suggest that if AMMs have a\nportion of loyal trade volume, they can sustainably set a non-zero take rate,\neven without losing liquidity to competitors with a zero take rate.\nFurthermore, we determine the optimal take rate depending on a number of model\nparameters including how much loyal trade volume pools have and how high the\ncompetitors' take rates are.\n"
    },
    {
        "paper_id": 2206.0468,
        "authors": "Katia Colaneri and Julia Eisenberg and Benedetta Salterini",
        "title": "Some Optimisation Problems in Insurance with a Terminal Distribution\n  Constraint",
        "comments": "23 pages, 5 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper, we study two optimisation settings for an insurance company,\nunder the constraint that the terminal surplus at a deterministic and finite\ntime $T$ follows a normal distribution with a given mean and a given variance.\nIn both cases, the surplus of the insurance company is assumed to follow a\nBrownian motion with drift. First, we allow the insurance company to pay\ndividends and seek to maximise the expected discounted dividend payments or to\nminimise the ruin probability under the terminal distribution constraint. Here,\nwe find explicit expressions for the optimal strategies in both cases: in\ndiscrete and continuous time settings. Second, we let the insurance company buy\na reinsurance contract for a pool of insured or a branch of business. To\nachieve a certain level of sustainability (i.e. the collected premia should be\nsufficient to buy reinsurance and to pay the occurring claims) the initial\ncapital is set to be zero. We only allow for piecewise constant reinsurance\nstrategies producing a normally distributed terminal surplus, whose mean and\nvariance lead to a given Value at Risk or Expected Shortfall at some confidence\nlevel $\\alpha$. We investigate the question which admissible reinsurance\nstrategy produces a smaller ruin probability, if the ruin-checks are due at\ndiscrete deterministic points in time.\n"
    },
    {
        "paper_id": 2206.04773,
        "authors": "Cheng Lin, Adel Daoud, Maria Branden",
        "title": "To What Extent Do Disadvantaged Neighborhoods Mediate Social Assistance\n  Dependency? Evidence from Sweden",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Occasional social assistance prevents individuals from a range of social\nills, particularly unemployment and poverty. It remains unclear, however, how\nand to what extent continued reliance on social assistance leads to individuals\nbecoming trapped in social assistance dependency. In this paper, we build on\nthe theory of cumulative disadvantage and examine whether the accumulated use\nof social assistance over the life course is associated with an increased risk\nof future social assistance recipiency. We also analyze the extent to which\nliving in disadvantaged neighborhoods constitutes an important mechanism in the\nexplanation of this association. Our analyses use Swedish population registers\nfor the full population of individuals born in 1981, and these individuals are\nfollowed for approximately 17 years. While most studies are limited by a lack\nof granular, life-history data, our granular individual-level data allow us to\napply causal-mediation analysis, and thereby quantify the extent to which the\nlikelihood of ending up in social assistance dependency is affected by residing\nin disadvantaged neighborhoods. Our findings show the accumulation of social\nassistance over the studied period is associated with a more than four-fold\nincrease on a risk ratio scale for future social assistance recipiency,\ncompared to never having received social assistance during the period examined.\nThen, we examine how social assistance dependency is mediated by prolonged\nexposure to disadvantaged neighborhoods. Our results suggest that the indirect\neffect of disadvantaged neighborhoods is weak to moderate. Therefore, social\nassistance dependency may be a multilevel process. Future research is to\nexplore how the mediating effects of disadvantaged neighborhoods vary in\ndifferent contexts.\n"
    },
    {
        "paper_id": 2206.0495,
        "authors": "Chiara Natalie Focacci, Mitja Kovac, Rok Spruk",
        "title": "The perils of Kremlin's influence: evidence from Ukraine",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  We examine the contribution of institutional integration to the institutional\nquality. To this end, we exploit the 2007 political crisis in Ukraine and\nexamine the effects of staying out of the European Union for 28 Ukrainian\nprovinces in the period 1996-2020. We construct novel subnational estimates of\ninstitutional quality for Ukraine and central and eastern European countries\nbased on the latent residual component extraction of institutional quality from\nthe existing governance indicators by making use of Bayesian posterior analysis\nunder non-informative objective prior function. By comparing the residualized\ninstitutional quality trajectories of Ukrainian provinces with their central\nand eastern European peers that were admitted to the European Union in 2004 and\nafter, we assess the institutional quality cost of being under Russian\npolitical influence and interference. Based on the large-scale synthetic\ncontrol analysis, we find evidence of large-scale negative institutional\nquality effects of staying out of the European Union such as heightened\npolitical instability and rampant deterioration of the rule of law and control\nof corruption. Statistical significance of the estimated effects is evaluated\nacross a comprehensive placebo simulation with more than 34 billion placebo\naverages for each institutional quality outcome.\n"
    },
    {
        "paper_id": 2206.05022,
        "authors": "Eric Ngondiep",
        "title": "A Fast Third-Step Second-Order Explicit Numerical Approach To\n  Investigating and Forecasting The Dynamic of Corruption And Poverty In\n  Cameroon",
        "comments": "21 pages, 5 tables, 5 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-sa/4.0/",
        "abstract": "  This paper constructs a third-step second-order numerical approach for\nsolving a mathematical model on the dynamic of corruption and poverty. The\nstability and error estimates of the proposed technique are analyzed using the\n$L^{2}$-norm. The developed algorithm is at less zero-stable and second-order\naccurate. Furthermore, the new method is explicit, fast and more efficient than\na large class of numerical schemes applied to nonlinear systems of ordinary\ndifferential equations and can serve as a robust tool for integrating general\nsystems of initial-value problems. Some numerical examples confirm the theory\nand also consider the corruption and poverty in Cameroon.\n"
    },
    {
        "paper_id": 2206.05081,
        "authors": "Carlo Campajola, Raffaele Cristodaro, Francesco Maria De Collibus, Tao\n  Yan, Nicolo' Vallarano, Claudio J. Tessone",
        "title": "The Evolution Of Centralisation on Cryptocurrency Platforms",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  More than ten years ago the blockchain was acclaimed as the solution to\novercome centralised trusted third parties for online payments. Through the\nyears the crypto-movement changed and evolved, although decentralisation\nremained the core ideology and the necessary feature every new crypto-project\nshould provide. In this paper we study the concept of centralisation in\ncryptocurrencies using a wide array of methodologies from the complex systems\nliterature, on a comparative collection of blockchains, in order to define the\nmany different levels a blockchain system may display (de-)centralisation and\nto question whether the present state of cryptocurrencies is, in a\ntechnological and economical sense, actually decentralised.\n"
    },
    {
        "paper_id": 2206.05134,
        "authors": "Giorgio Costa, Garud N. Iyengar",
        "title": "Distributionally Robust End-to-End Portfolio Construction",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  We propose an end-to-end distributionally robust system for portfolio\nconstruction that integrates the asset return prediction model with a\ndistributionally robust portfolio optimization model. We also show how to learn\nthe risk-tolerance parameter and the degree of robustness directly from data.\nEnd-to-end systems have an advantage in that information can be communicated\nbetween the prediction and decision layers during training, allowing the\nparameters to be trained for the final task rather than solely for predictive\nperformance. However, existing end-to-end systems are not able to quantify and\ncorrect for the impact of model risk on the decision layer. Our proposed\ndistributionally robust end-to-end portfolio selection system explicitly\naccounts for the impact of model risk. The decision layer chooses portfolios by\nsolving a minimax problem where the distribution of the asset returns is\nassumed to belong to an ambiguity set centered around a nominal distribution.\nUsing convex duality, we recast the minimax problem in a form that allows for\nefficient training of the end-to-end system.\n"
    },
    {
        "paper_id": 2206.05374,
        "authors": "Chiranjit Dutta, Nalini Ravishanker and Sumanta Basu",
        "title": "Modeling Multivariate Positive-Valued Time Series Using R-INLA",
        "comments": "19 pages, 1 figure",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  In this paper we describe fast Bayesian statistical analysis of vector\npositive-valued time series, with application to interesting financial data\nstreams. We discuss a flexible level correlated model (LCM) framework for\nbuilding hierarchical models for vector positive-valued time series. The LCM\nallows us to combine marginal gamma distributions for the positive-valued\ncomponent responses, while accounting for association among the components at a\nlatent level. We use integrated nested Laplace approximation (INLA) for fast\napproximate Bayesian modeling via the R-INLA package, building custom functions\nto handle this setup. We use the proposed method to model interdependencies\nbetween realized volatility measures from several stock indexes.\n"
    },
    {
        "paper_id": 2206.0541,
        "authors": "Bingyan Han",
        "title": "Cooperation between Independent Market Makers",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  With the digitalization of the financial market, dealers are increasingly\nhandling market-making activities by algorithms. Recent antitrust literature\nraises concerns on collusion caused by artificial intelligence. This paper\nstudies the possibility of cooperation between market makers via independent\nQ-learning. Market making with inventory risk is formulated as a repeated\ngeneral-sum game. Under a stag-hunt type payoff, we find that market makers can\nlearn cooperative strategies without communication. In general, high spreads\ncan have the largest probability even when the lowest spread is the unique Nash\nequilibrium. Moreover, introducing more agents into the game does not\nnecessarily eliminate the presence of supra-competitive spreads.\n"
    },
    {
        "paper_id": 2206.05425,
        "authors": "Guanxing Fu",
        "title": "Mean Field Portfolio Games with Consumption",
        "comments": "19 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We study mean field portfolio games with consumption. For general market\nparameters, we establish a one-to-one correspondence between Nash equilibria of\nthe game and solutions to some FBSDE, which is proved to be equivalent to some\nBSDE. Our approach, which is general enough to cover power, exponential and log\nutilities, relies on martingale optimality principle in [3,9] and dynamic\nprogramming principle in [6,7]. When the market parameters do not depend on the\nBrownian paths, we get the unique Nash equilibrium in closed form. As a\nbyproduct, when all market parameters are time-independent, we answer the\nquestion proposed in [12]: the strong equilibrium obtained in [12] is unique in\nthe essentially bounded space.\n"
    },
    {
        "paper_id": 2206.05568,
        "authors": "Benjamin Patrick Evans, Mikhail Prokopenko",
        "title": "Bounded strategic reasoning explains crisis emergence in multi-agent\n  market games",
        "comments": "10 pages + 7 page appendix, 7 figures",
        "journal-ref": null,
        "doi": "10.1098/rsos.221164",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The efficient market hypothesis (EMH), based on rational expectations and\nmarket equilibrium, is the dominant perspective for modelling economic markets.\nHowever, the most notable critique of the EMH is the inability to model periods\nof out-of-equilibrium behaviour in the absence of any significant external\nnews. When such dynamics emerge endogenously, the traditional economic\nframeworks provide no explanation for such behaviour and the deviation from\nequilibrium. This work offers an alternate perspective explaining the\nendogenous emergence of punctuated out-of-equilibrium dynamics based on bounded\nrational agents. In a concise market entrance game, we show how boundedly\nrational strategic reasoning can lead to endogenously emerging crises,\nexhibiting fat tails in \"returns\". We also show how other common stylised facts\nof economic markets, such as clustered volatility, can be explained due to\nagent diversity (or lack thereof) and the varying learning updates across the\nagents. This work explains various stylised facts and crisis emergence in\neconomic markets, in the absence of any external news, based purely on agent\ninteractions and bounded rational reasoning.\n"
    },
    {
        "paper_id": 2206.05705,
        "authors": "Mesrop T. Mesropyan and Vardan G. Bardakhchyan (Yerevan State\n  University)",
        "title": "Hellinger distance to normal distribution as market invariant",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Main purpose of distance based portfolio constructions is in portfolio\nimitation. Here we construct portfolio based on Hellinger distance from normal\ndistribution. We empirically found that minimum of this distance drastically\nvaries from market to market. Thus we suppose that it may be regarded as a form\nof market invariant, in a sense of helpful tool for market segmentation. We\nanalyze its sensitivity. Though mean sensitivity was small it showed extreme\nsensitivity in some cases.\n"
    },
    {
        "paper_id": 2206.05835,
        "authors": "Fatih Ozhamaratli (1) and Paolo Barucca (1) ((1) University College\n  London)",
        "title": "Deep Reinforcement Learning for Optimal Investment and Saving Strategy\n  Selection in Heterogeneous Profiles: Intelligent Agents working towards\n  retirement",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  The transition from defined benefit to defined contribution pension plans\nshifts the responsibility for saving toward retirement from governments and\ninstitutions to the individuals. Determining optimal saving and investment\nstrategy for individuals is paramount for stable financial stance and for\navoiding poverty during work-life and retirement, and it is a particularly\nchallenging task in a world where form of employment and income trajectory\nexperienced by different occupation groups are highly diversified. We introduce\na model in which agents learn optimal portfolio allocation and saving\nstrategies that are suitable for their heterogeneous profiles. We use deep\nreinforcement learning to train agents. The environment is calibrated with\noccupation and age dependent income evolution dynamics. The research focuses on\nheterogeneous income trajectories dependent on agent profiles and incorporates\nthe behavioural parameterisation of agents. The model provides a flexible\nmethodology to estimate lifetime consumption and investment choices for\nheterogeneous profiles under varying scenarios.\n"
    },
    {
        "paper_id": 2206.0591,
        "authors": "Zitao Song, Xuyang Jin, Chenliang Li",
        "title": "Safe-FinRL: A Low Bias and Variance Deep Reinforcement Learning\n  Implementation for High-Freq Stock Trading",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In recent years, many practitioners in quantitative finance have attempted to\nuse Deep Reinforcement Learning (DRL) to build better quantitative trading (QT)\nstrategies. Nevertheless, many existing studies fail to address several serious\nchallenges, such as the non-stationary financial environment and the bias and\nvariance trade-off when applying DRL in the real financial market. In this\nwork, we proposed Safe-FinRL, a novel DRL-based high-freq stock trading\nstrategy enhanced by the near-stationary financial environment and low bias and\nvariance estimation. Our main contributions are twofold: firstly, we separate\nthe long financial time series into the near-stationary short environment;\nsecondly, we implement Trace-SAC in the near-stationary financial environment\nby incorporating the general retrace operator into the Soft Actor-Critic.\nExtensive experiments on the cryptocurrency market have demonstrated that\nSafe-FinRL has provided a stable value estimation and a steady policy\nimprovement and reduced bias and variance significantly in the near-stationary\nfinancial environment.\n"
    },
    {
        "paper_id": 2206.06026,
        "authors": "Konstantin G\\\"orgen, Abdolreza Nazemi, Melanie Schienle",
        "title": "Robust Knockoffs for Controlling False Discoveries With an Application\n  to Bond Recovery Rates",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  We address challenges in variable selection with highly correlated data that\nare frequently present in finance, economics, but also in complex natural\nsystems as e.g. weather. We develop a robustified version of the knockoff\nframework, which addresses challenges with high dependence among possibly many\ninfluencing factors and strong time correlation. In particular, the repeated\nsubsampling strategy tackles the variability of the knockoffs and the\ndependency of factors. Simultaneously, we also control the proportion of false\ndiscoveries over a grid of all possible values, which mitigates variability of\nselected factors from ad-hoc choices of a specific false discovery level. In\nthe application for corporate bond recovery rates, we identify new important\ngroups of relevant factors on top of the known standard drivers. But we also\nshow that out-of-sample, the resulting sparse model has similar predictive\npower to state-of-the-art machine learning models that use the entire set of\npredictors.\n"
    },
    {
        "paper_id": 2206.06109,
        "authors": "Ariel Neufeld, Julian Sester, Mario \\v{S}iki\\'c",
        "title": "Markov Decision Processes under Model Uncertainty",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We introduce a general framework for Markov decision problems under model\nuncertainty in a discrete-time infinite horizon setting. By providing a dynamic\nprogramming principle we obtain a local-to-global paradigm, namely solving a\nlocal, i.e., a one time-step robust optimization problem leads to an optimizer\nof the global (i.e. infinite time-steps) robust stochastic optimal control\nproblem, as well as to a corresponding worst-case measure. Moreover, we apply\nthis framework to portfolio optimization involving data of the S&P 500. We\npresent two different types of ambiguity sets; one is fully data-driven given\nby a Wasserstein-ball around the empirical measure, the second one is described\nby a parametric set of multivariate normal distributions, where the\ncorresponding uncertainty sets of the parameters are estimated from the data.\nIt turns out that in scenarios where the market is volatile or bearish, the\noptimal portfolio strategies from the corresponding robust optimization problem\noutperforms the ones without model uncertainty, showcasing the importance of\ntaking model uncertainty into account.\n"
    },
    {
        "paper_id": 2206.06201,
        "authors": "Jackie Grant, Mark Hindmarsh and Sergey E. Koposov",
        "title": "The distribution of loss to future USS pensions due to the UUK cuts of\n  April 2022",
        "comments": "29 pp, 7 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We present the first global analysis of the impact of the April 2022 cuts to\nthe future pensions of members of the Universities Superannuation Scheme. For\nthe 196,000 active members, if Consumer Price Inflation (CPI) remains at its\nhistoric average of 2.5%, the distribution of the range of cuts peaks between\n30%-35%. This peak increases to 40%-45% cuts if CPI averages 3.0%. The global\nloss across current USS scheme members, in today's money, is calculated to be\n16-18 billion GBP, with most of the 71,000 staff under the age of 40 losing\nbetween 100k-200k GBP each, for CPI averaging 2.5%-3.0%. A repeated claim made\nduring the formal consultation by the body representing university management\n(Universities UK) that those earning under 40k GBP would receive a \"headline\"\ncut of 12% to their future pension is shown to be a serious underestimate for\nrealistic CPI projections.\n"
    },
    {
        "paper_id": 2206.0632,
        "authors": "Ramit Sawhney, Shivam Agarwal, Vivek Mittal, Paolo Rosso, Vikram\n  Nanda, Sudheer Chava",
        "title": "Cryptocurrency Bubble Detection: A New Stock Market Dataset, Financial\n  Task & Hyperbolic Models",
        "comments": "Proceedings of the 2022 Conference of the North American Chapter of\n  the Association for Computational Linguistics: Human Language Technologies",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  The rapid spread of information over social media influences quantitative\ntrading and investments. The growing popularity of speculative trading of\nhighly volatile assets such as cryptocurrencies and meme stocks presents a\nfresh challenge in the financial realm. Investigating such \"bubbles\" - periods\nof sudden anomalous behavior of markets are critical in better understanding\ninvestor behavior and market dynamics. However, high volatility coupled with\nmassive volumes of chaotic social media texts, especially for underexplored\nassets like cryptocoins pose a challenge to existing methods. Taking the first\nstep towards NLP for cryptocoins, we present and publicly release\nCryptoBubbles, a novel multi-span identification task for bubble detection, and\na dataset of more than 400 cryptocoins from 9 exchanges over five years\nspanning over two million tweets. Further, we develop a set of\nsequence-to-sequence hyperbolic models suited to this multi-span identification\ntask based on the power-law dynamics of cryptocurrencies and user behavior on\nsocial media. We further test the effectiveness of our models under zero-shot\nsettings on a test set of Reddit posts pertaining to 29 \"meme stocks\", which\nsee an increase in trade volume due to social media hype. Through quantitative,\nqualitative, and zero-shot analyses on Reddit and Twitter spanning cryptocoins\nand meme-stocks, we show the practical applicability of CryptoBubbles and\nhyperbolic models.\n"
    },
    {
        "paper_id": 2206.06723,
        "authors": "Elivelto Ebermam, Helder Knidel, Renato A. Krohling",
        "title": "Development of a hybrid method for stock trading based on TOPSIS, EMD\n  and ELM",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/publicdomain/zero/1.0/",
        "abstract": "  Deciding when to buy or sell a stock is not an easy task because the market\nis hard to predict, being influenced by political and economic factors. Thus,\nmethodologies based on computational intelligence have been applied to this\nchallenging problem. In this work, every day the stocks are ranked by technique\nfor order preference by similarity to ideal solution (TOPSIS) using technical\nanalysis criteria, and the most suitable stock is selected for purchase. Even\nso, it may occur that the market is not favorable to purchase on certain days,\nor even, the TOPSIS make an incorrect selection. To improve the selection,\nanother method should be used. So, a hybrid model composed of empirical mode\ndecomposition (EMD) and extreme learning machine (ELM) is proposed. The EMD\ndecomposes the series into several sub-series, and thus the main omponent\n(trend) is extracted. This component is processed by the ELM, which performs\nthe prediction of the next element of component. If the value predicted by the\nELM is greater than the last value, then the purchase of the stock is\nconfirmed. The method was applied in a universe of 50 stocks in the Brazilian\nmarket. The selection made by TOPSIS showed promising results when compared to\nthe random selection and the return generated by the Bovespa index.\nConfirmation with the EMD-ELM hybrid model was able to increase the percentage\nof profit tradings.\n"
    },
    {
        "paper_id": 2206.06764,
        "authors": "Michele Vodret, Iacopo Mastromatteo, Bence Toth and Michael Benzaquen",
        "title": "Microfounding GARCH Models and Beyond: A Kyle-inspired Model with\n  Adaptive Agents",
        "comments": "20 pages, 8 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We relax the strong rationality assumption for the agents in the paradigmatic\nKyle model of price formation, thereby reconciling the framework of\nasymmetrically informed traders with the Adaptive Market Hypothesis, where\nagents use inductive rather than deductive reasoning. Building on these ideas,\nwe propose a stylised model able to account parsimoniously for a rich\nphenomenology, ranging from excess volatility to volatility clustering. While\ncharacterising the excess-volatility dynamics, we provide a microfoundation for\nGARCH models. Volatility clustering is shown to be related to the self-excited\ndynamics induced by traders' behaviour, and does not rely on clustered\nfundamental innovations. Finally, we propose an extension to account for the\nfragile dynamics exhibited by real markets during flash crashes.\n"
    },
    {
        "paper_id": 2206.06802,
        "authors": "Liu Ziyin, Katsuya Ito, Kentaro Imajo, Kentaro Minami",
        "title": "Power Laws and Symmetries in a Minimal Model of Financial Market Economy",
        "comments": "Preprint of a version to be published in Physical Review Research",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  A financial market is a system resulting from the complex interaction between\nparticipants in a closed economy. We propose a minimal microscopic model of the\nfinancial market economy based on the real economy's symmetry constraint and\nminimality requirement. We solve the proposed model analytically in the\nmean-field regime, which shows that various kinds of universal power-law-like\nbehaviors in the financial market may depend on one another, just like the\ncritical exponents in physics. We then discuss the parameters in the proposed\nmodel, and we show that each parameter in our model can be related to\nmeasurable quantities in the real market, which enables us to discuss the cause\nof a few kinds of social and economic phenomena.\n"
    },
    {
        "paper_id": 2206.0682,
        "authors": "Laura Ch\\'emali, Camille Souffron (ENS-PSL)",
        "title": "Repenser le financement des entreprises vertueuses et les politiques\n  prudentielles en int{\\'e}grant la solvabilit{\\'e} socio-environnementale",
        "comments": "in French language",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Despite the amount of savings available and the money supply managed by\nfinancial institutions, significant market failures and the failure of carbon\npricing strategies prevent sufficient financing of the transition, notably\nthrough bank credit. Aware of the links between natural, monetary and\nproductive aggregates, we propose the development of ''eco-systemic''\nprudential policies by exposing the interdependence between macro, micro and\nenvironmental prudential measures. These would be based on a reorientation of\ncorporate accounting standards towards the concept of socio-environmental\nsolvency, notably the CARE-TDL model (integration of human and natural capital\nalongside financial capital on the liabilities side of the balance sheet). In\nan ecosystemic framework, this solvency of virtuous companies would compensate\nin accounting terms for the lack of financial solvency. The State would then be\nthe guarantor in order to facilitate their access to financing, also reduced by\nBasel III and Solvency II. This policy develops a system of reallocation of\nfinancing capacities from non-virtuous companies to the most virtuous ones with\npublic guarantees, aiming to reduce the debt ratio while increasing green\ninvestments, with monetary policies of rates but also of volumes and ratios\ndifferentiated according to the types of assets and the greening of bank\nbalance sheets, and finally forms of public-private partnership. Facilitating\nthe financing of green companies would green capital but increase it, partly\nneutralising the positive environmental impact. It is therefore necessary to\nlimit the credit expansion of ''brown'' companies. This would reduce risky\noperations and favour less leveraged investments more connected to the real\neconomy, reducing systemic financial risk. -- The Agenda 2030 Policy Briefs\nseries (PoCFiN Kedge Business School - SDSN France - Institut Rousseau)\nmobilises economists and practitioners to identify an agenda of economic and\nfinancial reforms to achieve the 2030 Agenda, at territorial, national and\nsupranational levels. These are published after peer review.\n"
    },
    {
        "paper_id": 2206.0713,
        "authors": "Ivan Arraut, Joao Alexandre Lobo Marques, and Sergio Gomes",
        "title": "The probability flow in the Stock market and Spontaneous symmetry\n  breaking in Quantum Finance",
        "comments": "15 pages, preprint of a published version. arXiv admin note:\n  substantial text overlap with arXiv:2004.11270",
        "journal-ref": "Mathematics 2021, 9, 2777",
        "doi": "10.3390/math9212777",
        "license": "http://creativecommons.org/licenses/by-sa/4.0/",
        "abstract": "  The Spontaneous Symmetry breaking in Quantum Finance considers the martingale\ncondition in the stock market as a vacuum state if we express the financial\nequations in the Hamiltonian form. The original analysis for this phenomena\nignores completely the kinetic terms in the neighborhood of the minimal of the\npotential terms. This is correct in most of the cases. However, when we deal\nwith the Martingale condition, it comes out that the kinetic terms can also\nbehave as potential terms and then reproduce a shift on the effective location\nof the vacuum (Martingale). In this paper we analyze the effective symmetry\nbreaking patterns and the connected vacuum degeneracy for these special\ncircumstances. Within the same scenario, we analyze the connection between the\nflow of information and the multiplicity of martingale states, providing in\nthis way powerful tools for analyzing the dynamic of the stock market.\n"
    },
    {
        "paper_id": 2206.07132,
        "authors": "Hannah Gampe and Christopher Griffin",
        "title": "Dynamics of a Binary Option Market with Exogenous Information and Price\n  Sensitivity",
        "comments": "16 pages, 6 figures",
        "journal-ref": null,
        "doi": "10.1016/j.cnsns.2022.106994",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper, we derive and analyze a continuous of a binary option market\nwith exogenous information. The resulting non-linear system has a discontinuous\nright hand side, which can be analyzed using zero-dimensional Filippov\nsurfaces. Under general assumptions on purchasing rules, we show that when\nexogenous information is constant in the binary asset market, the price always\nconverges. We then investigate market prices in the case of changing\ninformation, showing empirically that price sensitivity has a strong effect on\nprice lag vs. information. We conclude with open questions on general $n$-ary\noption markets. As a by-product of the analysis, we show that these markets are\nequivalent to a simple recurrent neural network, helping to explain some of the\npredictive power associated with prediction markets, which are usually designed\nas $n$-ary option markets.\n"
    },
    {
        "paper_id": 2206.07537,
        "authors": "Francesco de Cunzo, Alberto Petri, Andrea Zaccaria, Angelica Sbardella",
        "title": "The trickle down from environmental innovation to productive complexity",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We study the empirical relationship between green technologies and industrial\nproduction at very fine-grained levels by employing Economic Complexity\ntechniques. Firstly, we use patent data on green technology domains as a proxy\nfor competitive green innovation and data on exported products as a proxy for\ncompetitive industrial production. Secondly, with the aim of observing how\ngreen technological development trickles down into industrial production, we\nbuild a bipartite directed network linking single green technologies at time\n$t_1$ to single products at time $t_2 \\ge t_1$ on the basis of their\ntime-lagged co-occurrences in the technological and industrial specialization\nprofiles of countries. Thirdly we filter the links in the network by employing\na maximum entropy null-model. In particular, we find that the industrial\nsectors most connected to green technologies are related to the processing of\nraw materials, which we know to be crucial for the development of clean energy\ninnovations. Furthermore, by looking at the evolution of the network over time,\nwe observe that more complex green technological know-how requires more time to\nbe transmitted to industrial production, and is also linked to more complex\nproducts.\n"
    },
    {
        "paper_id": 2206.07596,
        "authors": "Jing Liu, Laura Bowling, Christopher Kucharik, Sadia Jame, Uris\n  Baldos, Larissa Jarvis, Navin Ramankutty, and Thomas Hertel",
        "title": "Multi-scale Analysis of Nitrogen Loss Mitigation in the US Corn Belt",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
        "abstract": "  Reducing the size of the hypoxic zone in the Gulf of Mexico has proven to be\na challenging task. A variety of mitigation options have been proposed, each\nlikely to produce markedly different patterns of mitigation with widely varying\nconsequences for the economy. The general consensus is that no single measure\nalone is sufficient to achieve the EPA Task Force goal for reducing the Gulf\nhypoxic zone and it appears that a combination of management practices must be\nemployed. However, absent a highly resolved, multi-scale framework for\nassessing these policy combinations, it has been unclear what pattern of\nmitigation is likely to emerge from different policies and what the\nconsequences would be for local, regional and national land use, food prices\nand farm returns. We address this research gap by utilizing a novel multi-scale\nframework for evaluating alternative N loss management policies in the\nMississippi River basin. This combines fine-scale agro-ecosystem responses with\nan economic model capturing domestic and international market and price\nlinkages. We find that wetland restoration combined with improved N use\nefficiency, along with a leaching tax could reduce the Mississippi River N load\nby 30-53\\% while only modestly increasing corn prices. This study underscores\nthe value of fine-resolution analysis and the potential of combined economic\nand ecological instruments in tackling nonpoint source nitrate pollution.\n"
    },
    {
        "paper_id": 2206.07831,
        "authors": "Jaros{\\l}aw Kwapie\\'n, Marcin W\\k{a}torek, Marija Bezbradica, Martin\n  Crane, Tai Tan Mai, Stanis{\\l}aw Dro\\.zd\\.z",
        "title": "Analysis of inter-transaction time fluctuations in the cryptocurrency\n  market",
        "comments": null,
        "journal-ref": "Chaos 32, 083142 (2022)",
        "doi": "10.1063/5.0104707",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We analyse tick-by-tick data representing major cryptocurrencies traded on\nsome different cryptocurrency trading platforms. We focus on such quantities\nlike the inter-transaction times, the number of transactions in time unit, the\ntraded volume, and volatility. We show that the inter-transaction times show\nlong-range power-law autocorrelations. These lead to multifractality expressed\nby the right-side asymmetry of the singularity spectra $f(\\alpha)$ indicating\nthat the periods of increased market activity are characterised by richer\nmultifractality compared to the periods of quiet market. We also show that\nneither the stretched exponential distribution nor the power-law-tail\ndistribution are able to model universally the cumulative distribution\nfunctions of the quantities considered in this work. For each quantity, some\ndata sets can be modeled by the former, some data sets by the latter, while\nboth fail in other cases. An interesting, yet difficult to account for,\nobservation is that parallel data sets from different trading platforms can\nshow disparate statistical properties.\n"
    },
    {
        "paper_id": 2206.08117,
        "authors": "Jin Hyuk Choi, Heeyoung Kwon and Kasper Larsen",
        "title": "Trading constraints in continuous-time Kyle models",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In a continuous-time Kyle setting, we prove global existence of an\nequilibrium when the insider faces a terminal trading constraint. We prove that\nour equilibrium model produces output consistent with several empirical\nstylized facts such as autocorrelated aggregate holdings, decreasing price\nimpacts over the trading day, and U shaped optimal trading patterns.\n"
    },
    {
        "paper_id": 2206.08133,
        "authors": "Leonardo Massai, Giacomo Como, Fabio Fagnani",
        "title": "Equilibria in Network Constrained Energy Markets",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  We study an energy market composed of producers who compete to supply energy\nto different markets and want to maximize their profits. The energy market is\nmodeled by a graph representing a constrained power network where nodes\nrepresent the markets and links are the physical lines with a finite capacity\nconnecting them. Producers play a networked Cournot game on such a network\ntogether with a centralized authority, called market maker, that facilitates\nthe trade between geographically separate markets via the constrained power\nnetwork and aims to maximize a certain welfare function. We first prove a\ngeneral result that links the optimal action of the market maker with the\ncapacity constraint enforced on the power network. Under mild assumptions, we\nstudy the existence and uniqueness of Nash equilibria and exploit our general\nresult to prove a connection between capacity bottlenecks in the power network\nand the emergence of price differences between different markets that are\nseparated by saturated lines, a phenomenon that is often observed in real power\nnetworks.\n"
    },
    {
        "paper_id": 2206.08401,
        "authors": "Ziqiao Ao, Lin William Cong, Gergely Horvath, Luyao Zhang",
        "title": "Is decentralized finance actually decentralized? A social network\n  analysis of the Aave protocol on the Ethereum blockchain",
        "comments": "Accepted at 29th Annual Global Finance Conference featuring Professor\n  Robert Engle, The 2003 Nobel Laureate in Economic Sciences",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Decentralized finance (DeFi) has the potential to disrupt centralized finance\nby validating peer-to-peer transactions through tamper-proof smart contracts,\nthus significantly lowering the transaction cost charged by financial\nintermediaries. However, the actual realization of peer-to-peer transactions\nand the levels and effects of decentralization are largely unknown. Our\nresearch pioneers a blockchain network study that applies social network\nanalysis to measure the level, dynamics, and impacts of decentralization in\nDeFi token transactions on the Ethereum blockchain. First, we find a\nsignificant core-periphery structure in the AAVE token transaction network\nwhere the cores include the two largest centralized crypto exchanges. Second,\nwe provide evidence that multiple network features consistently characterize\ndecentralization dynamics. Finally, we document that a more decentralized\nnetwork significantly predicts a higher return and lower volatility of the\ndecentralized market of AAVE tokens on the Ethereum blockchain. We point out\nthat our approach is seminal for inspiring future extensions related to the\nfacets of application scenarios, research questions, and methodologies on the\nmechanics of blockchain decentralization.\n"
    },
    {
        "paper_id": 2206.08541,
        "authors": "Benjamin Avanzi and Yanfeng Li and Bernard Wong and Alan Xian",
        "title": "Ensemble distributional forecasting for insurance loss reserving",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Loss reserving generally focuses on identifying a single model that can\ngenerate superior predictive performance. However, different loss reserving\nmodels specialise in capturing different aspects of loss data. This is\nrecognised in practice in the sense that results from different models are\noften considered, and sometimes combined. For instance, actuaries may take a\nweighted average of the prediction outcomes from various loss reserving models,\noften based on subjective assessments.\n  In this paper, we propose a systematic framework to objectively combine (i.e.\nensemble) multiple _stochastic_ loss reserving models such that the strengths\noffered by different models can be utilised effectively. Our framework contains\ntwo main innovations compared to existing literature and practice. Firstly, our\ncriteria model combination considers the full distributional properties of the\nensemble and not just the central estimate - which is of particular importance\nin the reserving context. Secondly, our framework is that it is tailored for\nthe features inherent to reserving data. These include, for instance, accident,\ndevelopment, calendar, and claim maturity effects. Crucially, the relative\nimportance and scarcity of data across accident periods renders the problem\ndistinct from the traditional ensembling techniques in statistical learning.\n  Our framework is illustrated with a complex synthetic dataset. In the\nresults, the optimised ensemble outperforms both (i) traditional model\nselection strategies, and (ii) an equally weighted ensemble. In particular, the\nimprovement occurs not only with central estimates but also relevant quantiles,\nsuch as the 75th percentile of reserves (typically of interest to both insurers\nand regulators). The framework developed in this paper can be implemented\nthanks to an R package, `ADLP`, which is available from CRAN.\n"
    },
    {
        "paper_id": 2206.08745,
        "authors": "Alessandra Cornaro and Giorgio Rizzini",
        "title": "Environmentally extended input-output analysis in complex networks: a\n  multilayer approach",
        "comments": null,
        "journal-ref": null,
        "doi": "10.1007/s10479-022-05133-0",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper we propose a methodology suitable for a comprehensive analysis\nof the global embodied energy flow trough a complex network approach. To this\nend, we extend the existing literature, providing a multilayer framework based\non the environmentally extended input-output analysis. The multilayer\nstructure, with respect to the traditional approach, allows us to unveil the\ndifferent role of sectors and economies in the system. In order to identify key\nsectors and economies, we make use of hub and authority scores, by adapting to\nour framework an extension of the Kleinberg algorithm, called Multi-Dimensional\nHITS (MD-HITS). A numerical analysis based on multi-region input-output tables\nshows how the proposed approach provides meaningful insights.\n"
    },
    {
        "paper_id": 2206.08753,
        "authors": "Andrei N. Soklakov",
        "title": "Information Geometry of Risks and Returns",
        "comments": "25 pages, 2 figures",
        "journal-ref": "Risk, June (2023)",
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  We reveal a geometric structure underlying both hedging and investment\nproducts. The structure follows from a simple formula expressing investment\nrisks in terms of returns. This informs optimal product designs. Optimal pure\nhedging (including cost-optimal products) and hybrid hedging (where a partial\nhedge is built into an optimal investment product) are considered. Duality\nbetween hedging and investment is demonstrated with applications to optimal\nrisk recycling. A geometric interpretation of rationality is presented.\n"
    },
    {
        "paper_id": 2206.08754,
        "authors": "Atif Ansar and Bent Flyvbjerg",
        "title": "How to Solve Big Problems: Bespoke Versus Platform Strategies",
        "comments": null,
        "journal-ref": "Oxford Review of Economic Policy, vol. 38, no. 2, 2022, pp. 338-36",
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  How should government and business solve big problems? In bold leaps or in\nmany smaller moves? We show that bespoke, one-off projects are prone to poorer\noutcomes than projects built on a repeatable platform. Repeatable projects are\ncheaper, faster, and scale at lower risk of failure. We compare evidence from\n203 space missions at NASA and SpaceX, on cost, speed-to-market, schedule, and\nscalability. We find that SpaceX's platform strategy was 10X cheaper and 2X\nfaster than NASA's bespoke strategy. Moreover, SpaceX's platform strategy was\nfinancially less risky, virtually eliminating cost overruns. Finally, we show\nthat achieving platform repeatability is a strategically diligent process\ninvolving experimental learning sequences. Sectors of the economy where\ngovernments find it difficult to control spending or timeframes or to realize\nplanned benefits - e.g., health, education, climate, defence - are ripe for a\nplatform rethink.\n"
    },
    {
        "paper_id": 2206.08781,
        "authors": "Callum Rhys Tilbury",
        "title": "Reinforcement Learning for Economic Policy: A New Frontier?",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Agent-based computational economics is a field with a rich academic history,\nyet one which has struggled to enter mainstream policy design toolboxes,\nplagued by the challenges associated with representing a complex and dynamic\nreality. The field of Reinforcement Learning (RL), too, has a rich history, and\nhas recently been at the centre of several exponential developments. Modern RL\nimplementations have been able to achieve unprecedented levels of\nsophistication, handling previously unthinkable degrees of complexity. This\nreview surveys the historical barriers of classical agent-based techniques in\neconomic modelling, and contemplates whether recent developments in RL can\novercome any of them.\n"
    },
    {
        "paper_id": 2206.08922,
        "authors": "Chongrui Zhu",
        "title": "On the closed-form expected NPVs of double barrier strategies for\n  regular diffusions",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The core of the research is to provide the explicit expression for the\nexpected net present values (NPVs) of double barrier strategies for regular\ndiffusions on the real line without solving differential equations. Under the\nso-called bail-out setting, the value of the expected NPVs of an insurance\ncompany varies according to the choice of a pair of policies, which consist of\ndividend payments paid out and capital injections received. In the case of the\ndouble barrier strategy, the expected NPVs are expressible with the help of\ncertain types of functions allowing explicit expression in some cases, which is\ncalled the bivariate $q$-scale function in the article. This is accomplished by\nmaking use of a perturbation technique in \\cite{czarna2014dividend}, which\ncould lead to the linear equation system. In addition, a condition ensuring the\nexistence of an optimal (upper) barrier level is presented. In the end,\nexamples fitting the condition for selecting the optimal barrier are given.\n"
    },
    {
        "paper_id": 2206.08938,
        "authors": "Alessandro Danovi, Marzio Roma, Davide Meloni, Stefano Olgiati,\n  Fernando Metelli",
        "title": "Baseline validation of a bias-mitigated loan screening model based on\n  the European Banking Authority's trust elements of Big Data & Advanced\n  Analytics applications using Artificial Intelligence",
        "comments": "13 pages, 4 tables, 7 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/publicdomain/zero/1.0/",
        "abstract": "  The goal of our 4-phase research project was to test if a\nmachine-learning-based loan screening application (5D) could detect bad loans\nsubject to the following constraints: a) utilize a minimal-optimal number of\nfeatures unrelated to the credit history, gender, race or ethnicity of the\nborrower (BiMOPT features); b) comply with the European Banking Authority and\nEU Commission principles on trustworthy Artificial Intelligence (AI). All\ndatasets have been anonymized and pseudoanonymized. In Phase 0 we selected a\nsubset of 10 BiMOPT features out of a total of 84 features; in Phase I we\ntrained 5D to detect bad loans in a historical dataset extracted from a\nmandatory report to the Bank of Italy consisting of 7,289 non-performing loans\n(NPLs) closed in the period 2010-2021; in Phase II we assessed the baseline\nperformance of 5D on a distinct validation dataset consisting of an active\nportolio of 63,763 outstanding loans (performing and non-performing) for a\ntotal financed value of over EUR 11.5 billion as of December 31, 2021; in Phase\nIII we will monitor the baseline performance for a period of 5 years (2023-27)\nto assess the prospective real-world bias-mitigation and performance of the 5D\nsystem and its utility in credit and fintech institutions. At baseline, 5D\ncorrectly detected 1,461 bad loans out of a total of 1,613 (Sensitivity = 0.91,\nPrevalence = 0.0253;, Positive Predictive Value = 0.19), and correctly\nclassified 55,866 out of the other 62,150 exposures (Specificity = 0.90,\nNegative Predictive Value = 0.997). Our preliminary results support the\nhypothesis that Big Data & Advanced Analytics applications based on AI can\nmitigate bias and improve consumer protection in the loan screening process\nwithout compromising the efficacy of the credit risk assessment. Further\nvalidation is required to assess the prospective performance and utility of 5D\nin credit and fintech institutions.\n"
    },
    {
        "paper_id": 2206.09037,
        "authors": "Amirhesam Badeanlou and Andrea Araldo and Marco Diana",
        "title": "Assessing transportation accessibility equity via open data",
        "comments": null,
        "journal-ref": "hEART Conference 2022",
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We propose a methodology to assess transportation accessibility inequity in\nmetropolitan areas. The methodology is based on the classic analysis tools of\nLorenz curves and Gini indices, but the novelty resides in the fact that it can\nbe easily applied in an automated way to several cities around the World, with\nno need for customized data treatment. Indeed, our equity metrics can be\ncomputed solely relying on open data, publicly available in standardized form.\nWe showcase our method and study transp\n"
    },
    {
        "paper_id": 2206.09041,
        "authors": "Mark Joseph Bennett",
        "title": "Accelerating Machine Learning Training Time for Limit Order Book\n  Prediction",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Financial firms are interested in simulation to discover whether a given\nalgorithm involving financial machine learning will operate profitably. While\nmany versions of this type of algorithm have been published recently by\nresearchers, the focus herein is on a particular machine learning training\nproject due to the explainable nature and the availability of high frequency\nmarket data. For this task, hardware acceleration is expected to speed up the\ntime required for the financial machine learning researcher to obtain the\nresults. As the majority of the time can be spent in classifier training, there\nis interest in faster training steps. A published Limit Order Book algorithm\nfor predicting stock market direction is our subject, and the machine learning\ntraining process can be time-intensive especially when considering the\niterative nature of model development. To remedy this, we deploy Graphical\nProcessing Units (GPUs) produced by NVIDIA available in the data center where\nthe computer architecture is geared to parallel high-speed arithmetic\noperations. In the studied configuration, this leads to significantly faster\ntraining time allowing more efficient and extensive model development.\n"
    },
    {
        "paper_id": 2206.09218,
        "authors": "Joshua Aizenman, Yothin Jinjarak, Donghyun Park, and Huanhuan Zheng",
        "title": "Good-Bye Original Sin, Hello Risk On-Off, Financial Fragility, and\n  Crises?",
        "comments": null,
        "journal-ref": "Journal of International Money and Finance, 117, 102442 (2021)",
        "doi": "10.1016/j.jimonfin.2021.102442",
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  We analyze the sovereign bond issuance data of eight major emerging markets\n(EMs) - Brazil, China, India, Indonesia, Mexico, Russia, South Africa and\nTurkey from 1970 to 2018. Our analysis suggests that (i) EM local currency\nbonds tend to be smaller in size, shorter in maturity, or lower in coupon rate\nthan foreign currency bonds; (ii) EMs are more likely to issue local-currency\nsovereign bonds if their currencies appreciated before the global financial\ncrisis of 2008 (GFC); (iii) inflation-targeting monetary policy increases the\nlikelihood of issuing local-currency debt before GFC but not after; and (iv)\nEMs that offer higher sovereign yields are more likely to issue local-currency\nbonds after GFC. Future data will allow us to test and identify structural\nchanges associated with the COVID-19 pandemic and its aftermath.\n"
    },
    {
        "paper_id": 2206.0922,
        "authors": "Enrico Dall'Acqua, Riccardo Longoni, Andrea Pallavicini",
        "title": "Rough-Heston Local-Volatility Model",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In industrial applications it is quite common to use stochastic volatility\nmodels driven by semi-martingale Markov volatility processes. However, in order\nto fit exactly market volatilities, these models are usually extended by adding\na local volatility term. Here, we consider the case of singular Volterra\nprocesses, and we extend them by adding a local-volatility term to their Markov\nlift by preserving the stylized results implied by these models on\nplain-vanilla options. In particular, we focus on the rough-Heston model, and\nwe analyze the small time asymptotics of its implied local-volatility function\nin order to provide a proper extrapolation scheme to be used in calibration.\n"
    },
    {
        "paper_id": 2206.09235,
        "authors": "Tomasz R. Bielecki, Igor Cialenco, Andrzej Ruszczy\\'nski",
        "title": "Risk Filtering and Risk-Averse Control of Markovian Systems Subject to\n  Model Uncertainty",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  We consider a Markov decision process subject to model uncertainty in a\nBayesian framework, where we assume that the state process is observed but its\nlaw is unknown to the observer. In addition, while the state process and the\ncontrols are observed at time $t$, the actual cost that may depend on the\nunknown parameter is not known at time $t$. The controller optimizes total cost\nby using a family of special risk measures, that we call risk filters and that\nare appropriately defined to take into account the model uncertainty of the\ncontrolled system. These key features lead to non-standard and non-trivial\nrisk-averse control problems, for which we derive the Bellman principle of\noptimality. We illustrate the general theory on two practical examples: optimal\ninvestment and clinical trials.\n"
    },
    {
        "paper_id": 2206.09279,
        "authors": "Natalia Porto, Pablo de la Vega, and Manuela Cerimelo",
        "title": "Going Green: Estimating the Potential of Green Jobs in Argentina",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  This paper aims to identify and characterize the potential of green jobs in\nArgentina, i.e., those that would benefit from a transition to a green economy,\nusing occupational green potential scores calculated in US O*NET data. We apply\nthe greenness scores to Argentine household survey data and estimate that 25%\nof workers are in green jobs, i.e., have a high green potential. However, when\ntaking into account the informality dimension, we find that 15% of workers and\n12% of wage earners are in formal green jobs. We then analyze the relationship\nbetween the greenness scores (with emphasis on the nexus with decent work) and\nvarious labor and demographic variables at the individual level. We find that\nfor the full sample of workers the green potential is relatively greater for\nmen, the elderly, those with very high qualifications, those in formal\npositions, and those in specific sectors such as construction, transportation,\nmining, and industry. These are the groups that are likely to be the most\nbenefited by the greening of the Argentine economy. When we restrict the sample\nto wage earners, the green potential score is positively associated with\ninformality.\n"
    },
    {
        "paper_id": 2206.09657,
        "authors": "Battulga Gankhuu",
        "title": "Parameter Estimation Methods of Required Rate of Return on Stock",
        "comments": "24",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  In this study, we introduce new estimation methods for the required rate of\nreturn of the stochastic dividend discount model (DDM) and the private company\nvaluation model, which will appear below. To estimate the required rate of\nreturn, we use the maximum likelihood method, the Bayesian method, and the\nKalman filtering. We apply the model to a set of firms from the S\\&P 500 index\nusing historical dividend and price data over a 32--year period. Overall,\nsuggested methods can be used to estimate the required rate of return.\n"
    },
    {
        "paper_id": 2206.09666,
        "authors": "Battulga Gankhuu",
        "title": "The Log Private Company Valuation Model",
        "comments": "31 pages. arXiv admin note: substantial text overlap with\n  arXiv:2201.06012",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  For a public company, pricing and hedging models of options and\nequity--linked life insurance products have been sufficiently developed.\nHowever, for a private company, because of unobserved prices, pricing and\nhedging models of the European options and life insurance products are in their\nearly stages of development. For this reason, this paper introduces a log\nprivate company valuation model, which is based on the dynamic Gordon growth\nmodel. In this paper, we obtain closed--form pricing and hedging formulas of\nthe European options and equity--linked life insurance products for private\ncompanies. Also, the paper provides Maximum Likelihood (ML) estimators of our\nmodel, Expectation Maximization (EM) algorithm, and valuation formula for\nprivate companies.\n"
    },
    {
        "paper_id": 2206.09772,
        "authors": "Luis Goncalves de Faria",
        "title": "An Agent-Based Model With Realistic Financial Time Series: A Method for\n  Agent-Based Models Validation",
        "comments": "79 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  This paper proposes a methodology to empirically validate an agent-based\nmodel (ABM) that generates artificial financial time series data comparable\nwith real-world financial data. The approach is based on comparing the results\nof the ABM against the stylised facts -- the statistical properties of the\nempirical time-series of financial data. The stylised facts appear to be\nuniversal and are observed across different markets, financial instruments and\ntime periods, hence they can serve to validate models of financial markets. If\na given model does not consistently replicate these stylised facts, then we can\nreject it as being empirically inadequate. We discuss each stylised fact, the\nempirical evidence for it, and introduce appropriate metrics for testing the\npresence of these in model generated data. Moreover we investigate the ability\nof our model to correctly reproduce these stylised facts. We validate our model\nagainst a comprehensive list of empirical phenomena that qualify as a stylised\nfact, of both low and high frequency financial data that can be addressed by\nmeans of a relatively simple ABM of financial markets. This procedure is able\nto show whether the model, as an abstraction of reality, has a meaningful\nempirical counterpart and the significance of this analysis for the purposes of\nABM validation and their empirical reliability.\n"
    },
    {
        "paper_id": 2206.09877,
        "authors": "Lech A. Grzelak, Juliusz Jablecki, Dariusz Gatarek",
        "title": "Efficient Pricing and Calibration of High-Dimensional Basket Options",
        "comments": "23 pages, 21 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  This paper studies equity basket options -- i.e., multi-dimensional\nderivatives whose payoffs depend on the value of a weighted sum of the\nunderlying stocks -- and develops a new and innovative approach to ensure\nconsistency between options on individual stocks and on the index comprising\nthem. Specifically, we show how to resolve a well-known problem that when\nindividual constituent distributions of an equity index are inferred from the\nsingle-stock option markets and combined in a multi-dimensional\nlocal/stochastic volatility model, the resulting basket option prices will not\ngenerate a skew matching that of the options on the equity index corresponding\nto the basket. To address this ``insufficient skewness'', we proceed in two\nsteps. First, we propose an ``effective'' local volatility model by mapping the\ngeneral multi-dimensional basket onto a collection of marginal distributions.\nSecond, we build a multivariate dependence structure between all the marginal\ndistributions assuming a jump-diffusion model for the effective projection\nparameters, and show how to calibrate the basket to the index smile. Numerical\ntests and calibration exercises demonstrate an excellent fit for a basket of as\nmany as 30 stocks with fast calculation time.\n"
    },
    {
        "paper_id": 2206.09899,
        "authors": "Federico Mecchia and Marcellino Gaudenzi",
        "title": "The dynamics of the prices of the companies of the STOXX Europe 600\n  Index through the logit model and neural network",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  The aim of the present work is analysing and understanding the dynamics of\nthe prices of companies, depending on whether they are included or excluded\nfrom the STOXX Europe 600 Index. For this reason, data regarding the companies\nof the Index in question was collected and analysed also through the use of\nlogit models and neural networks in order to find the independent variables\nthat affect the changes in prices and thus determine the dynamics over time.\n"
    },
    {
        "paper_id": 2206.10014,
        "authors": "Matthew F. Dixon, Nicholas G. Polson and Kemen Goicoechea",
        "title": "Deep Partial Least Squares for Empirical Asset Pricing",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  We use deep partial least squares (DPLS) to estimate an asset pricing model\nfor individual stock returns that exploits conditioning information in a\nflexible and dynamic way while attributing excess returns to a small set of\nstatistical risk factors. The novel contribution is to resolve the non-linear\nfactor structure, thus advancing the current paradigm of deep learning in\nempirical asset pricing which uses linear stochastic discount factors under an\nassumption of Gaussian asset returns and factors. This non-linear factor\nstructure is extracted by using projected least squares to jointly project firm\ncharacteristics and asset returns on to a subspace of latent factors and using\ndeep learning to learn the non-linear map from the factor loadings to the asset\nreturns. The result of capturing this non-linear risk factor structure is to\ncharacterize anomalies in asset returns by both linear risk factor exposure and\ninteraction effects. Thus the well known ability of deep learning to capture\noutliers, shed lights on the role of convexity and higher order terms in the\nlatent factor structure on the factor risk premia. On the empirical side, we\nimplement our DPLS factor models and exhibit superior performance to LASSO and\nplain vanilla deep learning models. Furthermore, our network training times are\nsignificantly reduced due to the more parsimonious architecture of DPLS.\nSpecifically, using 3290 assets in the Russell 1000 index over a period of\nDecember 1989 to January 2018, we assess our DPLS factor model and generate\ninformation ratios that are approximately 1.2x greater than deep learning. DPLS\nexplains variation and pricing errors and identifies the most prominent latent\nfactors and firm characteristics.\n"
    },
    {
        "paper_id": 2206.10105,
        "authors": "Wenpin Tang and David D. Yao",
        "title": "Polynomial Voting Rules",
        "comments": "24 pages, 1 figure",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We propose and study a new class of polynomial voting rules for a general\ndecentralized decision/consensus system, and more specifically for the PoS\n(Proof of Stake) protocol. The main idea, inspired by the Penrose square-root\nlaw and the more recent quadratic voting rule, is to differentiate a voter's\nvoting power and the voter's share (fraction of the total in the system). We\nshow that while voter shares form a martingale process that converge to a\nDirichlet distribution, their voting powers follow a super-martingale process\nthat decays to zero over time. This prevents any voter from controlling the\nvoting process, and thus enhances security. For both limiting results, we also\nprovide explicit rates of convergence. When the initial total volume of votes\n(or stakes) is large, we show a phase transition in share stability (or the\nlack thereof), corresponding to the voter's initial share relative to the\ntotal. We also study the scenario in which trading (of votes/stakes) among the\nvoters is allowed, and quantify the level of risk sensitivity (or risk averse)\nin three categories, corresponding to the voter's utility being a\nsuper-martingale, a sub-martingale, and a martingale. For each category, we\nidentify the voter's best strategy in terms of participation and trading.\n"
    },
    {
        "paper_id": 2206.10173,
        "authors": "Christian Bongiorno, Damien Challet",
        "title": "Statistical inference of lead-lag at various timescales between\n  asynchronous time series from p-values of transfer entropy",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Symbolic transfer entropy is a powerful non-parametric tool to detect\nlead-lag between time series. Because a closed expression of the distribution\nof Transfer Entropy is not known for finite-size samples, statistical testing\nis often performed with bootstraps whose slowness prevents the inference of\nlarge lead-lag networks between long time series. On the other hand, the\nasymptotic distribution of Transfer Entropy between two time series is known.\nIn this work, we derive the asymptotic distribution of the test for one time\nseries having a larger Transfer Entropy than another one on a target time\nseries. We then measure the convergence speed of both tests in the small sample\nsize limits via benchmarks. We then introduce Transfer Entropy between\ntime-shifted time series, which allows to measure the timescale at which\ninformation transfer is maximal and vanishes. We finally apply these methods to\ntick-by-tick price changes of several hundreds of stocks, yielding non-trivial\nstatistically validated networks.\n"
    },
    {
        "paper_id": 2206.10214,
        "authors": "Susan Athey, Kristen Grabarz, Michael Luca, Nils Wernerfelt",
        "title": "The Effectiveness of Digital Interventions on COVID-19 Attitudes and\n  Beliefs",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  During the course of the COVID-19 pandemic, a common strategy for public\nhealth organizations around the world has been to launch interventions via\nadvertising campaigns on social media. Despite this ubiquity, little has been\nknown about their average effectiveness. We conduct a large-scale program\nevaluation of campaigns from 174 public health organizations on Facebook and\nInstagram that collectively reached 2.1 billion individuals and cost around\n\\$40 million. We report the results of 819 randomized experiments that measured\nthe impact of these campaigns across standardized, survey-based outcomes. We\nfind on average these campaigns are effective at influencing self-reported\nbeliefs, shifting opinions close to 1% at baseline with a cost per influenced\nperson of about \\$3.41. There is further evidence that campaigns are especially\neffective at influencing users' knowledge of how to get vaccines. Our results\nrepresent, to the best of our knowledge, the largest set of online public\nhealth interventions analyzed to date.\n"
    },
    {
        "paper_id": 2206.1033,
        "authors": "Alessandro Avellone and Stefano Benati and Rosanna Grassi and Giorgio\n  Rizzini",
        "title": "On Finding the Community with Maximum Persistence Probability",
        "comments": null,
        "journal-ref": "4OR-Q J Oper Res (2023)",
        "doi": "10.1007/s10288-023-00559-z",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The persistence probability is a statistical index that has been proposed to\ndetect one or more communities embedded in a network. Even though its\ndefinition is straightforward, e.g, the probability that a random walker\nremains in a group of nodes, it has been seldom applied possibly for the\ndifficulty of developing an efficient algorithm to calculate it. Here, we\npropose a new mathematical programming model to find the community with the\nlargest persistence probability. The model is integer fractional programming,\nbut it can be reduced to mixed-integer linear programming with an appropriate\nvariable substitution. Nevertheless, the problem can be solved in a reasonable\ntime for networks of small size only, therefore we developed some heuristic\nprocedures to approximate the optimal solution. First, we elaborated a\nrandomized greedy-ascent method, taking advantage of a peculiar data structure\nto generate feasible solutions fast. After analyzing the greedy output and\ndetermining where the optimal solution is eventually located, we implemented\nimproving procedures based on a local exchange, but applying different long\nterm diversification principles, that are based on variable neighborhood search\nand random restart. Next, we applied the algorithms on simulated graphs that\nreproduce accurately the clustering characteristics found in real networks to\ndetermine the reliability and the effectiveness of our methodology. Finally, we\napplied our method to two real networks, comparing our findings to what found\nby two well-known alternative community detection procedures.\n"
    },
    {
        "paper_id": 2206.10419,
        "authors": "C\\'ecilia Aubrun, Michael Benzaquen, Jean-Philippe Bouchaud",
        "title": "Multivariate Quadratic Hawkes Processes -- Part I: Theoretical Analysis",
        "comments": "23 pages, 5 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Quadratic Hawkes (QHawkes) processes have proved effective at reproducing the\nstatistics of price changes, capturing many of the stylised facts of financial\nmarkets. Motivated by the recently reported strong occurrence of endogenous\nco-jumps (simultaneous price jumps of several assets) we extend QHawkes to a\nmultivariate framework (MQHawkes), that is considering several financial assets\nand their interactions. Assuming that quadratic kernels write as the sum of a\ntime-diagonal component and a rank one (trend) contribution, we investigate\nendogeneity ratios and the resulting stationarity conditions. We then derive\nthe so-called Yule-Walker equations relating covariances and feedback kernels,\nwhich are essential to calibrate the MQHawkes process on empirical data.\nFinally, we investigate the volatility distribution of the process and find\nthat, as in the univariate case, it exhibits power-law behavior, with an\nexponent that can be exactly computed in some limiting cases.\n"
    },
    {
        "paper_id": 2206.10489,
        "authors": "Michail Anthropelos and Paul Schneider",
        "title": "Optimal Investment and Equilibrium Pricing under Ambiguity",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We consider portfolio selection under nonparametric $\\alpha$-maxmin ambiguity\nin the neighbourhood of a reference distribution. We show strict concavity of\nthe portfolio problem under ambiguity aversion. Implied demand functions are\nnondifferentiable, resemble observed bid-ask spreads, and are consistent with\nexisting parametric limiting participation results under ambiguity. Ambiguity\nseekers exhibit a discontinuous demand function, implying an empty set of\nreservation prices. If agents have identical, or sufficiently similar prior\nbeliefs, the first-best equilibrium is no trade. Simple conditions yield the\nexistence of a Pareto-efficient second-best equilibrium, implying that\nheterogeneity in ambiguity preferences is sufficient for mutually beneficial\ntransactions among all else homogeneous traders. These equilibria reconcile\nmany observed phenomena in liquid high-information financial markets, such as\nliquidity dry-ups, portfolio inertia, and negative risk premia.\n"
    },
    {
        "paper_id": 2206.1051,
        "authors": "Fabienne Cantner, Nico Nachtigall, Lisa S. Hamm, Andrea Cadavid,\n  Lennart Adenaw, Allister Loder, Markus B. Siewert, Sebastian Goerg, Markus\n  Lienkamp, Klaus Bogenberger",
        "title": "A nation-wide experiment: fuel tax cuts and almost free public transport\n  for three months in Germany -- Report 2 First wave results",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
        "abstract": "  In spring 2022, the German federal government agreed on a set of measures\nthat aim at reducing households' financial burden resulting from a recent price\nincrease, especially in energy and mobility. These measures include among\nothers, a nation-wide public transport ticket for 9 EUR per month and a fuel\ntax cut that reduces fuel prices by more than 15%. In transportation research\nthis is an almost unprecedented behavioral experiment. It allows to study not\nonly behavioral responses in mode choice and induced demand but also to assess\nthe effectiveness of transport policy instruments. We observe this natural\nexperiment with a three-wave survey and an app-based travel diary on a sample\nof hundreds of participants as well as an analysis of traffic counts. In this\nsecond report, we update the information on study participation, provide first\ninsights on the smartphone app usage as well as insights on the first wave\nresults, particularly on the 9 EUR-ticket purchase intention.\n"
    },
    {
        "paper_id": 2206.10601,
        "authors": "Xize Wang (National University of Singapore)",
        "title": "Has the Relationship between Urban and Suburban Automobile Travel\n  Changed across Generations? Comparing Millennials and Generation Xers in the\n  United States",
        "comments": null,
        "journal-ref": "Transportation Research Part A: Policy and Practice, 129, 107-122\n  (2019)",
        "doi": "10.1016/j.tra.2019.08.012",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Using U.S. nationwide travel surveys for 1995, 2001, 2009 and 2017, this\nstudy compares Millennials with their previous generation (Gen Xers) in terms\nof their automobile travel across different neighborhood patterns. At the age\nof 16 to 28 years old, Millennials have lower daily personal vehicle miles\ntraveled and car trips than Gen Xers in urban (higher-density) and suburban\n(lower-density) neighborhoods. Such differences remain unchanged after\nadjusting for the socio-economic, vehicle ownership, life cycle, year-specific\nand regional-specific factors. In addition, the associations between\nresidential density and automobile travel for the 16- to 28-year-old\nMillennials are flatter than that for Gen Xers, controlling for the\naforementioned covariates. These generational differences remain for the 24- to\n36-year-old Millennials, during the period when the U.S. economy was recovering\nfrom the recession. These findings show that, in both urban and suburban\nneighborhoods, Millennials in the U.S. are less auto-centric than the previous\ngeneration during early life stages, regardless of economic conditions. Whether\nsuch difference persists over later life stages remains an open question and is\nworth continuous attention.\n"
    },
    {
        "paper_id": 2206.10662,
        "authors": "Jherek Healy",
        "title": "Accurate and consistent calculation of the mean and variance in\n  Monte-Carlo simulations",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  In parallelized Monte-Carlo simulations, the order of summation is not always\nthe same. When the mean is calculated in running fashion, this may create an\nartificial randomness in results which ought to be reproducible. This note\ntakes a look at the problem and proposes to combine the running mean and\nvariance algorithm with an accurate and robust summing algorithm in order to\nincrease the accuracy and robustness of the Monte-Carlo estimates.\n"
    },
    {
        "paper_id": 2206.10736,
        "authors": "Jin Fang, Jiacheng Weng, Yi Xiang, Xinwen Zhang",
        "title": "Imitate then Transcend: Multi-Agent Optimal Execution with Dual-Window\n  Denoise PPO",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  A novel framework for solving the optimal execution and placement problems\nusing reinforcement learning (RL) with imitation was proposed. The RL agents\ntrained from the proposed framework consistently outperformed the industry\nbenchmark time-weighted average price (TWAP) strategy in execution cost and\nshowed great generalization across out-of-sample trading dates and tickers. The\nimpressive performance was achieved from three aspects. First, our RL network\narchitecture called Dual-window Denoise PPO enabled efficient learning in a\nnoisy market environment. Second, a reward scheme with imitation learning was\ndesigned, and a comprehensive set of market features was studied. Third, our\nflexible action formulation allowed the RL agent to tackle optimal execution\nand placement collectively resulting in better performance than solving\nindividual problems separately. The RL agent's performance was evaluated in our\nmulti-agent realistic historical limit order book simulator in which price\nimpact was accurately assessed. In addition, ablation studies were also\nperformed, confirming the superiority of our framework.\n"
    },
    {
        "paper_id": 2206.10877,
        "authors": "Paul Hofmarcher, Sourav Adhikari, Bettina Gr\\\"un",
        "title": "Gaining Insights on U.S. Senate Speeches Using a Time Varying Text Based\n  Ideal Point Model",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Estimating political positions of lawmakers has a long tradition in political\nscience. We present the time varying text based ideal point model to study the\npolitical positions of lawmakers based on text data. In addition to identifying\npolitical positions, our model also provides insights into topical contents and\ntheir change over time. We use our model to analyze speeches given in the U.S.\nSenate between 1981 and 2017 and demonstrate how the results allow to conclude\nthat partisanship between Republicans and Democrats increased in recent years.\nFurther we investigate the political positions of speakers over time as well as\nat a specific point in time to identify speakers which are positioned at the\nextremes of their political party based on their speeches. The topics extracted\nare inspected to assess how their term compositions differ in dependence of the\npolitical position as well as how these term compositions change over time.\n"
    },
    {
        "paper_id": 2206.11056,
        "authors": "Kailai Wang (University of Houston), Xize Wang (National University of\n  Singapore)",
        "title": "Generational Differences in Automobility: Comparing America's\n  Millennials and Gen Xers Using Gradient Boosting Decision Trees",
        "comments": null,
        "journal-ref": "Cities, 114, 103204 (2021)",
        "doi": "10.1016/j.cities.2021.103204",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Whether the Millennials are less auto-centric than the previous generations\nhas been widely discussed in the literature. Most existing studies use\nregression models and assume that all factors are linear-additive in\ncontributing to the young adults' driving behaviors. This study relaxes this\nassumption by applying a non-parametric statistical learning method, namely the\ngradient boosting decision trees (GBDT). Using U.S. nationwide travel surveys\nfor 2001 and 2017, this study examines the non-linear dose-response effects of\nlifecycle, socio-demographic and residential factors on daily driving distances\nof Millennial and Gen-X young adults. Holding all other factors constant,\nMillennial young adults had shorter predicted daily driving distances than\ntheir Gen-X counterparts. Besides, residential and economic factors explain\naround 50% of young adults' daily driving distances, while the collective\ncontributions for life course events and demographics are about 33%. This study\nalso identifies the density ranges for formulating effective land use policies\naiming at reducing automobile travel demand.\n"
    },
    {
        "paper_id": 2206.11072,
        "authors": "Jimei Shen, Zhehu Yuan, Yifan Jin",
        "title": "AlphaMLDigger: A Novel Machine Learning Solution to Explore Excess\n  Return on Investment",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  How to quickly and automatically mine effective information and serve\ninvestment decisions has attracted more and more attention from academia and\nindustry. And new challenges have arisen with the global pandemic. This paper\nproposes a two-phase AlphaMLDigger that effectively finds excessive returns in\na highly fluctuated market. In phase 1, a deep sequential natural language\nprocessing (NLP) model is proposed to transfer Sina Microblog blogs to market\nsentiment. In phase 2, the predicted market sentiment is combined with social\nnetwork indicator features and stock market history features to predict the\nstock movements with different Machine Learning models and optimizers. The\nresults show that the ensemble models achieve an accuracy of 0.984 and\nsignificantly outperform the baseline model. In addition, we find that COVID-19\nbrings data shift to China's stock market.\n"
    },
    {
        "paper_id": 2206.11105,
        "authors": "Alex Garivaltis",
        "title": "Recursive Overbetting of a Satellite Investment Account",
        "comments": "This paper has been withdrawn by the author",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper builds a core-satellite model of semi-static Kelly betting and\nlog-optimal investment. We study the problem of a saver whose core portfolio\nconsists in unlevered (1x) retirement plans with no access to margin debt.\nHowever, the agent has a satellite investment account with recourse to\nsignificant, but not unlimited, leverage; accordingly, we study optimal\ncontrollers for the satellite gearing ratio. On a very short time horizon, the\nbest policy is to overbet the satellite, whereby the overriding objective is to\nraise the aggregate beta toward a growth-optimal level. On an infinite horizon,\nby contrast, the correct behavior is to blithely ignore the core and optimize\nthe exponential growth rate of the satellite, which will anyways come to\ndominate the entire bankroll in the limit. For time horizons strictly between\nzero and infinity, the optimal strategy is not so simple: there is a key\ntrade-off between the instantaneous growth rate of the composite bankroll, and\nthat of the satellite itself, which suffers ongoing volatility drag from the\noverbetting. Thus, a very perspicacious policy is called for, since any losses\nin the satellite will constrain the agent's access to leverage in the\ncontinuation problem. We characterize the optimal feedback controller, and\ncompute it in earnest by solving the corresponding HJB equation recursively and\nbackward in time. This solution is then compared to the best open-loop\ncontroller, which, in spite of its relative simplicity, is expected to perform\nsimilarly in practical situations.\n"
    },
    {
        "paper_id": 2206.11344,
        "authors": "J. du Pisanie, J.S. Allison, I.J.H. Visagie",
        "title": "A proposed simulation technique for population stability testing in\n  credit risk scorecards",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Credit risk scorecards are logistic regression models, fitted to large and\ncomplex data sets, employed by the financial industry to model the probability\nof default of a potential customer. In order to ensure that a scorecard remains\na representative model of the population one tests the hypothesis of population\nstability; specifying that the distribution of clients' attributes remains\nconstant over time. Simulating realistic data sets for this purpose is\nnontrivial as these data sets are multivariate and contain intricate\ndependencies. The simulation of these data sets are of practical interest for\nboth practitioners and for researchers; practitioners may wish to consider the\neffect that a specified change in the properties of the data has on the\nscorecard and its usefulness from a business perspective, while researchers may\nwish to test a newly developed technique in credit scoring.\n  We propose a simulation technique based on the specification of bad ratios,\nthis is explained below. Practitioners can generally not be expected to provide\nrealistic parameter values for a scorecard; these models are simply too complex\nand contain too many parameters to make such a specification viable. However,\npractitioners can often confidently specify the bad ratio associated with two\ndifferent levels of a specific attribute. That is, practitioners are often\ncomfortable with making statements such as \"on average a new customer is 1.5\ntimes as likely to default as an existing customer with similar attributes\". We\npropose a method which can be used to obtain parameter values for a scorecard\nbased on specified bad ratios. The proposed technique is demonstrated using a\nrealistic example and we show that the simulated data sets adhere closely to\nthe specified bad ratios. The paper provides a link to a github project in\nwhich the R code used in order to generate the results shown can be found.\n"
    },
    {
        "paper_id": 2206.114,
        "authors": "Emily Aiken, Guadalupe Bedoya, Joshua Blumenstock, Aidan Coville",
        "title": "Program Targeting with Machine Learning and Mobile Phone Data: Evidence\n  from an Anti-Poverty Intervention in Afghanistan",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Can mobile phone data improve program targeting? By combining rich survey\ndata from a \"big push\" anti-poverty program in Afghanistan with detailed mobile\nphone logs from program beneficiaries, we study the extent to which machine\nlearning methods can accurately differentiate ultra-poor households eligible\nfor program benefits from ineligible households. We show that machine learning\nmethods leveraging mobile phone data can identify ultra-poor households nearly\nas accurately as survey-based measures of consumption and wealth; and that\ncombining survey-based measures with mobile phone data produces classifications\nmore accurate than those based on a single data source.\n"
    },
    {
        "paper_id": 2206.11811,
        "authors": "Hamed Kazemipoor, Mohammad Ebrahim Sadeghi, Agnieszka Szmelter-Jarosz,\n  Mohadese Aghabozorgi",
        "title": "Providing a model for the issue of multi-period ambulance location",
        "comments": "International Journal of Innovation in Engineering (IJIE), 2021",
        "journal-ref": null,
        "doi": "10.52547/ijie.1.2.13",
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  In this study, two mathematical models have been developed for assigning\nemergency vehicles, namely ambulances, to geographical areas. The first model,\nwhich is based on the assignment problem, the ambulance transfer (moving\nambulances) between locations has not been considered. As ambulance transfer\ncan improve system efficiency by decreasing the response time as well as\noperational cost, we consider this in the second model, which is based on the\ntransportation problem. Both models assume that the demand of all geographical\nlocations must be met. The major contributions of this study are: ambulance\ntransfer between locations, day split into several time slots, and demand\ndistribution of the geographical zone. To the best of our knowledge the first\ntwo have not been studied before. These extensions allow us to have a more\nrealistic model of the real-world operation. Although, in previous studies,\nmaximizing coverage has been the main objective of the goal, here, minimizing\noperating costs is a function of the main objective, because we have assumed\nthat the demand of all geographical areas must be met.\n"
    },
    {
        "paper_id": 2206.11847,
        "authors": "Mohammad Mousakhani, Fatemeh Saghafi, Mohammad Hasanzadeh, Mohammad\n  Ebrahim Sadeghi",
        "title": "Proposing Dynamic Model of Functional Interactions of IoT Technological\n  Innovation System by Using System Dynamics and Fuzzy DEMATEL",
        "comments": "in Farsi. Journal of Operational Research and Its Applications, 2020",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  One of the emerging technologies, which is expected to have tremendous\neffects on community development, is the Internet of Things technology. Given\nthe prospects for this technology and the country's efforts to its development,\npolicymaking for this technology is very crucial. The technological innovation\nsystem is one of the most important dynamic approaches in the field of modern\ntechnology policy. In this approach, by analyzing various functions which are\ninfluencing the development of a technology, the proper path to technological\nadvancement is explained. For this reason, 10 major factors influencing the\ndevelopment of emerging technologies have been identified based on previous\nstudies in this area and the effect of ten factors on each other was identified\nby using the system dynamics and the fuzzy DEMATEL method and the interactions\nbetween these functions were modeled. Market formation, resource mobilization,\nexploitation of the regime and policymaking and coordination functions have the\nmost direct effect on the other functions. Also, policymaking and coordination,\nmarket formation, entrepreneurial activities, creating structure and resource\nmobilization have the most total impact on the other functions. Regard to\nresource constraint in the system, the policy makers should focus on those\nfactors which have the most direct and total impact on the others that in this\nresearch are market formation, entrepreneurial activities, resource\nmobilization and policymaking and coordination. Given the dynamic nature of\ntechnology development, this model can help policymakers in the decision making\nprocess for the development of the Internet of Things.\n"
    },
    {
        "paper_id": 2206.1185,
        "authors": "Rasoul Jamshidi, Mohammad Ebrahim Sadeghi",
        "title": "Neural network based human reliability analysis method in production\n  systems",
        "comments": "Journal of Applied Research on Industrial Engineering, 2021",
        "journal-ref": null,
        "doi": "10.22105/JARIE.2021.277071.1274",
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Purpose: In addition to playing an important role in creating economic\nsecurity and investment development, insurance companies also invest. The\ncountry's insurance industry as one of the country's financial institutions has\na special place in the investment process and special attention to appropriate\ninvestment policies in the field of insurance industry is essential. So that\nthe efficiency of this industry in allocating the existing budget stimulates\nother economic sectors. This study seeks to model investment in the performance\nof dynamic networks of insurance companies.\n  Methodology: In this paper, a new investment model is designed to examine the\ndynamic network performance of insurance companies in Iran. The designed model\nis implemented using GAMS software and the outputs of the model are analyzed\nbased on regression method. The required information has been collected based\non the statistics of insurance companies in Iran between 1393 and 1398.\n  Findings: After evaluating these units, out of 15 companies evaluated, 6\ncompanies had unit performance and were introduced as efficient companies. The\naverage efficiency of insurance companies is 0.78 and the standard deviation is\n0.2. The results show that the increase in the value of investments is due to\nthe large reduction in costs and in terms of capital and net profit of\ncompanies is a large number that has a clear and strong potential for insurance\ncompanies.\n  Originality/Value: In this paper, investment modeling is performed to examine\nthe performance of dynamic networks of insurance companies in Iran.\n"
    },
    {
        "paper_id": 2206.11853,
        "authors": "Rasoul Jamshidi, Mohammad Ebrahim Sadeghi",
        "title": "Application of accelerated life testing in human reliability analysis",
        "comments": "International Journal of Research in Industrial Engineering is\n  licensed, 2021",
        "journal-ref": null,
        "doi": "10.22105/RIEJ.2021.288263.1227",
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  As manufacturers and technologies become more complicated, manufacturing\nerrors such as machine failure and human error have also been considered more\nover the past. Since machines and humans are not error-proof, managing the\nmachines and human errors is a significant challenge in manufacturing systems.\nThere are numerous methods for investigating human errors, fatigue, and\nreliability that categorized under Human Reliability Analysis (HRA) methods.\nHRA methods use some qualitative factors named Performance Shaping Factors\n(PSFs) to estimate Human Error Probability (HEP). Since the PSFs can be\nconsidered as the acceleration factors in Accelerated Life Test (ALT). We\ndeveloped a method for Accelerated Human Fatigue Test (AHFT) to calculate human\nfatigue, according to fatigue rate and other effective factors. The proposed\nmethod reduces the time and cost of human fatigue calculation. AHFT first\nextracts the important factors affecting human fatigue using Principal\nComponent Analysis (PCA) and then uses the accelerated test to calculate the\neffect of PSFs on human fatigue. The proposed method has been applied to a real\ncase, and the provided results show that human fatigue can be calculated more\neffectively using the proposed method.\n"
    },
    {
        "paper_id": 2206.11933,
        "authors": "Jos\\'e Pedro Gaiv\\~ao and Benito Pires",
        "title": "Chaotic time series in financial processes consisting of savings with\n  piecewise constant monthly contributions",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We investigate the time series generated by an elementary and deterministic\nfinancial process that consists in making monthly contributions to a savings\naccount subjected to the devaluation by a monthly negative real interest rate.\nThe monthly contribution is a piecewise constant function of the account\nbalance. We show that a dichotomy holds for such a financial time series:\neither the financial time series are asymptotic to finitely many periodic\nsequences or the financial time series have an uncountable (Cantor) set of\n{\\omega}-limit points. We also provide explicit parameters for which the\nfinancial process is chaotic in the sense that the financial time series have\nsensitive dependence on initial conditions at points of a Cantor attractor.\n"
    },
    {
        "paper_id": 2206.11937,
        "authors": "Daiwei Zhu, Weiwei Shen, Annarita Giani, Saikat Ray Majumder, Bogdan\n  Neculaes, Sonika Johri",
        "title": "Copula-based Risk Aggregation with Trapped Ion Quantum Computers",
        "comments": "10 pages, 9 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Copulas are mathematical tools for modeling joint probability distributions.\nSince copulas enable one to conveniently treat the marginal distribution of\neach variable and the interdependencies among variables separately, in the past\n60 years they have become an essential analysis tool on classical computers in\nvarious fields ranging from quantitative finance and civil engineering to\nsignal processing and medicine. The recent finding that copulas can be\nexpressed as maximally entangled quantum states has revealed a promising\napproach to practical quantum advantages: performing tasks faster, requiring\nless memory, or, as we show, yielding better predictions. Studying the\nscalability of this quantum approach as both the precision and the number of\nmodeled variables increase is crucial for its adoption in real-world\napplications. In this paper, we successfully apply a Quantum Circuit Born\nMachine (QCBM) based approach to modeling 3- and 4-variable copulas on trapped\nion quantum computers. We study the training of QCBMs with different levels of\nprecision and circuit design on a simulator and a state-of-the-art trapped ion\nquantum computer. We observe decreased training efficacy due to the increased\ncomplexity in parameter optimization as the models scale up. To address this\nchallenge, we introduce an annealing-inspired strategy that dramatically\nimproves the training results. In our end-to-end tests, various configurations\nof the quantum models make a comparable or better prediction in risk\naggregation tasks than the standard classical models. Our detailed study of the\ncopula paradigm using quantum computing opens opportunities for its deployment\nin various industries.\n"
    },
    {
        "paper_id": 2206.11973,
        "authors": "Xiaotong Sun, Charalampos Stasinakis, Georgios Sermpinis",
        "title": "Liquidity Risks in Lending Protocols: Evidence from Aave Protocol",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Lending Protocols (LPs), as blockchain-based lending systems, allow any\nagents to borrow and lend cryptocurrencies. However, liquidity risks could\noccur, especially when salient loans are initiated by a particular group of\nborrowers. This paper proposes measurements of liquidity risks, focusing on\nboth available liquidity and market concentration in LPs. By using Aave as a\ncase study, we find that liquidity risks are highly volatile and show complex\neffects on Aave, and liquidity in Aave may affect across on-chain lending\nmarket. Compared to new users, regular users that repeatedly borrow\ncryptocurrencies may negatively affect Aave protocol, implying that user\nloyalty is a double-edged sword for LPs.\n"
    },
    {
        "paper_id": 2206.12095,
        "authors": "Jatin Dhingra and Kartikeya Singh and Siddhartha P. Chakrabarty",
        "title": "Leverage Ratio: An empirical study of the European banking system",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  This paper empirically analyzes a dataset published by the European Banking\nAuthority. Our main aim was to study how the Leverage Ratio is affected by\nadverse financial scenarios. This was be followed by observing how Leverage\nRatio exposures are correlated to various other financial variables and how\nvarious regression techniques can be used to explain the correlation.\n"
    },
    {
        "paper_id": 2206.12148,
        "authors": "Pei-Ting Wang and Chung-Han Hsieh",
        "title": "On Data-Driven Log-Optimal Portfolio: A Sliding Window Approach",
        "comments": "To appear in the IFAC-PapersOnline (25th International Symposium on\n  Mathematical Theory of Network and Systems)",
        "journal-ref": "IFAC-PapersOnline, vol. 55, no. 30, pp. 474-479, 2022",
        "doi": "10.1016/j.ifacol.2022.11.098",
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  In this paper, we propose a data-driven sliding window approach to solve a\nlog-optimal portfolio problem. In contrast to many of the existing papers, this\napproach leads to a trading strategy with time-varying portfolio weights rather\nthan fixed constant weights. We show, by conducting various empirical studies,\nthat the approach possesses a superior trading performance to the classical\nlog-optimal portfolio in the sense of having a higher cumulative rate of\nreturns.\n"
    },
    {
        "paper_id": 2206.1222,
        "authors": "Hansjoerg Albrecher and Pablo Azcue and Nora Muler",
        "title": "Optimal dividends under a drawdown constraint and a curious square-root\n  rule",
        "comments": "40 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  In this paper we address the problem of optimal dividend payout strategies\nfrom a surplus process governed by Brownian motion with drift under a drawdown\nconstraint, i.e. the dividend rate can never decrease below a given fraction\n$a$ of its historical maximum. We solve the resulting two-dimensional optimal\ncontrol problem and identify the value function as the unique viscosity\nsolution of the corresponding Hamilton-Jacobi-Bellman equation. We then derive\nsufficient conditions under which a two-curve strategy is optimal, and show how\nto determine its concrete form using calculus of variations. We establish a\nsmooth-pasting principle and show how it can be used to prove the optimality of\ntwo-curve strategies for sufficiently large initial and maximum dividend rate.\nWe also give a number of numerical illustrations in which the optimality of the\ntwo-curve strategy can be established for instances with smaller values of the\nmaximum dividend rate, and the concrete form of the curves can be determined.\nOne observes that the resulting drawdown strategies nicely interpolate between\nthe solution for the classical unconstrained dividend problem and the one for a\nratcheting constraint as recently studied in Albrecher et al. (2022). When the\nmaximum allowed dividend rate tends to infinity, we show a surprisingly simple\nand somewhat intriguing limit result in terms of the parameter $a$ for the\nsurplus level on from which, for sufficiently large current dividend rate, a\ntake-the-money-and-run strategy is optimal in the presence of the drawdown\nconstraint.\n"
    },
    {
        "paper_id": 2206.12264,
        "authors": "Kai Xing, Shang Li, Xiaoguang Yang",
        "title": "Impact of the political risk on food reserve ratio: evidence across\n  countries",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Using an unbalanced panel data covering 75 countries from 1991 to 2019, we\nexplore how the political risk impacts on food reserve ratio. The empirical\nfindings show that an increasing political risk negatively affect food reserve\nratio, and same effects hold for both internal risk and external risk.\nMoreover, we find that the increasing external or internal risks both\nnegatively affect production and exports, but external risk does not\nsignificantly impact on imports and it positively impacts on consumption, while\ninternal risk negatively impacts on imports and consumption. The results\nsuggest that most of governments have difficulty to raise subsequent food\nreserve ratio in face of an increasing political risk, no matter it is an\ninternal risk or an external risk although the mechanisms behind the impacts\nare different.\n"
    },
    {
        "paper_id": 2206.12277,
        "authors": "Hamed Nozari, Mohammad Ebrahim Sadeghi, Javid Ghahremani nahr, Seyyed\n  Esmaeil Najafi",
        "title": "Quantitative Analysis of Implementation Challenges of IoT-Based Digital\n  Supply Chain (Supply Chain 0/4)",
        "comments": "in Farsi, Journal of Quality & Standard Management (JQSM), 2022",
        "journal-ref": null,
        "doi": "10.22034/jsqm.2022.314683.1380",
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Over the past thirty years, logistics has undergone tremendous change from a\npurely operational performance that led to sales or production and focused on\nsecuring supply and delivery lines to customers, with the transformation of\nintelligent technologies into a professional operator. In today's world, the\nfourth generation of industry has forced companies to rethink how their supply\nchain is designed. In this case, in addition to the need for adaptation, supply\nchains also have the opportunity to reach operational horizons, use emerging\ndigital supply chain business models, and transform the company into a digital\nsupply chain. One of the transformational technologies that has had a\ntremendous impact on the supply chain in this regard is IoT technology. This\ntechnology, as one of the largest sources of data production, can facilitate\nsupply chain processes in all its dimensions. However, due to the presence of\nthe Internet and the location of supply chain components in the context of\ninformation networks, this digital supply chain always faces major challenges.\nTherefore, in this paper, an attempt was made to examine and prioritize the\nmost important challenges of implementing a supply chain 0/4 using a nonlinear\nhierarchical analysis method. In order to investigate these challenges, the\nopinions of experts active in the supply chain of Fast-moving consumer goods\nindustries (FMCG) were used as a case study as well as some academic\nexperts.The results show that the lack of technological infrastructure and\nsecurity challenges are the most important challenges of implementing supply\nchain 0/4 in the era of digital developments, which should be given special\nattention for a successful implementation.\n"
    },
    {
        "paper_id": 2206.12282,
        "authors": "Pat Tong Chio",
        "title": "A comparative study of the MACD-base trading strategies: evidence from\n  the US stock market",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  In recent years, more and more investors use technical analysis methods in\ntheir own trading. Evaluating the effectiveness of technical analysis has\nbecome more feasible due to increasing computing capability and blooming public\ndata, which indie investors can perform stock analysis and backtest their own\ntrading strategy conveniently. The Moving Average Convergence Divergence (MACD)\nindicator is one of the popular technical indicators that are widely used in\ndifferent strategies. In order to verify the MACD effectiveness, in this\nthesis, I use the MACD indicator with traditional parameters (12, 26, 9) to\nbuild various trading strategies. Then, I apply these strategies to stocks\nlisted on three indices in the US stock market (i.e., Dow-Jones, Nasdaq, and\nS&P 500) and evaluate its performance in terms of win rate, profitability,\nSharpe ratio, number of trades and maximum drawdown. The backtesting is\nprogrammed using Python, covering the period between 01/01/2015 and 28-08-2021.\nThe result shows that the win-rate of the strategy with only the MACD indicator\nis less than 50%. However, the win-rate is improved for the trading strategies\nthat combine the MACD indicator with other momentum indicators like the Money\nFlow Index (MFI) and the Relative Strength Index (RSI). Based on this result, I\nredesign the MACD mathematical formula by taking the trading volume and daily\nprice volatility into consideration to derive a new indicator called VPVMA. The\nresults show that the win-rate and risk-adjust performance of this new trading\nstrategy have been improved significantly. In general, the findings suggest\nthat while all the MACD trading strategies mentioned above can generate\npositive returns, the performance is not good without using other momentum\nindicators. Hence, the VPVMA indicator performs better.\n"
    },
    {
        "paper_id": 2206.12399,
        "authors": "Kim Weston",
        "title": "Existence of an equilibrium with limited participation",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  A limited participation economy models the real-world phenomenon that some\neconomic agents have access to more of the financial market than others. We\nprove the global existence of a Radner equilibrium with limited participation,\nwhere the agents have exponential preferences and derive utility from both\nrunning consumption and terminal wealth. Our analysis centers around the\nexistence and uniqueness of a solution to a coupled system of quadratic\nbackward stochastic differential equations (BSDEs). We prove that the BSDE\nsystem has a unique $\\mathcal{S}^\\infty\\times\\text{bmo}$ solution. We define a\ncandidate equilibrium in terms of the BSDE solution and prove through a\nverification argument that the candidate is a Radner equilibrium with limited\nparticipation. This work generalizes the model of Basak and Cuoco (1998) to\nallow for a stock with a general dividend stream and agents with exponential\npreferences. We also provide an explicit example.\n"
    },
    {
        "paper_id": 2206.12511,
        "authors": "Carole Bernard, Stephan Sturm",
        "title": "Cost-efficiency in Incomplete Markets",
        "comments": "31 pages. Examples and Counterexamples have been relegated to a\n  separate document, upon journal editor's request",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper studies the topic of cost-efficiency in incomplete markets. A\npayoff is called cost-efficient if it achieves a given probability distribution\nat some given investment horizon with a minimum initial budget. Extensive\nliterature exists for the case of a complete financial market. We show how the\nproblem can be extended to incomplete markets and how the main results from the\ntheory of complete markets still hold in adapted form. In particular, we find\nthat in incomplete markets, the optimal portfolio choice for non-decreasing\npreferences that are diversification-loving (a notion introduced in this paper)\nmust be \"perfectly\" cost-efficient. This notion of perfect cost-efficiency is\nshown to be equivalent to the fact that the payoff can be rationalized, i.e.,\nit is the solution to an expected utility problem.\n"
    },
    {
        "paper_id": 2206.12528,
        "authors": "Fengyu Han and Yue Wang",
        "title": "Predicting Stock Price Movement after Disclosure of Corporate Annual\n  Reports: A Case Study of 2021 China CSI 300 Stocks",
        "comments": "My experimental conditions were not set correctly, and almost all the\n  data in the table were filled in incorrectly. I had to repeat all the\n  experiments and make updated descriptions, but the wrong data and\n  descriptions caused confusion to others. I may need several months to redo\n  the experiment, so I hope to withdraw my manuscript first",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In the current stock market, computer science and technology are more and\nmore widely used to analyse stocks. Not same as most related machine learning\nstock price prediction work, this work study the predicting the tendency of the\nstock price on the second day right after the disclosure of the companies'\nannual reports. We use a variety of different models, including decision tree,\nlogistic regression, random forest, neural network, prototypical networks. We\nuse two sets of financial indicators (key and expanded) to conduct experiments,\nthese financial indicators are obtained from the EastMoney website disclosed by\ncompanies, and finally we find that these models are not well behaved to\npredict the tendency. In addition, we also filter stocks with ROE greater than\n0.15 and net cash ratio greater than 0.9. We conclude that according to the\nfinancial indicators based on the just-released annual report of the company,\nthe predictability of the stock price movement on the second day after\ndisclosure is weak, with maximum accuracy about 59.6% and maximum precision\nabout 0.56 on our test set by the random forest classifier, and the stock\nfiltering does not improve the performance. And random forests perform best in\ngeneral among all these models which conforms to some work's findings.\n"
    },
    {
        "paper_id": 2206.1261,
        "authors": "Marlon G. Boarnet (University of Southern California), Xize Wang\n  (University of Southern California), Douglas Houston (University of\n  California, Irvine)",
        "title": "Can New Light Rail Reduce Personal Vehicle Carbon Emissions? A\n  Before-After, Experimental-Control Evaluation in Los Angeles",
        "comments": null,
        "journal-ref": "Journal of Regional Science, 57(3), 523-539 (2017)",
        "doi": "10.1111/jors.12275",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper uses a before-after, experimental-control group method to evaluate\nthe impacts of the newly opened Expo light rail transit line in Los Angeles on\npersonal vehicle greenhouse gas (GHG) emissions. We applied the California Air\nResources Board's EMFAC 2011 emission model to estimate the amount of daily\naverage CO2 emissions from personal vehicle travel for 160 households across\ntwo waves, before and after the light rail opened. The 160 households were part\nof an experimental-ccontrol group research design. Approximately half of the\nhouseholds live within a half-mile of new Expo light rail stations (the\nexperimental group) and the balance of the sampled households live beyond a\nhalf-mile from Expo light rail stations (the control group). Households tracked\nodometer mileage for all household vehicles for seven days in two sample waves,\nbefore the Expo Line opened (fall, 2011) and after the Expo Line opened (fall,\n2012). Our analysis indicates that opening the Expo Line had a statistically\nsignificant impact on average daily CO2 emissions from motor vehicles. We found\nthat the CO2 emission of households who reside within a half-mile of an Expo\nLine station was 27.17 percent smaller than those living more than a half-mile\nfrom a station after the opening of the light rail, while no significant\ndifference exists before the opening. A difference-in-difference model suggests\nthat the opening of the Expo Line is associated with 3,145 g less of household\nvehicle CO2 emissions per day as a treatment effect. A sensitivity analysis\nindicates that the emission reduction effect is also present when the\nexperimental group of households is redefined to be those living within a\nkilometer from the new light rail stations.\n"
    },
    {
        "paper_id": 2206.12613,
        "authors": "Marlon G. Boarnet (University of Southern California), Xize Wang\n  (University of California, Berkeley)",
        "title": "Urban Spatial Structure and the Potential for Vehicle Miles Traveled\n  Reduction: The Effects of Accessibility to Jobs within and beyond Employment\n  Sub-centers",
        "comments": null,
        "journal-ref": "The Annals of Regional Science, 62(2), 381-404 (2019)",
        "doi": "10.1007/s00168-019-00900-7",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This research examines the relationship between urban polycentric spatial\nstructure and driving. We identified 46 employment sub-centers in the Los\nAngeles Combined Statistical Area and calculated access to jobs that are within\nand beyond these sub-centers. To address potential endogeneity problems, we use\naccess to historically important places and transportation infrastructure in\nthe early 20th century as instrumental variables for job accessibility indices.\nOur Two-stage Tobit models show that access to jobs is negatively associated\nwith household vehicle miles traveled in this region. Among various\naccessibility measures, access to jobs outside sub-centers has the largest\nelasticity (-0.155). We examine the location of places in the top quintile of\naccess to non-centered jobs and find that those locations are often inner ring\nsuburban developments, near the core of the urban area and not far from\nsub-centers, suggesting that strategies of infill development that fill in the\ngaps between sub-centers, rather than focusing on already accessible downtowns\nand large sub-centers, may be the best land use approach to reduce VMT.\n"
    },
    {
        "paper_id": 2206.12696,
        "authors": "Zhaoqi Zang, Xiangdong Xu, Kai Qu, Ruiya Chen and Anthony Chen",
        "title": "Travel time reliability in transportation networks: A review of\n  methodological developments",
        "comments": "This is an extended version of the paper submitted to the TR Part C",
        "journal-ref": null,
        "doi": "10.1016/j.trc.2022.103866",
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  The unavoidable travel time variability in transportation networks, resulted\nfrom the widespread supply side and demand side uncertainties, makes travel\ntime reliability (TTR) be a common and core interest of all the stakeholders in\ntransportation systems, including planners, travelers, service providers, and\nmanagers. This common and core interest stimulates extensive studies on\nmodeling TTR. Researchers have developed a range of theories and models of TTR,\nmany of which have been incorporated into transportation models, policies, and\nproject appraisals. Adopting the network perspective, this paper aims to\nprovide an integrated framework for reviewing the methodological developments\nof modeling TTR in transportation networks, including its characterization,\nevaluation and valuation, and traffic assignment. Specifically, the TTR\ncharacterization provides a whole picture of travel time distribution in\ntransportation networks. TTR evaluation and TTR valuation (known as the value\nof reliability, VOR) simply and intuitively interpret abstract characterized\nTTR to be well understood by different stakeholders of transportation systems.\nTTR-based traffic assignment investigates the effects of TTR on the individual\nusers travel behavior and consequently the collective network flow pattern. As\nthe above three topics are mainly separately studied in different disciplines\nand research areas, the integrated framework allows us to better understand\ntheir relationships and may contribute to developing possible combinations of\nTTR modeling philosophy. Also, the network perspective enables to focus on\ncommon challenges of modeling TTR, especially the uncertainty propagation from\nthe uncertainty sources to the TTR at spatial levels including link, route, and\nthe entire network. Some directions for future research are discussed in the\nera of new data environment, applications, and emerging technologies.\n"
    },
    {
        "paper_id": 2206.12835,
        "authors": "Anand Deo, Karthyek Murthy and Tirtho Sarker",
        "title": "Combining Retrospective Approximation with Importance Sampling for\n  Optimising Conditional Value at Risk",
        "comments": "12 pages, 4 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  This paper investigates the use of retrospective approximation solution\nparadigm in solving risk-averse optimization problems effectively via\nimportance sampling (IS). While IS serves as a prominent means for tackling the\nlarge sample requirements in estimating tail risk measures such as Conditional\nValue at Risk (CVaR), its use in optimization problems driven by CVaR is\ncomplicated by the need to tailor the IS change of measure differently to\ndifferent optimization iterates and the circularity which arises as a\nconsequence. The proposed algorithm overcomes these challenges by employing a\nunivariate IS transformation offering uniform variance reduction in a\nretrospective approximation procedure well-suited for tuning the IS parameter\nchoice. The resulting simulation based approximation scheme enjoys both the\ncomputational efficiency bestowed by retrospective approximation and\nlogarithmically efficient variance reduction offered by importance sampling\n"
    }
]