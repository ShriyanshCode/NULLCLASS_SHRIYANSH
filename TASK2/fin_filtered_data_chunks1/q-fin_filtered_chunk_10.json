[
    {
        "paper_id": 1910.0575,
        "authors": "Beatrice Acciaio and Julien Guyon",
        "title": "Inversion of Convex Ordering: Local Volatility Does Not Maximize the\n  Price of VIX Futures",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  It has often been stated that, within the class of continuous stochastic\nvolatility models calibrated to vanillas, the price of a VIX future is\nmaximized by the Dupire local volatility model. In this article we prove that\nthis statement is incorrect: we build a continuous stochastic volatility model\nin which a VIX future is strictly more expensive than in its associated local\nvolatility model. More generally, in this model, strictly convex payoffs on a\nsquared VIX are strictly cheaper than in the associated local volatility model.\nThis corresponds to an inversion of convex ordering between local and\nstochastic variances, when moving from instantaneous variances to squared VIX,\nas convex payoffs on instantaneous variances are always cheaper in the local\nvolatility model. We thus prove that this inversion of convex ordering, which\nis observed in the SPX market for short VIX maturities, can be produced by a\ncontinuous stochastic volatility model. We also prove that the model can be\nextended so that, as suggested by market data, the convex ordering is preserved\nfor long maturities.\n"
    },
    {
        "paper_id": 1910.05902,
        "authors": "Abootaleb Shirvani, Yuan Hu, Svetlozar T. Rachev, and Frank J. Fabozzi",
        "title": "Option Pricing with Mixed Levy Subordinated Price Process and Implied\n  Probability Weighting Function",
        "comments": null,
        "journal-ref": "The Journal of Derivatives Winter 2020, jod.2020.1.102;",
        "doi": "10.3905/jod.2020.1.102",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  It is essential to incorporate the impact of investor behavior when modeling\nthe dynamics of asset returns. In this paper, we reconcile behavioral finance\nand rational finance by incorporating investor behavior within the framework of\ndynamic asset pricing theory. To include the views of investors, we employ the\nmethod of subordination which has been proposed in the literature by including\nbusiness (intrinsic, market) time. We define a mixed Levy subordinated model by\nadding a single subordinated Levy process to the well-known log-normal model,\nresulting in a new log-price process. We apply the proposed models to study the\nbehavioral finance notion of \"greed and fear\" disposition from the perspective\nof rational dynamic asset pricing theory. The greedy or fearful disposition of\noption traders is studied using the shape of the probability weighting\nfunction. We then derive the implied probability weighting function for the\nfear and greed deposition of option traders in comparison to spot traders. Our\nresult shows the diminishing sensitivity of option traders. Diminishing\nsensitivity results in option traders overweighting the probability of big\nlosses in comparison to spot traders.\n"
    },
    {
        "paper_id": 1910.05999,
        "authors": "Matteo Brachetta and Claudia Ceci",
        "title": "A BSDE-based approach for the optimal reinsurance problem under partial\n  information",
        "comments": "30 pages, 3 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We investigate the optimal reinsurance problem under the criterion of\nmaximizing the expected utility of terminal wealth when the insurance company\nhas restricted information on the loss process. We propose a risk model with\nclaim arrival intensity and claim sizes distribution affected by an\nunobservable environmental stochastic factor. By filtering techniques (with\nmarked point process observations), we reduce the original problem to an\nequivalent stochastic control problem under full information. Since the\nclassical Hamilton-Jacobi-Bellman approach does not apply, due to the infinite\ndimensionality of the filter, we choose an alternative approach based on\nBackward Stochastic Differential Equations (BSDEs). Precisely, we characterize\nthe value process and the optimal reinsurance strategy in terms of the unique\nsolution to a BSDE driven by a marked point process.\n"
    },
    {
        "paper_id": 1910.06242,
        "authors": "Anirban Chakraborti, Hrishidev, Kiran Sharma and Hirdesh K. Pharasi",
        "title": "Phase separation and scaling in correlation structures of financial\n  markets",
        "comments": "27 pages, 15 figures, 3 Tables",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Financial markets, being spectacular examples of complex systems, display\nrich correlation structures among price returns of different assets. The\ncorrelation structures change drastically, akin to phase transitions in\nphysical phenomena, as do the influential stocks (leaders) and sectors\n(communities), during market events like crashes. It is crucial to detect their\nsignatures for timely intervention or prevention. Here we use eigenvalue\ndecomposition and eigen-entropy, computed from eigen-centralities of different\nstocks in the cross-correlation matrix, to extract information about the\ndisorder in the market. We construct a `phase space', where different market\nevents (bubbles, crashes, etc.) undergo phase separation and display\norder-disorder transitions. An entropy functional exhibits scaling behavior. We\npropose a generic indicator that facilitates the continuous monitoring of the\ninternal structure of the market -- important for managing risk and\nstress-testing the financial system. Our methodology would help in\nunderstanding and foreseeing tipping points or fluctuation patterns in complex\nsystems.\n"
    },
    {
        "paper_id": 1910.06432,
        "authors": "Tim Leung and Yang Zhou",
        "title": "Optimal Dynamic Futures Portfolio in a Regime-Switching Market Framework",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We study the problem of dynamically trading futures in a regime-switching\nmarket. Modeling the underlying asset price as a Markov-modulated diffusion\nprocess, we present a utility maximization approach to determine the optimal\nfutures trading strategy. This leads to the analysis of the associated system\nof Hamilton-Jacobi-Bellman (HJB) equations, which are reduced to a system of\nlinear ODEs. We apply our stochastic framework to two models, namely, the\nRegime-Switching Geometric Brownian Motion (RS-GBM) model and Regime-Switching\nExponential Ornstein-Uhlenbeck (RS-XOU) model. Numerical examples are provided\nto illustrate the investor's optimal futures positions and portfolio value\nacross market regimes.\n"
    },
    {
        "paper_id": 1910.06463,
        "authors": "Andrew Papanicolaou and Shiva Chandra",
        "title": "Singular Perturbation Expansion for Utility Maximization with\n  Order-$\\epsilon$ Quadratic Transaction Costs",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  We present an expansion for portfolio optimization in the presence of small,\ninstantaneous, quadratic transaction costs. Specifically, the magnitude of\ntransaction costs has a coefficient that is of the order $\\epsilon$ small,\nwhich leads to the optimization problem having an asymptotically-singular\nHamilton-Jacobi-Bellman equation whose solution can be expanded in powers of\n$\\sqrt\\epsilon$. In this paper we derive explicit formulae for the first two\nterms of this expansion. Analysis and simulation are provided to show the\nbehavior of this approximating solution.\n"
    },
    {
        "paper_id": 1910.06499,
        "authors": "Ailton Castro Pinheiro",
        "title": "Precisamos de uma Contabilidade Ambiental para as \"Amaz\\^onias\"\n  Paraense?",
        "comments": "in Portuguese",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/publicdomain/zero/1.0/",
        "abstract": "  This paper has the following objectives: to understand the concepts of\nEnvironmental Accounting in Brazil; Make criticisms and propositions anchored\nin the reality or demand of environmental accounting for Amazonia Paraense. The\nmethodological strategy was a critical analysis of Ferreira's books (2007);\nRibeiro (2010) and Tinoco and Kraemer (2011) using their correlation with the\nscientific production of authors discussing the Paraense Amazon, besides our\nexperience as researchers of this territory. As a result, we created three\nsections: one for understanding the current constructs of environmental\naccounting, one for criticism and one for propositions.\n"
    },
    {
        "paper_id": 1910.0666,
        "authors": "Imma Valentina Curato and Simona Sanfelici",
        "title": "Stochastic leverage effect in high-frequency data: a Fourier based\n  analysis",
        "comments": "Accepted for publication in Econometrics and Statistics",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The stochastic leverage effect, defined as the standardized covariation\nbetween the returns and their related volatility, is analyzed in a stochastic\nvolatility model set-up. A novel estimator of the effect is defined using a\npre-estimation of the Fourier coefficients of the return and the volatility\nprocesses. The consistency of the estimator is proven. Moreover, its finite\nsample properties are studied in the presence of microstructure noise effects.\nThe Fourier methodology is applied to S\\&P500 futures prices to investigate the\nmagnitude of the stochastic leverage effect detectable at high-frequency.\n"
    },
    {
        "paper_id": 1910.06739,
        "authors": "Roman G. Smirnov and Kunpeng Wang",
        "title": "The Cobb-Douglas production function revisited",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Charles Cobb and Paul Douglas in 1928 used data from the US manufacturing\nsector for 1899-1922 to introduce what is known today as the Cobb-Douglas\nproduction function that has been widely used in economic theory for decades.\nWe employ the R programming language to fit the formulas for the parameters of\nthe Cobb-Douglas production function generated by the authors recently via the\nbi-Hamiltonian approach to the same data set utilized by Cobb and Douglas. We\nconclude that the formulas for the output elasticities and total factor\nproductivity are compatible with the original 1928 data.\n"
    },
    {
        "paper_id": 1910.06746,
        "authors": "Lijuan Ma, Marcel Ausloos, Christophe Schinckus, and H. L. Felicia\n  Chong",
        "title": "Fundamental Analysis in China: An Empirical Study of the Relationship\n  between Financial Ratios and Stock Prices",
        "comments": "30 pages, 19 Tables, 36 references",
        "journal-ref": "Theoretical Economics Letters, 2018, 8, 3411-3437",
        "doi": "10.4236/tel.2018.815209",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The informational context is regularly questioned in a transitional economic\nregime like the one implemented in China or Vietnam. This article investigates\nthis issue and the predictive power of fundamental analysis in such context and\nmore precisely in a Chinese context with an analysis of 3 different industries\n(media, power, and steel). Through 3 different kinds of correlation, we examine\n25 financial determinants for 60 Chinese listed companies between 2011 and\n2015. Our results show that fundamental analysis can effectively be used as an\ninvestment tool in transitional economic context. Contrasting with the EMH for\nwhich the accounting information is instantaneously integrated into the\nfinancial information (stock prices), our study suggests that these two levels\nof information are not synchronized in China opening therefore a door for a\nfundamental analysis based prediction. Furthermore, our results also indicate\nthat accounting information illustrates quite well the economic reality since\nfinancial reports in each industry can disclose a part of stock value\ninformation in line with the economic situation of the industry under\nconsideration.\n"
    },
    {
        "paper_id": 1910.06872,
        "authors": "Ben-Zhang Yang, Xiaoping Lu, Guiyuan Ma, Song-Ping Zhu",
        "title": "Robust portfolio optimization with multi-factor stochastic volatility",
        "comments": null,
        "journal-ref": "Journal of Optimization Theory and Applications, 2020",
        "doi": "10.1007/s10957-020-01687-w",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper studies a robust portfolio optimization problem under the\nmulti-factor volatility model introduced by Christoffersen et al. (2009). The\noptimal strategy is derived analytically under the worst-case scenario with or\nwithout derivative trading. To illustrate the effects of ambiguity, we compare\nour optimal robust strategy with some strategies that ignore the information of\nuncertainty, and provide the corresponding welfare analysis. The effects of\nderivative trading to the optimal portfolio selection are also discussed by\nconsidering alternative strategies. Our study is further extended to the cases\nwith jump risks in asset price and correlated volatility factors, respectively.\nNumerical experiments are provided to demonstrate the behavior of the optimal\nportfolio and utility loss.\n"
    },
    {
        "paper_id": 1910.0691,
        "authors": "Hansjoerg Albrecher and Pablo Azcue and Nora Muler",
        "title": "Optimal ratcheting of dividends in insurance",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We address a long-standing open problem in risk theory, namely the optimal\nstrategy to pay out dividends from an insurance surplus process, if the\ndividend rate can never be decreased. The optimality criterion here is to\nmaximize the expected value of the aggregate discounted dividend payments up to\nthe time of ruin. In the framework of the classical Cram\\'{e}r-Lundberg risk\nmodel, we solve the corresponding two-dimensional optimal control problem and\nshow that the value function is the unique viscosity solution of the\ncorresponding Hamilton-Jacobi-Bellman equation. We also show that the value\nfunction can be approximated arbitrarily closely by ratcheting strategies with\nonly a finite number of possible dividend rates and identify the free boundary\nand the optimal strategies in several concrete examples. These implementations\nillustrate that the restriction of ratcheting does not lead to a large\nefficiency loss when compared to the classical un-constrained optimal dividend\nstrategy.\n"
    },
    {
        "paper_id": 1910.07158,
        "authors": "Chuancun Yin",
        "title": "Stochastic Orderings of Multivariate Elliptical Distributions",
        "comments": "21pages",
        "journal-ref": "J. Appl. Probab. 58 (2021) 551-568",
        "doi": "10.1017/jpr.2020.104",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Let ${\\bf X}$ and ${\\bf X}$ be two $n$-dimensional elliptical random vectors,\nwe establish an identity for $E[f({\\bf Y})]-E[f({\\bf X})]$, where $f: \\Bbb{R}^n\n\\rightarrow \\Bbb{R}$ fulfilling some regularity conditions. Using this identity\nwe provide a unified derivation of sufficient and necessary conditions for\nclassifying multivariate elliptical random vectors according to several main\nintegral stochastic orders. As a consequence we obtain new inequalities by\napplying it to multivariate elliptical distributions. The results generalize\nthe corresponding ones for multivariate normal random vectors in the\nliterature.\n"
    },
    {
        "paper_id": 1910.07241,
        "authors": "Damir Filipovi\\'c, Kathrin Glau, Yuji Nakatsukasa, Francesco Statti",
        "title": "Weighted Monte Carlo with least squares and randomized extended Kaczmarz\n  for option pricing",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We propose a methodology for computing single and multi-asset European option\nprices, and more generally expectations of scalar functions of (multivariate)\nrandom variables. This new approach combines the ability of Monte Carlo\nsimulation to handle high-dimensional problems with the efficiency of function\napproximation. Specifically, we first generalize the recently developed method\nfor multivariate integration in [arXiv:1806.05492] to integration with respect\nto probability measures. The method is based on the principle \"approximate and\nintegrate\" in three steps i) sample the integrand at points in the integration\ndomain, ii) approximate the integrand by solving a least-squares problem, iii)\nintegrate the approximate function. In high-dimensional applications we face\nmemory limitations due to large storage requirements in step ii). Combining\nweighted sampling and the randomized extended Kaczmarz algorithm we obtain a\nnew efficient approach to solve large-scale least-squares problems. Our\nconvergence and cost analysis along with numerical experiments show the\neffectiveness of the method in both low and high dimensions, and under the\nassumption of a limited number of available simulations.\n"
    },
    {
        "paper_id": 1910.07417,
        "authors": "Ljudmila A. Bordag",
        "title": "Portfolio optimization in the case of an exponential utility function\n  and in the presence of an illiquid asset",
        "comments": "28 pages, 1 Table. arXiv admin note: text overlap with\n  arXiv:1512.06295",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We study an optimization problem for a portfolio with a risk-free, a liquid,\nand an illiquid risky asset. The illiquid risky asset is sold in an exogenous\nrandom moment with a prescribed liquidation time distribution. The investor\nprefers a negative or a positive exponential utility function. We prove that\nboth cases are connected by a one-to-one analytical substitution and are\nidentical from the economic, analytical, or Lie algebraic points of view.\n  It is well known that the exponential utility function is connected with the\nHARA utility function through a limiting procedure if the parameter of the HARA\nutility function is going to infinity. We show that the optimization problem\nwith the exponential utility function is not connected to the HARA case by the\nlimiting procedure and we obtain essentially different results.\n  For the main three dimensional PDE with the exponential utility function we\nobtain the complete set of the nonequivalent Lie group invariant reductions to\ntwo dimensional PDEs according to an optimal system of subalgebras of the\nadmitted Lie algebra. We prove that in just one case the invariant reduction is\nconsistent with the boundary condition. This reduction represents a significant\nsimplification of the original problem.\n"
    },
    {
        "paper_id": 1910.07564,
        "authors": "Jifei Wang, Lingjing Wang",
        "title": "Residual Switching Network for Portfolio Optimization",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper studies deep learning methodologies for portfolio optimization in\nthe US equities market. We present a novel residual switching network that can\nautomatically sense changes in market regimes and switch between momentum and\nreversal predictors accordingly. The residual switching network architecture\ncombines two separate residual networks (ResNets), namely a switching module\nthat learns stock market conditions, and the main module that learns momentum\nand reversal predictors. We demonstrate that over-fitting noisy financial data\ncan be controlled with stacked residual blocks and further incorporating the\nattention mechanism can enhance powerful predictive properties. Over the period\n2008 to H12017, the residual switching network (Switching-ResNet) strategy\nverified superior out-of-sample performance with an average annual Sharpe ratio\nof 2.22, compared with an average annual Sharpe ratio of 0.81 for the ANN-based\nstrategy and 0.69 for the linear model.\n"
    },
    {
        "paper_id": 1910.07707,
        "authors": "Hector Galindo-Silva, Guy Tchuente",
        "title": "Fighting for Not-So-Religious Souls: The Role of Religious Competition\n  in Secular Conflicts",
        "comments": null,
        "journal-ref": "J. Econ. Behav. Organ. 2021; 191: 127-152",
        "doi": "10.1016/j.jebo.2021.08.027",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Many countries embroiled in non-religious civil conflicts have experienced a\ndramatic increase in religious competition in recent years. This study examines\nwhether increasing competition between religions affects violence in\nnon-religious or secular conflicts. The study focuses on Colombia, a deeply\nCatholic country that has suffered one of the world's longest-running internal\nconflicts and, in the last few decades, has witnessed an intense increase in\nreligious competition between the Catholic Church and new non-Catholic\nchurches. The estimation of a dynamic treatment effect model shows that\nestablishing the first non-Catholic church in a municipality substantially\nincreases the probability of conflict-related violence. The effect is larger\nfor violence by guerrilla groups, and is concentrated on municipalities where\nthe establishment of the first non-Catholic church leads to more intense\nreligious competition. Further analysis suggests that the increase in guerrilla\nviolence is associated with an expectation among guerrilla groups that their\nmembership will decline as a consequence of more intense competition with\nreligious groups for followers.\n"
    },
    {
        "paper_id": 1910.07859,
        "authors": "Tomas Kala",
        "title": "Currency Based on Time Standard",
        "comments": "28 pages, 5 tables, 1 figure",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The Total Economic Time Capacity of a Year 525600 minutes is postulated as a\ntime standard for a new Monetary Minute currency in this evaluation study.\nConsequently, the Monetary Minute MonMin is defined as a 1/525600 part of the\nTotal Economic Time Capacity of a Year. The Value CMonMin of the Monetary\nMinute MonMin is equal to a 1/525600 part of the GDP, p.c., expressed in a\nspecific state currency C. There is described how the Monetary Minutes MonMin\nare determined, and how their values CMonMin are calculated based on the GDP\nand all the population in specific economies. The Monetary Minutes trace\ndifferent aggregate productivity, i.e. exploitation of the total time capacity\nof a year for generating of the GDP in economies of different states.\n"
    },
    {
        "paper_id": 1910.07971,
        "authors": "Jean-Philippe Aguilar",
        "title": "The value of power-related options under spectrally negative L\\'evy\n  processes",
        "comments": "Minor typos corrected + DOI added + MSC classes added",
        "journal-ref": null,
        "doi": "10.1007/s11147-020-09174-0",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We provide analytical tools for pricing power options with exotic features\n(capped or log payoffs, gap options ...) in the framework of exponential L\\'evy\nmodels driven by one-sided stable or tempered stable processes. Pricing\nformulas take the form of fast converging series of powers of the log-forward\nmoneyness and of the time-to-maturity; these series are obtained via a\nfactorized integral representation in the Mellin space evaluated by means of\nresidues in $\\mathbb{C}$ or $\\mathbb{C}^2$. Comparisons with numerical methods\nand efficiency tests are also discussed.\n"
    },
    {
        "paper_id": 1910.08158,
        "authors": "Wenyuan Wang, Xueyuan Wu and Cheng Chi",
        "title": "Optimal implementation delay of taxation with trade-off for L\\'{e}vy\n  risk Processes",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper we consider two problems on optimal implementation delay of\ntaxation with trade-off for spectrally negative L\\'{e}vy insurance risk\nprocesses. In the first case, we assume that an insurance company starts to pay\ntax when its surplus reaches a certain level $b$ and at the termination time of\nthe business there is a terminal value incurred to the company. The total\nexpected discounted value of tax payments plus the terminal value is maximized\nto obtain the optimal implementation level $b^*$. In the second case, the\ncompany still pays tax subject to an implementation level $a$ but with capital\ninjections to prevent bankruptcy. The total expected discounted value of tax\npayments minus the capital injection costs is maximized to obtain the optimal\nimplementation level $a^*$. Numerical examples are also given to illustrate the\nmain results in this paper.\n"
    },
    {
        "paper_id": 1910.08344,
        "authors": "Samuel Drapeau and Yunbo Zhang",
        "title": "Pricing and Hedging Performance on Pegged FX Markets Based on a Regime\n  Switching Model",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper investigates the hedging performance of pegged foreign exchange\nmarket in a regime switching (RS) model introduced in a recent paper by\nDrapeau, Wang and Wang (2019). We compare two prices, an exact solution and\nfirst order approximation and provide the bounds for the error. We provide\nexact RS delta, approximated RS delta as well as mean variance hedging\nstrategies for this specific model and compare their performance. To improve\nthe efficiency of the pricing and calibration procedure, the Fourier approach\nof this regime-switching model is developed in our work. It turns out that: 1\n-- the calibration of the volatility surface with this regime switching model\noutperforms on real data the classical SABR model; 2 -- the Fourier approach is\nsignificantly faster than the direct approach; 3 -- in terms of hedging, the\napproximated RS delta hedge is a viable alternative to the exact RS delta hedge\nwhile significantly faster.\n"
    },
    {
        "paper_id": 1910.08531,
        "authors": "Zura Kakushadze",
        "title": "Healthy... Distress... Default",
        "comments": "6 pages; no changes to the paper; a LaTeX command added for URLs to\n  display properly",
        "journal-ref": "Journal of Risk & Control 6(1) (2019) 113-119, Invited Editorial",
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We discuss a simple, exactly solvable model of stochastic stock dynamics that\nincorporates regime switching between healthy and distressed regimes. Using\nthis model, which is analytically tractable, we discuss a way of extracting\nexpected returns for stocks from realized CDS spreads, essentially, the CDS\nmarket sentiment about future stock returns. This alpha/signal could be useful\nin a cross-sectional (statistical arbitrage) context for equities trading.\n"
    },
    {
        "paper_id": 1910.08611,
        "authors": "Y\\'erali Gandica, Sophie B\\'ereau, and Jean-Yves Gnabo",
        "title": "A multilevel analysis to systemic exposure: insights from local and\n  system-wide information",
        "comments": "12 pages, 3 figures and 3 tables",
        "journal-ref": null,
        "doi": "10.1038/s41598-020-74259-7",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In the aftermath of the financial crisis, the growing literature on financial\nnetworks has widely documented the predictive power of topological\ncharacteristics (e.g. degree centrality measures) to explain the systemic\nimpact or systemic vulnerability of financial institutions. In this work, we\nshow that considering alternative topological measures based on local\nsub-network environment improves our ability to identify systemic institutions.\nTo provide empirical evidence, we apply a two-step procedure. First, we recover\nnetwork communities (i.e. close-peer environment) on a spillover network of\nfinancial institutions. Second, we regress alternative measures of\nvulnerability on three levels of topological measures: the global level (i.e.\nfirm topological characteristics computed over the whole system), local level\n(i.e. firm topological characteristics computed over the community) and\naggregated level by averaging individual characteristics over the community.\nThe sample includes $46$ financial institutions (banks, broker-dealers,\ninsurance and real-estate companies) listed in the Standard \\& Poor's 500\nindex. Our results confirm the informational content of topological metrics\nbased on close-peer environment. Such information is different from the one\nembeds in traditional system wide topological metrics and is proved to be\npredictor of distress for financial institutions in time of crisis.\n"
    },
    {
        "paper_id": 1910.08627,
        "authors": "Carlo Requi\\~ao da Cunha and Roberto da Silva",
        "title": "On the quantum behavior and clustering properties of correlated\n  financial portfolios",
        "comments": "13 pages, 6 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  We investigate 17 digital currencies making an analogy with quantum systems\nand develop the concept of eigenportfolios. We show that the density of states\nof the correlation matrix of these assets shows a behavior between that of the\nWishart ensemble and one whose elements are Cauchy distributed. A metric for\nthe participation matrix based on superposition of Gaussian functions is\nproposed and we show that small eigenvalues correspond to localized states.\nNonetheless, some level of localization is also present for bigger eigenvalues\nprobably caused by the fat tails of the distribution of returns of these\nassets. We also show through a clustering study that the digital currencies\ntend to stagger together. We conclude the paper showing that this correlation\nstructure leads to an Epps effect.\n"
    },
    {
        "paper_id": 1910.08628,
        "authors": "Jeremy Turiel and Tomaso Aste",
        "title": "Sector Neutral Portfolios: Long memory motifs persistence in market\n  structure dynamics",
        "comments": "14 pages, 4 figures",
        "journal-ref": "In: Cherifi H., Gaito S., Mendes J., Moro E., Rocha L. (eds)\n  Complex Networks and Their Applications VIII. COMPLEX NETWORKS 2019. Studies\n  in Computational Intelligence, vol 882. Springer, Cham",
        "doi": "10.1007/978-3-030-36683-4_46",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We study soft persistence (existence in subsequent temporal layers of motifs\nfrom the initial layer) of motif structures in Triangulated Maximally Filtered\nGraphs (TMFG) generated from time-varying Kendall correlation matrices computed\nfrom stock prices log-returns over rolling windows with exponential smoothing.\nWe observe long-memory processes in these structures in the form of power law\ndecays in the number of persistent motifs. The decays then transition to a\nplateau regime with a power-law decay with smaller exponent. We demonstrate\nthat identifying persistent motifs allows for forecasting and applications to\nportfolio diversification. Balanced portfolios are often constructed from the\nanalysis of historic correlations, however not all past correlations are\npersistently reflected into the future. Sector neutrality has also been a\ncentral theme in portfolio diversification and systemic risk. We present an\nunsupervised technique to identify persistently correlated sets of stocks.\nThese are empirically found to identify sectors driven by strong fundamentals.\nApplications of these findings are tested in two distinct ways on four\ndifferent markets, resulting in significant reduction in portfolio volatility.\nA persistence-based measure for portfolio allocation is proposed and shown to\noutperform volatility weighting when tested out of sample.\n"
    },
    {
        "paper_id": 1910.08641,
        "authors": "Juan Dong, Lyudmila Korobenko, Deniz Sezer",
        "title": "Nonhedgeable risk and Credit Risk Pricing",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We introduce a new model for pricing corporate bonds, which is a modification\nof the classical model of Merton. In this new model, we drop the liquidity\nassumption of the firm's asset value process, and assume that there is a\nliquidly traded asset in the market whose value is correlated with the firm's\nasset value, and all portfolios can be constructed using solely this asset and\nthe money market account. We formulate the market price of the corporate bond\nas the product of the price of an optimal replicating portfolio and exp(- kappa\nx replication error), where kappa is a positive constant. The interpretation is\nthat the representative investor accepts the price of the optimal replicating\nportfolio as a benchmark, however, requests compensation for the non-hedgeable\nrisk. We show that if the replication error is measured relative to the firm's\nvalue, the resulting formula is arbitrage free with mild restrictions on the\nparameters.\n"
    },
    {
        "paper_id": 1910.08858,
        "authors": "Sathya Ramesh, Ragib Mostofa, Marco Bornstein, John Dobelman",
        "title": "Beating the House: Identifying Inefficiencies in Sports Betting Markets",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Inefficient markets allow investors to consistently outperform the market. To\ndemonstrate that inefficiencies exist in sports betting markets, we created a\nbetting algorithm that generates above market returns for the NFL, NBA, NCAAF,\nNCAAB, and WNBA betting markets. To formulate our betting strategy, we\ncollected and examined a novel dataset of bets, and created a non-parametric\nwin probability model to find positive expected value situations. As the United\nStates Supreme Court has recently repealed the federal ban on sports betting,\nresearch on sports betting markets is increasingly relevant for the growing\nsports betting industry.\n"
    },
    {
        "paper_id": 1910.08946,
        "authors": "Frank Bosserhoff, Mitja Stadje",
        "title": "Robustness of Delta Hedging in a Jump-Diffusion Model",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Suppose an investor aims at Delta hedging a European contingent claim\n$h(S(T))$ in a jump-diffusion model, but incorrectly specifies the stock\nprice's volatility and jump sensitivity, so that any hedging strategy is\ncalculated under a misspecified model. When does the erroneously computed\nstrategy super-replicate the true claim in an appropriate sense? If the\nmisspecified volatility and jump sensitivity dominate the true ones, we show\nthat following the misspecified Delta strategy does super-replicate $h(S(T))$\nin expectation among a wide collection of models. We also show that if a robust\npricing operator with a whole class of models is used, the corresponding hedge\nis dominating the contingent claim under each model in expectation. Our results\nrely on proving stochastic flow properties of the jump-diffusion and the\nconvexity of the value function. In the pure Poisson case, we establish that an\noverestimation of the jump sensitivity results in an almost sure one-sided\nhedge. Moreover, in general the misspecified price of the option dominates the\ntrue one if the volatility and the jump sensitivity are overestimated.\n"
    },
    {
        "paper_id": 1910.09132,
        "authors": "Yiju Ma, Kevin Swandi, Archie Chapman and Gregor Verbic",
        "title": "Multi-Stage Compound Real Options Valuation in Residential PV-Battery\n  Investment",
        "comments": "10 pages, 7 figures, submitted to Elsevier Energy",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Strategic valuation of efficient and well-timed network investments under\nuncertain electricity market environment has become increasingly challenging,\nbecause there generally exist multiple interacting options in these\ninvestments, and failing to systematically consider these options can lead to\ndecisions that undervalue the investment. In our work, a real options valuation\n(ROV) framework is proposed to determine the optimal strategy for executing\nmultiple interacting options within a distribution network investment, to\nmitigate the risk of financial losses in the presence of future uncertainties.\nTo demonstrate the characteristics of the proposed framework, we determine the\noptimal strategy to economically justify the investment in residential\nPV-battery systems for additional grid supply during peak demand periods. The\noptions to defer, and then expand, are considered as multi-stage compound\noptions, since the option to expand is a subsequent option of the former. These\noptions are valued via the least squares Monte Carlo method, incorporating\nuncertainty over growing power demand, varying diesel fuel price, and the\ndeclining cost of PV-battery technology as random variables. Finally, a\nsensitivity analysis is performed to demonstrate how the proposed framework\nresponds to uncertain events. The proposed framework shows that executing the\ninteracting options at the optimal timing increases the investment value.\n"
    },
    {
        "paper_id": 1910.09153,
        "authors": "Lu Bai, Lixin Cui, Lixiang Xu, Yue Wang, Zhihong Zhang, Edwin R.\n  Hancock",
        "title": "Entropic Dynamic Time Warping Kernels for Co-evolving Financial Time\n  Series Analysis",
        "comments": "Previously, the original version of this manuscript appeared as\n  arXiv:1902.09947v2, that was submitted as a replacement by a mistake. Now,\n  that article has been replaced to correct the error, and this manuscript is\n  distinct from that article",
        "journal-ref": "IEEE Transactions on Neural Networks and Learning Systems, 2020",
        "doi": "10.1109/TNNLS.2020.3006738",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this work, we develop a novel framework to measure the similarity between\ndynamic financial networks, i.e., time-varying financial networks.\nParticularly, we explore whether the proposed similarity measure can be\nemployed to understand the structural evolution of the financial networks with\ntime. For a set of time-varying financial networks with each vertex\nrepresenting the individual time series of a different stock and each edge\nbetween a pair of time series representing the absolute value of their Pearson\ncorrelation, our start point is to compute the commute time matrix associated\nwith the weighted adjacency matrix of the network structures, where each\nelement of the matrix can be seen as the enhanced correlation value between\npairwise stocks. For each network, we show how the commute time matrix allows\nus to identify a reliable set of dominant correlated time series as well as an\nassociated dominant probability distribution of the stock belonging to this\nset. Furthermore, we represent each original network as a discrete dominant\nShannon entropy time series computed from the dominant probability\ndistribution. With the dominant entropy time series for each pair of financial\nnetworks to hand, we develop a similarity measure based on the classical\ndynamic time warping framework, for analyzing the financial time-varying\nnetworks. We show that the proposed similarity measure is positive definite and\nthus corresponds to a kernel measure on graphs. The proposed kernel bridges the\ngap between graph kernels and the classical dynamic time warping framework for\nmultiple financial time series analysis. Experiments on time-varying networks\nextracted through New York Stock Exchange (NYSE) database demonstrate the\neffectiveness of the proposed approach.\n"
    },
    {
        "paper_id": 1910.09202,
        "authors": "Jan Rosenzweig",
        "title": "Conservation Laws in a Limit Order Book",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  We present a class of macroscopic models of the Limit Order Book to simulate\nthe aggregate behaviour of market makers in response to trading flows. The\nresulting models are solved numerically and asymptotically, and a class of\nsimilarity solutions linked to order book formation and recovery is explored.\nThe main result is that order book recovery from aggressive liquidity taking\nfollows a simple $t^{1/3}$ scaling law.\n"
    },
    {
        "paper_id": 1910.09504,
        "authors": "Gautier Marti",
        "title": "CorrGAN: Sampling Realistic Financial Correlation Matrices Using\n  Generative Adversarial Networks",
        "comments": null,
        "journal-ref": "ICASSP 2020 - 2020 IEEE International Conference on Acoustics,\n  Speech and Signal Processing (ICASSP)",
        "doi": "10.1109/ICASSP40776.2020.9053276",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We propose a novel approach for sampling realistic financial correlation\nmatrices. This approach is based on generative adversarial networks.\nExperiments demonstrate that generative adversarial networks are able to\nrecover most of the known stylized facts about empirical correlation matrices\nestimated on asset returns. This is the first time such results are documented\nin the literature. Practical financial applications range from trading\nstrategies enhancement to risk and portfolio stress testing. Such generative\nmodels can also help ground empirical finance deeper into science by allowing\nfor falsifiability of statements and more objective comparison of empirical\nmethods.\n"
    },
    {
        "paper_id": 1910.09544,
        "authors": "Daniel Muller, Tshilidzi Marwala",
        "title": "Relative Net Utility and the Saint Petersburg Paradox",
        "comments": "extension of the discussion about the paradox, additional examples,\n  and proofreading",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The famous Saint Petersburg Paradox (St. Petersburg Paradox) shows that the\ntheory of expected value does not capture the real-world economics of\ndecision-making problems. Over the years, many economic theories were developed\nto resolve the paradox and explain gaps in the economic value theory in the\nevaluation of economic decisions, the subjective utility of the expected\noutcomes, and risk aversion as observed in the game of the St. Petersburg\nParadox. In this paper, we use the concept of the relative net utility to\nresolve the St. Petersburg Paradox. Because the net utility concept is able to\nexplain both behavioral economics and the St. Petersburg Paradox, it is deemed\nto be a universal approach to handling utility. This paper shows how the\ninformation content of the notion of net utility value allows us to capture a\nbroader context of the impact of a decision's possible achievements. It\ndiscusses the necessary conditions that the utility function has to conform to\navoid the paradox. Combining these necessary conditions allows us to define the\ntheorem of indifference in the evaluation of economic decisions and to present\nthe role of the relative net utility and net utility polarity in a value\nrational decision-making process.\n"
    },
    {
        "paper_id": 1910.09834,
        "authors": "Yanfei Bai, Zhongbao Zhou, Helu Xiao, Rui Gao, Feimin Zhong",
        "title": "A hybrid stochastic differential reinsurance and investment game with\n  bounded memory",
        "comments": "35 pages, 9 figures, 9 tables",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper investigates a hybrid stochastic differential reinsurance and\ninvestment game between one reinsurer and two insurers, including a stochastic\nStackelberg differential subgame and a non-zero-sum stochastic differential\nsubgame. The reinsurer, as the leader of the Stackelberg game, can price\nreinsurance premium and invest its wealth in a financial market that contains a\nrisk-free asset and a risky asset. The two insurers, as the followers of the\nStackelberg game, can purchase proportional reinsurance from the reinsurer and\ninvest in the same financial market. The competitive relationship between two\ninsurers is modeled by the non-zero-sum game, and their decision making will\nconsider the relative performance measured by the difference in their terminal\nwealth. We consider wealth processes with delay to characterize the bounded\nmemory feature. This paper aims to find the equilibrium strategy for the\nreinsurer and insurers by maximizing the expected utility of the reinsurer's\nterminal wealth with delay and maximizing the expected utility of the\ncombination of insurers' terminal wealth and the relative performance with\ndelay. By using the idea of backward induction and the dynamic programming\napproach, we derive the equilibrium strategy and value functions explicitly.\nThen, we provide the corresponding verification theorem. Finally, some\nnumerical examples and sensitivity analysis are presented to demonstrate the\neffects of model parameters on the equilibrium strategy. We find the delay\nfactor discourages or stimulates investment depending on the length of delay.\nMoreover, competitive factors between two insurers make their optimal\nreinsurance-investment strategy interact, and reduce reinsurance demand and\nreinsurance premium price.\n"
    },
    {
        "paper_id": 1910.09855,
        "authors": "Yan Dolinsky and Jonathan Zouari",
        "title": "The Value of Insider Information for Super--Replication with Quadratic\n  Transaction Costs",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We study super--replication of European contingent claims in an illiquid\nmarket with insider information. Illiquidity is captured by quadratic\ntransaction costs and insider information is modeled by an investor who can\npeek into the future. Our main result describes the scaling limit of the\nsuper--replication prices when the number of trading periods increases to\ninfinity. Moreover, the scaling limit gives us the asymptotic value of being an\ninsider.\n"
    },
    {
        "paper_id": 1910.09947,
        "authors": "Daniel Snashall and Dave Cliff",
        "title": "Adaptive-Aggressive Traders Don't Dominate",
        "comments": "To be published as a chapter in \"Agents and Artificial Intelligence\"\n  edited by Jaap van den Herik, Ana Paula Rocha, and Luc Steels; forthcoming\n  2019/2020. 24 Pages, 1 Figure, 7 Tables",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  For more than a decade Vytelingum's Adaptive-Aggressive (AA) algorithm has\nbeen recognized as the best-performing automated auction-market trading-agent\nstrategy currently known in the AI/Agents literature; in this paper, we\ndemonstrate that it is in fact routinely outperformed by another algorithm when\nexhaustively tested across a sufficiently wide range of market scenarios. The\nnovel step taken here is to use large-scale compute facilities to brute-force\nexhaustively evaluate AA in a variety of market environments based on those\nused for testing it in the original publications. Our results show that even in\nthese simple environments AA is consistently out-performed by IBM's GDX\nalgorithm, first published in 2002. We summarize here results from more than\none million market simulation experiments, orders of magnitude more testing\nthan was reported in the original publications that first introduced AA. A 2019\nICAART paper by Cliff claimed that AA's failings were revealed by testing it in\nmore realistic experiments, with conditions closer to those found in real\nfinancial markets, but here we demonstrate that even in the simple experiment\nconditions that were used in the original AA papers, exhaustive testing shows\nAA to be outperformed by GDX. We close this paper with a discussion of the\nmethodological implications of our work: any results from previous papers where\nany one trading algorithm is claimed to be superior to others on the basis of\nonly a few thousand trials are probably best treated with some suspicion now.\nThe rise of cloud computing means that the compute-power necessary to subject\ntrading algorithms to millions of trials over a wide range of conditions is\nreadily available at reasonable cost: we should make use of this; exhaustive\ntesting such as is shown here should be the norm in future evaluations and\ncomparisons of new trading algorithms.\n"
    },
    {
        "paper_id": 1910.09978,
        "authors": "Christoph Bandt",
        "title": "Order patterns, their variation and change points in financial time\n  series and Brownian motion",
        "comments": null,
        "journal-ref": "Statistical Papers 61 (4), 1565-1588 (2020)",
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Order patterns and permutation entropy have become useful tools for studying\nbiomedical, geophysical or climate time series. Here we study day-to-day market\ndata, and Brownian motion which is a good model for their order patterns. A\ncrucial point is that for small lags (1 up to 6 days), pattern frequencies in\nfinancial data remain essentially constant. The two most important order\nparameters of a time series are turning rate and up-down balance. For change\npoints in EEG brain data, turning rate is excellent while for financial data,\nup-down balance seems the best. The fit of Brownian motion with respect to\nthese parameters is tested, providing a new version of a forgotten test by\nBienaym'e.\n"
    },
    {
        "paper_id": 1910.10005,
        "authors": "Bernard De Meyer and Moussa Dabo",
        "title": "The CMMV Pricing Model in Practice",
        "comments": "19 pages, 6 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Mainstream financial econometrics methods are based on models well tuned to\nreplicate price dynamics, but with little to no economic justification. In\nparticular, the randomness in these models is assumed to result from a\ncombination of exogenous factors. In this paper, we present a model originating\nfrom game theory, whose corresponding price dynamics are a direct consequence\nof the information asymmetry between private and institutional investors. This\nmodel, namely the CMMV pricing model, is therefore rooted in market\nmicrostructure. The pricing methods derived from it also appear to fit very\nwell historical price data. Indeed, as evidenced in the last section of the\npaper, the CMMV model does a very good job predicting option prices from\nreadily available data. It also enables to recover the dynamic of the\nvolatility surface.\n"
    },
    {
        "paper_id": 1910.10098,
        "authors": "Amani Moin and Emin G\\\"un Sirer and Kevin Sekniqi",
        "title": "A Classification Framework for Stablecoin Designs",
        "comments": "24 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Stablecoins promise to bridge fiat currencies with the world of\ncryptocurrencies. They provide a way for users to take advantage of the\nbenefits of digital currencies, such as ability to transfer assets over the\ninternet, provide assurance on minting schedules and scarcity, and enable new\nasset classes, while also partially mitigating their volatility risks. In this\npaper, we systematically discuss general design, decompose existing stablecoins\ninto various component design elements, explore their strengths and drawbacks,\nand identify future directions.\n"
    },
    {
        "paper_id": 1910.10099,
        "authors": "J. Lussange, S. Palminteri, S. Bourgeois-Gironde, B. Gutkin",
        "title": "Mesoscale impact of trader psychology on stock markets: a multi-agent AI\n  approach",
        "comments": "9 pages, 15 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Recent advances in the fields of machine learning and neurofinance have\nyielded new exciting research perspectives in practical inference of\nbehavioural economy in financial markets and microstructure study. We here\npresent the latest results from a recently published stock market simulator\nbuilt around a multi-agent system architecture, in which each agent is an\nautonomous investor trading stocks by reinforcement learning (RL) via a\ncentralised double-auction limit order book. The RL framework allows for the\nimplementation of specific behavioural and cognitive traits known to trader\npsychology, and thus to study the impact of these traits on the whole stock\nmarket at the mesoscale. More precisely, we narrowed our agent design to three\nsuch psychological biases known to have a direct correspondence with RL theory,\nnamely delay discounting, greed, and fear. We compared ensuing simulated data\nto real stock market data over the past decade or so, and find that market\nstability benefits from larger populations of agents prone to delay discounting\nand most astonishingly, to greed.\n"
    },
    {
        "paper_id": 1910.10606,
        "authors": "Milan Kumar Das and Anindya Goswami and Sharan Rajani",
        "title": "Inference of Binary Regime Models with Jump Discontinuities",
        "comments": "22 pages, 7 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Identifying the instances of jumps in a discrete-time-series sample of a jump\ndiffusion model is a challenging task. We have developed a novel statistical\ntechnique for jump detection and volatility estimation in a return time series\ndata using a threshold method. The consistency of the volatility estimator has\nbeen obtained. Since we have derived the threshold and the volatility estimator\nsimultaneously by solving an implicit equation, we have obtained unprecedented\naccuracy across a wide range of parameter values. Using this method, the\nincrements attributed to jumps have been removed from a large collection of\nhistorical data of Indian sectorial indices. Subsequently, we have tested the\npresence of regime-switching dynamics in the volatility coefficient using a new\ndiscriminating statistic. The statistic has been shown to be sensitive to the\ntransition kernel of the regime-switching model. We perform the testing using\nthe Bootstrap method and find a clear indication of presence of multiple\nregimes of volatility in the data. A link to all Python codes is given in the\nconclusion. The methodology is suitable for analyzing high-frequency data and\nmay be applied for algorithmic trading.\n"
    },
    {
        "paper_id": 1910.10673,
        "authors": "Mariola Ndrio, Anna Winnicki and Subhonmesh Bose",
        "title": "Pricing Economic Dispatch with AC Power Flow via Local Multipliers and\n  Conic Relaxation",
        "comments": "18 pages, 3 figures and 1 table. Journal paper",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We analyze pricing mechanisms in electricity markets with AC power flow\nequations that define a nonconvex feasible set for the economic dispatch\nproblem. Specifically, we consider two possible pricing schemes. The first\namong these prices are derived from Lagrange multipliers that satisfy\nKarush-Kuhn-Tucker conditions for local optimality of the nonconvex market\nclearing problem. The second is derived from optimal dual multipliers of the\nconvex semidefinite programming (SDP) based relaxation of the market clearing\nproblem. Relationships between these prices, their revenue adequacy and market\nequilibrium properties are derived and compared. The SDP prices are shown to\nequal distribution locational marginal prices derived with second-order conic\nrelaxations of power flow equations over radial distribution networks. We\nillustrate our theoretical findings through numerical experiments.\n"
    },
    {
        "paper_id": 1910.11216,
        "authors": "Marius Zoican and Sorin Zoican",
        "title": "Fragmentation of Distributed Exchanges",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Distributed securities exchanges may become de facto fragmented if they span\ngeographical regions with asymmetric computer infrastructure. First, we build\nan economic model of a decentralized exchange with two miner clusters, standing\nin for compact areas of economic activity (e.g., cities). \"Local\" miners in the\narea with relatively higher trading activity only join a decentralized exchange\nif they enjoy a large speed advantage over \"long-distance\" competitors. This is\ndue to a transfer of economic value across miners, specifically from high- to\nlow-activity clusters. Second, we estimate the speed advantage of \"local\" over\n\"long-distance\" miners in a series of Monte Carlo experiments over a\ntwo-cluster, unstructured peer-to-peer network simulated in C. We find that the\nspeed advantage increases in the level of infrastructure asymmetry between\nclusters. Cross-region DEX blockchains are feasible as long as the asymmetry\nlevels in trading activity and infrastructure availability across regions are\npositively correlated.\n"
    },
    {
        "paper_id": 1910.11337,
        "authors": "V\\'itor V. Vasconcelos, Phillip M. Hannam, Simon A. Levin, Jorge M.\n  Pacheco",
        "title": "Coalition-structured governance improves cooperation to provide public\n  goods",
        "comments": "16 pages (2 figures) plus supplementary material (1 figure)",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  While the benefits of common and public goods are shared, they tend to be\nscarce when contributions are provided voluntarily. Failure to cooperate in the\nprovision or preservation of these goods is fundamental to sustainability\nchallenges, ranging from local fisheries to global climate change. In the real\nworld, such cooperative dilemmas occur in multiple interactions with complex\nstrategic interests and frequently without full information. We argue that\nvoluntary cooperation enabled across multiple coalitions (akin to\npolycentricity) not only facilitates greater generation of non-excludable\npublic goods, but may also allow evolution toward a more cooperative, stable,\nand inclusive approach to governance. Contrary to any previous study, we show\nthat these merits of multi-coalition governance are far more general than the\nsingular examples occurring in the literature, and are robust under diverse\nconditions of excludability, congestability of the non-excludable public good,\nand arbitrary shapes of the return-to-contribution function. We first confirm\nthe intuition that a single coalition without enforcement and with players\npursuing their self-interest without knowledge of returns to contribution is\nprone to cooperative failure. Next, we demonstrate that the same pessimistic\nmodel but with a multi-coalition structure of governance experiences relatively\nhigher cooperation by enabling recognition of marginal gains of cooperation in\nthe game at stake. In the absence of enforcement, public-goods regimes that\nevolve through a proliferation of voluntary cooperative forums can maintain and\nincrease cooperation more successfully than singular, inclusive regimes.\n"
    },
    {
        "paper_id": 1910.11405,
        "authors": "Lin Hu, Anqi Li, and Ilya Segal",
        "title": "The Politics of Personalized News Aggregation",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We study how personalized news aggregation for rationally inattentive voters\n(NARI) affects policy polarization and public opinion. In a two-candidate\nelectoral competition model, an attention-maximizing infomediary aggregates\nsource data about candidates' valence into easy-to-digest news. Voters decide\nwhether to consume news, trading off the expected gain from improved expressive\nvoting against the attention cost. NARI generates policy polarization even if\ncandidates are office-motivated. Personalized news aggregation makes extreme\nvoters the disciplining entity of policy polarization, and the skewness of\ntheir signals is crucial for sustaining a high degree of policy polarization in\nequilibrium. Analysis of disciplining voters yields insights into the\nequilibrium and welfare consequences of regulating infomediaries.\n"
    },
    {
        "paper_id": 1910.1157,
        "authors": "Levon Amatuni, Juudit Ottelin, Bernhard Steubing, Jos\\'e Mogollon",
        "title": "Does car sharing reduce greenhouse gas emissions? Life cycle assessment\n  of the modal shift and lifetime shift rebound effects",
        "comments": "10 pages, 4 figures (in the end of the file)",
        "journal-ref": "Journal of Cleaner Production Volume 266, 1 September 2020, 121869",
        "doi": "10.1016/j.jclepro.2020.121869",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Car-sharing platforms provide access to a shared rather than a private fleet\nof automobiles distributed in the region. Participation in such services\ninduces changes in mobility behaviour as well as vehicle ownership patterns\nthat could have positive environmental impacts. This study contributes to the\nunderstanding of the total mobility-related greenhouse gas emissions reduction\nrelated to business-to-consumer car-sharing participation. A comprehensive\nmodel which takes into account distances travelled annually by the major urban\ntransport modes as well as their life-cycle emissions factors is proposed, and\nthe before-and-after analysis is conducted for an average car-sharing member in\nthree geographical cases (Netherlands, San Francisco, Calgary). In addition to\nnon-operational emissions for all the transport modes involved, this approach\nconsiders the rebound effects associated with the modal shift effect\n(substituting driving distances with alternative modes) and the lifetime shift\neffect for the shared automobiles, phenomena which have been barely analysed in\nthe previous studies. As a result, in contrast to the previous impact\nassessments in the field, a significantly more modest reduction of the annual\ntotal mobility-related life-cycle greenhouse gas emissions caused by\ncar-sharing participation has been estimated, 3-18% for three geographical case\nstudies investigated (versus up to 67% estimated previously). This suggests the\nsignificance of the newly considered effects and provides with the practical\nimplications for improved assessments in the future.\n"
    },
    {
        "paper_id": 1910.1178,
        "authors": "Bayram Cakir and Ipek Ergul",
        "title": "Inequality in Turkey: Looking Beyond Growth",
        "comments": "33 Pages, 9 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper investigates the relationships between economic growth, investment\nin human capital and income equality in Turkey. The conclusion drawn based on\nthe data from the OECD and the World Bank suggests that economic growth can\nimprove income equality depending on the expenditures undertaken by the\ngovernment. As opposed to the standard view that economic growth and income\ninequality are positively related, the findings of this paper suggest that\nother factors such as education and healthcare spending are also driving\nfactors of income inequality in Turkey. The proven positive impact of\ninvestment in education and health care on income equality could aid\npolicymakers who aim to achieve fairer income equality and economic growth, in\ninvestment decisions.\n"
    },
    {
        "paper_id": 1910.1184,
        "authors": "Sven Husmann, Antoniya Shivarova, Rick Steinert",
        "title": "Sparsity and Stability for Minimum-Variance Portfolios",
        "comments": "Minimum-Variance Portfolio, LASSO, Turnover constraint, Out-of-sample\n  variance, Asset selection, Short-sale budget",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The popularity of modern portfolio theory has decreased among practitioners\nbecause of its unfavorable out-of-sample performance. Estimation errors tend to\naffect the optimal weight calculation noticeably, especially when a large\nnumber of assets is considered. To overcome these issues, many methods have\nbeen proposed in recent years, although most only address a small set of\npractically relevant questions related to portfolio allocation. This study\ntherefore sheds light on different covariance estimation techniques, combines\nthem with sparse model approaches, and includes a turnover constraint that\ninduces stability. We use two datasets - comprising 319 and 100 companies of\nthe S&P 500, respectively - to create a realistic and reproducible data\nfoundation for our empirical study. To the best of our knowledge, this study is\nthe first to show that it is possible to maintain the low-risk profile of\nefficient estimation methods while automatically selecting only a subset of\nassets and further inducing low portfolio turnover. Moreover, we provide\nevidence that using the LASSO as the sparsity-generating model is insufficient\nto lower turnover when the involved tuning parameter can change over time.\n"
    },
    {
        "paper_id": 1910.11904,
        "authors": "Sascha Desmettre, Gunther Leobacher, L.C.G. Rogers",
        "title": "Change of drift in one-dimensional diffusions",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  It is generally understood that a given one-dimensional diffusion may be\ntransformed by Cameron-Martin-Girsanov measure change into another\none-dimensional diffusion with the same volatility but a different drift. But\nto achieve this we have to know that the change-of-measure local martingale\nthat we write down is a true martingale; we provide a complete characterization\nof when this happens. This is then used to discuss absence of arbitrage in a\ngeneralized Heston model including the case where the Feller condition for the\nvolatility process is violated.\n"
    },
    {
        "paper_id": 1910.1213,
        "authors": "Tathagata Banerjee and Zachary Feinstein",
        "title": "Price mediated contagion through capital ratio requirements with VWAP\n  liquidation prices",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We develop a framework for price-mediated contagion in financial systems\nwhere banks are forced to liquidate assets to satisfy a risk-weight based\ncapital adequacy requirement. In constructing this modeling framework, we\nintroduce a two-tier pricing structure: the volume weighted average price that\nis obtained by any bank liquidating assets and the terminal mark-to-market\nprice used to account for all assets held at the end of the clearing process.\nWe consider the case of multiple illiquid assets and develop conditions for the\nexistence and uniqueness of clearing prices. We provide a closed-form\nrepresentation for the sensitivity of these clearing prices to the system\nparameters, and use this result to quantify: (1) the cost of regulation, in\nstress scenarios, faced by the system as a whole and the individual banks, and\n(2) the value of providing bailouts to consider when such notions are\nfinancially advisable. Numerical case studies are provided to study the\napplication of this model to data.\n"
    },
    {
        "paper_id": 1910.12281,
        "authors": "Vladimir Puzyrev",
        "title": "Deep convolutional autoencoder for cryptocurrency market analysis",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This study attempts to analyze patterns in cryptocurrency markets using a\nspecial type of deep neural networks, namely a convolutional autoencoder. The\nmethod extracts the dominant features of market behavior and classifies the 40\nstudied cryptocurrencies into several classes for twelve 6-month periods\nstarting from 15th May 2013. Transitions from one class to another with time\nare related to the maturement of cryptocurrencies. In speculative\ncryptocurrency markets, these findings have potential implications for\ninvestment and trading strategies.\n"
    },
    {
        "paper_id": 1910.12498,
        "authors": "Emily P. Harvey, Dion R.J. O'Neale",
        "title": "Using network science to quantify economic disruptions in regional\n  input-output networks",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Input Output (IO) tables provide a standardised way of looking at monetary\nflows between all industries in an economy. IO tables can be thought of as\nnetworks - with the nodes being different industries and the edges being the\nflows between them. We develop a network-based analysis to consider a\nmulti-regional IO network at regional and subregional level within a country.\nWe calculate both traditional matrix-based IO measures (e.g. 'multipliers') and\nnew network theory-based measures at this higher spatial resolution. We\ncontrast these methods with the results of a disruption model applied to the\nsame IO data in order to demonstrate that betweenness centrality gives a good\nindication of flow on economic disruption, while eigenvector-type centrality\nmeasures give results comparable to traditional IO multipliers.We also show the\neffects of treating IO networks at different levels of spatial aggregation.\n"
    },
    {
        "paper_id": 1910.12516,
        "authors": "Julio Backhoff-Veraguas, Patrick Beissner, Ulrich Horst",
        "title": "Robust Contracting in General Contract Spaces",
        "comments": "Literature review has been updated and expanded",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We consider a general framework of optimal mechanism design under adverse\nselection and ambiguity about the type distribution of agents. We prove the\nexistence of optimal mechanisms under minimal assumptions on the contract space\nand prove that centralized contracting implemented via mechanisms is equivalent\nto delegated contracting implemented via a contract menu under these\nassumptions. Our abstract existence results are applied to a series of\napplications that include models of optimal risk sharing and of optimal\nportfolio delegation.\n"
    },
    {
        "paper_id": 1910.12545,
        "authors": "Timo Dimitriadis, Andrew J. Patton, Patrick W. Schmidt",
        "title": "Testing Forecast Rationality for Measures of Central Tendency",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Rational respondents to economic surveys may report as a point forecast any\nmeasure of the central tendency of their (possibly latent) predictive\ndistribution, for example the mean, median, mode, or any convex combination\nthereof. We propose tests of forecast rationality when the measure of central\ntendency used by the respondent is unknown. We overcome an identification\nproblem that arises when the measures of central tendency are equal or in a\nlocal neighborhood of each other, as is the case for (exactly or nearly)\nsymmetric distributions. As a building block, we also present novel tests for\nthe rationality of mode forecasts. We apply our tests to income forecasts from\nthe Federal Reserve Bank of New York's Survey of Consumer Expectations. We find\nthese forecasts are rationalizable as mode forecasts, but not as mean or median\nforecasts. We also find heterogeneity in the measure of centrality used by\nrespondents when stratifying the sample by past income, age, job stability, and\nsurvey experience.\n"
    },
    {
        "paper_id": 1910.12692,
        "authors": "Jonas Crevecoeur and Jens Robben and Katrien Antonio",
        "title": "A hierarchical reserving model for reported non-life insurance claims",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Traditional non-life reserving models largely neglect the vast amount of\ninformation collected over the lifetime of a claim. This information includes\ncovariates describing the policy, claim cause as well as the detailed history\ncollected during a claim's development over time. We present the hierarchical\nreserving model as a modular framework for integrating a claim's history and\nclaim-specific covariates into the development process. Hierarchical reserving\nmodels decompose the joint likelihood of the development process over time.\nMoreover, they are tailored to the portfolio at hand by adding a layer to the\nmodel for each of the events registered during the development of a claim (e.g.\nsettlement, payment). Layers are modelled with statistical learning (e.g.\ngeneralized linear models) or machine learning methods (e.g. gradient boosting\nmachines) and use claim-specific covariates. As a result of its flexibility,\nthis framework incorporates many existing reserving models, ranging from\naggregate models designed for run-off triangles to individual models using\nclaim-specific covariates. This connection allows us to develop a data-driven\nstrategy for choosing between aggregate and individual reserving; an important\ndecision for reserving practitioners. We illustrate our method with a case\nstudy on a real insurance data set and deduce new insights in the covariates\ndriving the development of claims. Moreover, we evaluate the method's\nperformance on a large number of simulated portfolios representing several\nrealistic development scenarios and demonstrate the flexibility and robustness\nof the hierarchical reserving model.\n"
    },
    {
        "paper_id": 1910.13115,
        "authors": "Huai-Long Shi, Wei-Xing Zhou",
        "title": "Horse race of weekly idiosyncratic momentum strategies with respect to\n  various risk metrics: Evidence from the Chinese stock market",
        "comments": "36 pages including 15 tables",
        "journal-ref": "The North American Journal of Economics and Finance, 58, 101478\n  (2021)",
        "doi": "10.1016/j.najef.2021.101478",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper focuses on the horse race of weekly idiosyncratic momentum (IMOM)\nwith respect to various idiosyncratic risk metrics. Using the A-share\nindividual stocks in the Chinese market from January 1997 to December 2017, we\nfirst evaluate the performance of the weekly momentum based on raw returns and\nidiosyncratic returns, respectively. After that the univariate portfolio\nanalysis is conducted to investigate the return predictability with respect to\nvarious idiosyncratic risk metrics. Further, we perform a comparative study on\nthe performance of the IMOM portfolios with respect to various risk metrics. At\nlast, we explore the possible explanations to IMOM as well as risk based IMOM\nportfolios. We find that 1) there are prevailing contrarian effect and IMOM\neffect for the whole sample; 2) the negative relations exist between most of\nthe idiosyncratic risk metrics and the cross-sectional stock returns, and\nbetter performance is linked to idiosyncratic volatility (IVol) and maximum\ndrawdowns (IMDs); 3) additionally, the IVol-based and IMD-based IMOM portfolios\nexhibit better explanatory power to the IMOM portfolios with respect to other\nrisk metrics; 4) finally, higher profitability of IMOM as well as IVol-based\nand IMD-based IMOM portfolios is found to be related to upside market states,\nhigh levels of liquidity and high levels of investor sentiment.\n"
    },
    {
        "paper_id": 1910.13205,
        "authors": "Olivier Gu\\'eant, Iuliia Manziuk",
        "title": "Deep reinforcement learning for market making in corporate bonds:\n  beating the curse of dimensionality",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In corporate bond markets, which are mainly OTC markets, market makers play a\ncentral role by providing bid and ask prices for a large number of bonds to\nasset managers from all around the globe. Determining the optimal bid and ask\nquotes that a market maker should set for a given universe of bonds is a\ncomplex task. Useful models exist, most of them inspired by that of Avellaneda\nand Stoikov. These models describe the complex optimization problem faced by\nmarket makers: proposing bid and ask prices in an optimal way for making money\nout of the difference between bid and ask prices while mitigating the market\nrisk associated with holding inventory. While most of the models only tackle\none-asset market making, they can often be generalized to a multi-asset\nframework. However, the problem of solving numerically the equations\ncharacterizing the optimal bid and ask quotes is seldom tackled in the\nliterature, especially in high dimension. In this paper, our goal is to propose\na numerical method for approximating the optimal bid and ask quotes over a\nlarge universe of bonds in a model \\`a la Avellaneda-Stoikov. Because we aim at\nconsidering a large universe of bonds, classical finite difference methods as\nthose discussed in the literature cannot be used and we present therefore a\ndiscrete-time method inspired by reinforcement learning techniques. More\nprecisely, the approach we propose is a model-based actor-critic-like algorithm\ninvolving deep neural networks.\n"
    },
    {
        "paper_id": 1910.13286,
        "authors": "Giorgia Callegaro and Andrea Mazzoran and Carlo Sgarra",
        "title": "A Self-Exciting Modelling Framework for Forward Prices in Power Markets",
        "comments": "27 pages, 7 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We propose and investigate two model classes for forward power price\ndynamics, based on continuous branching processes with immigration, and on\nHawkes processes with exponential kernel, respectively. The models proposed\nexhibit jumps clustering features. Models of this kind have been already\nproposed for the spot price dynamics, but the main purpose of the present work\nis to investigate the performances of such models in describing the forward\ndynamics. We adopt a Heath-Jarrow-Morton approach in order to capture the whole\nforward curve evolution. By examining daily data in the French power market, we\nperform a goodness-of-fit test and we present our conclusions about the\nadequacy of these models in describing the forward prices evolution.\n"
    },
    {
        "paper_id": 1910.13338,
        "authors": "Mehdi Tomas, Mathieu Rosenbaum",
        "title": "From microscopic price dynamics to multidimensional rough volatility\n  models",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Rough volatility is a well-established statistical stylised fact of financial\nassets. This property has lead to the design and analysis of various new rough\nstochastic volatility models. However, most of these developments have been\ncarried out in the mono-asset case. In this work, we show that some specific\nmultivariate rough volatility models arise naturally from microstructural\nproperties of the joint dynamics of asset prices. To do so, we use Hawkes\nprocesses to build microscopic models that reproduce accurately high frequency\ncross-asset interactions and investigate their long term scaling limits. We\nemphasize the relevance of our approach by providing insights on the role of\nmicroscopic features such as momentum and mean-reversion on the\nmultidimensional price formation process. We in particular recover classical\nproperties of high-dimensional stock correlation matrices.\n"
    },
    {
        "paper_id": 1910.13385,
        "authors": "Russell Golman and Aditi Jain and Sonica Saraf",
        "title": "Hipsters and the Cool: A Game Theoretic Analysis of Social Identity,\n  Trends and Fads",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Cultural trends and popularity cycles can be observed all around us, yet our\ntheories of social influence and identity expression do not explain what\nperpetuates these complex, often unpredictable social dynamics. We propose a\ntheory of social identity expression based on the opposing, but not mutually\nexclusive, motives to conform and to be unique among one's neighbors in a\nsocial network. We then model the social dynamics that arise from these\nmotives. We find that the dynamics typically enter random walks or stochastic\nlimit cycles rather than converging to a static equilibrium. We also prove that\nwithout social network structure or, alternatively, without the uniqueness\nmotive, reasonable adaptive dynamics would necessarily converge to equilibrium.\nThus, we show that nuanced psychological assumptions (recognizing preferences\nfor uniqueness along with conformity) and realistic social network structure\nare both necessary for explaining how complex, unpredictable cultural trends\nemerge.\n"
    },
    {
        "paper_id": 1910.13443,
        "authors": "Adam Safron",
        "title": "Multilevel evolutionary developmental optimization (MEDO): A theoretical\n  framework for understanding preferences and selection dynamics",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  What is motivation and how does it work? Where do goals come from and how do\nthey vary within and between species and individuals? Why do we prefer some\nthings over others? MEDO is a theoretical framework for understanding these\nquestions in abstract terms, as well as for generating and evaluating specific\nhypotheses that seek to explain goal-oriented behavior. MEDO views preferences\nas selective pressures influencing the likelihood of particular outcomes. With\nrespect to biological organisms, these patterns must compete and cooperate in\nshaping system evolution. To the extent that shaping processes are themselves\naltered by experience, this enables feedback relationships where histories of\nreward and punishment can impact future motivation. In this way, various biases\ncan undergo either amplification or attenuation, resulting in preferences and\nbehavioral orientations of varying degrees of inter-temporal and\ninter-situational stability. MEDO specifically models all shaping dynamics in\nterms of natural selection operating on multiple levels--genetic, neural, and\ncultural--and even considers aspects of development to themselves be\nevolutionary processes. Thus, MEDO reflects a kind of generalized Darwinism, in\nthat it assumes that natural selection provides a common principle for\nunderstanding the emergence of complexity within all dynamical systems in which\nreplication, variation, and selection occur. However, MEDO combines this\nevolutionary perspective with economic decision theory, which describes both\nthe preferences underlying individual choices, as well as the preferences\nunderlying choices made by engineers in designing optimized systems. In this\nway, MEDO uses economic decision theory to describe goal-oriented behaviors as\nwell as the interacting evolutionary optimization processes from which they\nemerge. (Please note: this manuscript was written and finalized in 2012.)\n"
    },
    {
        "paper_id": 1910.13534,
        "authors": "Martin Frank, Michael Herty, Torsten Trimborn",
        "title": "Microscopic Derivation of Mean Field Game Models",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Mean field game theory studies the behavior of a large number of interacting\nindividuals in a game theoretic setting and has received a lot of attention in\nthe past decade (Lasry and Lions, Japanese journal of mathematics, 2007). In\nthis work, we derive mean field game partial differential equation systems from\ndeterministic microscopic agent dynamics. The dynamics are given by a\nparticular class of ordinary differential equations, for which an optimal\nstrategy can be computed (Bressan, Milan Journal of Mathematics, 2011). We use\nthe concept of Nash equilibria and apply the dynamic programming principle to\nderive the mean field limit equations and we study the scaling behavior of the\nsystem as the number of agents tends to infinity and find several mean field\ngame limits. Especially we avoid in our derivation the notion of measure\nderivatives. Novel scales are motivated by an example of an agent-based\nfinancial market model.\n"
    },
    {
        "paper_id": 1910.13668,
        "authors": "Peter Baxendale, Ting-Kam Leonard Wong",
        "title": "Random concave functions",
        "comments": "42 pages, 8 figures. Substantially revised. To appear in The Annals\n  of Applied Probability",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Spaces of convex and concave functions appear naturally in theory and\napplications. For example, convex regression and log-concave density estimation\nare important topics in nonparametric statistics. In stochastic portfolio\ntheory, concave functions on the unit simplex measure the concentration of\ncapital, and their gradient maps define novel investment strategies. The\ngradient maps may also be regarded as optimal transport maps on the simplex. In\nthis paper we construct and study probability measures supported on spaces of\nconcave functions. These measures may serve as prior distributions in Bayesian\nstatistics and Cover's universal portfolio, and induce distribution-valued\nrandom variables via optimal transport. The random concave functions are\nconstructed on the unit simplex by taking a suitably scaled (mollified, or\nsoft) minimum of random hyperplanes. Depending on the regime of the parameters,\nwe show that as the number of hyperplanes tends to infinity there are several\npossible limiting behaviors. In particular, there is a transition from a\ndeterministic almost sure limit to a non-trivial limiting distribution that can\nbe characterized using convex duality and Poisson point processes.\n"
    },
    {
        "paper_id": 1910.13729,
        "authors": "Yan-Hong Yang, Ying-Hui Shao",
        "title": "Time-dependent lead-lag relationships between the VIX and VIX futures\n  markets",
        "comments": "8 pages including 4 figures",
        "journal-ref": null,
        "doi": "10.1016/j.najef.2020.101196",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We utilize the symmetric thermal optimal path (TOPS) method to examine the\ndynamic interaction patterns between the VIX and VIX futures markets. We\ndocument that the VIX dominates the VIX futures more in the first few years,\nespecially before the introduction of VIX options. We further observe that the\nTOPS paths show an alternate lead-lag relationship instead of a dominance\nbetween the VIX and VIX futures in most of the time periods. Meanwhile, we find\nthat the VIX futures have been increasingly more important in the price\ndiscovery since the launch of several VIX ETPs.\n"
    },
    {
        "paper_id": 1910.13803,
        "authors": "Marcel Ausloos",
        "title": "Rank-size law, financial inequality indices and gain concentrations by\n  cyclist teams. The case of a multiple stage bicycle race, like Tour de France",
        "comments": "33 pages, 3 Tables, 4 figures, 39 references; to be published in\n  Physica A Keywords : Professional cyclist multistage races, Tour de France,\n  Financial gains hierarchy, Financial indices, Rank-size Law",
        "journal-ref": null,
        "doi": "10.1016/j.physa.2019.123161",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This note examines financial distributions to competing teams at the end of\nthe most famous multiple stage professional (male) bicyclist race, TOUR DE\nFRANCE. A rank-size law (RSL) is calculated for the team financial gains. The\nRSL is found to be hyperbolic with a surprisingly simple decay exponent (about\nequal to -1). Yet, the financial gain distributions unexpectedly do not obey\nPareto principle of factor sparsity. Next, several (8) inequality indices are\nconsidered : the Entropy, the Hirschman-Herfindahl, Theil, Pietra-Hoover, Gini,\nRosenbluth indices, the Coefficient of Variation and the Concentration Index\nare calculated for outlining diversity measures. The connection between such\nindices and their concentration aspects meanings are presented as support of\nthe RSL findings. The results emphasize that the sum of skills and team\nstrategies are effectively contributing to the financial gains distributions.\nFrom theoretical and practical points of view, the findings suggest that one\nshould investigate other \"long multiple stage races\" and rewarding rules.\nIndeed, money prize rules coupling to stage difficulty might influence and\nmaybe enhance (or deteriorate) purely sportive aspects in group competitions.\nDue to the delay in the peer review process, the 2019 results can be examined.\nThey are discussed in an Appendix; the value of the exponent (-1.2) is pointed\nout to mainly originating from the so called \"king effect\"; the tail of the RSL\nrather looks like an exponential.\n"
    },
    {
        "paper_id": 1910.13882,
        "authors": "Ravi Kashyap",
        "title": "Michael Milken: The Junk Dealer",
        "comments": null,
        "journal-ref": null,
        "doi": "10.3905/jpe.2019.1.095",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We take a closer look at the life and legacy of Micheal Milken. We discuss\nwhy Michael Milken, also know as the Junk Bond King, was not just any other\nKing or run-of-the-mill Junk Dealer, but \"The Junk Dealer\". We find parallels\nbetween the three parts to any magic act and what Micheal Milken did, showing\nthat his accomplishments were nothing short of a miracle. His compensation at\nthat time captures to a certain extent the magnitude of the changes he brought\nabout, the eco-system he created for businesses to flourish, the impact he had\non the wider economy and also on the future growth and development of American\nIndustry. We emphasize two of his contributions to the financial industry that\nhave grown in importance over the years. One was the impetus given to the\nPrivate Equity industry and the use of LBOs. The second was the realization\nthat thorough research was the key to success, financial and otherwise. Perhaps\nan unintended consequence of the growth in junk bonds and tailored financing\nwas the growth of Silicon valley and technology powerhouses in the California\nbay area. Investors witnessed that there was a possibility for significant\nreturns and that financial success could be had due to the risk mitigation that\nMilken demonstrated by investing in portfolios of so called high risk and low\nprofile companies. We point out the current trend in many regions of the world,\nwhich is the birth of financial and technology firms and we suggest that\nfinding innovative ways of financing could be the key to the sustained growth\nof these eco-systems.\n"
    },
    {
        "paper_id": 1910.1396,
        "authors": "Sven Husmann, Antoniya Shivarova, Rick Steinert",
        "title": "Cross-validated covariance estimators for high-dimensional\n  minimum-variance portfolios",
        "comments": "Covariance Estimation; Portfolio Optimization; High-dimensionality;\n  Cross-validation",
        "journal-ref": null,
        "doi": "10.1007/s11408-020-00376-y",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The global minimum-variance portfolio is a typical choice for investors\nbecause of its simplicity and broad applicability. Although it requires only\none input, namely the covariance matrix of asset returns, estimating the\noptimal solution remains a challenge. In the presence of high-dimensionality in\nthe data, the sample covariance estimator becomes ill-conditioned and leads to\nsuboptimal portfolios out-of-sample. To address this issue, we review recently\nproposed efficient estimation methods for the covariance matrix and extend the\nliterature by suggesting a multi-fold cross-validation technique for selecting\nthe necessary tuning parameters within each method. Conducting an extensive\nempirical analysis with four datasets based on the S&P 500, we show that the\ndata-driven choice of specific tuning parameters with the proposed\ncross-validation improves the out-of-sample performance of the global\nminimum-variance portfolio. In addition, we identify estimators that are\nstrongly influenced by the choice of the tuning parameter and detect a clear\nrelationship between the selection criterion within the cross-validation and\nthe evaluated performance measure.\n"
    },
    {
        "paper_id": 1910.13969,
        "authors": "Giuseppe Carlo Calafiore, Marisa Hillary Morales, Vittorio Tiozzo,\n  Serge Marquie",
        "title": "A Classifiers Voting Model for Exit Prediction of Privately Held\n  Companies",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Predicting the exit (e.g. bankrupt, acquisition, etc.) of privately held\ncompanies is a current and relevant problem for investment firms. The\ndifficulty of the problem stems from the lack of reliable, quantitative and\npublicly available data. In this paper, we contribute to this endeavour by\nconstructing an exit predictor model based on qualitative data, which blends\nthe outcomes of three classifiers, namely, a Logistic Regression model, a\nRandom Forest model, and a Support Vector Machine model. The output of the\ncombined model is selected on the basis of the majority of the output classes\nof the component models. The models are trained using data extracted from the\nThomson Reuters Eikon repository of 54697 US and European companies over the\n1996-2011 time span. Experiments have been conducted for predicting whether the\ncompany eventually either gets acquired or goes public (IPO), against the\ncomplementary event that it remains private or goes bankrupt, in the considered\ntime window. Our model achieves a 63\\% predictive accuracy, which is quite a\nvaluable figure for Private Equity investors, who typically expect very high\nreturns from successful investments.\n"
    },
    {
        "paper_id": 1910.14005,
        "authors": "Alexander Wagner and Stan Uryasev",
        "title": "Portfolio Optimization with Expectile and Omega Functions",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper proves equivalences of portfolio optimization problems with\nnegative expectile and omega ratio. We derive subgradients for the negative\nexpectile as a function of the portfolio from a known dual representation of\nexpectile and general theory about subgradients of risk measures. We also give\nan elementary derivation of the gradient of negative expectile under some\nassumptions and provide an example where negative expectile is demonstrably not\ndifferentiable. We conducted a case study and solved portfolio optimization\nproblems with negative expectile objective and constraint.\n"
    },
    {
        "paper_id": 1910.14023,
        "authors": "John Stachurski",
        "title": "Firm Entry and Exit with Unbounded Productivity Growth",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In Hopenhayn's (1992) entry-exit model productivity is bounded, implying that\nthe predicted firm size distribution cannot match the power law tail observable\nin the data. In this paper we remove the boundedness assumption and, in this\nmore general setting, provide an exact characterization of existence of\nstationary equilibria, as well as a novel sufficient condition for existence\nbased on treating production as a Lyapunov function. We also provide new\nrepresentations of the rate of entry and aggregate supply. Finally, we prove\nthat the firm size distribution has a power law tail under a very broad set of\nproductivity growth specifications.\n"
    },
    {
        "paper_id": 1910.14282,
        "authors": "Christian Meier, Lingfei Li, Gongqiu Zhang",
        "title": "Markov Chain Approximation of One-Dimensional Sticky Diffusions",
        "comments": null,
        "journal-ref": "Adv. Appl. Probab. 53 (2021) 335-369",
        "doi": "10.1017/apr.2020.65",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We develop continuous time Markov chain (CTMC) approximation of\none-dimensional diffusions with a lower sticky boundary. Approximate solutions\nto the action of the Feynman-Kac operator associated with a sticky diffusion\nand first passage probabilities are obtained using matrix exponentials. We show\nhow to compute matrix exponentials efficiently and prove that a carefully\ndesigned scheme achieves second order convergence. We also propose a scheme\nbased on CTMC approximation for the simulation of sticky diffusions, for which\nthe Euler scheme may completely fail. The efficiency of our method and its\nadvantages over alternative approaches are illustrated in the context of bond\npricing in a sticky short rate model for low interest environment.\n"
    },
    {
        "paper_id": 1910.14413,
        "authors": "Imke Redeker and Ralf Wunderlich",
        "title": "Credit risk with asymmetric information and a switching default\n  threshold",
        "comments": "20 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We investigate the impact of available information on the estimation of the\ndefault probability within a generalized structural model for credit risk. The\ntraditional structural model where default is triggered when the value of the\nfirm's asset falls below a constant threshold is extended by relaxing the\nassumption of a constant default threshold. The default threshold at which the\nfirm is liquidated is modeled as a random variable whose value is chosen by the\nmanagement of the firm and dynamically adjusted to account for changes in the\neconomy or the appointment of a new firm management. Investors on the market\nhave no access to the value of the threshold and only anticipate the\ndistribution of the threshold. We distinguish different information levels on\nthe firm's assets and derive explicit formulas for the conditional default\nprobability given these information levels. Numerical results indicate that the\ninformation level has a considerable impact on the estimation of the default\nprobability and the associated credit yield spread.\n"
    },
    {
        "paper_id": 1910.14522,
        "authors": "Alan L. Lewis",
        "title": "Option-based Equity Risk Premiums",
        "comments": "44 pages, 12 figures, revised Appendix A, added reference to an\n  application to COVID-19 pandemic, supplemented conclusions with a remark\n  about Ross Recovery",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We construct the term structure of the (forward-looking, US market) equity\nrisk premium from SPX option chains. The method is \"model-light\". Risk-neutral\nprobability densities are estimated by fitting $N$-component Gaussian mixture\nmodels to option quotes, where $N$ is a small integer (here 4 or 5). These\ndensities are transformed to their real-world equivalents by exponential\ntilting with a single parameter: the Coefficient of Relative Risk Aversion\n$\\kappa$. From history, I estimate $\\kappa = 3 \\pm 0.5$. From the inferred\nreal-world densities, the equity risk premium is readily calculated. Three term\nstructures serve as examples.\n"
    },
    {
        "paper_id": 1910.14652,
        "authors": "Natalia Zdanowska",
        "title": "Exploring cities of Central and Eastern Europe within transnational\n  company networks: the core-periphery effect",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  After the Fall of the Berlin Wall, Central Eastern European cities (CEEc)\nintegrated the globalized world, characterized by a core-periphery structure\nand hierarchical interactions between cities. This article gives evidence of\nthe core-periphery effect on CEEc in 2013 in terms of differentiation of their\nurban functions after 1989. We investigate the position of all CEEc in\ntransnational company networks in 2013. We examine the orientations of\nownership links between firms, the spatial patterns of these networks and the\nspecialization of firms in CEEc involved. The major contribution of this paper\nconsists in giving proof of a core-periphery structure within Central Eastern\nEurope itself, but also of the diffusion of innovations theory as not only\nlarge cities, but also medium-sized and small ones are part of the\nmultinational networks of firms. These findings provide significant insights\nfor the targeting of specific regional policies of the European Union.\n"
    },
    {
        "paper_id": 1910.14658,
        "authors": "Natalia Zdanowska",
        "title": "Spatial polarisation within foreign trade and transnational firms'\n  networks. The Case of Central and Eastern Europe",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  After the fall of the Berlin Wall, Central and Eastern Europe were subject to\nstrong polarisation processes. This article proposes examines two neglected\naspects regarding the transition period: a comparative static assessment of\nforeign trade since 1967 until 2012 and a city-centred analysis of\ntransnational companies in 2013. Results show a growing economic\ndifferentiation between the North-West and South-East as well as a division\nbetween large metropolises and other cities. These findings may complement the\ntargeting of specific regional strategies such as those conceived within the\nCohesion policy of the European Union.\n"
    },
    {
        "paper_id": 1911.00033,
        "authors": "Natalia Zdanowska",
        "title": "Integration into \\'economie-monde and regionalisation of the Central\n  Eastern European space since 1989",
        "comments": "Paper in French",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The fall of the Berlin Wall in 1989, modified the relations between cities of\nthe former communist bloc. The European and worldwide reorientation of\ninteractions that followed raises the question of the actual state of\nhistorical relationships between Central Eastern European cities, but also with\nex-USSR and ex-Yugoslavian ones. Do Central and Eastern European cities\nreproduce trajectories from the past in a new economic context? This paper will\nexamine their evolution in terms of trade exchanges and air traffic connexions\nsince 1989. They are confronted with transnational firm networks for the recent\nyears. The main contribution is to show a progressive formation of several\neconomic regions in Central and Eastern Europe as a result of integration into\nBraudel's \\'economie-monde.\n"
    },
    {
        "paper_id": 1911.00281,
        "authors": "Jia Yue, Ben-Zhang Yang, Ming-Hui Wang, Nan-Jing Huang",
        "title": "Asset Prices with Investor Protection and Past Information",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper, we consider a dynamic asset pricing model in an approximate\nfractional economy to address empirical regularities related to both investor\nprotection and past information. Our newly developed model features not only in\nterms with a controlling shareholder who diverts a fraction of the output, but\nalso good (or bad) memory in his budget dynamics which can be well-calibrated\nby a pathwise way from the historical data. We find that poorer investor\nprotection leads to higher stock holdings of controlling holders, lower gross\nstock returns, lower interest rates, and lower modified stock volatilities if\nthe ownership concentration is sufficiently high. More importantly, by\nestablishing an approximation scheme for good (bad) memory of investors on the\nhistorical market information, we conclude that good (bad) memory would\nincrease (decrease) aforementioned dynamics and reveal that good (bad) memory\nstrengthens (weakens) investor protection for minority shareholder when the\nownership concentration is sufficiently high, while good (bad) memory inversely\nweakens (strengthens) investor protection for minority shareholder when the\nownership concentration is sufficiently low. Our model's implications are\nconsistent with a number of interesting facts documented in the recent\nliterature.\n"
    },
    {
        "paper_id": 1911.00386,
        "authors": "Flavia Antonacci and Cristina Costantini and Fernanda D'Ippoliti and\n  Marco Papi",
        "title": "Risk Neutral Valuation of Inflation-Linked Interest Rate Derivatives",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We propose a model for the joint evolution of European inflation, the\nEuropean Central Bank official interest rate and the short-term interest rate,\nin a stochastic, continuous time setting.\n  We derive the valuation equation for a contingent claim depending potentially\non all three factors. This valuation equation reduces to a finite number of\nCauchy problems for a degenerate parabolic PDE with non-local terms. We show\nthat the price of the contingent claim is the only viscosity solution of the\nvaluation equation.\n  We also provide an efficient numerical scheme to compute the price and\nimplement it in an example.\n"
    },
    {
        "paper_id": 1911.00512,
        "authors": "F. Swen Kuh and Grace S. Chiu and Anton H. Westveld",
        "title": "Modeling National Latent Socioeconomic Health and Examination of Policy\n  Effects via Causal Inference",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This research develops a socioeconomic health index for nations through a\nmodel-based approach which incorporates spatial dependence and examines the\nimpact of a policy through a causal modeling framework. As the gross domestic\nproduct (GDP) has been regarded as a dated measure and tool for benchmarking a\nnation's economic performance, there has been a growing consensus for an\nalternative measure---such as a composite `wellbeing' index---to holistically\ncapture a country's socioeconomic health performance. Many conventional ways of\nconstructing wellbeing/health indices involve combining different observable\nmetrics, such as life expectancy and education level, to form an index.\nHowever, health is inherently latent with metrics actually being observable\nindicators of health. In contrast to the GDP or other conventional health\nindices, our approach provides a holistic quantification of the overall\n`health' of a nation. We build upon the latent health factor index (LHFI)\napproach that has been used to assess the unobservable ecological/ecosystem\nhealth. This framework integratively models the relationship between metrics,\nthe latent health, and the covariates that drive the notion of health. In this\npaper, the LHFI structure is integrated with spatial modeling and statistical\ncausal modeling, so as to evaluate the impact of a policy variable (mandatory\nmaternity leave days) on a nation's socioeconomic health, while formally\naccounting for spatial dependency among the nations. We apply our model to\ncountries around the world using data on various metrics and potential\ncovariates pertaining to different aspects of societal health. The approach is\nstructured in a Bayesian hierarchical framework and results are obtained by\nMarkov chain Monte Carlo techniques.\n"
    },
    {
        "paper_id": 1911.00667,
        "authors": "Haotian Zhong, Wei Li and Marlon G. Boarnet",
        "title": "A two-dimensional propensity score matching method for longitudinal\n  quasi-experimental studies: A focus on travel behavior and the built\n  environment",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The lack of longitudinal studies of the relationship between the built\nenvironment and travel behavior has been widely discussed in the literature.\nThis paper discusses how standard propensity score matching estimators can be\nextended to enable such studies by pairing observations across two dimensions:\nlongitudinal and cross-sectional. Researchers mimic randomized controlled\ntrials (RCTs) and match observations in both dimensions, to find synthetic\ncontrol groups that are similar to the treatment group and to match subjects\nsynthetically across before-treatment and after-treatment time periods. We call\nthis a two-dimensional propensity score matching (2DPSM). This method\ndemonstrates superior performance for estimating treatment effects based on\nMonte Carlo evidence. A near-term opportunity for such matching is identifying\nthe impact of transportation infrastructure on travel behavior.\n"
    },
    {
        "paper_id": 1911.00715,
        "authors": "Ren-jie Han, Shi-yuan Liu, Qian Li",
        "title": "Do Chinese Internet Users Exist Heterogeneity in Search Behavior?",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Investor attention is an important concept in behavioral finance. Many\narticles have conducted cross-disciplinary research leading by this concept. In\nthis paper, we use data extraction technology to collect a large number of\nBaidu Index keyword search volume data. After analyzing the data, we draw a\nconclusion that has not been paid attention to in all the past research. We\nfind heterogeneity in searching by internet users in China. Firstly, in terms\nof search behavior, internet users are more inclined to use the PC end to\nobtain information when facing areas which need to be taken seriously by them.\nSecondly, attention is heterogeneous while searching. When Internet users\nsearch for information in mobile end, their attention is divergent, and search\nfor seemingly unrelated keywords at the same time which limits their attention\nto information.\n"
    },
    {
        "paper_id": 1911.00877,
        "authors": "Alan Bain, Matthieu Mariapragassam, Christoph Reisinger",
        "title": "Calibration of Local-Stochastic and Path-Dependent Volatility Models to\n  Vanilla and No-Touch Options",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We propose a generic calibration framework to both vanilla and no-touch\noptions for a large class of continuous semi-martingale models. The method\nbuilds upon the forward partial integro-differential equation (PIDE) derived in\nHambly et al. (2016), which allows fast computation of up-and-out call prices\nfor the complete set of strikes, barriers and maturities. It also utilises a\nnovel two-states particle method to estimate the Markovian projection of the\nvariance onto the spot and running maximum. We detail a step-by-step procedure\nfor a Heston-type local-stochastic volatility model with local vol-of-vol, as\nwell as two path-dependent volatility models where the local volatility\ncomponent depends on the running maximum. In numerical tests, we benchmark\nthese new models against standard models for a set of EURUSD market data, all\nthree models are seen to calibrate well within the market no-touch bid--ask.\n"
    },
    {
        "paper_id": 1911.00919,
        "authors": "Sebastien Valeyre, Denis S. Grebenkov, and Sofiane Aboura",
        "title": "The Reactive Beta Model",
        "comments": null,
        "journal-ref": "J. Finan. Res. 42, 71-113 (2019)",
        "doi": "10.1111/jfir.1217",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We present a reactive beta model that includes the leverage effect to allow\nhedge fund managers to target a near-zero beta for market neutral strategies.\nFor this purpose, we derive a metric of correlation with leverage effect to\nidentify the relation between the market beta and volatility changes. An\nempirical test based on the most popular market neutral strategies is run from\n2000 to 2015 with exhaustive data sets including 600 US stocks and 600 European\nstocks. Our findings confirm the ability of the reactive beta model to withdraw\nan important part of the bias from the beta estimation and from most popular\nmarket neutral strategies.\n"
    },
    {
        "paper_id": 1911.00946,
        "authors": "Federico Echenique and Taisuke Imai and Kota Saito",
        "title": "Decision Making under Uncertainty: An Experimental Study in Market\n  Settings",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We implement nonparametric revealed-preference tests of subjective expected\nutility theory and its generalizations. We find that a majority of subjects'\nchoices are consistent with the maximization of some utility function. They\nrespond to price changes in the direction subjective expected utility theory\npredicts, but not to a degree that makes them consistent with the theory.\nMaxmin expected utility a dds no explanatory power. The degree of deviations\nfrom the theory is uncorrelated with demographic characteristics. Our findings\nare essentially the same in laboratory data with a student population and in a\npanel survey with a general sample of the U.S. population.\n"
    },
    {
        "paper_id": 1911.00992,
        "authors": "Philippe G. LeFloch and Jean-Marc Mercier",
        "title": "The Transport-based Mesh-free Method (TMM) and its applications in\n  finance: a review",
        "comments": "7 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We review a numerical technique, referred to as the Transport-based Mesh-free\nMethod (TMM), and we discuss its applications to mathematical finance. We\nrecently introduced this method from a numerical standpoint and investigated\nthe accuracy of integration formulas based on the Monte-Carlo methodology:\nquantitative error bounds were discussed and, in this short note, we outline\nthe main ideas of our approach. The techniques of transportation and\nreproducing kernels lead us to a very efficient methodology for numerical\nsimulations in many practical applications, and provide some light on the\nmethods used by the artificial intelligence community. For applications in the\nfinance industry, our method allows us to compute many types of risk measures\nwith an accurate and fast algorithm. We propose theoretical arguments as well\nas extensive numerical tests in order to justify sharp convergence rates,\nleading to rather optimal computational times. Cases of direct interest in\nfinance support our claims and the importance of the problem of the curse of\ndimensionality in finance applications is briefly discussed.\n"
    },
    {
        "paper_id": 1911.01073,
        "authors": "Marco Guerzoni, Consuelo R. Nava, Massimiliano Nuccio",
        "title": "The survival of start-ups in time of crisis. A machine learning approach\n  to measure innovation",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper shows how data science can contribute to improving empirical\nresearch in economics by leveraging on large datasets and extracting\ninformation otherwise unsuitable for a traditional econometric approach. As a\ntest-bed for our framework, machine learning algorithms allow us to create a\nnew holistic measure of innovation built on a 2012 Italian Law aimed at\nboosting new high-tech firms. We adopt this measure to analyse the impact of\ninnovativeness on a large population of Italian firms which entered the market\nat the beginning of the 2008 global crisis. The methodological contribution is\norganised in different steps. First, we train seven supervised learning\nalgorithms to recognise innovative firms on 2013 firmographics data and select\na combination of those with best predicting power. Second, we apply the former\non the 2008 dataset and predict which firms would have been labelled as\ninnovative according to the definition of the law. Finally, we adopt this new\nindicator as regressor in a survival model to explain firms' ability to remain\nin the market after 2008. Results suggest that the group of innovative firms\nare more likely to survive than the rest of the sample, but the survival\npremium is likely to depend on location.\n"
    },
    {
        "paper_id": 1911.01203,
        "authors": "Alexander J. M. Kell, Matthew Forshaw, A. Stephen McGough",
        "title": "ElecSim: Monte-Carlo Open-Source Agent-Based Model to Inform Policy for\n  Long-Term Electricity Planning",
        "comments": "e-Energy '19 Proceedings of the Tenth ACM International Conference on\n  Future Energy Systems",
        "journal-ref": null,
        "doi": "10.1145/3307772.3335321",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Due to the threat of climate change, a transition from a fossil-fuel based\nsystem to one based on zero-carbon is required. However, this is not as simple\nas instantaneously closing down all fossil fuel energy generation and replacing\nthem with renewable sources -- careful decisions need to be taken to ensure\nrapid but stable progress. To aid decision makers, we present a new tool,\nElecSim, which is an open-sourced agent-based modelling framework used to\nexamine the effect of policy on long-term investment decisions in electricity\ngeneration. ElecSim allows non-experts to rapidly prototype new ideas.\n  Different techniques to model long-term electricity decisions are reviewed\nand used to motivate why agent-based models will become an important strategic\ntool for policy. We motivate why an open-source toolkit is required for\nlong-term electricity planning.\n  Actual electricity prices are compared with our model and we demonstrate that\nthe use of a Monte-Carlo simulation in the system improves performance by\n$52.5\\%$. Further, using ElecSim we demonstrate the effect of a carbon tax to\nencourage a low-carbon electricity supply. We show how a {\\pounds}40 ($\\$50$)\nper tonne of CO2 emitted would lead to 70% renewable electricity by 2050.\n"
    },
    {
        "paper_id": 1911.01272,
        "authors": "Zura Kakushadze and Willie Yu",
        "title": "iCurrency?",
        "comments": "33 pages; a few trivial typos corrected, no other changes; to appear\n  in World Economics",
        "journal-ref": "World Economics 20(4) (2019) 151-175",
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We discuss the idea of a purely algorithmic universal world iCurrency set\nforth in [Kakushadze and Liew, 2014] (https://ssrn.com/abstract=2542541) and\nexpanded in [Kakushadze and Liew, 2017] (https://ssrn.com/abstract=3059330) in\nlight of recent developments, including Libra. Is Libra a contender to become\niCurrency? Among other things, we analyze the Libra proposal, including the\nstability and volatility aspects, and discuss various issues that must be\naddressed. For instance, one cannot expect a cryptocurrency such as Libra to\ntrade in a narrow band without a robust monetary policy. The presentation in\nthe main text of the paper is intentionally nontechnical. It is followed by an\nextensive appendix with a mathematical description of the dynamics of\n(crypto)currency exchange rates in target zones, mechanisms for keeping the\nexchange rate from breaching the band, the role of volatility, etc.\n"
    },
    {
        "paper_id": 1911.0133,
        "authors": "Daniel J. Diroff (Akvelon, Inc.)",
        "title": "Bitcoin Coin Selection with Leverage",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We present a new Bitcoin coin selection algorithm, \"coin selection with\nleverage\", which aims to improve upon cost savings than that of standard\nknapsack like approaches. Parameters to the new algorithm are available to be\ntuned at the users discretion to address other goals of coin selection. Our\napproach naturally fits as a replacement for the standard knapsack ingredient\nof full coin selection procedures.\n"
    },
    {
        "paper_id": 1911.01391,
        "authors": "Agostino Capponi, Sveinn Olafsson, Thaleia Zariphopoulou",
        "title": "Personalized Robo-Advising: Enhancing Investment through Client\n  Interaction",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Automated investment managers, or robo-advisors, have emerged as an\nalternative to traditional financial advisors. The viability of robo-advisors\ncrucially depends on their ability to offer personalized financial advice. We\nintroduce a novel framework, in which a robo-advisor interacts with a client to\nsolve an adaptive mean-variance portfolio optimization problem. The risk-return\ntradeoff adapts to the client's risk profile, which depends on idiosyncratic\ncharacteristics, market returns, and economic conditions. We show that the\noptimal investment strategy includes both myopic and intertemporal hedging\nterms which are impacted by the dynamics of the client's risk profile. We\ncharacterize the optimal portfolio personalization via a tradeoff faced by the\nrobo-advisor between receiving client information in a timely manner and\nmitigating behavioral biases in the risk profile communicated by the client. We\nargue that the optimal portfolio's Sharpe ratio and return distribution improve\nif the robo-advisor counters the client's tendency to reduce market exposure\nduring economic contractions when the market risk-return tradeoff is more\nfavorable.\n"
    },
    {
        "paper_id": 1911.01568,
        "authors": "Sung-Gook Choi and Deok-Sun Lee",
        "title": "Engel's law in the commodity composition of exports",
        "comments": "7 figures, 4 tables",
        "journal-ref": "Scientific Reports 9, 15871 (2019)",
        "doi": "10.1038/s41598-019-52281-8",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Different shares of distinct commodity sectors in production, trade, and\nconsumption illustrate how resources and capital are allocated and invested.\nEconomic progress has been claimed to change the share distribution in a\nuniversal manner as exemplified by the Engel's law for the household\nexpenditure and the shift from primary to manufacturing and service sector in\nthe three sector model. Searching for large-scale quantitative evidence of such\ncorrelation, we analyze the gross-domestic product (GDP) and international\ntrade data based on the standard international trade classification (SITC) in\nthe period 1962 to 2000. Three categories, among ten in the SITC, are found to\nhave their export shares significantly correlated with the GDP over countries\nand time; The machinery category has positive and food and crude materials have\nnegative correlations. The export shares of commodity categories of a country\nare related to its GDP by a power-law with the exponents characterizing the\nGDP-elasticity of their export shares. The distance between two countries in\nterms of their export portfolios is measured to identify several clusters of\ncountries sharing similar portfolios in 1962 and 2000. We show that the\ncountries whose GDP is increased significantly in the period are likely to\ntransit to the clusters displaying large share of the machinery category.\n"
    },
    {
        "paper_id": 1911.017,
        "authors": "Magnus Wiese, Lianjun Bai, Ben Wood, Hans Buehler",
        "title": "Deep Hedging: Learning to Simulate Equity Option Markets",
        "comments": null,
        "journal-ref": "NeurIPS 2019 Workshop on Robust AI in Financial Services: Data,\n  Fairness, Explainability, Trustworthiness, and Privacy",
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We construct realistic equity option market simulators based on generative\nadversarial networks (GANs). We consider recurrent and temporal convolutional\narchitectures, and assess the impact of state compression. Option market\nsimulators are highly relevant because they allow us to extend the limited\nreal-world data sets available for the training and evaluation of option\ntrading strategies. We show that network-based generators outperform classical\nmethods on a range of benchmark metrics, and adversarial training achieves the\nbest performance. Our work demonstrates for the first time that GANs can be\nsuccessfully applied to the task of generating multivariate financial time\nseries.\n"
    },
    {
        "paper_id": 1911.01826,
        "authors": "Abootaleb Shirvani and Dimitri Volchenkov",
        "title": "A Regulated Market Under Sanctions: On Tail Dependence Between Oil,\n  Gold, and Tehran Stock Exchange Index",
        "comments": null,
        "journal-ref": "Journal of Vibration Testing and System Dynamics, Vol. 3(2),\n  297-311 (2019)",
        "doi": "10.5890/JVTSD.2019.09.004",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We demonstrate that the tail dependence should always be taken into account\nas a proxy for systematic risk of loss for investments. We provide the clear\nstatistical evidence of that the structure of investment portfolios on a\nregulated market should be adjusted to the price of gold. Our finding suggests\nthat the active bartering of oil for goods would prevent collapsing the\nnational market facing international sanctions.\n"
    },
    {
        "paper_id": 1911.02067,
        "authors": "Humoud Alsabah, Agostino Capponi, Octavio Ruiz Lacedelli, and Matt\n  Stern",
        "title": "Robo-advising: Learning Investors' Risk Preferences via Portfolio\n  Choices",
        "comments": null,
        "journal-ref": null,
        "doi": "10.1093/jjfinec/nbz040",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We introduce a reinforcement learning framework for retail robo-advising. The\nrobo-advisor does not know the investor's risk preference, but learns it over\ntime by observing her portfolio choices in different market environments. We\ndevelop an exploration-exploitation algorithm which trades off costly\nsolicitations of portfolio choices by the investor with autonomous trading\ndecisions based on stale estimates of investor's risk aversion. We show that\nthe algorithm's value function converges to the optimal value function of an\nomniscient robo-advisor over a number of periods that is polynomial in the\nstate and action space. By correcting for the investor's mistakes, the\nrobo-advisor may outperform a stand-alone investor, regardless of the\ninvestor's opportunity cost for making portfolio decisions.\n"
    },
    {
        "paper_id": 1911.02194,
        "authors": "Abootaleb Shirvani, Svetlozar T. Rachev, and Frank J. Fabozzi",
        "title": "A Rational Finance Explanation of the Stock Predictability Puzzle",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper, we address one of the main puzzles in finance observed in the\nstock market by proponents of behavioral finance: the stock predictability\npuzzle. We offer a statistical model within the context of rational finance\nwhich can be used without relying on behavioral finance assumptions to model\nthe predictability of stock returns. We incorporate the predictability of stock\nreturns into the well-known Black-Scholes option pricing formula. Empirically,\nwe analyze the option and spot trader's market predictability of stock prices\nby defining a forward-looking measure which we call \"implied excess\npredictability\". The empirical results indicate the effect of option trader's\npredictability of stock returns on the price of stock options is an increasing\nfunction of moneyness, while this effect is decreasing for spot traders. These\nempirical results indicate potential asymmetric predictability of stock prices\nby spot and option traders. We show in pricing options with the strike price\nsignificantly higher or lower than the stock price, the predictability of the\nunderlying stock's return should be incorporated into the option pricing\nformula. In pricing options that have moneyness close to one, stock return\npredictability is not incorporated into the option pricing model because stock\nreturn predictability is the same for both types of traders. In other words,\nspot traders and option traders are equally informed about the future value of\nthe stock market in this case. Comparing different volatility measures, we find\nthat the difference between implied and realized variances or variance risk\npremium can potentially be used as a stock return predictor.\n"
    },
    {
        "paper_id": 1911.02205,
        "authors": "Richard Y. Chen",
        "title": "The Fourier Transform Method for Volatility Functional Inference by\n  Asynchronous Observations",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We study the volatility functional inference by Fourier transforms. This\nspectral framework is advantageous in that it harnesses the power of harmonic\nanalysis to handle missing data and asynchronous observations without any\nartificial time alignment nor data imputation. Under conditions, this spectral\napproach is consistent and we provide limit distributions using irregular and\nasynchronous observations. When observations are synchronous, the Fourier\ntransform method for volatility functionals attains both the optimal\nconvergence rate and the efficient bound in the sense of Le Cam and H\\'ajek.\nAnother finding is asynchronicity or missing data as a form of noise produces\n\"interference\" in the spectrum estimation and impacts on the convergence rate\nof volatility functional estimators. This new methodology extends previous\napplications of volatility functionals, including principal component analysis,\ngeneralized method of moments, continuous-time linear regression models et\ncetera, to high-frequency datasets of which asynchronicity is a prevailing\nfeature.\n"
    },
    {
        "paper_id": 1911.02261,
        "authors": "Christos E. Kountzakis, Damiano Rossello",
        "title": "Acceptability Indices of Performance for Bounded C\\`adl\\`ag Processes",
        "comments": "22 pages, 6 Figures, 2 Tables, 2 Appendixes",
        "journal-ref": null,
        "doi": "10.1080/17442508.2019.1687705",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Indices of acceptability are well suited to frame the axiomatic features of\nmany performance measures, associated to terminal random cash flows.We extend\nthis notion to classes of c\\`adl\\`ag processes modelling cash flows over a\nfixed investment horizon.We provide a representation result for bounded paths.\nWe suggest an acceptability index based both on the static Average\nValue-at-Risk functional and the running minimum of the paths, which eventually\nrepresents a RAROC-type model. Some numerical comparisons clarify the magnitude\nof performance evaluation for processes.\n"
    },
    {
        "paper_id": 1911.02296,
        "authors": "John Armstrong and Cristin Buescu",
        "title": "Collectivised Pension Investment with Exponential Kihlstrom--Mirman\n  Preferences",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In a collectivised pension fund, investors agree that any money remaining in\nthe fund when they die can be shared among the survivors.\n  We give a numerical algorithm to compute the optimal investment-consumption\nstrategy for an infinite collective of identical investors with exponential\nKihlstrom--Mirman preferences, investing in the Black--Scholes market in\ncontinuous time but consuming in discrete time. Our algorithm can also be\napplied to an individual investor.\n  We derive an analytic formula for the optimal consumption in the special case\nof an individual who chooses not to invest in the financial markets. We prove\nthat our problem formulation for a fund with an infinite number of members is a\ngood approximation to a fund with a large, but finite number of members.\n"
    },
    {
        "paper_id": 1911.02361,
        "authors": "Jaros{\\l}aw Duda, Robert Syrek, Henryk Gurgul",
        "title": "Modelling bid-ask spread conditional distributions using hierarchical\n  correlation reconstruction",
        "comments": "10 pages, 7 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  While we would like to predict exact values, available incomplete information\nis rarely sufficient - usually allowing only to predict conditional probability\ndistributions. This article discusses hierarchical correlation reconstruction\n(HCR) methodology for such prediction on example of usually unavailable bid-ask\nspreads, predicted from more accessible data like closing price, volume,\nhigh/low price, returns. In HCR methodology we first normalize marginal\ndistributions to nearly uniform like in copula theory. Then we model (joint)\ndensities as linear combinations of orthonormal polynomials, getting its\ndecomposition into (mixed) moments. Then here we model each moment (separately)\nof predicted variable as a linear combination of mixed moments of known\nvariables using least squares linear regression - getting accurate description\nwith interpretable coefficients describing linear relations between moments.\nCombining such predicted moments we get predicted density as a polynomial, for\nwhich we can e.g. calculate expected value, but also variance to evaluate\nuncertainty of such prediction, or we can use the entire distribution e.g. for\nmore accurate further calculations or generating random values. There were\nperformed 10-fold cross-validation log-likelihood tests for 22 DAX companies,\nleading to very accurate predictions, especially when using individual models\nfor each company as there were found large differences between their behaviors.\nAdditional advantage of the discussed methodology is being computationally\ninexpensive, finding and evaluation a model with hundreds of parameters and\nthousands of data points takes a second on a laptop.\n"
    },
    {
        "paper_id": 1911.02449,
        "authors": "Zoltan Neda, Istvan Gere, Tamas S. Biro, Geza Toth and Noemi Derzsy",
        "title": "Scaling in Income Inequalities and its Dynamical Origin",
        "comments": "10 pages, 6 Figures",
        "journal-ref": null,
        "doi": "10.1016/j.physa.2020.124491",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We provide an analytically treatable model that describes in a unified manner\nincome distribution for all income categories. The approach is based on a\nmaster equation with growth and reset terms. The model assumptions on the\ngrowth and reset rates are tested on an exhaustive database with incomes on\nindividual level spanning a nine year period in the Cluj county (Romania). In\nagreement with our theoretical predictions we find that income distributions\ncomputed for several years collapse on a master-curve when a properly\nnormalised income is considered. The Beta Prime distribution is appropriate to\nfit the collapsed data and it is shown that distributions derived for other\ncountries are following similar trends with different fit parameters. The\nnon-universal feature of the fit parameters suggests that for a more realistic\nmodelling the model parameters have to be linked with specific socio-economic\nregulations.\n"
    },
    {
        "paper_id": 1911.02502,
        "authors": "Junming Yang, Yaoqi Li, Xuanyu Chen, Jiahang Cao and Kangkang Jiang",
        "title": "Deep Learning for Stock Selection Based on High Frequency Price-Volume\n  Data",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Training a practical and effective model for stock selection has been a\ngreatly concerned problem in the field of artificial intelligence. Even though\nsome of the models from previous works have achieved good performance in the\nU.S. market by using low-frequency data and features, training a suitable model\nwith high-frequency stock data is still a problem worth exploring. Based on the\nhigh-frequency price data of the past several days, we construct two separate\nmodels-Convolution Neural Network and Long Short-Term Memory-which can predict\nthe expected return rate of stocks on the current day, and select the stocks\nwith the highest expected yield at the opening to maximize the total return. In\nour CNN model, we propose improvements on the CNNpred model presented by E.\nHoseinzade and S. Haratizadeh in their paper which deals with low-frequency\nfeatures. Such improvements enable our CNN model to exploit the convolution\nlayer's ability to extract high-level factors and avoid excessive loss of\noriginal information at the same time. Our LSTM model utilizes Recurrent Neural\nNetwork'advantages in handling time series data. Despite considerable\ntransaction fees due to the daily changes of our stock position, annualized net\nrate of return is 62.27% for our CNN model, and 50.31% for our LSTM model.\n"
    },
    {
        "paper_id": 1911.02614,
        "authors": "Christa Cuchiero and Sara Svaluto-Ferro",
        "title": "Infinite dimensional polynomial processes",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We introduce polynomial processes taking values in an arbitrary Banach space\n$B$ via their infinitesimal generator $L$ and the associated martingale\nproblem. We obtain two representations of the (conditional) moments in terms of\nsolutions of a system of ODEs on the truncated tensor algebra of dual\nrespectively bidual spaces. We illustrate how the well-known moment formulas\nfor finite dimensional or probability-measure valued polynomial processes can\nbe deduced in this general framework. As an application we consider polynomial\nforward variance curve models which appear in particular as Markovian lifts of\n(rough) Bergomi-type volatility models. Moreover, we show that the signature\nprocess of a $d$-dimensional Brownian motion is polynomial and derive its\nexpected value via the polynomial approach.\n"
    },
    {
        "paper_id": 1911.02906,
        "authors": "Claudio Fontana, Alessandro Gnoatto, and Guillaume Szulda",
        "title": "Multiple yield curve modelling with CBI processes",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We develop a modelling framework for multiple yield curves driven by\ncontinuous-state branching processes with immigration (CBI processes).\nExploiting the self-exciting behavior of CBI jump processes, this approach can\nreproduce the relevant empirical features of spreads between different\ninterbank rates. In particular, we introduce multi-curve models driven by a\nflow of tempered alpha-stable CBI processes. Such models are especially\nparsimonious and tractable, and can generate contagion effects among different\nspreads. We provide a complete analytical framework, including a detailed study\nof discounted exponential moments of CBI processes. The proposed approach\nallows for explicit valuation formulae for all linear interest rate derivatives\nand semi-closed formulae for non-linear derivatives via Fourier techniques and\nquantization. We show that a simple specification of the model can be\nsuccessfully calibrated to market data.\n"
    },
    {
        "paper_id": 1911.03,
        "authors": "Elijah D. Bolluyt, Cristina Comaniciu",
        "title": "Dynamic Influence on Replicator Evolution for the Propagation of\n  Competing Technologies",
        "comments": null,
        "journal-ref": "in IEEE Transactions on Evolutionary Computation, vol. 23, no. 5,\n  pp. 899-903, Oct. 2019",
        "doi": "10.1109/TEVC.2018.2881973",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This work introduces a novel modified Replicator Dynamics model, which\nincludes external influences on the population. This framework models a\nrealistic market into which companies, the external dynamic influences, invest\nresources in order to bolster their product's standing and increase their\nmarket share. The dynamic influences change in each time step of the game, and\ndirectly modify the payoff matrix of the population's interactions. The model\ncan learn from real data how each influence affects the market, and can be used\nto simulate and predict the outcome of a real system. We specifically analyze\nhow a new technology can compete and attempt to unseat an entrenched technology\nas the market leader. We establish a relationship between the external\ninfluences and the population payoff matrix and show how the system can be\nimplemented to predict outcomes in a real market by simulating the rise of the\nAndroid mobile operating system over its primary competition, the iPhone, from\n2009 to 2017.\n"
    },
    {
        "paper_id": 1911.03245,
        "authors": "Samuel Drapeau and Mekonnen Tadese",
        "title": "Dual Representation of Expectile based Expected Shortfall and Its\n  Properties",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The expectile can be considered as a generalization of quantile. While\nexpected shortfall is a quantile based risk measure, we study its counterpart\n-- the expectile based expected shortfall -- where expectile takes the place of\nquantile. We provide its dual representation in terms of Bochner integral.\nAmong other properties, we show that it is bounded from below in terms of\nconvex combinations of expected shortfalls, and also from above by the smallest\nlaw invariant, coherent and comonotonic risk measure, for which we give the\nexplicit formulation of the corresponding distortion function. As a benchmark\nto the industry standard expected shortfall we further provide its comparative\nasymptotic behavior in terms of extreme value distributions. Based on these\nresults, we finally compute explicitly the expectile based expected shortfall\nfor some selected class of distributions.\n"
    },
    {
        "paper_id": 1911.0338,
        "authors": "Guillermo Angeris, Hsien-Tang Kao, Rei Chiang, Charlie Noyes, Tarun\n  Chitra",
        "title": "An analysis of Uniswap markets",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Uniswap -- and other constant product markets -- appear to work well in\npractice despite their simplicity. In this paper, we give a simple formal\nanalysis of constant product markets and their generalizations, showing that,\nunder some common conditions, these markets must closely track the reference\nmarket price. We also show that Uniswap satisfies many other desirable\nproperties and numerically demonstrate, via a large-scale agent-based\nsimulation, that Uniswap is stable under a wide range of market conditions.\n"
    },
    {
        "paper_id": 1911.03467,
        "authors": "Damjana Kokol Bukov\\v{s}ek, Toma\\v{z} Ko\\v{s}ir, Bla\\v{z}\n  Moj\\v{s}kerc, Matja\\v{z} Omladi\\v{c}",
        "title": "Relation between Blomqvist's beta and other measures of concordance of\n  copulas",
        "comments": "13 pages, 4 figures. arXiv admin note: substantial text overlap with\n  arXiv:1909.06648",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  An investigation is presented of how a comprehensive choice of four most\nimportant measures of concordance (namely Spearman's rho, Kendall's tau,\nSpearman's footrule, and Gini's gamma) relate to the fifth one, i.e., the\nBlomqvist's beta. In order to work out these results we present a novel method\nof estimating the values of the four measures of concordance on a family of\ncopulas with fixed value of beta. These results are primarily aimed at the\ncommunity of practitioners trying to find the right copula to be employed on\ntheir data. However, the proposed method as such may be of independent interest\nfrom theoretical point of view.\n"
    },
    {
        "paper_id": 1911.04059,
        "authors": "Kenichi Hirayama, Akihiko Noda",
        "title": "Measuring the Time-Varying Market Efficiency in the Prewar and Wartime\n  Japanese Stock Market, 1924-1943",
        "comments": "34 pages, 4 figures, 6 tables",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This study explores the time-varying structure of market efficiency in the\nprewar and wartime Japanese stock market using a new market\ncapitalization-weighted stock price index, the equity performance index. We\nexamine whether the adaptive market hypothesis (AMH) is supported in that era.\nFirst, we find that the degree of market efficiency in the prewar and wartime\nJapanese stock market varies over time and with major historical events. This\nimplies that the AMH is supported in this market. Second, we find that the\nvariation in market efficiency observed in this study is significantly\ndifferent from that in previous studies because of whether the price index is\ncapitalization weighted. Finally, as government intervention in the market\nintensified throughout the 1930s, market efficiency declined as the war risk\npremium rose, especially from the time when the Pacific War became inevitable.\n"
    },
    {
        "paper_id": 1911.0409,
        "authors": "Steven Pav",
        "title": "A post hoc test on the Sharpe ratio",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We describe a post hoc test for the Sharpe ratio, analogous to Tukey's test\nfor pairwise equality of means. The test can be applied after rejection of the\nhypothesis that all population Signal-Noise ratios are equal. The test is\napplicable under a simple correlation structure among asset returns.\nSimulations indicate the test maintains nominal type I rate under a wide range\nof conditions and is moderately powerful under reasonable alternatives.\n"
    },
    {
        "paper_id": 1911.04199,
        "authors": "Lawrence Middleton, James Dodd, Graham Baird",
        "title": "Quantitative earnings enhancement from share buybacks",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper aims to explore the mechanical effect of a company's share\nrepurchase on earnings per share (EPS). In particular, while a share repurchase\nscheme will reduce the overall number of shares, suggesting that the EPS may\nincrease, clearly the expenditure will reduce the net earnings of a company,\nintroducing a trade-off between these competing effects. We first of all review\naccretive share repurchases, then characterise the increase in EPS as a\nfunction of price paid by the company. Subsequently, we analyse and quantify\nthe estimated difference in earnings growth between a company's natural growth\nin the absence of buyback scheme to that with its earnings altered as a result\nof the buybacks. We conclude with an examination of the effect of share\nrepurchases in two cases studies in the US stock-market.\n"
    },
    {
        "paper_id": 1911.04223,
        "authors": "Torben Koch and Tiziano Vargiolu",
        "title": "Optimal Installation of Solar Panels with Price Impact: a Solvable\n  Singular Stochastic Control Problem",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We consider a price-maker company which generates electricity and sells it in\nthe spot market. The company can increase its level of installed power by\nirreversible installations of solar panels. In absence of the company's\neconomic activities, the spot electricity price evolves as an\nOrnstein-Uhlenbeck process, and therefore it has a mean-reverting behavior. The\ncurrent level of the company's installed power has a permanent impact on the\nelectricity price and affects its mean-reversion level. The company aims at\nmaximizing the total expected profits from selling electricity in the market,\nnet of the total expected proportional costs of installation. This problem is\nmodeled as a two-dimensional degenerate singular stochastic control problem in\nwhich the installation strategy is identified as the company's control\nvariable. We follow a guess-and-verify approach to solve the problem. We find\nthat the optimal installation strategy is triggered by a curve which separates\nthe waiting region, where it is not optimal to install additional panels, and\nthe installation region, where it is. Such a curve depends on the current level\nof the company's installed power, and is the unique strictly increasing\nfunction which solves a first-order ordinary differential equation (ODE).\nFinally, our study is complemented by a numerical analysis of the dependency of\nthe optimal installation strategy on the model's parameters.\n"
    },
    {
        "paper_id": 1911.04435,
        "authors": "Theodoros P. Pantelidis, Joseph Y. J. Chow, Saeid Rasulkhani",
        "title": "A many-to-many assignment game and stable outcome algorithm to evaluate\n  collaborative Mobility-as-a-Service platforms",
        "comments": null,
        "journal-ref": "Transportation Research Part B 104 (2020) 79-100",
        "doi": "10.1016/j.trb.2020.08.002",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  As Mobility as a Service (MaaS) systems become increasingly popular, travel\nis changing from unimodal trips to personalized services offered by a platform\nof mobility operators. Evaluation of MaaS platforms depends on modeling both\nuser route decisions as well as operator service and pricing decisions. We\nadopt a new paradigm for traffic assignment in a MaaS network of multiple\noperators using the concept of stable matching to allocate costs and determine\nprices offered by operators corresponding to user route choices and operator\nservice choices without resorting to nonconvex bilevel programming\nformulations. Unlike our prior work, the proposed model allows travelers to\nmake multimodal, multi-operator trips, resulting in stable cost allocations\nbetween competing network operators to provide MaaS for users. An algorithm is\nproposed to efficiently generate stability conditions for the stable outcome\nmodel. Extensive computational experiments demonstrate the use of the model to\nhandling pricing responses of MaaS operators in technological and capacity\nchanges, government acquisition, consolidation, and firm entry, using the\nclassic Sioux Falls network. The proposed algorithm replicates the same\nstability conditions as explicit path enumeration while taking only 17 seconds\ncompared to explicit path enumeration timing out over 2 hours.\n"
    },
    {
        "paper_id": 1911.04489,
        "authors": "Daniel Philps, Artur d'Avila Garcez, Tillman Weyde",
        "title": "Making Good on LSTMs' Unfulfilled Promise",
        "comments": "33rd Conference on Neural Information Processing Systems (NeurIPS\n  2019), Vancouver, Canada. arXiv admin note: text overlap with\n  arXiv:1812.02340",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  LSTMs promise much to financial time-series analysis, temporal and\ncross-sectional inference, but we find that they do not deliver in a real-world\nfinancial management task. We examine an alternative called Continual Learning\n(CL), a memory-augmented approach, which can provide transparent explanations,\ni.e. which memory did what and when. This work has implications for many\nfinancial applications including credit, time-varying fairness in decision\nmaking and more. We make three important new observations. Firstly, as well as\nbeing more explainable, time-series CL approaches outperform LSTMs as well as a\nsimple sliding window learner using feed-forward neural networks (FFNN).\nSecondly, we show that CL based on a sliding window learner (FFNN) is more\neffective than CL based on a sequential learner (LSTM). Thirdly, we examine how\nreal-world, time-series noise impacts several similarity approaches used in CL\nmemory addressing. We provide these insights using an approach called Continual\nLearning Augmentation (CLA) tested on a complex real-world problem, emerging\nmarket equities investment decision making. CLA provides a test-bed as it can\nbe based on different types of time-series learners, allowing testing of LSTM\nand FFNN learners side by side. CLA is also used to test several distance\napproaches used in a memory recall-gate: Euclidean distance (ED), dynamic time\nwarping (DTW), auto-encoders (AE) and a novel hybrid approach, warp-AE. We find\nthat ED under-performs DTW and AE but warp-AE shows the best overall\nperformance in a real-world financial task.\n"
    },
    {
        "paper_id": 1911.04844,
        "authors": "Giordano De Marzo, Andrea Gabrielli, Andrea Zaccaria, and Luciano\n  Pietronero",
        "title": "Dynamical approach to Zipf's law",
        "comments": null,
        "journal-ref": "Phys. Rev. Research 3, 013084 (2021)",
        "doi": "10.1103/PhysRevResearch.3.013084",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The rank-size plots of a large number of different physical and\nsocio-economic systems are usually said to follow Zipf's law, but a unique\nframework for the comprehension of this ubiquitous scaling law is still\nlacking. Here we show that a dynamical approach is crucial: during their\nevolution, some systems are attracted towards Zipf's law, while others presents\nZipf's law only temporarily and, therefore, spuriously. A truly Zipfian\ndynamics is characterized by a dynamical constraint, or coherence, among the\nparameters of the generating PDF, and the number of elements in the system. A\nclear-cut example of such coherence is natural language. Our framework allows\nus to derive some quantitative results that go well beyond the usual Zipf's\nlaw: i) earthquakes can evolve only incoherently and thus show Zipf's law\nspuriously; this allows an assessment of the largest possible magnitude of an\nearthquake occurring in a geographical region. ii) We prove that Zipfian\ndynamics are not additive, explaining analytically why US cities evolve\ncoherently, while world cities do not. iii) Our concept of coherence can be\nused for model selection, for example, the Yule-Simon process can describe the\ndynamics of world countries' GDP. iv) World cities present spurious Zipf's law\nand we use this property for estimating the maximal population of an urban\nagglomeration.\n"
    },
    {
        "paper_id": 1911.04865,
        "authors": "Martin Mihelich and Yan Shu",
        "title": "Analytical solution of $k$th price auction",
        "comments": "10 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We provide an exact analytical solution of the Nash equilibrium for the $k$th\nprice auction by using inverse of distribution functions. As applications, we\nidentify the unique symmetric equilibrium where the valuations have polynomial\ndistribution, fat tail distribution and exponential distributions.\n"
    },
    {
        "paper_id": 1911.05052,
        "authors": "Yu Zheng, Bowei Chen, Timothy M. Hospedales, Yongxin Yang",
        "title": "Index Tracking with Cardinality Constraints: A Stochastic Neural\n  Networks Approach",
        "comments": "Accepted to AAAI 2020 (Oral). 8 pages, 4 figures. Camera-ready\n  version",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Partial (replication) index tracking is a popular passive investment\nstrategy. It aims to replicate the performance of a given index by constructing\na tracking portfolio which contains some constituents of the index. The\ntracking error optimisation is quadratic and NP-hard when taking the L0\nconstraint into account so it is usually solved by heuristic methods such as\nevolutionary algorithms. This paper introduces a simple, efficient and scalable\nconnectionist model as an alternative. We propose a novel reparametrisation\nmethod and then solve the optimisation problem with stochastic neural networks.\nThe proposed approach is examined with S&P 500 index data for more than 10\nyears and compared with widely used index tracking approaches such as forward\nand backward selection and the largest market capitalisation methods. The\nempirical results show our model achieves excellent performance. Compared with\nthe benchmarked models, our model has the lowest tracking error, across a range\nof portfolio sizes. Meanwhile it offers comparable performance to the others on\nsecondary criteria such as volatility, Sharpe ratio and maximum drawdown.\n"
    },
    {
        "paper_id": 1911.05116,
        "authors": "Nicholas Beale, Heather Battey, Anthony C. Davison, and Robert S.\n  MacKay",
        "title": "An Unethical Optimization Principle",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  If an artificial intelligence aims to maximise risk-adjusted return, then\nunder mild conditions it is disproportionately likely to pick an unethical\nstrategy unless the objective function allows sufficiently for this risk. Even\nif the proportion ${\\eta}$ of available unethical strategies is small, the\nprobability ${p_U}$ of picking an unethical strategy can become large; indeed\nunless returns are fat-tailed ${p_U}$ tends to unity as the strategy space\nbecomes large. We define an Unethical Odds Ratio Upsilon (${\\Upsilon}$) that\nallows us to calculate ${p_U}$ from ${\\eta}$, and we derive a simple formula\nfor the limit of ${\\Upsilon}$ as the strategy space becomes large. We give an\nalgorithm for estimating ${\\Upsilon}$ and ${p_U}$ in finite cases and discuss\nhow to deal with infinite strategy spaces. We show how this principle can be\nused to help detect unethical strategies and to estimate ${\\eta}$. Finally we\nsketch some policy implications of this work.\n"
    },
    {
        "paper_id": 1911.05122,
        "authors": "Moritz Vo{\\ss}",
        "title": "A two-player portfolio tracking game",
        "comments": "35 pages, 6 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We study the competition of two strategic agents for liquidity in the\nbenchmark portfolio tracking setup of Bank, Soner, Vo{\\ss} (2017).\nSpecifically, both agents track their own stochastic running trading targets\nwhile interacting through common aggregated temporary and permanent price\nimpact \\`a la Almgren and Chriss (2001). The resulting stochastic linear\nquadratic differential game with terminal state constraints allows for a unique\nand explicitly available open-loop Nash equilibrium. Our results reveal how the\nequilibrium strategies of the two players take into account the other agent's\ntrading targets: either in an exploitative intent or by providing liquidity to\nthe competitor, depending on the relation between temporary and permanent price\nimpact. As a consequence, different behavioral patterns can emerge as optimal\nin equilibrium. These insights complement and extend existing studies in the\nliterature on predatory trading models examined in the context of optimal\nportfolio liquidation games.\n"
    },
    {
        "paper_id": 1911.05193,
        "authors": "Michael Dubrovsky, Marshall Ball, Bogdan Penkovsky",
        "title": "Optical Proof of Work",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Most cryptocurrencies rely on Proof-of-Work (PoW) \"mining\" for resistance to\nSybil and double-spending attacks, as well as a mechanism for currency\nissuance. Hashcash PoW has successfully secured the Bitcoin network since its\ninception, however, as the network has expanded to take on additional value\nstorage and transaction volume, Bitcoin PoW's heavy reliance on electricity has\ncreated scalability issues, environmental concerns, and systemic risks. Mining\nefforts have concentrated in areas with low electricity costs, creating single\npoints of failure. Although PoW security properties rely on imposing a\ntrivially verifiable economic cost on miners, there is no fundamental reason\nfor it to consist primarily of electricity cost. The authors propose a novel\nPoW algorithm, Optical Proof of Work (oPoW), to eliminate energy as the primary\ncost of mining. Proposed algorithm imposes economic difficulty on the miners,\nhowever, the cost is concentrated in hardware (capital expense-CAPEX) rather\nthan electricity (operating expenses-OPEX). The oPoW scheme involves minimal\nmodifications to Hashcash-like PoW schemes, inheriting safety/security\nproperties from such schemes.\n  Rapid growth and improvement in silicon photonics over the last two decades\nhas led to the commercialization of silicon photonic co-processors (integrated\ncircuits that use photons instead of electrons to perform specialized computing\ntasks) for low-energy deep learning. oPoW is optimized for this technology such\nthat miners are incentivized to use specialized, energy-efficient photonics for\ncomputation. Beyond providing energy savings, oPoW has the potential to improve\nnetwork scalability, enable decentralized mining outside of low electricity\ncost areas, and democratize issuance. Due to the CAPEX dominance of mining\ncosts, oPoW hashrate will be significantly less sensitive to underlying coin\nprice declines.\n"
    },
    {
        "paper_id": 1911.05271,
        "authors": "Pascal Michaillat, Emmanuel Saez",
        "title": "Beveridgean Unemployment Gap",
        "comments": null,
        "journal-ref": "Journal of Public Economics Plus 2 (2021) 100009",
        "doi": "10.1016/j.pubecp.2021.100009",
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  This paper develops a sufficient-statistic formula for the unemployment gap\n-- the difference between the actual unemployment rate and the efficient\nunemployment rate. While lowering unemployment puts more people into work, it\nforces firms to post more vacancies and to devote more resources to recruiting.\nThis unemployment-vacancy tradeoff, governed by the Beveridge curve, determines\nthe efficient unemployment rate. Accordingly, the unemployment gap can be\nmeasured from three sufficient statistics: elasticity of the Beveridge curve,\nsocial cost of unemployment, and cost of recruiting. Applying this formula to\nthe United States, 1951--2019, we find that the efficient unemployment rate\naverages 4.3%, always remains between 3.0% and 5.4%, and has been stable\nbetween 3.8% and 4.6% since 1990. As a result, the unemployment gap is\ncountercyclical, reaching 6 percentage points in slumps. The US labor market is\ntherefore generally inefficient and especially inefficiently slack in slumps.\nIn turn, the unemployment gap is a crucial statistic to design labor-market and\nmacroeconomic policies.\n"
    },
    {
        "paper_id": 1911.05309,
        "authors": "Mengying Zhu, Xiaolin Zheng, Yan Wang, Yuyuan Li, Qianqiao Liang",
        "title": "Adaptive Portfolio by Solving Multi-armed Bandit via Thompson Sampling",
        "comments": "conference",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  As the cornerstone of modern portfolio theory, Markowitz's mean-variance\noptimization is considered a major model adopted in portfolio management.\nHowever, due to the difficulty of estimating its parameters, it cannot be\napplied to all periods. In some cases, naive strategies such as\nEqually-weighted and Value-weighted portfolios can even get better performance.\nUnder these circumstances, we can use multiple classic strategies as multiple\nstrategic arms in multi-armed bandit to naturally establish a connection with\nthe portfolio selection problem. This can also help to maximize the rewards in\nthe bandit algorithm by the trade-off between exploration and exploitation. In\nthis paper, we present a portfolio bandit strategy through Thompson sampling\nwhich aims to make online portfolio choices by effectively exploiting the\nperformances among multiple arms. Also, by constructing multiple strategic\narms, we can obtain the optimal investment portfolio to adapt different\ninvestment periods. Moreover, we devise a novel reward function based on users'\ndifferent investment risk preferences, which can be adaptive to various\ninvestment styles. Our experimental results demonstrate that our proposed\nportfolio strategy has marked superiority across representative real-world\nmarket datasets in terms of extensive evaluation criteria.\n"
    },
    {
        "paper_id": 1911.05363,
        "authors": "Mario Coccia",
        "title": "How do scientific disciplines evolve in applied sciences? The properties\n  of scientific fission and ambidextrous scientific drivers",
        "comments": "44 pages, 6 figures, 6 tables",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  One of the fundamental questions in science is how scientific disciplines\nevolve and sustain progress in society. No studies to date allows us to explain\nthe endogenous processes that support the evolution of scientific disciplines\nand emergence of new scientific fields in applied sciences of physics. This\nstudy confronts this problem here by investigating the evolution of\nexperimental physics to explain and generalize some characteristics of the\ndynamics of applied sciences. Empirical analysis suggests properties about the\nevolution of experimental physics and in general of applied sciences, such as:\na) scientific fission, the evolution of scientific disciplines generates a\nprocess of division into two or more research fields that evolve as autonomous\nentities over time; b) ambidextrous drivers of science, the evolution of\nscience via scientific fission is due to scientific discoveries or new\ntechnologies; c) new driving research fields, the drivers of scientific\ndisciplines are new research fields rather than old ones; d) science driven by\ndevelopment of general purpose technologies, the evolution of experimental\nphysics and applied sciences is due to the convergence of experimental and\ntheoretical branches of physics associated with the development of computer,\ninformation systems and applied computational science. Results also reveal that\naverage duration of the upwave of scientific production in scientific fields\nsupporting experimental physics is about 80 years. Overall, then, this study\nbegins the process of clarifying and generalizing, as far as possible, some\ncharacteristics of the evolutionary dynamics of scientific disciplines that can\nlay a foundation for the development of comprehensive properties explaining the\nevolution of science as a whole for supporting fruitful research policy\nimplications directed to advancement of science and technological progress in\nsociety.\n"
    },
    {
        "paper_id": 1911.05462,
        "authors": "Jean-Michel Fayolle and Vincent Lemaire and Thibaut Montes and Gilles\n  Pag\\`es",
        "title": "Quantization-based Bermudan option pricing in the $FX$ world",
        "comments": null,
        "journal-ref": "Journal of Computational Finance, Volume 25, Number 2, September\n  2021",
        "doi": "10.21314/JCF.2021.008",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper proposes two numerical solution based on Product Optimal\nQuantization for the pricing of Foreign Echange (FX) linked long term Bermudan\noptions e.g. Bermudan Power Reverse Dual Currency options, where we take into\naccount stochastic domestic and foreign interest rates on top of stochastic FX\nrate, hence we consider a 3-factor model. For these two numerical methods, we\ngive an estimation of the $L^2$-error induced by such approximations and we\nillustrate them with market-based examples that highlight the speed of such\nmethods.\n"
    },
    {
        "paper_id": 1911.05523,
        "authors": "Luca De Gennaro Aquino, Carole Bernard",
        "title": "Bounds on Multi-asset Derivatives via Neural Networks",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Using neural networks, we compute bounds on the prices of multi-asset\nderivatives given information on prices of related payoffs. As a main example,\nwe focus on European basket options and include information on the prices of\nother similar options, such as spread options and/or basket options on\nsubindices. We show that, in most cases, adding further constraints gives rise\nto bounds that are considerably tighter and discuss the maximizing/minimizing\ncopulas achieving such bounds. Our approach follows the literature on\nconstrained optimal transport and, in particular, builds on a recent paper by\nEckstein and Kupper (2019, Appl. Math. Optim.).\n"
    },
    {
        "paper_id": 1911.0562,
        "authors": "Johannes Ruf, Weiguan Wang",
        "title": "Neural networks for option pricing and hedging: a literature review",
        "comments": "Minor changes. Accepted for publications in Journal of Computational\n  Finance",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Neural networks have been used as a nonparametric method for option pricing\nand hedging since the early 1990s. Far over a hundred papers have been\npublished on this topic. This note intends to provide a comprehensive review.\nPapers are compared in terms of input features, output variables, benchmark\nmodels, performance measures, data partition methods, and underlying assets.\nFurthermore, related work and regularisation techniques are discussed.\n"
    },
    {
        "paper_id": 1911.05814,
        "authors": "Paolo Magrassi",
        "title": "Econophysics deserves a revamping",
        "comments": "arXiv admin note: text overlap with arXiv:1208.5316",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  The paper argues that attracting more economists and adopting a more-precise\ndefinition of dynamic complexity might help econophysics acquire more attention\nin the economics community and bring new lymph to economic research. It may be\nnecessary to concentrate less on the applications than on the basics of\neconomic complexity, beginning with expansion and deepening of the study of\nsmall systems with few interacting components, while until thus far complexity\nhas been assumed to be a prerogative of complicated systems only. It is\npossible that without a thorough analysis at that level, the understanding of\nsystems that are at the same time complex and complicated will continue to\nelude economics and econophysics research altogether. To that purpose, the\npaper initiates and frames a definition of dynamic complexity grounded on the\nconcept of non-linear dynamical system.\n"
    },
    {
        "paper_id": 1911.05892,
        "authors": "Sumitra Ganesh, Nelson Vadori, Mengda Xu, Hua Zheng, Prashant Reddy,\n  Manuela Veloso",
        "title": "Reinforcement Learning for Market Making in a Multi-agent Dealer Market",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Market makers play an important role in providing liquidity to markets by\ncontinuously quoting prices at which they are willing to buy and sell, and\nmanaging inventory risk. In this paper, we build a multi-agent simulation of a\ndealer market and demonstrate that it can be used to understand the behavior of\na reinforcement learning (RL) based market maker agent. We use the simulator to\ntrain an RL-based market maker agent with different competitive scenarios,\nreward formulations and market price trends (drifts). We show that the\nreinforcement learning agent is able to learn about its competitor's pricing\npolicy; it also learns to manage inventory by smartly selecting asymmetric\nprices on the buy and sell sides (skewing), and maintaining a positive (or\nnegative) inventory depending on whether the market price drift is positive (or\nnegative). Finally, we propose and test reward formulations for creating risk\naverse RL-based market maker agents.\n"
    },
    {
        "paper_id": 1911.05952,
        "authors": "Sayantan Banerjee and Kousik Guhathakurta",
        "title": "Change-point Analysis in Financial Networks",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  A major impact of globalization has been the information flow across the\nfinancial markets rendering them vulnerable to financial contagion. Research\nhas focused on network analysis techniques to understand the extent and nature\nof such information flow. It is now an established fact that a stock market\ncrash in one country can have a serious impact on other markets across the\nglobe. It follows that such crashes or critical regimes will affect the network\ndynamics of the global financial markets. In this paper, we use sequential\nchange point detection in dynamic networks to detect changes in the network\ncharacteristics of thirteen stock markets across the globe. Our method helps us\nto detect changes in network behavior across all known stock market crashes\nduring the period of study. In most of the cases, we can detect a change in the\nnetwork characteristics prior to crash. Our work thus opens the possibility of\nusing this technique to create a warning bell for critical regimes in financial\nmarkets.\n"
    },
    {
        "paper_id": 1911.06123,
        "authors": "Riley Jones and Adriana Ocejo",
        "title": "Assessing Guaranteed Minimum Income Benefits and Rationality of\n  Exercising Reset Options in Variable",
        "comments": null,
        "journal-ref": null,
        "doi": "10.5539/ijsp.v8n5p13",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  A variable annuity is an equity-linked financial product typically offered by\ninsurance companies. The policyholder makes an upfront payment to the insurance\ncompany and, in return, the insurer is required to make a series of payments\nstarting at an agreed upon date. For a higher premium, many insurance companies\noffer additional guarantees or options which protect policyholders from various\nmarket risks. This research is centered around two of these options: the\nguaranteed minimum income benefit (GMIB) and the reset option. The sensitivity\nof various parameters on the value of the GMIB is explored, particularly the\nguaranteed payment rate set by the insurer. Additionally, a critical value for\nfuture interest rates is calculated to determine the rationality of exercising\nthe reset option. This will be able to provide insight to both the policyholder\nand policy writer on how their future projections on the performance of the\nstock market and interest rates should guide their respective actions of\nexercising and pricing variable annuity options. This can help provide details\ninto the value of adding options to a variable annuity for companies that are\nlooking to make variable annuity policies more attractive in a competitive\nmarket.\n"
    },
    {
        "paper_id": 1911.06126,
        "authors": "Giuseppe Brandi, Ruggero Gramatica, Tiziana Di Matteo",
        "title": "Unveil stock correlation via a new tensor-based decomposition method",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Portfolio allocation and risk management make use of correlation matrices and\nheavily rely on the choice of a proper correlation matrix to be used. In this\nregard, one important question is related to the choice of the proper sample\nperiod to be used to estimate a stable correlation matrix. This paper addresses\nthis question and proposes a new methodology to estimate the correlation matrix\nwhich doesn't depend on the chosen sample period. This new methodology is based\non tensor factorization techniques. In particular, combining and normalizing\nfactor components, we build a correlation matrix which shows emerging\nstructural dependency properties not affected by the sample period. To retrieve\nthe factor components, we propose a new tensor decomposition (which we name\nSlice-Diagonal Tensor (SDT) factorization) and compare it to the two most used\ntensor decompositions, the Tucker and the PARAFAC. We have that the new\nfactorization is more parsimonious than the Tucker decomposition and more\nflexible than the PARAFAC. Moreover, this methodology applied to both simulated\nand empirical data shows results which are robust to two non-parametric tests,\nnamely Kruskal-Wallis and Kolmogorov-Smirnov tests. Since the resulting\ncorrelation matrix features stability and emerging structural dependency\nproperties, it can be used as alternative to other correlation matrices type of\nmeasures, including the Person correlation.\n"
    },
    {
        "paper_id": 1911.06159,
        "authors": "Marcus C. Christiansen, Boualem Djehiche",
        "title": "Nonlinear reserving and multiple contract modifications in life\n  insurance",
        "comments": null,
        "journal-ref": "Insurance: Mathematics and Economics 93, July 2020, 187-195",
        "doi": "10.1016/j.insmatheco.2020.05.004",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Life insurance cash flows become reserve dependent when contract conditions\nare modified during the contract term on condition that actuarial equivalence\nis maintained. As a result, insurance cash flows and prospective reserves\ndepend on each other in a circular way, and it is a non-trivial problem to\nsolve that circularity and make cash flows and prospective reserves\nwell-defined. In Markovian models, the (stochastic) Thiele equation and the\nCantelli Theorem are the standard tools for solving the circularity issue and\nfor maintaining actuarial equivalence. This paper expands the stochastic Thiele\nequation and the Cantelli Theorem to non-Markovian frameworks and presents a\nrecursive scheme for the calculation of multiple contract modifications.\n"
    },
    {
        "paper_id": 1911.06193,
        "authors": "B. Shravan Kumar, Vadlamani Ravi and Rishabh Miglani",
        "title": "Predicting Indian stock market using the psycho-linguistic features of\n  financial news",
        "comments": "45 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
        "abstract": "  Financial forecasting using news articles is an emerging field. In this\npaper, we proposed hybrid intelligent models for stock market prediction using\nthe psycholinguistic variables (LIWC and TAALES) extracted from news articles\nas predictor variables. For prediction purpose, we employed various intelligent\ntechniques such as Multilayer Perceptron (MLP), Group Method of Data Handling\n(GMDH), General Regression Neural Network (GRNN), Random Forest (RF), Quantile\nRegression Random Forest (QRRF), Classification and regression tree (CART) and\nSupport Vector Regression (SVR). We experimented on the data of 12 companies\nstocks, which are listed in the Bombay Stock Exchange (BSE). We employed\nchi-squared and maximum relevance and minimum redundancy (MRMR) feature\nselection techniques on the psycho-linguistic features obtained from the new\narticles etc. After extensive experimentation, using the Diebold-Mariano test,\nwe conclude that GMDH and GRNN are statistically the best techniques in that\norder with respect to the MAPE and NRMSE values.\n"
    },
    {
        "paper_id": 1911.06206,
        "authors": "Niko Hauzenberger and Michael Pfarrhofer",
        "title": "Bayesian state-space modeling for analyzing heterogeneous network\n  effects of US monetary policy",
        "comments": "JEL: C11, C23, C32, C58, E52; Keywords: production networks, monetary\n  policy shocks, high-frequency identification, spatio-temporal modeling",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Understanding disaggregate channels in the transmission of monetary policy is\nof crucial importance for effectively implementing policy measures. We extend\nthe empirical econometric literature on the role of production networks in the\npropagation of shocks along two dimensions. First, we allow for\nindustry-specific responses that vary over time, reflecting non-linearities and\ncross-sectional heterogeneities in direct transmission channels. Second, we\nallow for time-varying network structures and dependence. This feature captures\nboth variation in the structure of the production network, but also differences\nin cross-industry demand elasticities. We find that impacts vary substantially\nover time and the cross-section. Higher-order effects appear to be particularly\nimportant in periods of economic and financial uncertainty, often coinciding\nwith tight credit market conditions and financial stress. Differentials in\nindustry-specific responses can be explained by how close the respective\nindustries are to end-consumers.\n"
    },
    {
        "paper_id": 1911.064,
        "authors": "Zeng-Xian Lin, Xiao Fan Liu",
        "title": "Tracking the circulation routes of fresh coins in Bitcoin: A way of\n  identifying coin miners with transaction network structural properties",
        "comments": "an English version of the journal article published in Chinese",
        "journal-ref": "Journal of Nanjing University of Information Science &\n  Technology(Natural Science Edition), 2018(4): 450-455",
        "doi": "10.13878/j.cnki.jnuist.2018.04.009",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Bitcoin draws the highest degree of attention among cryptocurrencies, while\ncoin mining is one of the most important fashion of profiting in the Bitcoin\necosystem. This paper constructs fresh coin circulation networks by tracking\nthe fresh coin transfer routes with transaction referencing in Bitcoin\nblockchain. This paper proposes a heuristic algorithm to identifying coin\nminers by comparing coin circulation networks from different mining pools and\nthereby inferring the common profit distribution schemes of Bitcoin mining\npools. Furthermore, this paper characterizes the increasing trend of Bitcoin\nminer numbers during recent years.\n"
    },
    {
        "paper_id": 1911.06552,
        "authors": "Dmytro Ivasiuk",
        "title": "An approximate solution for the power utility optimization under\n  predictable returns",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This work derives an approximate analytical single period solution of the\nportfolio choice problem for the power utility function. It is possible to do\nso if we consider that the asset returns follow a multivariate normal\ndistribution. It is shown in the literature that the log-normal distribution\nseems to be a good proxy of the normal distribution in case if the standard\ndeviation of the last one is way smaller than its mean. So we can use this\nproperty because this happens to be true for gross portfolio returns. In\naddition, we present a different solution method that relies on the machine\nlearning algorithm called Gradient Descent. It is a powerful tool to solve a\nwide range of problems, and it was possible to implement this approach to\nportfolio selection. Besides, the paper provides a simulation study, where we\ncompare the derived results with the well-known solution, which uses a Taylor\nseries expansion of the utility function.\n"
    },
    {
        "paper_id": 1911.06698,
        "authors": "Oleg Kolesnikov, Alexander Markov, Daulet Smagulov, Sergejs Solovjovs",
        "title": "Cyber bonds and their pricing models",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Motivated by the developments in cyber risk treatment in the finance\nindustry, we propose a general framework of cyber bond, whose main purpose is\nto insure (compensate) losses of a cyber attack. Based on a database of\npublicly available cyber events, we determine cyber loss distribution\nparameters and use them to numerically simulate cyber bond price, yield, and\nother characteristics. We also consider two possible approaches to cyber bond\ncoupon calculation.\n"
    },
    {
        "paper_id": 1911.06893,
        "authors": "Ravi Kashyap",
        "title": "Imitation in the Imitation Game",
        "comments": "arXiv admin note: substantial text overlap with arXiv:1907.04659,\n  arXiv:1703.08812",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We discuss the objectives of automation equipped with non-trivial decision\nmaking, or creating artificial intelligence, in the financial markets and\nprovide a possible alternative. Intelligence might be an unintended consequence\nof curiosity left to roam free, best exemplified by a frolicking infant. For\nthis unintentional yet welcome aftereffect to set in a foundational list of\nguiding principles needs to be present. A consideration of these requirements\nallows us to propose a test of intelligence for trading programs, on the lines\nof the Turing Test, long the benchmark for intelligent machines. We discuss the\napplication of this methodology to the dilemma in finance, which is whether,\nwhen and how much to Buy, Sell or Hold.\n"
    },
    {
        "paper_id": 1911.07288,
        "authors": "Lim Tze Yee, Tony She, Kezia Irene",
        "title": "Application of Principal Component Analysis in Chinese Sovereign Bond\n  Market and Principal Component-Based Fixed Income Immunization",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  This paper analyses the Chinese Sovereign bond yield to find out the\nprincipal factors affecting the term structure of interest rate changes. We\napply Principal Component Analysis (PCA) on our data consisting of the Chinese\nSovereign bond from January 2002 till May 2018 with the different yield to\nmaturity. Then we will discuss the multi-factor immunization model (method on\nhedging market risk) on a bond portfolio.\n"
    },
    {
        "paper_id": 1911.07313,
        "authors": "Daniel Ritter",
        "title": "Mathematical Modeling of Systemic Risk in Financial Networks: Managing\n  Default Contagion and Fire Sales",
        "comments": "PhD Thesis",
        "journal-ref": "Dissertation, LMU M\\\"unchen: Fakult\\\"at f\\\"ur Mathematik,\n  Informatik und Statistik, 2019:\n  http://nbn-resolving.de/urn:nbn:de:bvb:19-241619",
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  As impressively shown by the financial crisis in 2007/08, contagion effects\nin financial networks harbor a great threat for the stability of the entire\nsystem. Without sufficient capital requirements for banks and other financial\ninstitutions, shocks that are locally confined at first can spread through the\nentire system and be significantly amplified by various contagion channels. The\naim of this thesis is thus to investigate in detail two selected contagion\nchannels of this so-called systemic risk, provide mathematical models and\nderive consequences for the systemic risk management of financial institutions.\nThe first contagion channel we consider is default contagion. The underlying\neffect is here that insolvent institutions cannot service their debt or other\nfinancial obligations anymore - at least partially. Debtors and other directly\nimpacted parties in the system are thus forced to write off their losses and\ncan possibly be driven into insolvency themselves due to their incurred\nfinancial losses. This on the other hand starts a new round in the default\ncontagion process. In our model we simplistically describe each institution by\nall the financial positions it is exposed to as well as its initial capital. In\ndoing so, our starting point is the work of Detering et al. (2017) - a model\nfor contagion in unweighted networks - which particularly considers the exact\nnetwork configuration to be random and derives asymptotic results for large\nnetworks. We extend this model such that weighted networks can be considered\nand an application to financial networks becomes possible. More precisely, for\nany given initial shock we deduce an explicit asymptotic expression for the\ntotal damage caused in the system by contagion and provide a necessary and\nsufficient criterion for an unshocked financial system to be stable against\nsmall shocks. Moreover, ...\n"
    },
    {
        "paper_id": 1911.07526,
        "authors": "Shubhangi Sikaria, Rituparna Sen and Neelesh S. Upadhye",
        "title": "Bayesian Filtering for Multi-period Mean-Variance Portfolio Selection",
        "comments": "19 pages, 6 figures",
        "journal-ref": "2021 Journal of Statistical Theory and Practice, 15: 40",
        "doi": "10.1007/s42519-021-00175-2",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  For a long investment time horizon, it is preferable to rebalance the\nportfolio weights at intermediate times. This necessitates a multi-period\nmarket model in which portfolio optimization is usually done through dynamic\nprogramming. However, this assumes a known distribution for the parameters of\nthe financial time series. We consider the situation where this distribution is\nunknown and needs to be estimated from the data that is arriving dynamically.\nWe applied Bayesian filtering through dynamic linear models to sequentially\nupdate the parameters. We considered uncertain investment lifetime to make the\nmodel more adaptive to the market conditions. These updated parameters are put\ninto the dynamic mean-variance problem to arrive at optimal efficient\nportfolios. Extensive simulations are conducted to study the effect of varying\nunderlying parameters and investment horizon on the performance of the method.\nAn implementation of this model to the S&P500 illustrates that the Bayesian\nupdating is strongly favored by the data and that it is practically\nimplementable.\n"
    },
    {
        "paper_id": 1911.07719,
        "authors": "Eduardo Abi Jaber (CES, UP1 UFR27)",
        "title": "The Laplace transform of the integrated Volterra Wishart process",
        "comments": null,
        "journal-ref": "Mathematical Finance, 2022, 32 (1), pp.309-348",
        "doi": "10.1111/mafi.12334",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We establish an explicit expression for the conditional Laplace transform of\nthe integrated Volterra Wishart process in terms of a certain resolvent of the\ncovariance function. The core ingredient is the derivation of the conditional\nLaplace transform of general Gaussian processes in terms of Fredholm's\ndeterminant and resolvent. Furthermore , we link the characteristic exponents\nto a system of non-standard infinite dimensional matrix Riccati equations. This\nleads to a second representation of the Laplace transform for a special case of\nconvolution kernel. In practice, we show that both representations can be\napproximated by either closed form solutions of conventional Wishart\ndistributions or finite dimensional matrix Riccati equations stemming from\nconventional linear-quadratic models. This allows fast pricing in a variety of\nhighly flexible models, ranging from bond pricing in quadratic short rate\nmodels with rich autocorrelation structures, long range dependence and possible\ndefault risk, to pricing basket options with covariance risk in multivariate\nrough volatility models.\n"
    },
    {
        "paper_id": 1911.07773,
        "authors": "Rafael P. Greminger",
        "title": "Optimal Search and Discovery",
        "comments": "Added link to publication, fixed typos",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper studies a search problem where a consumer is initially aware of\nonly a few products. At every point in time, the consumer then decides between\nsearching among alternatives he is already aware of and discovering more\nproducts. I show that the optimal policy for this search and discovery problem\nis fully characterized by tractable reservation values. Moreover, I prove that\na predetermined index fully specifies the purchase decision of a consumer\nfollowing the optimal search policy. Finally, a comparison highlights\ndifferences to classical random and directed search.\n"
    },
    {
        "paper_id": 1911.08247,
        "authors": "Herb Kunze, Davide La Torre, Simone Marsiglio",
        "title": "A Multicriteria Macroeconomic Model with Intertemporal Equity and\n  Spatial Spillovers",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  We analyze a macroeconomic model with intergenerational equity considerations\nand spatial spillovers, which gives rise to a multicriteria optimization\nproblem. Intergenerational equity requires to add in the definition of social\nwelfare a long run sustainability criterion to the traditional discounted\nutilitarian criterion. The spatial structure allows for the possibility of\nheterogeneiity and spatial diffusion implies that all locations within the\nspatial domain are interconnected via spatial spillovers. We rely on different\ntechniques (scalarization, $\\epsilon$-constraint method and goal programming)\nto analyze such a spatial multicriteria problem, relying on numerical\napproaches to illustrate the nature of the trade-off between the discounted\nutilitarian and the sustainability criteria.\n"
    },
    {
        "paper_id": 1911.0826,
        "authors": "Susobhan Ghosh, Sujit Gujar, Praveen Paruchuri, Easwar Subramanian,\n  Sanjay P. Bhat",
        "title": "Bidding in Smart Grid PDAs: Theory, Analysis and Strategy (Extended\n  Version)",
        "comments": "Accepted for publication in the proceedings of the Thirty-Fourth AAAI\n  Conference on Artificial Intelligence (AAAI-20)",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Periodic Double Auctions (PDAs) are commonly used in the real world for\ntrading, e.g. in stock markets to determine stock opening prices, and energy\nmarkets to trade energy in order to balance net demand in smart grids,\ninvolving trillions of dollars in the process. A bidder, participating in such\nPDAs, has to plan for bids in the current auction as well as for the future\nauctions, which highlights the necessity of good bidding strategies. In this\npaper, we perform an equilibrium analysis of single unit single-shot double\nauctions with a certain clearing price and payment rule, which we refer to as\nACPR, and find it intractable to analyze as number of participating agents\nincrease. We further derive the best response for a bidder with complete\ninformation in a single-shot double auction with ACPR. Leveraging the theory\ndeveloped for single-shot double auction and taking the PowerTAC wholesale\nmarket PDA as our testbed, we proceed by modeling the PDA of PowerTAC as an\nMDP. We propose a novel bidding strategy, namely MDPLCPBS. We empirically show\nthat MDPLCPBS follows the equilibrium strategy for double auctions that we\npreviously analyze. In addition, we benchmark our strategy against the baseline\nand the state-of-the-art bidding strategies for the PowerTAC wholesale market\nPDAs, and show that MDPLCPBS outperforms most of them consistently.\n"
    },
    {
        "paper_id": 1911.08412,
        "authors": "Michael Roberts and Indranil SenGupta",
        "title": "Infinitesimal generators for two-dimensional L\\'evy process-driven\n  hypothesis testing",
        "comments": null,
        "journal-ref": "Annals of Finance, 2020",
        "doi": "10.1007/s10436-019-00355-y",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper, we present the testing of four hypotheses on two streams of\nobservations that are driven by L\\'evy processes. This is applicable for\nsequential decision making on the state of two-sensor systems. In one case,\neach sensor receives or does not receive a signal obstructed by noise. In\nanother, each sensor receives data-driven by L\\'evy processes with large or\nsmall jumps. In either case, these give rise to four possibilities.\nInfinitesimal generators are presented and analyzed. Bounds for infinitesimal\ngenerators in terms of \\emph{super-solutions} and \\emph{sub-solutions} are\ncomputed. An application of this procedure for the stochastic model is also\npresented in relation to the financial market.\n"
    },
    {
        "paper_id": 1911.08448,
        "authors": "Ivan Cherednik",
        "title": "Artificial intelligence approach to momentum risk-taking",
        "comments": "60 pages including 5 figures and various tables; v2: profit-taking,\n  Bessel and hypergeometric functions were added, editing; v3: connections to\n  fBM, random processes, references were added, editing; v4: further editing",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We propose a mathematical model of momentum risk-taking, which is essentially\nreal-time risk management focused on short-term volatility of stock markets.\nIts implementation, our fully automated momentum equity trading system\npresented systematically, proved to be successful in extensive historical and\nreal-time experiments. Momentum risk-taking is one of the key components of\ngeneral decision-making, a challenge for artificial intelligence and machine\nlearning with deep roots in cognitive science; its variants beyond stock\nmarkets are discussed. We begin with a new algebraic-type theory of news impact\non share-prices, which describes well their power growth, periodicity, and the\nmarket phenomena like price targets and profit-taking. This theory generally\nrequires Bessel and hypergeometric functions. Its discretization results in\nsome tables of bids, which are basically expected returns for main investment\nhorizons, the key in our trading system. The ML procedures we use are similar\nto those in neural networking. A preimage of our approach is the new contract\ncard game provided at the end, a combination of bridge and poker. Relations to\nrandom processes and the fractional Brownian motion are outlined.\n"
    },
    {
        "paper_id": 1911.08647,
        "authors": "Jonathan Sadighian",
        "title": "Deep Reinforcement Learning in Cryptocurrency Market Making",
        "comments": "19 pages, 12 figures, 5 tables",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper sets forth a framework for deep reinforcement learning as applied\nto market making (DRLMM) for cryptocurrencies. Two advanced policy\ngradient-based algorithms were selected as agents to interact with an\nenvironment that represents the observation space through limit order book\ndata, and order flow arrival statistics. Within the experiment, a forward-feed\nneural network is used as the function approximator and two reward functions\nare compared. The performance of each combination of agent and reward function\nis evaluated by daily and average trade returns. Using this DRLMM framework,\nthis paper demonstrates the effectiveness of deep reinforcement learning in\nsolving stochastic inventory control challenges market makers face.\n"
    },
    {
        "paper_id": 1911.08944,
        "authors": "Stanis{\\l}aw Dro\\.zd\\.z, Ludovico Minati, Pawe{\\l} O\\'swi\\k{e}cimka,\n  Marek Stanuszek, Marcin W\\k{a}torek",
        "title": "Competition of noise and collectivity in global cryptocurrency trading:\n  route to a self-contained market",
        "comments": null,
        "journal-ref": "Chaos 30, 023122 (2020)",
        "doi": "10.1063/1.5139634",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Cross-correlations in fluctuations of the daily exchange rates within the\nbasket of the 100 highest-capitalization cryptocurrencies over the period\nOctober 1, 2015, through March 31, 2019, are studied. The corresponding\ndynamics predominantly involve one leading eigenvalue of the correlation\nmatrix, while the others largely coincide with those of Wishart random\nmatrices. However, the magnitude of the principal eigenvalue, and thus the\ndegree of collectivity, strongly depends on which cryptocurrency is used as a\nbase. It is largest when the base is the most peripheral cryptocurrency; when\nmore significant ones are taken into consideration, its magnitude\nsystematically decreases, nevertheless preserving a sizable gap with respect to\nthe random bulk, which in turn indicates that the organization of correlations\nbecomes more heterogeneous. This finding provides a criterion for recognizing\nwhich currencies or cryptocurrencies play a dominant role in the global\ncrypto-market. The present study shows that over the period under\nconsideration, the Bitcoin (BTC) predominates, hallmarking exchange rate\ndynamics at least as influential as the US dollar. The BTC started dominating\naround the year 2017, while further cryptocurrencies, like the Ethereum (ETH)\nand even Ripple (XRP), assumed similar trends. At the same time, the USD, an\noriginal value determinant for the cryptocurrency market, became increasingly\ndisconnected, its related characteristics eventually approaching those of a\nfictitious currency. These results are strong indicators of incipient\nindependence of the global cryptocurrency market, delineating a self-contained\ntrade resembling the Forex.\n"
    },
    {
        "paper_id": 1911.09209,
        "authors": "Vasilios Mavroudis",
        "title": "Bounded Temporal Fairness for FIFO Financial Markets",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Financial exchange operators cater to the needs of their users while\nsimultaneously ensuring compliance with the financial regulations. In this\nwork, we focus on the operators' commitment for fair treatment of all competing\nparticipants. We first discuss unbounded temporal fairness and then investigate\nits implementation and infrastructure requirements for exchanges. We find that\nthese requirements can be fully met only under ideal conditions and argue that\nunbounded fairness in FIFO markets is unrealistic. To further support this\nclaim, we analyse several real-world incidents and show that subtle\nimplementation inefficiencies and technical optimizations suffice to give\nunfair advantages to a minority of the participants. We finally introduce,\n{\\epsilon}-fairness, a bounded definition of temporal fairness and discuss how\nit can be combined with non-continuous market designs to provide equal\nparticipant treatment with minimum divergence from the existing market\noperation.\n"
    },
    {
        "paper_id": 1911.09359,
        "authors": "Liu Guang and Wang Xiaojie and Li Ruifan",
        "title": "Multi-Scale RCNN Model for Financial Time-series Classification",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Financial time-series classification (FTC) is extremely valuable for\ninvestment management. In past decades, it draws a lot of attention from a wide\nextent of research areas, especially Artificial Intelligence (AI). Existing\nresearches majorly focused on exploring the effects of the Multi-Scale (MS)\nproperty or the Temporal Dependency (TD) within financial time-series.\nUnfortunately, most previous researches fail to combine these two properties\neffectively and often fall short of accuracy and profitability. To effectively\ncombine and utilize both properties of financial time-series, we propose a\nMulti-Scale Temporal Dependent Recurrent Convolutional Neural Network\n(MSTD-RCNN) for FTC. In the proposed method, the MS features are simultaneously\nextracted by convolutional units to precisely describe the state of the\nfinancial market. Moreover, the TD and complementary across different scales\nare captured through a Recurrent Neural Network. The proposed method is\nevaluated on three financial time-series datasets which source from the Chinese\nstock market. Extensive experimental results indicate that our model achieves\nthe state-of-the-art performance in trend classification and simulated trading,\ncompared with classical and advanced baseline models.\n"
    },
    {
        "paper_id": 1911.09441,
        "authors": "Sergey I. Nikulin, Olga S. Rozanova",
        "title": "Some analytically solvable problems of the mean-field games theory",
        "comments": "13 pages, 1 figure",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We study the mean field games equations, consisting of the coupled\nKolmogorov-Fokker-Planck and Hamilton-Jacobi-Bellman equations. The equations\nare complemented by initial and terminal conditions. It is shown that with some\nspecific choice of data, this problem can be reduced to solving a quadratically\nnonlinear system of ODEs. This situation occurs naturally in economic\napplications. As an example, the problem of forming an investor's opinion on an\nasset is considered.\n"
    },
    {
        "paper_id": 1911.09681,
        "authors": "Matata Ponyo Mapon and Jean-Paul K. Tsasa",
        "title": "The artefact of the Natural Resources Curse",
        "comments": "in French",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper reexamines the validity of the natural resource curse hypothesis,\nusing the database of mineral exporting countries. Our findings are as follows:\n(i) Resource-rich countries (RRCs) do not necessarily exhibit poor political,\neconomic and social performance; (ii) RRCs that perform poorly have a low\ndiversified exports portfolio; (iii) In contrast, RRCs with a low diversified\nexports portfolio do not necessarily perform poorly. Then, we develop a model\nof strategic interaction from a Bayesian game setup to study the role of\nleadership and governance in the management of natural resources. We show that\nan improvement in the leadership-governance binomial helps to discipline the\nbehavior of lobby groups (theorem 1) and generate a Pareto improvement in the\nmanagement of natural resources (theorem 2). Evidence from the World Bank\nGroup's CPIA data confirms the later finding. Our results remain valid after\nsome robustness checks.\n"
    },
    {
        "paper_id": 1911.09858,
        "authors": "Sheikh Rabiul Islam, William Eberle, Sheikh K. Ghafoor, Sid C. Bundy,\n  Douglas A. Talbert, and Ambareen Siraj",
        "title": "Investigating bankruptcy prediction models in the presence of extreme\n  class imbalance and multiple stages of economy",
        "comments": "Under review in Expert Systems with Applications",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In the area of credit risk analytics, current Bankruptcy Prediction Models\n(BPMs) struggle with (a) the availability of comprehensive and real-world data\nsets and (b) the presence of extreme class imbalance in the data (i.e., very\nfew samples for the minority class) that degrades the performance of the\nprediction model. Moreover, little research has compared the relative\nperformance of well-known BPM's on public datasets addressing the class\nimbalance problem. In this work, we apply eight classes of well-known BPMs, as\nsuggested by a review of decades of literature, on a new public dataset named\nFreddie Mac Single-Family Loan-Level Dataset with resampling (i.e., adding\nsynthetic minority samples) of the minority class to tackle class imbalance.\nAdditionally, we apply some recent AI techniques (e.g., tree-based ensemble\ntechniques) that demonstrate potentially better results on models trained with\nresampled data. In addition, from the analysis of 19 years (1999-2017) of data,\nwe discover that models behave differently when presented with sudden changes\nin the economy (e.g., a global financial crisis) resulting in abrupt\nfluctuations in the national default rate. In summary, this study should aid\npractitioners/researchers in determining the appropriate model with respect to\ndata that contains a class imbalance and various economic stages.\n"
    },
    {
        "paper_id": 1911.09985,
        "authors": "Aastha M. Sathe and N. S. Upadhye",
        "title": "Estimation of the Parameters of Symmetric Stable ARMA and ARMA-GARCH\n  Models",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this article, we first propose the modified Hannan-Rissanen Method for\nestimating the parameters of the autoregressive moving average (ARMA) process\nwith symmetric stable noise and symmetric stable generalized autoregressive\nconditional heteroskedastic (GARCH) noise. Next, we propose the modified\nempirical characteristic function method for the estimation of GARCH parameters\nwith symmetric stable noise. Further, we show the efficiency, accuracy, and\nsimplicity of our methods through Monte-Carlo simulation. Finally, we apply our\nproposed methods to model financial data.\n"
    },
    {
        "paper_id": 1911.10047,
        "authors": "John Armstrong and Cristin Buescu",
        "title": "Collectivised Pension Investment with Homogeneous Epstein-Zin\n  Preferences",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In a collectivised pension fund, investors agree that any money remaining in\nthe fund when they die can be shared among the survivors.\n  We compute analytically the optimal investment-consumption strategy for a\nfund of $n$ identical investors with homogeneous Epstein--Zin preferences,\ninvesting in the Black--Scholes market in continuous time but consuming in\ndiscrete time. Our result holds for arbitrary mortality distributions.\n  We also compute the optimal strategy for an infinite fund of investors, and\nprove the convergence of the optimal strategy as $n\\to \\infty$. The proof of\nconvergence shows that effective strategies for inhomogeneous funds can be\nobtained using the optimal strategies found in this paper for homogeneous\nfunds, using the results of [2].\n  We find that a constant consumption strategy is suboptimal even for infinite\ncollectives investing in markets where assets provide no return so long as\ninvestors are \"satisfaction risk-averse.\" This suggests that annuities and\ndefined benefit investments will always be suboptimal investments.\n  We present numerical results examining the importance of the fund size, $n$,\nand the market parameters.\n"
    },
    {
        "paper_id": 1911.10104,
        "authors": "Sheikh Rabiul Islam, William Eberle, Sheikh K. Ghafoor",
        "title": "Towards Quantification of Explainability in Explainable Artificial\n  Intelligence Methods",
        "comments": "Submitted to FLAIRS-33",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Artificial Intelligence (AI) has become an integral part of domains such as\nsecurity, finance, healthcare, medicine, and criminal justice. Explaining the\ndecisions of AI systems in human terms is a key challenge--due to the high\ncomplexity of the model, as well as the potential implications on human\ninterests, rights, and lives . While Explainable AI is an emerging field of\nresearch, there is no consensus on the definition, quantification, and\nformalization of explainability. In fact, the quantification of explainability\nis an open challenge. In our previous work, we incorporated domain knowledge\nfor better explainability, however, we were unable to quantify the extent of\nexplainability. In this work, we (1) briefly analyze the definitions of\nexplainability from the perspective of different disciplines (e.g., psychology,\nsocial science), properties of explanation, explanation methods, and\nhuman-friendly explanations; and (2) propose and formulate an approach to\nquantify the extent of explainability. Our experimental result suggests a\nreasonable and model-agnostic way to quantify explainability\n"
    },
    {
        "paper_id": 1911.10106,
        "authors": "Alex S.L. Tse, Harry Zheng",
        "title": "Speculative Trading, Prospect Theory and Transaction Costs",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  A speculative agent with Prospect Theory preference chooses the optimal time\nto purchase and then to sell an indivisible risky asset to maximize the\nexpected utility of the round-trip profit net of transaction costs. The\noptimization problem is formulated as a sequential optimal stopping problem and\nwe provide a complete characterization of the solution. Depending on the\npreference and market parameters, the optimal strategy can be \"buy and hold\",\n\"buy low sell high\", \"buy high sell higher\" or \"no trading\". Behavioral\npreference and market friction interact in a subtle way which yields surprising\nimplications on the agent's trading patterns. For example, increasing the\nmarket entry fee does not necessarily curb speculative trading, but instead it\nmay induce a higher reference point under which the agent becomes more\nrisk-seeking and in turn is more likely to trade.\n"
    },
    {
        "paper_id": 1911.10107,
        "authors": "Zihao Zhang, Stefan Zohren, and Stephen Roberts",
        "title": "Deep Reinforcement Learning for Trading",
        "comments": "16 pages, 3 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We adopt Deep Reinforcement Learning algorithms to design trading strategies\nfor continuous futures contracts. Both discrete and continuous action spaces\nare considered and volatility scaling is incorporated to create reward\nfunctions which scale trade positions based on market volatility. We test our\nalgorithms on the 50 most liquid futures contracts from 2011 to 2019, and\ninvestigate how performance varies across different asset classes including\ncommodities, equity indices, fixed income and FX markets. We compare our\nalgorithms against classical time series momentum strategies, and show that our\nmethod outperforms such baseline models, delivering positive profits despite\nheavy transaction costs. The experiments show that the proposed algorithms can\nfollow large market trends without changing positions and can also scale down,\nor hold, through consolidation periods.\n"
    },
    {
        "paper_id": 1911.10116,
        "authors": "Krishna Dasaratha, Kevin He",
        "title": "Aggregative Efficiency of Bayesian Learning in Networks",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  When individuals in a social network learn about an unknown state from\nprivate signals and neighbors' actions, the network structure often causes\ninformation loss. We consider rational agents and Gaussian signals in the\ncanonical sequential social-learning problem and ask how the network changes\nthe efficiency of signal aggregation. Rational actions in our model are\nlog-linear functions of observations and admit a signal-counting interpretation\nof accuracy. Networks where agents observe multiple neighbors but not their\ncommon predecessors confound information, and even a small amount of\nconfounding can lead to much lower accuracy. In a class of networks where\nagents move in generations and observe the previous generation, we quantify the\ninformation loss with an aggregative efficiency index. Aggregative efficiency\nis a simple function of network parameters: increasing in observations and\ndecreasing in confounding. Later generations contribute little additional\ninformation, even with arbitrarily large generations.\n"
    },
    {
        "paper_id": 1911.10149,
        "authors": "Francesca Biagini, Thomas Reitsam",
        "title": "Asset Price Bubbles in market models with proportional transaction costs",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We study asset price bubbles in market models with proportional transaction\ncosts $\\lambda\\in (0,1)$ and finite time horizon $T$ in the setting of [49]. By\nfollowing [28], we define the fundamental value $F$ of a risky asset $S$ as the\nprice of a super-replicating portfolio for a position terminating in one unit\nof the asset and zero cash. We then obtain a dual representation for the\nfundamental value by using the super-replication theorem of [50]. We say that\nan asset price has a bubble if its fundamental value differs from the ask-price\n$(1+\\lambda)S$. We investigate the impact of transaction costs on asset price\nbubbles and show that our model intrinsically includes the birth of a bubble.\n"
    },
    {
        "paper_id": 1911.10254,
        "authors": "Eric Benhamou, Beatrice Guez and Nicolas Paris1",
        "title": "Omega and Sharpe ratio",
        "comments": "10 pages, 5 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Omega ratio, defined as the probability-weighted ratio of gains over losses\nat a given level of expected return, has been advocated as a better performance\nindicator compared to Sharpe and Sortino ratio as it depends on the full return\ndistribution and hence encapsulates all information about risk and return. We\ncompute Omega ratio for the normal distribution and show that under some\ndistribution symmetry assumptions, the Omega ratio is oversold as it does not\nprovide any additional information compared to Sharpe ratio. Indeed, for\nreturns that have elliptic distributions, we prove that the optimal portfolio\naccording to Omega ratio is the same as the optimal portfolio according to\nSharpe ratio. As elliptic distributions are a weak form of symmetric\ndistributions that generalized Gaussian distributions and encompass many fat\ntail distributions, this reduces tremendously the potential interest for the\nOmega ratio.\n"
    },
    {
        "paper_id": 1911.10297,
        "authors": "Pawel Dlotko, Wanling Qiu, Simon Rudkin",
        "title": "Financial ratios and stock returns reappraised through a topological\n  data analysis lens",
        "comments": "23 Pages, Preliminary Draft",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Firm financials are well established as return predictors, being the\ninspiration for a large set of anomalies in the asset pricing literature.\nEmploying topological data analysis we revisit the question of association\nbetween seven of the most commonly studied financial ratios and stock returns.\nSpecifically the TDA Ball Mapper algorithm is applied to visualise the point\ncloud of financial ratios as an abstract two-dimensional graph readily allowing\nfor identification of interdependencies between factors. These relationships\nare seldom monotonic, opportunities for investors to profitably exploit this\nknowledge provided by TDA abound. Clear potential offered by the tools of TDA\nto shed new light on asset pricing models is demonstrated. Scope for benefit is\nlimited only by the availability of information to the analyst.\n"
    },
    {
        "paper_id": 1911.1045,
        "authors": "Haipeng Xing",
        "title": "A singular stochastic control approach for optimal pairs trading with\n  proportional transaction costs",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Optimal trading strategies for pairs trading have been studied by models that\ntry to find either optimal shares of stocks by assuming no transaction costs or\noptimal timing of trading fixed numbers of shares of stocks with transaction\ncosts. To find optimal strategies which determine optimally both trade times\nand number of shares in pairs trading process, we use a singular stochastic\ncontrol approach to study an optimal pairs trading problem with proportional\ntransaction costs. Assuming a cointegrated relationship for a pair of stock\nlog-prices, we consider a portfolio optimization problem which involves dynamic\ntrading strategies with proportional transaction costs. We show that the value\nfunction of the control problem is the unique viscosity solution of a nonlinear\nquasi-variational inequality, which is equivalent to a free boundary problem\nfor the singular stochastic control value function. We then develop a discrete\ntime dynamic programming algorithm to compute the transaction regions, and show\nthe convergence of the discretization scheme. We illustrate our approach with\nnumerical examples and discuss the impact of different parameters on\ntransaction regions. We study the out-of-sample performance in an empirical\nstudy that consists of six pairs of U.S. stocks selected from different\nindustry sectors, and demonstrate the efficiency of the optimal strategy.\n"
    },
    {
        "paper_id": 1911.10948,
        "authors": "Mariano Zeron-Medina Laris, Ignacio Ruiz",
        "title": "Denting the FRTB IMA computational challenge via Orthogonal Chebyshev\n  Sliding Technique",
        "comments": "A version of this paper has been peer-reviewed and will be published\n  in Wilmott Magazine in January 2021",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper we introduce a new technique based on high-dimensional\nChebyshev Tensors that we call \\emph{Orthogonal Chebyshev Sliding Technique}.\nWe implemented this technique inside the systems of a tier-one bank, and used\nit to approximate Front Office pricing functions in order to reduce the\nsubstantial computational burden associated with the capital calculation as\nspecified by FRTB IMA. In all cases, the computational burden reductions\nobtained were of more than $90\\%$, while keeping high degrees of accuracy, the\nlatter obtained as a result of the mathematical properties enjoyed by Chebyshev\nTensors.\n"
    },
    {
        "paper_id": 1911.10972,
        "authors": "Andrew Schaug and Harish Chandra",
        "title": "On unbiased simulations of stochastic bridges conditioned on extrema",
        "comments": "24 pages, 26 images",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Stochastic bridges are commonly used to impute missing data with a lower\nsampling rate to generate data with a higher sampling rate, while preserving\nkey properties of the dynamics involved in an unbiased way. While the\ngeneration of Brownian bridges and Ornstein-Uhlenbeck bridges is well\nunderstood, unbiased generation of such stochastic bridges subject to a given\nextremum has been less explored in the literature. After a review of known\nresults, we compare two algorithms for generating Brownian bridges constrained\nto a given extremum, one of which generalises to other diffusions. We further\napply this to generate unbiased Ornstein-Uhlenbeck bridges and unconstrained\nprocesses, both constrained to a given extremum, along with more tractable\nnumerical approximations of these algorithms. Finally, we consider the case of\ndrift, and applications to geometric Brownian motions.\n"
    },
    {
        "paper_id": 1911.11205,
        "authors": "Fernando C\\'ordova-Lepe",
        "title": "Income growth with high inequality implies loss of well-being: A\n  mathematical model",
        "comments": "in Spanish",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  A mathematical model of measurement of the perception of well-being for\ngroups with increasing incomes, but proportionally unequal is proposed.\nAssuming that welfare grows with own income and decreases with relative\ninequality (income of the other concerning one's own), possible scenarios for\nlong-term behavior in welfare functions are concluded. Also, it is proved that\na high relative inequality (parametric definition) always implies the loss of\nthe self-perception of the well-being of the most disadvantaged group.\n"
    },
    {
        "paper_id": 1911.11226,
        "authors": "Anshul Verma, Orazio Angelini, Tiziana Di Matteo",
        "title": "A new set of cluster driven composite development indicators",
        "comments": "Accepted in EPJ Data Science",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Composite development indicators used in policy making often subjectively\naggregate a restricted set of indicators. We show, using dimensionality\nreduction techniques, including Principal Component Analysis (PCA) and for the\nfirst time information filtering and hierarchical clustering, that these\ncomposite indicators miss key information on the relationship between different\nindicators. In particular, the grouping of indicators via topics is not\nreflected in the data at a global and local level. We overcome these issues by\nusing the clustering of indicators to build a new set of cluster driven\ncomposite development indicators that are objective, data driven, comparable\nbetween countries, and retain interpretabilty. We discuss their consequences on\ninforming policy makers about country development, comparing them with the top\nPageRank indicators as a benchmark. Finally, we demonstrate that our new set of\ncomposite development indicators outperforms the benchmark on a dataset\nreconstruction task.\n"
    },
    {
        "paper_id": 1911.11362,
        "authors": "Vikranth Lokeshwar, Vikram Bhardawaj, Shashi Jain",
        "title": "Neural network for pricing and universal static hedging of contingent\n  claims",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We present here a regress later based Monte Carlo approach that uses neural\nnetworks for pricing high-dimensional contingent claims. The choice of specific\narchitecture of the neural networks used in the proposed algorithm provides for\ninterpretability of the model, a feature that is often desirable in the\nfinancial context. Specifically, the interpretation leads us to demonstrate\nthat any contingent claim -- possibly high dimensional and path-dependent --\nunder the Markovian and the no-arbitrage assumptions, can be semi-statically\nhedged using a portfolio of short maturity options. We show how the method can\nbe used to obtain an upper and lower bound to the true price, where the lower\nbound is obtained by following a sub-optimal policy, while the upper bound by\nexploiting the dual formulation. Unlike other duality based upper bounds where\none typically has to resort to nested simulation for constructing\nsuper-martingales, the martingales in the current approach come at no extra\ncost, without the need for any sub-simulations. We demonstrate through\nnumerical examples the simplicity and efficiency of the method for both pricing\nand semi-static hedging of path-dependent options\n"
    },
    {
        "paper_id": 1911.11475,
        "authors": "Will Hicks",
        "title": "Closed Quantum Black-Scholes: Quantum Drift and the Heisenberg Equation\n  of Motion",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this article we model a financial derivative price as an observable on the\nmarket state function. We apply geometric techniques to integrating the\nHeisenberg Equation of Motion. We illustrate how the non-commutative nature of\nthe model introduces quantum interference effects that can act as either a drag\nor a boost on the resulting return. The ultimate objective is to investigate\nthe nature of quantum drift in the Accardi-Boukas quantum Black-Scholes\nframework which involves modelling the financial market as a quantum\nobservable, and introduces randomness through the Hudson-Parthasarathy quantum\nstochastic calculus. In particular we aim to differentiate randomness that is\nintroduced through external noise (quantum stochastic calculus) and randomness\nthat is fundamental to a quantum system (Heisenberg Equation of Motion).\n"
    },
    {
        "paper_id": 1911.11501,
        "authors": "Masaaki Fujii",
        "title": "Probabilistic Approach to Mean Field Games and Mean Field Type Control\n  Problems with Multiple Populations",
        "comments": "Forthcoming in Minimax Theory and its Applications. 50 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this work, we systematically investigate mean field games and mean field\ntype control problems with multiple populations using a coupled system of\nforward-backward stochastic differential equations of McKean-Vlasov type\nstemming from Pontryagin's stochastic maximum principle. Although the same cost\nfunctions as well as the coefficient functions of the state dynamics are shared\namong the agents within each population, they can be different population by\npopulation. We study the mean field limit for the three different situations;\n(i) every agent is non-cooperative; (ii) the agents within each population are\ncooperative; and (iii) the agents in some populations are cooperative but those\nin the other populations are not. We provide several sets of sufficient\nconditions for the existence of a mean field equilibrium for each of these\ncases. Furthermore, under appropriate conditions, we show that the mean field\nsolution to each of these problems actually provides an approximate Nash\nequilibrium for the corresponding game with a large but finite number of\nagents.\n"
    },
    {
        "paper_id": 1911.11569,
        "authors": "Husna Betul Coskun and Huseyin Coskun",
        "title": "Direct and indirect transactions and requirements",
        "comments": "44 pages, 6 figures, 16 tables",
        "journal-ref": null,
        "doi": "10.31219/osf.io/w2a4d",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The indirect transactions between sectors of an economic system has been a\nlong-standing open problem. There have been numerous attempts to conceptually\ndefine and mathematically formulate this notion in various other scientific\nfields in literature as well. The existing direct and indirect effects\nformulations, however, can neither determine the direct and indirect\ntransactions separately nor quantify these transactions between two individual\nsectors of interest in a multisectoral economic system. The novel concepts of\nthe direct, indirect and transfer (total) transactions between any two sectors\nare introduced, and the corresponding requirements matrices and coefficients\nare systematically formulated relative to both final demands and gross outputs\nbased on the system decomposition theory in the present manuscript. It is\ndemonstrated theoretically and through illustrative examples that the proposed\nrequirements matrices accurately define and correctly quantify the\ncorresponding direct, indirect, and total interactions and relationships. The\nproposed requirements matrices for the US economy using aggregated input-output\ntables for multiple years are then presented and briefly analyzed.\n"
    },
    {
        "paper_id": 1911.11819,
        "authors": "David Zhao, Alessandro Rinaldo, Christopher Brookins",
        "title": "Cryptocurrency Price Prediction and Trading Strategies Using Support\n  Vector Machines",
        "comments": "Corrected typos",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Few assets in financial history have been as notoriously volatile as\ncryptocurrencies. While the long term outlook for this asset class remains\nunclear, we are successful in making short term price predictions for several\nmajor crypto assets. Using historical data from July 2015 to November 2019, we\ndevelop a large number of technical indicators to capture patterns in the\ncryptocurrency market. We then test various classification methods to forecast\nshort-term future price movements based on these indicators. On both PPV and\nNPV metrics, our classifiers do well in identifying up and down market moves\nover the next 1 hour. Beyond evaluating classification accuracy, we also\ndevelop a strategy for translating 1-hour-ahead class predictions into trading\ndecisions, along with a backtester that simulates trading in a realistic\nenvironment. We find that support vector machines yield the most profitable\ntrading strategies, which outperform the market on average for Bitcoin,\nEthereum and Litecoin over the past 22 months, since January 2018.\n"
    },
    {
        "paper_id": 1911.1188,
        "authors": "Yinheng Li, Junhao Wang, Yijie Cao",
        "title": "A General Framework on Enhancing Portfolio Management with Reinforcement\n  Learning",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Portfolio management is the art and science in fiance that concerns\ncontinuous reallocation of funds and assets across financial instruments to\nmeet the desired returns to risk profile. Deep reinforcement learning (RL) has\ngained increasing interest in portfolio management, where RL agents are trained\nbase on financial data to optimize the asset reallocation process. Though there\nare prior efforts in trying to combine RL and portfolio management, previous\nworks did not consider practical aspects such as transaction costs or short\nselling restrictions, limiting their applicability. To address these\nlimitations, we propose a general RL framework for asset management that\nenables continuous asset weights, short selling and making decisions with\nrelevant features. We compare the performance of three different RL algorithms:\nPolicy Gradient with Actor-Critic (PGAC), Proximal Policy Optimization (PPO),\nand Evolution Strategies (ES) and demonstrate their advantages in a simulated\nenvironment with transaction costs. Our work aims to provide more options for\nutilizing RL frameworks in real-life asset management scenarios and can benefit\nfurther research in financial applications.\n"
    },
    {
        "paper_id": 1911.11971,
        "authors": "Kristoffer Glover",
        "title": "With or without replacement? Sampling uncertainty in Shepp's urn scheme",
        "comments": "14 pages, 3 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We introduce a variant of Shepp's classical urn problem in which the optimal\nstopper does not know whether sampling from the urn is done with or without\nreplacement. By considering the problem's continuous-time analog, we provide\nbounds on the value function and in the case of a balanced urn (with an equal\nnumber of each ball type) an explicit solution is found. Surprisingly, the\noptimal strategy for the balanced urn is the same as in the classical urn\nproblem.\n"
    },
    {
        "paper_id": 1911.12231,
        "authors": "Bernhard Hientzsch",
        "title": "Introduction to Solving Quant Finance Problems with Time-Stepped FBSDE\n  and Deep Learning",
        "comments": "24 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this introductory paper, we discuss how quantitative finance problems\nunder some common risk factor dynamics for some common instruments and\napproaches can be formulated as time-continuous or time-discrete\nforward-backward stochastic differential equations (FBSDE) final-value or\ncontrol problems, how these final value problems can be turned into control\nproblems, how time-continuous problems can be turned into time-discrete\nproblems, and how the forward and backward stochastic differential equations\n(SDE) can be time-stepped. We obtain both forward and backward time-stepped\ntime-discrete stochastic control problems (where forward and backward indicate\nin which direction the Y SDE is time-stepped) that we will solve with\noptimization approaches using deep neural networks for the controls and\nstochastic gradient and other deep learning methods for the actual\noptimization/learning. We close with examples for the forward and backward\nmethods for an European option pricing problem. Several methods and approaches\nare new.\n"
    },
    {
        "paper_id": 1911.12469,
        "authors": "Koichi Miyamoto, Kenji Shiohara",
        "title": "Reduction of Qubits in Quantum Algorithm for Monte Carlo Simulation by\n  Pseudo-random Number Generator",
        "comments": "14 pages, 12 figures",
        "journal-ref": "Phys. Rev. A 102, 022424 (2020)",
        "doi": "10.1103/PhysRevA.102.022424",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  It is known that quantum computers can speed up Monte Carlo simulation\ncompared to classical counterparts. There are already some proposals of\napplication of the quantum algorithm to practical problems, including\nquantitative finance. In many problems in finance to which Monte Carlo\nsimulation is applied, many random numbers are required to obtain one sample\nvalue of the integrand, since those problems are extremely high-dimensional\nintegrations, for example, risk measurement of credit portfolio. This leads to\nthe situation that the required qubit number is too large in the naive\nimplementation where a quantum register is allocated per random number. In this\npaper, we point out that we can reduce qubits keeping quantum speed up if we\nperform calculation similar to classical one, that is, estimate the average of\nintegrand values sampled by a pseudo-random number generator (PRNG) implemented\non a quantum circuit. We present not only the overview of the idea but also\nconcrete implementation of PRNG and application to credit risk measurement.\nActually, reduction of qubits is a trade-off against increase of circuit depth.\nTherefore full reduction might be impractical, but such a trade-off between\nspeed and memory space will be important in adjustment of calculation setting\nconsidering machine specs, if large-scale Monte Carlo simulation by quantum\ncomputer is in operation in the future.\n"
    },
    {
        "paper_id": 1911.1254,
        "authors": "Ehsan Hoseinzade, Saman Haratizadeh, Arash Khoeini",
        "title": "U-CNNpred: A Universal CNN-based Predictor for Stock Markets",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The performance of financial market prediction systems depends heavily on the\nquality of features it is using. While researchers have used various techniques\nfor enhancing the stock specific features, less attention has been paid to\nextracting features that represent general mechanism of financial markets. In\nthis paper, we investigate the importance of extracting such general features\nin stock market prediction domain and show how it can improve the performance\nof financial market prediction. We present a framework called U-CNNpred, that\nuses a CNN-based structure. A base model is trained in a specially designed\nlayer-wise training procedure over a pool of historical data from many\nfinancial markets, in order to extract the common patterns from different\nmarkets. Our experiments, in which we have used hundreds of stocks in S\\&P 500\nas well as 14 famous indices around the world, show that this model can\noutperform baseline algorithms when predicting the directional movement of the\nmarkets for which it has been trained for. We also show that the base model can\nbe fine-tuned for predicting new markets and achieve a better performance\ncompared to the state of the art baseline algorithms that focus on constructing\nmarket-specific models from scratch.\n"
    },
    {
        "paper_id": 1911.12582,
        "authors": "Wanling Rudkin, Charlie X Cai",
        "title": "Reaction Asymmetries to Social Responsibility Index Recomposition: A\n  Matching Portfolio Approach",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Listing on the Dow Jones Sustainability Index is seen as a gold-standard,\nverifying to the market that a firm is fully engaged with a corporate social\nresponsibility agenda. Robustly quantifying the impact of listing, and\nde-listing, against any industry level shocks, as well as evolution in the\ncompetitive relationship between firms within the industry, provides a strength\nabsent in existing works. It is shown that cumulative abnormal returns on\nstocks added to the index are significantly positive in the three trading weeks\nprior to the official announcement. The post-listing correction result posited\nto date is also demonstrated to hold; the proportion of periods with\nsignificant negative returns is low, however. Announcement, rather than\neffective dates are critical to returns. Differentials between these stages in\nthe chronology is an important contribution of this paper. Most effects end\nbefore the membership changes become effective. Whilst there are considerable\ngains to be made, they come pre-announcement date and require foresight to\nexploit. Investors must research likely new members to gain maximum return.\n"
    },
    {
        "paper_id": 1911.12596,
        "authors": "Peiwan Wang, Lu Zong and Ye Ma",
        "title": "An Integrated Early Warning System for Stock Market Turbulence",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This study constructs an integrated early warning system (EWS) that\nidentifies and predicts stock market turbulence. Based on switching ARCH\n(SWARCH) filtering probabilities of the high volatility regime, the proposed\nEWS first classifies stock market crises according to an indicator function\nwith thresholds dynamically selected by the two-peak method. A hybrid algorithm\nis then developed in the framework of a long short-term memory (LSTM) network\nto make daily predictions that alert turmoils. In the empirical evaluation\nbased on ten-year Chinese stock data, the proposed EWS yields satisfying\nresults with the test-set accuracy of $96.6\\%$ and on average $2.4$ days of the\nforewarned period. The model's stability and practical value in real-time\ndecision-making are also proven by the cross-validation and back-testing.\n"
    },
    {
        "paper_id": 1911.12623,
        "authors": "Cl\\'emence Alasseur, Heythem Farhat and Marcelo Saguan",
        "title": "A Principal-Agent approach to Capacity Remuneration Mechanisms",
        "comments": "44 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We propose to study electricity capacity remuneration mechanism design\nthrough a Principal-Agent approach. The Principal represents the aggregation of\nelectricity consumers (or a representative entity), subject to the physical\nrisk of shortage, and the Agent represents the electricity capacity owners, who\ninvest in capacity and produce electricity to satisfy consumers' demand, and\nare subject to financial risks. Following the methodology of Cvitanic et al.\n(2017), we propose an optimal contract, from consumers' perspective, which\ncomplements the revenue capacity owners achieved from the spot energy market,\nand incentivizes both parties to perform an optimal level of investments while\nsharing the physical and financial risks. Numerical results provide insights on\nthe necessity of a capacity remuneration mechanism and also show how this is\nespecially true when the level of uncertainties on demand or production side\nincreases.\n"
    },
    {
        "paper_id": 1911.12816,
        "authors": "Mahmoud Mahfouz, Angelos Filos, Cyrine Chtourou, Joshua Lockhart,\n  Samuel Assefa, Manuela Veloso, Danilo Mandic, Tucker Balch",
        "title": "On the Importance of Opponent Modeling in Auction Markets",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The dynamics of financial markets are driven by the interactions between\nparticipants, as well as the trading mechanisms and regulatory frameworks that\ngovern these interactions. Decision-makers would rather not ignore the impact\nof other participants on these dynamics and should employ tools and models that\ntake this into account. To this end, we demonstrate the efficacy of applying\nopponent-modeling in a number of simulated market settings. While our\nsimulations are simplified representations of actual market dynamics, they\nprovide an idealized \"playground\" in which our techniques can be demonstrated\nand tested. We present this work with the aim that our techniques could be\nrefined and, with some effort, scaled up to the full complexity of real-world\nmarket scenarios. We hope that the results presented encourage practitioners to\nadopt opponent-modeling methods and apply them online systems, in order to\nenable not only reactive but also proactive decisions to be made.\n"
    },
    {
        "paper_id": 1911.12944,
        "authors": "Jaehyun Kim, Hyungbin Park, Jonghwa Park",
        "title": "Pricing and hedging short-maturity Asian options in local volatility\n  models",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper discusses the short-maturity behavior of Asian option prices and\nhedging portfolios. We consider the risk-neutral valuation and the delta value\nof the Asian option having a H\\\"older continuous payoff function in a local\nvolatility model. The main idea of this analysis is that the local volatility\nmodel can be approximated by a Gaussian process at short maturity $T.$ By\ncombining this approximation argument with Malliavin calculus, we conclude that\nthe short-maturity behaviors of Asian option prices and the delta values are\napproximately expressed as those of their European counterparts with volatility\n$$\\sigma_{A}(T):=\\sqrt{\\frac{1}{T^3}\\int_0^T\\sigma^2(t,S_0)(T-t)^2\\,dt}\\,,$$\nwhere $\\sigma(\\cdot,\\cdot)$ is the local volatility function and $S_0$ is the\ninitial value of the stock. In addition, we show that the convergence rate of\nthe approximation is determined by the H\\\"older exponent of the payoff\nfunction. Finally, the short-maturity asymptotics of Asian call and put options\nare discussed from the viewpoint of the large deviation principle.\n"
    },
    {
        "paper_id": 1911.12969,
        "authors": "Ulrich Horst and Wei Xu",
        "title": "The Microstructure of Stochastic Volatility Models with Self-Exciting\n  Jump Dynamics",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We provide a general probabilistic framework within which we establish\nscaling limits for a class of continuous-time stochastic volatility models with\nself-exciting jump dynamics. In the scaling limit, the joint dynamics of asset\nreturns and volatility is driven by independent Gaussian white noises and two\nindependent Poisson random measures that capture the arrival of exogenous\nshocks and the arrival of self-excited shocks, respectively. Various\nwell-studied stochastic volatility models with and without self-exciting\nprice/volatility co-jumps are obtained as special cases under different scaling\nregimes. We analyze the impact of external shocks on the market dynamics,\nespecially their impact on jump cascades and show in a mathematically rigorous\nmanner that many small external shocks may tigger endogenous jump cascades in\nasset returns and stock price volatility.\n"
    },
    {
        "paper_id": 1911.13123,
        "authors": "Jaehyuk Choi and Lixin Wu",
        "title": "The equivalent constant-elasticity-of-variance (CEV) volatility of the\n  stochastic-alpha-beta-rho (SABR) model",
        "comments": null,
        "journal-ref": "Journal of Economic Dynamics and Control, 128:104143, 2021",
        "doi": "10.1016/j.jedc.2021.104143",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This study presents new analytic approximations of the\nstochastic-alpha-beta-rho (SABR) model. Unlike existing studies that focus on\nthe equivalent Black-Scholes (BS) volatility, we instead derive the equivalent\nconstant-elasticity-of-variance (CEV) volatility. Our approach effectively\nreduces the approximation error in a way similar to the control variate method\nbecause the CEV model is the zero vol-of-vol limit of the SABR model. Moreover,\nthe CEV volatility approximation yields a finite value at a zero strike and\nthus conveniently leads to a small-time asymptotics for the mass at zero. The\nnumerical results compare favorably with the BS volatility approximations in\nterms of the approximation accuracy, small-strike volatility asymptotics, and\nno-arbitrage region.\n"
    },
    {
        "paper_id": 1911.13288,
        "authors": "Omer Berat Sezer, Mehmet Ugur Gudelek, Ahmet Murat Ozbayoglu",
        "title": "Financial Time Series Forecasting with Deep Learning : A Systematic\n  Literature Review: 2005-2019",
        "comments": "13 figures, 13 tables, submitted to Applied Soft Computing",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Financial time series forecasting is, without a doubt, the top choice of\ncomputational intelligence for finance researchers from both academia and\nfinancial industry due to its broad implementation areas and substantial\nimpact. Machine Learning (ML) researchers came up with various models and a\nvast number of studies have been published accordingly. As such, a significant\namount of surveys exist covering ML for financial time series forecasting\nstudies. Lately, Deep Learning (DL) models started appearing within the field,\nwith results that significantly outperform traditional ML counterparts. Even\nthough there is a growing interest in developing models for financial time\nseries forecasting research, there is a lack of review papers that were solely\nfocused on DL for finance. Hence, our motivation in this paper is to provide a\ncomprehensive literature review on DL studies for financial time series\nforecasting implementations. We not only categorized the studies according to\ntheir intended forecasting implementation areas, such as index, forex,\ncommodity forecasting, but also grouped them based on their DL model choices,\nsuch as Convolutional Neural Networks (CNNs), Deep Belief Networks (DBNs),\nLong-Short Term Memory (LSTM). We also tried to envision the future for the\nfield by highlighting the possible setbacks and opportunities, so the\ninterested researchers can benefit.\n"
    },
    {
        "paper_id": 1911.133,
        "authors": "Indranil SenGupta, William Nganje, Erik Hanson",
        "title": "Refinements of Barndorff-Nielsen and Shephard model: an analysis of\n  crude oil price with machine learning",
        "comments": null,
        "journal-ref": "Annals of Data Science, 2021",
        "doi": "10.1007/s40745-020-00256-2",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  A commonly used stochastic model for derivative and commodity market analysis\nis the Barndorff-Nielsen and Shephard (BN-S) model. Though this model is very\nefficient and analytically tractable, it suffers from the absence of long range\ndependence and many other issues. For this paper, the analysis is restricted to\ncrude oil price dynamics. A simple way of improving the BN-S model with the\nimplementation of various machine learning algorithms is proposed. This refined\nBN-S model is more efficient and has fewer parameters than other models which\nare used in practice as improvements of the BN-S model. The procedure and the\nmodel show the application of data science for extracting a \"deterministic\ncomponent\" out of processes that are usually considered to be completely\nstochastic. Empirical applications validate the efficacy of the proposed model\nfor long range dependence.\n"
    },
    {
        "paper_id": 1912.00011,
        "authors": "Jaelle Scheuerman, Jason L. Harman, Nicholas Mattei, K. Brent Venable",
        "title": "Heuristic Strategies in Uncertain Approval Voting Environments",
        "comments": "arXiv admin note: text overlap with arXiv:1905.12104",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In many collective decision making situations, agents vote to choose an\nalternative that best represents the preferences of the group. Agents may\nmanipulate the vote to achieve a better outcome by voting in a way that does\nnot reflect their true preferences. In real world voting scenarios, people\noften do not have complete information about other voter preferences and it can\nbe computationally complex to identify a strategy that will maximize their\nexpected utility. In such situations, it is often assumed that voters will vote\ntruthfully rather than expending the effort to strategize. However, being\ntruthful is just one possible heuristic that may be used. In this paper, we\nexamine the effectiveness of heuristics in single winner and multi-winner\napproval voting scenarios with missing votes. In particular, we look at\nheuristics where a voter ignores information about other voting profiles and\nmakes their decisions based solely on how much they like each candidate. In a\nbehavioral experiment, we show that people vote truthfully in some situations\nand prioritize high utility candidates in others. We examine when these\nbehaviors maximize expected utility and show how the structure of the voting\nenvironment affects both how well each heuristic performs and how humans employ\nthese heuristics.\n"
    },
    {
        "paper_id": 1912.00244,
        "authors": "Tao Chen and Michael Ludkovski",
        "title": "A Machine Learning Approach to Adaptive Robust Utility Maximization and\n  Hedging",
        "comments": "33 pages, 24 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We investigate the adaptive robust control framework for portfolio\noptimization and loss-based hedging under drift and volatility uncertainty.\nAdaptive robust problems offer many advantages but require handling a double\noptimization problem (infimum over market measures, supremum over the control)\nat each instance. Moreover, the underlying Bellman equations are intrinsically\nmulti-dimensional. We propose a novel machine learning approach that solves for\nthe local saddle-point at a chosen set of inputs and then uses a nonparametric\n(Gaussian process) regression to obtain a functional representation of the\nvalue function. Our algorithm resembles control randomization and regression\nMonte Carlo techniques but also brings multiple innovations, including adaptive\nexperimental design, separate surrogates for optimal control and the local\nworst-case measure, and computational speed-ups for the sup-inf optimization.\nThanks to the new scheme we are able to consider settings that have been\npreviously computationally intractable and provide several new financial\ninsights about learning and optimal trading under unknown market parameters. In\nparticular, we demonstrate the financial advantages of adaptive robust\nframework compared to adaptive and static robust alternatives.\n"
    },
    {
        "paper_id": 1912.00269,
        "authors": "Tommi Ekholm",
        "title": "Optimal forest rotation under carbon pricing and forest damage risk",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Forests will have two notable economic roles in the future: providing\nrenewable raw material and storing carbon to mitigate climate change. The\npricing of forest carbon leads to longer rotation times and consequently larger\ncarbon stocks, but also exposes landowners to a greater risk of forest damage.\nThis paper investigates optimal forest rotation under carbon pricing and forest\ndamage risk. I provide the optimality conditions for this problem and\nillustrate the setting with numerical calculations representing boreal forests\nunder a range of carbon prices and damage probabilities. The relation between\ndamage probability and carbon price towards the optimal rotation length is\nnearly linear, with carbon pricing having far greater impact. As such,\nincreasing forest carbon stocks by lengthening rotations is an economically\nattractive method for climate change mitigation, despite the forest damage\nrisk. Carbon pricing also increases land expectation value and reduces the\neconomic risks of the landowner. The production possibility frontier under\noptimal rotation suggests that significantly larger forests carbon stocks are\nachievable, but imply lower harvests. However, forests' societally optimal role\nbetween these two activities is not yet clear-cut; but rests on the future\ndevelopment of relative prices between timber, carbon and other commodities\ndependent on land-use.\n"
    },
    {
        "paper_id": 1912.00359,
        "authors": "Antoine Fosset, Jean-Philippe Bouchaud, and Michael Benzaquen",
        "title": "Endogenous Liquidity Crises",
        "comments": "21 pages, 11 figures, 1 table",
        "journal-ref": null,
        "doi": "10.1088/1742-5468/ab7c64",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Empirical data reveals that the liquidity flow into the order book\n(depositions, cancellations andmarket orders) is influenced by past price\nchanges. In particular, we show that liquidity tends todecrease with the\namplitude of past volatility and price trends. Such a feedback mechanism inturn\nincreases the volatility, possibly leading to a liquidity crisis. Accounting\nfor such effects withina stylized order book model, we demonstrate numerically\nthat there exists a second order phasetransition between a stable regime for\nweak feedback to an unstable regime for strong feedback,in which liquidity\ncrises arise with probability one. We characterize the critical exponents,\nwhichappear to belong to a new universality class. We then propose a simpler\nmodel for spread dynamicsthat maps onto a linear Hawkes process which also\nexhibits liquidity crises. If relevant for thereal markets, such a phase\ntransition scenario requires the system to sit below, but very close tothe\ninstability threshold (self-organised criticality), or else that the feedback\nintensity is itself timedependent and occasionally visits the unstable region.\nAn alternative scenario is provided by a classof non-linear Hawkes process that\nshow occasional \"activated\" liquidity crises, without having to bepoised at the\nedge of instability.\n"
    },
    {
        "paper_id": 1912.00454,
        "authors": "Ludovic Mathys",
        "title": "On Extensions of the Barone-Adesi & Whaley Method to Price American-Type\n  Options",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The present article provides an efficient and accurate hybrid method to price\nAmerican standard options in certain jump-diffusion models as well as American\nbarrier-type options under the Black & Scholes framework. Our method\ngeneralizes the quadratic approximation scheme of Barone-Adesi & Whaley (1987)\nand several of its extensions. Using perturbative arguments, we decompose the\nearly exercise pricing problem into sub-problems of different orders and solve\nthese sub-problems successively. The obtained solutions are combined to recover\napproximations to the original pricing problem of multiple orders, with the\n0-th order version matching the general Barone-Adesi & Whaley ansatz. We test\nthe accuracy and efficiency of the approximations via numerical simulations.\nThe results show a clear dominance of higher order approximations over their\nrespective 0-th order version and reveal that significantly more pricing\naccuracy can be obtained by relying on approximations of the first few orders.\nAdditionally, they suggest that increasing the order of any approximation by\none generally refines the pricing precision, however that this happens at the\nexpense of greater computational costs.\n"
    },
    {
        "paper_id": 1912.00469,
        "authors": "Ludovic Mathys",
        "title": "Valuing Tradeability in Exponential L\\'evy Models",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The present article provides a novel theoretical way to evaluate tradeability\nin markets of ordinary exponential L\\'evy type. We consider non-tradeability as\na particular type of market illiquidity and investigate its impact on the price\nof the assets. Starting from an adaption of the continuous-time optional asset\nreplacement problem initiated by McDonald and Siegel (1986), we derive\ntradeability premiums and subsequently characterize them in terms of\nfree-boundary problems. This provides a simple way to compute non-tradeability\nvalues, e.g. by means of standard numerical techniques, and, in particular, to\nexpress the price of a non-tradeable asset as a percentage of the price of a\ntradeable equivalent. Our approach is illustrated via numerical examples where\nwe discuss various properties of the tradeability premiums.\n"
    },
    {
        "paper_id": 1912.00691,
        "authors": "Hongshan Li, Zhongyi Huang",
        "title": "Artificial boundary method for the solution of pricing European options\n  under the Heston model",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper considers the valuation of a European call option under the Heston\nstochastic volatility model. We present the asymptotic solution to the option\npricing problem in powers of the volatility of variance. Then we introduce the\nartificial boundary method for solving the problem on a truncated domain, and\nderive several artificial boundary conditions (ABCs) on the artificial boundary\nof the bounded computational domain. A typical finite difference scheme and\nquadrature rule are used for the numerical solution of the reduced problem.\nNumerical experiments show that the proposed ABCs are able to improve the\naccuracy of the results and have a significant advantage over the widely-used\nboundary conditions by Heston in the original paper (Heston, 1993).\n"
    },
    {
        "paper_id": 1912.00712,
        "authors": "Shaogao Lv, Yongchao Hou, Hongwei Zhou",
        "title": "Financial Market Directional Forecasting With Stacked Denoising\n  Autoencoder",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Forecasting stock market direction is always an amazing but challenging\nproblem in finance. Although many popular shallow computational methods (such\nas Backpropagation Network and Support Vector Machine) have extensively been\nproposed, most algorithms have not yet attained a desirable level of\napplicability. In this paper, we present a deep learning model with strong\nability to generate high level feature representations for accurate financial\nprediction. Precisely, a stacked denoising autoencoder (SDAE) from deep\nlearning is applied to predict the daily CSI 300 index, from Shanghai and\nShenzhen Stock Exchanges in China. We use six evaluation criteria to evaluate\nits performance compared with the back propagation network, support vector\nmachine. The experiment shows that the underlying financial model with deep\nmachine technology has a significant advantage for the prediction of the CSI\n300 index.\n"
    },
    {
        "paper_id": 1912.01091,
        "authors": "Keith A. Lewis",
        "title": "A Simple Proof of the Fundamental Theorem of Asset Pricing",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  A simple statement and accessible proof of a version of the Fundamental\nTheorem of Asset Pricing in discrete time is provided. Careful distinction is\nmade between prices and cash flows in order to provide uniform treatment of all\ninstruments. There is no need for a ``real-world'' measure in order to specify\na model for derivative securities, one simply specifies an arbitrage free\nmodel, tunes it to market data, and gets down to the business of pricing,\nhedging, and managing the risk of derivative securities.\n"
    },
    {
        "paper_id": 1912.01129,
        "authors": "Bastien Baldacci, Iuliia Manziuk, Thibaut Mastrolia, Mathieu Rosenbaum",
        "title": "Market making and incentives design in the presence of a dark pool: a\n  deep reinforcement learning approach",
        "comments": "31 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We consider the issue of a market maker acting at the same time in the lit\nand dark pools of an exchange. The exchange wishes to establish a suitable\nmake-take fees policy to attract transactions on its venues. We first solve the\nstochastic control problem of the market maker without the intervention of the\nexchange. Then we derive the equations defining the optimal contract to be set\nbetween the market maker and the exchange. This contract depends on the trading\nflows generated by the market maker's activity on the two venues. In both\ncases, we show existence and uniqueness, in the viscosity sense, of the\nsolutions of the Hamilton-Jacobi-Bellman equations associated to the market\nmaker and exchange's problems. We finally design deep reinforcement learning\nalgorithms enabling us to approximate efficiently the optimal controls of the\nmarket maker and the optimal incentives to be provided by the exchange.\n"
    },
    {
        "paper_id": 1912.0128,
        "authors": "Kathrin Glau, Ricardo Pachon and Christian P\\\"otz",
        "title": "Speed-up credit exposure calculations for pricing and risk management",
        "comments": "arXiv admin note: substantial text overlap with arXiv:1905.00238",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We introduce a new method to calculate the credit exposure of European and\npath-dependent options. The proposed method is able to calculate accurate\nexpected exposure and potential future exposure profiles under the risk-neutral\nand the real-world measure. Key advantage of is that it delivers an accuracy\ncomparable to a full re-evaluation and at the same time it is faster than a\nregression-based method. Core of the approach is solving a dynamic programming\nproblem by function approximation. This yields a closed form approximation\nalong the paths together with the option's delta and gamma. The simple\nstructure allows for highly efficient evaluation of the exposures, even for a\nlarge number of simulated paths. The approach is flexible in the model choice,\npayoff profiles and asset classes. We validate the accuracy of the method\nnumerically for three different equity products and a Bermudan interest rate\nswaption. Benchmarking against the popular least-squares Monte Carlo approach\nshows that our method is able to deliver a higher accuracy in a faster runtime.\n"
    },
    {
        "paper_id": 1912.01281,
        "authors": "Yushi Hamaguchi",
        "title": "Time-inconsistent consumption-investment problems in incomplete markets\n  under general discount functions",
        "comments": "30 pages",
        "journal-ref": "SIAM J. Control Optim., 59(3), pp: 2121--2146 (2021)",
        "doi": "10.1137/19M1303782",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper, we study a time-inconsistent consumption-investment problem\nwith random endowments in a possibly incomplete market under general discount\nfunctions. We provide a necessary condition and a verification theorem for an\nopen-loop equilibrium consumption-investment pair in terms of a coupled\nforward-backward stochastic differential equation. Moreover, we prove the\nuniqueness of the open-loop equilibrium pair by showing that the original\ntime-inconsistent problem is equivalent to an associated time-consistent one.\n"
    },
    {
        "paper_id": 1912.01387,
        "authors": "S. Maurer, T.E. Sharp, M.V. Tretyakov",
        "title": "Pricing FX Options under Intermediate Currency",
        "comments": "An updated version with changes through out and an additional\n  calibration example",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  We suggest an intermediate currency approach that allows us to price options\non all FX markets simultaneously under the same risk-neutral measure which\nensures consistency of FX option prices across all markets. In particular, it\nis sufficient to calibrate a model to the volatility smile on the domestic\nmarket as, due to the consistency of pricing formulas, the model automatically\nreproduces the correct smile for the inverse pair (the foreign market). We\nfirst consider the case of two currencies and then the multi-currency setting.\nWe illustrate the intermediate currency approach by applying it to the Heston\nand SABR stochastic volatility models, to the model in which exchange rates are\ndescribed by an extended skewed normal distribution, and also to the model-free\napproach of option pricing\n"
    },
    {
        "paper_id": 1912.01455,
        "authors": "Ali Al-Aradi, Adolfo Correia, Danilo de Frietas Naiff, Gabriel Jardim,\n  Yuri Saporito",
        "title": "Extensions of the Deep Galerkin Method",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We extend the Deep Galerkin Method (DGM) introduced in Sirignano and\nSpiliopoulos (2018)} to solve a number of partial differential equations (PDEs)\nthat arise in the context of optimal stochastic control and mean field games.\nFirst, we consider PDEs where the function is constrained to be positive and\nintegrate to unity, as is the case with Fokker-Planck equations. Our approach\ninvolves reparameterizing the solution as the exponential of a neural network\nappropriately normalized to ensure both requirements are satisfied. This then\ngives rise to nonlinear a partial integro-differential equation (PIDE) where\nthe integral appearing in the equation is handled by a novel application of\nimportance sampling. Secondly, we tackle a number of Hamilton-Jacobi-Bellman\n(HJB) equations that appear in stochastic optimal control problems. The key\ncontribution is that these equations are approached in their unsimplified\nprimal form which includes an optimization problem as part of the equation. We\nextend the DGM algorithm to solve for the value function and the optimal\ncontrol \\simultaneously by characterizing both as deep neural networks.\nTraining the networks is performed by taking alternating stochastic gradient\ndescent steps for the two functions, a technique inspired by the policy\nimprovement algorithms (PIA).\n"
    },
    {
        "paper_id": 1912.01605,
        "authors": "Virginia Tsoukatou",
        "title": "Examination of the Correlation between Working Time Reduction and\n  Employment",
        "comments": null,
        "journal-ref": "Journal of Applied Economics and Business, Vol 7, No. 4, 15-41\n  (2019)",
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In recent years, it has been debated whether a reduction in working hours\nwould be a viable solution to tackle the unemployment caused by technological\nchange. The improvement of existing production technology is gradually being\nseen to reduce labor demand. Although this debate has been at the forefront for\nmany decades, the high and persistent unemployment encountered in the European\nUnion has renewed interest in implementing this policy in order to increase\nemployment. According to advocates of reducing working hours, this policy will\nincrease the number of workers needed during the production process, increasing\nemployment. However, the contradiction expressed by advocates of working time\nreduction is that the increase in labor costs will lead to a reduction in\nbusiness activity and ultimately to a reduction in demand for human resources.\nIn this article, we will attempt to answer the question of whether reducing\nworking hours is a way of countering the potential decline in employment due to\ntechnological change. In order to answer this question, the aforementioned\nconflicting views will be examined. As we will see during our statistical\nexamination of the existing empirical studies, the reduction of working time\ndoes not lead to increased employment and cannot be seen as a solution to the\nlong-lasting unemployment.\n"
    },
    {
        "paper_id": 1912.01708,
        "authors": "Bruce Knuteson",
        "title": "Celebrating Three Decades of Worldwide Stock Market Manipulation",
        "comments": "8 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  As the decade turns, we reflect on nearly thirty years of successful\nmanipulation of the world's public equity markets. This reflection highlights a\nfew of the key enabling ingredients and lessons learned along the way. A\nquantitative understanding of market impact and its decay, which we cover\nbriefly, lets you move long-term market prices to your advantage at acceptable\ncost. Hiding your footprints turns out to be less important than moving prices\nin the direction most people want them to move. Widespread (if misplaced) trust\nof market prices -- buttressed by overestimates of the cost of manipulation and\nunderestimates of the benefits to certain market participants -- makes price\nmanipulation a particularly valuable and profitable tool. Of the many recent\nstories heralding the dawn of the present golden age of misinformation, the\nmanipulation leading to the remarkable increase in the market capitalization of\nthe world's publicly traded companies over the past three decades is among the\nbest.\n"
    },
    {
        "paper_id": 1912.01793,
        "authors": "Shuzhen Yang",
        "title": "Multi-time state mean-variance model in continuous time",
        "comments": "32 pages, 3figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In the continuous time mean-variance model, we want to minimize the variance\n(risk) of the investment portfolio with a given mean at terminal time. However,\nthe investor can stop the investment plan at any time before the terminal time.\nTo solve this kind of problem, we consider to minimize the variances of the\ninvestment portfolio at multi-time state. The advantage of this multi-time\nstate mean-variance model is that we can minimize the risk of the investment\nportfolio along the investment period. To obtain the optimal strategy of the\nmulti-time state mean-variance model, we introduce a sequence of Riccati\nequations which are connected by a jump boundary condition. Based on this\nsequence Riccati equations, we establish the relationship between the means and\nvariances of this multi-time state mean-variance model. Furthermore, we use an\nexample to verify that minimizing the variances of the multi-time state can\naffect the average of Maximum-Drawdown of the investment portfolio.\n"
    },
    {
        "paper_id": 1912.01809,
        "authors": "Jiequn Han, Ruimeng Hu",
        "title": "Deep Fictitious Play for Finding Markovian Nash Equilibrium in\n  Multi-Agent Games",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We propose a deep neural network-based algorithm to identify the Markovian\nNash equilibrium of general large $N$-player stochastic differential games.\nFollowing the idea of fictitious play, we recast the $N$-player game into $N$\ndecoupled decision problems (one for each player) and solve them iteratively.\nThe individual decision problem is characterized by a semilinear\nHamilton-Jacobi-Bellman equation, to solve which we employ the recently\ndeveloped deep BSDE method. The resulted algorithm can solve large $N$-player\ngames for which conventional numerical methods would suffer from the curse of\ndimensionality. Multiple numerical examples involving identical or\nheterogeneous agents, with risk-neutral or risk-sensitive objectives, are\ntested to validate the accuracy of the proposed algorithm in large group games.\nEven for a fifty-player game with the presence of common noise, the proposed\nalgorithm still finds the approximate Nash equilibrium accurately, which, to\nour best knowledge, is difficult to achieve by other numerical algorithms.\n"
    },
    {
        "paper_id": 1912.01952,
        "authors": "Zongxi Li and A. Max Reppen and Ronnie Sircar",
        "title": "A Mean Field Games Model for Cryptocurrency Mining",
        "comments": "37 pages, 7 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We propose a mean field game model to study the question of how\ncentralization of reward and computational power occur in Bitcoin-like\ncryptocurrencies. Miners compete against each other for mining rewards by\nincreasing their computational power. This leads to a novel mean field game of\njump intensity control, which we solve explicitly for miners maximizing\nexponential utility, and handle numerically in the case of miners with power\nutilities. We show that the heterogeneity of their initial wealth distribution\nleads to greater imbalance of the reward distribution, and increased wealth\nheterogeneity over time, or a \"rich get richer\" effect. This concentration\nphenomenon is aggravated by a higher bitcoin mining reward, and reduced by\ncompetition. Additionally, an advantaged miner with cost advantages such as\naccess to cheaper electricity, contributes a significant amount of\ncomputational power in equilibrium, unaffected by competition from less\nefficient miners. Hence, cost efficiency can also result in the type of\ncentralization seen among miners of cryptocurrencies.\n"
    },
    {
        "paper_id": 1912.02373,
        "authors": "Hossein Kamalzadeh, Saeid Nassim Sobhan, Azam Boskabadi, Mohsen\n  Hatami, Amin Gharehyakheh",
        "title": "Modeling and Prediction of Iran's Steel Consumption Based on Economic\n  Activity Using Support Vector Machines",
        "comments": "13 pages, 13 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The steel industry has great impacts on the economy and the environment of\nboth developed and underdeveloped countries. The importance of this industry\nand these impacts have led many researchers to investigate the relationship\nbetween a country's steel consumption and its economic activity resulting in\nthe so-called intensity of use model. This paper investigates the validity of\nthe intensity of use model for the case of Iran's steel consumption and extends\nthis hypothesis by using the indexes of economic activity to model the steel\nconsumption. We use the proposed model to train support vector machines and\npredict the future values for Iran's steel consumption. The paper provides\ndetailed correlation tests for the factors used in the model to check for their\nrelationships with the steel consumption. The results indicate that Iran's\nsteel consumption is strongly correlated with its economic activity following\nthe same pattern as the economy has been in the last four decades.\n"
    },
    {
        "paper_id": 1912.02416,
        "authors": "Patrick Chang and Roger Bukuru and Tim Gebbie",
        "title": "Revisiting the Epps effect using volume time averaging: An exercise in R",
        "comments": "23 pages, 9 figures, 2 tables, 13 algorithms, link to our supporting\n  R code: https://github.com/rogerbukuru/Exploring-The-Epps-Effect-R, Submitted\n  to CSDA, corrects a figure error with regards to the Nyquist frequency",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  We revisit and demonstrate the Epps effect using two well-known\nnon-parametric covariance estimators; the Malliavin and Mancino (MM), and\nHayashi and Yoshida (HY) estimators. We show the existence of the Epps effect\nin the top 10 stocks from the Johannesburg Stock Exchange (JSE) by various\nmethods of aggregating Trade and Quote (TAQ) data. Concretely, we compare\ncalendar time sampling with two volume time sampling methods: asset intrinsic\nvolume time averaging, and volume time averaging synchronised in volume time\nacross assets relative to the least and most liquid asset clocks. We reaffirm\nthe argument made in much of the literature that the MM estimator is more\nrepresentative of trade time reality because it does not over-estimate\nshort-term correlations in an asynchronous event driven world. We confirm well\nknown market phenomenology with the aim of providing some standardised R based\nsimulation tools.\n"
    },
    {
        "paper_id": 1912.02423,
        "authors": "Kevin Kuo",
        "title": "Generative Synthesis of Insurance Datasets",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  One of the impediments in advancing actuarial research and developing open\nsource assets for insurance analytics is the lack of realistic publicly\navailable datasets. In this work, we develop a workflow for synthesizing\ninsurance datasets leveraging CTGAN, a recently proposed neural network\narchitecture for generating tabular data. Applying the proposed workflow to\npublicly available data in the domains of general insurance pricing and life\ninsurance shock lapse modeling, we evaluate the synthesized datasets from a few\nperspectives: machine learning efficacy, distributions of variables, and\nstability of model parameters. This workflow is implemented via an R interface\nto promote adoption by researchers and data owners.\n"
    },
    {
        "paper_id": 1912.02488,
        "authors": "Damian Jelito, Marcin Pitera, {\\L}ukasz Stettner",
        "title": "Long-run risk sensitive impulse control",
        "comments": null,
        "journal-ref": "SIAM Journal on Control and Optimization 58(4), 2020, 2446-2468",
        "doi": "10.1137/19M1305355",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper we consider long-run risk sensitive average cost impulse\ncontrol applied to a continuous-time Feller-Markov process. Using the\nprobabilistic approach, we show how to get a solution to a suitable\ncontinuous-time Bellman equation and link it with the impulse control problem.\nThe optimal strategy for the underlying problem is constructed as a limit of\ndyadic impulse strategies by exploiting regularity properties of the linked\nrisk sensitive optimal stopping value functions. In particular, this shows that\nthe discretized setting could be used to approximate near optimal strategies\nfor the underlying continuous time control problem, which facilitates the usage\nof the standard approximation tools. For completeness, we present examples of\nprocesses that could be embedded into our framework.\n"
    },
    {
        "paper_id": 1912.02684,
        "authors": "Simon Cramer, Torsten Trimborn",
        "title": "Stylized Facts and Agent-Based Modeling",
        "comments": "arXiv admin note: text overlap with arXiv:1812.02726",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The existence of stylized facts in financial data has been documented in many\nstudies. In the past decade the modeling of financial markets by agent-based\ncomputational economic market models has become a frequently used modeling\napproach. The main purpose of these models is to replicate stylized facts and\nto identify sufficient conditions for their creations. In this paper we\nintroduce the most prominent examples of stylized facts and especially present\nstylized facts of financial data. Furthermore, we given an introduction to\nagent-based modeling. Here, we not only provide an overview of this topic but\nintroduce the idea of universal building blocks for agent-based economic market\nmodels.\n"
    },
    {
        "paper_id": 1912.02753,
        "authors": "Filipe Fontanela, Antoine Jacquier, Mugad Oumgari",
        "title": "A Quantum algorithm for linear PDEs arising in Finance",
        "comments": "15 pages, 3 tables, 9 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We propose a hybrid quantum-classical algorithm, originated from quantum\nchemistry, to price European and Asian options in the Black-Scholes model. Our\napproach is based on the equivalence between the pricing partial differential\nequation and the Schrodinger equation in imaginary time. We devise a strategy\nto build a shallow quantum circuit approximation to this equation, only\nrequiring few qubits. This constitutes a promising candidate for the\napplication of Quantum Computing techniques (with large number of qubits\naffected by noise) in Quantitative Finance.\n"
    },
    {
        "paper_id": 1912.02775,
        "authors": "Henry Hanifan and John Cartlidge",
        "title": "Fools Rush In: Competitive Effects of Reaction Time in Automated Trading",
        "comments": "12 pages, 9 figures. Author's accepted manuscript. Published in\n  ICAART 2020: Proceedings of the 12th International Conference on Agents and\n  Artificial Intelligence, pages 82-93. Valletta, Malta, Feb. 2020. V2 edits:\n  source code links moved from reference list to footnotes",
        "journal-ref": "In Proceedings of the 12th International Conference on Agents and\n  Artificial Intelligence - Volume 1: ICAART, pages 82-93 (2020)",
        "doi": "10.5220/0008973700820093",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We explore the competitive effects of reaction time of automated trading\nstrategies in simulated financial markets containing a single exchange with\npublic limit order book and continuous double auction matching. A large body of\nresearch conducted over several decades has been devoted to trading agent\ndesign and simulation, but the majority of this work focuses on pricing\nstrategy and does not consider the time taken for these strategies to compute.\nIn real-world financial markets, speed is known to heavily influence the design\nof automated trading algorithms, with the generally accepted wisdom that faster\nis better. Here, we introduce increasingly realistic models of trading speed\nand profile the computation times of a suite of eminent trading algorithms from\nthe literature. Results demonstrate that: (a) trading performance is impacted\nby speed, but faster is not always better; (b) the Adaptive-Aggressive (AA)\nalgorithm, until recently considered the most dominant trading strategy in the\nliterature, is outperformed by the simplistic Shaver (SHVR) strategy - shave\none tick off the current best bid or ask - when relative computation times are\naccurately simulated.\n"
    },
    {
        "paper_id": 1912.02869,
        "authors": "Amihai Glazer, Refael Hassin, Irit Nowik",
        "title": "How Advance Sales can Reduce Profits: When to Buy, When to Sell, and\n  What Price to Charge",
        "comments": null,
        "journal-ref": "Manufacturing and Service Operations Management 2023",
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  A consumer who wants to consume a good in a particular period may\nnevertheless attempt to buy it earlier if he is concerned that in delaying he\nwould find the good already sold. This paper considers a model in which the\ngood may be offered in two periods; the period in which all consumers most\nvalue the good (period 2), and an earlier period (period 1). Examining the\nprofit-maximizing strategy of the firm under unbounded demand, we find that\neven with no cost of making the product available early, the firm does not\nprofit, and usually loses, by making the product available early.\nInterestingly, the price that maximizes profits induces all arrivals to occur\nearly, or all arrivals to occur late, depending on the parameters. The firm\nwould not set a price which induces consumers to arrive in both periods. In\nparticular, if the firm controls the penalty for arriving early, then it should\nset a high penalty so that no one arrives early. The Nash equilibrium behavior\nof consumers, when deciding if and when to arrive is more complicated than one\nmay suppose, and can generate some unexpected behavior. For example, when there\nis unbounded demand, most potential consumers decide not to arrive at all.\nAdditionally, the arrival rate may decline with the surplus a person gets from\nbuying the good. Surprisingly, we find that an increase in the number of units\nfor sale increases the number of consumers who arrive early. Moreover, we find\nthat the profit-maximizing price increases with the number of units offered for\nsale. This too is unexpected as an increase in supply often results in price\nreduction. In our case, an increase in the number of units on sale also\nincreases demand, and the seller may profit by increasing the price. In the\nsingle-unit case, we give closed solutions for the equilibrium customer\nbehavior and profit-maximizing firm strategy and conduct sensitivity analysis.\n"
    },
    {
        "paper_id": 1912.03158,
        "authors": "Michael Pfarrhofer, Anna Stelzer",
        "title": "The international effects of central bank information shocks",
        "comments": "JEL: C11, C30, E3, D31; Keywords: Bayesian global vector\n  autoregression, high-frequency identification, spillover effects, factor\n  stochastic volatility",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We explore the international transmission of monetary policy and central bank\ninformation shocks by the Federal Reserve and the European Central Bank.\nIdentification of these shocks is achieved by using a combination of\nhigh-frequency market surprises around announcement dates of policy decisions\nand sign restrictions. We propose a high-dimensional macroeconometric framework\nfor modeling aggregate quantities alongside country-specific variables to study\ninternational shock propagation and spillover effects. Our results are in line\nwith the established literature focusing on individual economies, and moreover\nsuggest substantial international spillover effects in both directions for\nmonetary policy and central bank information shocks. In addition, we detect\nheterogeneities in the transmission of ECB policy actions to individual member\nstates.\n"
    },
    {
        "paper_id": 1912.0327,
        "authors": "Sai Srikar Nimmagadda, Pawan Sasanka Ammanamanchi",
        "title": "BitMEX Funding Correlation with Bitcoin Exchange Rate",
        "comments": "9 pages,5 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  This paper examines the relationship between Inverse Perpetual Swap\ncontracts, a Bitcoin derivative akin to futures and the margin funding interest\nrates levied on BitMEX. This paper proves the Heteroskedastic nature of funding\nrates and goes onto establish a causal relationship between the funding rates\nand the Bitcoin inverse Perpetual swap contracts based on Granger causality.\nThe paper further dwells into developing a predictive model for funding rates\nusing best-fitted GARCH models. Implications of the results are presented, and\nfunding rates as a predictive tool for gauging the market trend is discussed.\n"
    },
    {
        "paper_id": 1912.03311,
        "authors": "Dingli Xi, Timothy Ian O'Brien, Elnaz Irannezhad",
        "title": "Investigating the Investment Behaviors in Cryptocurrency",
        "comments": "(In-press) Journal of Alternative Investments",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This study investigates the socio-demographic characteristics that individual\ncryptocurrency investors exhibit and the factors which go into their investment\ndecisions in different Initial Coin Offerings. A web based revealed preference\nsurvey was conducted among Australian and Chinese blockchain and cryptocurrency\nfollowers, and a Multinomial Logit model was applied to inferentially analyze\nthe characteristics of cryptocurrency investors and the determinants of the\nchoice of investment in cryptocurrency coins versus other types of ICO tokens.\nThe results show a difference between the determinant of these two choices\namong Australian and Chinese cryptocurrency folks. The significant factors of\nthese two choices include age, gender, education, occupation, and investment\nexperience, and they align well with the behavioural literature. Furthermore,\nalongside differences in how they rank the attributes of ICOs, there is further\nvariance between how Chinese and Australian investors rank deterrence factors\nand investment strategies.\n"
    },
    {
        "paper_id": 1912.03404,
        "authors": "Hyungbin Park",
        "title": "Convergence rates of large-time sensitivities with the\n  Hansen--Scheinkman decomposition",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper investigates the large-time asymptotic behavior of the\nsensitivities of cash flows. In quantitative finance, the price of a cash flow\nis expressed in terms of a pricing operator of a Markov diffusion process. We\nstudy the extent to which the pricing operator is affected by small changes of\nthe underlying Markov diffusion. The main idea is a partial differential\nequation (PDE) representation of the pricing operator by incorporating the\nHansen--Scheinkman decomposition method. The sensitivities of the cash flows\nand their large-time convergence rates can be represented via simple\nexpressions in terms of eigenvalues and eigenfunctions of the pricing operator.\nFurthermore, compared to the work of Park (Finance Stoch. 4:773-825, 2018),\nmore detailed convergence rates are provided. In addition, we discuss the\napplication of our results to three practical problems: utility maximization,\nentropic risk measures, and bond prices. Finally, as examples, explicit results\nfor several market models such as the Cox--Ingersoll--Ross (CIR) model, 3/2\nmodel and constant elasticity of variance (CEV) model are presented.\n"
    },
    {
        "paper_id": 1912.03556,
        "authors": "Silvia Bartolucci, Fabio Caccioli, Pierpaolo Vivo",
        "title": "A percolation model for the emergence of the Bitcoin Lightning Network",
        "comments": "22 pages, 6 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The Lightning Network is a so-called second-layer technology built on top of\nthe Bitcoin blockchain to provide \"off-chain\" fast payment channels between\nusers, which means that not all transactions are settled and stored on the main\nblockchain. In this paper, we model the emergence of the Lightning Network as a\n(bond) percolation process and we explore how the distributional properties of\nthe volume and size of transactions per user may impact its feasibility. The\nagents are all able to reciprocally transfer Bitcoins using the main blockchain\nand also - if economically convenient - to open a channel on the Lightning\nNetwork and transact \"off chain\". We base our approach on fitness-dependent\nnetwork models: as in real life, a Lightning channel is opened with a\nprobability that depends on the \"fitness\" of the concurring nodes, which in\nturn depends on wealth and volume of transactions. The emergence of a connected\ncomponent is studied numerically and analytically as a function of the\nparameters, and the phase transition separating regions in the phase space\nwhere the Lightning Network is sustainable or not is elucidated. We\ncharacterize the phase diagram determining the minimal volume of transactions\nthat would make the Lightning Network sustainable for a given level of fees or,\nalternatively, the maximal cost the Lightning ecosystem may impose for a given\naverage volume of transactions. The model includes parameters that could be in\nprinciple estimated from publicly available data once the evolution of the\nLighting Network will have reached a stationary operable state, and is fairly\nrobust against different choices of the distributions of parameters and fitness\nkernels.\n"
    },
    {
        "paper_id": 1912.03651,
        "authors": "Ale\\v{s} \\v{C}ern\\'y and Johannes Ruf",
        "title": "Simplified stochastic calculus with applications in Economics and\n  Finance",
        "comments": null,
        "journal-ref": "European Journal of Operational Research 293(2), 547-560, 2021",
        "doi": "10.1016/j.ejor.2020.12.037",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The paper introduces a simple way of recording and manipulating general\nstochastic processes without explicit reference to a probability measure. In\nthe new calculus, operations traditionally presented in a measure-specific way\nare instead captured by tracing the behaviour of jumps (also when no jumps are\nphysically present). The calculus is fail-safe in that, under minimal\nassumptions, all informal calculations yield mathematically well-defined\nstochastic processes. The calculus is also intuitive as it allows the user to\npretend all jumps are of compound Poisson type. The new calculus is very\neffective when it comes to computing drifts and expected values that possibly\ninvolve a change of measure. Such drift calculations yield, for example,\npartial integro-differential equations, Hamilton-Jacobi-Bellman equations,\nFeynman-Kac formulae, or exponential moments needed in numerous applications.\nWe provide several illustrations of the new technique, among them a novel\nresult on the Margrabe option to exchange one defaultable asset for another.\n"
    },
    {
        "paper_id": 1912.03692,
        "authors": "Kihun Nam",
        "title": "Global Well-posedness of Non-Markovian Multidimensional Superquadratic\n  BSDE",
        "comments": "There is a major error",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  (Working Paper) Using a purely probabilistic argument, we prove the global\nwell-posedness of multidimensional superquadratic backward stochastic\ndifferential equations (BSDEs) without Markovian assumption. The key technique\nis the interplay between the local well-posedness of fully coupled\npath-dependent forward backward stochastic differential equations (FBSDEs) and\nbackward iterations of the superquadratic BSDE. The superquadratic BSDE studied\nin this article includes quadratic BSDEs appear in stochastic differential game\nand price impact model. We also study the well-posedness of superquadratic\nFBSDEs and FBSDEs with measurable coefficients using the corresponding BSDE\nresults. Our result also provides the well-posedness of a system of\npath-dependent quasilinear partial differential equations where the\nnonlinearity has superquadratic growth in the gradient of the solution.\n"
    },
    {
        "paper_id": 1912.03781,
        "authors": "Giovanna Tagliaferri, Daria Scacciatelli, Pierfrancesco Alaimo Di Loro",
        "title": "VAT tax gap prediction: a 2-steps Gradient Boosting approach",
        "comments": "27 pages, 4 figures, 8 tables Presented at NTTS 2019 conference Under\n  review at another peer-reviewed journal",
        "journal-ref": "Stat Methods Appl, pub. 06/06/2022",
        "doi": "10.1007/s10260-022-00643-4",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Tax evasion is the illegal evasion of taxes by individuals, corporations, and\ntrusts. The revenue loss from tax avoidance can undermine the effectiveness and\nequity of the government policies. A standard measure of tax evasion is the tax\ngap, that can be estimated as the difference between the total amounts of tax\ntheoretically collectable and the total amounts of tax actually collected in a\ngiven period. This paper presents an original contribution to bottom-up\napproach, based on results from fiscal audits, through the use of Machine\nLearning. The major disadvantage of bottom-up approaches is represented by\nselection bias when audited taxpayers are not randomly selected, as in the case\nof audits performed by the Italian Revenue Agency. Our proposal, based on a\n2-steps Gradient Boosting model, produces a robust tax gap estimate and, embeds\na solution to correct for the selection bias which do not require any\nassumptions on the underlying data distribution. The 2-steps Gradient Boosting\napproach is used to estimate the Italian Value-added tax (VAT) gap on\nindividual firms on the basis of fiscal and administrative data income tax\nreturns gathered from Tax Administration Data Base, for the fiscal year 2011.\nThe proposed method significantly boost the performance in predicting with\nrespect to the classical parametric approaches.\n"
    },
    {
        "paper_id": 1912.03946,
        "authors": "Bruno Bouchard (CEREMADE), Xiaolu Tan",
        "title": "Understanding the dual formulation for the hedging of path-dependent\n  options with price impact",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We consider a general path-dependent version of the hedging problem with\nprice impact of Bouchard et al. (2019), in which a dual formulation for the\nsuper-hedging price is obtained by means of PDE arguments, in a Markovian\nsetting and under strong regularity conditions. Using only probabilistic\narguments, we prove, in a path-dependent setting and under weak regularity\nconditions, that any solution to this dual problem actually allows one to\nconstruct explicitly a perfect hedging portfolio. From a pure probabilistic\npoint of view, our approach also allows one to exhibit solutions to a specific\nclass of second order forward backward stochastic differential equations, in\nthe sense of Cheridito et al. (2007). Existence of a solution to the dual\noptimal control problem is also addressed in particular settings. As a\nby-product of our arguments, we prove a version of It{\\^o}'s Lemma for\npath-dependent functionals that are only C^{0,1} in the sense of Dupire.\n"
    },
    {
        "paper_id": 1912.04009,
        "authors": "Alexandre Miot and Gilles Drigout",
        "title": "An empirical study of neural networks for trend detection in time series",
        "comments": null,
        "journal-ref": null,
        "doi": "10.1007/s42979-020-00362-1",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Detecting structure in noisy time series is a difficult task. One intuitive\nfeature is the notion of trend. From theoretical hints and using simulated time\nseries, we empirically investigate the efficiency of standard recurrent neural\nnetworks (RNNs) to detect trends. We show the overall superiority and\nversatility of certain standard RNNs structures over various other estimators.\nThese RNNs could be used as basic blocks to build more complex time series\ntrend estimators.\n"
    },
    {
        "paper_id": 1912.04012,
        "authors": "Somayeh Kokabisaghi, Eric J Pauwels, Andre B Dorsman",
        "title": "To snipe or not to snipe, that is the question! Transitions in sniping\n  behaviour among competing algorithmic traders",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper we extend the investigation into the transition from sure to\nprobabilistic sniping as introduced in Menkveld and Zoican \\cite{mz2017}. In\nthat paper, the authors introduce a stylized version of a competitive game in\nwhich high frequency traders (HFTs) interact with each other and liquidity\ntraders. The authors then show that risk aversion plays an important role in\nthe transition from sure to mixed (or probabilistic) sniping. In this paper, we\nre-interpret and extend these conclusions in the context of repeated games and\nhighlight some differences in results. In particular, we identify situations in\nwhich probabilistic sniping is genuinely profitable that are qualitatively\ndifferent from the ones obtained in \\cite{mz2017}. It turns out that beyond a\nspecific risk aversion threshold the game resembles the well-known prisoner's\ndilemma, in that probabilistic sniping becomes a way to cooperate among the\nHFTs that leaves all the participants better off. In order to turn this into a\nviable strategy for the repeated game, we show how compliance can be monitored\nthrough the use of sequential statistical testing. Keywords: algorithmic\ntrading, bandits, high-frequency exchange, Nash equilibrium, repeated games,\nsniping, subgame-perfect equilibrium, Sequential probability ratio, transition\n"
    },
    {
        "paper_id": 1912.04015,
        "authors": "Somayeh Kokabisaghi, Mohammadesmaeil Ezazi, Reza Tehrani, Nourmohammad\n  Yaghoubi",
        "title": "Sanction or Financial Crisis? An Artificial Neural Network-Based\n  Approach to model the impact of oil price volatility on Stock and industry\n  indices",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper, we model the impact of oil price volatility on Tehranstock and\nindustry indices in two periods of international sanctions and post-sanction.\nTo analyse the purpose of study, we use Feed-forward neural net-works. The\nperiod of study is from 2008 to 2018 that is split in two periods during\ninternational energy sanction and post-sanction. The results show that\nFeed-forward neural networks perform well in predicting stock market and\nindustry, which means oil price volatility has a significant impact on stock\nand industry market indices. During post-sanction and global financial crisis,\nthe model performs better in predicting industry index. Additionally, oil\nprice-stock market index prediction performs better in the period of\ninternational sanctions. Herein, these results are, up to some extent,\nimportant for financial market analysts and policy makers to understand which\nfactors and when influence the financial market, especially in an oil-dependent\ncountry such asIran with uncertainty in the international politics. Keywords:\nFeed-forward neural networks,Industry index,International energy sanction,Oil\nprice volatility,Tehran stock index\n"
    },
    {
        "paper_id": 1912.04086,
        "authors": "Erik B{\\o}lviken and Yinzhi Wang",
        "title": "Optimal reinsurance for risk over surplus ratios",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Optimal reinsurance when Value at Risk and expected surplus is balanced\nthrough their ratio is studied, and it is demonstrated how results for\nrisk-adjusted surplus can be utilized. Simplifications for large portfolios are\nderived, and this large-portfolio study suggests a new condition on the\nreinsurance pricing regime which is crucial for the results obtained. One or\ntwo-layer contracts now become optimal for both risk-adjusted surplus and the\nrisk over expected surplus ratio, but there is no second layer when portfolios\nare large or when reinsurance prices are below some threshold. Simple\napproximations of the optimum portfolio are considered, and their degree of\ndegradation compared to the optimum is studied which leads to theoretical\ndegradation rates as the number of policies grows. The theory is supported by\nnumerical experiments which suggest that the shape of the claim severity\ndistributions may not be of primary importance when designing an optimal\nreinsurance program. It is argued that the approach can be applied to\nConditional Value at Risk as well.\n"
    },
    {
        "paper_id": 1912.04175,
        "authors": "Yinzhi Wang and Erik B{\\o}lviken",
        "title": "How much is optimal reinsurance degraded by error?",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The literature on optimal reinsurance does not deal with how much the\neffectiveness of such solutions is degraded by errors in parameters and models.\nThe issue is investigated through both asymptotics and numerical studies. It is\nshown that the rate of degradation is often $O(1/n)$ as the sample size $n$ of\nhistorical observations becomes infinite. Criteria based on Value at Risk are\nexceptions that may achieve only $O(1/\\sqrt{n})$. These theoretical results are\nsupported by numerical studies. A Bayesian perspective on how to integrate risk\ncaused by parameter error is offered as well.\n"
    },
    {
        "paper_id": 1912.04221,
        "authors": "Kangjianan Xie",
        "title": "Leakage of rank-dependent functionally generated trading strategies",
        "comments": "13 pages, 2 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper investigates the so-called leakage effect of trading strategies\ngenerated functionally from rank-dependent portfolio generating functions. This\neffect measures the loss in wealth of trading strategies due to renewing the\nportfolio constituent stocks. Theoretically, the leakage effect of a trading\nstrategy is expressed explicitly by a finite-variation term. The computation of\nthe leakage is different from what previous research has suggested. The method\nto estimate leakage in discrete time is then introduced with some practical\nconsiderations. An empirical example illustrates the leakage of the\ncorresponding trading strategies under different constituent list sizes.\n"
    },
    {
        "paper_id": 1912.04242,
        "authors": "Jacobo Roa-Vicens, Yuanbo Wang, Virgile Mison, Yarin Gal, Ricardo\n  Silva",
        "title": "Adversarial recovery of agent rewards from latent spaces of the limit\n  order book",
        "comments": "Published as a workshop paper on NeurIPS 2019 Workshop on Robust AI\n  in Financial Services. 33rd Conference on Neural Information Processing\n  Systems (NeurIPS 2019), Vancouver, Canada",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Inverse reinforcement learning has proved its ability to explain state-action\ntrajectories of expert agents by recovering their underlying reward functions\nin increasingly challenging environments. Recent advances in adversarial\nlearning have allowed extending inverse RL to applications with non-stationary\nenvironment dynamics unknown to the agents, arbitrary structures of reward\nfunctions and improved handling of the ambiguities inherent to the ill-posed\nnature of inverse RL. This is particularly relevant in real time applications\non stochastic environments involving risk, like volatile financial markets.\nMoreover, recent work on simulation of complex environments enable learning\nalgorithms to engage with real market data through simulations of its latent\nspace representations, avoiding a costly exploration of the original\nenvironment. In this paper, we explore whether adversarial inverse RL\nalgorithms can be adapted and trained within such latent space simulations from\nreal market data, while maintaining their ability to recover agent rewards\nrobust to variations in the underlying dynamics, and transfer them to new\nregimes of the original environment.\n"
    },
    {
        "paper_id": 1912.04274,
        "authors": "Roberto Ernani Porcher Junior",
        "title": "DAY TRADE: across the statistics | DAY TRADE: do outro lado das\n  estatisticas",
        "comments": "32 pages, in Portuguese (BR)",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper questions some current ideas about the practice of specific\ncapital market operations - the so-called day trading operations. The text\nadvanced from theoretical propositions to a detailed analysis of the study\nentitled \"Is it possible to live by day-trading?\" (CHAGUE and GIOVANNETTI,\n2019), to which it offers a counterpoint. This investigation reveal the\nexistence of important elements that are not yet properly weighed in the\ntreatment of the current theme, leading to loss of dimensions that are - or\nshould be - inseparable from this type of research. The conclusion reached is\nthat the existing scientific evidence does not show that the adoption of the\nday trade as an occupation is economically unsustainable, nor does it prove the\nimpossibility of evolution of the day traders' operational performance over\ntime.\n  -----\n  O presente trabalho coloca em questionamento algumas ideias atualmente\ndifundidas sobre a pratica de operacoes especificas do mercado de capitais - as\nchamadas operacoes day trade. Partindo de proposicoes teoricas, o texto avanca\na uma analise detalhada do estudo intitulado \"E possivel viver de day-trading?\"\n(CHAGUE e GIOVANNETTI, 2019), ao qual oferece contraponto. Essa investigacao\nrevela a existencia de importantes elementos que ainda nao estao devidamente\nsopesados no tratamento do tema em voga, acarretando perda de dimensoes que sao\n- ou deveriam ser - indissociaveis desse tipo de pesquisa. A conclusao\nalcancada e de que as evidencias cientificas ate entao existentes nao\ndemonstram que a adocao do day trade como profissao seja insustentavel\neconomicamente, nem comprovam a impossibilidade de evolucao da performance\noperacional dos day traders ao longo do tempo.\n"
    },
    {
        "paper_id": 1912.04281,
        "authors": "Melaku Haile Likka, Shimeles Ololo Sinkie and Berhane Megerssa",
        "title": "Willingness to Pay for Community-Based Health Insurance among Rural\n  Households of Southwest Ethiopia",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Use of healthcare services is inadequate in Ethiopia in spite of the high\nburden of diseases. User-fee charges are the most important factor for this\ndeficiency in healthcare utilization. Hence, the country is introducing\ncommunity based and social health insurances since 2010 to tackle such\nproblems. This study was conducted cross-sectionally, in March 2013, to assess\nwillingness of rural households to pay for community-based health insurance in\nDebub Bench district of Southwest Ethiopia. Two-stage sampling technique was\nused to select 845 households. Selected households were contacted using simple\nrandom sampling technique. Double bounded dichotomous choice method was used to\nillicit the willingness to pay. Data were analyzed with STATA 11. Krinsky and\nRob method was used to calculate the mean/median with 95% CI willingness to pay\nafter the predictors have been estimated using Seemingly Unrelated Bivariate\nProbit Regression. Eight hundred and eight (95.6%) of the sampled households\nwere interviewed. Among them 629(77.8%) households were willing to join the\nproposed CBHI scheme. About 54% of the households in the district were willing\nto pay either the initial or second bids presented. On average, these\nhouseholds were willingness to pay was 162.61 Birr per household (8.9 US$)\nannually. If the community based health insurance is rolled out in the\ndistrict, about half of households will contribute 163 Birr (8.9 US$) annually.\nIf the premium exceeds the amount specified, majority of the households would\nnot join the scheme. Key words: community based health insurance, willingness\nto pay, contingent valuation method, double bounded dichotomous choice, Krinsky\nand Robb, rural households, Ethiopia.\n"
    },
    {
        "paper_id": 1912.04308,
        "authors": "R\\'egis Houssou, J\\'er\\^ome Bovay, Stephan Robert",
        "title": "Adaptive Financial Fraud Detection in Imbalanced Data with Time-Varying\n  Poisson Processes",
        "comments": "Accepted for publication in the Journal Of Financial Risk Management\n  (JFRM). Comments welcome",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper discusses financial fraud detection in imbalanced dataset using\nhomogeneous and non-homogeneous Poisson processes. The probability of\npredicting fraud on the financial transaction is derived. Applying our\nmethodology to the financial dataset shows a better predicting power than a\nbaseline approach, especially in the case of higher imbalanced data.\n"
    },
    {
        "paper_id": 1912.04492,
        "authors": "Zura Kakushadze and Juan Andr\\'es Serur",
        "title": "151 Estrategias de Trading (151 Trading Strategies)",
        "comments": "398 pages; in Spanish; complete English version is freely\n  downloadable from https://ssrn.com/abstract=3247865; English edition: Z.\n  Kakushadze and J.A. Serur. 151 Trading Strategies. Cham, Switzerland:\n  Palgrave Macmillan, an imprint of Springer Nature, 1st Edition (2018), XX,\n  480 pp; ISBN 978-3-030-02791-9",
        "journal-ref": "151 Estrategias de Trading (Spanish Edition, 2019), 398 pp; ISBN\n  978-1071261873",
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This book, which is in Spanish, provides detailed descriptions, including\nover 550 mathematical formulas, for over 150 trading strategies across a host\nof asset classes (and trading styles). This includes stocks, options, fixed\nincome, futures, ETFs, indexes, commodities, foreign exchange, convertibles,\nstructured assets, volatility (as an asset class), real estate, distressed\nassets, cash, cryptocurrencies, miscellany (such as weather, energy,\ninflation), global macro, infrastructure, and tax arbitrage. Some strategies\nare based on machine learning algorithms (such as artificial neural networks,\nBayes, k-nearest neighbors). We also give: source code for illustrating\nout-of-sample backtesting with explanatory notes; around 2,000 bibliographic\nreferences; and over 900 glossary, acronym and math definitions. The\npresentation is intended to be descriptive and pedagogical.\n  -----\n  Este libro proporciona descripciones detalladas, que incluyen m\\'as de 550\nf\\'ormulas matem\\'aticas, para m\\'as de 150 estrategias de trading para una\ngran cantidad de clases de activos y estilos de trading. Esto incluye acciones,\nopciones, bonos (renta fija), futuros, ETFs, \\'indices, commodities, divisas,\nbonos convertibles, activos estructurados, volatilidad (como clase de activos),\nbienes inmuebles, activos en distress, efectivo, criptomonedas, miscel\\'aneos\n(como clima, energ\\'ia, inflaci\\'on), macro global, infraestructura y arbitraje\nimpositivo. Algunas estrategias se basan en algoritmos de aprendizaje\nautom\\'atico (como redes neuronales artificiales, Bayes, k vecinos m\\'as\ncercanos). El libro tambi\\'en incluye c\\'odigo para backtesting fuera de la\nmuestra con notas explicativas; cerca de 2,000 referencias bibliogr\\'aficas;\nm\\'as de 900 t\\'erminos que comprenden el glosario, acr\\'onimos y definiciones\nmatem\\'aticas. La presentaci\\'on pretende ser descriptiva y pedag\\'ogica.\n"
    },
    {
        "paper_id": 1912.04565,
        "authors": "Masaaki Kijima and Christopher Ting",
        "title": "Market Price of Trading Liquidity Risk and Market Depth",
        "comments": "46 Pages, 12 Figures, To appear in the International Journal of\n  Theoretical and Applied Finance",
        "journal-ref": null,
        "doi": "10.1142/S0219024919500456",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Price impact of a trade is an important element in pre-trade and post-trade\nanalyses. We introduce a framework to analyze the market price of liquidity\nrisk, which allows us to derive an inhomogeneous Bernoulli ordinary\ndifferential equation. We obtain two closed form solutions, one of which\nreproduces the linear function of the order flow in Kyle (1985) for informed\ntraders. However, when traders are not as asymmetrically informed, an S-shape\nfunction of the order flow is obtained. We perform an empirical intra-day\nanalysis on Nikkei futures to quantify the price impact of order flow and\ncompare our results with industry's heuristic price impact functions. Our model\nof order flow yields a rich framework for not only to estimate the liquidity\nrisk parameters, but also to provide a plausible cause of why volatility and\ncorrelation are stochastic in nature. Finally, we find that the market depth\nencapsulates the market price of liquidity risk.\n"
    },
    {
        "paper_id": 1912.04652,
        "authors": "Constantinos Kardaras and Johannes Ruf",
        "title": "Filtration shrinkage, the structure of deflators, and failure of market\n  completeness",
        "comments": "Minor corrections; Accepted for publication by Finance and\n  Stochastics",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We analyse the structure of local martingale deflators projected on smaller\nfiltrations. In a general continuous-path setting, we show that the local\nmartingale part in the multiplicative Doob-Meyer decomposition of projected\nlocal martingale deflators are themselves local martingale deflators in the\nsmaller information market. Via use of a Bayesian filtering approach, we\ndemonstrate the exact mechanism of how updates on the possible class of models\nunder less information result in the strict supermartingale property of\nprojections of such deflators. Finally, we demonstrate that these projections\nare unable to span all possible local martingale deflators in the smaller\ninformation market, by investigating a situation where market completeness is\nnot retained under filtration shrinkage.\n"
    },
    {
        "paper_id": 1912.04815,
        "authors": "Leonardo Massai and Giacomo Como and Fabio Fagnani",
        "title": "Equilibria and Systemic Risk in Saturated Networks",
        "comments": "26 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We undertake a fundamental study of network equilibria modeled as solutions\nof fixed point equations for monotone linear functions with saturation\nnonlinearities. The considered model extends one originally proposed to study\nsystemic risk in networks of financial institutions interconnected by mutual\nobligations and is one of the simplest continuous models accounting for shock\npropagation phenomena and cascading failure effects. It also characterizes Nash\nequilibria of constrained quadratic network games with strategic\ncomplementarities. We first derive explicit expressions for network equilibria\nand prove necessary and sufficient conditions for their uniqueness encompassing\nand generalizing results available in the literature. Then, we study jump\ndiscontinuities of the network equilibria when the exogenous flows cross\ncertain regions of measure 0 representable as graphs of continuous functions.\nFinally, we discuss some implications of our results in the two main motivating\napplications. In financial networks, this bifurcation phenomenon is responsible\nfor how small shocks in the assets of a few nodes can trigger major aggregate\nlosses to the system and cause the default of several agents. In constrained\nquadratic network games, it induces a blow-up behavior of the sensitivity of\nNash equilibria with respect to the individual benefits.\n"
    },
    {
        "paper_id": 1912.04941,
        "authors": "Svitlana Vyetrenko, David Byrd, Nick Petosa, Mahmoud Mahfouz, Danial\n  Dervovic, Manuela Veloso, Tucker Hybinette Balch",
        "title": "Get Real: Realism Metrics for Robust Limit Order Book Market Simulations",
        "comments": null,
        "journal-ref": "NeurIPS 2019 Workshop on Robust AI in Financial Services: Data,\n  Fairness, Explainability, Trustworthiness, and Privacy",
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Machine learning (especially reinforcement learning) methods for trading are\nincreasingly reliant on simulation for agent training and testing. Furthermore,\nsimulation is important for validation of hand-coded trading strategies and for\ntesting hypotheses about market structure. A challenge, however, concerns the\nrobustness of policies validated in simulation because the simulations lack\nfidelity. In fact, researchers have shown that many market simulation\napproaches fail to reproduce statistics and stylized facts seen in real\nmarkets. As a step towards addressing this we surveyed the literature to\ncollect a set of reference metrics and applied them to real market data and\nsimulation output. Our paper provides a comprehensive catalog of these metrics\nincluding mathematical formulations where appropriate. Our results show that\nthere are still significant discrepancies between simulated markets and real\nones. However, this work serves as a benchmark against which we can measure\nfuture improvement.\n"
    },
    {
        "paper_id": 1912.05113,
        "authors": "Takashi Akamatsu, Tomoya Mori, Minoru Osawa, Yuki Takayama",
        "title": "Multimodal agglomeration in economic geography",
        "comments": "43 pages, 11 figures (main text)",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Multimodal agglomerations, in the form of the existence of many cities,\ndominate modern economic geography. We focus on the mechanism by which\nmultimodal agglomerations realize endogenously. In a spatial model with\nagglomeration and dispersion forces, the spatial scale (local or global) of the\ndispersion force determines whether endogenous spatial distributions become\nmultimodal. Multimodal patterns can emerge only under a global dispersion\nforce, such as competition effects, which induce deviations to locations\ndistant from an existing agglomeration and result in a separate agglomeration.\nA local dispersion force, such as the local scarcity of land, causes the\nflattening of existing agglomerations. The resulting spatial configuration is\nunimodal if such a force is the only source of dispersion. This view allows us\nto categorize extant models into three prototypical classes: those with only\nglobal, only local, and local and global dispersion forces. The taxonomy\nfacilitates model choice depending on each study's objective.\n"
    },
    {
        "paper_id": 1912.05164,
        "authors": "Dirk Bergemann, Francisco Castro, Gabriel Weintraub",
        "title": "Third-Degree Price Discrimination Versus Uniform Pricing",
        "comments": "26 pages, 5 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We compare the profit of the optimal third-degree price discrimination policy\nagainst a uniform pricing policy. A uniform pricing policy offers the same\nprice to all segments of the market. Our main result establishes that for a\nbroad class of third-degree price discrimination problems with concave profit\nfunctions (in the price space) and common support, a uniform price is\nguaranteed to achieve one half of the optimal monopoly profits. This profit\nbound holds for any number of segments and prices that the seller might use\nunder third-degree price discrimination. We establish that these conditions are\ntight and that weakening either common support or concavity can lead to\narbitrarily poor profit comparisons even for regular or monotone hazard rate\ndistributions.\n"
    },
    {
        "paper_id": 1912.05228,
        "authors": "Junjie Hu, Wolfgang Karl H\\\"ardle, Weiyu Kuo",
        "title": "Risk of Bitcoin Market: Volatility, Jumps, and Forecasts",
        "comments": "40 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  Cryptocurrency, the most controversial and simultaneously the most\ninteresting asset, has attracted many investors and speculators in recent\nyears. The visibly significant market capitalization of cryptos also motivates\nmodern financial instruments such as futures and options. Those will depend on\nthe dynamics, volatility, or even the jumps of cryptos. We provide a\ncomprehensive investigation of the risk dynamics of the Bitcoin Market from a\nrealized volatility perspective. The Bitcoin market is extremely risky in the\nsense of volatility, entangled jumps, and extensive consecutive jumps, which\nreflect the major incidents worldwide. Empirical study shows that the lagged\nrealized variance increases the future realized variance, while the jumps,\nespecially positive ones, significantly reduce future realized variance. The\nout-of-sample forecasting model reveals that, in terms of forecasting accuracy\nand utility gain, investors interested in the long-term realized variance\nbenefit from explicitly modelling the jumps and signed estimators, which is\nunnecessary for the short-term realized variance forecast.\n"
    },
    {
        "paper_id": 1912.05273,
        "authors": "V. Sasidevan and Nils Bertschinger",
        "title": "Systemic Risk: Fire-Walling Financial Systems Using Network-Based\n  Approaches",
        "comments": "19 pages, 7 figures",
        "journal-ref": null,
        "doi": "10.1007/978-981-13-8319-9_16",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The latest financial crisis has painfully revealed the dangers arising from a\nglobally interconnected financial system. Conventional approaches based on the\nnotion of the existence of equilibrium and those which rely on statistical\nforecasting have seen to be inadequate to describe financial systems in any\nreasonable way. A more natural approach is to treat financial systems as\ncomplex networks of claims and obligations between various financial\ninstitutions present in an economy. The generic framework of complex networks\nhas been successfully applied across several disciplines, e.g., explaining\ncascading failures in power transmission systems and epidemic spreading. Here\nwe review various network models addressing financial contagion via direct\ninter-bank contracts and indirectly via overlapping portfolios of financial\ninstitutions. In particular, we discuss the implications of the\n\"robust-yet-fragile\" nature of financial networks for cost-effective regulation\nof systemic risk.\n"
    },
    {
        "paper_id": 1912.05383,
        "authors": "Elisa Alos, Frido Rolloos, Kenichiro Shiraya",
        "title": "On the difference between the volatility swap strike and the zero vanna\n  implied volatility",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper, Malliavin calculus is applied to arrive at exact formulas for\nthe difference between the volatility swap strike and the zero vanna implied\nvolatility for volatilities driven by fractional noise. To the best of our\nknowledge, our estimate is the first to derive the rigorous relationship\nbetween the zero vanna implied volatility and the volatility swap strike. In\nparticular, we will see that the zero vanna implied volatility is a better\napproximation for the volatility swap strike than the ATMI.\n"
    },
    {
        "paper_id": 1912.05438,
        "authors": "Yerkin Kitapbayev",
        "title": "Closed form optimal exercise boundary of the American put option",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We present three models of stock price with time-dependent interest rate,\ndividend yield, and volatility, respectively, that allow for explicit forms of\nthe optimal exercise boundary of the finite maturity American put option. The\noptimal exercise boundary satisfies the nonlinear integral equation of Volterra\ntype. We choose time-dependent parameters of the model so that the integral\nequation for the exercise boundary can be solved in the closed form. We also\ndefine the contracts of put type with time-dependent strike price that support\nthe explicit optimal exercise boundary.\n"
    },
    {
        "paper_id": 1912.05484,
        "authors": "Michael B. Giles, Abdul-Lateef Haji-Ali",
        "title": "Sub-sampling and other considerations for efficient risk estimation in\n  large portfolios",
        "comments": null,
        "journal-ref": "Journal of Computational Finance, 26:1 (2022)",
        "doi": "10.21314/JCF.2022.019",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Computing risk measures of a financial portfolio comprising thousands of\nderivatives is a challenging problem because (a) it involves a nested\nexpectation requiring multiple evaluations of the loss of the financial\nportfolio for different risk scenarios and (b) evaluating the loss of the\nportfolio is expensive and the cost increases with its size. In this work, we\nlook at applying Multilevel Monte Carlo (MLMC) with adaptive inner sampling to\nthis problem and discuss several practical considerations. In particular, we\ndiscuss a sub-sampling strategy whose computational complexity does not\nincrease with the size of the portfolio. We also discuss several control\nvariates that significantly improve the efficiency of MLMC in our setting.\n"
    },
    {
        "paper_id": 1912.05576,
        "authors": "Hanna Hottenrott, Michael Rose, Cornelia Lawson",
        "title": "The Rise of Multiple Institutional Affiliations in Academia",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This study provides the first systematic, international, large-scale evidence\non the extent and nature of multiple institutional affiliations on journal\npublications. Studying more than 15 million authors and 22 million articles\nfrom 40 countries we document that: In 2019, almost one in three articles was\n(co-)authored by authors with multiple affiliations and the share of authors\nwith multiple affiliations increased from around 10% to 16% since 1996. The\ngrowth of multiple affiliations is prevalent in all fields and it is stronger\nin high impact journals. About 60% of multiple affiliations are between\ninstitutions from within the academic sector. International co-affiliations,\nwhich account for about a quarter of multiple affiliations, most often involve\ninstitutions from the United States, China, Germany and the United Kingdom,\nsuggesting a core-periphery network. Network analysis also reveals a number\ncommunities of countries that are more likely to share affiliations. We discuss\npotential causes and show that the timing of the rise in multiple affiliations\ncan be linked to the introduction of more competitive funding structures such\nas 'excellence initiatives' in a number of countries. We discuss implications\nfor science and science policy.\n"
    },
    {
        "paper_id": 1912.05635,
        "authors": "Dmytro Zherlitsyn, Stanislav Levytskyi, Denys Mykhailyk, Victoriia\n  Ogloblina",
        "title": "Assessment of Financial Potential as a Determinant of Enterprise\n  Development",
        "comments": "6th International Conference on Strategies, Models and Technologies\n  of Economic Systems Management (SMTESM 2019)",
        "journal-ref": null,
        "doi": "10.2991/smtesm-19.2019.42",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Financial potential is an important part of enterprise activities. The\ntechnique of the enterprise's financial potential assessment is offered in the\npaper. It is presented by particular stages, where each stage is related to a\ncertain task. The characteristics of the company's financial potential, based\non the analysis of the related literature, are determined. The implementation\nof each task is carried out. Thus, the study proposes a mechanism for managing\nthe financial potential of enterprises, which allows to emphasize the elements\nthat can be useful for economic development. It is based on the general\nstrategic principles of the enterprise management. The study results can be\nused to assess enterprise purposes and develop the formation goals of its\nfinancial potential. It can also help to forecast and separate main directions\nof accumulation, formation, and distribution of financial resources. It should\nbe noted, that analysis and control over the financial potential formation\nstrategy, as well as the use of analysis results for specifying the strategic\ndirections of the enterprise development, are of high importance. Therefore,\nthe management of the financial potential is a system of rational management of\nbusiness financing, which includes the formation of financial relations,\nemerging as a result of finance resources flow.\n"
    },
    {
        "paper_id": 1912.05641,
        "authors": "Anna Denkowska, Stanis{\\l}aw Wanat",
        "title": "A Dynamic MST- deltaCovar Model Of Systemic Risk In The European\n  Insurance Sector",
        "comments": "JEL: G22, Statistical finance",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This work is an answer to the EIOPA 2017 report. It follows from the latter\nthat in order to assess the potential systemic risk we should take into account\nthe build-up of risk and in particular the risk that arises in time, as well as\nthe interlinkages in the financial sector and the whole economy. Our main tools\nused to analyse the systemic risk dynamics in the European insurance sector\nduring the years 2005-2019 are the topological indices of minimum spanning\ntrees (MST) and the deltaCoVaR measure. We address the following questions: 1)\nWhat is the contribution to systemic risk of each of the 28 largest European\ninsurance companies whose list includes also those appearing on the G-SIIs\nlist? 2) Does the analysis of the deltaCoVaR of those 28 insurance companies\nand the conclusions we draw agree with the our claims from our latest article\n[Wanat S., Denkowska A. 2019]. In clear: does the most important contribution\nto systemic risk come from the companies that have the highest betweenness\ncentrality or the highest degree in the MST obtained?\n"
    },
    {
        "paper_id": 1912.05773,
        "authors": "Petteri Piiroinen, Lassi Roininen, Martin Simon",
        "title": "Brexit Risk Implied by the SABR Martingale Defect in the EUR-GBP Smile",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We construct a data-driven statistical indicator for quantifying the tail\nrisk perceived by the EURGBP option market surrounding Brexit-related events.\nWe show that under lognormal SABR dynamics this tail risk is closely related to\nthe so-called martingale defect and provide a closed-form expression for this\ndefect which can be computed by solving an inverse calibration problem. In\norder to cope with the the uncertainty which is inherent to this inverse\nproblem, we adopt a Bayesian statistical parameter estimation perspective. We\nprobe the resulting posterior densities with a combination of optimization and\nadaptive Markov chain Monte Carlo methods, thus providing a careful uncertainty\nestimation for all of the underlying parameters and the martingale defect\nindicator. Finally, to support the feasibility of the proposed method, we\nprovide a Brexit \"fever curve\" for the year 2019.\n"
    },
    {
        "paper_id": 1912.05917,
        "authors": "Alexandre Pannier and Antoine Jacquier",
        "title": "On the uniqueness of solutions of stochastic Volterra equations",
        "comments": "The proof of Proposition 3.6, on Page 10 -- part 3 of the proof --\n  contains an erroneous inequality",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We prove strong existence and uniqueness, and H\\\"older regularity, of a large\nclass of stochastic Volterra equations, with singular kernels and non-Lipschitz\ndiffusion coefficient. Extending Yamada-Watanabe's theorem, our proof relies on\nan approximation of the process by a sequence of semimartingales with\nregularised kernels. We apply these results to the rough Heston model, with\nsquare-root diffusion coefficient, recently proposed in Mathematical Finance to\nmodel the volatility of asset prices.\n"
    },
    {
        "paper_id": 1912.06031,
        "authors": "Jean-Philippe Aguilar",
        "title": "Some pricing tools for the Variance Gamma model",
        "comments": "Minor typos corrected - DOI added - 38 pages",
        "journal-ref": null,
        "doi": "10.1142/S0219024920500259",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We establish several closed pricing formula for various path-independent\npayoffs, under an exponential L\\'evy model driven by the Variance Gamma\nprocess. These formulas take the form of quickly convergent series and are\nobtained via tools from Mellin transform theory as well as from\nmultidimensional complex analysis. Particular focus is made on the symmetric\nprocess, but extension to the asymmetric process is also provided. Speed of\nconvergence and comparison with numerical methods (Fourier transform,\nquadrature approximations, Monte Carlo simulations) are also discussed; notable\nfeature is the accelerated convergence of the series for short term options,\nwhich constitutes an interesting improvement of numerical Fourier inversion\ntechniques.\n"
    },
    {
        "paper_id": 1912.06193,
        "authors": "Nick James, Max Menzies, Jennifer Chan",
        "title": "Changes to the extreme and erratic behaviour of cryptocurrencies during\n  COVID-19",
        "comments": "Accepted manuscript. Numerous minor edits compared to v3. Equal\n  contribution from first two authors",
        "journal-ref": "Physica A: Statistical Mechanics and its Applications 565 (2021)\n  125581",
        "doi": "10.1016/j.physa.2020.125581",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper introduces new methods for analysing the extreme and erratic\nbehaviour of time series to evaluate the impact of COVID-19 on cryptocurrency\nmarket dynamics. Across 51 cryptocurrencies, we examine extreme behaviour\nthrough a study of distribution extremities, and erratic behaviour through\nstructural breaks. First, we analyse the structure of the market as a whole and\nobserve a reduction in self-similarity as a result of COVID-19, particularly\nwith respect to structural breaks in variance. Second, we compare and contrast\nthese two behaviours, and identify individual anomalous cryptocurrencies.\nTether (USDT) and TrueUSD (TUSD) are consistent outliers with respect to their\nreturns, while Holo (HOT), NEXO (NEXO), Maker (MKR) and NEM (XEM) are\nfrequently observed as anomalous with respect to both behaviours and time. Even\namong a market known as consistently volatile, this identifies individual\ncryptocurrencies that behave most irregularly in their extreme and erratic\nbehaviour and shows these were more affected during the COVID-19 market crisis.\n"
    },
    {
        "paper_id": 1912.06202,
        "authors": "Emily Diana, Michael Kearns, Seth Neel, Aaron Roth",
        "title": "Optimal, Truthful, and Private Securities Lending",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We consider a fundamental dynamic allocation problem motivated by the problem\nof $\\textit{securities lending}$ in financial markets, the mechanism underlying\nthe short selling of stocks. A lender would like to distribute a finite number\nof identical copies of some scarce resource to $n$ clients, each of whom has a\nprivate demand that is unknown to the lender. The lender would like to maximize\nthe usage of the resource $\\mbox{---}$ avoiding allocating more to a client\nthan her true demand $\\mbox{---}$ but is constrained to sell the resource at a\npre-specified price per unit, and thus cannot use prices to incentivize\ntruthful reporting. We first show that the Bayesian optimal algorithm for the\none-shot problem $\\mbox{---}$ which maximizes the resource's expected usage\naccording to the posterior expectation of demand, given reports $\\mbox{---}$\nactually incentivizes truthful reporting as a dominant strategy. Because true\ndemands in the securities lending problem are often sensitive information that\nthe client would like to hide from competitors, we then consider the problem\nunder the additional desideratum of (joint) differential privacy. We give an\nalgorithm, based on simple dynamics for computing market equilibria, that is\nsimultaneously private, approximately optimal, and approximately\ndominant-strategy truthful. Finally, we leverage this private algorithm to\nconstruct an approximately truthful, optimal mechanism for the extensive form\nmulti-round auction where the lender does not have access to the true joint\ndistributions between clients' requests and demands.\n"
    },
    {
        "paper_id": 1912.06236,
        "authors": "Jie Fang, Shutao Xia, Jianwu Lin, Yong Jiang",
        "title": "Automatic Financial Feature Construction",
        "comments": "Its final version has been accepted by KDD ML in Finance 2020",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In automatic financial feature construction task, the state-of-the-art\ntechnic leverages reverse polish expression to represent the features, then use\ngenetic programming (GP) to conduct its evolution process. In this paper, we\npropose a new framework based on neural network, alpha discovery neural network\n(ADNN). In this work, we made several contributions. Firstly, in this task, we\nmake full use of neural network overwhelming advantage in feature extraction to\nconstruct highly informative features. Secondly, we use domain knowledge to\ndesign the object function, batch size, and sampling rules. Thirdly, we use\npre-training to replace the GP evolution process. According to neural network\nuniversal approximation theorem, pre-training can conduct a more effective and\nexplainable evolution process. Experiment shows that ADNN can remarkably\nproduce more diversified and higher informative features than GP. Besides, ADNN\ncan serve as a data augmentation algorithm. It further improves the the\nperformance of financial features constructed by GP.\n"
    },
    {
        "paper_id": 1912.0641,
        "authors": "P-J. Tisserand (LMT), M. Ragueneau",
        "title": "A mechanical and economical based framework to help decision-makers for\n  natural hazards and malicious events impact on infrastructure prevention",
        "comments": null,
        "journal-ref": "WCRR 2019, 12th World Congress on Railway Research, Oct 2019,\n  Tokyo, France",
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Many studies in economics deal with the non-reliability cost to assess\ninsurance fees or investment analyses, but none takes into consideration the\nmechanical aspect of reliability analysis. Other studies in mechanics give some\ntools and methods to carry out reliability analyses and fragility study. This\nstudy developed a framework where economical and mechanical considerations for\ninfrastructure investment decision-making. The theoretical reasoning is here\ndeveloped to couple mechanical reliability analyses, which are composed of\nfragility curves, and economical reliability analyses, which is based on\nresilience cost functions. This coupling is carried out with some probabilistic\nconsiderations, giving the concept of \"probable cost of failure\". The strength\nof this framework is that it can be used to analyze all possible critical\ncomponents in a network with all possible natural hazards or malicious event or\nother undesired events which it is possible to assess its probability of\noccurrence. The results of the analysis are indicators of probable cost of\nfailure of an infrastructure, which represents the insurance fee. These\nindicators can be computed for railway lines, for critical components, for\nevents. This tool enables decision-makers to prioritize safety investments and\nto guide strategic choices. The next step of this study will be to develop\nsmart data analysis tools, because of this framework needs and produces a lot\nof data, which must be smartly analyzed and presented.\n"
    },
    {
        "paper_id": 1912.06426,
        "authors": "Ying Chen and Ulrich Horst and Hoang Hai Tran",
        "title": "Portfolio liquidation under transient price impact -- theoretical\n  solution and implementation with 100 NASDAQ stocks",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We derive an explicit solution for deterministic market impact parameters in\nthe Graewe and Horst (2017) portfolio liquidation model. The model allows to\ncombine various forms of market impact, namely instantaneous, permanent and\ntemporary. We show that the solutions to the two benchmark models of Almgren\nand Chris (2001) and of Obizhaeva and Wang (2013) are obtained as special\ncases. We relate the different forms of market impact to the microstructure of\nlimit order book markets and show how the impact parameters can be estimated\nfrom public market data. We investigate the numerical performance of the\nderived optimal trading strategy based on high frequency limit order books of\n100 NASDAQ stocks that represent a range of market impact profiles. It shows\nthe strategy achieves significant cost savings compared to the benchmark models\nof Almgren and Chris (2001) and of Obizhaeva and Wang (2013).\n"
    },
    {
        "paper_id": 1912.06533,
        "authors": "Falko Baustian, Kate\\v{r}ina Filipov\\'a and Jan Posp\\'i\\v{s}il",
        "title": "Solution of option pricing equations using orthogonal polynomial\n  expansion",
        "comments": null,
        "journal-ref": "Applications of Mathematics, Volume 66 (4), pp. 553-582, 2021",
        "doi": "10.21136/AM.2021.0361-19",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper we study both analytic and numerical solutions of option\npricing equations using systems of orthogonal polynomials. Using a\nGalerkin-based method, we solve the parabolic partial diferential equation for\nthe Black-Scholes model using Hermite polynomials and for the Heston model\nusing Hermite and Laguerre polynomials. We compare obtained solutions to\nexisting semi-closed pricing formulas. Special attention is paid to the\nsolution of Heston model at the boundary with vanishing volatility.\n"
    },
    {
        "paper_id": 1912.06558,
        "authors": "Edward Wheatcroft, Henry Wynn, Kristina Lygnerud and Giorgio Bonvicini",
        "title": "The role of low temperature waste heat recovery in achieving 2050 goals:\n  a policy positioning paper",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Urban waste heat recovery, in which low temperature heat from urban sources\nis recovered for use in a district heat network, has a great deal of potential\nin helping to achieve 2050 climate goals. For example, heat from data centres,\nmetro systems, public sector buildings and waste water treatment plants could\nbe used to supply ten percent of Europe's heat demand. Despite this, at\npresent, urban waste heat recovery is not widespread and is an immature\ntechnology. To help achieve greater uptake, three policy recommendations are\nmade. First, policy raising awareness of waste heat recovery and creating a\nlegal framework is suggested. Second, it is recommended that pilot projects are\npromoted to help demonstrate technical and economic feasibility. Finally, a\npilot credit facility is proposed aimed at bridging the gap between potential\ninvestors and heat recovery projects.\n"
    },
    {
        "paper_id": 1912.06709,
        "authors": "Jan Posp\\'i\\v{s}il, Tom\\'a\\v{s} Sobotka and Philipp Ziegler",
        "title": "Robustness and sensitivity analyses for stochastic volatility models\n  under uncertain data structure",
        "comments": null,
        "journal-ref": "Empir. Econ. 57(6), 1935-1958, 2019",
        "doi": "10.1007/s00181-018-1535-3",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper we perform robustness and sensitivity analysis of several\ncontinuous-time stochastic volatility (SV) models with respect to the process\nof market calibration. The analyses should validate the hypothesis on\nimportance of the jump part in the underlying model dynamics. Also an impact of\nthe long memory parameter is measured for the approximative fractional SV\nmodel. For the first time, the robustness of calibrated models is measured\nusing bootstrapping methods on market data and Monte-Carlo filtering\ntechniques. In contrast to several other sensitivity analysis approaches for SV\nmodels, the newly proposed methodology does not require independence of\ncalibrated parameters - an assumption that is typically not satisfied in\npractice. Empirical study is performed on data sets of Apple Inc. equity\noptions traded in April and May 2015.\n"
    },
    {
        "paper_id": 1912.06809,
        "authors": "Lynn Boen, Karel J. in 't Hout",
        "title": "Operator splitting schemes for American options under the two-asset\n  Merton jump-diffusion model",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper deals with the efficient numerical solution of the two-dimensional\npartial integro-differential complementarity problem (PIDCP) that holds for the\nvalue of American-style options under the two-asset Merton jump-diffusion\nmodel. We consider the adaptation of various operator splitting schemes of both\nthe implicit-explicit (IMEX) and the alternating direction implicit (ADI) kind\nthat have recently been studied for partial integro-differential equations\n(PIDEs) in [3]. Each of these schemes conveniently treats the nonlocal integral\npart in an explicit manner. Their adaptation to PIDCPs is achieved through a\ncombination with the Ikonen-Toivanen splitting technique [14] as well as with\nthe penalty method [32]. The convergence behaviour and relative performance of\nthe acquired eight operator splitting methods is investigated in extensive\nnumerical experiments for American put-on-the-min and put-on-the-average\noptions.\n"
    },
    {
        "paper_id": 1912.06916,
        "authors": "\\c{C}a\\u{g}{\\i}n Ararat, Zachary Feinstein",
        "title": "Set-Valued Risk Measures as Backward Stochastic Difference Inclusions\n  and Equations",
        "comments": "27 pages",
        "journal-ref": "Finance and Stochastics 25 (1), 43-76, (2021)",
        "doi": "10.1007/s00780-020-00445-0",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Scalar dynamic risk measures for univariate positions in continuous time are\ncommonly represented as backward stochastic differential equations. In the\nmultivariate setting, dynamic risk measures have been defined and studied as\nfamilies of set-valued functionals in the recent literature. There are two\npossible extensions of scalar backward stochastic differential equations for\nthe set-valued framework: (1) backward stochastic differential inclusions,\nwhich evaluate the risk dynamics on the selectors of acceptable capital\nallocations; or (2) set-valued backward stochastic differential equations,\nwhich evaluate the risk dynamics on the full set of acceptable capital\nallocations as a singular object. In this work, the discrete time setting is\ninvestigated with difference inclusions and difference equations in order to\nprovide insights for such differential representations for set-valued dynamic\nrisk measures in continuous time.\n"
    },
    {
        "paper_id": 1912.06948,
        "authors": "Svetlana Boyarchenko and Sergei Levendorski\\u{i}",
        "title": "Gauge transformations in the dual space, and pricing and estimation in\n  the long run in affine jump-diffusion models",
        "comments": "Several typos are corrected, Theorem 3.3 and Lemma 4.6 are added",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We suggest a simple reduction of pricing European options in affine\njump-diffusion models to pricing options with modified payoffs in diffusion\nmodels. The procedure is based on the conjugation of the infinitesimal\ngenerator of the model with an operator of the form $e^{i\\Phi(-i\\dd_x)}$ (gauge\ntransformation in the dual space). A general procedure for the calculation of\nthe function $\\Phi$ is given, with examples. As applications, we consider\npricing in jump-diffusion models and their subordinated versions using the\neigenfunction expansion technique, and estimation of the extremely rare jumps\ncomponent. The beliefs of the market about yet unobserved extreme jumps and\npricing kernel can be recovered: the market prices allow one to see \"the shape\nof things to come\".\n"
    },
    {
        "paper_id": 1912.07115,
        "authors": "Olga Ivanova, d'Artis Kancs, Mark Thissen",
        "title": "EU Economic Modelling System",
        "comments": "30 pages",
        "journal-ref": null,
        "doi": "10.2791/184008",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This is the first study that attempts to assess the regional economic impacts\nof the European Institute of Innovation and Technology (EIT) investments in a\nspatially explicit macroeconomic model, which allows us to take into account\nall key direct, indirect and spatial spillover effects of EIT investments via\ninter-regional trade and investment linkages and a spatial diffusion of\ntechnology via an endogenously determined global knowledge frontier with\nendogenous growth engines driven by investments in knowledge and human capital.\nOur simulation results of highly detailed EIT expenditure data suggest that,\nbesides sizable direct effects in those regions that receive the EIT investment\nsupport, there are also significant spatial spillover effects to other\n(non-supported) EU regions. Taking into account all key indirect and spatial\nspillover effects is a particular strength of the adopted spatial general\nequilibrium methodology; our results suggest that they are important indeed and\nneed to be taken into account when assessing the impacts of EIT investment\npolicies on regional economies.\n"
    },
    {
        "paper_id": 1912.07163,
        "authors": "Pascal Michaillat, Emmanuel Saez",
        "title": "An Economical Business-Cycle Model",
        "comments": null,
        "journal-ref": "Oxford Economic Papers 74 (2022) 382-411",
        "doi": "10.1093/oep/gpab021",
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  This paper develops a new model of business cycles. The model is economical\nin that it is solved with an aggregate demand-aggregate supply diagram, and the\neffects of shocks and policies are obtained by comparative statics. The model\nbuilds on two unconventional assumptions. First, producers and consumers meet\nthrough a matching function. Thus, the model features unemployment, which\nfluctuates in response to aggregate demand and supply shocks. Second, wealth\nenters the utility function, so the model allows for permanent zero-lower-bound\nepisodes. In the model, the optimal monetary policy is to set the interest rate\nat the level that eliminates the unemployment gap. This optimal interest rate\nis computed from the prevailing unemployment gap and monetary multiplier (the\neffect of the nominal interest rate on the unemployment rate). If the\nunemployment gap is exceedingly large, monetary policy cannot eliminate it\nbefore reaching the zero lower bound, but a wealth tax can.\n"
    },
    {
        "paper_id": 1912.07165,
        "authors": "Ao Kong, Hongliang Zhu, Robert Azencott",
        "title": "Predicting intraday jumps in stock prices using liquidity measures and\n  technical indicators",
        "comments": "45 pages, 9 figures, 10 tables",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Predicting the intraday stock jumps is a significant but challenging problem\nin finance. Due to the instantaneity and imperceptibility characteristics of\nintraday stock jumps, relevant studies on their predictability remain limited.\nThis paper proposes a data-driven approach to predict intraday stock jumps\nusing the information embedded in liquidity measures and technical indicators.\nSpecifically, a trading day is divided into a series of 5-minute intervals, and\nat the end of each interval, the candidate attributes defined by liquidity\nmeasures and technical indicators are input into machine learning algorithms to\npredict the arrival of a stock jump as well as its direction in the following\n5-minute interval. Empirical study is conducted on the level-2 high-frequency\ndata of 1271 stocks in the Shenzhen Stock Exchange of China to validate our\napproach. The result provides initial evidence of the predictability of jump\narrivals and jump directions using level-2 stock data as well as the\neffectiveness of using a combination of liquidity measures and technical\nindicators in this prediction. We also reveal the superiority of using random\nforest compared to other machine learning algorithms in building prediction\nmodels. Importantly, our study provides a portable data-driven approach that\nexploits liquidity and technical information from level-2 stock data to predict\nintraday price jumps of individual stocks.\n"
    },
    {
        "paper_id": 1912.07445,
        "authors": "Eduardo Abi Jaber (CES)",
        "title": "Weak existence and uniqueness for affine stochastic Volterra equations\n  with L1-kernels",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We provide existence, uniqueness and stability results for affine stochastic\nVolterra equations with $L^1$-kernels and jumps. Such equations arise as\nscaling limits of branching processes in population genetics and self-exciting\nHawkes processes in mathematical finance. The strategy we adopt for the\nexistence part is based on approximations using stochastic Volterra equations\nwith $L^2$-kernels combined with a general stability result. Most importantly,\nwe establish weak uniqueness using a duality argument on the Fourier--Laplace\ntransform via a deterministic Riccati--Volterra integral equation. We\nillustrate the applicability of our results on Hawkes processes and a class of\nhyper-rough Volterra Heston models with a Hurst index $H \\in (-1/2,1/2]$.\n"
    },
    {
        "paper_id": 1912.07601,
        "authors": "Joaquim Andrade, Pedro Cordeiro and Guilherme Lambais",
        "title": "Estimating a Behavioral New Keynesian Model",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper analyzes identification issues of a behavorial New Keynesian model\nand estimates it using likelihood-based and limited-information methods with\nidentification-robust confidence sets. The model presents some of the same\ndifficulties that exist in simple benchmark DSGE models, but the analytical\nsolution is able to indicate in what conditions the cognitive discounting\nparameter (attention to the future) can be identified and the robust estimation\nmethods is able to confirm its importance for explaining the proposed\nbehavioral model.\n"
    },
    {
        "paper_id": 1912.077,
        "authors": "Sidra Mehtab, Jaydip Sen",
        "title": "A Robust Predictive Model for Stock Price Prediction Using Deep Learning\n  and Natural Language Processing",
        "comments": "6 pages, 18 Tables",
        "journal-ref": null,
        "doi": "10.36227/techrxiv.15023361.v1",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Prediction of future movement of stock prices has been a subject matter of\nmany research work. There is a gamut of literature of technical analysis of\nstock prices where the objective is to identify patterns in stock price\nmovements and derive profit from it. Improving the prediction accuracy remains\nthe single most challenge in this area of research. We propose a hybrid\napproach for stock price movement prediction using machine learning, deep\nlearning, and natural language processing. We select the NIFTY 50 index values\nof the National Stock Exchange of India, and collect its daily price movement\nover a period of three years (2015 to 2017). Based on the data of 2015 to 2017,\nwe build various predictive models using machine learning, and then use those\nmodels to predict the closing value of NIFTY 50 for the period January 2018\ntill June 2019 with a prediction horizon of one week. For predicting the price\nmovement patterns, we use a number of classification techniques, while for\npredicting the actual closing price of the stock, various regression models\nhave been used. We also build a Long and Short-Term Memory - based deep\nlearning network for predicting the closing price of the stocks and compare the\nprediction accuracies of the machine learning models with the LSTM model. We\nfurther augment the predictive model by integrating a sentiment analysis module\non twitter data to correlate the public sentiment of stock prices with the\nmarket sentiment. This has been done using twitter sentiment and previous week\nclosing values to predict stock price movement for the next week. We tested our\nproposed scheme using a cross validation method based on Self Organizing Fuzzy\nNeural Networks and found extremely interesting results.\n"
    },
    {
        "paper_id": 1912.07701,
        "authors": "Lucia Larise Stavarache (1), Donatas Narbutis (2), Toyotaro Suzumura\n  (3), Ray Harishankar (1), Augustas \\v{Z}altauskas (2) ((1) IBM Global\n  Business Services, (2) IBM Lithuania, Client Innovation Center Baltic, (3)\n  IBM T.J. Watson Research Center)",
        "title": "Exploring Multi-Banking Customer-to-Customer Relations in AML Context\n  with Poincar\\'e Embeddings",
        "comments": "NeurIPS 2019 Workshop on Robust AI in Financial Services\n  (https://sites.google.com/view/robust-ai-in-fs-2019)",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In the recent years money laundering schemes have grown in complexity and\nspeed of realization, affecting financial institutions and millions of\ncustomers globally. Strengthened privacy policies, along with in-country\nregulations, make it hard for banks to inner- and cross-share, and report\nsuspicious activities for the AML (Anti-Money Laundering) measures. Existing\ntopologies and models for AML analysis and information sharing are subject to\nmajor limitations, such as compliance with regulatory constraints, extended\ninfrastructure to run high-computation algorithms, data quality and span,\nproving cumbersome and costly to execute, federate, and interpret. This paper\nproposes a new topology for exploring multi-banking customer social relations\nin AML context -- customer-to-customer, customer-to-transaction, and\ntransaction-to-transaction -- using a 3D modeling topological algebra\nformulated through Poincar\\'e embeddings.\n"
    },
    {
        "paper_id": 1912.08695,
        "authors": "Zachary Feinstein and Andreas Sojmark",
        "title": "A Dynamic Default Contagion Model: From Eisenberg-Noe to the Mean Field",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this work we introduce a model of default contagion that combines the\napproaches of Eisenberg-Noe interbank networks and dynamic mean field\ninteractions. The proposed contagion mechanism provides an endogenous rule for\nearly defaults in a network of financial institutions. The main result is to\ndemonstrate a mean field interaction that can be found as the limit of the\nfinite bank system generated from a finite Eisenberg-Noe style network. In this\nway, we connect two previously disparate frameworks for systemic risk, and in\nturn we provide a bridge for exploiting recent advances in mean field analysis\nwhen modelling systemic risk. The mean field limit is shown to be well-posed\nand is identified as a certain conditional McKean-Vlasov type problem that\nrespects the original network topology under suitable assumptions.\n"
    },
    {
        "paper_id": 1912.08713,
        "authors": "Andrey Itkin and Fazlollah Soleymani",
        "title": "Four-factor model of Quanto CDS with jumps-at-default and stochastic\n  recovery",
        "comments": "30 pages, 7 figures, 2 tables. arXiv admin note: text overlap with\n  arXiv:1711.07133",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper we modify the model of Itkin, Shcherbakov and Veygman, (2019)\n(ISV2019), proposed for pricing Quanto Credit Default Swaps (CDS) and risky\nbonds, in several ways. First, it is known since the Lehman Brothers bankruptcy\nthat the recovery rate could significantly vary right before or at default,\ntherefore, in this paper we consider it to be stochastic. Second, to reduce\ncomplexity of the model, we treat the domestic interest rate as deterministic,\nbecause, as shown in ISV2019, volatility of the domestic interest rate does not\ncontribute much to the value of the Quanto CDS spread. Finally, to solve the\ncorresponding systems of 4D partial differential equations we use a different\nflavor of the Radial Basis Function (RBF) method which is a combination of\nlocalized RBF and finite-difference methods, and is known in the literature as\nRBF-FD. Results of our numerical experiments presented in the paper demonstrate\nthat the influence of volatility of the recovery rate is significant if the\ncorrelation between the recovery rate and the log-intensity of the default is\nnon-zero. Also, the impact of the recovery mean-reversion rate on the Quanto\nCDS spread could be comparable with the impact due to jump-at-default in the FX\nrate.\n"
    },
    {
        "paper_id": 1912.08791,
        "authors": "Firuz Kamalov",
        "title": "Forecasting significant stock price changes using neural networks",
        "comments": null,
        "journal-ref": "Neural Computing and Applications (2020)",
        "doi": "10.1007/s00521-020-04942-3",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Stock price prediction is a rich research topic that has attracted interest\nfrom various areas of science. The recent success of machine learning in speech\nand image recognition has prompted researchers to apply these methods to asset\nprice prediction. The majority of literature has been devoted to predicting\neither the actual asset price or the direction of price movement. In this\npaper, we study a hitherto little explored question of predicting significant\nchanges in stock price based on previous changes using machine learning\nalgorithms. We are particularly interested in the performance of neural network\nclassifiers in the given context. To this end, we construct and test three\nneural network models including multi-layer perceptron, convolutional net, and\nlong short term memory net. As benchmark models we use random forest and\nrelative strength index methods. The models are tested using 10-year daily\nstock price data of four major US public companies. Test results show that\npredicting significant changes in stock price can be accomplished with a high\ndegree of accuracy. In particular, we obtain substantially better results than\nsimilar studies that forecast the direction of price change.\n"
    },
    {
        "paper_id": 1912.08863,
        "authors": "Erhan Bayraktar, Leonid Dolinskyi and Yan Dolinsky",
        "title": "Extended Weak Convergence and Utility Maximization with Proportional\n  Transaction Costs",
        "comments": "to appear in Finance and Stochastics",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper we study utility maximization with proportional transaction\ncosts. Assuming extended weak convergence of the underlying processes we prove\nthe convergence of the corresponding utility maximization problems. Moreover,\nwe establish a limit theorem for the optimal trading strategies. The proofs are\nbased on the extended weak convergence theory developed in [1] and the\nMeyer--Zheng topology introduced in [24].\n"
    },
    {
        "paper_id": 1912.08916,
        "authors": "Shteryo Nozharov",
        "title": "Hybrid threats as an exogenous economic shock",
        "comments": null,
        "journal-ref": "Economic Archive (4), pp.21-29 (2019)",
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The aim of this study is to contribute to the theory of exogenous economic\nshocks and their equivalents in an attempt to explain business cycle\nfluctuations, which still do not have a clear explanation. To this end the\nauthor has developed an econometric model based on a regression analysis.\nAnother objective is to tackle the issue of hybrid threats, which have not yet\nbeen subjected to a cross-disciplinary research. These were reviewed in terms\nof their economic characteristics in order to complement research in the fields\nof defence and security.\n"
    },
    {
        "paper_id": 1912.09012,
        "authors": "Brett R Gordon, Kinshuk Jerath, Zsolt Katona, Sridhar Narayanan,\n  Jiwoong Shin, Kenneth C Wilbur",
        "title": "Inefficiencies in Digital Advertising Markets",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Digital advertising markets are growing and attracting increased scrutiny.\nThis paper explores four market inefficiencies that remain poorly understood:\nad effect measurement, frictions between and within advertising channel\nmembers, ad blocking and ad fraud. These topics are not unique to digital\nadvertising, but each manifests in new ways in markets for digital ads. We\nidentify relevant findings in the academic literature, recent developments in\npractice, and promising topics for future research.\n"
    },
    {
        "paper_id": 1912.09273,
        "authors": "Safoora Zarei and Ali R. Fallahi",
        "title": "Pay-As-You-Drive Insurance Pricing Model",
        "comments": null,
        "journal-ref": "American Journal of Statistics and Actuarial Science, Vol.2, Issue\n  1, pp 1 - 9, 2020",
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Every time drivers take to the road, and with each mile that they drive,\nexposes themselves and others to the risk of an accident. Insurance premiums\nare only weakly linked to mileage, however, and have lump-sum characteristics\nlargely. The result is too much driving, and too many accidents. In this paper,\nwe introduce some useful theoretical results for Pay-As-You-Drive in Automobile\ninsurances. We consider a counting process and also find the distribution of\ndiscounted collective risk model when the counting process is non-homogeneous\nPoisson.\n"
    },
    {
        "paper_id": 1912.09524,
        "authors": "David Rushing Dewhurst, Yi Li, Alexander Bogdan, Jasmine Geng",
        "title": "Evolving ab initio trading strategies in heterogeneous environments",
        "comments": "20 pages (10 main body, 10 appendix), 11 figures (6 main body, 5\n  appendix), open-source matching engine implementation available at\n  https://gitlab.com/daviddewhurst/verdantcurve",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Securities markets are quintessential complex adaptive systems in which\nheterogeneous agents compete in an attempt to maximize returns. Species of\ntrading agents are also subject to evolutionary pressure as entire classes of\nstrategies become obsolete and new classes emerge. Using an agent-based model\nof interacting heterogeneous agents as a flexible environment that can\nendogenously model many diverse market conditions, we subject deep neural\nnetworks to evolutionary pressure to create dominant trading agents. After\nanalyzing the performance of these agents and noting the emergence of anomalous\nsuperdiffusion through the evolutionary process, we construct a method to turn\nhigh-fitness agents into trading algorithms. We backtest these trading\nalgorithms on real high-frequency foreign exchange data, demonstrating that\nelite trading algorithms are consistently profitable in a variety of market\nconditions---even though these algorithms had never before been exposed to real\nfinancial data. These results provide evidence to suggest that developing\n\\textit{ab initio} trading strategies by repeated simulation and evolution in a\nmechanistic market model may be a practical alternative to explicitly training\nmodels with past observed market data.\n"
    },
    {
        "paper_id": 1912.09569,
        "authors": "Leonardo Belen, Alejandro Baranek, Xavier Gonzalez",
        "title": "Argentum: a collaborative saving and investment platform for unstable\n  countries",
        "comments": "14 pages, 3 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
        "abstract": "  A crypto coin designed to provide a stabilization instrument backed up by\nminded like financial investments instruments to maintain the purchase value of\nsavings across time, in order to construct new tools for unstable economies.\n"
    },
    {
        "paper_id": 1912.09573,
        "authors": "Alev Meral",
        "title": "Comparison of various risk measures for an optimal portfolio",
        "comments": "28 pages, 9 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper, we search for optimal portfolio strategies in the presence of\nvarious risk measure that are common in financial applications. Particularly,\nwe deal with the static optimization problem with respect to Value at Risk,\nExpected Loss and Expected Utility Loss measures. To do so, under the Black-\nScholes model for the financial market, Martingale method is applied to give\nclosed-form solutions for the optimal terminal wealths; then via representation\nproblem the optimal portfolio strategies are achieved. We compare the\nperformances of these measures on the terminal wealths and optimal strategies\nof such constrained investors. Finally, we present some numerical results to\ncompare them in several respects to give light to further studies.\n"
    },
    {
        "paper_id": 1912.09679,
        "authors": "Christian Lax, Torsten Trimborn",
        "title": "From Disequilibrium Markets to Equilibrium",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The modeling of financial markets as disequilibrium models by ordinary\ndifferential equations has become a popular modeling tool. One famous example\nof such a model is the Beja-Goldman model(The Journal of Finance, 1980) which\nwe consider in this paper. We study the passage from disequilibrium dynamics to\nequilibrium. Mathematically, this limit corresponds to an asymptotic limit also\nknown as a Tikhonov-Fenichel reduction. Furthermore, we analyze the stability\nof the reduced equilibrium model and discuss the economic implications. We\nconduct several numerical examples to visualize and support our analysis.\n"
    },
    {
        "paper_id": 1912.09702,
        "authors": "Anastasios Evgenidis and Apostolos Fasianos",
        "title": "Monetary Policy and Wealth Inequalities in Great Britain: Assessing the\n  role of unconventional policies for a decade of household data",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper explores whether unconventional monetary policy operations have\nredistributive effects on household wealth. Drawing on household balance sheet\ndata from the Wealth and Asset Survey, we construct monthly time series\nindicators on the distribution of different asset types held by British\nhouseholds for the period that the monetary policy switched as the policy rate\nreached the zero lower bound (2006-2016). Using this series, we estimate the\nresponse of wealth inequalities on monetary policy, taking into account the\neffect of unconventional policies conducted by the Bank of England in response\nto the Global Financial Crisis. Our evidence reveals that unconventional\nmonetary policy shocks have significant long-lasting effects on wealth\ninequality: an expansionary monetary policy in the form of asset purchases\nraises wealth inequality across households, as measured by their Gini\ncoefficients of net wealth, housing wealth, and financial wealth. The evidence\nof our analysis helps to raise awareness of central bankers about the\nredistributive effects of their monetary policy decisions.\n"
    },
    {
        "paper_id": 1912.09764,
        "authors": "Angela Rita Provenzano, Daniele Trifir\\`o, Nicola Jean, Giacomo Le\n  Pera, Maurizio Spadaccino, Luca Massaron and Claudio Nordio",
        "title": "An Artificial Intelligence approach to Shadow Rating",
        "comments": "18 pages, 10 figures, 6 tables",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We analyse the effectiveness of modern deep learning techniques in predicting\ncredit ratings over a universe of thousands of global corporate entities\nobligations when compared to most popular, traditional machine-learning\napproaches such as linear models and tree-based classifiers. Our results show a\nadequate accuracy over different rating classes when applying categorical\nembeddings to artificial neural networks (ANN) architectures.\n"
    },
    {
        "paper_id": 1912.09814,
        "authors": "Aymeric Vi\\'e and Alfredo J. Morales",
        "title": "How connected is too connected? Impact of network topology on systemic\n  risk and collapse of complex economic systems",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Economic interdependencies have become increasingly present in globalized\nproduction, financial and trade systems. While establishing interdependencies\namong economic agents is crucial for the production of complex products, they\nmay also increase systemic risks due to failure propagation. It is crucial to\nidentify how network connectivity impacts both the emergent production and risk\nof collapse of economic systems. In this paper we propose a model to study the\neffects of network structure on the behavior of economic systems by varying the\ndensity and centralization of connections among agents. The complexity of\nproduction increases with connectivity given the combinatorial explosion of\nparts and products. Emergent systemic risks arise when interconnections\nincrease vulnerabilities. Our results suggest a universal description of\neconomic collapse given in the emergence of tipping points and phase\ntransitions in the relationship between network structure and risk of\nindividual failure. This relationship seems to follow a sigmoidal form in the\ncase of increasingly denser or centralized networks. The model sheds new light\non the relevance of policies for the growth of economic complexity, and\nhighlights the trade-off between increasing the potential production of the\nsystem and its robustness to collapse. We discuss the policy implications of\nintervening in the organization of interconnections and system features, and\nstress how different network structures and node characteristics suggest\ndifferent directions in order to promote complex and robust economic systems.\n"
    },
    {
        "paper_id": 1912.09964,
        "authors": "Mark Kiermayer, Christian Wei{\\ss}",
        "title": "Grouping of Contracts in Insurance using Neural Networks",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Despite the high importance of grouping in practice, there exists little\nresearch on the respective topic. The present work presents a complete\nframework for grouping and a novel method to optimize model points. Model\npoints are used to substitute clusters of contracts in an insurance portfolio\nand thus yield a smaller, computationally less burdensome portfolio. This\ngrouped portfolio is controlled to have similar characteristics as the original\nportfolio. We provide numerical results for term life insurance and defined\ncontribution plans, which indicate the superiority of our approach compared to\nK-means clustering, a common baseline algorithm for grouping. Lastly, we show\nthat the presented concept can optimize a fixed number of model points for the\nentire portfolio simultaneously. This eliminates the need for any\npre-clustering of the portfolio, e.g. by K-means clustering, and therefore\npresents our method as an entirely new and independent methodology.\n"
    },
    {
        "paper_id": 1912.10097,
        "authors": "Niklas Stoehr, Fabian Braesemann, Michael Frommelt, Shi Zhou",
        "title": "Mining the Automotive Industry: A Network Analysis of Corporate\n  Positioning and Technological Trends",
        "comments": "Preprint version to be published in Springer Nature (presented at\n  CompleNet 2020)",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The digital transformation is driving revolutionary innovations and new\nmarket entrants threaten established sectors of the economy such as the\nautomotive industry. Following the need for monitoring shifting industries, we\npresent a network-centred analysis of car manufacturer web pages. Solely\nexploiting publicly-available information, we construct large networks from web\npages and hyperlinks. The network properties disclose the internal corporate\npositioning of the three largest automotive manufacturers, Toyota, Volkswagen\nand Hyundai with respect to innovative trends and their international outlook.\nWe tag web pages concerned with topics like e-mobility and environment or\nautonomous driving, and investigate their relevance in the network. Sentiment\nanalysis on individual web pages uncovers a relationship between page linking\nand use of positive language, particularly with respect to innovative trends.\nWeb pages of the same country domain form clusters of different size in the\nnetwork that reveal strong correlations with sales market orientation. Our\napproach maintains the web content's hierarchical structure imposed by the web\npage networks. It, thus, presents a method to reveal hierarchical structures of\nunstructured text content obtained from web scraping. It is highly transparent,\nreproducible and data driven, and could be used to gain complementary insights\ninto innovative strategies of firms and competitive landscapes, which would not\nbe detectable by the analysis of web content alone.\n"
    },
    {
        "paper_id": 1912.10105,
        "authors": "Yitao Li and Umar Islambekov and Cuneyt Akcora and Ekaterina Smirnova\n  and Yulia R. Gel and Murat Kantarcioglu",
        "title": "Dissecting Ethereum Blockchain Analytics: What We Learn from Topology\n  and Geometry of Ethereum Graph",
        "comments": "Will appear in SIAM International Conference on Data Mining (SDM20).\n  May 7 - 9, 2020. Cincinnati, Ohio, U.S",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Blockchain technology and, in particular, blockchain-based cryptocurrencies\noffer us information that has never been seen before in the financial world. In\ncontrast to fiat currencies, all transactions of crypto-currencies and\ncrypto-tokens are permanently recorded on distributed ledgers and are publicly\navailable. As a result, this allows us to construct a transaction graph and to\nassess not only its organization but to glean relationships between transaction\ngraph properties and crypto price dynamics. The ultimate goal of this paper is\nto facilitate our understanding on horizons and limitations of what can be\nlearned on crypto-tokens from local topology and geometry of the Ethereum\ntransaction network whose even global network properties remain scarcely\nexplored. By introducing novel tools based on topological data analysis and\nfunctional data depth into Blockchain Data Analytics, we show that Ethereum\nnetwork (one of the most popular blockchains for creating new crypto-tokens)\ncan provide critical insights on price strikes of crypto-tokens that are\notherwise largely inaccessible with conventional data sources and traditional\nanalytic methods.\n"
    },
    {
        "paper_id": 1912.10237,
        "authors": "Gifty Malhotra, R. Srivastava, H.C. Taneja",
        "title": "Comparative Study of Two Extensions of Heston Stochastic Volatility\n  Model",
        "comments": "15 pages, 3 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In the option valuation literature, the shortcomings of one factor stochastic\nvolatility models have traditionally been addressed by adding jumps to the\nstock price process. An alternate approach in the context of option pricing and\ncalibration of implied volatility is the addition of a few other factors to the\nvolatility process. This paper contemplates two extensions of the Heston\nstochastic volatility model. Out of which, one considers the addition of jumps\nto the stock price process (a stochastic volatility jump diffusion model) and\nanother considers an additional stochastic volatility factor varying at a\ndifferent time scale (a multiscale stochastic volatility model). An empirical\nanalysis is carried out on the market data of options with different strike\nprices and maturities, to compare the pricing performance of these models and\nto capture their implied volatility fit. The unknown parameters of these models\nare calibrated using the non-linear least square optimization. It has been\nfound that the multiscale stochastic volatility model performs better than the\nHeston stochastic volatility model and the stochastic volatility jump diffusion\nmodel for the data set under consideration.\n"
    },
    {
        "paper_id": 1912.10328,
        "authors": "Maziar Sahamkhadam, Andreas Stephan",
        "title": "Portfolio optimization based on forecasting models using vine copulas:\n  An empirical assessment for the financial crisis",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We employ and examine vine copulas in modeling symmetric and asymmetric\ndependency structures and forecasting financial returns. We analyze the asset\nallocations performed during the 2008-2009 financial crisis and test different\nportfolio strategies such as maximum Sharpe ratio, minimum variance, and\nminimum conditional Value-at-Risk. We then specify the regular, drawable, and\ncanonical vine copulas, such as the Student-t, Clayton, Frank, Joe, Gumbel, and\nmixed copulas, and analyze both in-sample and out-of-sample portfolio\nperformances. Out-of-sample portfolio back-testing shows that vine copulas\nreduce portfolio risk better than simple copulas. Our econometric analysis of\nthe outcomes of the various models shows that in terms of reducing conditional\nValue-at-Risk, D-vines appear to be better than R- and C-vines. Overall, we\nfind that the Student-t drawable vine copula models perform best with regard to\nrisk reduction, both for the entire period 2005-2012 as well as during the\nfinancial crisis.\n"
    },
    {
        "paper_id": 1912.10343,
        "authors": "Boyue Fang, Yutong Feng",
        "title": "Design of High-Frequency Trading Algorithm Based on Machine Learning",
        "comments": "23pages, 11figures, 6tables, 3algorithms",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Based on iterative optimization and activation function in deep learning, we\nproposed a new analytical framework of high-frequency trading information, that\nreduced structural loss in the assembly of Volume-synchronized probability of\nInformed Trading ($VPIN$), Generalized Autoregressive Conditional\nHeteroscedasticity (GARCH) and Support Vector Machine (SVM) to make full use of\nthe order book information. Amongst the return acquisition procedure in\nmarket-making transactions, uncovering the relationship between discrete\ndimensional data from the projection of high-dimensional time-series would\nsignificantly improve the model effect. $VPIN$ would prejudge market liquidity,\nand this effectiveness backtested with CSI300 futures return.\n"
    },
    {
        "paper_id": 1912.1038,
        "authors": "Shuxin Guo, Qiang Liu",
        "title": "The Black-Scholes-Merton dual equation",
        "comments": "34 pages, 5 tables",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  We derive the Black-Scholes-Merton dual equation, which has exactly the same\nform as the Black-Scholes-Merton equation. The novel and general equation works\nfor options with a payoff of homogeneous of degree one, including European,\nAmerican, Bermudan, Asian, barrier, lookback, etc., and leads to new insights\ninto pricing and hedging. Perceptibly, a put-call equality emerges - all the\nput options can be priced as their corresponding calls by simultaneously\nswapping stock price (dividend yield) for strike price (risk-free rate), and\nvice versa. Equally important, we provide simple analytic formulas for hedging\nparameters delta and gamma, which elevate the put-call equality to true\npractical applicability. For futures options, the put-call equality leads to\n\"symmetric\" properties between puts and calls or among puts (calls).\n"
    },
    {
        "paper_id": 1912.10526,
        "authors": "Gary Venter and Kailan Shang",
        "title": "Building and Testing Yield Curve Generators for P&C Insurance",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Interest-rate risk is a key factor for property-casualty insurer capital. P&C\ncompanies tend to be highly leveraged, with bond holdings much greater than\ncapital. For GAAP capital, bonds are marked to market but liabilities are not,\nso shifts in the yield curve can have a significant impact on capital.\nYield-curve scenario generators are one approach to quantifying this risk. They\nproduce many future simulated evolutions of the yield curve, which can be used\nto quantify the probabilities of bond-value changes that would result from\nvarious maturity-mix strategies. Some of these generators are provided as\nblack-box models where the user gets only the projected scenarios. One focus of\nthis paper is to provide methods for testing generated scenarios from such\nmodels by comparing to known distributional properties of yield curves.\n  P&C insurers hold bonds to maturity and manage cash-flow risk by matching\nasset and liability flows. Derivative pricing and stochastic volatility are of\nlittle concern over the relevant time frames. This requires different models\nand model testing than what is common in the broader financial markets.\n  To complicate things further, interest rates for the last decade have not\nbeen following the patterns established in the sixty years following WWII. We\nare now coming out of the period of very low rates, yet are still not returning\nto what had been thought of as normal before that. Modeling and model testing\nare in an evolving state while new patterns emerge.\n  Our analysis starts with a review of the literature on interest-rate model\ntesting, with a P&C focus, and an update of the tests for current market\nbehavior. We then discuss models, and use them to illustrate the fitting and\ntesting methods. The testing discussion does not require the model-building\nsection.\n"
    },
    {
        "paper_id": 1912.1064,
        "authors": "Gifty Malhotra, R. Srivastava, H.C. Taneja",
        "title": "Pricing of the Geometric Asian Options Under a Multifactor Stochastic\n  Volatility Model",
        "comments": "29 pages, 2 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper focuses on the pricing of continuous geometric Asian options\n(GAOs) under a multifactor stochastic volatility model. The model considers\nfast and slow mean reverting factors of volatility, where slow volatility\nfactor is approximated by a quadratic arc. The asymptotic expansion of the\nprice function is assumed, and the first order price approximation is derived\nusing the perturbation techniques for both floating and fixed strike GAOs. Much\nsimplified pricing formulae for the GAOs are obtained in this multifactor\nstochastic volatility framework. The zeroth order term in the price\napproximation is the modified Black-Scholes price for the GAOs. This modified\nprice is expressed in terms of the Black-Scholes price for the GAOs. The\naccuracy of the approximate option pricing formulae is established, and the\nmodel parameter is also estimated by capturing the volatility smiles.\n"
    },
    {
        "paper_id": 1912.10709,
        "authors": "Yijian Chuan, Lan Wu",
        "title": "Centralizing-Unitizing Standardized High-Dimensional Directional\n  Statistics and Its Applications in Finance",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Cross-sectional \"Information Coefficient\" (IC) is a widely and deeply\naccepted measure in portfolio management. The paper gives an insight into IC in\nview of high-dimensional directional statistics: IC is a linear operator on the\ncomponents of a centralizing-unitizing standardized random vector of\nnext-period cross-sectional returns. Our primary research first clearly defines\nIC with the high-dimensional directional statistics, discussing its first two\nmoments. We derive the closed-form expressions of the directional statistics'\ncovariance matrix and IC's variance in a homoscedastic condition. Also, we\nsolve the optimization of IC's maximum expectation and minimum variance.\nSimulation intuitively characterizes the standardized directional statistics\nand IC's p.d.f.. The empirical analysis of the Chinese stock market uncovers\ninteresting facts about the standardized vectors of cross-sectional returns and\nhelps obtain the time series of the measure in the real market. The paper\ndiscovers a potential application of directional statistics in finance, proves\nexplicit results of the projected normal distribution, and reveals IC's nature.\n"
    },
    {
        "paper_id": 1912.10806,
        "authors": "Xinyi Li, Yinchuan Li, Hongyang Yang, Liuqing Yang, Xiao-Yang Liu",
        "title": "DP-LSTM: Differential Privacy-inspired LSTM for Stock Prediction Using\n  Financial News",
        "comments": "arXiv admin note: text overlap with arXiv:1908.01112",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Stock price prediction is important for value investments in the stock\nmarket. In particular, short-term prediction that exploits financial news\narticles is promising in recent years. In this paper, we propose a novel deep\nneural network DP-LSTM for stock price prediction, which incorporates the news\narticles as hidden information and integrates difference news sources through\nthe differential privacy mechanism. First, based on the autoregressive moving\naverage model (ARMA), a sentiment-ARMA is formulated by taking into\nconsideration the information of financial news articles in the model. Then, an\nLSTM-based deep neural network is designed, which consists of three components:\nLSTM, VADER model and differential privacy (DP) mechanism. The proposed DP-LSTM\nscheme can reduce prediction errors and increase the robustness. Extensive\nexperiments on S&P 500 stocks show that (i) the proposed DP-LSTM achieves 0.32%\nimprovement in mean MPA of prediction result, and (ii) for the prediction of\nthe market index S&P 500, we achieve up to 65.79% improvement in MSE.\n"
    },
    {
        "paper_id": 1912.10813,
        "authors": "Matthias J. Feiler and Thibaut Ajdler",
        "title": "Model uncertainty in financial forecasting",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Models necessarily capture only parts of a reality. Prediction models aim at\ncapturing a future reality. In this paper we address the question of how the\nfuture is constructed (or: imagined) in an investment context where market\nparticipants form expectations on the returns of a risky investment. We observe\nthat the participants' model choices are subject to unforeseeable change. The\nobjective of the paper is to demonstrate that the resulting uncertainty may be\nreduced by incorporating relations among competing models in the estimation\nprocess.\n"
    },
    {
        "paper_id": 1912.10858,
        "authors": "Xuan-Hong Dang, Syed Yousaf Shah, Petros Zerfos",
        "title": "\"The Squawk Bot\": Joint Learning of Time Series and Text Data Modalities\n  for Automated Financial Information Filtering",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Multimodal analysis that uses numerical time series and textual corpora as\ninput data sources is becoming a promising approach, especially in the\nfinancial industry. However, the main focus of such analysis has been on\nachieving high prediction accuracy while little effort has been spent on the\nimportant task of understanding the association between the two data\nmodalities. Performance on the time series hence receives little explanation\nthough human-understandable textual information is available. In this work, we\naddress the problem of given a numerical time series, and a general corpus of\ntextual stories collected in the same period of the time series, the task is to\ntimely discover a succinct set of textual stories associated with that time\nseries. Towards this goal, we propose a novel multi-modal neural model called\nMSIN that jointly learns both numerical time series and categorical text\narticles in order to unearth the association between them. Through multiple\nsteps of data interrelation between the two data modalities, MSIN learns to\nfocus on a small subset of text articles that best align with the performance\nin the time series. This succinct set is timely discovered and presented as\nrecommended documents, acting as automated information filtering, for the given\ntime series. We empirically evaluate the performance of our model on\ndiscovering relevant news articles for two stock time series from Apple and\nGoogle companies, along with the daily news articles collected from the Thomson\nReuters over a period of seven consecutive years. The experimental results\ndemonstrate that MSIN achieves up to 84.9% and 87.2% in recalling the ground\ntruth articles respectively to the two examined time series, far more superior\nto state-of-the-art algorithms that rely on conventional attention mechanism in\ndeep learning.\n"
    },
    {
        "paper_id": 1912.10866,
        "authors": "Holly Brannelly, Andrea Macrina, Gareth W. Peters",
        "title": "Quantile Diffusions for Risk Analysis",
        "comments": "34 pages, 6 figures, 1 table",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  We develop a novel approach for the construction of quantile processes\ngoverning the stochastic dynamics of quantiles in continuous time. Two classes\nof quantile diffusions are identified: the first, which we largely focus on,\nfeatures a dynamic random quantile level and allows for direct interpretation\nof the resulting quantile process characteristics such as location, scale,\nskewness and kurtosis, in terms of the model parameters. The second type are\nfunction-valued quantile diffusions and are driven by stochastic parameter\nprocesses, which determine the entire quantile function at each point in time.\nBy the proposed innovative and simple -- yet powerful -- construction method,\nquantile processes are obtained by transforming the marginals of a diffusion\nprocess under a composite map consisting of a distribution and a quantile\nfunction. Such maps, analogous to rank transmutation maps, produce the\nmarginals of the resulting quantile process. We discuss the relationship and\ndifferences between our approach and existing methods and characterisations of\nquantile processes in discrete and continuous time. As an example of an\napplication of quantile diffusions, we show how probability measure\ndistortions, a form of dynamic tilting, can be induced. Though particularly\nuseful in financial mathematics and actuarial science, examples of which are\ngiven in this work, measure distortions feature prominently across multiple\nresearch areas. For instance, dynamic distributional approximations\n(statistics), non-parametric and asymptotic analysis (mathematical statistics),\ndynamic risk measures (econometrics), behavioural economics, decision making\n(operations research), signal processing (information theory), and not least in\ngeneral risk theory including applications thereof, for example in the context\nof climate change.\n"
    },
    {
        "paper_id": 1912.10955,
        "authors": "Luciano Pietronero, Andrea Gabrielli, and Andrea Zaccaria",
        "title": "Economic Complexity: why we like \"Complexity weighted diversification\"",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  A recent paper by Hausmann and collaborators (1) reaches the important\nconclusion that Complexity-weighted diversification is the essential element to\npredict country growth. We like this result because Complexity-weighted\ndiversification is precisely the first equation of the Fitness algorithm that\nwe introduced in 2012 (2,3). However, contrary to what is claimed in (1), it is\nincorrect to say that diversification is contained also in the ECI algorithm\n(4). We discuss the origin of this misunderstanding and show that the ECI\nalgorithm contains exactly zero diversification. This is actually one of the\nreasons for the poor performances of ECI which leads to completely unrealistic\nresults, as for instance, the derivation that Qatar or Saudi Arabia are\nindustrially more competitive than China (5,6). Another important element of\nour new approach is the representation of the economic dynamics of countries as\ntrajectories in the GDPpc-Fitness space (7-10). In some way also this has been\nrediscovered by Hausmann and collaborators and renamed as \"Stream plots\", but,\ngiven their weaker metrics and methods, they propose it to use it only for a\nqualitative insight, while ours led to quantitative and successful forecasting.\nThe Fitness approach has paved the way to a robust and testable framework for\nEconomic Complexity resulting in a highly competitive scheme for growth\nforecasting (7-10). According to a recent report by Bloomberg (9): The new\nFitness method, \"systematically outperforms standard methods, despite requiring\nmuch less data\".\n"
    },
    {
        "paper_id": 1912.10958,
        "authors": "Andre Assumpcao",
        "title": "Electoral Crime Under Democracy: Information Effects from Judicial\n  Decisions in Brazil",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper examines voters' responses to the disclosure of electoral crime\ninformation in large democracies. I focus on Brazil, where the electoral court\nmakes candidates' criminal records public before every election. Using a sample\nof local candidates running for office between 2004 and 2016, I find that a\nconviction for an electoral crime reduces candidates' probability of election\nand vote share by 10.3 and 12.9 percentage points (p.p.), respectively. These\nresults are not explained by (potential) changes in judge, voter, or candidate\nbehavior over the electoral process. I additionally perform machine\nclassification of court documents to estimate heterogeneous punishment for\nsevere and trivial crimes. I document a larger electoral penalty (6.5 p.p.) if\ncandidates are convicted for severe crimes. These results supplement the\ninformation shortcut literature by examining how judicial information\ninfluences voters' decisions and showing that voters react more strongly to\nmore credible sources of information.\n"
    },
    {
        "paper_id": 1912.11059,
        "authors": "Shengli Chen, Zili Zhang",
        "title": "Forecasting Implied Volatility Smile Surface via Deep Learning and\n  Attention Mechanism",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The implied volatility smile surface is the basis of option pricing, and the\ndynamic evolution of the option volatility smile surface is difficult to\npredict. In this paper, attention mechanism is introduced into LSTM, and a\nvolatility surface prediction method combining deep learning and attention\nmechanism is pioneeringly established. LSTM's forgetting gate makes it have\nstrong generalization ability, and its feedback structure enables it to\ncharacterize the long memory of financial volatility. The application of\nattention mechanism in LSTM networks can significantly enhance the ability of\nLSTM networks to select input features. The experimental results show that the\ntwo strategies constructed using the predicted implied volatility surfaces have\nhigher returns and Sharpe ratios than that the volatility surfaces are not\npredicted. This paper confirms that the use of AI to predict the implied\nvolatility surface has theoretical and economic value. The research method\nprovides a new reference for option pricing and strategy.\n"
    },
    {
        "paper_id": 1912.1106,
        "authors": "Sebastian Becker, Patrick Cheridito and Arnulf Jentzen",
        "title": "Pricing and hedging American-style options with deep learning",
        "comments": null,
        "journal-ref": "Journal of Risk and Financial Management 13, 7 (2020)",
        "doi": "10.3390/jrfm13070158",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper we introduce a deep learning method for pricing and hedging\nAmerican-style options. It first computes a candidate optimal stopping policy.\nFrom there it derives a lower bound for the price. Then it calculates an upper\nbound, a point estimate and confidence intervals. Finally, it constructs an\napproximate dynamic hedging strategy. We test the approach on different\nspecifications of a Bermudan max-call option. In all cases it produces highly\naccurate prices and dynamic hedging strategies with small replication errors.\n"
    },
    {
        "paper_id": 1912.11166,
        "authors": "Aniruddha Dutta, Saket Kumar and Meheli Basu",
        "title": "A Gated Recurrent Unit Approach to Bitcoin Price Prediction",
        "comments": "8 figures, 16 pages",
        "journal-ref": null,
        "doi": "10.3390/jrfm13020023",
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  In today's era of big data, deep learning and artificial intelligence have\nformed the backbone for cryptocurrency portfolio optimization. Researchers have\ninvestigated various state of the art machine learning models to predict\nBitcoin price and volatility. Machine learning models like recurrent neural\nnetwork (RNN) and long short-term memory (LSTM) have been shown to perform\nbetter than traditional time series models in cryptocurrency price prediction.\nHowever, very few studies have applied sequence models with robust feature\nengineering to predict future pricing. in this study, we investigate a\nframework with a set of advanced machine learning methods with a fixed set of\nexogenous and endogenous factors to predict daily Bitcoin prices. We study and\ncompare different approaches using the root mean squared error (RMSE).\nExperimental results show that gated recurring unit (GRU) model with recurrent\ndropout performs better better than popular existing models. We also show that\nsimple trading strategies, when implemented with our proposed GRU model and\nwith proper learning, can lead to financial gain.\n"
    },
    {
        "paper_id": 1912.11172,
        "authors": "Tianyi Liu and Enlu Zhou",
        "title": "Online Quantification of Input Model Uncertainty by Two-Layer Importance\n  Sampling",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Stochastic simulation has been widely used to analyze the performance of\ncomplex stochastic systems and facilitate decision making in those systems.\nStochastic simulation is driven by the input model, which is a collection of\nprobability distributions that model the stochasticity in the system. The input\nmodel is usually estimated using a finite amount of data, which introduces the\nso-called input model uncertainty to the simulation output. How to quantify\ninput uncertainty has been studied extensively, and many methods have been\nproposed for the batch data setting, i.e., when all the data are available at\nonce. However, methods for \"streaming data\" arriving sequentially in time are\nstill in demand, despite that streaming data have become increasingly prevalent\nin modern applications. To fill this gap, we propose a two-layer importance\nsampling framework that incorporates streaming data for online input\nuncertainty quantification. Under this framework, we develop two algorithms\nthat suit different application scenarios: the first scenario is when data come\nat a fast speed and there is no time for any new simulation in between updates;\nthe second is when data come at a moderate speed and a few but limited\nsimulations are allowed at each time stage. We prove the consistency and\nasymptotic convergence rate results, which theoretically show the efficiency of\nour proposed approach. We further demonstrate the proposed algorithms on a\nnumerical example of the news vendor problem.\n"
    },
    {
        "paper_id": 1912.11216,
        "authors": "Inga Ivanova",
        "title": "Evolutionary Dynamics of Investors Expectations and Market Price\n  Movement",
        "comments": "57 pages, 13 figures, 6 tables",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The paper presents a step forward into the development of the theory of\nmeaning. Stock and financial markets are examined from\ncommunication-theoretical perspective on the dynamics of information and\nmeaning. This study focuses on the link between the dynamics of investors'\nexpectations and market price movement. The model for market asset price\ndynamiscs, based on non-linear evolutionary equation linking investors'\nexpectations and market asset price movement, is provided. Model predictions\nare tested on various FX, energy, food, and indices markets along different\ntime frames. The results suggest that model predicted time series is\nco-integrated with asset time series which implies that the prop[osed model can\nbe used to forecast future price movement.\n"
    },
    {
        "paper_id": 1912.1125,
        "authors": "G. Filatrella and N. De Liso",
        "title": "Predicting one type of technological motion? A nonlinear map to study\n  the 'sailing-ship' effect",
        "comments": null,
        "journal-ref": "Soft Comput (2019), published online December 21",
        "doi": "10.1007/s00500-019-04622-y",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this work we use a proven model to study a dynamic duopolistic competition\nbetween an old and a new technology which, through improved technical\nperformance - e.g. data transmission capacity - fight in order to conquer\nmarket share. The process whereby an old technology fights a new one off\nthrough own improvements has been named 'sailing-ship effect'. In the\nsimulations proposed, intentional improvements of both the old and the new\ntechnology are affected by the values of three key parameters: one\nscientific-technological, one purely technological and the third purely\neconomic. The interaction between these components gives rise to different\noutcomes in terms of prevalence of one technology over the other.\n"
    },
    {
        "paper_id": 1912.11341,
        "authors": "Arunav Gupta, Lucas Nguyen, Camille Dunning, Ka Ming Chan",
        "title": "Quantifying the Effects of the 2008 Recession using the Zillow Dataset",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  This report explores the use of Zillow's housing metrics dataset to\ninvestigate the effects of the 2008 US subprime mortgage crisis on various US\nlocales. We begin by exploring the causes of the recession and the metrics\navailable to us in the dataset. We settle on using the Zillow Home Value Index\n(ZHVI) because it is seasonally adjusted and able to account for a variety of\ninventory factors. Then, we explore three methodologies for quantifying\nrecession impact: (a) Principal Components Analysis, (b) Area Under Baseline,\nand (c) ARIMA modeling and Confidence Intervals. While PCA does not yield\nuseable results, we ended up with six cities from both AUB and ARIMA analysis,\nthe top 3 \"losers\" and \"gainers\" of the 2008 recession, as determined by each\nanalysis. This gave us 12 cities in total. Finally, we tested the robustness of\nour analysis against three \"common knowledge\" metrics for the recession:\ngeographic clustering, population trends, and unemployment rate. While we did\nfind some overlap between the results of our analysis and geographic\nclustering, there was no positive regression outcome from comparing our\nmethodologies to population trends and the unemployment rate.\n"
    },
    {
        "paper_id": 1912.11351,
        "authors": "Irena Gao, Marynia Kolak",
        "title": "Healthy Access for Healthy Places: A Multidimensional Food Access\n  Measure",
        "comments": "8 pages, 1 figure",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  When it comes to preventive healthcare, place matters. It is increasingly\nclear that social factors, particularly reliable access to healthy food, are as\ndeterminant to health and health equity as medical care. However, food access\nstudies often only present one-dimensional measurements of access. We\nhypothesize that food access is a multidimensional concept and evaluated\nPenchansky and Thomas's 1981 definition of access. In our approach, we identify\nten variables contributing to food access in the City of Chicago and use\nprincipal component analysis to identify vulnerable populations with low\naccess. Our results indicate that within the urban environment of the case\nstudy site, affordability is the most important factor in low food\naccessibility, followed by urban youth, reduced mobility, and higher immigrant\npopulation.\n"
    },
    {
        "paper_id": 1912.11665,
        "authors": "Hung T. Diep and Gabriel Desgranges",
        "title": "Dynamics of the Price Behavior in Stock Market: A Statistical Physics\n  Approach",
        "comments": "to appear in Physica A (2021)",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We study in this paper the time evolution of stock markets using a\nstatistical physics approach. Each agent is represented by a spin having a\nnumber of discrete states $q$ or continuous states, describing the tendency of\nthe agent for buying or selling. The market ambiance is represented by a\nparameter $T$ which plays the role of the temperature in physics. We show that\nthere is a critical value of $T$, say $T_c$, where strong fluctuations between\nindividual states lead to a disordered situation in which there is no majority:\nthe numbers of sellers and buyers are equal, namely the market clearing. We\nhave considered three models: $q=3$ ( sell, buy, wait), $q=5$ (5 states between\nabsolutely buy and absolutely sell), and $q=\\infty$. The specific measure, by\nthe government or by economic organisms, is parameterized by $H$ applied on the\nmarket at the time $t_1$ and removed at the time $t_2$. We have used Monte\nCarlo simulations to study the time evolution of the price as functions of\nthose parameters. Many striking results are obtained. In particular we show\nthat the price strongly fluctuates near $T_c$ and there exists a critical value\n$H_c$ above which the boosting effect remains after $H$ is removed. This\nhappens only if $H$ is applied in the critical region. Otherwise, the effect of\n$H$ lasts only during the time of the application of $H$. The second party of\nthe paper deals with the price variation using a time-dependent mean-field\ntheory. By supposing that the sellers and the buyers belong to two distinct\ncommunities with their characteristics different in both intra-group and\ninter-group interactions, we find the price oscillation with time.\n"
    },
    {
        "paper_id": 1912.11761,
        "authors": "Jie Fang, Shutao Xia, Jianwu Lin, Zhikang Xia, Xiang Liu, and Yong\n  Jiang",
        "title": "Alpha Discovery Neural Network based on Prior Knowledge",
        "comments": "Accepted by KDD-2020-ML in Finance, 8 pages. KDD 2020 Workshop on\n  Machine Learning in Finance. https://sites.google.com/view/kdd-mlf-2020/home",
        "journal-ref": "KDD 2020 Workshop on Machine Learning in Finance",
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Genetic programming (GP) is the state-of-the-art in financial automated\nfeature construction task. It employs reverse polish expression to represent\nfeatures and then conducts the evolution process. However, with the development\nof deep learning, more powerful feature extraction tools are available. This\npaper proposes Alpha Discovery Neural Network (ADNN), a tailored neural network\nstructure which can automatically construct diversified financial technical\nindicators based on prior knowledge. We mainly made three contributions. First,\nwe use domain knowledge in quantitative trading to design the sampling rules\nand object function. Second, pre-training and model pruning has been used to\nreplace genetic programming, because it can conduct more efficient evolution\nprocess. Third, the feature extractors in ADNN can be replaced by different\nfeature extractors and produce different functions. The experiment results show\nthat ADNN can construct more informative and diversified features than GP,\nwhich can effectively enriches the current factor pool. The fully-connected\nnetwork and recurrent network are better at extracting information from the\nfinancial time series than the convolution neural network. In real practice,\nfeatures constructed by ADNN can always improve multi-factor strategies'\nrevenue, sharpe ratio, and max draw-down, compared with the investment\nstrategies without these factors.\n"
    },
    {
        "paper_id": 1912.11858,
        "authors": "M. Carmen Boado-Penas, Julia Eisenberg and Paul Kr\\\"uhner",
        "title": "Maximising with-profit pensions without guarantees",
        "comments": null,
        "journal-ref": null,
        "doi": "10.1007/s13385-019-00220-2",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Currently, pension providers are running into trouble mainly due to the\nultra-low interest rates and the guarantees associated to some pension\nbenefits. With the aim of reducing the pension volatility and providing\nadequate pension levels with no guarantees, we carry out mathematical analysis\nof a new pension design in the accumulation phase. The individual's premium is\nsplit into the individual and collective part and invested in funds. In times\nwhen the return from the individual fund exits a predefined corridor, a certain\nnumber of units is transferred to or from the collective account smoothing in\nthis way the volatility of the individual fund. The target is to maximise the\ntotal accumulated capital, consisting of the individual account and a portion\nof the collective account due to a so-called redistribution index, at\nretirement by controlling the corridor width. We also discuss the necessary and\nsufficient conditions that have to be put on the redistribution index in order\nto avoid arbitrage opportunities for contributors.\n"
    },
    {
        "paper_id": 1912.12113,
        "authors": "\\c{S}ule \\c{S}ahin and Shaun Levitan",
        "title": "A Stochastic Investment Model for Actuarial Use in South Africa",
        "comments": "34 pages, 15 figures, 6 tables",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper, we propose a stochastic investment model for actuarial use in\nSouth Africa by modelling price inflation rates, share dividends, long term and\nshort-term interest rates for the period 1960-2018 and inflation-linked bonds\nfor the period 2000-2018. Possible bi-directional relations between the\neconomic series have been considered, the parameters and their confidence\nintervals have been estimated recursively to examine their stability and the\nmodel validation has been tested. The model is designed to provide long-term\nforecasts that should find application in long-term modelling for institutions\nsuch as pension funds and life insurance companies in South Africa\n"
    },
    {
        "paper_id": 1912.12226,
        "authors": "Alessandro Doldi and Marco Frittelli",
        "title": "Multivariate Systemic Optimal Risk Transfer Equilibrium",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  A Systemic Optimal Risk Transfer Equilibrium (SORTE) was introduced in:\n\"Systemic optimal risk transfer equilibrium\", Mathematics and Financial\nEconomics (2021), for the analysis of the equilibrium among financial\ninstitutions or in insurance-reinsurance markets. A SORTE conjugates the\nclassical B\\\"{u}hlmann's notion of a risk exchange equilibrium with a capital\nallocation principle based on systemic expected utility optimization. In this\npaper we extend such a notion to the case when the value function to be\noptimized is multivariate in a general sense, and it is not simply given by the\nsum of univariate utility functions. This takes into account the fact that\npreferences of single agents might depend on the actions of other participants\nin the game. Technically, the extension of SORTE to the new setup requires\ndeveloping a theory for multivariate utility functions and selecting at the\nsame time a suitable framework for the duality theory. Conceptually, this more\ngeneral framework allows us to introduce and study a Nash Equilibrium property\nof the optimizer. We prove existence, uniqueness, and the Nash Equilibrium\nproperty of the newly defined Multivariate Systemic Optimal Risk Transfer\nEquilibrium.\n"
    },
    {
        "paper_id": 1912.12329,
        "authors": "M. Carmen Boado-Penas, Julia Eisenberg, Ralf Korn",
        "title": "Transforming public pensions: A mixed scheme with a credit granted by\n  the state",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Birth rates have dramatically decreased and, with continuous improvements in\nlife expectancy, pension expenditure is on an irreversibly increasing path.\nThis will raise serious concerns for the sustainability of the public pension\nsystems usually financed on a pay-as-you-go (PAYG) basis where current\ncontributions cover current pension expenditure. With this in mind, the aim of\nthis paper is to propose a mixed pension system that consists of a combination\nof a classical PAYG scheme and an increase of the contribution rate invested in\na funding scheme. The investment of the funding part is designed so that the\nPAYG pension system is financially sustainable at a particular level of\nprobability and at the same time provide some gains to individuals. In this\nsense, we make the individuals be an active part to face the demographic risks\ninherent in the PAYG and re-establish its financial sustainability.\n"
    },
    {
        "paper_id": 1912.12351,
        "authors": "Ipek Turker and Bayram Cakir",
        "title": "Reading Macroeconomics From the Yield Curve: The Turkish Case",
        "comments": "22 pages, 14 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper aims to analyze the relationship between yield curve -being a line\nof the interests in various maturities at a given time- and GDP growth in\nTurkey. The paper focuses on analyzing the yield curve in relation to its\npredictive power on Turkish macroeconomic dynamics using the linear regression\nmodel. To do so, the interest rate spreads of different maturities are used as\na proxy of the yield curve. Findings of the OLS regression are similar to that\nfound in the literature and supports the positive relation between slope of\nyield curve and GDP growth in Turkey. Moreover, the predicted values of the GDP\ngrowth from interest rate spread closely follow the actual GDP growth in\nTurkey, indicating its predictive power on the economic activity.\n"
    },
    {
        "paper_id": 1912.12354,
        "authors": "Armine Karami, Raphael Benichou, Michael Benzaquen, Jean-Philippe\n  Bouchaud",
        "title": "Conditional Correlations and Principal Regression Analysis for Futures",
        "comments": "12 pages, 14 figures, 1 table",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We explore the effect of past market movements on the instantaneous\ncorrelations between assets within the futures market. Quantifying this effect\nis of interest to estimate and manage the risk associated to portfolios of\nfutures in a non-stationary context. We apply and extend a previously reported\nmethod called the Principal Regression Analysis (PRA) to a universe of $84$\nfutures contracts between $2009$ and $2019$. We show that the past up (resp.\ndown) 10 day trends of a novel predictor -- the eigen-factor -- tend to reduce\n(resp. increase) instantaneous correlations. We then carry out a multifactor\nPRA on sectorial predictors corresponding to the four futures sectors (indexes,\ncommodities, bonds and currencies), and show that the effect of past market\nmovements on the future variations of the instantaneous correlations can be\ndecomposed into two significant components. The first component is due to the\nmarket movements within the index sector, while the second component is due to\nthe market movements within the bonds sector.\n"
    },
    {
        "paper_id": 1912.12472,
        "authors": "Carlo Marinelli",
        "title": "Positivity of mild solution to stochastic evolution equations with an\n  application to forward rates",
        "comments": "31 pages, no figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We prove a maximum principle for mild solutions to stochastic evolution\nequations with (locally) Lipschitz coefficients and Wiener noise on weighted\n$L^2$ spaces. As an application, we provide sufficient conditions for the\npositivity of forward rates in the Heath-Jarrow-Morton model, considering the\nassociated Musiela SPDE on a homogeneous weighted Sobolev space.\n"
    },
    {
        "paper_id": 1912.12521,
        "authors": "Aditya Maheshwari and Traian Pirvu",
        "title": "Portfolio Optimization under Correlation Constraint",
        "comments": "19 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We consider the problem of portfolio optimization with a correlation\nconstraint. The framework is the multiperiod stochastic financial market\nsetting with one tradable stock, stochastic income and a non-tradable index.\nThe correlation constraint is imposed on the portfolio and the non-tradable\nindex at some benchmark time horizon. The goal is to maximize portofolio's\nexpected exponential utility subject to the correlation constraint. Two types\nof optimal portfolio strategies are considered: the subgame perfect and the\nprecommitment ones. We find analytical expressions for the constrained subgame\nperfect (CSGP) and the constrained precommitment (CPC) portfolio strategies.\nBoth these portfolio strategies yield significantly lower risk when compared to\nthe unconstrained setting, at the cost of a small utility loss. The performance\nof the CSGP and CPC portfolio strategies is similar.\n"
    },
    {
        "paper_id": 1912.12571,
        "authors": "Ruben Loaiza-Maya, Gael M. Martin, and David T. Frazier",
        "title": "Focused Bayesian Prediction",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We propose a new method for conducting Bayesian prediction that delivers\naccurate predictions without correctly specifying the unknown true data\ngenerating process. A prior is defined over a class of plausible predictive\nmodels. After observing data, we update the prior to a posterior over these\nmodels, via a criterion that captures a user-specified measure of predictive\naccuracy. Under regularity, this update yields posterior concentration onto the\nelement of the predictive class that maximizes the expectation of the accuracy\nmeasure. In a series of simulation experiments and empirical examples we find\nnotable gains in predictive accuracy relative to conventional likelihood-based\nprediction.\n"
    },
    {
        "paper_id": 1912.1259,
        "authors": "Mohamed Arbi Madani and Zied Ftiti",
        "title": "The Generalisation of the DMCA Coefficient to Serve Distinguishing\n  Between Hedge and Safe Haven Capabilities of the Gold",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  This paper aims to investigate the role of gold as a hedge and/or safe haven\nagainst oil price and currency market movements for medium (calm period) and\nlarge (extreme movement) fluctuations. In revisiting the role of gold, our\nstudy proposes new insights into the literature. First, our empirical design\nrelaxes the assumption of homogeneous investors in favour of agents with\ndifferent horizons. Second, we develop a new measure of correlation based on\nthe fractal approach, called the q-detrending moving average cross-correlation\ncoefficient. This allows us to measure the dependence for calm and extreme\nmovements. The proposed measure is both time-varying and time-scale varying,\ntaking into account the complex pattern of commodities and financial time\nseries (chaotic, non-stationary, etc.). Using intraday data from May 2017 to\nMarch 2019, including 35608 observations for each variable, our results are as\nfollows. First, we show a negative and significant average and tail dependence\nfor all time scales between gold and USD exchange rates that is consistent with\nthe gold's role as an effective hedge and safe-haven asset. Second, this study\nputs out average independence and positive and significant tail independence\nbetween gold and oil indicating that gold can be used by investors as a weak\nhedge but cannot be used as an effective safe-haven asset under exceptional\nmarket circumstances for all time scales. Third, we examine the hedging and\nstabilising benefits of gold over calm and turmoil periods for gold-oil futures\nand gold-currency portfolios by estimation of the optimal portfolio weights and\nthe optimal hedge ratio. We confirm the usefulness of gold for hedging and safe\nhavens at different investment horizons, which favors the inclusion of gold\nfutures in oil futures and currency portfolios for risk management purposes.\n"
    },
    {
        "paper_id": 1912.12611,
        "authors": "Anand Deo and Sandeep Juneja",
        "title": "Credit Risk: Simple Closed Form Approximate Maximum Likelihood Estimator",
        "comments": "79 pages, 3 figures, 14 tables",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We consider discrete default intensity based and logit type reduced form\nmodels for conditional default probabilities for corporate loans where we\ndevelop simple closed form approximations to the maximum likelihood estimator\n(MLE) when the underlying covariates follow a stationary Gaussian process. In a\npractically reasonable asymptotic regime where the default probabilities are\nsmall, say 1-3% annually, the number of firms and the time period of data\navailable is reasonably large, we rigorously show that the proposed estimator\nbehaves similarly or slightly worse than the MLE when the underlying model is\ncorrectly specified. For more realistic case of model misspecification, both\nestimators are seen to be equally good, or equally bad. Further, beyond a\npoint, both are more-or-less insensitive to increase in data. These conclusions\nare validated on empirical and simulated data. The proposed approximations\nshould also have applications outside finance, where logit-type models are used\nand probabilities of interest are small.\n"
    },
    {
        "paper_id": 1912.12615,
        "authors": "Anna Knezevic, Nikolai Dokuchaev",
        "title": "Approximating intractable short ratemodel distribution with neural\n  network",
        "comments": "Incorrect methodology for generation of stochastic scenarios",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We propose an algorithm which predicts each subsequent time step relative to\nthe previous timestep of intractable short rate model (when adjusted for drift\nand overall distribution of previous percentile result) and show that the\nmethod achieves superior outcomes to the unbiased estimate both on the trained\ndataset and different validation data.\n"
    },
    {
        "paper_id": 1912.12864,
        "authors": "Bart Cockx, Michael Lechner, Joost Bollens",
        "title": "Priority to unemployed immigrants? A causal machine learning evaluation\n  of training in Belgium",
        "comments": "78 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Based on administrative data of unemployed in Belgium, we estimate the labour\nmarket effects of three training programmes at various aggregation levels using\nModified Causal Forests, a causal machine learning estimator. While all\nprogrammes have positive effects after the lock-in period, we find substantial\nheterogeneity across programmes and unemployed. Simulations show that\n'black-box' rules that reassign unemployed to programmes that maximise\nestimated individual gains can considerably improve effectiveness: up to 20\npercent more (less) time spent in (un)employment within a 30 months window. A\nshallow policy tree delivers a simple rule that realizes about 70 percent of\nthis gain.\n"
    },
    {
        "paper_id": 1912.1294,
        "authors": "Sandip Dutta and Vignesh Prabhu",
        "title": "Effect of Franchised Business models on Fast Food Company Stock Prices\n  in Recession and Recovery with Weibull Analysis",
        "comments": "There are 19 pages and 27 figures with 9 different stocks. We decided\n  to publish this work in progress paper as it does contain important\n  thought-provoking information. Please send us any comments and questions you\n  may have to the primary contact: Dr. Sandip Dutta, sdutta@clemson.edu",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  At the initial stages of this research, the assumption was that the\nfranchised businesses perhaps should not be affected much by recession as there\nare multiple cash pools available inherent to the franchised business model.\nHowever, after analyzing the available data, it indicated otherwise, the stock\nprice performance as discussed indicates a different pattern. The stock price\ndata is analyzed with an unconventional tool, Weibull distribution and\nobservations confirmed the presence of either a reverse trend in franchised\nbusiness than what is observed for non-franchised or the franchised stock\nfollowed large food suppliers. There is a layered ownership and cash flow in a\nfranchised business model. The parent company run by franchiser depends on the\nperformance of child companies run by franchisees. Both parent and child\ncompanies are run as independent businesses but only the parent company is\nlisted as a stock ticker in stock exchange. Does this double layer of vertical\noperation, cash reserve, and cash flow protect them better in recession? The\ndata analyzed in this paper indicates that the recession effect can be more\nsevere; and if it dives with the average market, expect a slower recovery of\nstock prices in a franchised business model. This paper characterizes the\ndifferences and explains the natural experiment with available financial data.\n"
    },
    {
        "paper_id": 1912.12983,
        "authors": "Jay Damask",
        "title": "A Consistently Oriented Basis for Eigenanalysis",
        "comments": null,
        "journal-ref": "Int J Data Sci Anal 10, 301-319 (2020)",
        "doi": "10.1007/s41060-020-00227-z",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Repeated application of machine-learning, eigen-centric methods to an\nevolving dataset reveals that eigenvectors calculated by well-established\ncomputer implementations are not stable along an evolving sequence. This is\nbecause the sign of any one eigenvector may point along either the positive or\nnegative direction of its associated eigenaxis, and for any one eigen call the\nsign does not matter when calculating a solution. This work reports an\nalgorithm that creates a consistently oriented basis of eigenvectors. The\nalgorithm postprocesses any well-established eigen call and is therefore\nagnostic to the particular implementation of the latter. Once consistently\noriented, directional statistics can be applied to the eigenvectors in order to\ntrack their motion and summarize their dispersion. When a consistently oriented\neigensystem is applied to methods of machine-learning, the time series of\ntraining weights becomes interpretable in the context of the machine-learning\nmodel. Ordinary linear regression is used to demonstrate such interpretability.\nA reference implementation of the algorithm reported herein has been written in\nPython and is freely available, both as source code and through the thucyd\nPython package.\n"
    },
    {
        "paper_id": 1912.1311,
        "authors": "Donghan Kim",
        "title": "Open Markets",
        "comments": "47 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  An open market is a subset of an entire equity market composed of a certain\nfixed number of top capitalization stocks. Though the number of stocks in the\nopen market is fixed, the constituents of the market change over time as each\ncompany's rank by its market capitalization fluctuates. When one is allowed to\ninvest also in the money market, the open market resembles the entire 'closed'\nequity market in the sense that the equivalence of market viability (lack of\narbitrage) and the existence of numeraire portfolio (portfolio which cannot be\noutperformed) holds. When access to the money market is prohibited, some topics\nsuch as Capital Asset Pricing Model (CAPM), construction of functionally\ngenerated portfolios, and the concept of the universal portfolio are presented\nin the open market setting.\n"
    },
    {
        "paper_id": 1912.13259,
        "authors": "Carlo Marinelli, Luca Scarpa",
        "title": "On the positivity of local mild solutions to stochastic evolution\n  equations",
        "comments": "10 pages, no figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We provide sufficient conditions on the coefficients of a stochastic\nevolution equation on a Hilbert space of functions driven by a cylindrical\nWiener process ensuring that its mild solution is positive if the initial datum\nis positive. As an application, we discuss the positivity of forward rates in\nthe Heath-Jarrow-Morton model via Musiela's stochastic PDE.\n"
    },
    {
        "paper_id": 1912.13275,
        "authors": "V. Macchiati, G. Brandi, G. Cimini, G. Caldarelli, D. Paolotti, T. Di\n  Matteo",
        "title": "Systemic liquidity contagion in the European interbank market",
        "comments": "32 pages, 24 figures Submitted to Journal of Economic Interaction and\n  Coordination",
        "journal-ref": "Journal of Economic Interaction and Coordination (2021)",
        "doi": "10.1007/s11403-021-00338-1",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Systemic liquidity risk, defined by the IMF as \"the risk of simultaneous\nliquidity difficulties at multiple financial institutions\", is a key topic in\nmacroprudential policy and financial stress analysis. Specialized models to\nsimulate funding liquidity risk and contagion are available but they require\nnot only banks' bilateral exposures data but also balance sheet data with\nsufficient granularity, which are hardly available. Alternatively, risk\nanalyses on interbank networks have been done via centrality measures of the\nunderlying graph capturing the most interconnected and hence more prone to risk\nspreading banks. In this paper, we propose a model which relies on an epidemic\nmodel which simulate a contagion on the interbank market using the funding\nliquidity shortage mechanism as contagion process. The model is enriched with\ncountry and bank risk features which take into account the heterogeneity of the\ninterbank market. The proposed model is particularly useful when full set of\ndata necessary to run specialized models is not available. Since the interbank\nnetwork is not fully available, an economic driven reconstruction method is\nalso proposed to retrieve the interbank network by constraining the standard\nreconstruction methodology to real financial indicators. We show that the\ncontagion model is able to reproduce systemic liquidity risk across different\nyears and countries. This result suggests that the proposed model can be\nsuccessfully used as a valid alternative to more complex ones.\n"
    },
    {
        "paper_id": 2001.00078,
        "authors": "Jack Clark and Gillian K. Hadfield",
        "title": "Regulatory Markets for AI Safety",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We propose a new model for regulation to achieve AI safety: global regulatory\nmarkets. We first sketch the model in general terms and provide an overview of\nthe costs and benefits of this approach. We then demonstrate how the model\nmight work in practice: responding to the risk of adversarial attacks on AI\nmodels employed in commercial drones.\n"
    },
    {
        "paper_id": 2001.00122,
        "authors": "Adnan Rebei",
        "title": "Entropic Decision Making",
        "comments": "84 pages, 22 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Using results from neurobiology on perceptual decision making and value-based\ndecision making, the problem of decision making between lotteries is\nreformulated in an abstract space where uncertain prospects are mapped to\ncorresponding active neuronal representations. This mapping allows us to\nmaximize non-extensive entropy in the new space with some constraints instead\nof a utility function. To achieve good agreements with behavioral data, the\nconstraints must include at least constraints on the weighted average of the\nstimulus and on its variance. Both constraints are supported by the\nadaptability of neuronal responses to an external stimulus. By analogy with\nthermodynamic and information engines, we discuss the dynamics of choice\nbetween two lotteries as they are being processed simultaneously in the brain\nby rate equations that describe the transfer of attention between lotteries and\nwithin the various prospects of each lottery. This model is able to give new\ninsights on risk aversion and on behavioral anomalies not accounted for by\nProspect Theory.\n"
    },
    {
        "paper_id": 2001.00465,
        "authors": "Guglielmo D'Amico and Riccardo De Blasis",
        "title": "A review of the Dividend Discount Model: from deterministic to\n  stochastic models",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This chapter presents a review of the dividend discount models starting from\nthe basic models (Williams 1938, Gordon and Shapiro 1956) to more recent and\ncomplex models (Ghezzi and Piccardi 2003, Barbu et al. 2017, D'Amico and De\nBlasis 2018) with a focus on the modelling of the dividend process rather than\nthe discounting factor, that is assumed constant in most of the models. The\nChapter starts with an introduction of the basic valuation model with some\ngeneral aspects to consider when performing the computation. Then, Section 1.3\npresents the Gordon growth model (Gordon 1962) with some of its extensions\n(Malkiel 1963, Fuller and Hsia 1984, Molodovsky et al. 1965, Brooks and Helms\n1990, Barsky and De Long 1993), and reports some empirical evidence. Extended\nreviews of the Gordon stock valuation model and its extensions can be found in\nKamstra (2003) and Damodaran (2012). In Section 1.4, the focus is directed to\nmore recent advancements which make us of the Markov chain to model the\ndividend process (Hurley and Johnson 1994, Yao 1997, Hurley and Johnson 1998,\nGhezzi and Piccardi 2003, Barbu et al. 2017, D'Amico and De Blasis 2018). The\nadvantage of these models is the possibility to obtain a different valuation\nthat depends on the state of the dividend series, allowing the model to be\ncloser to reality. In addition, these models permit to obtain a measure of the\nrisk of the single stock or a portfolio of stocks.\n"
    },
    {
        "paper_id": 2001.00478,
        "authors": "Angelo Tartaglia (INAF and Department of Applied Science and\n  Technology, Politecnico di Torino, Italy)",
        "title": "Growth and inequalities in a physicist's view",
        "comments": "16 pages, 5 figures",
        "journal-ref": null,
        "doi": "10.1007/s41247-020-00071-6",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  It is still common wisdom amongst economists, politicians and lay people that\neconomic growth is a necessity of our social systems, at least to avoid\ndistributional conflicts. This paper challenges such belief moving from a\npurely physical theoretical perspective. It formally considers the constraints\nimposed by a finite environment on the prospect of continuous growth, including\nthe dynamics of costs. As costs grow faster than production it is easy to\ndeduce a final unavoidable global collapse. Then, analyzing and discussing the\nevolution of the unequal share of wealth under the premises of growth and\ncompetition, it is shown that the increase of inequalities is a necessary\nconsequence of the premises.\n"
    },
    {
        "paper_id": 2001.00516,
        "authors": "Ivan Arraut, Alan Au, Alan Ching-biu Tse and Joao Alexandre Lobo\n  Marques",
        "title": "On the probability flow in the Stock market I: The Black-Scholes case",
        "comments": "10 pages including references",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  It is known that the probability is not a conserved quantity in the stock\nmarket, given the fact that it corresponds to an open system. In this paper we\nanalyze the flow of probability in this system by expressing the ideal\nBlack-Scholes equation in the Hamiltonian form. We then analyze how the\nnon-conservation of probability affects the stability of the prices of the\nStocks. Finally, we find the conditions under which the probability might be\nconserved in the market, challenging in this way the non-Hermitian nature of\nthe Black-Scholes Hamiltonian.\n"
    },
    {
        "paper_id": 2001.00529,
        "authors": "Marcel Br\\\"autigam and Marie Kratz",
        "title": "The Impact of the Choice of Risk and Dispersion Measure on\n  Procyclicality",
        "comments": "41 pages, 4 Figures, 2 Tables",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Procyclicality of historical risk measure estimation means that one tends to\nover-estimate future risk when present realized volatility is high and vice\nversa under-estimate future risk when the realized volatility is low. Out of it\ndifferent questions arise, relevant for applications and theory: What are the\nfactors which affect the degree of procyclicality? More specifically, how does\nthe choice of risk measure affect this? How does this behaviour vary with the\nchoice of realized volatility estimator? How do different underlying model\nassumptions influence the pro-cyclical effect? In this paper we consider three\ndifferent well-known risk measures (Value-at-Risk, Expected Shortfall,\nExpectile), the r-th absolute centred sample moment, for any integer $r>0$, as\nrealized volatility estimator (this includes the sample variance and the sample\nmean absolute deviation around the sample mean) and two models (either an iid\nmodel or an augmented GARCH($p$,$q$) model). We show that the strength of\nprocyclicality depends on these three factors, the choice of risk measure, the\nrealized volatility estimator and the model considered. But, no matter the\nchoices, the procyclicality will always be present.\n"
    },
    {
        "paper_id": 2001.00622,
        "authors": "Samuel Drapeau and Peng Luo and Alexander Schied and Dewen Xiong",
        "title": "An FBSDE approach to market impact games with stochastic parameters",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We analyze a market impact game between $n$ risk averse agents who compete\nfor liquidity in a market impact model with permanent price impact and\nadditional slippage. Most market parameters, including volatility and drift,\nare allowed to vary stochastically. Our first main result characterizes the\nNash equilibrium in terms of a fully coupled system of forward-backward\nstochastic differential equations (FBSDEs). Our second main result provides\nconditions under which this system of FBSDEs has indeed a unique solution,\nwhich in turn yields the unique Nash equilibrium. We furthermore obtain\nclosed-form solutions in special situations and analyze them numerically\n"
    },
    {
        "paper_id": 2001.00737,
        "authors": "Abootaleb Shirvani, Frank J. Fabozzi, and Stoyan V. Stoyanov",
        "title": "Option Pricing in an Investment Risk-Return Setting",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper, we combine modern portfolio theory and option pricing theory\nso that a trader who takes a position in a European option contract and the\nunderlying assets can construct an optimal portfolio such that at the moment of\nthe contract's maturity the contract is perfectly hedged. We derive both the\noptimal holdings in the underlying assets for the trader's optimal\nmean-variance portfolio and the amount of unhedged risk prior to maturity.\nSolutions assuming the cases where the price dynamics in the underlying assets\nfollow discrete binomial price dynamics, continuous diffusions, stochastic\nvolatility, volatility-of-volatility, and Merton-jump diffusion are derived.\n"
    },
    {
        "paper_id": 2001.00889,
        "authors": "Andre Assumpcao and Julio Trecenti",
        "title": "Judicial Favoritism of Politicians: Evidence from Small Claims Court",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Multiple studies have documented racial, gender, political ideology, or\nethnical biases in comparative judicial systems. Supplementing this literature,\nwe investigate whether judges rule cases differently when one of the litigants\nis a politician. We suggest a theory of power collusion, according to which\njudges might use rulings to buy cooperation or threaten members of the other\nbranches of government. We test this theory using a sample of small claims\ncases in the state of S\\~ao Paulo, Brazil, where no collusion should exist. The\nresults show a negative bias of 3.7 percentage points against litigant\npoliticians, indicating that judges punish, rather than favor, politicians in\ncourt. This punishment in low-salience cases serves as a warning sign for\npoliticians not to cross the judiciary when exercising checks and balances,\nsuggesting yet another barrier to judicial independence in development\nsettings.\n"
    },
    {
        "paper_id": 2001.00918,
        "authors": "Wenhang Bao",
        "title": "Fairness in Multi-agent Reinforcement Learning for Stock Trading",
        "comments": "arXiv admin note: substantial text overlap with arXiv:1906.11046;\n  text overlap with arXiv:1907.10323 by other authors",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Unfair stock trading strategies have been shown to be one of the most\nnegative perceptions that customers can have concerning trading and may result\nin long-term losses for a company. Investment banks usually place trading\norders for multiple clients with the same target assets but different order\nsizes and diverse requirements such as time frame and risk aversion level,\nthereby total earning and individual earning cannot be optimized at the same\ntime. Orders executed earlier would affect the market price level, so late\nexecution usually means additional implementation cost. In this paper, we\npropose a novel scheme that utilizes multi-agent reinforcement learning systems\nto derive stock trading strategies for all clients which keep a balance between\nrevenue and fairness. First, we demonstrate that Reinforcement learning (RL) is\nable to learn from experience and adapt the trading strategies to the complex\nmarket environment. Secondly, we show that the Multi-agent RL system allows\ndeveloping trading strategies for all clients individually, thus optimizing\nindividual revenue. Thirdly, we use the Generalized Gini Index (GGI)\naggregation function to control the fairness level of the revenue across all\nclients. Lastly, we empirically demonstrate the superiority of the novel scheme\nin improving fairness meanwhile maintaining optimization of revenue.\n"
    },
    {
        "paper_id": 2001.00919,
        "authors": "Tarun Chitra",
        "title": "Competitive equilibria between staking and on-chain lending",
        "comments": "25 pages, Accepted to Stanford Blockchain Conference and MIT\n  Cryptoeconomic Systems '20",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
        "abstract": "  Proof of Stake (PoS) is a burgeoning Sybil resistance mechanism that aims to\nhave a digital asset (\"token\") serve as security collateral in crypto networks.\nHowever, PoS has so far eluded a comprehensive threat model that encompasses\nboth Byzantine attacks from distributed systems and financial attacks that\narise from the dual usage of the token as a means of payment and a Sybil\nresistance mechanism. In particular, the existence of derivatives markets makes\nmalicious coordination among validators easier to execute than in Proof of Work\nsystems. We demonstrate that it is also possible for on-chain lending smart\ncontracts to cannibalize network security in PoS systems. When the yield\nprovided by these contracts is more attractive than the inflation rate provided\nfrom staking, stakers will tend to remove their staked tokens and lend them\nout, thus reducing network security. In this paper, we provide a simple\nstochastic model that describes how rational validators with varying risk\npreferences react to changes in staking and lending returns. For a particular\nconfiguration of this model, we provide a formal proof of a phase transition\nbetween equilibria in which tokens are predominantly staked and those in which\nthey are predominantly lent. We further validate this emergent adversarial\nbehavior (e.g. reduced staked token supply) with agent-based simulations that\nsample transitions under more realistic conditions. Our results illustrate that\nrational, non-adversarial actors can dramatically reduce PoS network security\nif block rewards are not calibrated appropriately above the expected yields of\non-chain lending.\n"
    },
    {
        "paper_id": 2001.0092,
        "authors": "Andres Quiros-Granados, JAvier Trejos-Zelaya",
        "title": "Estimation of the yield curve for Costa Rica using combinatorial\n  optimization metaheuristics applied to nonlinear regression",
        "comments": "13 pages, 2 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
        "abstract": "  The term structure of interest rates or yield curve is a function relating\nthe interest rate with its own term. Nonlinear regression models of\nNelson-Siegel and Svensson were used to estimate the yield curve using a sample\nof historical data supplied by the National Stock Exchange of Costa Rica. The\noptimization problem involved in the estimation process of model parameters is\naddressed by the use of four well known combinatorial optimization\nmetaheuristics: Ant colony optimization, Genetic algorithm, Particle swarm\noptimization and Simulated annealing. The aim of the study is to improve the\nlocal minima obtained by a classical quasi-Newton optimization method using a\ndescent direction. Good results with at least two metaheuristics are achieved,\nParticle swarm optimization and Simulated annealing. Keywords: Yield curve,\nnonlinear regression, Nelson-\n"
    },
    {
        "paper_id": 2001.01036,
        "authors": "A. Alexandre Trindade, Abootaleb Shirvani, and Xiaohan Ma",
        "title": "A Socioeconomic Well-Being Index",
        "comments": null,
        "journal-ref": "Applied Economics and Finance, Vol. 7, No. 4; July 2020",
        "doi": "10.11114/aef.v7i4.4855",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  An annual well-being index constructed from thirteen socioeconomic factors is\nproposed in order to dynamically measure the mood of the US citizenry.\nEconometric models are fitted to the log-returns of the index in order to\nquantify its tail risk and perform option pricing and risk budgeting. By\nproviding a statistically sound assessment of socioeconomic content, the index\nis consistent with rational finance theory, enabling the construction and\nvaluation of insurance-type financial instruments to serve as contracts written\nagainst it. Endogenously, the VXO volatility measure of the stock market\nappears to be the greatest contributor to tail risk. Exogenously,\n\"stress-testing\" the index against the politically important factors of trade\nimbalance and legal immigration, quantify the systemic risk. For probability\nlevels in the range of 5% to 10%, values of trade below these thresholds are\nassociated with larger downward movements of the index than for immigration at\nthe same level. The main intent of the index is to provide early-warning for\nnegative changes in the mood of citizens, thus alerting policy makers and\nprivate agents to potential future market downturns.\n"
    },
    {
        "paper_id": 2001.01127,
        "authors": "Nicola Uras and Lodovica Marchesi and Michele Marchesi and Roberto\n  Tonelli",
        "title": "Forecasting Bitcoin closing price series using linear regression and\n  neural networks models",
        "comments": "25 pages, 4 figures, Journal",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper studies how to forecast daily closing price series of Bitcoin,\nusing data on prices and volumes of prior days. Bitcoin price behaviour is\nstill largely unexplored, presenting new opportunities. We compared our results\nwith two modern works on Bitcoin prices forecasting and with a well-known\nrecent paper that uses Intel, National Bank shares and Microsoft daily NASDAQ\nclosing prices spanning a 3-year interval. We followed different approaches in\nparallel, implementing both statistical techniques and machine learning\nalgorithms. The SLR model for univariate series forecast uses only closing\nprices, whereas the MLR model for multivariate series uses both price and\nvolume data. We applied the ADF -Test to these series, which resulted to be\nindistinguishable from a random walk. We also used two artificial neural\nnetworks: MLP and LSTM. We then partitioned the dataset into shorter sequences,\nrepresenting different price regimes, obtaining best result using more than one\nprevious price, thus confirming our regime hypothesis. All the models were\nevaluated in terms of MAPE and relativeRMSE. They performed well, and were\noverall better than those obtained in the benchmarks. Based on the results, it\nwas possible to demonstrate the efficacy of the proposed methodology and its\ncontribution to the state-of-the-art.\n"
    },
    {
        "paper_id": 2001.01443,
        "authors": "Serguei Pergamenchtchikov and Alena Shishkova",
        "title": "Hedging problems for Asian options with transactions costs",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper, we consider the problem of hedging Asian options in financial\nmarkets with transaction costs. For this, we use the asymptotic hedging\napproach. The main task of asymptotic hedging in financial markets with\ntransaction costs is to prove the probability convergence of the terminal value\nof the investment portfolio to the payment function when the number of\nportfolio revisions tends to be $n$ to infinity. In practice, this means that\nthe investor, using such a strategy, is able to compensation payments for all\nfinancial transactions, even if their number increases unlimitedly.\n"
    },
    {
        "paper_id": 2001.01518,
        "authors": "Sudarshan Kumar, Tiziana Di Matteo, Anindya S. Chakrabarti",
        "title": "Disentangling shock diffusion on complex networks: Identification\n  through graph planarity",
        "comments": "21 pages; 5 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Large scale networks delineating collective dynamics often exhibit cascading\nfailures across nodes leading to a system-wide collapse. Prominent examples of\nsuch phenomena would include collapse on financial and economic networks.\nIntertwined nature of the dynamics of nodes in such network makes it difficult\nto disentangle the source and destination of a shock that percolates through\nthe network, a property known as reflexivity. In this article, a novel\nmethodology is proposed which combines vector autoregression model with an\nunique identification restrictions obtained from the topological structure of\nthe network to uniquely characterize cascades. In particular, we show that\nplanarity of the network allows us to statistically estimate a dynamical\nprocess consistent with the observed network and thereby uniquely identify a\npath for shock propagation from any chosen epicenter to all other nodes in the\nnetwork. We analyze the distress propagation mechanism in closed loops giving\nrise to a detailed picture of the effect of feedback loops in transmitting\nshocks. We show usefulness and applications of the algorithm in two networks\nwith dynamics at different time-scales: worldwide GDP growth network and stock\nnetwork. In both cases, we observe that the model predicts the impact of the\nshocks emanating from the US would be concentrated within the cluster of\ndeveloped countries and the developing countries show very muted response,\nwhich is consistent with empirical observations over the past decade.\n"
    },
    {
        "paper_id": 2001.01605,
        "authors": "Shuyao Wu, Jiao Huang, Shuangcheng Li",
        "title": "Classifying ecosystem disservices and comparing their effects with\n  ecosystem services in Beijing, China",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
        "abstract": "  To completely understand the effects of urban ecosystems, the effects of\necosystem disservices should be considered along with the ecosystem services\nand require more research attention. In this study, we tried to better\nunderstand its formation through the use of cascade flowchart and\nclassification systems and compare their effects with ecosystem services. It is\nvitally important to differentiate final and intermediate ecosystem disservices\nfor understanding the negative effects of the ecosystem on human well-being.\nThe proposed functional classification of EDS (i.e. provisioning, regulating\nand cultural EDS) should also help better bridging EDS and ES studies. In\naddition, we used Beijing as a case study area to value the EDS caused by urban\necosystems and compare the findings with ES values. The results suggested that\nalthough EDS caused great financial loss the potential economic gain from\necosystem services still significantly outweigh the loss. Our study only sheds\nlight on valuating the net effects of urban ecosystems. In the future, we\nbelieve that EDS valuation should be at least equally considered in ecosystem\nvaluation studies to create more comprehensive and sustainable development\npolicies, land use proposals and management plans.\n"
    },
    {
        "paper_id": 2001.01612,
        "authors": "Pierre Chen, Edmond Lezmi, Thierry Roncalli, Jiali Xu",
        "title": "A Note on Portfolio Optimization with Quadratic Transaction Costs",
        "comments": "18 pages, 5 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this short note, we consider mean-variance optimized portfolios with\ntransaction costs. We show that introducing quadratic transaction costs makes\nthe optimization problem more difficult than using linear transaction costs.\nThe reason lies in the specification of the budget constraint, which is no\nlonger linear. We provide numerical algorithms for solving this issue and\nillustrate how transaction costs may considerably impact the expected returns\nof optimized portfolios.\n"
    },
    {
        "paper_id": 2001.01641,
        "authors": "Huirong Liu",
        "title": "Housing Investment, Stock Market Participation and Household Portfolio\n  choice: Evidence from China's Urban Areas",
        "comments": "26 pages, 4 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper employs the survey data of CHFS (2013) to investigate the impact\nof housing investment on household stock market participation and portfolio\nchoice. The results show that larger housing investment encourages the\nhousehold participation in the stock market, but reduces the proportion of\ntheir stockholding. The above conclusion remains true even when the endogeneity\nproblem is controlled with risk attitude classification, Heckman model test and\nsubsample regression. This study shows that the growth in the housing market\nwill not lead to stock market development because of lack of household\nfinancial literacy and the low expected yield on stock market.\n"
    },
    {
        "paper_id": 2001.01646,
        "authors": "Khaled Masoumifard, Mohammad Zokaei",
        "title": "The Optimal Dynamic Reinsurance Strategies in Multidimensional Portfolio",
        "comments": "26 Pages, 24 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/publicdomain/zero/1.0/",
        "abstract": "  The present paper addresses the issue of choosing an optimal dynamic\nreinsurance policy, which is state-dependent, for an insurance company that\noperates under multiple insurance business lines. The optimal survival function\nis characterized as the unique nondecreasing viscosity solution of the\nassociated Hamilton-Jacobi-Bellman equation (HJB) equation with limit one at\ninfinity. The finite difference method (FDM) has been utilized for the\nnumerical solution of the optimal survival function and optimal dynamic\nreinsurance strategies and the proof for the convergence of the numerical\nsolution to the survival probability function is provided.\n"
    },
    {
        "paper_id": 2001.01718,
        "authors": "Nima Golshani, Ehsan Rahimi, Ramin Shabanpour, Kouros Mohammadian,\n  Joshua Auld, Hubert Ley",
        "title": "Passengers' Travel Behavior in Response to Unplanned Transit Disruptions",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Public transit disruption is becoming more common across different transit\nservices, which can have a destructive influence on the resiliency and\nreliability of the transportation system. Utilizing a recently collected data\nof transit users in the Chicago Metropolitan Area, the current study aims to\nanalyze how transit users respond to unplanned service disruption and disclose\nthe factors that affect their behavior.\n"
    },
    {
        "paper_id": 2001.01789,
        "authors": "Jim Gatheral, Paul Jusselin and Mathieu Rosenbaum",
        "title": "The quadratic rough Heston model and the joint S&P 500/VIX smile\n  calibration problem",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Fitting simultaneously SPX and VIX smiles is known to be one of the most\nchallenging problems in volatility modeling. A long-standing conjecture due to\nJulien Guyon is that it may not be possible to calibrate jointly these two\nquantities with a model with continuous sample-paths. We present the quadratic\nrough Heston model as a counterexample to this conjecture. The key idea is the\ncombination of rough volatility together with a price-feedback (Zumbach)\neffect.\n"
    },
    {
        "paper_id": 2001.0186,
        "authors": "Sergey Nadtochiy",
        "title": "A simple microstructural explanation of the concavity of price impact",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This article provides a simple explanation of the asymptotic concavity of the\nprice impact of a meta-order via the microstructural properties of the market.\nThis explanation is made more precise by a model in which the local\nrelationship between the order flow and the fundamental price (i.e. the local\nprice impact) is linear, with a constant slope, which makes the model\ndynamically consistent. Nevertheless, the expected impact on midprice from a\nlarge sequence of co-directional trades is nonlinear and asymptotically\nconcave. The main practical conclusion of the proposed explanation is that,\nthroughout a meta-order, the volumes at the best bid and ask prices change (on\naverage) in favor of the executor. This conclusion, in turn, relies on two more\nconcrete predictions, one of which can be tested, at least for large-tick\nstocks, using publicly available market data.\n"
    },
    {
        "paper_id": 2001.01998,
        "authors": "Dariusz Zawisza",
        "title": "A note on the worst case approach for a market with a stochastic\n  interest rate",
        "comments": null,
        "journal-ref": "Applicationes Mathematicae 45 (2018), 151-160",
        "doi": "10.4064/am2348-2-2018",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We solve robust optimization problem and show the example of the market model\nfor which the worst case measure is not a martingale measure. In our model the\ninstantaneous interest rate is determined by the Hull-White model and the\ninvestor employs the HARA utility to measure his satisfaction.To protect\nagainst the model uncertainty he uses the worst case measure approach. The\nproblem is formulated as a stochastic game between the investor and the market\nfrom the other side. PDE methods are used to find the saddle point and the\nprecise verification argument is provided.\n"
    },
    {
        "paper_id": 2001.02099,
        "authors": "Cl\\'emence Alasseur and Corinne Chaton and Emma Hubert",
        "title": "Optimal contracts under adverse selection for staple goods: efficiency\n  of in-kind insurance",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  An income loss can have a negative impact on households, forcing them to\nreduce their consumption of some staple goods. This can lead to health issues\nand, consequently, generate significant costs for society. We suggest that\nconsumers can, to prevent these negative consequences, buy insurance to secure\nsufficient consumption of a staple good if they lose part of their income. We\ndevelop a two-period/two-good principal-agent problem with adverse selection\nand endogenous reservation utility to model insurance with in-kind benefits.\nThis model allows us to obtain semi-explicit solutions for the insurance\ncontract and is applied to the context of fuel poverty. For this application,\nour model allows to conclude that, even in the least efficient scenario from\nthe households point of view, i.e., when the insurance is provided by a\nmonopoly, this mechanism decreases significantly the risk of fuel poverty of\nhouseholds by ensuring them a sufficient consumption of energy. The\neffectiveness of in-kind insurance is highlighted through a comparison with\nincome insurance, but our results nevertheless underline the need to regulate\nsuch insurance market.\n"
    },
    {
        "paper_id": 2001.02115,
        "authors": "Rickard Nyman, Paul Ormerod",
        "title": "Understanding the Great Recession Using Machine Learning Algorithms",
        "comments": "arXiv admin note: substantial text overlap with arXiv:1701.01428",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Nyman and Ormerod (2017) show that the machine learning technique of random\nforests has the potential to give early warning of recessions. Applying the\napproach to a small set of financial variables and replicating as far as\npossible a genuine ex ante forecasting situation, over the period since 1990\nthe accuracy of the four-step ahead predictions is distinctly superior to those\nactually made by the professional forecasters. Here we extend the analysis by\nexamining the contributions made to the Great Recession of the late 2000s by\neach of the explanatory variables. We disaggregate private sector debt into its\nhousehold and non-financial corporate components. We find that both household\nand non-financial corporate debt were key determinants of the Great Recession.\nWe find a considerable degree of non-linearity in the explanatory models. In\ncontrast, the public sector debt to GDP ratio appears to have made very little\ncontribution. It did rise sharply during the Great Recession, but this was as a\nconsequence of the sharp fall in economic activity rather than it being a\ncause. We obtain similar results for both the United States and the United\nKingdom.\n"
    },
    {
        "paper_id": 2001.022,
        "authors": "Simon J. Berrebi, Kari E. Watkins",
        "title": "Whos Ditching the Bus?",
        "comments": null,
        "journal-ref": null,
        "doi": "10.1016/j.tra.2020.02.016",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper uses stop-level passenger count data in four cities to understand\nthe nation-wide bus ridership decline between 2012 and 2018. The local\ncharacteristics associated with ridership change are evaluated in Portland,\nMiami, Minneapolis/St-Paul, and Atlanta. Poisson models explain ridership as a\ncross-section and the change thereof as a panel. While controlling for the\nchange in frequency, jobs, and population, the correlation with local\nsocio-demographic characteristics are investigated using data from the American\nCommunity Survey. The effect of changing neighborhood demographics on bus\nridership are modeled using Longitudinal Employer-Household Dynamics data. At a\npoint in time, neighborhoods with high proportions of non-white, carless, and\nmost significantly, high-school-educated residents are the most likely to have\nhigh ridership. Over time, white neighborhoods are losing the most ridership\nacross all four cities. In Miami and Atlanta, places with high concentrations\nof residents with college education and without access to a car also lose\nridership at a faster rate. In Minneapolis/St-Paul, the proportion of\ncollege-educated residents is linked to ridership gain. The sign and\nsignificance of these results remain consistent even when controlling for\nintra-urban migration. Although bus ridership is declining across neighborhood\ncharacteristics, these results suggest that the underlying cause of bus\nridership decline must be primarily affecting the travel behavior of white bus\nriders.\n"
    },
    {
        "paper_id": 2001.02322,
        "authors": "Hector Galindo-Silva",
        "title": "External Threats, Political Turnover and Fiscal Capacity",
        "comments": null,
        "journal-ref": "Econ Polit. 2020; 32: 430-462",
        "doi": "10.1111/ecpo.12155",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In most of the recent literature on state capacity, the significance of wars\nin state-building assumes that threats from foreign countries generate common\ninterests among domestic groups, leading to larger investments in state\ncapacity. However, many countries that have suffered external conflicts don't\nexperience increased unity. Instead, they face factional politics that often\nlead to destructive civil wars. This paper develops a theory of the impact of\ninterstate conflicts on fiscal capacity in which fighting an external threat is\nnot always a common-interest public good, and in which interstate conflicts can\nlead to civil wars. The theory identifies conditions under which an increased\nrisk of external conflict decreases the chance of civil war, which in turn\nresults in a government with a longer political life and with more incentives\nto invest in fiscal capacity. These conditions depend on the cohesiveness of\ninstitutions, but in a non-trivial and novel way: a higher risk of an external\nconflict that results in lower political turnover, but that also makes a\nforeign invasion more likely, contributes to state-building only if\ninstitutions are sufficiently incohesive.\n"
    },
    {
        "paper_id": 2001.02404,
        "authors": "Frido Rolloos",
        "title": "Nonparametric Pricing and Hedging of Volatility Swaps in Stochastic\n  Volatility Models",
        "comments": "Revision of section on hedging volatility swaps",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper the zero vanna implied volatility approximation for the price\nof freshly minted volatility swaps is generalised to seasoned volatility swaps.\nWe also derive how volatility swaps can be hedged using a strip of vanilla\noptions with weights that are directly related to trading intuition.\nAdditionally, we derive first and second order hedges for volatility swaps\nusing only variance swaps. As dynamically trading variance swaps is in general\ncheaper and operationally less cumbersome compared to dynamically rebalancing a\ncontinuous strip of options, our result makes the hedging of volatility swaps\nboth practically feasible and robust. Within the class of stochastic volatility\nmodels our pricing and hedging results are model-independent and can be\nimplemented at almost no computational cost.\n"
    },
    {
        "paper_id": 2001.02508,
        "authors": "Mukaramah Harun",
        "title": "Pursuing More Sustainable Energy Consumption by Analyzing Sectoral\n  Direct and Indirect Energy Use in Malaysia: An Input-Output Analysis",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Malaysia is experiencing ever increasing domestic energy consumption. This\nstudy is an attempt at analyzing the changes in sectoral energy intensities in\nMalaysia for the period 1995 to 2011. The study quantifies the sectoral total,\ndirect, and indirect energy intensities to track the sectors that are\nresponsible for the increasing energy consumption. The energy input-output\nmodel which is a frontier method for examining resource embodiments in goods\nand services on a sectoral scale that is popular among scholars has been\napplied in this study.\n"
    },
    {
        "paper_id": 2001.02509,
        "authors": "Mukaramah Harun",
        "title": "Ways to Reduce Cost of Living: A Case Study among Low Income Household\n  in Kubang Pasu, Kedah, Malaysia",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  This study was conducted to examine and understand the spending behavior of\nlow income households (B40), namely households with income of RM3800 and below.\nThe study focused on the area Kubang Pasu District, Kedah.\n"
    },
    {
        "paper_id": 2001.0251,
        "authors": "Mukaramah Harun",
        "title": "Infrastructure Disparities in Northern Malaysia",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  This study examines the disparities of infrastructure in four states in\nNorthern Peninsular Malaysia. This study used a primer data which is collected\nby using a face to face interview with a structure questionnaire on head of\nhousehold at Kedah, Perlis, Penang and Perak. The list of respondents is\nprovided by the Department of Statistics of Malaysia (DOS). The Department of\nStatistics of Malaysia (DOS) uses the population observation in 2010 to\ndetermine the respondents is provided by the Department of Statistics of\nMalaysia (DOS).\n"
    },
    {
        "paper_id": 2001.02783,
        "authors": "Iftekhairul Islam (1) and Fahad Shaon (1) ((1) The University of Texas\n  at Dallas)",
        "title": "If the Prospect of Some Occupations Are Stagnating With Technological\n  Advancement? A Task Attribute Approach to Detect Employment Vulnerability",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Two distinct trends can prove the existence of technological unemployment in\nthe US. First, there are more open jobs than the number of unemployed persons\nlooking for a job, and second, the shift of the Beveridge curve. There have\nbeen many attempts to find the cause of technological unemployment. However,\nall of these approaches fail when it comes to evaluating the impact of modern\ntechnologies on employment future. This study hypothesizes that rather than\nlooking into skill requirement or routine non-routine discrimination of tasks,\na holistic approach is required to predict which occupations are going to be\nvulnerable with the advent of this 4th industrial revolution, i.e., widespread\napplication of AI, ML algorithms, and Robotics. Three critical attributes are\nconsidered: bottleneck, hazardous, and routine. Forty-five relevant attributes\nare chosen from the O*NET database that can define these three types of tasks.\nPerforming Principal Axis Factor Analysis, and K-medoid clustering, the study\ndiscovers a list of 367 vulnerable occupations. The study further analyzes the\nlast nine years of national employment data and finds that over the previous\nfour years, the growth of vulnerable occupations is only half than that of\nnon-vulnerable ones despite the long rally of economic expansion.\n"
    },
    {
        "paper_id": 2001.02804,
        "authors": "Paul Minard",
        "title": "Institutions and China's comparative development",
        "comments": null,
        "journal-ref": null,
        "doi": "10.13140/RG.2.2.26690.73922",
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Robust assessment of the institutionalist account of comparative development\nis hampered by problems of omitted variable bias and reverse causation, since\ninstitutional quality is not randomly assigned with respect to geographic and\nhuman capital endowments. A recent series of papers has applied spatial\nregression discontinuity designs to estimate the impact of institutions on\nincomes at international borders, drawing inference from the abrupt\ndiscontinuity in governance at borders, whereas other determinants of income\nvary smoothly across borders. I extend this literature by assessing the\nimportance of sub-national variation in institutional quality at provincial\nborders in China. Employing nighttime lights emissions as a proxy for income,\nacross multiple specifications I find no evidence in favour of an\ninstitutionalist account of the comparative development of Chinese provinces.\n"
    },
    {
        "paper_id": 2001.02863,
        "authors": "Weipan Xu, Xiaozhen Qin, Xun Li, Haohui\"Caron\" Chen, Morgan Frank,\n  Alex Rutherford, Andrew Reeson and Iyad Rahwan",
        "title": "China's First Workforce Skill Taxonomy",
        "comments": "20 pages, 4 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  China is the world's second largest economy. After four decades of economic\nmiracles, China's economy is transitioning into an advanced, knowledge-based\neconomy. Yet, we still lack a detailed understanding of the skills that underly\nthe Chinese labor force, and the development and spatial distribution of these\nskills. For example, the US standardized skill taxonomy O*NET played an\nimportant role in understanding the dynamics of manufacturing and\nknowledge-based work, as well as potential risks from automation and\noutsourcing. Here, we use Machine Learning techniques to bridge this gap,\ncreating China's first workforce skill taxonomy, and map it to O*NET. This\nenables us to reveal workforce skill polarization into social-cognitive skills\nand sensory-physical skills, and to explore the China's regional inequality in\nlight of workforce skills, and compare it to traditional metrics such as\neducation. We build an online tool for the public and policy makers to explore\nthe skill taxonomy: skills.sysu.edu.cn. We will also make the taxonomy dataset\npublicly available for other researchers upon publication.\n"
    },
    {
        "paper_id": 2001.02959,
        "authors": "Roy Cerqueti and Luca De Benedictis and Valerio Leone Sciabolazza",
        "title": "Segregation with Social Linkages: Evaluating Schelling's Model with\n  Networked Individuals",
        "comments": "38 pages, 24 figures",
        "journal-ref": null,
        "doi": "10.1111/meca.12367",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper generalizes the original Schelling (1969, 1971a,b, 2006) model of\nracial and residential segregation to a context of variable externalities due\nto social linkages. In a setting in which individuals' utility function is a\nconvex combination of a heuristic function \u007fa la Schelling, of the distance to\nfriends, and of the cost of moving, the prediction of the original model gets\nattenuated: the segregation equilibria are not the unique solutions. While the\ncost of distance has a monotonic pro-status-quo effect, equivalent to that of\nmodels of migration and gravity models, if friends and neighbours are formed\nfollowing independent processes the location of friends in space generates an\nexternality that reinforces the initial configuration if the distance to\nfriends is minimal, and if the degree of each agent is high. The effect on\nsegregation equilibria crucially depends on the role played by network\nexternalities.\n"
    },
    {
        "paper_id": 2001.02966,
        "authors": "Jinwoo Park",
        "title": "Clustering Approaches for Global Minimum Variance Portfolio",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
        "abstract": "  The only input to attain the portfolio weights of global minimum variance\nportfolio (GMVP) is the covariance matrix of returns of assets being considered\nfor investment. Since the population covariance matrix is not known, investors\nuse historical data to estimate it. Even though sample covariance matrix is an\nunbiased estimator of the population covariance matrix, it includes a great\namount of estimation error especially when the number of observed data is not\nmuch bigger than number of assets. As it is difficult to estimate the\ncovariance matrix with high dimensionality all at once, clustering stocks is\nproposed to come up with covariance matrix in two steps: firstly, within a\ncluster and secondly, between clusters. It decreases the estimation error by\nreducing the number of features in the data matrix. The motivation of this\ndissertation is that the estimation error can still remain high even after\nclustering, if a large amount of stocks is clustered together in a single\ngroup. This research proposes to utilize a bounded clustering method in order\nto limit the maximum cluster size. The result of experiments shows that not\nonly the gap between in-sample volatility and out-of-sample volatility\ndecreases, but also the out-of-sample volatility gets reduced. It implies that\nwe need a bounded clustering algorithm so that maximum clustering size can be\nprecisely controlled to find the best portfolio performance.\n"
    },
    {
        "paper_id": 2001.03043,
        "authors": "Mukaramah Harun",
        "title": "Determinants of Social-economic Mobility in the Northern Region of\n  Malaysia",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Colleting the data through a survey in the Northern region of Malaysia;\nKedah, Perlis, Penang and Perak, this study investigates intergenerational\nsocial mobility in Malaysia. We measure and analyzed the factors that influence\nsocial-economic mobility by using binary choice model (logit model). Social\nmobility can be measured in several ways, by income, education, occupation or\nsocial class. More often, economic research has focused on some measure of\nincome. Social mobility variable is measured using the difference between\neducational achievement between a father and son. If there is a change of at\nleast of two educational levels between a father and son, then this study will\nassign the value one which means that social mobility has occurred.\n"
    },
    {
        "paper_id": 2001.03045,
        "authors": "Mukaramah Harun",
        "title": "Estimating the Impact of GST Implementation on Cost of Production and\n  Cost of Living in Malaysia",
        "comments": "in Malaysian language",
        "journal-ref": null,
        "doi": "10.17576/JEM-2016-5002-02",
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  The implementation of Goods and Services Tax(GST) is often attributed as the\nmain cause of the rising prices of goods and services. The main objective of\nthis study is to estimate the extent of GST implementation impact on the costs\nof production, which in turn have implication on households living costs.\n"
    },
    {
        "paper_id": 2001.03046,
        "authors": "Mukaramah Harun",
        "title": "Relationship between Type of Risks and Income of the Rural Households in\n  the Pattani Province of Thailand",
        "comments": null,
        "journal-ref": "Asian Social Science 2014",
        "doi": "10.5539/ass.v10n17p204",
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  This study examines the relationship between type of risks and income of the\nrural households in Pattani province,Thailand using the standard multiple\nregression analysis.A multi-stage sampling technique is employed to select 600\nhouseholds of 12 districts in the rural Pattani province and a structured\nquestionnaire is used for data collection.Evidences from descriptive analysis\nshow that the type of risks faced by households in rural Pattani province are\njob loss,reduction of salary,household member died,household members who work\nhave accident,marital problem and infection of crops or livestock.In\naddition,result from the regression analysis suggests that job loss,household\nmember died and marital problem have significant negative effects on the\nhouseholds income.The result suggests that job loss has adverse impact on\nhouseholds income.\n"
    },
    {
        "paper_id": 2001.03099,
        "authors": "Yufang Wang and Haiyan Wang",
        "title": "Using Networks and Partial Differential Equations to Predict Bitcoin\n  Price",
        "comments": null,
        "journal-ref": null,
        "doi": "10.1063/5.0002759",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Over the past decade, the blockchain technology and its Bitcoin\ncryptocurrency have received considerable attention. Bitcoin has experienced\nsignificant price swings in daily and long-term valuations. In this paper, we\npropose a partial differential equation (PDE) model on the bitcoin transaction\nnetwork for predicting bitcoin price. Through analysis of bitcoin subgraphs or\nchainlets, the PDE model captures the influence of transaction patterns on\nbitcoin price over time and combines the effect of all chainlet clusters. In\naddition, Google Trends Index is incorporated to the PDE model to reflect the\neffect of bitcoin market sentiment. The experiment shows that the average\naccuracy of daily bitcoin price prediction is 0.82 for 362 consecutive days in\n2017. The results demonstrate the PDE model is capable of predicting bitcoin\nprice. The paper is the first attempt to apply a PDE model to the bitcoin\ntransaction network for predicting bitcoin price.\n"
    },
    {
        "paper_id": 2001.03101,
        "authors": "Vincent Lemaire and Thibaut Montes and Gilles Pag\\`es",
        "title": "Stationary Heston model: Calibration and Pricing of exotics using\n  Product Recursive Quantization",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  A major drawback of the Standard Heston model is that its implied volatility\nsurface does not produce a steep enough smile when looking at short maturities.\nFor that reason, we introduce the Stationary Heston model where we replace the\ndeterministic initial condition of the volatility by its invariant measure and\nshow, based on calibrated parameters, that this model produce a steeper smile\nfor short maturities than the Standard Heston model. We also present numerical\nsolution based on Product Recursive Quantization for the evaluation of exotic\noptions (Bermudan and Barrier options).\n"
    },
    {
        "paper_id": 2001.0319,
        "authors": "Christoph K\\\"uhn and Alexander Molitor",
        "title": "Semimartingale price systems in models with transaction costs beyond\n  efficient friction",
        "comments": "48 pages",
        "journal-ref": "Finance and Stochastics, 26, 927-982 (2022)",
        "doi": "10.1007/s00780-022-00484-9",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  A standing assumption in the literature on proportional transaction costs is\nefficient friction. Together with robust no free lunch with vanishing risk, it\nrules out strategies of infinite variation, as they usually appear in\nfrictionless markets. In this paper, we show how the models with and without\ntransaction costs can be unified.\n  The bid and the ask price of a risky asset are given by c\\'adl\\'ag processes\nwhich are locally bounded from below and may coincide at some points. In a\nfirst step, we show that if the bid-ask model satisfies \"no unbounded profit\nwith bounded risk\" for simple strategies, then there exists a semimartingale\nlying between the bid and the ask price process.\n  In a second step, under the additional assumption that the zeros of the\nbid-ask spread are either starting points of an excursion away from zero or\ninner points from the right, we show that for every bounded predictable\nstrategy specifying the amount of risky assets, the semimartingale can be used\nto construct the corresponding self-financing risk-free position in a\nconsistent way. Finally, the set of most general strategies is introduced,\nwhich also provides a new view on the frictionless case.\n"
    },
    {
        "paper_id": 2001.03197,
        "authors": "Baichuan Mo, Zhejing Cao, Hongmou Zhang, Yu Shen, Jinhua Zhao",
        "title": "Competition between shared autonomous vehicles and public transit: A\n  case study in Singapore",
        "comments": null,
        "journal-ref": null,
        "doi": "10.1016/j.trc.2021.103058",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Emerging autonomous vehicles (AV) can either supplement the public\ntransportation (PT) system or compete with it. This study examines the\ncompetitive perspective where both AV and PT operators are profit-oriented with\ndynamic adjustable supply strategies under five regulatory structures regarding\nwhether the AV operator is allowed to change the fleet size and whether the PT\noperator is allowed to adjust headway. Four out of the five scenarios are\nconstrained competition while the other one focuses on unconstrained\ncompetition to find the Nash Equilibrium. We evaluate the competition process\nas well as the system performance from the standpoints of four stakeholders --\nthe AV operator, the PT operator, passengers, and the transport authority. We\nalso examine the impact of PT subsidies on the competition results including\nboth demand-based and supply-based subsidies. A heuristic algorithm is proposed\nto update supply strategies for AV and PT based on the operators' historical\nactions and profits. An agent-based simulation model is implemented in the\nfirst-mile scenario in Tampines, Singapore. We find that the competition can\nresult in higher profits and higher system efficiency for both operators\ncompared to the status quo. After the supply updates, the PT services are\nspatially concentrated to shorter routes feeding directly to the subway station\nand temporally concentrated to peak hours. On average, the competition reduces\nthe travel time of passengers but increases their travel costs. Nonetheless,\nthe generalized travel cost is reduced when incorporating the value of time.\nWith respect to the system efficiency, the bus supply adjustment increases the\naverage vehicle load and reduces the total vehicle kilometer traveled measured\nby the passenger car equivalent (PCE), while the AV supply adjustment does the\nopposite.\n"
    },
    {
        "paper_id": 2001.03213,
        "authors": "Mustafa Abdallah, Parinaz Naghizadeh, Ashish R. Hota, Timothy Cason,\n  Saurabh Bagchi, and Shreyas Sundaram",
        "title": "Behavioral and Game-Theoretic Security Investments in Interdependent\n  Systems Modeled by Attack Graphs",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We consider a system consisting of multiple interdependent assets, and a set\nof defenders, each responsible for securing a subset of the assets against an\nattacker. The interdependencies between the assets are captured by an attack\ngraph, where an edge from one asset to another indicates that if the former\nasset is compromised, an attack can be launched on the latter asset. Each edge\nhas an associated probability of successful attack, which can be reduced via\nsecurity investments by the defenders. In such scenarios, we investigate the\nsecurity investments that arise under certain features of human decision-making\nthat have been identified in behavioral economics. In particular, humans have\nbeen shown to perceive probabilities in a nonlinear manner, typically\noverweighting low probabilities and underweighting high probabilities. We show\nthat suboptimal investments can arise under such weighting in certain network\ntopologies. We also show that pure strategy Nash equilibria exist in settings\nwith multiple (behavioral) defenders, and study the inefficiency of the\nequilibrium investments by behavioral defenders compared to a centralized\nsocially optimal solution.\n"
    },
    {
        "paper_id": 2001.03246,
        "authors": "Jeffrey Ding and Allan Dafoe",
        "title": "The Logic of Strategic Assets: From Oil to Artificial Intelligence",
        "comments": "Added references and corrected typos",
        "journal-ref": null,
        "doi": "10.1080/09636412.2021.1915583",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  What resources and technologies are strategic? This question is often the\nfocus of policy and theoretical debates, where the label \"strategic\" designates\nthose assets that warrant the attention of the highest levels of the state. But\nthese conversations are plagued by analytical confusion, flawed heuristics, and\nthe rhetorical use of \"strategic\" to advance particular agendas. We aim to\nimprove these conversations through conceptual clarification, introducing a\ntheory based on important rivalrous externalities for which socially optimal\nbehavior will not be produced alone by markets or individual national security\nentities. We distill and theorize the most important three forms of these\nexternalities, which involve cumulative-, infrastructure-, and\ndependency-strategic logics. We then employ these logics to clarify three\nimportant cases: the Avon 2 engine in the 1950s, the U.S.-Japan technology\nrivalry in the late 1980s, and contemporary conversations about artificial\nintelligence.\n"
    },
    {
        "paper_id": 2001.03333,
        "authors": "Zineb Lanbouri and Saaid Achchab",
        "title": "A new approach for trading based on Long Short Term Memory technique",
        "comments": "7 pages, 6 figures",
        "journal-ref": "IJCSI (International Journal of Computer Science Issues), Volume\n  16, Issue 1, March 2019",
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  The stock market prediction has always been crucial for stakeholders, traders\nand investors. We developed an ensemble Long Short Term Memory (LSTM) model\nthat includes two-time frequencies (annual and daily parameters) in order to\npredict the next-day Closing price (one step ahead). Based on a four-step\napproach, this methodology is a serial combination of two LSTM algorithms. The\nempirical experiment is applied to 417 NY stock exchange companies. Based on\nOpen High Low Close metrics and other financial ratios, this approach proves\nthat the stock market prediction can be improved.\n"
    },
    {
        "paper_id": 2001.03401,
        "authors": "Rickard Nyman and Paul Ormerod",
        "title": "Text as Data: Real-time Measurement of Economic Welfare",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Economists are showing increasing interest in the use of text as an input to\neconomic research. Here, we analyse online text to construct a real time metric\nof welfare. For purposes of description, we call it the Feel Good Factor (FGF).\nThe particular example used to illustrate the concept is confined to data from\nthe London area, but the methodology is readily generalisable to other\ngeographical areas. The FGF illustrates the use of online data to create a\nmeasure of welfare which is not based, as GDP is, on value added in a\nmarket-oriented economy. There is already a large literature which measures\nwellbeing/happiness. But this relies on conventional survey approaches, and\nhence on the stated preferences of respondents. In unstructured online media\ntext, users reveal their emotions in ways analogous to the principle of\nrevealed preference in consumer demand theory. The analysis of online media\noffers further advantages over conventional survey-based measures of sentiment\nor well-being. It can be carried out in real time rather than with the lags\nwhich are involved in survey approaches. In addition, it is very much cheaper.\n"
    },
    {
        "paper_id": 2001.03461,
        "authors": "Eleftheria Kontou, Noreen C. McDonald",
        "title": "Associating Ridesourcing with Road Safety Outcomes: Insights from Austin\n  Texas",
        "comments": null,
        "journal-ref": null,
        "doi": "10.1371/journal.pone.0248311",
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Improving road safety and setting targets for reducing traffic-related\ncrashes and deaths are highlighted as part of the United Nation's sustainable\ndevelopment goals and vision zero efforts around the globe. The advent of\ntransportation network companies, such as ridesourcing, expands mobility\noptions in cities and may impact road safety outcomes. In this study, we\nanalyze the effects of ridesourcing use on road crashes, injuries, fatalities,\nand driving while intoxicated (DWI) offenses in Travis County Texas. Our\napproach leverages real-time ridesourcing volume to explain variation in road\nsafety outcomes. Spatial panel data models with fixed effects are deployed to\nexamine whether the use of ridesourcing is significantly associated with road\ncrashes and other safety metrics. Our results suggest that for a 10% increase\nin ridesourcing trips, we expect a 0.12% decrease in road crashes (p<0.05), a\n0.25% decrease in road injuries (p<0.001), and a 0.36% decrease in DWI offenses\n(p<0.0001) in Travis County. Ridesourcing use is not associated with road\nfatalities at a 0.05 significance level. This study augments existing work\nbecause it moves beyond binary indicators of ridesourcing presence or absence\nand analyzes patterns within an urbanized area rather than metropolitan-level\nvariation. Contributions include developing a data-rich approach for assessing\nthe impacts of ridesourcing use on our transportation system's safety, which\nmay serve as a template for future analyses of other US cities. Our findings\nprovide feedback to policymakers by clarifying associations between\nridesourcing use and traffic safety, while helping identify sets of actions to\nachieve safer and more efficient shared mobility systems.\n"
    },
    {
        "paper_id": 2001.03486,
        "authors": "Ahmad Zafarullah Abdul Jalil, Mukaramah Harun, Siti Hadijah Che Mat",
        "title": "Macroeconomic Instability And Fiscal Decentralization: An Empirical\n  Analysis",
        "comments": null,
        "journal-ref": "PRAGUE ECONOMIC PAPERS 2012",
        "doi": "10.18267/j.pep.416",
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  The main objective of this paper is to fill a critical gap in the literature\nby analyzing the effects of decentralization on the macroeconomic stability. A\nsurvey of the voluminous literature on decentralization suggests that the\nquestion of the links between decentralization and macroeconomic stability has\nbeen relatively scantily analyzed. Even though there is still a lot of room for\nanalysis as far as the effects of decentralization on other aspects of the\neconomy are concerned, we believe that it is in this area that a more thorough\nanalyses are mostly called for. Through this paper, we will try to shed more\nlight on the issue notably by looking at other dimension of macroeconomic\nstability than the ones usually employed in previous studies as well as by\nexamining other factors that might accentuate or diminish the effects of\ndecentralization on macroeconomic stability. Our results found that\ndecentralization appears to lead to a decrease in inflation rate. However, we\ndo not find any correlation between decentralization with the level of fiscal\ndeficit. Our results also show that the impact of decentralization on inflation\nis conditional on the level of perceived corruption and political institutions.\n"
    },
    {
        "paper_id": 2001.03487,
        "authors": "Siti Hadijah Che Mata, Ahmad Zafarullah Abdul Jalil, Mukaramah Harun",
        "title": "Does Non-Farm Income Improve The Poverty and Income Inequality Among\n  Agricultural Household In Rural Kedah?",
        "comments": "procedia economics and finance 2012",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  This paper used a primary data collected through a surveys among farmers in\nrural Kedah to examine the effect of non farm income on poverty and income\ninequality. This paper employed two method, for the first objective which is to\nexamine the impact of non farm income to poverty, we used poverty decomposition\ntechniques - Foster, greer and Thorbecke (FGT) as has been done by Adams\n(2004). For the second objective, which is to examine the impact of non farm\nincome to income inequality, we used Gini decomposition techniques.\n"
    },
    {
        "paper_id": 2001.03488,
        "authors": "Mukaramah Harun, A.R. Zakariah, M. Azali",
        "title": "Constructing a Social Accounting Matrix Framework to Analyse the Impact\n  of Public Expenditure on Income Distribution in Malaysia",
        "comments": null,
        "journal-ref": "Jurnal Ekonomi Malaysia 2012",
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  The use of the social accounting matrix (SAM) in income distribution analysis\nis a method recommended by economists. However, until now, there have only been\na few SAM developed in Malaysia. The last SAM produced for Malaysia was\ndeveloped in 1984 based upon data from 1970 and has not been updated since this\ntime despite the significance changes in the structure of the Malaysian\neconomy. The paper proposes a new Malaysian SAM framework to analyse public\nexpenditure impact on income distribution in Malaysia. The SAM developed in the\npresent paper is based on more recent data, providing an up-to date and\ncoherent picture of the complexity of the Malaysian economy. The paper\ndescribes the structure of the SAM framework with a detailed aggregation and\ndisaggregation of accounts related to public expenditure and income\ndistribution issues. In the SAM utilized in the present study, the detailed\nframework of the different components of public expenditure in the production\nsectors and household groups is essential in the analysis of the different\neffects of the various public expenditure programmes on the incomes of\nhouseholds among different groups.\n"
    },
    {
        "paper_id": 2001.03646,
        "authors": "Rong Fan and Xuegang (Jeff) Ban",
        "title": "Commuting Service Platform: Concept and Analysis",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We propose and investigate the concept of commuting service platforms (CSP)\nthat leverage emerging mobility services to provide commuting services and\nconnect directly commuters (employees) and their worksites (employers). By\napplying the two-sided market analysis framework, we show under what conditions\na CSP may present the two-sidedness. Both the monopoly and duopoly CSPs are\nthen analyzed. We showhowthe price allocation, i.e., the prices charged to\ncommuters and worksites, can impact the participation and profit of the CSPs.\nWe also add demand constraints to the duopoly model so that the participation\nrates ofworksites and employees are (almost) the same. With demand constraints,\nthe competition between the two CSPs becomes less intense in general.\nDiscussions are presented on how the results and findings in this paper may\nhelp build CSP in practice and how to develop new, CSP-based travel demand\nmanagement strategies.\n"
    },
    {
        "paper_id": 2001.03733,
        "authors": "Julia Eisenberg and Zbigniew Palmowski",
        "title": "Optimal Dividends Paid in a Foreign Currency for a L\\'evy Insurance Risk\n  Model",
        "comments": "arXiv admin note: text overlap with arXiv:1604.06892",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper considers an optimal dividend distribution problem for an\ninsurance company where the dividends are paid in a foreign currency. In the\nabsence of dividend payments, our risk process follows a spectrally negative\nL\\'evy process. We assume that the exchange rate is described by a an\nexponentially L\\'evy process, possibly containing the same risk sources like\nthe surplus of the insurance company under consideration. The control mechanism\nchooses the amount of dividend payments. The objective is to maximise the\nexpected dividend payments received until the time of ruin and a penalty\npayment at the time of ruin, which is an increasing function of the size of the\nshortfall at ruin. A complete solution is presented to the corresponding\nstochastic control problem. Via the corresponding Hamilton--Jacobi--Bellman\nequation we find the necessary and sufficient conditions for optimality of a\nsingle dividend barrier strategy. A number of numerical examples illustrate the\ntheoretical analysis.\n"
    },
    {
        "paper_id": 2001.03967,
        "authors": "Enrique Villamor (1), Pablo Olivares (2) ((1) FIU, (2) Ryerson\n  University)",
        "title": "Pricing Exchange Options under Stochastic Correlation",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper we study the pricing of exchange options when underlying assets\nhave stochastic volatility and stochastic correlation. An approximation using a\nclosed-form approximation based on a Taylor expansion of the conditional price\nis proposed. Numerical results are illustrated for exchanges between WTI and\nBrent type oil prices.\n"
    },
    {
        "paper_id": 2001.03982,
        "authors": "Saeed Nosratabadi, Amirhosein Mosavi and Zoltan Lakner",
        "title": "Food Supply Chain and Business Model Innovation",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper investigates the contribution of business model innovations in\nimprovement of food supply chains. Through a systematic literature review, the\nnotable business model innovations in the food industry are identified,\nsurveyed, and evaluated. Findings reveal that the innovations in value\nproposition, value creation processes, and value delivery processes of business\nmodels are the successful strategies proposed in food industry. It is further\ndisclosed that rural female entrepreneurs, social movements, and also urban\nconditions are the most important driving forces inducing the farmers to\nreconsider their business models. In addition, the new technologies and\nenvironmental factors are the secondary contributors in business model\ninnovation for the food processors. It is concluded that digitalization has\ndisruptively changed the food distributors models. E-commerce models and\ninternet of things are reported as the essential factors imposing the retailers\nto innovate their business models. Furthermore, the consumption demand and the\nproduct quality are two main factors affecting the business models of all the\nfirms operating in the food supply chain regardless of their positions in the\nchain. The findings of the current study provide an insight into the food\nindustry to design a sustainable business model to bridge the gap between food\nsupply and food demand.\n"
    },
    {
        "paper_id": 2001.04097,
        "authors": "Yuichi Ikeda and Hidetoshi Takeda",
        "title": "Reconstruction of Interbank Network using Ridge Entropy Maximization\n  Model",
        "comments": "submitted to a special issue dedicated to WEHIA 2019 of the Journal\n  of Economic Interaction and Coordination",
        "journal-ref": "a special issue dedicated to WEHIA 2019 of the Journal of Economic\n  Interaction and Coordination",
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We develop a network reconstruction model based on entropy maximization\nconsidering the sparsity of networks. We reconstruct the interbank network in\nJapan from financial data in individual banks' balance sheets using the\ndeveloped reconstruction model from 2000 to 2016. The observed sparsity of the\ninterbank network is successfully reproduced. We examine the characteristics of\nthe reconstructed interbank network by calculating important network\nattributes. We obtain the following characteristics, which are consistent with\nthe previously known stylized facts. Although we do not introduce the mechanism\nto generate the core and peripheral structure, we impose the constraints to\nconsider the sparsity that is no transactions within the same bank category\nexcept for major commercial banks, the core and peripheral structure has\nspontaneously emerged. We identify major nodes in each community using the\nvalue of PageRank and degree to examine the changing role of each bank\ncategory. The observed changing role of banks is considered a result of the\nquantitative and qualitative monetary easing policy started by the Bank of\nJapan in April 2013.\n"
    },
    {
        "paper_id": 2001.04185,
        "authors": "Valerio Volpati, Michael Benzaquen, Zoltan Eisler, Iacopo\n  Mastromatteo, Bence Toth, Jean-Philippe Bouchaud",
        "title": "Zooming In on Equity Factor Crowding",
        "comments": "7 pages, 5 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Crowding is most likely an important factor in the deterioration of strategy\nperformance, the increase of trading costs and the development of systemic\nrisk. We study the imprints of \\emph{crowding} on both anonymous market data\nand a large database of metaorders from institutional investors in the U.S.\nequity market. We propose direct metrics of crowding that capture the presence\nof investors contemporaneously trading the same stock in the same direction by\nlooking at fluctuations of the imbalances of trades executed on the market. We\nidentify significant signs of crowding in well known equity signals, such as\nFama-French factors and especially Momentum. We show that the rebalancing of a\nMomentum portfolio can explain between 1-2\\% of order flow, and that this\npercentage has been significantly increasing in recent years.\n"
    },
    {
        "paper_id": 2001.04188,
        "authors": "Antika Sinha, Sudip Mukherjee and Bikas K Chakrabarti",
        "title": "Econophysics Through Computation",
        "comments": "Invited review article for `Journal of Physics Through Computation'\n  (Clausius Scientific Press)",
        "journal-ref": "Journal of Physics Through Computation (2020) Vol. 3: 1-54",
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We introduce here very briefly, through some selective choices of problems\nand through the sample computer simulation programs (following the request of\nthe editor for this invited review in the Journal of Physics Through\nComputation), the newly developed field of econophysics. Though related\nattempts could be traced much earlier (see the Appendix), the formal researches\nin econophysics started in 1995. We hope, the readers (students \\& researchers)\ncan start themselves to enjoy the excitement, through the sample computer\nprograms given, and eventually can undertake researches in the frontier\nproblems, through the indicated survey literature provided.\n"
    },
    {
        "paper_id": 2001.04237,
        "authors": "Frank Klinker",
        "title": "Exponential moving average versus moving exponential average",
        "comments": null,
        "journal-ref": "Math. Semesterber. 58 (2011), no. 1, 97-107",
        "doi": "10.1007/s00591-010-0080-8",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this note we discuss the mathematical tools to define trend indicators\nwhich are used to describe market trends. We explain the relation between\naverages and moving averages on the one hand and the so called exponential\nmoving average (EMA) on the other hand. We present a lot of examples and give\nthe definition of the most frequently used trend indicator, the MACD, and\ndiscuss its properties.\n"
    },
    {
        "paper_id": 2001.04283,
        "authors": "Iacopo Savelli, Thomas Morstyn",
        "title": "Electricity prices and tariffs to keep everyone happy: a framework for\n  fixed and nodal prices coexistence in distribution grids with optimal tariffs\n  for investment cost recovery",
        "comments": null,
        "journal-ref": null,
        "doi": "10.1016/j.omega.2021.102450",
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Some consumers, particularly households, are unwilling to face volatile\nelectricity prices, and they can perceive as unfair price differentiation in\nthe same local area. For these reasons, nodal prices in distribution networks\nare rarely employed. However, the increasing availability of renewable\nresources and emerging price-elastic behaviours pave the way for the effective\nintroduction of marginal nodal pricing schemes in distribution networks. The\naim of the proposed framework is to show how traditional non-flexible consumers\ncan coexist with flexible users in a local distribution area. Flexible users\nwill pay nodal prices, whereas non-flexible consumers will be charged a fixed\nprice derived from the underlying nodal prices. Moreover, the developed\napproach shows how a distribution system operator should manage the local grid\nby optimally determining the lines to be expanded, and the collected network\ntariff levied on grid users, while accounting for both congestion rent and\ninvestment costs. The proposed model is formulated as a non-linear integer\nbilevel program, which is then recast as an equivalent single optimization\nproblem, by using integer algebra and complementarity relations. The power\nflows in the distribution area are modelled by resorting to a second-order cone\nrelaxation, whose solution is exact for radial networks under mild assumptions.\nThe final model results in a mixed-integer quadratically constrained program,\nwhich can be solved with off-the-shelf solvers. Numerical test cases based on\nboth 5-bus and 33-bus networks are reported to show the effectiveness of the\nproposed method.\n"
    },
    {
        "paper_id": 2001.04867,
        "authors": "Davide La Vecchia, Alban Moor, Olivier Scaillet",
        "title": "A Higher-Order Correct Fast Moving-Average Bootstrap for Dependent Data",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We develop and implement a novel fast bootstrap for dependent data. Our\nscheme is based on the i.i.d. resampling of the smoothed moment indicators. We\ncharacterize the class of parametric and semi-parametric estimation problems\nfor which the method is valid. We show the asymptotic refinements of the\nproposed procedure, proving that it is higher-order correct under mild\nassumptions on the time series, the estimating functions, and the smoothing\nkernel. We illustrate the applicability and the advantages of our procedure for\nGeneralized Empirical Likelihood estimation. As a by-product, our fast\nbootstrap provides higher-order correct asymptotic confidence distributions.\nMonte Carlo simulations on an autoregressive conditional duration model provide\nnumerical evidence that the novel bootstrap yields higher-order accurate\nconfidence intervals. A real-data application on dynamics of trading volume of\nstocks illustrates the advantage of our method over the routinely-applied\nfirst-order asymptotic theory, when the underlying distribution of the test\nstatistic is skewed or fat-tailed.\n"
    },
    {
        "paper_id": 2001.05095,
        "authors": "Minoru Osawa, Jos\\'e M. Gaspar",
        "title": "Production externalities and dispersion process in a multi-region\n  economy",
        "comments": "32 pages, 12 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We consider an economic geography model with two inter-regional proximity\nstructures: one governing goods trade and the other governing production\nexternalities across regions. We investigate how the introduction of the latter\naffects the timing of endogenous agglomeration and the spatial distribution of\nworkers across regions. As transportation costs decline, the economy undergoes\na progressive dispersion process. Mono-centric agglomeration emerges when\ninter-regional trade and/or production externalities incur high transportation\ncosts, while uniform dispersion occurs when these costs become negligibly small\n(i.e., when distance dies). In multi-regional geography, the network structure\nof production externalities can determine the geographical distribution of\nworkers as economic integration increases. If production externalities are\ngoverned solely by geographical distance, a mono-centric spatial distribution\nemerges in the form of suburbanization. However, if geographically distant\npairs of regions are connected through tight production linkages, multi-centric\nspatial distribution can be sustainable.\n"
    },
    {
        "paper_id": 2001.05124,
        "authors": "Yue Zhou",
        "title": "Rational Kernel on Pricing Models of Inflation Derivatives",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The aim of this thesis is to analyze and renovate few main-stream models on\ninflation derivatives. In the first chapter of the thesis, concepts of\nfinancial instruments and fundamental terms are introduced, such as coupon\nbond, inflation-indexed bond, swap.\n  In the second chapter of the thesis, classic models along the history of\ndeveloping quantified interest rate models are introduced and analyzed.\nMoreover, the classification of interest rate models is introduced to help\naudiences understand the intrinsic ideology behind each type of models.\n  In the third chapter of the thesis, the related mathematical knowledge is\nintroduced. This part has the contribution on understanding the terms and\nrelation among terms in each model introduced previously.\n  In the fourth part of the thesis, the renovation of HJM frame work is\nintroduced and analysis has been initiated.\n"
    },
    {
        "paper_id": 2001.05248,
        "authors": "Ofelia Bonesini and Antoine Jacquier and Chloe Lacombe",
        "title": "A theoretical analysis of Guyon's toy volatility model",
        "comments": "33 pages, 1 figure",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We provide a thorough analysis of the path-dependent volatility model\nintroduced by Guyon \\cite{G17}, proving existence and uniqueness of a strong\nsolution, characterising its behaviour at boundary points, providing asymptotic\nclosed-form option prices as well as deriving small-time behaviour estimates.\n"
    },
    {
        "paper_id": 2001.05575,
        "authors": "Sallahuddin Hassan, Zalila Othman, Mukaramah Harun",
        "title": "Ownership Structure Variation and Firm Efficiency",
        "comments": null,
        "journal-ref": "Asian Social Science 2014",
        "doi": "10.5539/ass.v10n11p233",
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Firms with different ownership structures could be argued to have different\nlevels of efficiency.Highly concentrated firms are expected to be more\nefficient as this type of ownership structure may alleviate the conflict of\ninterest between managers and shareholders.In Malaysia, public-listed firms\nhave been found to have highly concentrated ownership structure.However,\nwhether this evidence holds for every industry has not been established.Hence,\nthe objective of this paper is to investigate whether there are variations in\nownership structure and firm's efficiency across sectors.To achieve this\nobjective, the frequency distributions of ownership structure were calculated\nand firms efficiency scores for consumer products, industrial products,\nconstruction and trading/services sectors were measured.Data Envelopment\nAnalysis(DEA) under the assumptions of constant returns to scale(CRS) and\nvariable returns to scale(VRS) was employed to estimate firms efficiency\nscores.A sample of 156 firms listed on the Kuala Lumpur Stock Exchange(KLSE)\nwas selected using the stratified random sampling method. The findings have\nshown that there are variations in firm ownership structure and efficiency\nacross sectors.\n"
    },
    {
        "paper_id": 2001.05788,
        "authors": "Nicola Secomandi",
        "title": "Quadratic Hedging and Optimization of Option Exercise Policies",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Quadratic hedging of option payoffs generates the variance optimal martingale\nmeasure. When an option features an exercise policy and its cash flows are\nhedged according to this approach, it may be tempting to optimize such a policy\nunder this measure. Because the variance optimal martingale measure may not be\nan equivalent probability measure, focusing on American options we show that\nthe resulting exercise policy may be unappealing. This drawback can sometimes\nbe remedied by imposing time consistency on exercise policies, but in general\npersists even in this case, which compounds the familiar issue that valuing an\noption using this measure may not result in an arbitrage free value. An\nalternative and known approach bypasses both of these pitfalls by optimizing\noption exercise policies under any given equivalent martingale measure and\nanchoring quadratic hedging to the resulting value of this policy. Additional\nresearch may assess on realistic applications the magnitude of the limitations\nassociated with optimizing option exercise policies based on the variance\noptimal martingale measure.\n"
    },
    {
        "paper_id": 2001.05906,
        "authors": "Philipp Harms, Chong Liu, Ariel Neufeld",
        "title": "Supermartingale deflators in the absence of a num\\'eraire",
        "comments": "33 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper we study arbitrage theory of financial markets in the absence\nof a num\\'eraire both in discrete and continuous time. In our main results, we\nprovide a generalization of the classical equivalence between no unbounded\nprofits with bounded risk (NUPBR) and the existence of a supermartingale\ndeflator. To obtain the desired results, we introduce a new approach based on\ndisintegration of the underlying probability space into spaces where the market\ncrashes at deterministic times.\n"
    },
    {
        "paper_id": 2001.06003,
        "authors": "Virginia Tsoukatou",
        "title": "Examining the correlation of the level of wage inequality with labor\n  market institutions",
        "comments": null,
        "journal-ref": "Journal of Economics and Political Economy Vol 6, No. 4, 323-343\n  (2020)",
        "doi": "10.5281/zenodo.3609865",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Technological change is responsible for major changes in the labor market.\nOne of the offspring of technological change is the SBTC, which is for many\neconomists the leading cause of the increasing wage inequality. However,\ndespite that the technological change affected similarly the majority of the\ndeveloped countries, nevertheless, the level of the increase of wage inequality\nwasn't similar. Following the predictions of the SBTC theory, the different\nlevels of inequality could be due to varying degrees of skill inequality\nbetween economies, possibly caused by variations in the number of skilled\nworkers available. However, recent research shows that the difference mentioned\nabove can explain a small percentage of the difference between countries.\nTherefore, most of the resulting inequality could be due to the different ways\nin which the higher level of skills is valued in each labor market. The\nposition advocated in this article is that technological change is largely\ngiven for all countries without much scope to reverse. Therefore, in order to\nillustrate the changes in the structure of wage distribution that cause wage\ninequality, we need to understand how technology affects labor market\ninstitutions.In this sense, the pay inequality caused by technological progress\nis not a phenomenon we passively accept. On the contrary, recognizing that the\nstructure and the way labor market institutions function is largely influenced\nby the way institutions respond to technological change, we can understand and\nmaybe reverse this underlying wage inequality.\n"
    },
    {
        "paper_id": 2001.06052,
        "authors": "Hossein Alidaee, Eric Auerbach, Michael P. Leung",
        "title": "Recovering Network Structure from Aggregated Relational Data using\n  Penalized Regression",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Social network data can be expensive to collect. Breza et al. (2017) propose\naggregated relational data (ARD) as a low-cost substitute that can be used to\nrecover the structure of a latent social network when it is generated by a\nspecific parametric random effects model. Our main observation is that many\neconomic network formation models produce networks that are effectively\nlow-rank. As a consequence, network recovery from ARD is generally possible\nwithout parametric assumptions using a nuclear-norm penalized regression. We\ndemonstrate how to implement this method and provide finite-sample bounds on\nthe mean squared error of the resulting estimator for the distribution of\nnetwork links. Computation takes seconds for samples with hundreds of\nobservations. Easy-to-use code in R and Python can be found at\nhttps://github.com/mpleung/ARD.\n"
    },
    {
        "paper_id": 2001.06166,
        "authors": "Somouaoga Bonkoungou and Alexander S. Nesterov",
        "title": "Comparing School Choice and College Admission Mechanisms By Their\n  Immunity to Strategic Admissions",
        "comments": "30 pages, 1 long table; added references, minor changes",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Recently dozens of school districts and college admissions systems around the\nworld have reformed their admission rules. As a main motivation for these\nreforms the policymakers cited strategic flaws of the rules: students had\nstrong incentives to game the system, which caused dramatic consequences for\nnon-strategic students. However, almost none of the new rules were\nstrategy-proof. We explain this puzzle. We show that after the reforms the\nrules became more immune to strategic admissions: each student received a\nsmaller set of schools that he can get in using a strategy, weakening\nincentives to manipulate. Simultaneously, the admission to each school became\nstrategy-proof to a larger set of students, making the schools more available\nfor non-strategic students. We also show that the existing explanation of the\npuzzle due to Pathak and S\\\"onmez (2013) is incomplete.\n"
    },
    {
        "paper_id": 2001.06275,
        "authors": "Jianhao Su",
        "title": "Corporate Governance, Noise Trading and Liquidity of Stocks",
        "comments": "22 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Our main task is to study the effect of corporate governance on the market\nliquidity of listed companies' stocks. We establish a theoretical model that\ncontains the heterogeneity of investors' beliefs to explain the mechanisms by\nwhich corporate governance improves liquidity of the corporate stocks. In this\nprocess we found that the existence of noise traders who are semi-informed in\nthe market is an important condition for corporate governance to have the\neffect of improving liquidity of the stocks. We further find that the strength\nof this effect is affected by the degree of noise traders' participation in\nmarket transactions. Our model reveals that corporate governance and the degree\nof noise traders' participation in transactions have a synergistic effect on\nimproving the liquidity of the stocks.\n"
    },
    {
        "paper_id": 2001.06356,
        "authors": "Paolo Bartesaghi, Gian Paolo Clemente and Rosanna Grassi",
        "title": "Community structure in the World Trade Network based on communicability\n  distances",
        "comments": "40 pages, 19 figures",
        "journal-ref": "Journal of Economic Interaction and Coordination (2020)",
        "doi": "10.1007/s11403-020-00309-y",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper, we investigate the mesoscale structure of the World Trade\nNetwork. In this framework, a specific role is assumed by short and long-range\ninteractions, and hence by the distance, between countries. Therefore, we\nidentify clusters through a new procedure that exploits Estrada communicability\ndistance and the vibrational communicability distance, which turn out to be\nparticularly suitable for catching the inner structure of the economic network.\nThe proposed methodology aims at finding the distance threshold that maximizes\na specific modularity function defined for general metric spaces. Main\nadvantages regard the computational efficiency of the procedure as well as the\npossibility to inspect intercluster and intracluster properties of the\nresulting communities. The numerical analysis highlights peculiar relationships\nbetween countries and provides a rich set of information that can hardly be\nachieved within alternative clustering approaches.\n"
    },
    {
        "paper_id": 2001.06412,
        "authors": "Axel A. Araneda and Nils Bertschinger",
        "title": "The sub-fractional CEV model",
        "comments": "Final version. Forthcoming in Physica A",
        "journal-ref": "Physica A, 2021",
        "doi": "10.1016/j.physa.2021.125974",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The sub-fractional Brownian motion (sfBm) is a stochastic process,\ncharacterized by non-stationarity in their increments and long-range\ndependency, considered as an intermediate step between the standard Brownian\nmotion (Bm) and the fractional Brownian motion (fBm). The mixed process, a\nlinear combination between a Bm and an independent sfBm, called mixed\nsub-fractional Brownian motion (msfBm), keeps the features of the sfBm adding\nthe semi-martingale property for H>3/4, is a suitable candidate to use in price\nfluctuation modeling, in particular for option pricing. In this note, we arrive\nat the European Call price under the Constant Elasticity of Variance (CEV)\nmodel driven by a mixed sub-fractional Brownian motion. Empirical tests show\nthe capacity of the proposed model to capture the temporal structure of option\nprices across different maturities.\n"
    },
    {
        "paper_id": 2001.06445,
        "authors": "Vassilis Polimenis",
        "title": "Trading on the Floor after Sweeping the Book",
        "comments": null,
        "journal-ref": "Review of Futures Markets, 2006, vol. 14(4), pp. 451-459",
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Informed traders need to trade fast in order to profit from their private\ninformation before it becomes public. Fast electronic markets provide such\nliquidity. Slow markets provide execution in an auction based trading floor.\nHybrid markets combine both execution venues. In its main result, the paper\nshows that to compensate for their slow and risky executions, trading floors\nneed to be at least twice as deep as the sweeping facility. Furthermore, when a\nstand-alone trading floor is enhanced with the addition of a sweeping facility,\noverall informed trading will decline because it is easier for informed traders\nto extract the full value of their private info.\n"
    },
    {
        "paper_id": 2001.06457,
        "authors": "Mahkameh Zarekarizi, Vivek Srikrishnan, and Klaus Keller",
        "title": "Neglecting Uncertainties Biases House-Elevation Decisions to Manage\n  Riverine Flood Risks",
        "comments": null,
        "journal-ref": null,
        "doi": "10.1038/s41467-020-19188-9",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Homeowners around the world elevate houses to manage flood risks. Deciding\nhow high to elevate a house poses a nontrivial decision problem. The U.S.\nFederal Emergency Management Agency (FEMA) recommends elevating existing houses\nto the Base Flood Elevation (the elevation of the 100-yr flood) plus a\nfreeboard. This recommendation neglects many uncertainties. Here we analyze a\ncase-study of riverine flood risk management using a multi-objective robust\ndecision-making framework in the face of deep uncertainties. While the\nquantitative results are location-specific, the approach and overall insights\nare generalizable. We find strong interactions between the economic,\nengineering, and Earth science uncertainties, illustrating the need for\nexpanding on previous integrated analyses to further understand the nature and\nstrength of these connections. Considering deep uncertainties surrounding flood\nhazards, the discount rate, the house lifetime, and the fragility can increase\nthe economically optimal house elevation to values well above FEMA\nrecommendation.\n"
    },
    {
        "paper_id": 2001.06548,
        "authors": "Stephen Clark",
        "title": "Who voted for a No Deal Brexit? A Composition Model of Great Britains\n  2019 European Parliamentary Elections",
        "comments": "This article is complied with the main manuscript followed by the\n  supplementary materials showing cartographic maps, scatter plots and marginal\n  fit plots",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The purpose of this paper is to use the votes cast at the 2019 European\nelections held in United Kingdom to re-visit the analysis conducted subsequent\nto its 2016 European Union referendum vote. This exercise provides a staging\npost on public opinion as the United Kingdom moves to leave the European Union\nduring 2020. A composition data analysis in a seemingly unrelated regression\nframework is adopted that respects the compositional nature of the vote\noutcome; each outcome is a share that adds up to 100% and each outcome is\nrelated to the alternatives. Contemporary explanatory data for each counting\narea is sourced from the themes of socio-demographics, employment, life\nsatisfaction and place. The study find that there are still strong and stark\ndivisions in the United Kingdom, defined by age, qualifications, employment and\nplace. The use of a compositional analysis approach produces challenges in\nregards to the interpretation of these models, but marginal plots are seen to\naid the interpretation somewhat.\n"
    },
    {
        "paper_id": 2001.06567,
        "authors": "Anna Denkowska and Stanis{\\l}aw Wanat",
        "title": "A tail dependence-based MST and their topological indicators in\n  modelling systemic risk in the European insurance sector",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In the present work we analyse the dynamics of indirect connections between\ninsurance companies that result from market price channels. In our analysis we\nassume that the stock quotations of insurance companies reflect market\nsentiments which constitute a very important systemic risk factor.\nInterlinkages between insurers and their dynamics have a direct impact on\nsystemic risk contagion in the insurance sector. We propose herein a new hybrid\napproach to the analysis of interlinkages dynamics based on combining the\ncopula-DCC-GARCH model and Minimum Spanning Trees (MST). Using the\ncopula-DCC-GARCH model we determine the tail dependence coefficients. Then, for\neach analysed period we construct MST based on these coefficients. The dynamics\nis analysed by means of time series of selected topological indicators of the\nMSTs in the years 2005-2019. Our empirical results show the usefulness of the\nproposed approach to the analysis of systemic risk in the insurance sector. The\ntimes series obtained from the proposed hybrid approach reflect the phenomena\noccurring on the market. The analysed MST topological indicators can be\nconsidered as systemic risk predictors.\n"
    },
    {
        "paper_id": 2001.06889,
        "authors": "Thiago C. Silva and Diego R. Amancio and Benjamin M. Tabak",
        "title": "Modeling Supply-Chain Networks with Firm-to-Firm Wire Transfers",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We study a novel economic network (supply chain) comprised of wire transfers\n(electronic payment transactions) among the universe of firms in Brazil (6.2\nmillion firms). We construct a directed and weighted network in which vertices\nrepresent cities and edges connote pairwise economic dependence between cities.\nCities (vertices) represent the collection of all firms in that location and\nlinks denote intercity wire transfers. We find a high degree of economic\nintegration among cities in the trade network, which is consistent with the\nhigh degree of specialization found across Brazilian cities. We are able to\nidentify which cities have a dominant role in the entire supply chain process\nusing centrality network measures. We find that the trade network has a\ndisassortative mixing pattern, which is consistent with the power-law shape of\nthe firm size distribution in Brazil. After the Brazilian recession in 2014, we\nfind that the disassortativity becomes even stronger as a result of the death\nof many small firms and the consequent concentration of economic flows on large\nfirms. Our results suggest that recessions have a large impact on the trade\nnetwork with meaningful and heterogeneous economic consequences across\nmunicipalities. We run econometric exercises and find that courts efficiency\nplays a dual role. From the customer perspective, it plays an important role in\nreducing contractual frictions as it increases economic transactions between\ndifferent cities. From the supplier perspective, cities that are central\nsuppliers to the supply chain seem to use courts inefficiency as a lawsuit\nbarrier from their customers.\n"
    },
    {
        "paper_id": 2001.06895,
        "authors": "Tomasz Kosmala, Randall Martyr, John Moriarty",
        "title": "Markov risk mappings and risk-sensitive optimal prediction",
        "comments": "21 pages. Improved introduction and a new result on the canonical\n  form for a Markovian family of dynamic risk measures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We formulate a probabilistic Markov property in discrete time under a dynamic\nrisk framework with minimal assumptions. This is useful for recursive solutions\nto risk-sensitive versions of dynamic optimisation problems such as optimal\nprediction, where at each stage the recursion depends on the whole future. The\nproperty holds for standard measures of risk used in practice, and is\nformulated in several equivalent versions including a representation via\nacceptance sets, a strong version, and a dual representation.\n"
    },
    {
        "paper_id": 2001.06914,
        "authors": "Ricardo T. Fernholz and Robert Fernholz",
        "title": "Permutation-Weighted Portfolios and the Efficiency of Commodity Futures\n  Markets",
        "comments": "18 pages, 8 figures, 2 tables",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  A market portfolio is a portfolio in which each asset is held at a weight\nproportional to its market value. Functionally generated portfolios are\nportfolios for which the logarithmic return relative to the market portfolio\ncan be decomposed into a function of the market weights and a process of\nlocally finite variation, and this decomposition is convenient for\ncharacterizing the long-term behavior of the portfolio. A permutation-weighted\nportfolio is a portfolio in which the assets are held at weights proportional\nto a permutation of their market values, and such a portfolio is functionally\ngenerated only for markets with two assets (except for the identity\npermutation). A reverse-weighted portfolio is a portfolio in which the asset\nwith the greatest market weight is assigned the smallest market weight, the\nasset with the second-largest weight is assigned the second-smallest, and so\nforth. Although the reverse-weighted portfolio in a market with four or more\nassets is not functionally generated, it is still possible to characterize its\nlong-term behavior using rank-based methods. This result is applied to a market\nof commodity futures, where we show that the reverse price-weighted portfolio\nsubstantially outperforms the price-weighted portfolio from 1977-2018.\n"
    },
    {
        "paper_id": 2001.0779,
        "authors": "Ulrike Malmendier and Demian Pouzo and Victoria Vanasco",
        "title": "Investor Experiences and International Capital Flows",
        "comments": "Accepted for publication in the Journal of International Economics",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We propose a novel explanation for classic international macro puzzles\nregarding capital flows and portfolio investment, which builds on modern\nmacro-finance models of experience-based belief formation. Individual\nexperiences of past macroeconomic outcomes have been shown to exert a\nlong-lasting influence on beliefs about future realizations, and to explain\ndomestic stock-market investment. We argue that experience effects can explain\nthe tendency of investors to hold an over proportional fraction of their equity\nwealth in domestic stocks (home bias), to invest in domestic equity markets in\nperiods of domestic crises (retrenchment), and to withdraw capital from foreign\nequity markets in periods of foreign crises (fickleness). Experience-based\nlearning generates additional implications regarding the strength of these\npuzzles in times of higher or lower economic activity and depending on the\ndemographic composition of market participants. We test and confirm these\npredictions in the data.\n"
    },
    {
        "paper_id": 2001.08003,
        "authors": "Armando Rungi, Loredana Fattorini, Kenan Huremovic",
        "title": "Measuring the Input Rank in Global Supply Networks",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We introduce the Input Rank as a measure of relevance of direct and indirect\nsuppliers in Global Value Chains. We conceive an intermediate input to be more\nrelevant for a downstream buyer if a decrease in that input's productivity\naffects that buyer more. In particular, in our framework, the relevance of any\ninput depends: i) on the network position of the supplier relative to the\nbuyer, ii) the patterns of intermediate inputs vs labor intensities connecting\nthe buyer and the supplier, iii) and the competitive pressures along supply\nchains. After we compute the Input Rank from both U.S. and world Input-Output\ntables, we provide useful insights on the crucial role of services inputs as\nwell as on the relatively higher relevance of domestic suppliers and suppliers\ncoming from regionally integrated partners. Finally, we test that the Input\nRank is a good predictor of vertical integration choices made by 20,489 U.S.\nparent companies controlling 154,836 subsidiaries worldwide.\n"
    },
    {
        "paper_id": 2001.0824,
        "authors": "Graham Baird, James Dodd, Lawrence Middleton",
        "title": "A growth adjusted price-earnings ratio",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The purpose of this paper is to introduce a new growth adjusted\nprice-earnings measure (GA-P/E) and assess its efficacy as measure of value and\npredictor of future stock returns. Taking inspiration from the interpretation\nof the traditional price-earnings ratio as a period of time, the new measure\ncomputes the requisite payback period whilst accounting for earnings growth.\nHaving derived the measure, we outline a number of its properties before\nconducting an extensive empirical study utilising a sorted portfolio\nmethodology. We find that the returns of the low GA-P/E stocks exceed those of\nthe high GA-P/E stocks, both in an absolute sense and also on a risk-adjusted\nbasis. Furthermore, the returns from the low GA-P/E porfolio was found to\nexceed those of the value portfolio arising from a P/E sort on the same pool of\nstocks. Finally, the returns of our GA-P/E sorted porfolios were subjected to\nanalysis by conducting regressions against the standard Fama and French risk\nfactors.\n"
    },
    {
        "paper_id": 2001.08374,
        "authors": "Zhengkun Li, Minh-Ngoc Tran, Chao Wang, Richard Gerlach and Junbin Gao",
        "title": "A Bayesian Long Short-Term Memory Model for Value at Risk and Expected\n  Shortfall Joint Forecasting",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Value-at-Risk (VaR) and Expected Shortfall (ES) are widely used in the\nfinancial sector to measure the market risk and manage the extreme market\nmovement. The recent link between the quantile score function and the\nAsymmetric Laplace density has led to a flexible likelihood-based framework for\njoint modelling of VaR and ES. It is of high interest in financial applications\nto be able to capture the underlying joint dynamics of these two quantities. We\naddress this problem by developing a hybrid model that is based on the\nAsymmetric Laplace quasi-likelihood and employs the Long Short-Term Memory\n(LSTM) time series modelling technique from Machine Learning to capture\nefficiently the underlying dynamics of VaR and ES. We refer to this model as\nLSTM-AL. We adopt the adaptive Markov chain Monte Carlo (MCMC) algorithm for\nBayesian inference in the LSTM-AL model. Empirical results show that the\nproposed LSTM-AL model can improve the VaR and ES forecasting accuracy over a\nrange of well-established competing models.\n"
    },
    {
        "paper_id": 2001.08376,
        "authors": "Asier Minondo",
        "title": "Comments are welcome",
        "comments": "19 pages, 2 figures, 7 tables",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Scholars present their new research at seminars and conferences, and send\ndrafts to peers, hoping to receive comments and suggestions that will improve\nthe quality of their work. Using a dataset of papers published in economics\njournals, this article measures how much peers' individual and collective\ncomments improve the quality of research. Controlling for the quality of the\nresearch idea and author, I find that a one standard deviation increase in the\nnumber of peers' individual and collective comments increases the quality of\nthe journal in which the research is published by 47%.\n"
    },
    {
        "paper_id": 2001.08432,
        "authors": "Mario Coccia",
        "title": "Effects of the institutional change based on democratization on origin\n  and diffusion of technological innovation",
        "comments": "41 pages, 7 tables, 8 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Political systems shape institutions and govern institutional change\nsupporting economic performance, production and diffusion of technological\ninnovation. This study shows, using global data of countries, that\ninstitutional change, based on a progressive democratization of countries, is a\ndriving force of inventions, adoption and diffusion of innovations in society.\nThe relation between technological innovation and level of democracy can be\nexplained with following factors: higher economic freedom in society, effective\nregulation, higher economic and political stability, higher investments in R&D\nand higher education, good economic governance and higher level of education\nsystem for training high-skilled human resources. Overall, then, the positive\nassociations between institutional change, based on a process of\ndemocratization, and paths of technological innovation can sustain best\npractices of political economy for the development of economies in the presence\nof globalization and geographical expansion of markets.\n"
    },
    {
        "paper_id": 2001.08442,
        "authors": "Ioane Muni Toke, Nakahiro Yoshida",
        "title": "Marked point processes and intensity ratios for limit order book\n  modeling",
        "comments": "38 pages, 7 figures, 6 tables",
        "journal-ref": "Japanese Journal of Statistics and Data Science (2022)",
        "doi": "10.1007/s42081-021-00137-9",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper extends the analysis of Muni Toke and Yoshida (2020) to the case\nof marked point processes. We consider multiple marked point processes with\nintensities defined by three multiplicative components, namely a common\nbaseline intensity, a state-dependent component specific to each process, and a\nstate-dependent component specific to each mark within each process. We show\nthat for specific mark distributions, this model is a combination of the ratio\nmodels defined in Muni Toke and Yoshida (2020). We prove convergence results\nfor the quasi-maximum and quasi-Bayesian likelihood estimators of this model\nand provide numerical illustrations of the asymptotic variances. We use these\nratio processes in order to model transactions occuring in a limit order book.\nModel flexibility allows us to investigate both state-dependency (emphasizing\nthe role of imbalance and spread as significant signals) and clustering.\nCalibration, model selection and prediction results are reported for\nhigh-frequency trading data on multiple stocks traded on Euronext Paris. We\nshow that the marked ratio model outperforms other intensity-based methods\n(such as \"pure\" Hawkes-based methods) in predicting the sign and aggressiveness\nof market orders on financial markets.\n"
    },
    {
        "paper_id": 2001.08615,
        "authors": "Alberto Tejero, Victor Rodriguez-Doncel and Ivan Pau",
        "title": "Knowledge Graphs for Innovation Ecosystems",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Innovation ecosystems can be naturally described as a collection of networked\nentities, such as experts, institutions, projects, technologies and products.\nRepresenting in a machine-readable form these entities and their relations is\nnot entirely attainable, due to the existence of abstract concepts such as\nknowledge and due to the confidential, non-public nature of this information,\nbut even its partial depiction is of strong interest. The representation of\ninnovation ecosystems incarnated as knowledge graphs would enable the\ngeneration of reports with new insights, the execution of advanced data\nanalysis tasks. An ontology to capture the essential entities and relations is\npresented, as well as the description of data sources, which can be used to\npopulate innovation knowledge graphs. Finally, the application case of the\nUniversidad Politecnica de Madrid is presented, as well as an insight of future\napplications.\n"
    },
    {
        "paper_id": 2001.08865,
        "authors": "Abootaleb Shirvani and Frank J. Fabozzi",
        "title": "Choosing the Right Return Distribution and the Excess Volatility Puzzle",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Proponents of behavioral finance have identified several \"puzzles\" in the\nmarket that are inconsistent with rational finance theory. One such puzzle is\nthe \"excess volatility puzzle\". Changes in equity prices are too large given\nchanges in the fundamentals that are expected to change equity prices. In this\npaper, we offer a resolution to the excess volatility puzzle within the context\nof rational finance. We empirically show that market inefficiency attributable\nto the volatility of excess return across time is caused by fitting an improper\ndistribution to the historical returns. Our results indicate that the variation\nof gross excess returns is attributable to poorly fitting the tail of the\nreturn distribution and that the puzzle disappears by employing a more\nappropriate distribution for the return data. The new distribution that we\nintroduce in this paper that better fits the historical return distribution of\nstocks explains the excess volatility in the market and thereby explains the\nvolatility puzzle. Failing to estimate the historical returns using the proper\ndistribution is only one possible explanation for the existence of the\nvolatility puzzle. However, it offers statistical models within the rational\nfinance framework which can be used without relying on behavioral finance\nassumptions when searching for an explanation for the volatility puzzle.\n"
    },
    {
        "paper_id": 2001.08906,
        "authors": "Roberto Daluiso, Emanuele Nastasi, Andrea Pallavicini and Giulio\n  Sartorelli",
        "title": "Pricing commodity swing options",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In commodity and energy markets swing options allow the buyer to hedge\nagainst futures price fluctuations and to select its preferred delivery\nstrategy within daily or periodic constraints, possibly fixed by observing\nquoted futures contracts. In this paper we focus on the natural gas market and\nwe present a dynamical model for commodity futures prices able to calibrate\nliquid market quotes and to imply the volatility smile for futures contracts\nwith different delivery periods. We implement the numerical problem by means of\na least-square Monte Carlo simulation and we investigate alternative approaches\nbased on reinforcement learning algorithms.\n"
    },
    {
        "paper_id": 2001.08911,
        "authors": "Sebastien Valeyre",
        "title": "Refined model of the covariance/correlation matrix between securities",
        "comments": "Universit\\'e Paris 13, Phd dissertation, May 2019, in French",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  A new methodology has been introduced to clean the correlation matrix of\nsingle stocks returns based on a constrained principal component analysis using\nfinancial data. Portfolios were introduced, namely \"Fundamental Maximum\nVariance Portfolios\", to capture in an optimal way the risks defined by\nfinancial criteria (\"Book\", \"Capitalization\", etc.). The constrained\neigenvectors of the correlation matrix, which are the linear combination of\nthese portfolios, are then analyzed. Thanks to this methodology, several\nstylized patterns of the matrix were identified: i) the increase of the first\neigenvalue with a time scale from 1 minute to several months seems to follow\nthe same law for all the significant eigenvalues with 2 regimes; ii) a\nuniversal law seems to govern the weights of all the \"Maximum variance\"\nportfolios, so according to that law, the optimal weights should be\nproportional to the ranking based on the financial studied criteria; iii) the\nvolatility of the volatility of the \"Maximum Variance\" portfolios, which are\nnot orthogonal, could be enough to explain a large part of the diffusion of the\ncorrelation matrix; iv) the leverage effect (increase of the first eigenvalue\nwith the decline of the stock market) occurs only for the first mode and cannot\nbe generalized for other factors of risk. The leverage effect on the beta,\nwhich is the sensitivity of stocks with the market mode, makes variable the\nweights of the first eigenvector.\n"
    },
    {
        "paper_id": 2001.08926,
        "authors": "Dingju Zhu",
        "title": "Big Data based Research on Mechanisms of Sharing Economy Restructuring\n  the World",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Many researches have discussed the phenomenon and definition of sharing\neconomy, but an understanding of sharing economy's reconstructions of the world\nremains elusive. We illustrate the mechanism of sharing economy's\nreconstructions of the world in detail based on big data including the\nmechanism of sharing economy's reconstructions of society, time and space,\nusers, industry, and self-reconstruction in the future, which is very important\nfor society to make full use of the reconstruction opportunity to upgrade our\nworld through sharing economy. On the one hand, we established the mechanisms\nfor sharing economy rebuilding society, industry, space-time, and users through\nqualitative analyses, and on the other hand, we demonstrated the rationality of\nthe mechanisms through quantitative analyses of big data.\n"
    },
    {
        "paper_id": 2001.08935,
        "authors": "Nikolay Khabarov, Alexey Smirnov, Michael Obersteiner",
        "title": "Social Cost of Carbon: What Do the Numbers Really Mean?",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Social cost of carbon (SCC) is estimated by integrated assessment models\n(IAM) and is widely used by government agencies to value climate policy\nimpacts. While there is an ongoing debate about obtained numerical estimates\nand related uncertainties, little attention has been paid so far to the SCC\ncalculation method itself.\n  This work attempts to fill the gap by providing theoretical background and\neconomic interpretation of the SCC calculation approach implemented in the\nopen-source IAM DICE (Dynamic Integrated model of Climate and the Economy). Our\nanalysis indicates that the present calculation method provides an\napproximation that might work pretty well in some cases, while in the other\ncases the estimated value substantially (by the factor of four) deviates from\nthe \"true\" value. This deviation stems from the inability of the present\ncalculation method to catch the linkages between two key IAM's components --\ncomplex interconnected systems -- climate and economy, both influenced by\nemission abatement policies. Within the modeling framework of DICE, the\npresently estimated SCC valuates policy-uncontrolled emissions against\neconomically unjustified consumption, which makes it irrelevant for application\nin climate-economic policies and, therefore, calls for a replacement by a more\nappropriate indicator.\n  An apparent SCC alternative, which can be employed for policy formulation is\nthe direct output of the DICE model -- the socially optimal marginal abatement\ncost (SMAC), which corresponds to technological possibilities at optimal level\nof carbon emissions abatement. In policy making, because of the previously\nemployed implicit approximation, great attention needs to be paid to the use of\nSCC estimates obtained earlier.\n"
    },
    {
        "paper_id": 2001.08979,
        "authors": "Amit Tewari",
        "title": "Forecasting NIFTY 50 benchmark Index using Seasonal ARIMA time series\n  models",
        "comments": null,
        "journal-ref": null,
        "doi": "10.13140/RG.2.2.10332.95364",
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  This paper analyses how Time Series Analysis techniques can be applied to\ncapture movement of an exchange traded index in a stock market. Specifically,\nSeasonal Auto Regressive Integrated Moving Average (SARIMA) class of models is\napplied to capture the movement of Nifty 50 index which is one of the most\nactively exchange traded contracts globally [1]. A total of 729 model parameter\ncombinations were evaluated and the most appropriate selected for making the\nfinal forecast based on AIC criteria [8]. NIFTY 50 can be used for a variety of\npurposes such as benchmarking fund portfolios, launching of index funds,\nexchange traded funds (ETFs) and structured products. The index tracks the\nbehaviour of a portfolio of blue chip companies, the largest and most liquid\nIndian securities and can be regarded as a true reflection of the Indian stock\nmarket [2].\n"
    },
    {
        "paper_id": 2001.09151,
        "authors": "Tai-Yu Ma, Sylvain Klein",
        "title": "Integrated ridesharing services with chance-constrained dynamic pricing\n  and demand learning",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The design of integrated mobility-on-demand services requires jointly\nconsidering the interactions between traveler choice behavior and operators'\noperation policies to design a financially sustainable pricing scheme. However,\nmost existing studies focus on the supply side perspective, disregarding the\nimpact of customer choice behavior in the presence of co-existing transport\nnetworks. We propose a modeling framework for dynamic integrated\nmobility-on-demand service operation policy evaluation with two service\noptions: door-to-door rideshare and rideshare with transit transfer. A new\nconstrained dynamic pricing model is proposed to maximize operator profit,\ntaking into account the correlated structure of different modes of transport.\nUser willingness to pay is considered as a stochastic constraint, resulting in\na more realistic ticket price setting while maximizing operator profit. Unlike\nmost studies, which assume that travel demand is known, we propose a demand\nlearning process to calibrate customer demand over time based on customers'\nhistorical purchase data. We evaluate the proposed methodology through\nsimulations under different scenarios on a test network by considering the\ninteractions of supply and demand in a multimodal market. Different scenarios\nin terms of customer arrival intensity, vehicle capacity, and the variance of\nuser willingness to pay are tested. Results suggest that the proposed\nchance-constrained assortment price optimization model allows increasing\noperator profit while keeping the proposed ticket prices acceptable.\n"
    },
    {
        "paper_id": 2001.09404,
        "authors": "Nick James, Max Menzies, Jennifer Chan",
        "title": "Semi-metric portfolio optimization: a new algorithm reducing\n  simultaneous asset shocks",
        "comments": "Accepted manuscript. Substantial additions since v2. Equal\n  contribution from first two authors",
        "journal-ref": "Econometrics 11, 8 (2023)",
        "doi": "10.3390/econometrics11010008",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper proposes a new method for financial portfolio optimization based\non reducing simultaneous asset shocks across a collection of assets. This may\nbe understood as an alternative approach to risk reduction in a portfolio based\non a new mathematical quantity. First, we apply recently introduced\nsemi-metrics between finite sets to determine the distance between time series'\nstructural breaks. Then, we build on the classical portfolio optimization\ntheory of Markowitz and use this distance between asset structural breaks for\nour penalty function, rather than portfolio variance. Our experiments are\npromising: on synthetic data, we show that our proposed method does indeed\ndiversify among time series with highly similar structural breaks and enjoys\nadvantages over existing metrics between sets. On real data, experiments\nillustrate that our proposed optimization method performs well relative to nine\nother commonly used options, producing the second-highest returns, the lowest\nvolatility, and second-lowest drawdown. The main implication for this method in\nportfolio management is reducing simultaneous asset shocks and potentially\nsharp associated drawdowns during periods of highly similar structural breaks,\nsuch as a market crisis. Our method adds to a considerable literature of\nportfolio optimization techniques in econometrics and could complement these\nvia portfolio averaging.\n"
    },
    {
        "paper_id": 2001.09443,
        "authors": "Gechun Liang, Xingchun Wang",
        "title": "Pricing vulnerable options in a hybrid credit risk model driven by\n  Heston-Nandi GARCH processes",
        "comments": "30 pages, 6 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper proposes a hybrid credit risk model, in closed form, to price\nvulnerable options with stochastic volatility. The distinctive features of the\nmodel are threefold. First, both the underlying and the option issuer's assets\nfollow the Heston-Nandi GARCH model with their conditional variance being\nreadily estimated and implemented solely on the basis of the observable prices\nin the market. Second, the model incorporates both idiosyncratic and systematic\nrisks into the asset dynamics of the underlying and the option issuer, as well\nas the intensity process. Finally, the explicit pricing formula of vulnerable\noptions enables us to undertake the comparative statistics analysis.\n"
    },
    {
        "paper_id": 2001.09446,
        "authors": "A. Jakovac",
        "title": "Finance from the viewpoint of physics",
        "comments": "34 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this note we review the basic mathematical ideas used in finance in the\nlanguage of modern physics. We focus on discrete time formalism, derive path\nintegral and Green's function formulas for pricing. We also discuss various\nrisk mitigation methods.\n"
    },
    {
        "paper_id": 2001.09579,
        "authors": "Dan Pirjol",
        "title": "Asymptotic expansion for the Hartman-Watson distribution",
        "comments": "19 pages, 5 figures. Expanded version of paper published in\n  Methodology and Computing in Applied Probability (2020). Added an explicit\n  treatment of the boundary case rho=1, and an improved error bound",
        "journal-ref": null,
        "doi": "10.1007/s11009-020-09827-5",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The Hartman-Watson distribution with density $f_r(t)$ is a probability\ndistribution defined on $t \\geq 0$ which appears in several problems of applied\nprobability. The density of this distribution is expressed in terms of an\nintegral $\\theta(r,t)$ which is difficult to evaluate numerically for small\n$t\\to 0$. Using saddle point methods, we obtain the first two terms of the\n$t\\to 0$ expansion of $\\theta(\\rho/t,t)$ at fixed $\\rho >0$. An error bound is\nobtained by numerical estimates of the integrand, which is furthermore uniform\nin $\\rho$. As an application we obtain the leading asymptotics of the density\nof the time average of the geometric Brownian motion as $t\\to 0$. This has the\nform $\\mathbb{P}(\\frac{1}{t} \\int_0^t e^{2(B_s+\\mu s)} ds \\in da) = (2\\pi\nt)^{-1/2} g(a,\\mu) e^{-\\frac{1}{t} J(a)} (1 + O(t))$, with an exponent $J(a)$\nwhich reproduces the known result obtained previously using Large Deviations\ntheory.\n"
    },
    {
        "paper_id": 2001.09664,
        "authors": "Dimitrios Tsiotas, Labros Sdrolias, Dimitrios Belias",
        "title": "The network paradigm as a modeling tool in regional economy: the case of\n  interregional commuting in Greece",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Network Science is an emerging discipline using the network paradigm to model\ncommunication systems as pair-sets of interconnected nodes and their linkages\n(edges). This paper applies this paradigm to study an interacting system in\nregional economy consisting of daily road transportation flows for labor\npurposes, the so-called commuting phenomenon. In particular, the commuting\nsystem in Greece including 39 non-insular prefectures is modeled into a complex\nnetwork and it is studied using measures and methods of complex network\nanalysis and empirical techniques. The study aims to detect the structural\ncharacteristics of the Greek interregional commuting network (GCN) and to\ninterpret how this network is related to the regional development. The analysis\nhighlights the effect of the spatial constraints in the structure of the GCN,\nit provides insights about the major road transport projects constructed the\nlast decade, and it outlines a populationcontrolled (gravity) pattern of\ncommuting, illustrating that high-populated regions attract larger volumes of\nthe commuting activity, which consequently affects their productivity. Overall,\nthis paper highlights the effectiveness of complex network analysis in the\nmodeling of systems of regional economy, such as the systems of spatial\ninteraction and the transportation networks, and it promotes the use of the\nnetwork paradigm to the regional research.\n"
    },
    {
        "paper_id": 2001.09666,
        "authors": "Serafeim Polyzos and Dimitrios Tsiotas",
        "title": "Regional airports in Greece, their characteristics and their importance\n  for the local economic development",
        "comments": "in Greek",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Technological developments worldwide are contributing to the improvement of\ntransport infrastructures and they are helping to reduce the overall transport\ncosts. At the same time, such developments along with the reduction in\ntransport costs are affecting the spatial interdependence between the regions\nand countries, a fact inducing significant effects on their economies and, in\ngeneral, on their growth-rates. A specific class of transport infrastructures\ncontributing significantly to overcoming the spatial constraints is the\nairtransport infrastructures. Nowadays, the importance of air-transport\ninfrastructures in the economic development is determinative, especially for\nthe geographically isolated regions, such as for the island regions of Greece.\nWithin this context, this paper studies the Greek airports and particularly the\nevolution of their overall transportation imprint, their geographical\ndistribution, and the volume of the transport activity of each airport. Also,\nit discusses, in a broad context, the seasonality of the Greek airport\nactivity, the importance of the airports for the local and regional\ndevelopment, and it formulates general conclusions.\n"
    },
    {
        "paper_id": 2001.09769,
        "authors": "Sidra Mehtab and Jaydip Sen",
        "title": "Stock Price Prediction Using Convolutional Neural Networks on a\n  Multivariate Timeseries",
        "comments": "The paper will be published in the Proceedings of the \"National\n  Conference on Machine Learning and Artificial Intelligence\" which will be\n  organized in New Delhi, India, during February 1 - 3, 2020. It contains 7\n  pages, 3 figures, and 19 tables. arXiv admin note: substantial text overlap\n  with arXiv:1912.07700",
        "journal-ref": null,
        "doi": "10.36227/techrxiv.15088734.v1",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Prediction of future movement of stock prices has been a subject matter of\nmany research work. In this work, we propose a hybrid approach for stock price\nprediction using machine learning and deep learning-based methods. We select\nthe NIFTY 50 index values of the National Stock Exchange of India, over a\nperiod of four years, from January 2015 till December 2019. Based on the NIFTY\ndata during the said period, we build various predictive models using machine\nlearning approaches, and then use those models to predict the Close value of\nNIFTY 50 for the year 2019, with a forecast horizon of one week. For predicting\nthe NIFTY index movement patterns, we use a number of classification methods,\nwhile for forecasting the actual Close values of NIFTY index, various\nregression models are built. We, then, augment our predictive power of the\nmodels by building a deep learning-based regression model using Convolutional\nNeural Network with a walk-forward validation. The CNN model is fine-tuned for\nits parameters so that the validation loss stabilizes with increasing number of\niterations, and the training and validation accuracies converge. We exploit the\npower of CNN in forecasting the future NIFTY index values using three\napproaches which differ in number of variables used in forecasting, number of\nsub-models used in the overall models and, size of the input data for training\nthe models. Extensive results are presented on various metrics for all\nclassification and regression models. The results clearly indicate that\nCNN-based multivariate forecasting model is the most effective and accurate in\npredicting the movement of NIFTY index values with a weekly forecast horizon.\n"
    },
    {
        "paper_id": 2001.0985,
        "authors": "Dan Pirjol, Lingjiong Zhu",
        "title": "Asymptotics of the time-discretized log-normal SABR model: The implied\n  volatility surface",
        "comments": "40 pages, 2 figures, 3 tables",
        "journal-ref": "Probability in the Engineering and Informational Sciences 2021,\n  Vol. 35, No. 4, 942-974",
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We propose a novel time discretization for the log-normal SABR model which is\na popular stochastic volatility model that is widely used in financial\npractice. Our time discretization is a variant of the Euler-Maruyama scheme. We\nstudy its asymptotic properties in the limit of a large number of time steps\nunder a certain asymptotic regime which includes the case of finite maturity,\nsmall vol-of-vol and large initial volatility with fixed product of vol-of-vol\nand initial volatility. We derive an almost sure limit and a large deviations\nresult for the log-asset price in the limit of large number of time steps. We\nderive an exact representation of the implied volatility surface for arbitrary\nmaturity and strike in this regime. Using this representation we obtain\nanalytical expansions of the implied volatility for small maturity and extreme\nstrikes, which reproduce at leading order known asymptotic results for the\ncontinuous time model.\n"
    },
    {
        "paper_id": 2001.10108,
        "authors": "Julio Backhoff Veraguas and A. Max Reppen and Ludovic Tangpi",
        "title": "Stochastic control of optimized certainty equivalents",
        "comments": "30 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Optimized certainty equivalents (OCEs) is a family of risk measures widely\nused by both practitioners and academics. This is mostly due to its\ntractability and the fact that it encompasses important examples, including\nentropic risk measures and average value at risk. In this work we consider\nstochastic optimal control problems where the objective criterion is given by\nan OCE risk measure, or put in other words, a risk minimization problem for\ncontrolled diffusions. A major difficulty arises since OCEs are often time\ninconsistent. Nevertheless, via an enlargement of state space we achieve a\nsubstitute of sorts for time consistency in fair generality. This allows us to\nderive a dynamic programming principle and thus recover central results of\n(risk-neutral) stochastic control theory. In particular, we show that the value\nof our risk minimization problem can be characterized via the viscosity\nsolution of a Hamilton--Jacobi--Bellman--Issacs equation. We further establish\nthe uniqueness of the latter under suitable technical conditions.\n"
    },
    {
        "paper_id": 2001.10173,
        "authors": "Si Ying Tan, Araz Taeihagh",
        "title": "Smart City Governance in Developing Countries: A Systematic Literature\n  Review",
        "comments": null,
        "journal-ref": "Sustainability 2020, 12(3), 899",
        "doi": "10.3390/su12030899",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Smart cities that make broad use of digital technologies have been touted as\npossible solutions for the population pressures faced by many cities in\ndeveloping countries and may help meet the rising demand for services and\ninfrastructure. Nevertheless, the high financial cost involved in\ninfrastructure maintenance, the substantial size of the informal economies, and\nvarious governance challenges are curtailing government idealism regarding\nsmart cities. This review examines the state of smart city development in\ndeveloping countries, which includes understanding the conceptualisations,\nmotivations, and unique drivers behind (and barriers to) smarty city\ndevelopment. A total of 56 studies were identified from a systematic literature\nreview from an initial pool of 3928 social sciences literature identified from\ntwo academic databases. Data were analysed using thematic synthesis and\nthematic analysis. The review found that technology-enabled smart cities in\ndeveloping countries can only be realised when concurrent socioeconomic, human,\nlegal, and regulatory reforms are instituted. Governments need to step up their\nefforts to fulfil the basic infrastructure needs of citizens, raise more\nrevenue, construct clear regulatory frameworks to mitigate the technological\nrisks involved, develop human capital, ensure digital inclusivity, and promote\nenvironmental sustainability. A supportive ecosystem that encourages citizen\nparticipation, nurtures start-ups, and promotes public-private partnerships\nneeds to be created to realise their smart city vision.\n"
    },
    {
        "paper_id": 2001.10278,
        "authors": "Sang Il Lee",
        "title": "Hyperparameter Optimization for Forecasting Stock Returns",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In recent years, hyperparameter optimization (HPO) has become an increasingly\nimportant issue in the field of machine learning for the development of more\naccurate forecasting models. In this study, we explore the potential of HPO in\nmodeling stock returns using a deep neural network (DNN). The potential of this\napproach was evaluated using technical indicators and fundamentals examined\nbased on the effect the regularization of dropouts and batch normalization for\nall input data. We found that the model using technical indicators and dropout\nregularization significantly outperforms three other models, showing a positive\npredictability of 0.53% in-sample and 1.11% out-of-sample, thereby indicating\nthe possibility of beating the historical average. We also demonstrate the\nstability of the model in terms of the changes in its feature importance over\ntime.\n"
    },
    {
        "paper_id": 2001.10384,
        "authors": "Peng Liu",
        "title": "Change of measure under the hard-to-borrow model",
        "comments": "7 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  As the Securities and Exchange Commission(SEC) has implemented a new\nregulation on short-sellings, short-sellers are required to repurchase stocks\nonce the clearing risk rises to a certain level. Avellaneda and Lipkin proposed\na fully coupled SDE system to describe the mechanism which is referred as\nHard-To-Borrow(HTB) models. Guiyuan Ma obtained the PDE system for both\nAmerican and European options. There is a technical error in Guiyuan Ma where\ntwo correlated Brownian motion should be converted before change of measure. In\nthis paper, I will provide supplement conditions.\n"
    },
    {
        "paper_id": 2001.10393,
        "authors": "Despoina Makariou, Pauline Barrieu, Yining Chen",
        "title": "A random forest based approach for predicting spreads in the primary\n  catastrophe bond market",
        "comments": "34 pages, 8 figures, 7 tables",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We introduce a random forest approach to enable spreads' prediction in the\nprimary catastrophe bond market. We investigate whether all information\nprovided to investors in the offering circular prior to a new issuance is\nequally important in predicting its spread. The whole population of non-life\ncatastrophe bonds issued from December 2009 to May 2018 is used. The random\nforest shows an impressive predictive power on unseen primary catastrophe bond\ndata explaining 93% of the total variability. For comparison, linear\nregression, our benchmark model, has inferior predictive performance explaining\nonly 47% of the total variability. All details provided in the offering\ncircular are predictive of spread but in a varying degree. The stability of the\nresults is studied. The usage of random forest can speed up investment\ndecisions in the catastrophe bond industry.\n"
    },
    {
        "paper_id": 2001.10432,
        "authors": "Claudiu Albulescu (CRIEF)",
        "title": "Investment behavior and firms' financial performance: A comparative\n  analysis using firm-level data from the wine industry",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper assesses the role of financial performance in explaining firms'\ninvestment dynamics in the wine industry from the three European Union (EU)\nlargest producers. The wine sector deserves special attention to investigate\nfirms' investment behavior given the high competition imposed by the\nlatecomers. More precisely, we investigate how the capitalization, liquidity\nand profitability influence the investment dynamics using firm-level data from\nthe wine industry from France (331 firms), Italy (335) firms and Spain (442)\nfirms. We use data from 2007 to 2014, drawing a comparison between these\ncountries, and relying on difference-and system-GMM estimators. Specifically,\nthe impact of profitability is positive and significant, while the\ncapitalization has a significant and negative impact on the investment dynamics\nonly in France and Spain. The influence of the liquidity ratio is negative and\nsignificant only in the case of Spain. Therefore, we notice different\ninvestment strategies for wine companies located in the largest producer\ncountries. It appears that these findings are in general robust to different\nspecifications of liquidity and profitability ratios, and to the different\nestimators we use.\n"
    },
    {
        "paper_id": 2001.10488,
        "authors": "Nassim Nicholas Taleb",
        "title": "Statistical Consequences of Fat Tails: Real World Preasymptotics,\n  Epistemology, and Applications",
        "comments": "Second Revised Edition, 2022",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The monograph investigates the misapplication of conventional statistical\ntechniques to fat tailed distributions and looks for remedies, when possible.\n  Switching from thin tailed to fat tailed distributions requires more than\n\"changing the color of the dress\". Traditional asymptotics deal mainly with\neither n=1 or $n=\\infty$, and the real world is in between, under of the \"laws\nof the medium numbers\" --which vary widely across specific distributions. Both\nthe law of large numbers and the generalized central limit mechanisms operate\nin highly idiosyncratic ways outside the standard Gaussian or Levy-Stable\nbasins of convergence.\n  A few examples:\n  + The sample mean is rarely in line with the population mean, with effect on\n\"naive empiricism\", but can be sometimes be estimated via parametric methods.\n  + The \"empirical distribution\" is rarely empirical.\n  + Parameter uncertainty has compounding effects on statistical metrics.\n  + Dimension reduction (principal components) fails.\n  + Inequality estimators (GINI or quantile contributions) are not additive and\nproduce wrong results.\n  + Many \"biases\" found in psychology become entirely rational under more\nsophisticated probability distributions\n  + Most of the failures of financial economics, econometrics, and behavioral\neconomics can be attributed to using the wrong distributions.\n  This book, the first volume of the Technical Incerto, weaves a narrative\naround published journal articles.\n"
    },
    {
        "paper_id": 2001.10561,
        "authors": "Asier Minondo",
        "title": "Who presents and where? An analysis of research seminars in US economics\n  departments",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Using a large dataset of research seminars held at US economics departments\nin 2018, I explore the factors that determine who is invited to present at a\nresearch seminar and whether the invitation is accepted. I find that\nhigh-quality scholars have a higher probability of being invited than\nlow-quality scholars, and researchers are more likely to accept an invitation\nif it is issued by a top economics department. The probability of being invited\nincreases with the size of the host department. Young and low-quality scholars\nhave a higher probability of accepting an invitation. The distance between the\nhost department and invited scholar reduces the probability of being invited\nand accepting the invitation. Female scholars do not have a lower probability\nof being invited to give a research seminar than men.\n"
    },
    {
        "paper_id": 2001.11012,
        "authors": "Alessandro Gnoatto, Nicole Seiffert",
        "title": "Cross Currency Valuation and Hedging in the Multiple Curve Framework",
        "comments": "42 pages",
        "journal-ref": "SIAM Journal on Financial Mathematics (2021) forthcoming",
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We generalize the results of Bielecki and Rutkowski (2015) on funding and\ncollateralization to a multi-currency framework and link their results with\nthose of Piterbarg (2012), Moreni and Pallavicini (2017), and Fujii et al.\n(2010b).\n  In doing this, we provide a complete study of absence of arbitrage in a\nmulti-currency market where, in each single monetary area, multiple interest\nrates coexist. We first characterize absence of arbitrage in the case without\ncollateral.\n  After that we study collateralization schemes in a very general situation:\nthe cash flows of the contingent claim and those associated to the collateral\nagreement can be specified in any currency. We study both segregation and\nrehypothecation and allow for cash and risky collateral in arbitrary currency\nspecifications. Absence of arbitrage and pricing in the presence of collateral\nare discussed under all possible combinations of conventions.\n  Our work provides a reference for the analysis of wealth dynamics, we also\nprovide valuation formulas that are a useful foundation for cross-currency\ncurve construction techniques. Our framework provides also a solid foundation\nfor the construction of multi-currency simulation models for the generation of\nexposure profiles in the context of xVA calculations.\n"
    },
    {
        "paper_id": 2001.11214,
        "authors": "Christian Bongiorno, Damien Challet",
        "title": "Nonparametric sign prediction of high-dimensional correlation matrix\n  coefficients",
        "comments": null,
        "journal-ref": null,
        "doi": "10.1209/0295-5075/133/48001",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We introduce a method to predict which correlation matrix coefficients are\nlikely to change their signs in the future in the high-dimensional regime, i.e.\nwhen the number of features is larger than the number of samples per feature.\nThe stability of correlation signs, two-by-two relationships, is found to\ndepend on three-by-three relationships inspired by Heider social cohesion\ntheory in this regime. We apply our method to US and Hong Kong equities\nhistorical data to illustrate how the structure of correlation matrices\ninfluences the stability of the sign of its coefficients.\n"
    },
    {
        "paper_id": 2001.11247,
        "authors": "Thomas Deschatre and Joseph Mikael",
        "title": "Deep combinatorial optimisation for optimal stopping time problems :\n  application to swing options pricing",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  A new method for stochastic control based on neural networks and using\nrandomisation of discrete random variables is proposed and applied to optimal\nstopping time problems. The method models directly the policy and does not need\nthe derivation of a dynamic programming principle nor a backward stochastic\ndifferential equation. Unlike continuous optimization where automatic\ndifferentiation is used directly, we propose a likelihood ratio method for\ngradient computation. Numerical tests are done on the pricing of American and\nswing options. The proposed algorithm succeeds in pricing high dimensional\nAmerican and swing options in a reasonable computation time, which is not\npossible with classical algorithms.\n"
    },
    {
        "paper_id": 2001.11249,
        "authors": "R\\\"udiger Frey, Kevin Kurt, Camilla Damian",
        "title": "How Safe are European Safe Bonds? An Analysis from the Perspective of\n  Modern Portfolio Credit Risk Models",
        "comments": "32 pages",
        "journal-ref": null,
        "doi": "10.1016/j.jbankfin.2020.105939",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Several proposals for the reform of the euro area advocate the creation of a\nmarket in synthetic securities backed by portfolios of sovereign bonds. Most\ndebated are the so-called European Safe Bonds or ESBies proposed by\nBrunnermeier, Langfield, Pagano,Reis, Van Nieuwerburgh and Vayanos (2017). The\npotential benefits of ESBies and other bond-backed securities hinge on the\nassertion that these products are really safe. In this paper we provide a\ncomprehensive quantitative study of the risks associated with ESBies and\nrelated products, using an affine credit risk model with regime switching as\nvehicle for our analysis. We discuss a recent proposal of Standard and Poors\nfor the rating of ESBies, we analyse the impact of model parameters and\nattachment points on the size and the volatility of the credit spread of ESBies\nand we consider several approaches to assess the market risk of ESBies.\nMoreover, we compare ESBies to synthetic securities created by pooling the\nsenior tranche of national bonds as suggested by Leandro and Zettelmeyer(2019).\nThe paper concludes with a brief discussion of the policy implications from our\nanalysis.\n"
    },
    {
        "paper_id": 2001.11275,
        "authors": "Amir T. Payandeh Najafabadi, Marjan Qazvini, Reza Ofoghi",
        "title": "The Impact of Oil and Gold Prices Shock on Tehran Stock Exchange: A\n  Copula Approach",
        "comments": null,
        "journal-ref": "Iranian Journal of Economic Studies Vol. 1, No. 2, Fall 2012,\n  23-47",
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  There are several researches that deal with the behavior of SEs and their\nrelationships with different economical factors. These range from papers\ndealing with this subject through econometrical procedures to statistical\nmethods known as copula. This article considers the impact of oil and gold\nprice on Tehran Stock Exchange market (TSE). Oil and gold are two factors that\nare essential for the economy of Iran and their price are determined in the\nglobal market. The model used in this study is ARIMA-Copula. We used data from\nJanuary 1998 to January 2011 as training data to find the appropriate model.\nThe cross validation of model is measured by data from January 2011 to June\n2011. We conclude that: (i) there is no significant direct relationship between\ngold price and the TSE index, but the TSE is indirectly influenced by gold\nprice through other factors such as oil; and (ii) the TSE is not independent of\nthe volatility in oil price and Clayton copula can describe such dependence\nstructure between TSE and the oil price. Based on the property of Clayton\ncopula, which has lower tail dependency, as the oil price drops, stock index\nfalls. This means that decrease in oil price has an adverse effect on Iranian\neconomy.\n"
    },
    {
        "paper_id": 2001.11301,
        "authors": "Nicole B\\\"auerle and Gregor Leimcke",
        "title": "Robust Optimal Investment and Reinsurance Problems with Learning",
        "comments": null,
        "journal-ref": "Scandinavian Actuarial Journal Volume 2021 (2), pp. 82-109, 2021",
        "doi": "10.1080/03461238.2020.1806917",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper we consider an optimal investment and reinsurance problem with\npartially unknown model parameters which are allowed to be learned. The model\nincludes multiple business lines and dependence between them. The aim is to\nmaximize the expected exponential utility of terminal wealth which is shown to\nimply a robust approach. We can solve this problem using a generalized HJB\nequation where derivatives are replaced by generalized Clarke gradients. The\noptimal investment strategy can be determined explicitly and the optimal\nreinsurance strategy is given in terms of the solution of an equation. Since\nthis equation is hard to solve, we derive bounds for the optimal reinsurance\nstrategy via comparison arguments.\n"
    },
    {
        "paper_id": 2001.11395,
        "authors": "Salvatore Tirone, Maddalena Ghio, Giulia Livieri, Vittorio\n  Giovannetti, Stefano Marmi",
        "title": "Kelly Betting with Quantum Payoff: a continuous variable approach",
        "comments": "18 pages, 8 figures",
        "journal-ref": "Quantum 5, 545 (2021)",
        "doi": "10.22331/q-2021-09-21-545",
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  The main purpose of this study is to introduce a semi-classical model\ndescribing betting scenarios in which, at variance with conventional\napproaches, the payoff of the gambler is encoded into the internal degrees of\nfreedom of a quantum memory element. In our scheme, we assume that the invested\ncapital is explicitly associated with the quantum analog of the free-energy\n(i.e. ergotropy functional by Allahverdyan, Balian, and Nieuwenhuizen) of a\nsingle mode of the electromagnetic radiation which, depending on the outcome of\nthe betting, experiences attenuation or amplification processes which model\nlosses and winning events. The resulting stochastic evolution of the quantum\nmemory resembles the dynamics of random lasing which we characterize within the\ntheoretical setting of Bosonic Gaussian channels. As in the classical Kelly\nCriterion for optimal betting, we define the asymptotic doubling rate of the\nmodel and identify the optimal gambling strategy for fixed odds and\nprobabilities of winning. The performance of the model are hence studied as a\nfunction of the input capital state under the assumption that the latter\nbelongs to the set of Gaussian density matrices (i.e. displaced, squeezed\nthermal Gibbs states) revealing that the best option for the gambler is to\ndevote all her/his initial resources into coherent state amplitude.\n"
    },
    {
        "paper_id": 2001.11585,
        "authors": "Geoff Boeing, Max Besbris, Ariela Schachter, John Kuk",
        "title": "Housing Search in the Age of Big Data: Smarter Cities or the Same Old\n  Blind Spots?",
        "comments": null,
        "journal-ref": null,
        "doi": "10.1080/10511482.2019.1684336",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Housing scholars stress the importance of the information environment in\nshaping housing search behavior and outcomes. Rental listings have increasingly\nmoved online over the past two decades and, in turn, online platforms like\nCraigslist are now central to the search process. Do these technology platforms\nserve as information equalizers or do they reflect traditional information\ninequalities that correlate with neighborhood sociodemographics? We synthesize\nand extend analyses of millions of US Craigslist rental listings and find they\nsupply significantly different volumes, quality, and types of information in\ndifferent communities. Technology platforms have the potential to broaden,\ndiversify, and equalize housing search information, but they rely on landlord\nbehavior and, in turn, likely will not reach this potential without a\nsignificant redesign or policy intervention. Smart cities advocates hoping to\nbuild better cities through technology must critically interrogate technology\nplatforms and big data for systematic biases.\n"
    },
    {
        "paper_id": 2001.11624,
        "authors": "Simon Clinet",
        "title": "Quasi-likelihood analysis for marked point processes and application to\n  marked Hawkes processes",
        "comments": "42 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We develop a quasi-likelihood analysis procedure for a general class of\nmultivariate marked point processes. As a by-product of the general method, we\nestablish under stability and ergodicity conditions the local asymptotic\nnormality of the quasi-log likelihood, along with the convergence of moments of\nquasi-likelihood and quasi-Bayesian estimators. To illustrate the general\napproach, we then turn our attention to a class of multivariate marked Hawkes\nprocesses with generalized exponential kernels, comprising among others the\nso-called Erlang kernels. We provide explicit conditions on the kernel\nfunctions and the mark dynamics under which a certain transformation of the\noriginal process is Markovian and $V$-geometrically ergodic. We finally prove\nthat the latter result, which is of interest in its own right, constitutes the\nkey ingredient to show that the generalized exponential Hawkes process falls\nunder the scope of application of the quasi-likelihood analysis.\n"
    },
    {
        "paper_id": 2001.11786,
        "authors": "Shuaiqiang Liu, \\'Alvaro Leitao, Anastasia Borovykh, Cornelis W.\n  Oosterlee",
        "title": "On Calibration Neural Networks for extracting implied information from\n  American options",
        "comments": "24 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Extracting implied information, like volatility and/or dividend, from\nobserved option prices is a challenging task when dealing with American\noptions, because of the computational costs needed to solve the corresponding\nmathematical problem many thousands of times. We will employ a data-driven\nmachine learning approach to estimate the Black-Scholes implied volatility and\nthe dividend yield for American options in a fast and robust way. To determine\nthe implied volatility, the inverse function is approximated by an artificial\nneural network on the computational domain of interest, which decouples the\noffline (training) and online (prediction) phases and thus eliminates the need\nfor an iterative process. For the implied dividend yield, we formulate the\ninverse problem as a calibration problem and determine simultaneously the\nimplied volatility and dividend yield. For this, a generic and robust\ncalibration framework, the Calibration Neural Network (CaNN), is introduced to\nestimate multiple parameters. It is shown that machine learning can be used as\nan efficient numerical technique to extract implied information from American\noptions.\n"
    },
    {
        "paper_id": 2001.11843,
        "authors": "Yuichi Ikeda",
        "title": "An Interacting Agent Model of Economic Crisis",
        "comments": "21 pages, 6 figures",
        "journal-ref": "Chapter 11 of the book titled by \"Complexity, Heterogeneity and\n  the Methods of Statistical Physics in Economics\" which will be published from\n  Springer in 2020",
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Most national economies are linked by international trade. Consequently,\neconomic globalization forms a massive and complex economic network with strong\nlinks, that is, interactions arising from increasing trade. Various interesting\ncollective motions are expected to emerge from strong economic interactions in\na global economy under trade liberalization. Among the various economic\ncollective motions, economic crises are our most intriguing problem. In our\nprevious studies, we have revealed that the Kuramoto's coupled limit-cycle\noscillator model and the Ising-like spin model on networks are invaluable tools\nfor characterizing the economic crises. In this study, we develop a\nmathematical theory to describe an interacting agent model that derives the\nKuramoto model and the Ising-like spin model by using appropriate\napproximations. Our interacting agent model suggests phase synchronization and\nspin ordering during economic crises. We confirm the emergence of the phase\nsynchronization and spin ordering during economic crises by analyzing various\neconomic time series data. We also develop a network reconstruction model based\non entropy maximization that considers the sparsity of the network. Here\nnetwork reconstruction means estimating a network's adjacency matrix from a\nnode's local information. The interbank network is reconstructed using the\ndeveloped model, and a comparison is made of the reconstructed network with the\nactual data. We successfully reproduce the interbank network and the known\nstylized facts. In addition, the exogenous shock acting on an industry\ncommunity in a supply chain network and financial sector are estimated.\nEstimation of exogenous shocks acting on communities of in the real economy in\nthe supply chain network provide evidence of the channels of distress\npropagating from the financial sector to the real economy through the supply\nchain network.\n"
    },
    {
        "paper_id": 2001.11861,
        "authors": "Elena Boguslavskaya, Lioudmila Vostrikova (LAREMA)",
        "title": "Revisiting integral functionals of geometric Brownian motion",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper we revisit the integral functional of geometric Brownian motion\n$I_t= \\int_0^t e^{-(\\mu s +\\sigma W_s)}ds$, where $\\mu\\in\\mathbb{R}$, $\\sigma >\n0$, and $(W_s )_s>0$ is a standard Brownian motion. Specifically, we calculate\nthe Laplace transform in $t$ of the cumulative distribution function and of the\nprobability density function of this functional.\n"
    },
    {
        "paper_id": 2001.11891,
        "authors": "N. Packham",
        "title": "Structured climate financing: valuation of CDOs on inhomogeneous asset\n  pools",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Recently, a number of structured funds have emerged as public-private\npartnerships with the intent of promoting investment in renewable energy in\nemerging markets. These funds seek to attract institutional investors by\ntranching the asset pool and issuing senior notes with a high credit quality.\nFinancing of renewable energy (RE) projects is achieved via two channels: small\nRE projects are financed indirectly through local banks that draw loans from\nthe fund's assets, whereas large RE projects are directly financed from the\nfund. In a bottom-up Gaussian copula framework, we examine the diversification\nproperties and RE exposure of the senior tranche. To this end, we introduce the\nLH++ model, which combines a homogeneous infinitely granular loan portfolio\nwith a finite number of large loans. Using expected tranche percentage notional\n(which takes a similar role as the default probability of a loan), tranche\nprices and tranche sensitivities in RE loans, we analyse the risk profile of\nthe senior tranche. We show how the mix of indirect and direct RE investments\nin the asset pool affects the sensitivity of the senior tranche to RE\ninvestments and how to balance a desired sensitivity with a target credit\nquality and target tranche size.\n"
    },
    {
        "paper_id": 2002.00085,
        "authors": "Marco Avellaneda, Brian Healy, Andrew Papanicolaou, George\n  Papanicolaou",
        "title": "PCA for Implied Volatility Surfaces",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Principal component analysis (PCA) is a useful tool when trying to construct\nfactor models from historical asset returns. For the implied volatilities of\nU.S. equities there is a PCA-based model with a principal eigenportfolio whose\nreturn time series lies close to that of an overarching market factor. The\nauthors show that this market factor is the index resulting from the daily\ncompounding of a weighted average of implied-volatility returns, with weights\nbased on the options' open interest (OI) and Vega. The authors also analyze the\nsingular vectors derived from the tensor structure of the implied volatilities\nof S&P500 constituents, and find evidence indicating that some type of OI and\nVega-weighted index should be one of at least two significant factors in this\nmarket.\n"
    },
    {
        "paper_id": 2002.00103,
        "authors": "Vishal Kamat and Samuel Norris",
        "title": "Estimating Welfare Effects in a Nonparametric Choice Model: The Case of\n  School Vouchers",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We develop new robust discrete choice tools to learn about the average\nwillingness to pay for a price subsidy and its effects on demand given\nexogenous, discrete variation in prices. Our starting point is a nonparametric,\nnonseparable model of choice. We exploit the insight that our welfare\nparameters in this model can be expressed as functions of demand for the\ndifferent alternatives. However, while the variation in the data reveals the\nvalue of demand at the observed prices, the parameters generally depend on its\nvalues beyond these prices. We show how to sharply characterize what we can\nlearn when demand is specified to be entirely nonparametric or to be\nparameterized in a flexible manner, both of which imply that the parameters are\nnot necessarily point identified. We use our tools to analyze the welfare\neffects of price subsidies provided by school vouchers in the DC Opportunity\nScholarship Program. We robustly find that the provision of the status quo\nvoucher and a wide range of counterfactual vouchers of different amounts have\npositive benefits net of costs. This positive effect can be explained by the\npopularity of low-tuition schools in the program; removing them from the\nprogram can result in a negative net benefit. Relative to our bounds, we also\nfind that comparable logit estimates potentially understate the benefits for\ncertain voucher amounts, and provide a misleading sense of robustness for\nalternative amounts.\n"
    },
    {
        "paper_id": 2002.00201,
        "authors": "Enrico Biffis, Fausto Gozzi, Cecilia Prosdocimi",
        "title": "Optimal portfolio choice with path dependent labor income: the infinite\n  horizon case",
        "comments": "32 pages, 0 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We consider an infinite horizon portfolio problem with borrowing constraints,\nin which an agent receives labor income which adjusts to financial market\nshocks in a path dependent way. This path-dependency is the novelty of the\nmodel, and leads to an infinite dimensional stochastic optimal control problem.\nWe solve the problem completely, and find explicitly the optimal controls in\nfeedback form. This is possible because we are able to find an explicit\nsolution to the associated infinite dimensional Hamilton-Jacobi-Bellman (HJB)\nequation, even if state constraints are present. To the best of our knowledge,\nthis is the first infinite dimensional generalization of Merton's optimal\nportfolio problem for which explicit solutions can be found. The explicit\nsolution allows us to study the properties of optimal strategies and discuss\ntheir financial implications.\n"
    },
    {
        "paper_id": 2002.00507,
        "authors": "Mariia Soloviova and Tiziano Vargiolu",
        "title": "Efficient representation of supply and demand curves on day-ahead\n  electricity markets",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Our paper aims to model supply and demand curves of electricity day-ahead\nauction in a parsimonious way. Our main task is to build an appropriate\nalgorithm to present the information about electricity prices and demands with\nfar less parameters than the original one. We represent each curve using\nmesh-free interpolation techniques based on radial basis function\napproximation. We describe results of this method for the day-ahead IPEX spot\nprice of Italy.\n"
    },
    {
        "paper_id": 2002.00724,
        "authors": "Katsuya Ito, Kei Nakagawa",
        "title": "NAPLES;Mining the lead-lag Relationship from Non-synchronous and\n  High-frequency Data",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In time-series analysis, the term \"lead-lag effect\" is used to describe a\ndelayed effect on a given time series caused by another time series. lead-lag\neffects are ubiquitous in practice and are specifically critical in formulating\ninvestment strategies in high-frequency trading. At present, there are three\nmajor challenges in analyzing the lead-lag effects. First, in practical\napplications, not all time series are observed synchronously. Second, the size\nof the relevant dataset and rate of change of the environment is increasingly\nfaster, and it is becoming more difficult to complete the computation within a\nparticular time limit. Third, some lead-lag effects are time-varying and only\nlast for a short period, and their delay lengths are often affected by external\nfactors. In this paper, we propose NAPLES (Negative And Positive lead-lag\nEStimator), a new statistical measure that resolves all these problems. Through\nexperiments on artificial and real datasets, we demonstrate that NAPLES has a\nstrong correlation with the actual lead-lag effects, including those triggered\nby significant macroeconomic announcements.\n"
    },
    {
        "paper_id": 2002.00816,
        "authors": "Christian Bayer, Denis Belomestny, Paul Hager, Paolo Pigato, John\n  Schoenmakers",
        "title": "Randomized optimal stopping algorithms and their convergence analysis",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper we study randomized optimal stopping problems and consider\ncorresponding forward and backward Monte Carlo based optimisation algorithms.\nIn particular we prove the convergence of the proposed algorithms and derive\nthe corresponding convergence rates.\n"
    },
    {
        "paper_id": 2002.00948,
        "authors": "Jean-Louis Arcand, Max-Olivier Hongler, Shekhar Hari Kumar, Daniele\n  Rinaldo",
        "title": "Can one hear the shape of a target zone?",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We develop an exchange rate target zone model with finite exit time and\nnon-Gaussian tails. We show how the tails are a consequence of time-varying\ninvestor risk aversion, which generates mean-preserving spreads in the\nfundamental distribution. We solve explicitly for stationary and non-stationary\nexchange rate paths, and show how both depend continuously on the distance to\nthe exit time and the target zone bands. This enables us to show how central\nbank intervention is endogenous to both the distance of the fundamental to the\nband and the underlying risk. We discuss how the feasibility of the target zone\nis shaped by the set horizon and the degree of underlying risk, and we\ndetermine a minimum time at which the required parity can be reached. We prove\nthat increases in risk after a certain threshold can yield endogenous regime\nshifts where the ``honeymoon effects'' vanish and the target zone cannot be\nfeasibly maintained. None of these results can be obtained by means of the\nstandard Gaussian or affine models. Numerical simulations allow us to recover\nall the exchange rate densities established in the target zone literature. The\ngenerality of our framework has important policy implications for modern target\nzone arrangements.\n"
    },
    {
        "paper_id": 2002.01528,
        "authors": "Yan Dolinsky",
        "title": "On Shortfall Risk Minimization for Game Options",
        "comments": "13 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper we study the existence of an optimal hedging strategy for the\nshortfall risk measure in the game options setup. We consider the continuous\ntime Black--Scholes (BS) model. Our first result says that in the case where\nthe game contingent claim (GCC) can be exercised only on a finite set of times,\nthere exists an optimal strategy. Our second and main result is an example\nwhich demonstrates that for the case where the GCC can be stopped on the all\ntime interval, optimal portfolio strategies need not always exist.\n"
    },
    {
        "paper_id": 2002.01578,
        "authors": "Geoff Boeing, Jake Wegmann, Junfeng Jiao",
        "title": "Rental Housing Spot Markets: How Online Information Exchanges Can\n  Supplement Transacted-Rents Data",
        "comments": null,
        "journal-ref": "Journal of Planning Education and Research, 2020",
        "doi": "10.1177/0739456X20904435",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Traditional US rental housing data sources such as the American Community\nSurvey and the American Housing Survey report on the transacted market - what\nexisting renters pay each month. They do not explicitly tell us about the spot\nmarket - i.e., the asking rents that current homeseekers must pay to acquire\nhousing - though they are routinely used as a proxy. This study compares\ngovernmental data to millions of contemporaneous rental listings and finds that\nasking rents diverge substantially from these most recent estimates.\nConventional housing data understate current market conditions and\naffordability challenges, especially in cities with tight and expensive rental\nmarkets.\n"
    },
    {
        "paper_id": 2002.01798,
        "authors": "Liang Yang, Zhengxiao Li, Shengwang Meng",
        "title": "Risk Loadings in Classification Ratemaking",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
        "abstract": "  The risk premium of a policy is the sum of the pure premium and the risk\nloading. In the classification ratemaking process, generalized linear models\nare usually used to calculate pure premiums, and various premium principles are\napplied to derive the risk loadings. No matter which premium principle is used,\nsome risk loading parameters should be given in advance subjectively. To\novercome this subjective problem and calculate the risk premium more reasonably\nand objectively, we propose a top-down method to calculate these risk loading\nparameters. First, we implement the bootstrap method to calculate the total\nrisk premium of the portfolio. Then, under the constraint that the portfolio's\ntotal risk premium should equal the sum of the risk premiums of each policy,\nthe risk loading parameters are determined. During this process, besides using\ngeneralized linear models, three kinds of quantile regression models are also\napplied, namely, traditional quantile regression model, fully parametric\nquantile regression model, and quantile regression model with coefficient\nfunctions. The empirical result shows that the risk premiums calculated by the\nmethod proposed in this study can reasonably differentiate the heterogeneity of\ndifferent risk classes.\n"
    },
    {
        "paper_id": 2002.018,
        "authors": "Mehmet Caner, Marcelo Medeiros, and Gabriel Vasconcelos",
        "title": "Sharpe Ratio Analysis in High Dimensions: Residual-Based Nodewise\n  Regression in Factor Models",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
        "abstract": "  We provide a new theory for nodewise regression when the residuals from a\nfitted factor model are used. We apply our results to the analysis of the\nconsistency of Sharpe ratio estimators when there are many assets in a\nportfolio. We allow for an increasing number of assets as well as time\nobservations of the portfolio. Since the nodewise regression is not feasible\ndue to the unknown nature of idiosyncratic errors, we provide a\nfeasible-residual-based nodewise regression to estimate the precision matrix of\nerrors which is consistent even when number of assets, p, exceeds the time span\nof the portfolio, n. In another new development, we also show that the\nprecision matrix of returns can be estimated consistently, even with an\nincreasing number of factors and p>n. We show that: (1) with p>n, the Sharpe\nratio estimators are consistent in global minimum-variance and mean-variance\nportfolios; and (2) with p>n, the maximum Sharpe ratio estimator is consistent\nwhen the portfolio weights sum to one; and (3) with p<<n, the\nmaximum-out-of-sample Sharpe ratio estimator is consistent.\n"
    },
    {
        "paper_id": 2002.02008,
        "authors": "Bryan Lim, Stefan Zohren, Stephen Roberts",
        "title": "Detecting Changes in Asset Co-Movement Using the Autoencoder\n  Reconstruction Ratio",
        "comments": null,
        "journal-ref": "Risk 2020",
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Detecting changes in asset co-movements is of much importance to financial\npractitioners, with numerous risk management benefits arising from the timely\ndetection of breakdowns in historical correlations. In this article, we propose\na real-time indicator to detect temporary increases in asset co-movements, the\nAutoencoder Reconstruction Ratio, which measures how well a basket of asset\nreturns can be modelled using a lower-dimensional set of latent variables. The\nARR uses a deep sparse denoising autoencoder to perform the dimensionality\nreduction on the returns vector, which replaces the PCA approach of the\nstandard Absorption Ratio, and provides a better model for non-Gaussian\nreturns. Through a systemic risk application on forecasting on the CRSP US\nTotal Market Index, we show that lower ARR values coincide with higher\nvolatility and larger drawdowns, indicating that increased asset co-movement\ndoes correspond with periods of market weakness. We also demonstrate that\nshort-term (i.e. 5-min and 1-hour) predictors for realised volatility and\nmarket crashes can be improved by including additional ARR inputs.\n"
    },
    {
        "paper_id": 2002.0201,
        "authors": "Yun Bai, Xixi Li, Hao Yu, and Suling Jia",
        "title": "Crude oil price forecasting incorporating news text",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Sparse and short news headlines can be arbitrary, noisy, and ambiguous,\nmaking it difficult for classic topic model LDA (latent Dirichlet allocation)\ndesigned for accommodating long text to discover knowledge from them.\nNonetheless, some of the existing research about text-based crude oil\nforecasting employs LDA to explore topics from news headlines, resulting in a\nmismatch between the short text and the topic model and further affecting the\nforecasting performance. Exploiting advanced and appropriate methods to\nconstruct high-quality features from news headlines becomes crucial in crude\noil forecasting. To tackle this issue, this paper introduces two novel\nindicators of topic and sentiment for the short and sparse text data. Empirical\nexperiments show that AdaBoost.RT with our proposed text indicators, with a\nmore comprehensive view and characterization of the short and sparse text data,\noutperforms the other benchmarks. Another significant merit is that our method\nalso yields good forecasting performance when applied to other futures\ncommodities.\n"
    },
    {
        "paper_id": 2002.02011,
        "authors": "Rising Odegua",
        "title": "Predicting Bank Loan Default with Extreme Gradient Boosting",
        "comments": "5 pages, 3 Figures, 4 Tables",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Loan default prediction is one of the most important and critical problems\nfaced by banks and other financial institutions as it has a huge effect on\nprofit. Although many traditional methods exist for mining information about a\nloan application, most of these methods seem to be under-performing as there\nhave been reported increases in the number of bad loans. In this paper, we use\nan Extreme Gradient Boosting algorithm called XGBoost for loan default\nprediction. The prediction is based on a loan data from a leading bank taking\ninto consideration data sets from both the loan application and the demographic\nof the applicant. We also present important evaluation metrics such as\nAccuracy, Recall, precision, F1-Score and ROC area of the analysis. This paper\nprovides an effective basis for loan credit approval in order to identify risky\ncustomers from a large number of loan applications using predictive modeling.\n"
    },
    {
        "paper_id": 2002.02107,
        "authors": "Luciana Barbosa, Cl\\'audia Nunes, Artur Rodrigues, Alberto Sardinha",
        "title": "Feed-in Tariff Contract Schemes and Regulatory Uncertainty",
        "comments": "In revision",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper presents a novel analysis of two feed-in tariffs (FIT) under\nmarket and regulatory uncertainty, namely a sliding premium with cap and floor\nand a minimum price guarantee. Regulatory uncertainty is modeled with a Poisson\nprocess, whereby a jump event may reduce the tariff before the signature of the\ncontract. Using a semi-analytical real options framework, we derive the project\nvalue, the optimal investment threshold, and the value of the investment\nopportunity for these schemes. Taking into consideration the optimal investment\nthreshold, we also compare the two aforementioned FITs with the fixed-price FIT\nand the fixed-premium FIT, which are policy schemes that have been extensively\nstudied in the literature. Our results show that increasing the likelihood of a\njump event lowers the investment threshold for all the schemes; moreover, the\ninvestment threshold also decreases when the tariff reduction increases. We\nalso compare the four schemes in terms of the corresponding optimal investment\nthresholds. For example, we find that the investment threshold of the sliding\npremium is lower than the minimum price guarantee. This result suggests that\nthe first regime is a better policy than the latter because it accelerates the\ninvestment while avoiding excessive earnings to producers.\n"
    },
    {
        "paper_id": 2002.02229,
        "authors": "An Chen, Mitja Stadje, Fangyuan Zhang",
        "title": "On the equivalence between Value-at-Risk- and Expected Shortfall-based\n  risk measures in non-concave optimization",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We study a non-concave optimization problem in which a financial company\nmaximizes the expected utility of the surplus under a risk-based regulatory\nconstraint. For this problem, we consider four different prevalent risk\nconstraints (Expected Shortfall, Expected Discounted Shortfall, Value-at-Risk,\nand Average Value-at-Risk), and investigate their effects on the optimal\nsolution. Our main contributions are in obtaining an analytical solution under\neach of the four risk constraints, in the form of the optimal terminal wealth.\nWe show that the four risk constraints lead to the same optimal solution, which\ndiffers from previous conclusions obtained from the corresponding concave\noptimization problem under a risk constraint. Compared with the benchmark\n(unconstrained) non-concave utility maximization problem, all four risk\nconstraints effectively and equivalently reduce the set of zero terminal\nwealth, but do not fully eliminate this set, indicating the success and failure\nof the respective financial regulations.\n"
    },
    {
        "paper_id": 2002.02271,
        "authors": "Dmitry Efimov, Di Xu, Luyang Kong, Alexey Nefedov and Archana\n  Anandakrishnan",
        "title": "Using generative adversarial networks to synthesize artificial financial\n  datasets",
        "comments": null,
        "journal-ref": "Robust AI in FS 2019 : NeurIPS 2019 Workshop on Robust AI in\n  Financial Services: Data, Fairness, Explainability, Trustworthiness, and\n  Privacy, December 2019, Vancouver, Canada",
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Generative Adversarial Networks (GANs) became very popular for generation of\nrealistically looking images. In this paper, we propose to use GANs to\nsynthesize artificial financial data for research and benchmarking purposes. We\ntest this approach on three American Express datasets, and show that properly\ntrained GANs can replicate these datasets with high fidelity. For our\nexperiments, we define a novel type of GAN, and suggest methods for data\npreprocessing that allow good training and testing performance of GANs. We also\ndiscuss methods for evaluating the quality of generated data, and their\ncomparison with the original real data.\n"
    },
    {
        "paper_id": 2002.02481,
        "authors": "Francois Belletti, Davis King, James Lottes, Yi-Fan Chen, John\n  Anderson",
        "title": "Sensitivity Analysis in the Dupire Local Volatility Model with\n  Tensorflow",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-sa/4.0/",
        "abstract": "  In a recent paper, we have demonstrated how the affinity between TPUs and\nmulti-dimensional financial simulation resulted in fast Monte Carlo simulations\nthat could be setup in a few lines of python Tensorflow code. We also presented\na major benefit from writing high performance simulations in an automated\ndifferentiation language such as Tensorflow: a single line of code enabled us\nto estimate sensitivities, i.e. the rate of change in price of financial\ninstrument with respect to another input such as the interest rate, the current\nprice of the underlying, or volatility. Such sensitivities (otherwise known as\nthe famous financial \"Greeks\") are fundamental for risk assessment and risk\nmitigation. In the present follow-up short paper, we extend the developments\nexposed in our previous work about the use of Tensor Processing Units and\nTensorflow for TPUs.\n"
    },
    {
        "paper_id": 2002.02583,
        "authors": "Alberto Ciacci, Takumi Sueshige, Hideki Takayasu, Kim Christensen,\n  Misako Takayasu",
        "title": "The microscopic relationships between triangular arbitrage and\n  cross-currency correlations in a simple agent based model of foreign exchange\n  markets",
        "comments": "Research article",
        "journal-ref": null,
        "doi": "10.1371/journal.pone.0234709",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Foreign exchange rates movements exhibit significant cross-correlations even\non very short time-scales. The effect of these statistical relationships become\nevident during extreme market events, such as flash crashes.In this scenario,\nan abrupt price swing occurring on a given market is immediately followed by\nanomalous movements in several related foreign exchange rates. Although a deep\nunderstanding of cross-currency correlations would be clearly beneficial for\nconceiving more stable and safer foreign exchange markets, the microscopic\norigins of these interdependencies have not been extensively investigated. We\nintroduce an agent-based model which describes the emergence of cross-currency\ncorrelations from the interactions between market makers and an arbitrager. Our\nmodel qualitatively replicates the time-scale vs. cross-correlation diagrams\nobserved in real trading data, suggesting that triangular arbitrage plays a\nprimary role in the entanglement of the dynamics of different foreign exchange\nrates. Furthermore, the model shows how the features of the cross-correlation\nfunction between two foreign exchange rates, such as its sign and value, emerge\nfrom the interplay between triangular arbitrage and trend-following strategies.\n"
    },
    {
        "paper_id": 2002.02604,
        "authors": "Tomasz R. Bielecki, Tao Chen, Igor Cialenco",
        "title": "Time-inconsistent Markovian control problems under model uncertainty\n  with application to the mean-variance portfolio selection",
        "comments": "22 pages, 4 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper we study a class of time-inconsistent terminal Markovian\ncontrol problems in discrete time subject to model uncertainty. We combine the\nconcept of the sub-game perfect strategies with the adaptive robust stochastic\nto tackle the theoretical aspects of the considered stochastic control problem.\nConsequently, as an important application of the theoretical results, by\napplying a machine learning algorithm we solve numerically the mean-variance\nportfolio selection problem under the model uncertainty.\n"
    },
    {
        "paper_id": 2002.02675,
        "authors": "Idris Kharroubi (LPSM UMR 8001), Thomas Lim (LaMME, ENSIIE), Xavier\n  Warin (EDF)",
        "title": "Discretization and Machine Learning Approximation of BSDEs with a\n  Constraint on the Gains-Process",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We study the approximation of backward stochastic differential equations\n(BSDEs for short) with a constraint on the gains process. We first discretize\nthe constraint by applying a so-called facelift operator at times of a grid. We\nshow that this discretely constrained BSDE converges to the continuously\nconstrained one as the mesh grid converges to zero. We then focus on the\napproximation of the discretely constrained BSDE. For that we adopt a machine\nlearning approach. We show that the facelift can be approximated by an\noptimization problem over a class of neural networks under constraints on the\nneural network and its derivative. We then derive an algorithm converging to\nthe discretely constrained BSDE as the number of neurons goes to infinity. We\nend by numerical experiments. Mathematics Subject Classification (2010): 65C30,\n65M75, 60H35, 93E20, 49L25.\n"
    },
    {
        "paper_id": 2002.02876,
        "authors": "Saeed Marzban, Erick Delage, Jonathan Yumeng Li",
        "title": "Equal Risk Pricing and Hedging of Financial Derivatives with Convex Risk\n  Measures",
        "comments": "Submitted to Quantitative Finance Journal, typos corrected, some\n  minor issues resolved",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper, we consider the problem of equal risk pricing and hedging in\nwhich the fair price of an option is the price that exposes both sides of the\ncontract to the same level of risk. Focusing for the first time on the context\nwhere risk is measured according to convex risk measures, we establish that the\nproblem reduces to solving independently the writer and the buyer's hedging\nproblem with zero initial capital. By further imposing that the risk measures\ndecompose in a way that satisfies a Markovian property, we provide dynamic\nprogramming equations that can be used to solve the hedging problems for both\nthe case of European and American options. All of our results are general\nenough to accommodate situations where the risk is measured according to a\nworst-case risk measure as is typically done in robust optimization. Our\nnumerical study illustrates the advantages of equal risk pricing over schemes\nthat only account for a single party, pricing based on quadratic hedging (i.e.\n$\\epsilon$-arbitrage pricing), or pricing based on a fixed equivalent\nmartingale measure (i.e. Black-Scholes pricing). In particular, the numerical\nresults confirm that when employing an equal risk price both the writer and the\nbuyer end up being exposed to risks that are more similar and on average\nsmaller than what they would experience with the other approaches.\n"
    },
    {
        "paper_id": 2002.03235,
        "authors": "Roc\\'io Paredes and Marco Vega",
        "title": "An internal fraud model for operational losses in retail banking",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper develops a dynamic internal fraud model for operational losses in\nretail banking. It considers public operational losses arising from internal\nfraud in retail banking within a group of international banks. Additionally,\nthe model takes into account internal factors such as the ethical quality of\nworkers and the risk controls set by bank managers. The model is validated by\nmeasuring the impact of macroeconomic indicators such as GDP growth and the\ncorruption perception upon the severity and frequency of losses implied by the\nmodel. In general,results show that internal fraud losses are pro-cyclical, and\nthat country specific corruption perceptions positively affects internal fraud\nlosses. Namely, when a country is perceived to be more corrupt, retail banking\nin that country will feature more severe internal fraud losses.\n"
    },
    {
        "paper_id": 2002.03286,
        "authors": "Sarah Boese, Tracy Cui, Samuel Johnston, Gianmarco Molino and Oleksii\n  Mostovyi",
        "title": "Stability and asymptotic analysis of the F\\\"ollmer-Schweizer\n  decomposition on a finite probability space",
        "comments": "17 pages, 3 figures. This paper is a part of an REU project conduced\n  in Summer 2019 at the University of Connecticut. Accepted in Involve",
        "journal-ref": "Involve 13 (2020) 607-623",
        "doi": "10.2140/involve.2020.13.607",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  First, we consider the problem of hedging in complete binomial models. Using\nthe discrete-time F\\\"ollmer-Schweizer decomposition, we demonstrate the\nequivalence of the backward induction and sequential regression approaches.\nSecond, in incomplete trinomial models, we examine the extension of the\nsequential regression approach for approximation of contingent claims. Then, on\na finite probability space, we investigate stability of the discrete-time\nF\\\"ollmer-Schweizer decomposition with respect to perturbations of the stock\nprice dynamics and, finally, perform its asymptotic analysis under simultaneous\nperturbations of the drift and volatility of the underlying discounted stock\nprice process, where we prove stability and obtain explicit formulas for the\nleading order correction terms.\n"
    },
    {
        "paper_id": 2002.03295,
        "authors": "Khaled Masoumifard and Mohammad Zokaei",
        "title": "Stochastic optimization of the Dividend strategy with reinsurance in\n  correlated multiple insurance lines of business",
        "comments": "39 pages, 10 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/publicdomain/zero/1.0/",
        "abstract": "  The present paper addresses the issue of the stochastic control of the\noptimal dynamic reinsurance policy and dynamic dividend strategy, which are\nstate-dependent, for an insurance company that operates under multiple\ninsurance lines of business. The aggregate claims model with a\nthinning-dependence structure is adopted for the risk process. In the\noptimization method, the maximum of the cumulative expected discounted dividend\npayouts with respect to the dividend and reinsurance strategies are considered\nas value function. This value function is characterized as the smallest super\nViscosity solution of the associated Hamilton-Jacobi- Bellman (HJB) equation.\nThe finite difference method (FDM) has been utilized for the numerical solution\nof the value function and the optimal control strategy and the proof for the\nconvergence of this numerical solution to the value function is provided. The\nfindings of this paper provide insights for the insurance companies as such\nthat based upon the lines in which they are operating, they can choose a vector\nof the optimal dynamic reinsurance strategies and consequently transfer some\npart of their risks to several reinsurers.\n"
    },
    {
        "paper_id": 2002.03319,
        "authors": "Marc van Kralingen, Diego Garlaschelli, Karolina Scholtus, Iman van\n  Lelyveld",
        "title": "Crowded trades, market clustering, and price instability",
        "comments": null,
        "journal-ref": "Entropy 23(3), 336 (2021)",
        "doi": "10.3390/e23030336",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Crowded trades by similarly trading peers influence the dynamics of asset\nprices, possibly creating systemic risk. We propose a market clustering measure\nusing granular trading data. For each stock the clustering measure captures the\ndegree of trading overlap among any two investors in that stock. We investigate\nthe effect of crowded trades on stock price stability and show that market\nclustering has a causal effect on the properties of the tails of the stock\nreturn distribution, particularly the positive tail, even after controlling for\ncommonly considered risk drivers. Reduced investor pool diversity could thus\nnegatively affect stock price stability.\n"
    },
    {
        "paper_id": 2002.03376,
        "authors": "Arne Lokka and Junwei Xu",
        "title": "Optimal liquidation trajectories for the Almgren-Chriss model with Levy\n  processes",
        "comments": "36 pages, 2 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We consider an optimal liquidation problem with infinite horizon in the\nAlmgren-Chriss framework, where the unaffected asset price follows a Levy\nprocess. The temporary price impact is described by a general function which\nsatisfies some reasonable conditions. We consider an investor with constant\nabsolute risk aversion, who wants to maximise the expected utility of the cash\nreceived from the sale of his assets, and show that this problem can be reduced\nto a deterministic optimisation problem which we are able to solve explicitly.\nIn order to compare our results with exponential Levy models, which provides a\nvery good statistical fit with observed asset price data for short time\nhorizons, we derive the (linear) Levy process approximation of such models. In\nparticular we derive expressions for the Levy process approximation of the\nexponential Variance-Gamma Levy process, and study properties of the\ncorresponding optimal liquidation strategy. We then provide a comparison of the\nliquidation trajectories for reasonable parameters between the Levy process\nmodel and the classical Almgren-Chriss model. In particular, we obtain an\nexplicit expression for the connection between the temporary impact function\nfor the Levy model and the temporary impact function for the Brownian motion\nmodel (the classical Almgren-Chriss model), for which the optimal liquidation\ntrajectories for the two models coincide.\n"
    },
    {
        "paper_id": 2002.03379,
        "authors": "Arne Lokka and Junwei Xu",
        "title": "Optimal liquidation for a risk averse investor in a one-sided limit\n  order book driven by a Levy process",
        "comments": "47 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In a one-sided limit order book, satisfying some realistic assumptions, where\nthe unaffected price process follows a Levy process, we consider a market agent\nthat wants to liquidate a large position of shares. We assume that the agent\nhas constant absolute risk aversion and aims at maximising the expected utility\nof the cash position at the end of time. The agent is then faced with the\nproblem of balancing the market risk and the cost of a rapid execution. In\nparticular we are interested in how the agent should go about optimally\nsubmitting orders. Since liquidation normally takes place within a short period\nof time, modelling the risk as a Levy process should provide a realistic model\nwith good statistical fit to observed market data, and thus the model should\nprovide a realistic reflection of the agent's market risk. We reduce the\noptimisation problem to a deterministic two-dimensional singular problem, to\nwhich we are able to derive an explicit solution in terms of the model data. In\nparticular we find an expression for the optimal intervention boundary, which\ncompletely characterise the optimal liquidation strategy.\n"
    },
    {
        "paper_id": 2002.03448,
        "authors": "Sergey Lototsky and Austin Pollok",
        "title": "Kelly Criterion: From a Simple Random Walk to L\\'{e}vy Processes",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The original Kelly criterion provides a strategy to maximize the long-term\ngrowth of winnings in a sequence of simple Bernoulli bets with an edge, that\nis, when the expected return on each bet is positive. The objective of this\nwork is to consider more general models of returns and the continuous time, or\nhigh frequency, limits of those models.\n"
    },
    {
        "paper_id": 2002.04067,
        "authors": "Adnan Malik, Karim Ullah, and Shakir Ullah",
        "title": "Knowledge Diffusion Process & Common Islamic Banking Governance\n  Principles: Integrative Perspective (s) of Managers and Shariah Scholars",
        "comments": "38 pages, 01 figure",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Islamic banks being commercial entities strive to earn profit within shariah\nambit. Therefore, they seem to be basing themselves upon two knowledge streams\nnamely i) Islamic jurisprudence principles, and ii) banking principles. Islamic\njurisprudence principles primarily aim at bringing shariah compliance while\nbanking principles focus profitability. These principles, making two schools of\nthought in the discipline, however, have their unique philosophies, principles,\nand practices, which are now gradually diffusing into an emergent set of\ngovernance principles basing the contemporary Islamic banking theory and\npractice. Governance systems of Islamic banks have elements of both\nconventional as well as Shariah, and need to have principles having components\nof banking and shariah sufficiently diffused for their successful operations in\na longer term. Aim of this research is to review the literature about the\nknowledge diffusion process of islamic banking principles which guides the\ngovernance of Islamic banks. This study review the literature using a method in\nwhich focus remain on bridging different areas which in this case are knowledge\ndiffusion and islamic banking governance principles.\n"
    },
    {
        "paper_id": 2002.04068,
        "authors": "Myriem Alijo, Otman Abdoun, Mostafa Bachran, Amal Bergam",
        "title": "Optimization by Hybridization of a Genetic Algorithm with the PROMOTHEE\n  Method: Management of Multicriteria Localization",
        "comments": "18 pages",
        "journal-ref": "Journal Economic Computation and Economic Cybernetics Studies and\n  Research, Vol 52, N 3 , pp. 171-188, 2018",
        "doi": "10.24818/18423264/52.3.18.12",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The decision to locate an economic activity of one or several countries is\nmade taking into account numerous parameters and criteria. Several studies have\nbeen carried out in this field, but they generally use information in a reduced\ncontext. The majority are based solely on parameters, using traditional methods\nwhich often lead to unsatisfactory solutions.This work consists in hybridizing\nthrough genetic algorithms, economic intelligence (EI) and multicriteria\nanalysis methods (MCA) to improve the decisions of territorial localization.\nThe purpose is to lead the company to locate its activity in the place that\nwould allow it a competitive advantage. This work also consists of identifying\nall the parameters that can influence the decision of the economic actors and\nequipping them with tools using all the national and international data\navailable to lead to a mapping of countries, regions or departments favorable\nto the location. Throughout our research, we have as a goal the realization of\na hybrid conceptual model of economic intelligence based on multicriteria on\nwith genetic algorithms in order to optimize the decisions of localization, in\nthis perspective we opted for the method of PROMETHEE (Preference Ranking\nOrganization for Method of Enrichment Evaluation), which has made it possible\nto obtain the best compromise between the various visions and various points of\nview.\n"
    },
    {
        "paper_id": 2002.04164,
        "authors": "Giuseppe Brandi and T. Di Matteo",
        "title": "On the statistics of scaling exponents and the Multiscaling Value at\n  Risk",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Scaling and multiscaling financial time series have been widely studied in\nthe literature. The research on this topic is vast and still flourishing. One\nway to analyze the scaling properties of time series is through the estimation\nof their scaling exponents, that are recognized as being valuable measures to\ndiscriminate between random, persistent, and anti-persistent behaviors in these\ntime series. In the literature, several methods have been proposed to study the\nmultiscaling property. In this paper, we use the generalized Hurst exponent\n(GHE) tool and we propose a novel statistical procedure based on GHE which we\nname Relative Normalized and Standardized Generalized Hurst Exponent (RNSGHE).\nThis method is used to robustly estimate and test the multiscaling property\nand, together with a combination of t-tests and F-tests, serves to discriminate\nbetween real and spurious scaling. Furthermore, we introduce a new tool to\nestimate the optimal aggregation time used in our methodology which we name\nAutocororrelation Segmented Regression. We numerically validate this procedure\non simulated time series by using the Multifractal Random Walk (MRW) and we\nthen apply it to real financial data. We present results for times series with\nand without anomalies and we compute the bias that such anomalies introduce in\nthe measurement of the scaling exponents. We also show how the use of proper\nscaling and multiscaling can ameliorate the estimation of risk measures such as\nValue at Risk (VaR). Finally, we propose a methodology based on Monte Carlo\nsimulation, which we name Multiscaling Value at Risk (MSVaR), that takes into\naccount the statistical properties of multiscaling time series. We show that by\nusing this statistical procedure in combination with the robustly estimated\nmultiscaling exponents, the one year forecasted MSVaR mimics the VaR on the\nannual data for the majority of the stocks analyzed.\n"
    },
    {
        "paper_id": 2002.04212,
        "authors": "Jack Sarkissian",
        "title": "Quantum coupled-wave theory of price formation in financial markets:\n  price measurement, dynamics and ergodicity",
        "comments": "Accepted for publication in Physica A (2020)",
        "journal-ref": null,
        "doi": "10.1016/j.physa.2020.124300",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We explore nature of price formation in financial markets and develop a\ntheory of bid and ask price dynamics in which the two prices form due to\nquantum-chaotic interaction between buy and sell orders. In this model bid and\nask prices are represented by eigenvalues of a 2x2 price operator corresponding\nto 'bid' and 'ask' eigenstates, while randomness of price operator results in\nprice fluctuations that destroy oscillatory effects. We show that this theory\nadequately captures behavior of bid-ask spread and allows to model bid and ask\nprice dynamics in a coordinated way. We also discuss ergodicity properties of\nprice formation and show how directional price movement occurs due to\nergodicity violation in a quantum process instead of the commonly believed\nforces acting on price. This theory has wide range of applications such as\ntrade execution modeling, large order pricing and risk valuation for illiquid\nsecurities.\n"
    },
    {
        "paper_id": 2002.04304,
        "authors": "Marc Rohloff, Alexander Vogt",
        "title": "Timing Excess Returns A cross-universe approach to alpha",
        "comments": "23 pages, 10 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We present a simple model that uses time series momentum in order to\nconstruct strategies that systematically outperform their benchmark. The\nsimplicity of our model is elegant: We only require a benchmark time series and\nseveral related investable indizes, not requiring regression or other models to\nestimate our parameters. We find that our one size fits all approach delivers\nsignificant outperformance in both equity and bond markets while meeting the\nex-ante risk requirements, nearly doubling yearly returns vs. the MSCI World\nand Bloomberg Barclays Euro Aggregate Corporate Bond benchmarks in a long-only\nbacktest. We then combine both approaches into an absolute return strategy by\nbenchmarking vs. the Eonia Total Return Index and find significant\noutperformance at a sharpe ratio of 1.8. Furthermore, we demonstrate that our\nmodel delivers a benefit versus a static portfolio with fixed mean weights,\nshowing that timing of excess return momentum has a sizeable benefit vs. static\nallocations. This also applies to the passively investable equity factors,\nwhere we outperform a static factor exposure portfolio with statistical\nsignificance. Also, we show that our model delivers an alpha after deducting\ntransaction costs.\n"
    },
    {
        "paper_id": 2002.04508,
        "authors": "Jean-Bernard Chatelain (PJSE), Kirsten Ralf",
        "title": "Ramsey Optimal Policy versus Multiple Equilibria with Fiscal and\n  Monetary Interactions",
        "comments": "Economics Bulletin, 2020",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We consider a frictionless constant endowment economy based on Leeper (1991).\nIn this economy, it is shown that, under an ad-hoc monetary rule and an ad-hoc\nfiscal rule, there are two equilibria. One has active monetary policy and\npassive fiscal policy, while the other has passive monetary policy and active\nfiscal policy. We consider an extended setup in which the policy maker\nminimizes a loss function under quasi-commitment, as in Schaumburg and\nTambalotti (2007). Under this formulation there exists a unique Ramsey\nequilibrium, with an interest rate peg and a passive fiscal policy. We thank\nJohn P. Conley, Luis de Araujo and one referree for their very helpful\ncomments.\n"
    },
    {
        "paper_id": 2002.04563,
        "authors": "Lucia Cipolina Kun, Simone Caenazzo, Ksenia Ponomareva",
        "title": "Mathematical Foundations of Regression Methods for the approximation of\n  the Forward Initial Margin",
        "comments": "Abridged version from the Journal of Derivatives",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Abundant literature has been published on approximation methods for the\nforward initial margin. The most popular ones being the family of regression\nmethods. This paper describes the mathematical foundations on which these\nregression approximation methods lie. We introduce mathematical rigor to show\nthat in essence, all the methods propose variations of approximations for the\nconditional expectation function, which is interpreted as an orthogonal\nprojection on Hilbert spaces. We show that each method is simply choosing a\ndifferent functional form to numerically estimate the conditional expectation.\nWe cover in particular the most popular methods in the literature so far,\nPolynomial approximation, Kernel regressions and Neural Networks.\n"
    },
    {
        "paper_id": 2002.04675,
        "authors": "Walter Farkas, Ludovic Mathys, Nikola Vasiljevi\\'c",
        "title": "Intra-Horizon Expected Shortfall and Risk Structure in Models with Jumps",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The present article deals with intra-horizon risk in models with jumps. Our\ngeneral understanding of intra-horizon risk is along the lines of the approach\ntaken in Boudoukh, Richardson, Stanton and Whitelaw (2004), Rossello (2008),\nBhattacharyya, Misra and Kodase (2009), Bakshi and Panayotov (2010), and\nLeippold and Vasiljevi\\'c (2019). In particular, we believe that quantifying\nmarket risk by strictly relying on point-in-time measures cannot be deemed a\nsatisfactory approach in general. Instead, we argue that complementing this\napproach by studying measures of risk that capture the magnitude of losses\npotentially incurred at any time of a trading horizon is necessary when dealing\nwith (m)any financial position(s). To address this issue, we propose an\nintra-horizon analogue of the expected shortfall for general profit and loss\nprocesses and discuss its key properties. Our intra-horizon expected shortfall\nis well-defined for (m)any popular class(es) of L\\'evy processes encountered\nwhen modeling market dynamics and constitutes a coherent measure of risk, as\nintroduced in Cheridito, Delbaen and Kupper (2004). On the computational side,\nwe provide a simple method to derive the intra-horizon risk inherent to popular\nL\\'evy dynamics. Our general technique relies on results for\nmaturity-randomized first-passage probabilities and allows for a derivation of\ndiffusion and single jump risk contributions. These theoretical results are\ncomplemented with an empirical analysis, where popular L\\'evy dynamics are\ncalibrated to S&P 500 index data and an analysis of the resulting intra-horizon\nrisk is presented.\n"
    },
    {
        "paper_id": 2002.04832,
        "authors": "Bal\\'azs Gerencs\\'er and Mikl\\'os R\\'asonyi",
        "title": "Invariant measures for multidimensional fractional stochastic volatility\n  models",
        "comments": "Generalized to multiple dimensions",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We establish convergence to an invariant measure as time tends to infinity,\nfor a large class of (possibly non-Markovian) stochastic volatility models. Our\narguments are based on a novel coupling idea for Markov chains which also\nextends to Markov chains in random environments in an efficient way.\n"
    },
    {
        "paper_id": 2002.04886,
        "authors": "M. Gietzmann, A. J. Ostaszewski, M. H. G. Schr\\\"oder",
        "title": "Guiding the guiders: Foundations of a market-driven theory of disclosure",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  A foundational approach is developed for a mathematical theory of managerial\ndisclosure in relation to asset pricing; this involves both the earnings\nguidance disclosed by firm management and market `trackers' pricing the firm's\nexposure to quotable risks.\n"
    },
    {
        "paper_id": 2002.05016,
        "authors": "Luca Guerrini, Adam Krawiec, Marek Szydlowski",
        "title": "Bifurcations in economic growth model with distributed time delay\n  transformed to ODE",
        "comments": "20 pages, 11 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We consider the model of economic growth with time delayed investment\nfunction. Assuming the investment is time distributed we can use the linear\nchain trick technique to transform delay differential equation system to\nequivalent system of ordinary differential system (ODE). The time delay\nparameter is a mean time delay of gamma distribution. We reduce the system with\ndistribution delay to both three and four-dimensional ODEs. We study the Hopf\nbifurcation in these systems with respect to two parameters: the time delay\nparameter and the rate of growth parameter. We derive the results from the\nanalytical as well as numerical investigations. From the former we obtain the\nsufficient criteria on the existence and stability of a limit cycle solution\nthrough the Hopf bifurcation. In numerical studies with the Dana and Malgrange\ninvestment function we found two Hopf bifurcations with respect to the rate\ngrowth parameter and detect the existence of stable long-period cycles in the\neconomy. We find that depending on the time delay and adjustment speed\nparameters the range of admissible values of the rate of growth parameter\nbreaks down into three intervals. First we have stable focus, then the limit\ncycle and again the stable solution with two Hopf bifurcations. Such behaviour\nappears for some middle interval of admissible range of values of the rate of\ngrowth parameter.\n"
    },
    {
        "paper_id": 2002.05143,
        "authors": "Archil Gulisashvili",
        "title": "Time-inhomogeneous Gaussian stochastic volatility models: Large\n  deviations and super roughness",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We introduce time-inhomogeneous stochastic volatility models, in which the\nvolatility is described by a nonnegative function of a Volterra type continuous\nGaussian process that may have very rough sample paths. The main results\nobtained in the paper are sample path and small-noise large deviation\nprinciples for the log-price process in a time-inhomogeneous super rough\nGaussian model under very mild restrictions. We use these results to study the\nasymptotic behavior of binary barrier options, exit time probability functions,\nand call options.\n"
    },
    {
        "paper_id": 2002.05209,
        "authors": "T. Brown, L. Reichenberg",
        "title": "Decreasing market value of variable renewables can be avoided by policy\n  action",
        "comments": "24 pages, 29 figures; accepted to Energy Economics",
        "journal-ref": "Energy Economics, Volume 100, August 2021, 105354",
        "doi": "10.1016/j.eneco.2021.105354",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Although recent studies have shown that electricity systems with shares of\nwind and solar above 80% can be affordable, economists have raised concerns\nabout market integration. Correlated generation from variable renewable sources\ndepresses market prices, which can cause wind and solar to cannibalise their\nown revenues and prevent them from covering their costs from the market. This\ncannibalisation appears to set limits on the integration of wind and solar, and\nthus to contradict studies that show that high shares are cost effective. Here\nwe show from theory and with simulation examples how market incentives interact\nwith prices, revenue and costs for renewable electricity systems. The decline\nin average revenue seen in some recent literature is due to an implicit policy\nassumption that technologies are forced into the system, whether it be with\nsubsidies or quotas. This decline is mathematically guaranteed regardless of\nwhether the subsidised technology is variable or not. If instead the driving\npolicy is a carbon dioxide cap or tax, wind and solar shares can rise without\ncannibalising their own market revenue, even at penetrations of wind and solar\nabove 80%. The strong dependence of market value on the policy regime means\nthat market value needs to be used with caution as a measure of market\nintegration. Declining market value is not necessarily a sign of integration\nproblems, but rather a result of policy choices.\n"
    },
    {
        "paper_id": 2002.05232,
        "authors": "Ankush Agarwal, Christian-Oliver Ewald and Yongjie Wang",
        "title": "Sharing of longevity basis risk in pension schemes with income-drawdown\n  guarantees",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This work studies a stochastic optimal control problem for a pension scheme\nwhich provides an income-drawdown policy to its members after their retirement.\nTo manage the scheme efficiently, the manager and members agree to share the\ninvestment risk based on a pre-decided risk-sharing rule. The objective is to\nmaximise both sides' utilities by controlling the manager's investment in risky\nassets and members' benefit withdrawals. We use stochastic affine class models\nto describe the force of mortality of the members' population and consider a\nlongevity bond whose coupon payment is linked to a survival index. In our\nframework, we also investigate the longevity basis risk, which arises when the\nmembers' and the longevity bond's reference populations show different\nmortality behaviours. By applying the dynamic programming principle to solve\nthe corresponding HJB equations, we derive optimal solutions for the single-\nand sub-population cases. Our numerical results show that by sharing the risk,\nboth manager and members increase their utility. Moreover, even in the presence\nof longevity basis risk, we demonstrate that the longevity bond acts as an\neffective hedging instrument.\n"
    },
    {
        "paper_id": 2002.05319,
        "authors": "Oscar Espinosa, Fabio Nieto",
        "title": "A study on the leverage effect on financial series using a TAR model: a\n  Bayesian approach",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This research shows that under certain mathematical conditions, a threshold\nautoregressive model (TAR) can represent the leverage effect based on its\nconditional variance function. Furthermore, the analytical expressions for the\nthird and fourth moment of the TAR model are obtained when it is weakly\nstationary.\n"
    },
    {
        "paper_id": 2002.05323,
        "authors": "Federico Echenique and Ruy Gonzalez and Alistair Wilson and Leeat\n  Yariv",
        "title": "Top of the Batch: Interviews and the Match",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Most doctors in the NRMP are matched to one of their most-preferred\ninternship programs. Since various surveys indicate similarities across\ndoctors' preferences, this suggests a puzzle. How can nearly everyone get a\nposition in a highly-desirable program when positions in each program are\nscarce? We provide one possible explanation for this puzzle. We show that the\npatterns observed in the NRMP data may be an artifact of the interview process\nthat precedes the match. Our analysis highlights the importance of interactions\noccurring outside of a matching clearinghouse for resulting outcomes, and casts\ndoubts on analysis of clearinghouses that take reported preferences at face\nvalue.\n"
    },
    {
        "paper_id": 2002.05571,
        "authors": "S\\\"oren Christensen and Jan Kallsen and Matthias Lenga",
        "title": "Are American options European after all?",
        "comments": "45 pages, 6 figures",
        "journal-ref": "Ann. Appl. Probab. 32(2): 853-892 (April 2022)",
        "doi": "10.1214/21-AAP1698",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We call a given American option representable if there exists a European\nclaim which dominates the American payoff at any time and such that the values\nof the two options coincide in the continuation region of the American option.\nThis concept has interesting implications from a probabilistic, analytic,\nfinancial, and numeric point of view. Relying on methods from Jourdain and\nMartini (2001, 2002), Chrsitensen (2014) and convex duality, we make a first\nstep towards verifying representability of American options.\n"
    },
    {
        "paper_id": 2002.05697,
        "authors": "L\\'ester Alfonso, Danahe E. Garcia-Ramirez, Ricardo Mansilla, C\\'esar\n  A. Terrero-Escalante",
        "title": "Analysis of intra-day fluctuations in the Mexican financial market index",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper, a statistical analysis of high frequency fluctuations of the\nIPC, the Mexican Stock Market Index, is presented. A sample of tick-to-tick\ndata covering the period from January 1999 to December 2002 was analyzed, as\nwell as several other sets obtained using temporal aggregation. Our results\nindicates that the highest frequency is not useful to understand the Mexican\nmarket because almost two thirds of the information corresponds to inactivity.\nFor the frequency where fluctuations start to be relevant, the IPC data does\nnot follows any alpha-stable distribution, including the Gaussian, perhaps\nbecause of the presence of autocorrelations. For a long range of\nlower-frequencies, but still in the intra-day regime, fluctuations can be\ndescribed as a truncated L\\'evy flight, while for frequencies above two-days, a\nGaussian distribution yields the best fit. Thought these results are consistent\nwith other previously reported for several markets, there are significant\ndifferences in the details of the corresponding descriptions.\n"
    },
    {
        "paper_id": 2002.0578,
        "authors": "Yunan Ye, Hengzhi Pei, Boxin Wang, Pin-Yu Chen, Yada Zhu, Jun Xiao, Bo\n  Li",
        "title": "Reinforcement-Learning based Portfolio Management with Augmented Asset\n  Movement Prediction States",
        "comments": "AAAI 2020",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Portfolio management (PM) is a fundamental financial planning task that aims\nto achieve investment goals such as maximal profits or minimal risks. Its\ndecision process involves continuous derivation of valuable information from\nvarious data sources and sequential decision optimization, which is a\nprospective research direction for reinforcement learning (RL). In this paper,\nwe propose SARL, a novel State-Augmented RL framework for PM. Our framework\naims to address two unique challenges in financial PM: (1) data heterogeneity\n-- the collected information for each asset is usually diverse, noisy and\nimbalanced (e.g., news articles); and (2) environment uncertainty -- the\nfinancial market is versatile and non-stationary. To incorporate heterogeneous\ndata and enhance robustness against environment uncertainty, our SARL augments\nthe asset information with their price movement prediction as additional\nstates, where the prediction can be solely based on financial data (e.g., asset\nprices) or derived from alternative sources such as news. Experiments on two\nreal-world datasets, (i) Bitcoin market and (ii) HighTech stock market with\n7-year Reuters news articles, validate the effectiveness of SARL over existing\nPM approaches, both in terms of accumulated profits and risk-adjusted profits.\nMoreover, extensive simulations are conducted to demonstrate the importance of\nour proposed state augmentation, providing new insights and boosting\nperformance significantly over standard RL-based PM method and other baselines.\n"
    },
    {
        "paper_id": 2002.05784,
        "authors": "Lior Sidi",
        "title": "Improving S&P stock prediction with time series stock similarity",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
        "abstract": "  Stock market prediction with forecasting algorithms is a popular topic these\ndays where most of the forecasting algorithms train only on data collected on a\nparticular stock. In this paper, we enriched the stock data with related stocks\njust as a professional trader would have done to improve the stock prediction\nmodels. We tested five different similarities functions and found\nco-integration similarity to have the best improvement on the prediction model.\nWe evaluate the models on seven S&P stocks from various industries over five\nyears period. The prediction model we trained on similar stocks had\nsignificantly better results with 0.55 mean accuracy, and 19.782 profit compare\nto the state of the art model with an accuracy of 0.52 and profit of 6.6.\n"
    },
    {
        "paper_id": 2002.05785,
        "authors": "Abhijit Chakraborty, Hiroyasu Inoue, Yoshi Fujiwara",
        "title": "Economic complexity of prefectures in Japan",
        "comments": "21 pages, 8 figures",
        "journal-ref": "PLoS ONE 15(8): e0238017 (2020)",
        "doi": "10.1371/journal.pone.0238017",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Every nation prioritizes the inclusive economic growth and development of all\nregions. However, we observe that economic activities are clustered in space,\nwhich results in a disparity in per-capita income among different regions. A\ncomplexity-based method was proposed by Hidalgo and Hausmann [PNAS 106,\n10570-10575 (2009)] to explain the large gaps in per-capita income across\ncountries. Although there have been extensive studies on countries' economic\ncomplexity using international export data, studies on economic complexity at\nthe regional level are relatively less studied. Here, we study the industrial\nsector complexity of prefectures in Japan based on the basic information of\nmore than one million firms. We aggregate the data as a bipartite network of\nprefectures and industrial sectors. We decompose the bipartite network as a\nprefecture-prefecture network and sector-sector network, which reveals the\nrelationships among them. Similarities among the prefectures and among the\nsectors are measured using a metric. From these similarity matrices, we cluster\nthe prefectures and sectors using the minimal spanning tree technique.The\ncomputed economic complexity index from the structure of the bipartite network\nshows a high correlation with macroeconomic indicators, such as per-capita\ngross prefectural product and prefectural income per person. We argue that this\nindex reflects the present economic performance and hidden potential of the\nprefectures for future growth.\n"
    },
    {
        "paper_id": 2002.05786,
        "authors": "Ahmet Murat Ozbayoglu, Mehmet Ugur Gudelek, Omer Berat Sezer",
        "title": "Deep Learning for Financial Applications : A Survey",
        "comments": "13 Figures, 15 Tables, submitted to Applied Soft Computing",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Computational intelligence in finance has been a very popular topic for both\nacademia and financial industry in the last few decades. Numerous studies have\nbeen published resulting in various models. Meanwhile, within the Machine\nLearning (ML) field, Deep Learning (DL) started getting a lot of attention\nrecently, mostly due to its outperformance over the classical models. Lots of\ndifferent implementations of DL exist today, and the broad interest is\ncontinuing. Finance is one particular area where DL models started getting\ntraction, however, the playfield is wide open, a lot of research opportunities\nstill exist. In this paper, we tried to provide a state-of-the-art snapshot of\nthe developed DL models for financial applications, as of today. We not only\ncategorized the works according to their intended subfield in finance but also\nanalyzed them based on their DL models. In addition, we also aimed at\nidentifying possible future implementations and highlighted the pathway for the\nongoing research within the field.\n"
    },
    {
        "paper_id": 2002.05789,
        "authors": "Taco de Wolff, Alejandro Cuevas, Felipe Tobar",
        "title": "Gaussian process imputation of multiple financial series",
        "comments": "Accepted at IEEE ICASSP 2020",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In Financial Signal Processing, multiple time series such as financial\nindicators, stock prices and exchange rates are strongly coupled due to their\ndependence on the latent state of the market and therefore they are required to\nbe jointly analysed. We focus on learning the relationships among financial\ntime series by modelling them through a multi-output Gaussian process (MOGP)\nwith expressive covariance functions. Learning these market dependencies among\nfinancial series is crucial for the imputation and prediction of financial\nobservations. The proposed model is validated experimentally on two real-world\nfinancial datasets for which their correlations across channels are analysed.\nWe compare our model against other MOGPs and the independent Gaussian process\non real financial data.\n"
    },
    {
        "paper_id": 2002.05791,
        "authors": "Daniel Bj\\\"orkegren and Burak Ceyhun Karaca",
        "title": "The Effect of Network Adoption Subsidies: Evidence from Digital Traces\n  in Rwanda",
        "comments": null,
        "journal-ref": "Journal of Development Economics 154 (2022)",
        "doi": "10.1016/j.jdeveco.2021.102762",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Governments spend billions of dollars subsidizing the adoption of different\ngoods. However, it is difficult to gauge whether those goods are resold, or are\nvalued by their ultimate recipients. This project studies a program to\nsubsidize the adoption of mobile phones in one of the poorest countries in the\nworld. Rwanda subsidized the equivalent of 8% of the stock of mobile phones for\nselect rural areas. We analyze the program using 5.3 billion transaction\nrecords from the dominant mobile phone network. Transaction records reveal\nwhere and how much subsidized handsets were ultimately used, and indicators of\nresale. Some subsidized handsets drifted from the rural areas where they were\nallocated to urban centers, but the subsidized handsets were used as much as\nhandsets purchased at retail prices, suggesting they were valued. Recipients\nare similar to those who paid for phones, but are highly connected to each\nother. We then simulate welfare effects using a network demand system that\naccounts for how each person's adoption affects the rest of the network.\nSpillovers are substantial: 73-76% of the operator revenue generated by the\nsubsidy comes from nonrecipients. We compare the enacted subsidy program to\ncounterfactual targeting based on different network heuristics.\n"
    },
    {
        "paper_id": 2002.06227,
        "authors": "Antonis Papapantoleon and Paulo Yanez Sarmiento",
        "title": "Detection of arbitrage opportunities in multi-asset derivatives markets",
        "comments": "25 pages, 4 figures. Forthcoming in Dependence Modeling",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We are interested in the existence of equivalent martingale measures and the\ndetection of arbitrage opportunities in markets where several multi-asset\nderivatives are traded simultaneously. More specifically, we consider a\nfinancial market with multiple traded assets whose marginal risk-neutral\ndistributions are known, and assume that several derivatives written on these\nassets are traded simultaneously. In this setting, there is a bijection between\nthe existence of an equivalent martingale measure and the existence of a copula\nthat couples these marginals. Using this bijection and recent results on\nimproved Fr\\'echet-Hoeffding bounds in the presence of additional information\non functionals of a copula by Lux and Papapantoleon [18], we can extend the\nresults of Tavin [33] on the detection of arbitrage opportunities to the\ngeneral multi-dimensional case. More specifically, we derive sufficient\nconditions for the absence of arbitrage and formulate an optimization problem\nfor the detection of a possible arbitrage opportunity. This problem can be\nsolved efficiently using numerical optimization routines. The most interesting\npractical outcome is the following: we can construct a financial market where\neach multi-asset derivative is traded within its own no-arbitrage interval, and\nyet when considered together an arbitrage opportunity may arise.\n"
    },
    {
        "paper_id": 2002.06243,
        "authors": "Yusuke Uchiyama, Kei Nakagawa",
        "title": "TPLVM: Portfolio Construction by Student's $t$-process Latent Variable\n  Model",
        "comments": null,
        "journal-ref": null,
        "doi": "10.3390/math8030449",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Optimal asset allocation is a key topic in modern finance theory. To realize\nthe optimal asset allocation on investor's risk aversion, various portfolio\nconstruction methods have been proposed. Recently, the applications of machine\nlearning are rapidly growing in the area of finance. In this article, we\npropose the Student's $t$-process latent variable model (TPLVM) to describe\nnon-Gaussian fluctuations of financial timeseries by lower dimensional latent\nvariables. Subsequently, we apply the TPLVM to minimum-variance portfolio as an\nalternative of existing nonlinear factor models. To test the performance of the\nproposed portfolio, we construct minimum-variance portfolios of global stock\nmarket indices based on the TPLVM or Gaussian process latent variable model. By\ncomparing these portfolios, we confirm the proposed portfolio outperforms that\nof the existing Gaussian process latent variable model.\n"
    },
    {
        "paper_id": 2002.06253,
        "authors": "Assaf Libman",
        "title": "Polytopes associated with lattices of subsets and maximising expectation\n  of random variables",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The present paper originated from a problem in Financial Mathematics\nconcerned with calculating the value of a European call option based on\nmultiple assets each following the binomial model. The model led to an\ninteresting family of polytopes $P(b)$ associated with the power-set\n$\\mathcal{L} = \\wp\\{1,\\dots,m\\}$ and parameterized by $b \\in \\mathbb{R}^m$,\neach of which is a collection of probability density function on $\\mathcal{L}$.\nFor each non-empty $P(b)$ there results a family of probability measures on\n$\\mathcal{L}^n$ and, given a function $F \\colon \\mathcal{L}^n \\to \\mathbb{R}$,\nour goal is to find among these probability measures one which maximises (resp.\nminimises) the expectation of $F$. In this paper we identify a family of such\nfunctions $F$, all of whose expectations are maximised (resp. minimised under\nsome conditions) by the same {\\em product} probability measure defined by a\ndistinguished vertex of $P(b)$ called the supervertex (resp. the subvertex).\nThe pay-offs of European call options belong to this family of functions.\n"
    },
    {
        "paper_id": 2002.06405,
        "authors": "Oksana Bashchenko and Alexis Marchal",
        "title": "Deep Learning for Asset Bubbles Detection",
        "comments": "25 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We develop a methodology for detecting asset bubbles using a neural network.\nWe rely on the theory of local martingales in continuous-time and use a deep\nnetwork to estimate the diffusion coefficient of the price process more\naccurately than the current estimator, obtaining an improved detection of\nbubbles. We show the outperformance of our algorithm over the existing\nstatistical method in a laboratory created with simulated data. We then apply\nthe network classification to real data and build a zero net exposure trading\nstrategy that exploits the risky arbitrage emanating from the presence of\nbubbles in the US equity market from 2006 to 2008. The profitability of the\nstrategy provides an estimation of the economical magnitude of bubbles as well\nas support for the theoretical assumptions relied on.\n"
    },
    {
        "paper_id": 2002.06555,
        "authors": "Marco Pangallo",
        "title": "Synchronization of endogenous business cycles",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Business cycles are positively correlated (``comove'') across countries.\nHowever, standard models that attribute comovement to propagation of exogenous\nshocks struggle to generate a level of comovement that is as high as in the\ndata. In this paper, we consider models that produce business cycles\nendogenously, through some form of non-linear dynamics -- limit cycles or\nchaos. These models generate stronger comovement, because they combine shock\npropagation with synchronization of endogenous dynamics. In particular, we\nstudy a demand-driven model in which business cycles emerge from strategic\ncomplementarities within countries, synchronizing their oscillations through\ninternational trade linkages. We develop an eigendecomposition that explores\nthe interplay between non-linear dynamics, shock propagation and network\nstructure, and use this theory to understand the mechanisms of synchronization.\nNext, we calibrate the model to data on 24 countries and show that the\nempirical level of comovement can only be matched by combining endogenous\nbusiness cycles with exogenous shocks. Our results lend support to the\nhypothesis that business cycles are at least in part caused by underlying\nnon-linear dynamics.\n"
    },
    {
        "paper_id": 2002.06878,
        "authors": "Chi Chen, Li Zhao, Wei Cao, Jiang Bian and Chunxiao Xing",
        "title": "Trimming the Sail: A Second-order Learning Paradigm for Stock Prediction",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Nowadays, machine learning methods have been widely used in stock prediction.\nTraditional approaches assume an identical data distribution, under which a\nlearned model on the training data is fixed and applied directly in the test\ndata. Although such assumption has made traditional machine learning techniques\nsucceed in many real-world tasks, the highly dynamic nature of the stock market\ninvalidates the strict assumption in stock prediction. To address this\nchallenge, we propose the second-order identical distribution assumption, where\nthe data distribution is assumed to be fluctuating over time with certain\npatterns. Based on such assumption, we develop a second-order learning paradigm\nwith multi-scale patterns. Extensive experiments on real-world Chinese stock\ndata demonstrate the effectiveness of our second-order learning paradigm in\nstock prediction.\n"
    },
    {
        "paper_id": 2002.06975,
        "authors": "Masaya Abe, Kei Nakagawa",
        "title": "Cross-sectional Stock Price Prediction using Deep Learning for Actual\n  Investment Management",
        "comments": "accepted for publication in 2020 International Artificial\n  Intelligence and Blockchain Conference (AIBC 2020)",
        "journal-ref": null,
        "doi": "10.1145/3399871.3399889",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Stock price prediction has been an important research theme both academically\nand practically. Various methods to predict stock prices have been studied\nuntil now. The feature that explains the stock price by a cross-section\nanalysis is called a \"factor\" in the field of finance. Many empirical studies\nin finance have identified which stocks having features in the cross-section\nrelatively increase and which decrease in terms of price. Recently, stock price\nprediction methods using machine learning, especially deep learning, have been\nproposed since the relationship between these factors and stock prices is\ncomplex and non-linear. However, there are no practical examples for actual\ninvestment management. In this paper, therefore, we present a cross-sectional\ndaily stock price prediction framework using deep learning for actual\ninvestment management. For example, we build a portfolio with information\navailable at the time of market closing and invest at the time of market\nopening the next day. We perform empirical analysis in the Japanese stock\nmarket and confirm the profitability of our framework.\n"
    },
    {
        "paper_id": 2002.071,
        "authors": "C\\'elestin Coquid\\'e, Jos\\'e Lages and Dima L. Shepelyansky",
        "title": "Crisis contagion in the world trade network",
        "comments": "Main article: 19 pages, 9 figures. Supplementary files: 7 pages, 2\n  figures, 4 tables, 1 video. Datasets, results, supplementary information, and\n  high quality figures available at this\n  http://perso.utinam.cnrs.fr/~lages/datasets/WTNcrisis/",
        "journal-ref": "Appl Netw Sci 5, 67 (2020)",
        "doi": "10.1007/s41109-020-00304-z",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We present a model of worldwide crisis contagion based on the Google matrix\nanalysis of the world trade network obtained from the UN Comtrade database. The\nfraction of bankrupted countries exhibits an \\textit{on-off} phase transition\ngoverned by a bankruptcy threshold $\\kappa$ related to the trade balance of the\ncountries. For $\\kappa>\\kappa_c$, the contagion is circumscribed to less than\n10\\% of the countries, whereas, for $\\kappa<\\kappa_c$, the crisis is global\nwith about 90\\% of the countries going to bankruptcy. We measure the total cost\nof the crisis during the contagion process. In addition to providing contagion\nscenarios, our model allows to probe the structural trading dependencies\nbetween countries. For different networks extracted from the world trade\nexchanges of the last two decades, the global crisis comes from the Western\nworld. In particular, the source of the global crisis is systematically the Old\nContinent and The Americas (mainly US and Mexico). Besides the economy of\nAustralia, those of Asian countries, such as China, India, Indonesia, Malaysia\nand Thailand, are the last to fall during the contagion. Also, the four BRIC\nare among the most robust countries to the world trade crisis.\n"
    },
    {
        "paper_id": 2002.07116,
        "authors": "Dahang Li",
        "title": "A New Pricing Theory That Solves the St. Petersburg Paradox",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The St. Petersburg Paradox, an important topic in probability theory, has not\nbeen solved in the last 280 years. Since Nicolaus Bernoulli proposed the St.\nPetersburg Paradox in 1738, many people had tried to solve it and had proposed\nvarious explanations, but all were not satisfactory. In this paper we propose a\nnew pricing theory with several rules, which incidentally resolves this\nparadox. The new pricing theory states that so-called fair (reasonable) pricing\nshould be judged by the seller and the buyer independently. Reasonable pricing\nfor the seller may not be appropriate for the buyer. The seller cares about\ncosts, while the buyer is concerned about the realistic prospect of returns.The\npricing theory we proposed can be applied to financial markets to solve the\nconfusion that financial asset return with fat tails distribution will cause\nthe option pricing formula to fail, thus making up the theoretical defects of\nquantitative financial pricing theory.\n"
    },
    {
        "paper_id": 2002.07117,
        "authors": "Pablo Olivares",
        "title": "Pricing Bitcoin Derivatives under Jump-Diffusion Models",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/publicdomain/zero/1.0/",
        "abstract": "  In recent years cryptocurrency trading has captured the attention of\npractitioners and academics. The volume of the exchange with standard\ncurrencies has known a dramatic increasing of late. This paper addresses to the\nneed of models describing a bitcoin-US dollar exchange dynamic and their use to\nevaluate European option having bitcoin as underlying asset.\n"
    },
    {
        "paper_id": 2002.07163,
        "authors": "Olha Danylo, Johannes Pirker, Guido Lemoine, Guido Ceccherini, Linda\n  See, Ian McCallum, Hadi, Florian Kraxner, Fr\\'ed\\'eric Achard, Steffen Fritz",
        "title": "Satellite reveals age and extent of oil palm plantations in Southeast\n  Asia",
        "comments": "25 pages, 6 figures and tables",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-sa/4.0/",
        "abstract": "  In recent decades, global oil palm production has shown an abrupt increase,\nwith almost 90% produced in Southeast Asia alone. Monitoring oil palm is\nlargely based on national surveys and inventories or one-off mapping studies.\nHowever, they do not provide detailed spatial extent or timely updates and\ntrends in oil palm expansion or age. Palm oil yields vary significantly with\nplantation age, which is critical for landscape-level planning. Here we show\nthe extent and age of oil palm plantations for the year 2017 across Southeast\nAsia using remote sensing. Satellites reveal a total of 11.66 (+/- 2.10)\nmillion hectares (Mha) of plantations with more than 45% located in Sumatra.\nPlantation age varies from ~7 years in Kalimantan to ~13 in Insular Malaysia.\nMore than half the plantations on Kalimantan are young (<7 years) and not yet\nin full production compared to Insular Malaysia where 45% of plantations are\nolder than 15 years, with declining yields. For the first time, these results\nprovide a consistent, independent, and transparent record of oil palm\nplantation extent and age structure, which are complementary to national\nstatistics.\n"
    },
    {
        "paper_id": 2002.07229,
        "authors": "Kieran Marray (1 and 2), Nikhil Krishna (3), and Jarel Tang (4) ((1)\n  Tinbergen Institute, (2) Institute for New Economic Thinking at the Oxford\n  Martin School, University of Oxford, (3) Trinity College, University of\n  Oxford, (4) The Queen's College, University of Oxford)",
        "title": "How Do Expectations Affect Learning About Fundamentals? Some\n  Experimental Evidence",
        "comments": "29 pages, 21 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We test how individuals with incorrect beliefs about their ability learn\nabout an external parameter (`fundamental') when they cannot separately\nidentify the effects of their ability, actions, and the parameter on their\noutput. Heidhues et al. (2018) argue that learning makes overconfident\nindividuals worse off as their beliefs about the fundamental get less accurate,\ncausing them to take worse actions. In our experiment, subjects take\nincorrectly-marked tests, and we measure how they learn about the marker's\naccuracy over time. Overconfident subjects put in less effort, and their\nbeliefs about the marker's accuracy got worse, as they learnt. Beliefs about\nthe proportion of correct answers marked as correct fell by 0.05 over the\nexperiment. We find no effect in underconfident subjects.\n"
    },
    {
        "paper_id": 2002.07389,
        "authors": "Janusz Milek",
        "title": "Quantum Implementation of Risk Analysis-relevant Copulas",
        "comments": "15 pages, 32+10 figures, 4+1 tables. Changes in v.2: updated\n  references p. 2 and 14-15, typo correction p. 6, quantum circuit diagrams\n  improved for gray print",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Modern quantitative risk management relies on an adequate modeling of the\ntail dependence and a possibly accurate quantification of risk measures, like\nValue at Risk (VaR), at high confidence levels like 1 in 100 or even 1 in 2000.\nQuantum computing makes such a quantification quadratically more efficient than\nthe Monte Carlo method; see (Woerner and Egger, 2018) and, for a broader\nperspective, (Or\\'us et al., 2018). An important element of the risk analysis\ntoolbox is copula, see (Jouanin et al., 2004) regarding financial applications.\nHowever, to the best knowledge of the author, no quantum computing\nimplementation for sampling from a risk modeling-relevant copula in explicit\nform has been published so far. Our focus here is implementation of simple yet\npowerful copula models, capable of a satisfactory capturing the joint tail\nbehaviour of the modelled risk factors. This paper deals with a few simple\ncopula families, including Multivariate B11 (MB11) copula family, presented in\n(Milek, 2014). We will show that this copula family is suitable for the risk\naggregation as it is exceptionally able to reproduce tail dependence\nstructures; see (Embrechts et al., 2016) for a relevant benchmark as well as\nnecessary and sufficient conditions regarding the ultimate feasible bivariate\ntail dependence structures. It turns out that such a discretized copula can be\nexpressed using simple constructs present in the quantum computing: binary\nfraction expansion format, comonotone/independent random variables, controlled\ngates, and convex combinations, and is therefore suitable for a quantum\ncomputer implementation. This paper presents design behind the quantum\nimplementation circuits, numerical and symbolic simulation results, and\nexperimental validation on IBM quantum computer. The paper proposes also a\ngeneric method for quantum implementation of any discretized copula.\n"
    },
    {
        "paper_id": 2002.07477,
        "authors": "Carmine de Franco, Christophe Geissler, Vincent Margot, Bruno Monnier",
        "title": "ESG investments: Filtering versus machine learning approaches",
        "comments": null,
        "journal-ref": "The Seventh Public Investors Conference, Oct 2018, Rome, Italy",
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We designed a machine learning algorithm that identifies patterns between ESG\nprofiles and financial performances for companies in a large investment\nuniverse. The algorithm consists of regularly updated sets of rules that map\nregions into the high-dimensional space of ESG features to excess return\npredictions. The final aggregated predictions are transformed into scores which\nallow us to design simple strategies that screen the investment universe for\nstocks with positive scores. By linking the ESG features with financial\nperformances in a non-linear way, our strategy based upon our machine learning\nalgorithm turns out to be an efficient stock picking tool, which outperforms\nclassic strategies that screen stocks according to their ESG ratings, as the\npopular best-in-class approach. Our paper brings new ideas in the growing field\nof financial literature that investigates the links between ESG behavior and\nthe economy. We show indeed that there is clearly some form of alpha in the ESG\nprofile of a company, but that this alpha can be accessed only with powerful,\nnon-linear techniques such as machine learning.\n"
    },
    {
        "paper_id": 2002.07561,
        "authors": "Annika Kemper, Maren D. Schmeck, and Anna Kh. Balci",
        "title": "The Market Price of Risk for Delivery Periods: Pricing Swaps and Options\n  in Electricity Markets",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In electricity markets, futures contracts typically function as a swap since\nthey deliver the underlying over a period of time. In this paper, we introduce\na market price for the delivery periods of electricity swaps, thereby opening\nan arbitrage-free pricing framework for derivatives based on these contracts.\nFurthermore, we use a weighted geometric averaging of an artificial geometric\nfutures price over the corresponding delivery period. Without any need for\napproximations, this averaging results in geometric swap price dynamics. Our\nframework allows for including typical features as the Samuelson effect,\nseasonalities, and stochastic volatility. In particular, we investigate the\npricing procedures for electricity swaps and options in line with Arismendi et\nal. (2016), Schneider and Tavin (2018), and Fanelli and Schmeck (2019). A\nnumerical study highlights the differences between these models depending on\nthe delivery period.\n"
    },
    {
        "paper_id": 2002.07566,
        "authors": "P\\'al Andr\\'as Papp, Roger Wattenhofer",
        "title": "Network-Aware Strategies in Financial Systems",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We study the incentives of banks in a financial network, where the network\nconsists of debt contracts and credit default swaps (CDSs) between banks. One\nof the most important questions in such a system is the problem of deciding\nwhich of the banks are in default, and how much of their liabilities these\nbanks can pay. We study the payoff and preferences of the banks in the\ndifferent solutions to this problem. We also introduce a more refined model\nwhich allows assigning priorities to payment obligations; this provides a more\nexpressive and realistic model of real-life financial systems, while it always\nensures the existence of a solution.\n  The main focus of the paper is an analysis of the actions that a single bank\ncan execute in a financial system in order to influence the outcome to its\nadvantage. We show that removing an incoming debt, or donating funds to another\nbank can result in a single new solution that is strictly more favorable to the\nacting bank. We also show that increasing the bank's external funds or\nmodifying the priorities of outgoing payments cannot introduce a more favorable\nnew solution into the system, but may allow the bank to remove some unfavorable\nsolutions, or to increase its recovery rate. Finally, we show how the actions\nof two banks in a simple financial system can result in classical game\ntheoretic situations like the prisoner's dilemma or the dollar auction,\ndemonstrating the wide expressive capability of the financial system model.\n"
    },
    {
        "paper_id": 2002.07595,
        "authors": "Jian Sun and Chenye Wu",
        "title": "Market Power in Convex Hull Pricing",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The start up costs in many kinds of generators lead to complex cost\nstructures, which in turn yield severe market loopholes in the locational\nmarginal price (LMP) scheme. Convex hull pricing (a.k.a. extended LMP) is\nproposed to improve the market efficiency by providing the minimal uplift\npayment to the generators. In this letter, we consider a stylized model where\nall generators share the same generation capacity. We analyze the generators'\npossible strategic behaviors in such a setting, and then propose an index for\nmarket power quantification in the convex hull pricing schemes.\n"
    },
    {
        "paper_id": 2002.07741,
        "authors": "P\\'al Andr\\'as Papp, Roger Wattenhofer",
        "title": "Default Ambiguity: Finding the Best Solution to the Clearing Problem",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We study financial networks with debt contracts and credit default swaps\nbetween specific pairs of banks. Given such a financial system, we want to\ndecide which of the banks are in default, and how much of their liabilities can\nthese defaulting banks pay. There can easily be multiple different solutions to\nthis problem, leading to a situation of default ambiguity, and a range of\npossible solutions to implement for a financial authority.\n  In this paper, we study the properties of the solution space of such\nfinancial systems, and analyze a wide range of reasonable objective functions\nfor selecting from the set of solutions. Examples of such objective functions\ninclude minimizing the number of defaulting banks, minimizing the amount of\nunpaid debt, maximizing the number of satisfied banks, and many others. We show\nthat for all of these objectives, it is NP-hard to approximate the optimal\nsolution to an $n^{1-\\epsilon}$ factor for any $\\epsilon>0$, with $n$ denoting\nthe number of banks. Furthermore, we show that this situation is rather\ndifficult to avoid from a financial regulator's perspective: the same hardness\nresults also hold if we apply strong restrictions on the weights of the debts,\nthe structure of the network, or the amount of funds that banks must possess.\nHowever, if we restrict both the network structure and the amount of funds\nsimultaneously, then the solution becomes unique, and it can be found\nefficiently.\n"
    },
    {
        "paper_id": 2002.0788,
        "authors": "Matteo Cinelli, Valerio Ficcadenti, Jessica Riccioni",
        "title": "The interconnectedness of the economic content in the speeches of the US\n  Presidents",
        "comments": null,
        "journal-ref": "Ann Oper Res (2019)",
        "doi": "10.1007/s10479-019-03372-2",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The speeches stated by influential politicians can have a decisive impact on\nthe future of a country. In particular, the economic content of such speeches\naffects the economy of countries and their financial markets. For this reason,\nwe examine a novel dataset containing the economic content of 951 speeches\nstated by 45 US Presidents from George Washington (April 1789) to Donald Trump\n(February 2017). In doing so, we use an economic glossary carried out by means\nof text mining techniques. The goal of our study is to examine the structure of\nsignificant interconnections within a network obtained from the economic\ncontent of presidential speeches. In such a network, nodes are represented by\ntalks and links by values of cosine similarity, the latter computed using the\noccurrences of the economic terms in the speeches. The resulting network\ndisplays a peculiar structure made up of a core (i.e. a set of highly central\nand densely connected nodes) and a periphery (i.e. a set of non-central and\nsparsely connected nodes). The presence of different economic dictionaries\nemployed by the Presidents characterize the core-periphery structure. The\nPresidents' talks belonging to the network's core share the usage of generic\n(non-technical) economic locutions like \"interest\" or \"trade\". While the use of\nmore technical and less frequent terms characterizes the periphery (e.g.\n\"yield\" ). Furthermore, the speeches close in time share a common economic\ndictionary. These results together with the economics glossary usages during\nthe US periods of boom and crisis provide unique insights on the economic\ncontent relationships among Presidents' speeches.\n"
    },
    {
        "paper_id": 2002.08207,
        "authors": "Daniel Guterding",
        "title": "Inventory effects on the price dynamics of VSTOXX futures quantified via\n  machine learning",
        "comments": null,
        "journal-ref": "JFDS 7, 126 (2021)",
        "doi": "10.1016/j.jfds.2021.06.001",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The VSTOXX index tracks the expected 30-day volatility of the EURO STOXX 50\nequity index. Futures on the VSTOXX index can, therefore, be used to hedge\nagainst economic uncertainty. We investigate the effect of trader inventory on\nthe price of VSTOXX futures through a combination of stochastic processes and\nmachine learning methods. We formulate a simple and efficient pricing\nmethodology for VSTOXX futures, which assumes a Heston-type stochastic process\nfor the underlying EURO STOXX 50 market. Under these dynamics, approximate\nanalytical formulas for the implied volatility smile and the VSTOXX index have\nrecently been derived. We use the EURO STOXX 50 option implied volatilities and\nthe VSTOXX index value to estimate the parameters of this Heston model.\nFollowing the calibration, we calculate theoretical VSTOXX future prices and\ncompare them to the actual market prices. While theoretical and market prices\nare usually in line, we also observe time periods, during which the market\nprice does not agree with our Heston model. We collect a variety of market\nfeatures that could potentially explain the price deviations and calibrate two\nmachine learning models to the price difference: a regularized linear model and\na random forest. We find that both models indicate a strong influence of\naccumulated trader positions on the VSTOXX futures price.\n"
    },
    {
        "paper_id": 2002.08245,
        "authors": "Tianping Zhang and Yuanqi Li and Yifei Jin and Jian Li",
        "title": "AutoAlpha: an Efficient Hierarchical Evolutionary Algorithm for Mining\n  Alpha Factors in Quantitative Investment",
        "comments": "7 pages, 6 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The multi-factor model is a widely used model in quantitative investment. The\nsuccess of a multi-factor model is largely determined by the effectiveness of\nthe alpha factors used in the model. This paper proposes a new evolutionary\nalgorithm called AutoAlpha to automatically generate effective formulaic alphas\nfrom massive stock datasets. Specifically, first we discover an inherent\npattern of the formulaic alphas and propose a hierarchical structure to quickly\nlocate the promising part of space for search. Then we propose a new Quality\nDiversity search based on the Principal Component Analysis (PCA-QD) to guide\nthe search away from the well-explored space for more desirable results. Next,\nwe utilize the warm start method and the replacement method to prevent the\npremature convergence problem. Based on the formulaic alphas we discover, we\npropose an ensemble learning-to-rank model for generating the portfolio. The\nbacktests in the Chinese stock market and the comparisons with several\nbaselines further demonstrate the effectiveness of AutoAlpha in mining\nformulaic alphas for quantitative trading.\n"
    },
    {
        "paper_id": 2002.08286,
        "authors": "Eunjung Noh and Kim Weston",
        "title": "Price impact equilibrium with transaction costs and TWAP trading",
        "comments": "22 pages, 1 figure",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We prove the existence of an equilibrium in a model with transaction costs\nand price impact where two agents are incentivized to trade towards a target.\nThe two types of frictions -- price impact and transaction costs -- lead the\nagents to two distinct changes in their optimal investment approach: price\nimpact causes agents to continuously trade in smaller amounts, while\ntransaction costs cause the agents to cease trading before the end of the\ntrading period. As the agents lose wealth because of transaction costs, the\nexchange makes a profit. We prove the existence of a strictly positive optimal\ntransaction cost from the exchange's perspective.\n"
    },
    {
        "paper_id": 2002.08466,
        "authors": "Jorge Barrera",
        "title": "Criptocurrencies, Fiat Money, Blockchains and Databases",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Two taxonomies of money that include cryptocurrencies are analyzed. A\ndefinition of the term cryptocurrency is given and a taxonomy of them is\npresented, based on how its price is fixed. The characteristics of the use of\ncurrent fiat money and the operation of two-level banking systems are\ndiscussed. Cryptocurrencies are compared with fiat money and the aspects in\nwhich the latter cannot be overcome are indicated. The characteristics of\nblockchains and databases are described. The possible cases of use of both\ntechnologies are compared, and it is noted that blockchains, in addition to\ncryptocurrencies and certain records, have not yet shown their usefulness,\nwhile databases constitute the foundation of most of the automated systems in\noperation.\n"
    },
    {
        "paper_id": 2002.08492,
        "authors": "Alexandre Carbonneau and Fr\\'ed\\'eric Godin",
        "title": "Equal Risk Pricing of Derivatives with Deep Hedging",
        "comments": "44 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This article presents a deep reinforcement learning approach to price and\nhedge financial derivatives. This approach extends the work of Guo and Zhu\n(2017) who recently introduced the equal risk pricing framework, where the\nprice of a contingent claim is determined by equating the optimally hedged\nresidual risk exposure associated respectively with the long and short\npositions in the derivative. Modifications to the latter scheme are considered\nto circumvent theoretical pitfalls associated with the original approach.\nDerivative prices obtained through this modified approach are shown to be\narbitrage-free. The current paper also presents a general and tractable\nimplementation for the equal risk pricing framework inspired by the deep\nhedging algorithm of Buehler et al. (2019). An $\\epsilon$-completeness measure\nallowing for the quantification of the residual hedging risk associated with a\nderivative is also proposed. The latter measure generalizes the one presented\nin Bertsimas et al. (2001) based on the quadratic penalty. Monte Carlo\nsimulations are performed under a large variety of market dynamics to\ndemonstrate the practicability of our approach, to perform benchmarking with\nrespect to traditional methods and to conduct sensitivity analyses.\n"
    },
    {
        "paper_id": 2002.08531,
        "authors": "Wujiang Lou",
        "title": "The Fair Basis: Funding and capital in the reduced form framework",
        "comments": "23 pages, 4 figures, 3 tables",
        "journal-ref": "Risk, June 2019",
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
        "abstract": "  A negative basis trade enters a long bond position and buys protection on the\nissuer of the bond through credit default swap (CDS), aiming at arbitrage\nprofit due to the bond-CDS basis. To classic reduced form model theorists, the\nexistence of the basis is an abnormality or merely liquidity noise. Such a\nview, however, fails to explain large basis trading losses incurred during the\nfinancial crisis. Employing a bond continuously hedged by CDS under a dynamic\nspread model with bond repo financing, we find that there is unhedged and\nunhedgeable residual jump to default risk that can't be diversified because of\ncredit correlation. An economic capital approach has to apply and a charge on\nthe use of capital follows. Together with the hedge funding cost, it allows us\nto better understand the basis's economics and to predict its fair level.\n"
    },
    {
        "paper_id": 2002.08532,
        "authors": "Wujiang Lou",
        "title": "Derivatives Discounting Explained",
        "comments": "38 pages, 3 figures, 1 table",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
        "abstract": "  Derivative pricing is about cash flow discounting at the riskfree rate. This\nteaching has lost its meaning post the financial crisis, due to the addition of\nextra value adjustments (XVA), which also made derivatives pricing and\nvaluation a very difficult task for investors. This article recovers a properly\ndefined discount rate that corresponds to different collateral and margin\nschemes. A binomial tree model is developed, enabling end-users to price in\ncounterparty default and funding risk. Coherent XVAs, if needed, naturally\nresult from decomposing the discount rate, and can be computed on the same\ntree.\n"
    },
    {
        "paper_id": 2002.08786,
        "authors": "Beatrice Acciaio, Julio Backhoff-Veraguas and Junchao Jia",
        "title": "Cournot-Nash equilibrium and optimal transport in a dynamic setting",
        "comments": "Literature review expanded, typos corrected, Remark 3.8 added",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We consider a large population dynamic game in discrete time. The peculiarity\nof the game is that players are characterized by time-evolving types, and so\nreasonably their actions should not anticipate the future values of their\ntypes. When interactions between players are of mean-field kind, we relate Nash\nequilibria for such games to an asymptotic notion of dynamic Cournot-Nash\nequilibria. Inspired by the works of Blanchet and Carlier for the static\nsituation, we interpret dynamic Cournot-Nash equilibria in the light of causal\noptimal transport theory. Further specializing to games of potential type, we\nestablish existence, uniqueness and characterization of equilibria. Moreover we\ndevelop, for the first time, a numerical scheme for causal optimal transport,\nwhich is then leveraged in order to compute dynamic Cournot-Nash equilibria.\nThis is illustrated in a detailed case study of a congestion game.\n"
    },
    {
        "paper_id": 2002.08849,
        "authors": "Wenjing Wang and Minjing Tao",
        "title": "Forecasting Realized Volatility Matrix With Copula-Based Models",
        "comments": "26 pages, 3 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Multivariate volatility modeling and forecasting are crucial in financial\neconomics. This paper develops a copula-based approach to model and forecast\nrealized volatility matrices. The proposed copula-based time series models can\ncapture the hidden dependence structure of realized volatility matrices. Also,\nthis approach can automatically guarantee the positive definiteness of the\nforecasts through either Cholesky decomposition or matrix logarithm\ntransformation. In this paper we consider both multivariate and bivariate\ncopulas; the types of copulas include Student's t, Clayton and Gumbel copulas.\nIn an empirical application, we find that for one-day ahead volatility matrix\nforecasting, these copula-based models can achieve significant performance both\nin terms of statistical precision as well as creating economically\nmean-variance efficient portfolio. Among the copulas we considered, the\nmultivariate-t copula performs better in statistical precision, while\nbivariate-t copula has better economical performance.\n"
    },
    {
        "paper_id": 2002.09036,
        "authors": "Takeshi Kato, Yasuyuki Kudo, Junichi Miyakoshi, Jun Otsuka, Hayato\n  Saigo, Kaori Karasawa, Hiroyuki Yamaguchi and Yasuo Deguchi",
        "title": "Rational Choice Hypothesis as X-point of Utility Function and Norm\n  Function",
        "comments": "15 pages, 13 figures. Published online at\n  http://redfame.com/journal/index.php/aef/article/view/4890 . Related to\n  arXiv:2002.09037",
        "journal-ref": "Applied Economics and Finance, Vol. 7, No. 4 (2020) 63-77",
        "doi": "10.11114/aef.v7i4.4890",
        "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
        "abstract": "  Towards the realization of a sustainable, fair and inclusive society, we\nproposed a novel decision-making model that incorporates social norms in a\nrational choice model from the standpoints of deontology and utilitarianism. We\nproposed a hypothesis that interprets choice of action as the X-point for\nindividual utility function that increases with actions and social norm\nfunction that decreases with actions. This hypothesis is based on humans\npsychologically balancing the value of utility and norms in selecting actions.\nUsing the hypothesis and approximation, we were able to isolate and infer\nutility function and norm function from real-world measurement data of actions\non environmental conditions and elucidate the interaction between the both\nfunctions that led from current status to target actions. As examples of\ncollective data that aggregate decision-making of individuals, we looked at the\nchanges in power usage before and after the Great East Japan Earthquake and the\ncorrelation between national GDP and CO2 emission in different countries. The\nfirst example showed that the perceived benefits of power (i.e., utility of\npower usage) was stronger than the power usage restrictions imposed by norms\nafter the earthquake, contrary to our expectation. The second example showed\nthat a reduction of CO2 emission in each country was not related to utility\nderived from GDP but to norms related to CO2 emission. Going forward, we will\napply this new X-point model to actual social practices involving normative\nproblems, and design the approaches for the diagnosis, prognosis and\nintervention of social systems by IT systems.\n"
    },
    {
        "paper_id": 2002.09037,
        "authors": "Takeshi Kato, Yasuyuki Kudo, Junichi Miyakoshi, Jun Otsuka, Hayato\n  Saigo, Kaori Karasawa, Hiroyuki Yamaguchi, Yoshinori Hiroi and Yasuo Deguchi",
        "title": "Sustainability and Fairness Simulations Based on Decision-Making Model\n  of Utility Function and Norm Function",
        "comments": "19 pages, 12 figures. Published online at\n  http://redfame.com/journal/index.php/aef/article/view/4825 . Related to\n  arXiv:2002.09036",
        "journal-ref": "Applied Economics and Finance, Vol. 7, No. 3 (2020) 96-114",
        "doi": "10.11114/aef.v7i3.4825",
        "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
        "abstract": "  We introduced a decision-making model based on value functions that included\nindividualistic utility function and socio-constructivistic norm function and\nproposed a norm-fostering process that recursively updates norm function\nthrough mutual recognition between the self and others. As an example, we\nlooked at the resource-sharing problem typical of economic activities and\nassumed the distribution of individual actions to define the (1) norm function\nfostered through mutual comparison of value/action ratio based on the equity\ntheory (progressive tax-like), (2) norm function proportional to resource\nutilization (proportional tax-like) and (3) fixed norm function independent of\nresource utilization (fixed tax-like). By carrying out numerical simulation, we\nshowed that the progressive tax-like norm function (i) does not increase\ndisparity for the distribution of the actions, unlike the other norm functions,\nand (ii) has high resource productivity and low Gini coefficient. Therefore the\nprogressive tax-like norm function has the highest sustainability and fairness.\n"
    },
    {
        "paper_id": 2002.09097,
        "authors": "Ying-Ying Shen (ECUST), Zhi-Qiang Jiang (ECUST), Jun-Chao Ma (ECUST),\n  Gang-Jin Wang (HNU), Wei-Xing Zhou (ECUST)",
        "title": "Sector connectedness in the Chinese stock markets",
        "comments": "17 pages, 7 figures, and 3 tables",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Uncovering the risk transmitting path within economic sectors in China is\ncrucial for understanding the stability of the Chinese economic system,\nespecially under the current situation of the China-US trade conflicts. In this\npaper, we try to uncover the risk spreading channels by means of volatility\nspillovers within the Chinese sectors using stock market data. By applying the\ngeneralized variance decomposition framework based on the VAR model and the\nrolling window approach, a set of connectedness matrices is obtained to reveal\nthe overall and dynamic spillovers within sectors. It is found that 17 sectors\n(mechanical equipment, electrical equipment, utilities, and so on) are risk\ntransmitters and 11 sectors (national defence, bank, non-bank finance, and so\non) are risk takers during the whole period. During the periods with the\nextreme risk events (the global financial crisis, the Chinese interbank\nliquidity crisis, the Chinese stock market plunge, and the China-US trade war),\nwe observe that the connectedness measures significantly increase and the\nfinancial sectors play a buffer role in stabilizing the economic system. The\nrobust tests suggest that our results are not sensitive to the changes of model\nparameters. Our results not only uncover the spillover effects within the\nChinese sectors, but also highlight the deep understanding of the risk\ncontagion patterns in the Chinese stock markets.\n"
    },
    {
        "paper_id": 2002.09108,
        "authors": "Qingyin Ma, Alexis Akira Toda",
        "title": "Asymptotic Linearity of Consumption Functions and Computational\n  Efficiency",
        "comments": null,
        "journal-ref": null,
        "doi": "10.1016/j.jmateco.2021.102562",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We prove that the consumption functions in optimal savings problems are\nasymptotically linear if the marginal utility is regularly varying. We also\nanalytically characterize the asymptotic marginal propensities to consume\n(MPCs) out of wealth. Our results are useful for obtaining good initial guesses\nwhen numerically computing consumption functions, and provide a theoretical\njustification for linearly extrapolating consumption functions outside the\ngrid.\n"
    },
    {
        "paper_id": 2002.09201,
        "authors": "Chengyuan Zhang and Fuxin Jiang and Shouyang Wang and Shaolong Sun",
        "title": "A New Decomposition Ensemble Approach for Tourism Demand Forecasting:\n  Evidence from Major Source Countries",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The Asian-pacific region is the major international tourism demand market in\nthe world, and its tourism demand is deeply affected by various factors.\nPrevious studies have shown that different market factors influence the tourism\nmarket demand at different timescales. Accordingly, the decomposition ensemble\nlearning approach is proposed to analyze the impact of different market factors\non market demand, and the potential advantages of the proposed method on\nforecasting tourism demand in the Asia-pacific region are further explored.\nThis study carefully explores the multi-scale relationship between tourist\ndestinations and the major source countries, by decomposing the corresponding\nmonthly tourist arrivals with noise-assisted multivariate empirical mode\ndecomposition. With the China and Malaysia as case studies, their respective\nempirical results show that decomposition ensemble approach significantly\nbetter than the benchmarks which include statistical model, machine learning\nand deep learning model, in terms of the level forecasting accuracy and\ndirectional forecasting accuracy.\n"
    },
    {
        "paper_id": 2002.09215,
        "authors": "Masaaki Fukasawa",
        "title": "Volatility has to be rough",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  First, we give an asymptotic expansion of short-dated at-the-money implied\nvolatility that refines the preceding works and proves in particular that\nnon-rough volatility models are inconsistent to a power law of volatility skew.\nSecond, we show that given a power law of volatility skew in an option market,\na continuous price dynamics of the underlying asset with non-rough volatility\nadmits an arbitrage opportunity. The volatility therefore has to be rough in a\nviable market of the underlying asset of which the volatility skew obeys a\npower law.\n"
    },
    {
        "paper_id": 2002.09272,
        "authors": "Takeshi Kato, Yasuyuki Kudo, Hiroyuki Mizuno and Yoshinori Hiroi",
        "title": "Regional Inequality Simulations Based on Asset Exchange Models with\n  Exchange Range and Local Support Bias",
        "comments": "14 pages, 8 figures. Published online at\n  http://redfame.com/journal/index.php/aef/article/view/4945",
        "journal-ref": "Applied Economics and Finance, Vol. 7, No. 5 (2020) 10-23",
        "doi": "10.11114/aef.v7i5.4945",
        "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
        "abstract": "  To gain insights into the problem of regional inequality, we proposed new\nregional asset exchange models based on existing kinetic income-exchange models\nin economic physics. We did this by setting the spatial exchange range and\nadding bias to asset fraction probability in equivalent exchanges. Simulations\nof asset distribution and Gini coefficients showed that suppressing regional\ninequality requires, firstly an increase in the intra-regional economic\ncirculation rate, and secondly the narrowing down of the exchange range\n(inter-regional economic zone). However, avoiding over-concentration of assets\ndue to repeat exchanges requires adding a third measure; the local support bias\n(distribution norm). A comprehensive solution incorporating these three\nmeasures enabled shifting the asset distribution from over-concentration to\nexponential distribution and eventually approaching the normal distribution,\nreducing the Gini coefficient further. Going forward, we will expand these\nmodels by setting production capacity based on assets, path dependency on\ntwo-dimensional space, bias according to disparity, and verify measures to\nreduce regional inequality in actual communities.\n"
    },
    {
        "paper_id": 2002.09445,
        "authors": "Oleksii Mostovyi",
        "title": "Stability of the indirect utility process",
        "comments": "34 pages, preliminary version",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We investigate the dynamic stability of the indirect utility process\nassociated with a (possibly suboptimal) trading strategy under perturbations of\nthe market. Establishing the reverse conjugacy characterizations first, we\nprove continuity and first-order convergence of the indirect-utility process\nunder simultaneous perturbations of the finite variation and martingale parts\nof the return of the risky asset.\n"
    },
    {
        "paper_id": 2002.09549,
        "authors": "Eyal Neuman and Moritz Vo{\\ss}",
        "title": "Optimal Signal-Adaptive Trading with Temporary and Transient Price\n  Impact",
        "comments": "31 pages, 4 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We study optimal liquidation in the presence of linear temporary and\ntransient price impact along with taking into account a general price\npredicting finite-variation signal. We formulate this problem as minimization\nof a cost-risk functional over a class of absolutely continuous and\nsignal-adaptive strategies. The stochastic control problem is solved by\nfollowing a probabilistic and convex analytic approach. We show that the\noptimal trading strategy is given by a system of four coupled forward-backward\nSDEs, which can be solved explicitly. Our results reveal how the induced\ntransient price distortion provides together with the predictive signal an\nadditional predictor about future price changes. As a consequence, the optimal\nsignal-adaptive trading rate trades off exploiting the predictive signal\nagainst incurring the transient displacement of the execution price from its\nunaffected level. This answers an open question from Lehalle and Neuman [29] as\nwe show how to derive the unique optimal signal-adaptive liquidation strategy\nwhen price impact is not only temporary but also transient.\n"
    },
    {
        "paper_id": 2002.09565,
        "authors": "Micah Goldblum, Avi Schwarzschild, Ankit B. Patel, Tom Goldstein",
        "title": "Adversarial Attacks on Machine Learning Systems for High-Frequency\n  Trading",
        "comments": "ACM International Conference on AI in Finance (ICAIF) 2021",
        "journal-ref": null,
        "doi": "10.1145/3490354.3494367",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Algorithmic trading systems are often completely automated, and deep learning\nis increasingly receiving attention in this domain. Nonetheless, little is\nknown about the robustness properties of these models. We study valuation\nmodels for algorithmic trading from the perspective of adversarial machine\nlearning. We introduce new attacks specific to this domain with size\nconstraints that minimize attack costs. We further discuss how these attacks\ncan be used as an analysis tool to study and evaluate the robustness properties\nof financial models. Finally, we investigate the feasibility of realistic\nadversarial attacks in which an adversarial trader fools automated trading\nsystems into making inaccurate predictions.\n"
    },
    {
        "paper_id": 2002.09578,
        "authors": "Xiaochun Meng, James W. Taylor, Souhaib Ben Taieb, Siran Li",
        "title": "Scores for Multivariate Distributions and Level Sets",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Forecasts of multivariate probability distributions are required for a\nvariety of applications. Scoring rules enable the evaluation of forecast\naccuracy, and comparison between forecasting methods. We propose a theoretical\nframework for scoring rules for multivariate distributions, which encompasses\nthe existing quadratic score and multivariate continuous ranked probability\nscore. We demonstrate how this framework can be used to generate new scoring\nrules. In some multivariate contexts, it is a forecast of a level set that is\nneeded, such as a density level set for anomaly detection or the level set of\nthe cumulative distribution as a measure of risk. This motivates consideration\nof scoring functions for such level sets. For univariate distributions, it is\nwell-established that the continuous ranked probability score can be expressed\nas the integral over a quantile score. We show that, in a similar way, scoring\nrules for multivariate distributions can be decomposed to obtain scoring\nfunctions for level sets. Using this, we present scoring functions for\ndifferent types of level set, including density level sets and level sets for\ncumulative distributions. To compute the scores, we propose a simple numerical\nalgorithm. We perform a simulation study to support our proposals, and we use\nreal data to illustrate usefulness for forecast combining and CoVaR estimation.\n"
    },
    {
        "paper_id": 2002.09656,
        "authors": "Yang Yifan, Guo Ju'e, Sun Shaolong, and Li Yixin",
        "title": "A new hybrid approach for crude oil price forecasting: Evidence from\n  multi-scale data",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Faced with the growing research towards crude oil price fluctuations\ninfluential factors following the accelerated development of Internet\ntechnology, accessible data such as Google search volume index are increasingly\nquantified and incorporated into forecasting approaches. In this paper, we\napply multi-scale data that including both GSVI data and traditional economic\ndata related to crude oil price as independent variables and propose a new\nhybrid approach for monthly crude oil price forecasting. This hybrid approach,\nbased on divide and conquer strategy, consists of K-means method, kernel\nprincipal component analysis and kernel extreme learning machine , where\nK-means method is adopted to divide input data into certain clusters, KPCA is\napplied to reduce dimension, and KELM is employed for final crude oil price\nforecasting. The empirical result can be analyzed from data and method levels.\nAt the data level, GSVI data perform better than economic data in level\nforecasting accuracy but with opposite performance in directional forecasting\naccuracy because of Herd Behavior, while hybrid data combined their advantages\nand obtain best forecasting performance in both level and directional accuracy.\nAt the method level, the approaches with K-means perform better than those\nwithout K-means, which demonstrates that divide and conquer strategy can\neffectively improve the forecasting performance.\n"
    },
    {
        "paper_id": 2002.09881,
        "authors": "Taurai Muvunza",
        "title": "An $\\alpha$-Stable Approach to Modelling Highly Speculative Assets and\n  Cryptocurrencies",
        "comments": "11 pages, Paper presented at The 4th PKU-NUS International Conference\n  in Quantitative Finance and Economics, 2019",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We investigate the behaviour of cryptocurrencies using data for bitcoin,\nethereum and ripple which account for over 70% of the cryptocurrency market. We\ndemonstrate that $\\alpha$-stable distribution is an appropriately sufficient\nmodel for highly speculative cryptocurrencies which outperforms other heavy\ntailed distributions that are used in financial econometrics. We find that the\nmaximum likelihood method proposed by DuMouchel (1971) produces estimates that\nfit the cryptocurrency return data much better than the quantile based approach\nof McCulloch (1986) and sample characteristic method by Koutrouvelis (1980).\nThe empirical results show that the leptokurtic feature presented in\ncryptocurrency return data can be captured by an $\\alpha$-stable distribution.\nThe findings highlight that $\\alpha$-stable distribution is not only\nparsimonious with its four free parameters but also a creative model that is\nclose to reality. This paper covers early reports and literature on\ncryptocurrencies and stable distributions.\n"
    },
    {
        "paper_id": 2002.09911,
        "authors": "Walter Farkas, Ludovic Mathys",
        "title": "Geometric Step Options with Jumps. Parity Relations, PIDEs, and\n  Semi-Analytical Pricing",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The present article studies geometric step options in exponential L\\'evy\nmarkets. Our contribution is manifold and extends several aspects of the\ngeometric step option pricing literature. First, we provide symmetry and parity\nrelations and derive various characterizations for both European-type and\nAmerican-type geometric double barrier step options. In particular, we are able\nto obtain a jump-diffusion disentanglement for the early exercise premium of\nAmerican-type geometric double barrier step contracts and its\nmaturity-randomized equivalent as well as to characterize the diffusion and\njump contributions to these early exercise premiums separately by means of\npartial integro-differential equations and ordinary integro-differential\nequations. As an application of our characterizations, we derive\nsemi-analytical pricing results for (regular) European-type and American-type\ngeometric down-and-out step call options under hyper-exponential jump-diffusion\nmodels. Lastly, we use the latter results to discuss the early exercise\nstructure of geometric step options once jumps are added and to subsequently\nprovide an analysis of the impact of jumps on the price and hedging parameters\nof (European-type and American-type) geometric step contracts.\n"
    },
    {
        "paper_id": 2002.10135,
        "authors": "Alexander J. McNeil",
        "title": "Modelling volatile time series with v-transforms and copulas",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
        "abstract": "  An approach to the modelling of volatile time series using a class of\nuniformity-preserving transforms for uniform random variables is proposed.\nV-transforms describe the relationship between quantiles of the stationary\ndistribution of the time series and quantiles of the distribution of a\npredictable volatility proxy variable. They can be represented as copulas and\npermit the formulation and estimation of models that combine arbitrary marginal\ndistributions with copula processes for the dynamics of the volatility proxy.\nThe idea is illustrated using a Gaussian ARMA copula process and the resulting\nmodel is shown to replicate many of the stylized facts of financial return\nseries and to facilitate the calculation of marginal and conditional\ncharacteristics of the model including quantile measures of risk. Estimation is\ncarried out by adapting the exact maximum likelihood approach to the estimation\nof ARMA processes and the model is shown to be competitive with standard GARCH\nin an empirical application to Bitcoin return data.\n"
    },
    {
        "paper_id": 2002.10194,
        "authors": "Len Patrick Dominic M. Garces and Gerald H. L. Cheang",
        "title": "A Put-Call Transformation of the Exchange Option Problem under\n  Stochastic Volatility and Jump Diffusion Dynamics",
        "comments": "42 pages, 2 figures, 1 table",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We price European and American exchange options where the underlying asset\nprices are modelled using a Merton (1976) jump-diffusion with a common Heston\n(1993) stochastic volatility process. Pricing is performed under an equivalent\nmartingale measure obtained by setting the second asset yield process as the\nnumeraire asset, as suggested by Bjerskund and Stensland (1993). Such a choice\nfor the numeraire reduces the exchange option pricing problem, a\ntwo-dimensional problem, to pricing a call option written on the ratio of the\nyield processes of the two assets, a one-dimensional problem. The joint\ntransition density function of the asset yield ratio process and the\ninstantaneous variance process is then determined from the corresponding\nKolmogorov backward equation via integral transforms. We then determine\nintegral representations for the European exchange option price and the early\nexercise premium and state a linked system of integral equations that\ncharacterizes the American exchange option price and the associated early\nexercise boundary. Properties of the early exercise boundary near maturity are\nalso discussed.\n"
    },
    {
        "paper_id": 2002.10202,
        "authors": "Gerald H. L. Cheang and Len Patrick Dominic M. Garces",
        "title": "Representation of Exchange Option Prices under Stochastic Volatility\n  Jump-Diffusion Dynamics",
        "comments": "45 pages, 1 figure",
        "journal-ref": "Quantitative Finance 20(2), 291-310",
        "doi": "10.1080/14697688.2019.1655785",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this article, we provide representations of European and American exchange\noption prices under stochastic volatility jump-diffusion (SVJD) dynamics\nfollowing models by Merton (1976), Heston (1993), and Bates (1996). A\nRadon-Nikodym derivative process is also introduced to facilitate the shift\nfrom the objective market measure to other equivalent probability measures,\nincluding the equivalent martingale measure. Under the equivalent martingale\nmeasure, we derive the integro-partial differential equation that characterizes\nthe exchange option prices. We also derive representations of the European\nexchange option price using the change-of-numeraire technique proposed by Geman\net al. (1995) and the Fourier inversion formula derived by Caldana and Fusai\n(2013), and show that these two representations are comparable. Lastly, we show\nthat the American exchange option price can be decomposed into the price of the\nEuropean exchange option and an early exercise premium.\n"
    },
    {
        "paper_id": 2002.10206,
        "authors": "C\\'onall Kelly, Gabriel Lord, Heru Maulana",
        "title": "The role of adaptivity in a numerical method for the Cox-Ingersoll-Ross\n  model",
        "comments": "25 pages, 4 figures, 2 tables. This version submitted to the Journal\n  of Computational and Applied Mathematics",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We demonstrate the effectiveness of an adaptive explicit Euler method for the\napproximate solution of the Cox-Ingersoll-Ross model. This relies on a class of\npath-bounded timestepping strategies which work by reducing the stepsize as\nsolutions approach a neighbourhood of zero. The method is hybrid in the sense\nthat a convergent backstop method is invoked if the timestep becomes too small,\nor to prevent solutions from overshooting zero and becoming negative. Under\nparameter constraints that imply Feller's condition, we prove that such a\nscheme is strongly convergent, of order at least 1/2. Control of the strong\nerror is important for multi-level Monte Carlo techniques. Under Feller's\ncondition we also prove that the probability of ever needing the backstop\nmethod to prevent a negative value can be made arbitrarily small. Numerically,\nwe compare this adaptive method to fixed step implicit and explicit schemes,\nand a novel semi-implicit adaptive variant. We observe that the adaptive\napproach leads to methods that are competitive in a domain that extends beyond\nFeller's condition, indicating suitability for the modelling of stochastic\nvolatility in Heston-type asset models.\n"
    },
    {
        "paper_id": 2002.10222,
        "authors": "Maximilian Beikirch, Torsten Trimborn",
        "title": "Novel Insights in the Levy-Levy-Solomon Agent-Based Economic Market\n  Model",
        "comments": "arXiv admin note: text overlap with arXiv:1904.04951",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The Levy-Levy-Solomon model (A microscopic model of the stock market: cycles,\nbooms, and crashes, Economic Letters 45 (1))is one of the most influential\nagent-based economic market models. In several publications this model has been\ndiscussed and analyzed. Especially Lux and Zschischang (Some new results on the\nLevy, Levy and Solomon microscopic stock market model, Physica A, 291(1-4))\nhave shown that the model exhibits finite-size effects. In this study we extend\nexisting work in several directions. First, we show simulations which reveal\nfinite-size effects of the model. Secondly, we shed light on the origin of\nthese finite-size effects. Furthermore, we demonstrate the sensitivity of the\nLevy-Levy-Solomon model with respect to random numbers. Especially, we can\nconclude that a low-quality pseudo random number generator has a huge impact on\nthe simulation results. Finally, we study the impact of the stopping criteria\nin the market clearance mechanism of the Levy-Levy-Solomon model.\n"
    },
    {
        "paper_id": 2002.10247,
        "authors": "Manav Kaushik and A K Giri",
        "title": "Forecasting Foreign Exchange Rate: A Multivariate Comparative Analysis\n  between Traditional Econometric, Contemporary Machine Learning & Deep\n  Learning Techniques",
        "comments": "10 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In todays global economy, accuracy in predicting macro-economic parameters\nsuch as the foreign the exchange rate or at least estimating the trend\ncorrectly is of key importance for any future investment. In recent times, the\nuse of computational intelligence-based techniques for forecasting\nmacroeconomic variables has been proven highly successful. This paper tries to\ncome up with a multivariate time series approach to forecast the exchange rate\n(USD/INR) while parallelly comparing the performance of three multivariate\nprediction modelling techniques: Vector Auto Regression (a Traditional\nEconometric Technique), Support Vector Machine (a Contemporary Machine Learning\nTechnique), and Recurrent Neural Networks (a Contemporary Deep Learning\nTechnique). We have used monthly historical data for several macroeconomic\nvariables from April 1994 to December 2018 for USA and India to predict USD-INR\nForeign Exchange Rate. The results clearly depict that contemporary techniques\nof SVM and RNN (Long Short-Term Memory) outperform the widely used traditional\nmethod of Auto Regression. The RNN model with Long Short-Term Memory (LSTM)\nprovides the maximum accuracy (97.83%) followed by SVM Model (97.17%) and VAR\nModel (96.31%). At last, we present a brief analysis of the correlation and\ninterdependencies of the variables used for forecasting.\n"
    },
    {
        "paper_id": 2002.10385,
        "authors": "Ben Moews and Gbenga Ibikunle",
        "title": "Predictive intraday correlations in stable and volatile market\n  environments: Evidence from deep learning",
        "comments": "15 pages, 6 figures, preprint submitted to Physica A",
        "journal-ref": null,
        "doi": "10.1016/j.physa.2020.124392",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Standard methods and theories in finance can be ill-equipped to capture\nhighly non-linear interactions in financial prediction problems based on\nlarge-scale datasets, with deep learning offering a way to gain insights into\ncorrelations in markets as complex systems. In this paper, we apply deep\nlearning to econometrically constructed gradients to learn and exploit lagged\ncorrelations among S&P 500 stocks to compare model behaviour in stable and\nvolatile market environments, and under the exclusion of target stock\ninformation for predictions. In order to measure the effect of time horizons,\nwe predict intraday and daily stock price movements in varying interval lengths\nand gauge the complexity of the problem at hand with a modification of our\nmodel architecture. Our findings show that accuracies, while remaining\nsignificant and demonstrating the exploitability of lagged correlations in\nstock markets, decrease with shorter prediction horizons. We discuss\nimplications for modern finance theory and our work's applicability as an\ninvestigative tool for portfolio managers. Lastly, we show that our model's\nperformance is consistent in volatile markets by exposing it to the environment\nof the recent financial crisis of 2007/2008.\n"
    },
    {
        "paper_id": 2002.10982,
        "authors": "Yiqing Lin, Zhenjie Ren, Nizar Touzi, Junjian Yang",
        "title": "Random horizon principal-agent problems",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We consider a general formulation of the random horizon Principal-Agent\nproblem with a continuous payment and a lump-sum payment at termination. In the\nEuropean version of the problem, the random horizon is chosen solely by the\nprincipal with no other possible action from the agent than exerting effort on\nthe dynamics of the output process. We also consider the American version of\nthe contract, which covers the seminal Sannikov's model, where the agent can\nalso quit by optimally choosing the termination time of the contract. Our main\nresult reduces such non-zero-sum stochastic differential games to appropriate\nstochastic control problems which may be solved by standard methods of\nstochastic control theory. This reduction is obtained by following Sannikov's\napproach, further developed by Cvitanic, Possamai, and Touzi. We first\nintroduce an appropriate class of contracts for which the agent's optimal\neffort is immediately characterized by the standard verification argument in\nstochastic control theory. We then show that this class of contracts is dense\nin an appropriate sense so that the optimization over this restricted family of\ncontracts represents no loss of generality. The result is obtained by using the\nrecent well-posedness result of random horizon second-order backward SDE.\n"
    },
    {
        "paper_id": 2002.1099,
        "authors": "Matthew Dixon and Igor Halperin",
        "title": "G-Learner and GIRL: Goal Based Wealth Management with Reinforcement\n  Learning",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We present a reinforcement learning approach to goal based wealth management\nproblems such as optimization of retirement plans or target dated funds. In\nsuch problems, an investor seeks to achieve a financial goal by making periodic\ninvestments in the portfolio while being employed, and periodically draws from\nthe account when in retirement, in addition to the ability to re-balance the\nportfolio by selling and buying different assets (e.g. stocks). Instead of\nrelying on a utility of consumption, we present G-Learner: a reinforcement\nlearning algorithm that operates with explicitly defined one-step rewards, does\nnot assume a data generation process, and is suitable for noisy data. Our\napproach is based on G-learning - a probabilistic extension of the Q-learning\nmethod of reinforcement learning.\n  In this paper, we demonstrate how G-learning, when applied to a quadratic\nreward and Gaussian reference policy, gives an entropy-regulated Linear\nQuadratic Regulator (LQR). This critical insight provides a novel and\ncomputationally tractable tool for wealth management tasks which scales to high\ndimensional portfolios. In addition to the solution of the direct problem of\nG-learning, we also present a new algorithm, GIRL, that extends our goal-based\nG-learning approach to the setting of Inverse Reinforcement Learning (IRL)\nwhere rewards collected by the agent are not observed, and should instead be\ninferred. We demonstrate that GIRL can successfully learn the reward parameters\nof a G-Learner agent and thus imitate its behavior. Finally, we discuss\npotential applications of the G-Learner and GIRL algorithms for wealth\nmanagement and robo-advising.\n"
    },
    {
        "paper_id": 2002.11017,
        "authors": "Amir Ban, Moran Koren",
        "title": "A Practical Approach to Social Learning",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Models of social learning feature either binary signals or abstract signal\nstructures often deprived of micro-foundations. Both models are limited when\nanalyzing interim results or performing empirical analysis. We present a method\nof generating signal structures which are richer than the binary model, yet are\ntractable enough to perform simulations and empirical analysis. We demonstrate\nthe method's usability by revisiting two classical papers: (1) we discuss the\neconomic significance of unbounded signals Smith and Sorensen (2000); (2) we\nuse experimental data from Anderson and Holt (1997) to perform econometric\nanalysis. Additionally, we provide a necessary and sufficient condition for the\noccurrence of action cascades.\n"
    },
    {
        "paper_id": 2002.11158,
        "authors": "Thiago W. Alves, Ionut Florescu, George Calhoun, Dragos Bozdog",
        "title": "SHIFT: A Highly Realistic Financial Market Simulation Platform",
        "comments": "To be presented at the 6th International Symposium in Computational\n  Economics and Finance in Paris, October 29-31, 2020",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper presents a new financial market simulator that may be used as a\ntool in both industry and academia for research in market microstructure. It\nallows multiple automated traders and/or researchers to simultaneously connect\nto an exchange-like environment, where they are able to asynchronously trade\nseveral financial assets at the same time. In its current iteration, this\norder-driven market implements the basic rules of U.S. equity markets,\nsupporting both market and limit orders, and executing them in a\nfirst-in-first-out fashion. We overview the system architecture and we present\npossible use cases. We demonstrate how a set of automated agents is capable of\nproducing a price process with characteristics similar to the statistics of\nreal price from financial markets. Finally, we detail a market stress scenario\nand we draw, what we believe to be, interesting conclusions about crash events.\n"
    },
    {
        "paper_id": 2002.11258,
        "authors": "Nicolas Essis-Breton and Patrice Gaillardetz",
        "title": "Fast Lower and Upper Estimates for the Price of Constrained Multiple\n  Exercise American Options by Single Pass Lookahead Search and\n  Nearest-Neighbor Martingale",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This article presents fast lower and upper estimates for a large class of\noptions: the class of constrained multiple exercise American options. Typical\noptions in this class are swing options with volume and timing constraints, and\npassport options with multiple lookback rights. The lower estimate algorithm\nuses the artificial intelligence method of lookahead search. The upper estimate\nalgorithm uses the dual approach to option pricing on a nearest-neighbor basis\nfor the martingale space. Probabilistic convergence guarantees are provided.\nSeveral numerical examples illustrate the approaches including a swing option\nwith four constraints, and a passport option with 16 constraints.\n"
    },
    {
        "paper_id": 2002.11523,
        "authors": "Evgeny Ponomarev, Ivan Oseledets, Andrzej Cichocki",
        "title": "Using Reinforcement Learning in the Algorithmic Trading Problem",
        "comments": null,
        "journal-ref": "ISSN 1064-2269, Journal of Communications Technology and\n  Electronics, 2019, Vol. 64, No. 12, pp. 1450-1457",
        "doi": "10.1134/S1064226919120131",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The development of reinforced learning methods has extended application to\nmany areas including algorithmic trading. In this paper trading on the stock\nexchange is interpreted into a game with a Markov property consisting of\nstates, actions, and rewards. A system for trading the fixed volume of a\nfinancial instrument is proposed and experimentally tested; this is based on\nthe asynchronous advantage actor-critic method with the use of several neural\nnetwork architectures. The application of recurrent layers in this approach is\ninvestigated. The experiments were performed on real anonymized data. The best\narchitecture demonstrated a trading strategy for the RTS Index futures\n(MOEX:RTSI) with a profitability of 66% per annum accounting for commission.\nThe project source code is available via the following link:\nhttp://github.com/evgps/a3c_trading.\n"
    },
    {
        "paper_id": 2002.11583,
        "authors": "Daniel Buncic",
        "title": "Econometric issues with Laubach and Williams' estimates of the natural\n  rate of interest",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Holston, Laubach and Williams' (2017) estimates of the natural rate of\ninterest are driven by the downward trending behaviour of 'other factor'\n$z_{t}$. I show that their implementation of Stock and Watson's (1998) Median\nUnbiased Estimation (MUE) to determine the size of the $\\lambda _{z}$ parameter\nwhich drives this downward trend in $z_{t}$ is unsound. It cannot recover the\nratio of interest $\\lambda _{z}=a_{r}\\sigma _{z}/\\sigma _{\\tilde{y}}$ from MUE\nrequired for the estimation of the full structural model. This failure is due\nto an 'unnecessary' misspecification in Holston et al.'s (2017) formulation of\nthe Stage 2 model. More importantly, their implementation of MUE on this\nmisspecified Stage 2 model spuriously amplifies the point estimate of $\\lambda\n_{z}$. Using a simulation experiment, I show that their procedure generates\nexcessively large estimates of $\\lambda _{z}$ when applied to data generated\nfrom a model where the true $\\lambda _{z}$ is equal to zero. Correcting the\nmisspecification in their Stage 2 model and the implementation of MUE leads to\na substantially smaller $\\lambda _{z}$ estimate, and with this, a more subdued\ndownward trending influence of 'other factor' $z_{t}$ on the natural rate.\nMoreover, the $\\lambda _{z}$ point estimate is statistically highly\ninsignificant, suggesting that there is no role for 'other factor' $z_{t}$ in\nthis model. I also discuss various other estimation issues that arise in\nHolston et al.'s (2017) model of the natural rate that make it unsuitable for\npolicy analysis.\n"
    },
    {
        "paper_id": 2002.1165,
        "authors": "Akshay Krishnamurthy, Thodoris Lykouris, Chara Podimata, and Robert\n  Schapire",
        "title": "Contextual Search in the Presence of Adversarial Corruptions",
        "comments": "The first version was titled \"Corrupted multidimensional binary\n  search: Learning in the presence of irrational agents\". An 8-page extended\n  abstract titled \"Contextual search in the presence of irrational agents\"\n  appeared at the 53rd ACM Symposium on the Theory of Computing (STOC '21)",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We study contextual search, a generalization of binary search in higher\ndimensions, which captures settings such as feature-based dynamic pricing.\nStandard formulations of this problem assume that agents act in accordance with\na specific homogeneous response model. In practice, however, some responses may\nbe adversarially corrupted. Existing algorithms heavily depend on the assumed\nresponse model being (approximately) accurate for all agents and have poor\nperformance in the presence of even a few such arbitrary misspecifications.\n  We initiate the study of contextual search when some of the agents can behave\nin ways inconsistent with the underlying response model. In particular, we\nprovide two algorithms, one based on multidimensional binary search methods and\none based on gradient descent. We show that these algorithms attain\nnear-optimal regret in the absence of adversarial corruptions and their\nperformance degrades gracefully with the number of such agents, providing the\nfirst results for contextual search in any adversarial noise model. Our\ntechniques draw inspiration from learning theory, game theory, high-dimensional\ngeometry, and convex analysis.\n"
    },
    {
        "paper_id": 2002.11705,
        "authors": "Tesi Aliaj and Aris Anagnostopoulos and Stefano Piersanti",
        "title": "Firms Default Prediction with Machine Learning",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Academics and practitioners have studied over the years models for predicting\nfirms bankruptcy, using statistical and machine-learning approaches. An earlier\nsign that a company has financial difficulties and may eventually bankrupt is\ngoing in \\emph{default}, which, loosely speaking means that the company has\nbeen having difficulties in repaying its loans towards the banking system.\nFirms default status is not technically a failure but is very relevant for bank\nlending policies and often anticipates the failure of the company. Our study\nuses, for the first time according to our knowledge, a very large database of\ngranular credit data from the Italian Central Credit Register of Bank of Italy\nthat contain information on all Italian companies' past behavior towards the\nentire Italian banking system to predict their default using machine-learning\ntechniques. Furthermore, we combine these data with other information regarding\ncompanies' public balance sheet data. We find that ensemble techniques and\nrandom forest provide the best results, corroborating the findings of Barboza\net al. (Expert Syst. Appl., 2017).\n"
    },
    {
        "paper_id": 2002.11865,
        "authors": "Massoomeh Rahsepar and Foivos Xanthos",
        "title": "On the extension property of dilatation monotone risk measures",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Let $\\mathcal{X}$ be a subset of $L^1$ that contains the space of simple\nrandom variables $\\mathcal{L}$ and $\\rho: \\mathcal{X} \\rightarrow\n(-\\infty,\\infty]$ a dilatation monotone functional with the Fatou property. In\nthis note, we show that $\\rho$ extends uniquely to a $\\sigma(L^1,\\mathcal{L})$\nlower semicontinuous and dilatation monotone functional $\\overline{\\rho}: L^1\n\\rightarrow (-\\infty,\\infty]$. Moreover, $\\overline{\\rho}$ preserves\nmonotonicity, (quasi)convexity, and cash-additivity of $\\rho$. Our findings\ncomplement recent extension results for quasiconvex law-invariant functionals\nproved in [17,20]. As an application of our results, we show that transformed\nnorm risk measures on Orlicz hearts admit a natural extension to $L^1$ that\nretains the robust representations obtained in [4,6].\n"
    },
    {
        "paper_id": 2002.11976,
        "authors": "Andreas Binder (1), Onkar Jadhav (1 and 2) and Volker Mehrmann (2)\n  ((1) MathConsult GmbH, Linz, Austria, (2) Institute of Mathematics, TU\n  Berlin, Berlin, Germany)",
        "title": "Model order reduction for parametric high dimensional models in the\n  analysis of financial risk",
        "comments": "39 pages, 10 figures",
        "journal-ref": null,
        "doi": "10.1186/s13362-021-00105-8",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper presents a model order reduction (MOR) approach for high\ndimensional problems in the analysis of financial risk. To understand the\nfinancial risks and possible outcomes, we have to perform several thousand\nsimulations of the underlying product. These simulations are expensive and\ncreate a need for efficient computational performance. Thus, to tackle this\nproblem, we establish a MOR approach based on a proper orthogonal decomposition\n(POD) method. The study involves the computations of high dimensional\nparametric convection-diffusion reaction partial differential equations (PDEs).\nPOD requires to solve the high dimensional model at some parameter values to\ngenerate a reduced-order basis. We propose an adaptive greedy sampling\ntechnique based on surrogate modeling for the selection of the sample parameter\nset that is analyzed, implemented, and tested on the industrial data. The\nresults obtained for the numerical example of a floater with cap and floor\nunder the Hull-White model indicate that the MOR approach works well for\nshort-rate models.\n"
    },
    {
        "paper_id": 2002.12274,
        "authors": "Paz Grimberg, Tobias Lauinger, Damon McCoy",
        "title": "Empirical Analysis of Indirect Internal Conversions in Cryptocurrency\n  Exchanges",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Algorithmic trading is well studied in traditional financial markets.\nHowever, it has received less attention in centralized cryptocurrency\nexchanges. The Commodity Futures Trading Commission (CFTC) attributed the\n$2010$ flash crash, one of the most turbulent periods in the history of\nfinancial markets that saw the Dow Jones Industrial Average lose $9\\%$ of its\nvalue within minutes, to automated order \"spoofing\" algorithms. In this paper,\nwe build a set of methodologies to characterize and empirically measure\ndifferent algorithmic trading strategies in Binance, a large centralized\ncryptocurrency exchange, using a complete data set of historical trades. We\nfind that a sub-strategy of triangular arbitrage is widespread, where bots\nconvert between two coins through an intermediary coin, and obtain a favorable\nexchange rate compared to the direct one. We measure the profitability of this\nstrategy, characterize its risks, and outline two strategies that algorithmic\ntrading bots use to mitigate their losses. We find that this strategy yields an\nexchange ratio that is $0.144\\%$, or $14.4$ basis points (bps) better than the\ndirect exchange ratio. $2.71\\%$ of all trades on Binance are attributable to\nthis strategy.\n"
    },
    {
        "paper_id": 2002.12857,
        "authors": "Jin Ma and Eunjung Noh",
        "title": "Equilibrium Model of Limit Order Books: A Mean-field Game View",
        "comments": "32 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper we study a continuous time equilibrium model of limit order\nbook (LOB) in which the liquidity dynamics follows a non-local, reflected\nmean-field stochastic differential equation (SDE) with evolving intensity.\nGeneralizing the basic idea of Ma et al. (2015), we argue that the frontier of\nthe LOB (e.g., the best asking price) is the value function of a mean-field\nstochastic control problem, as the limiting version of a Bertrand-type\ncompetition among the liquidity providers. With a detailed analysis on the\n$N$-seller static Bertrand game, we formulate a continuous time limiting\nmean-field control problem of the representative seller. We then validate the\ndynamic programming principle (DPP), and show that the value function is a\nviscosity solution of the corresponding Hamilton-Jacobi-Bellman (HJB) equation.\nWe argue that the value function can be used to obtain the equilibrium density\nfunction of the LOB, following the idea of Ma et al. (2015).\n"
    },
    {
        "paper_id": 2003.00033,
        "authors": "Hie Joo Ahn, Leland D. Crane",
        "title": "Dynamic Beveridge Curve Accounting",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We develop a dynamic decomposition of the empirical Beveridge curve, i.e.,\nthe level of vacancies conditional on unemployment. Using a standard model, we\nshow that three factors can shift the Beveridge curve: reduced-form matching\nefficiency, changes in the job separation rate, and out-of-steady-state\ndynamics. We find that the shift in the Beveridge curve during and after the\nGreat Recession was due to all three factors, and each factor taken separately\nhad a large effect. Comparing the pre-2010 period to the post-2010 period, a\nfall in matching efficiency and out-of-steady-state dynamics both pushed the\ncurve upward, while the changes in the separation rate pushed the curve\ndownward. The net effect was the observed upward shift in vacancies given\nunemployment. In previous recessions changes in matching efficiency were\nrelatively unimportant, while dynamics and the separation rate had more impact.\nThus, the unusual feature of the Great Recession was the deterioration in\nmatching efficiency, while separations and dynamics have played significant,\npartially offsetting roles in most downturns. The importance of these latter\ntwo margins contrasts with much of the literature, which abstracts from one or\nboth of them. We show that these factors affect the slope of the empirical\nBeveridge curve, an important quantity in recent welfare analyses estimating\nthe natural rate of unemployment.\n"
    },
    {
        "paper_id": 2003.00129,
        "authors": "Duc P. Truong, Erik Skau, Vladimir I. Valtchinov, Boian S. Alexandrov",
        "title": "Determination of Latent Dimensionality in International Trade Flow",
        "comments": null,
        "journal-ref": null,
        "doi": "10.1088/2632-2153/aba9ee",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Currently, high-dimensional data is ubiquitous in data science, which\nnecessitates the development of techniques to decompose and interpret such\nmultidimensional (aka tensor) datasets. Finding a low dimensional\nrepresentation of the data, that is, its inherent structure, is one of the\napproaches that can serve to understand the dynamics of low dimensional latent\nfeatures hidden in the data. Nonnegative RESCAL is one such technique,\nparticularly well suited to analyze self-relational data, such as dynamic\nnetworks found in international trade flows. Nonnegative RESCAL computes a low\ndimensional tensor representation by finding the latent space containing\nmultiple modalities. Estimating the dimensionality of this latent space is\ncrucial for extracting meaningful latent features. Here, to determine the\ndimensionality of the latent space with nonnegative RESCAL, we propose a latent\ndimension determination method which is based on clustering of the solutions of\nmultiple realizations of nonnegative RESCAL decompositions. We demonstrate the\nperformance of our model selection method on synthetic data and then we apply\nour method to decompose a network of international trade flows data from\nInternational Monetary Fund and validate the resulting features against\nempirical facts from economic literature.\n"
    },
    {
        "paper_id": 2003.0013,
        "authors": "James Wallbridge",
        "title": "Transformers for Limit Order Books",
        "comments": "16 pages, 6 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We introduce a new deep learning architecture for predicting price movements\nfrom limit order books. This architecture uses a causal convolutional network\nfor feature extraction in combination with masked self-attention to update\nfeatures based on relevant contextual information. This architecture is shown\nto significantly outperform existing architectures such as those using\nconvolutional networks (CNN) and Long-Short Term Memory (LSTM) establishing a\nnew state-of-the-art benchmark for the FI-2010 dataset.\n"
    },
    {
        "paper_id": 2003.00334,
        "authors": "Nian Yao, Zhiqiu Li, Zhichao Ling, Junfeng Lin",
        "title": "Asymptotic Smiles for an Affine Jump-Diffusion Model",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper, we study the asymptotic behaviors of implied volatility of an\naffine jump-diffusion model. Let log stock price under risk-neutral measure\nfollow an affine jump-diffusion model, we show that an explicit form of moment\ngenerating function for log stock price can be obtained by solving a set of\nordinary differential equations. A large-time large deviation principle for log\nstock price is derived by applying the G\\\"{a}rtner-Ellis theorem. We\ncharacterize the asymptotic behaviors of the implied volatility in the\nlarge-maturity and large-strike regime using rate function in the large\ndeviation principle. The asymptotics of the Black-Scholes implied volatility\nfor fixed-maturity, large-strike and fixed-maturity, small-strike regimes are\nalso studied. Numerical results are provided to validate the theoretical work.\n"
    },
    {
        "paper_id": 2003.0058,
        "authors": "Anton Pichler, Fran\\c{c}ois Lafond, J. Doyne Farmer",
        "title": "Technological interdependencies predict innovation dynamics",
        "comments": "10 pages, 4 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We propose a simple model where the innovation rate of a technological domain\ndepends on the innovation rate of the technological domains it relies on. Using\ndata on US patents from 1836 to 2017, we make out-of-sample predictions and\nfind that the predictability of innovation rates can be boosted substantially\nwhen network effects are taken into account. In the case where a technology$'$s\nneighborhood future innovation rates are known, the average predictability gain\nis 28$\\%$ compared to simpler time series model which do not incorporate\nnetwork effects. Even when nothing is known about the future, we find positive\naverage predictability gains of 20$\\%$. The results have important policy\nimplications, suggesting that the effective support of a given technology must\ntake into account the technological ecosystem surrounding the targeted\ntechnology.\n"
    },
    {
        "paper_id": 2003.00598,
        "authors": "Dat Thanh Tran, Juho Kanniainen, Moncef Gabbouj, Alexandros Iosifidis",
        "title": "Data Normalization for Bilinear Structures in High-Frequency Financial\n  Time-series",
        "comments": "6 pages, 3 tables, 1 figure",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Financial time-series analysis and forecasting have been extensively studied\nover the past decades, yet still remain as a very challenging research topic.\nSince the financial market is inherently noisy and stochastic, a majority of\nfinancial time-series of interests are non-stationary, and often obtained from\ndifferent modalities. This property presents great challenges and can\nsignificantly affect the performance of the subsequent analysis/forecasting\nsteps. Recently, the Temporal Attention augmented Bilinear Layer (TABL) has\nshown great performances in tackling financial forecasting problems. In this\npaper, by taking into account the nature of bilinear projections in TABL\nnetworks, we propose Bilinear Normalization (BiN), a simple, yet efficient\nnormalization layer to be incorporated into TABL networks to tackle potential\nproblems posed by non-stationarity and multimodalities in the input series. Our\nexperiments using a large scale Limit Order Book (LOB) consisting of more than\n4 million order events show that BiN-TABL outperforms TABL networks using other\nstate-of-the-arts normalization schemes by a large margin.\n"
    },
    {
        "paper_id": 2003.00656,
        "authors": "Michael Pinelis, David Ruppert",
        "title": "Machine Learning Portfolio Allocation",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We find economically and statistically significant gains when using machine\nlearning for portfolio allocation between the market index and risk-free asset.\nOptimal portfolio rules for time-varying expected returns and volatility are\nimplemented with two Random Forest models. One model is employed in forecasting\nthe sign probabilities of the excess return with payout yields. The second is\nused to construct an optimized volatility estimate. Reward-risk timing with\nmachine learning provides substantial improvements over the buy-and-hold in\nutility, risk-adjusted returns, and maximum drawdowns. This paper presents a\nnew theoretical basis and unifying framework for machine learning applied to\nboth return- and volatility-timing.\n"
    },
    {
        "paper_id": 2003.00803,
        "authors": "Fan Fang, Waichung Chung, Carmine Ventre, Michail Basios, Leslie\n  Kanthan, Lingbo Li, Fan Wu",
        "title": "Ascertaining price formation in cryptocurrency markets with DeepLearning",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The cryptocurrency market is amongst the fastest-growing of all the financial\nmarkets in the world. Unlike traditional markets, such as equities, foreign\nexchange and commodities, cryptocurrency market is considered to have larger\nvolatility and illiquidity. This paper is inspired by the recent success of\nusing deep learning for stock market prediction. In this work, we analyze and\npresent the characteristics of the cryptocurrency market in a high-frequency\nsetting. In particular, we applied a deep learning approach to predict the\ndirection of the mid-price changes on the upcoming tick. We monitored live\ntick-level data from $8$ cryptocurrency pairs and applied both statistical and\nmachine learning techniques to provide a live prediction. We reveal that\npromising results are possible for cryptocurrencies, and in particular, we\nachieve a consistent $78\\%$ accuracy on the prediction of the mid-price\nmovement on live exchange rate of Bitcoins vs US dollars.\n"
    },
    {
        "paper_id": 2003.00812,
        "authors": "James D. Miller, Roman Yampolskiy, Olle H\\\"aggstr\\\"om",
        "title": "An AGI Modifying Its Utility Function in Violation of the Orthogonality\n  Thesis",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  An artificial general intelligence (AGI) might have an instrumental drive to\nmodify its utility function to improve its ability to cooperate, bargain,\npromise, threaten, and resist and engage in blackmail. Such an AGI would\nnecessarily have a utility function that was at least partially observable and\nthat was influenced by how other agents chose to interact with it. This\ninstrumental drive would conflict with the orthogonality thesis since the\nmodifications would be influenced by the AGI's intelligence. AGIs in highly\ncompetitive environments might converge to having nearly the same utility\nfunction, one optimized to favorably influencing other agents through game\ntheory.\n"
    },
    {
        "paper_id": 2003.00884,
        "authors": "Amit K Chattopadhyay, Biswajit Debnath, Rihab El-Hassani, Sadhan Kumar\n  Ghosh, Rahul Baidya",
        "title": "Cleaner Production in Optimized Multivariate Networks: Operations\n  Management through a Roll of Dice",
        "comments": "18 pages, 9 figures; 3 additional appendices (14 pages)",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The importance of supply chain management in analyzing and later catalyzing\neconomic expectations while simultaneously prioritizing cleaner production\naspects is a vital component of modern finance. Such predictions, though, are\noften known to be less than accurate due to the ubiquitous uncertainty plaguing\nmost business decisions. Starting from a multi-dimensional cost function\ndefining the sustainability of the supply chain (SC) kernel, this article\noutlines a 4-component SC module - environmental, demand, economic, and social\nuncertainties - each ranked according to its individual weight. Our\nmathematical model then assesses the viability of a sustainable business by\nfirst ranking the potentially stochastic variables in order of their subjective\nimportance, and then optimizing the cost kernel, defined from a utility\nfunction. The model will then identify conditions (as equations) validating the\nsustainability of a business venture. The ranking is initially obtained from an\nAnalytical Hierarchical Process; the resultant weighted cost function is then\noptimized to analyze the impact of market uncertainty based on our supply chain\nmodel. Model predictions are then ratified against SME data to emphasize the\nimportance of cleaner production in business strategies.\n"
    },
    {
        "paper_id": 2003.00886,
        "authors": "Indrajit Saha and Veeraruna Kavitha",
        "title": "Financial replicator dynamics: emergence of systemic-risk-averting\n  strategies",
        "comments": "It will appear at the 10th International Conference on NETwork Games,\n  COntrol and OPtimization (NETGCOOP). 19 pages, two figures. In the current\n  version of the paper, many typos are corrected",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  We consider a random financial network with a large number of agents. The\nagents connect through credit instruments borrowed from each other or through\ndirect lending, and these create the liabilities. The settlement of the debts\nof various agents at the end of the contract period can be expressed as\nsolutions of random fixed point equations. Our first step is to derive these\nsolutions (asymptotically), using a recent result on random fixed point\nequations. We consider a large population in which agents adapt one of the two\navailable strategies, risky or risk-free investments, with an aim to maximize\ntheir expected returns (or surplus). We aim to study the emerging strategies\nwhen different types of replicator dynamics capture inter-agent interactions.\nWe theoretically reduced the analysis of the complex system to that of an\nappropriate ordinary differential equation (ODE). We proved that the\nequilibrium strategies converge almost surely to that of an attractor of the\nODE. We also derived the conditions under which a mixed evolutionary stable\nstrategy (ESS) emerges; in these scenarios the replicator dynamics converges to\nan equilibrium at which the expected returns of both the populations are equal.\nFurther the average dynamics (choices based on large observation sample) always\naverts systemic risk events (events with large fraction of defaults). We\nverified through Monte Carlo simulations that the equilibrium suggested by the\nODE method indeed represents the limit of the dynamics.\n"
    },
    {
        "paper_id": 2003.0093,
        "authors": "Bertram D\\\"uring, Nicos Georgiou, Sara Merino-Aceituno, Enrico Scalas",
        "title": "Continuum and thermodynamic limits for a simple random-exchange model",
        "comments": "33 pages, 2 figures, to be submitted",
        "journal-ref": "Stochastic Process. Appl. 149 (2022), 248-277",
        "doi": "10.1016/j.spa.2022.03.015",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We discuss various limits of a simple random exchange model that can be used\nfor the distribution of wealth. We start from a discrete state space - discrete\ntime version of this model and, under suitable scaling, we show its functional\nconvergence to a continuous space - discrete time model. Then, we show a\nthermodynamic limit of the empirical distribution to the solution of a kinetic\nequation of Boltzmann type. We solve this equation and we show that the\nsolutions coincide with the appropriate limits of the invariant measure for the\nMarkov chain. In this way we complete Boltzmann's program of deriving kinetic\nequations from random dynamics for this simple model. Three families of\ninvariant measures for the mean field limit are discovered and we show that\nonly two of those families can be obtained as limits of the discrete system and\nthe third is extraneous. Finally, we cast our results in the framework of\ninteger partitions and strengthen some results already available in the\nliterature.\n"
    },
    {
        "paper_id": 2003.01055,
        "authors": "Gianluca Cassese",
        "title": "Complete and competitive financial markets in a complex world",
        "comments": null,
        "journal-ref": "Finance and Stochastics 2021",
        "doi": "10.1007/s00780-021-00463-6",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We investigate the possibility of completing financial markets in a model\nwith no exogenous probability measure and market imperfections. A necessary and\nsufficient condition is obtained for such extension to be possible.\n"
    },
    {
        "paper_id": 2003.01206,
        "authors": "Mariola Ndrio, Khaled Alshehri and Subhonmesh Bose",
        "title": "A Scalar Parameterized Mechanism for Two-Sided Markets",
        "comments": "8 pages, 4 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/publicdomain/zero/1.0/",
        "abstract": "  We consider a market in which both suppliers and consumers compete for a\nproduct via scalar-parameterized supply offers and demand bids.\nScalar-parameterized offers/bids are appealing due to their modeling simplicity\nand desirable mathematical properties with the most prominent being bounded\nefficiency loss and price markup under strategic interactions. Our model\nincorporates production capacity constraints and minimum inelastic demand\nrequirements. Under perfect competition, the market mechanism yields\nallocations that maximize social welfare. When market participants are\nprice-anticipating, we show that there exists a unique Nash equilibrium, and\nprovide an efficient way to compute the resulting market allocation. Moreover,\nwe explicitly characterize the bounds on the welfare loss and prices observed\nat the Nash equilibrium.\n"
    },
    {
        "paper_id": 2003.0127,
        "authors": "Antoine Kornprobst and Matt Davison",
        "title": "Influence Of Climate Change On The Corn Yield In Ontario And Its Impact\n  On Corn Farms Income At The 2068 Horizon",
        "comments": "New version with slightly modified climate scenarios",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Our study aims at quantifying the impact of climate change on corn farming in\nOntario under several warming scenarios at the 2068 horizon. It is articulated\naround a discrete-time dynamic model of corn farm income with an annual\ntime-step, corresponding to one agricultural cycle from planting to harvest. At\neach period, we compute the income given the corn yield, which is highly\ndependent on weather variables. We also provide a reproducible forecast of the\nyearly distribution of corn yield for 10 cities in Ontario. The price of corn\nfutures at harvest time is taken into account and we fit our model by using 49\nyears of historical data. We then conduct out-of-sample Monte-Carlo simulations\nto obtain the farm income forecasts under a given climate change scenario.\n"
    },
    {
        "paper_id": 2003.01615,
        "authors": "Yongyang Cai",
        "title": "The Role of Uncertainty in Controlling Climate Change",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Integrated Assessment Models (IAMs) of the climate and economy aim to analyze\nthe impact and efficacy of policies that aim to control climate change, such as\ncarbon taxes and subsidies. A major characteristic of IAMs is that their\ngeophysical sector determines the mean surface temperature increase over the\npreindustrial level, which in turn determines the damage function. Most of the\nexisting IAMs are perfect-foresight forward-looking models, assuming that we\nknow all of the future information. However, there are significant\nuncertainties in the climate and economic system, including parameter\nuncertainty, model uncertainty, climate tipping risks, economic risks, and\nambiguity. For example, climate damages are uncertain: some researchers assume\nthat climate damages are proportional to instantaneous output, while others\nassume that climate damages have a more persistent impact on economic growth.\nClimate tipping risks represent (nearly) irreversible climate events that may\nlead to significant changes in the climate system, such as the Greenland ice\nsheet collapse, while the conditions, probability of tipping, duration, and\nassociated damage are also uncertain. Technological progress in carbon capture\nand storage, adaptation, renewable energy, and energy efficiency are uncertain\ntoo. In the face of these uncertainties, policymakers have to provide a\ndecision that considers important factors such as risk aversion, inequality\naversion, and sustainability of the economy and ecosystem. Solving this problem\nmay require richer and more realistic models than standard IAMs, and advanced\ncomputational methods. The recent literature has shown that these uncertainties\ncan be incorporated into IAMs and may change optimal climate policies\nsignificantly.\n"
    },
    {
        "paper_id": 2003.01783,
        "authors": "Joshua Aurand, Yu-Jui Huang",
        "title": "Mortality and Healthcare: a Stochastic Control Analysis under\n  Epstein-Zin Preferences",
        "comments": null,
        "journal-ref": "SIAM Journal on Control and Optimization, Vol. 59 (2021), No. 5,\n  pp 4051-4080",
        "doi": "10.1137/20M1331111",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper studies optimal consumption, investment, and healthcare spending\nunder Epstein-Zin preferences. Given consumption and healthcare spending plans,\nEpstein-Zin utilities are defined over an agent's random lifetime, partially\ncontrollable by the agent as healthcare reduces mortality growth. To the best\nof our knowledge, this is the first time Epstein-Zin utilities are formulated\non a controllable random horizon, via an infinite-horizon backward stochastic\ndifferential equation with superlinear growth. A new comparison result is\nestablished for the uniqueness of associated utility value processes. In a\nBlack-Scholes market, the stochastic control problem is solved through the\nrelated Hamilton-Jacobi-Bellman (HJB) equation. The verification argument\nfeatures a delicate containment of the growth of the controlled morality\nprocess, which is unique to our framework, relying on a combination of\nprobabilistic arguments and analysis of the HJB equation. In contrast to prior\nwork under time-separable utilities, Epstein-Zin preferences facilitate\ncalibration. The model-generated mortality closely approximates actual\nmortality data in the US and UK; moreover, the efficacy of healthcare can be\ncalibrated and compared between the two countries.\n"
    },
    {
        "paper_id": 2003.01809,
        "authors": "Yongyang Cai, Kenneth Judd, Rong Xu",
        "title": "Numerical Solution of Dynamic Portfolio Optimization with Transaction\n  Costs",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We apply numerical dynamic programming techniques to solve discrete-time\nmulti-asset dynamic portfolio optimization problems with proportional\ntransaction costs and shorting/borrowing constraints. Examples include problems\nwith multiple assets, and many trading periods in a finite horizon problem. We\nalso solve dynamic stochastic problems, with a portfolio including one\nrisk-free asset, an option, and its underlying risky asset, under the existence\nof transaction costs and constraints. These examples show that it is now\ntractable to solve such problems.\n"
    },
    {
        "paper_id": 2003.0182,
        "authors": "Thomas Spooner, Rahul Savani",
        "title": "Robust Market Making via Adversarial Reinforcement Learning",
        "comments": "7 pages, 3 figures; IJCAI-PRICAI '20 Conference Proceedings",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We show that adversarial reinforcement learning (ARL) can be used to produce\nmarket marking agents that are robust to adversarial and adaptively-chosen\nmarket conditions. To apply ARL, we turn the well-studied single-agent model of\nAvellaneda and Stoikov [2008] into a discrete-time zero-sum game between a\nmarket maker and adversary. The adversary acts as a proxy for other market\nparticipants that would like to profit at the market maker's expense. We\nempirically compare two conventional single-agent RL agents with ARL, and show\nthat our ARL approach leads to: 1) the emergence of risk-averse behaviour\nwithout constraints or domain-specific penalties; 2) significant improvements\nin performance across a set of standard metrics, evaluated with or without an\nadversary in the test environment, and; 3) improved robustness to model\nuncertainty. We empirically demonstrate that our ARL method consistently\nconverges, and we prove for several special cases that the profiles that we\nconverge to correspond to Nash equilibria in a simplified single-stage game.\n"
    },
    {
        "paper_id": 2003.01855,
        "authors": "Michael C. Nwogugu",
        "title": "Equity-Based Incentives, Production/Service Functions And Game Theory",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
        "abstract": "  EBIs/ESOs substantially change the traditional production/service function\nbecause ESOs/EBIs can have different psychological effects(motivation or\nde-motivation), and can create intangible capital and different economic\npayoffs. Although Game Theory is flawed, it can be helpful in describing\ninteractions in ESO/EBIs transactions. ESOs/EBIs involve two-stage games and\nthere are no perfect Nash Equilibria for the two sub-games. The large number of\nactual and potential participants in these games significantly complicates\nresolution of equilibria and increases the dynamism of the games given that\nplayers are more sensitive to other peoples moves in such games. This article:\na) analyzes how ESOs/EBIs affect traditional assumptions of production\nfunctions (in both the manufacturing and service sectors), b) analyzes\nESOs/EBIs transactions using game theory concepts, c) illustrates some of the\nlimitations of game theory.\n"
    },
    {
        "paper_id": 2003.01859,
        "authors": "Weiwei Jiang",
        "title": "Applications of deep learning in stock market prediction: recent\n  progress",
        "comments": "97 pages, 12 figures, 14 tables",
        "journal-ref": "Expert Systems with Applications, vol. 184, 115537, Dec 2021",
        "doi": "10.1016/j.eswa.2021.115537",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Stock market prediction has been a classical yet challenging problem, with\nthe attention from both economists and computer scientists. With the purpose of\nbuilding an effective prediction model, both linear and machine learning tools\nhave been explored for the past couple of decades. Lately, deep learning models\nhave been introduced as new frontiers for this topic and the rapid development\nis too fast to catch up. Hence, our motivation for this survey is to give a\nlatest review of recent works on deep learning models for stock market\nprediction. We not only category the different data sources, various neural\nnetwork structures, and common used evaluation metrics, but also the\nimplementation and reproducibility. Our goal is to help the interested\nresearchers to synchronize with the latest progress and also help them to\neasily reproduce the previous studies as baselines. Base on the summary, we\nalso highlight some future research directions in this topic.\n"
    },
    {
        "paper_id": 2003.01977,
        "authors": "Kristoffer Andersson and Cornelis Oosterlee",
        "title": "A deep learning approach for computations of exposure profiles for\n  high-dimensional Bermudan options",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper, we propose a neural network-based method for approximating\nexpected exposures and potential future exposures of Bermudan options. In a\nfirst phase, the method relies on the Deep Optimal Stopping algorithm, which\nlearns the optimal stopping rule from Monte-Carlo samples of the underlying\nrisk factors. Cashflow-paths are then created by applying the learned stopping\nstrategy on a new set of realizations of the risk factors. Furthermore, in a\nsecond phase the risk factors are regressed against the cashflow-paths to\nobtain approximations of pathwise option values. The regression step is carried\nout by ordinary least squares as well as neural networks, and it is shown that\nthe latter produces more accurate approximations.\n  The expected exposure is formulated, both in terms of the cashflow-paths and\nin terms of the pathwise option values and it is shown that a simple\nMonte-Carlo average yields accurate approximations in both cases. The potential\nfuture exposure is estimated by the empirical $\\alpha$-percentile.\n  Finally, it is shown that the expected exposures, as well as the potential\nfuture exposures can be computed under either, the risk neutral measure, or the\nreal world measure, without having to re-train the neural networks.\n"
    },
    {
        "paper_id": 2003.02035,
        "authors": "Yuri F. Saporito and Zhaoyu Zhang",
        "title": "PDGM: a Neural Network Approach to Solve Path-Dependent Partial\n  Differential Equations",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper, we propose a novel numerical method for Path-Dependent Partial\nDifferential Equations (PPDEs). These equations firstly appeared in the seminal\nwork of Dupire [2009], where the functional It\\^o calculus was developed to\ndeal with path-dependent financial derivatives contracts. More specificaly, we\ngeneralize the Deep Galerking Method (DGM) of Sirignano and Spiliopoulos [2018]\nto deal with these equations. The method, which we call Path-Dependent DGM\n(PDGM), consists of using a combination of feed-forward and Long Short-Term\nMemory architectures to model the solution of the PPDE. We then analyze several\nnumerical examples, many from the Financial Mathematics literature, that show\nthe capabilities of the method under very different situations.\n"
    },
    {
        "paper_id": 2003.02149,
        "authors": "Jarek Duda",
        "title": "Adaptive exponential power distribution with moving estimator for\n  nonstationary time series",
        "comments": "6 pages, 4 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  While standard estimation assumes that all datapoints are from probability\ndistribution of the same fixed parameters $\\theta$, we will focus on maximum\nlikelihood (ML) adaptive estimation for nonstationary time series: separately\nestimating parameters $\\theta_T$ for each time $T$ based on the earlier values\n$(x_t)_{t<T}$ using (exponential) moving ML estimator $\\theta_T=\\arg\\max_\\theta\nl_T$ for $l_T=\\sum_{t<T} \\eta^{T-t} \\ln(\\rho_\\theta (x_t))$ and some\n$\\eta\\in(0,1]$. Computational cost of such moving estimator is generally much\nhigher as we need to optimize log-likelihood multiple times, however, in many\ncases it can be made inexpensive thanks to dependencies. We focus on such\nexample: $\\rho(x)\\propto \\exp(-|(x-\\mu)/\\sigma|^\\kappa/\\kappa)$ exponential\npower distribution (EPD) family, which covers wide range of tail behavior like\nGaussian ($\\kappa=2$) or Laplace ($\\kappa=1$) distribution. It is also\nconvenient for such adaptive estimation of scale parameter $\\sigma$ as its\nstandard ML estimation is $\\sigma^\\kappa$ being average $\\|x-\\mu\\|^\\kappa$. By\njust replacing average with exponential moving average:\n$(\\sigma_{T+1})^\\kappa=\\eta(\\sigma_T)^\\kappa +(1-\\eta)|x_T-\\mu|^\\kappa$ we can\ninexpensively make it adaptive. It is tested on daily log-return series for\nDJIA companies, leading to essentially better log-likelihoods than standard\n(static) estimation, with optimal $\\kappa$ tails types varying between\ncompanies. Presented general alternative estimation philosophy provides tools\nwhich might be useful for building better models for analysis of nonstationary\ntime-series.\n"
    },
    {
        "paper_id": 2003.02173,
        "authors": "Marcus C. Christiansen and Christian Furrer",
        "title": "Dynamics of state-wise prospective reserves in the presence of\n  non-monotone information",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In the presence of monotone information, the stochastic Thiele equation\ndescribing the dynamics of state-wise prospective reserves is closely related\nto the classic martingale representation theorem. When the information utilized\nby the insurer is non-monotone, the classic martingale theory does not apply.\nBy taking an infinitesimal approach, we derive a generalized stochastic Thiele\nequation that allows for information discarding. En passant, we solve some open\nproblems for the classic case of monotone information. The results and their\nimplication in practice are illustrated via examples where information is\ndiscarded upon and after stochastic retirement.\n"
    },
    {
        "paper_id": 2003.02334,
        "authors": "Parisa Golbayani, Dan Wang, Ionut Florescu",
        "title": "Application of Deep Neural Networks to assess corporate Credit Rating",
        "comments": "19 pages, 10 tables",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Recent literature implements machine learning techniques to assess corporate\ncredit rating based on financial statement reports. In this work, we analyze\nthe performance of four neural network architectures (MLP, CNN, CNN2D, LSTM) in\npredicting corporate credit rating as issued by Standard and Poor's. We analyze\ncompanies from the energy, financial and healthcare sectors in US. The goal of\nthe analysis is to improve application of machine learning algorithms to credit\nassessment. To this end, we focus on three questions. First, we investigate if\nthe algorithms perform better when using a selected subset of features, or if\nit is better to allow the algorithms to select features themselves. Second, is\nthe temporal aspect inherent in financial data important for the results\nobtained by a machine learning algorithm? Third, is there a particular neural\nnetwork architecture that consistently outperforms others with respect to input\nfeatures, sectors and holdout set? We create several case studies to answer\nthese questions and analyze the results using ANOVA and multiple comparison\ntesting procedure.\n"
    },
    {
        "paper_id": 2003.02343,
        "authors": "Abhijit Chakraborty and Yuichi Ikeda",
        "title": "Bow-tie structure and community identification of global supply chain\n  network",
        "comments": "24 pages, 8 figures",
        "journal-ref": null,
        "doi": "10.1371/journal.pone.0239669",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We study on topological properties of global supply chain network in terms of\ndegree distribution, hierarchical structure, and degree-degree correlation in\nthe global supply chain network. The global supply chain data is constructed by\ncollecting various company data from the web site of Standard & Poor's Capital\nIQ platform in 2018. The in- and out-degree distributions are characterized by\na power law with in-degree exponent = 2.42 and out-degree exponent = 2.11. The\nclustering coefficient decays as power law with an exponent = 0.46. The nodal\ndegree-degree correlation indicates the absence of assortativity. The Bow-tie\nstructure of GWCC reveals that the OUT component is the largest and it consists\n41.1% of total firms. The GSCC component comprises 16.4% of total firms. We\nobserve that the firms in the upstream or downstream sides are mostly located a\nfew steps away from the GSCC. Furthermore, we uncover the community structure\nof the network and characterize them according to their location and industry\nclassification. We observe that the largest community consists of consumer\ndiscretionary sector mainly based in the US. These firms belong to the OUT\ncomponent in the bow-tie structure of the global supply chain network. Finally,\nwe confirm the validity for propositions S1 (short path length), S2 (power-law\ndegree distribution), S3 (high clustering coefficient), S4 (\"fit-gets-richer\"\ngrowth mechanism), S5 (truncation of power-law degree distribution), and S7\n(community structure with overlapping boundaries) in the global supply chain\nnetwork.\n"
    },
    {
        "paper_id": 2003.02515,
        "authors": "Steven Y. K. Wong (1), Jennifer Chan (2), Lamiae Azizi (2), and\n  Richard Y. D. Xu (1) ((1) University of Technology Sydney, (2) University of\n  Sydney)",
        "title": "Time-varying neural network for stock return prediction",
        "comments": "35 pages, 9 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We consider the problem of neural network training in a time-varying context.\nMachine learning algorithms have excelled in problems that do not change over\ntime. However, problems encountered in financial markets are often\ntime-varying. We propose the online early stopping algorithm and show that a\nneural network trained using this algorithm can track a function changing with\nunknown dynamics. We compare the proposed algorithm to current approaches on\npredicting monthly U.S. stock returns and show its superiority. We also show\nthat prominent factors (such as the size and momentum effects) and industry\nindicators, exhibit time varying stock return predictiveness. We find that\nduring market distress, industry indicators experience an increase in\nimportance at the expense of firm level features. This indicates that\nindustries play a role in explaining stock returns during periods of heightened\nrisk.\n"
    },
    {
        "paper_id": 2003.02842,
        "authors": "Patrick Chang, Etienne Pienaar, Tim Gebbie",
        "title": "Malliavin-Mancino estimators implemented with non-uniform fast Fourier\n  transforms",
        "comments": "29 pages, 15 figures, 3 tables, 10 algorithms, link to our supporting\n  Julia code: https://github.com/CHNPAT005/PCEPTG-MM-NUFFT; v3: Accepted\n  submitted version for SISC",
        "journal-ref": "SIAM J. Sci. Comput., 2020, 42(6), B1378 - B1403",
        "doi": "10.1137/20M1325903",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We implement and test kernel averaging Non-Uniform Fast Fourier Transform\n(NUFFT) methods to enhance the performance of correlation and covariance\nestimation on asynchronously sampled event-data using the Malliavin-Mancino\nFourier estimator. The methods are benchmarked for Dirichlet and Fej\\'{e}r\nFourier basis kernels. We consider test cases formed from Geometric Brownian\nmotions to replicate synchronous and asynchronous data for benchmarking\npurposes. We consider three standard averaging kernels to convolve the\nevent-data for synchronisation via over-sampling for use with the Fast Fourier\nTransform (FFT): the Gaussian kernel, the Kaiser-Bessel kernel, and the\nexponential of semi-circle kernel. First, this allows us to demonstrate the\nperformance of the estimator with different combinations of basis kernels and\naveraging kernels. Second, we investigate and compare the impact of the\naveraging scales explicit in each averaging kernel and its relationship between\nthe time-scale averaging implicit in the Malliavin-Mancino estimator. Third, we\ndemonstrate the relationship between time-scale averaging based on the number\nof Fourier coefficients used in the estimator to a theoretical model of the\nEpps effect. We briefly demonstrate the methods on Trade-and-Quote (TAQ) data\nfrom the Johannesburg Stock Exchange to make an initial visualisation of the\ncorrelation dynamics for various time-scales under market microstructure.\n"
    },
    {
        "paper_id": 2003.02878,
        "authors": "Shane Barratt, Jonathan Tuck, Stephen Boyd",
        "title": "Convex Optimization Over Risk-Neutral Probabilities",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We consider a collection of derivatives that depend on the price of an\nunderlying asset at expiration or maturity. The absence of arbitrage is\nequivalent to the existence of a risk-neutral probability distribution on the\nprice; in particular, any risk neutral distribution can be interpreted as a\ncertificate establishing that no arbitrage exists. We are interested in the\ncase when there are multiple risk-neutral probabilities. We describe a number\nof convex optimization problems over the convex set of risk neutral price\nprobabilities. These include computation of bounds on the cumulative\ndistribution, VaR, CVaR, and other quantities, over the set of risk-neutral\nprobabilities. After discretizing the underlying price, these problems become\nfinite dimensional convex or quasiconvex optimization problems, and therefore\nare tractable. We illustrate our approach using real options and futures\npricing data for the S&P 500 index and Bitcoin.\n"
    },
    {
        "paper_id": 2003.0299,
        "authors": "Hector Galindo-Silva",
        "title": "Conflict externalization and the quest for peace: theory and case\n  evidence from Colombia",
        "comments": null,
        "journal-ref": "Peace Econ. Peace Sci. Pub. Pol. 2021; 27(1): 29-50",
        "doi": "10.1515/peps-2020-0010",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  I study the relationship between the likelihood of a violent domestic\nconflict and the risk that such a conflict \"externalizes\" (i.e. spreads to\nanother country by creating an international dispute). I consider a situation\nin which a domestic conflict between a government and a rebel group has the\npotential to externalize. I show that the risk of externalization increases the\nlikelihood of a peaceful outcome, but only if the government is sufficiently\npowerful relative to the rebels, the risk of externalization is sufficiently\nhigh, and the foreign actor who can intervene in the domestic conflict is\nsufficiently uninterested in material costs and benefits. I show how this model\nhelps to understand the recent and successful peace process between the\nColombian government and the country's most powerful rebel group, the\nRevolutionary Armed Forces of Colombia (FARC).\n"
    },
    {
        "paper_id": 2003.03035,
        "authors": "Masaaki Fujii and Akihiko Takahashi",
        "title": "A Mean Field Game Approach to Equilibrium Pricing with Market Clearing\n  Condition",
        "comments": "Revised. Forthcoming in SIAM J. Control Optim",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  In this work, we study an equilibrium-based continuous asset pricing problem\nwhich seeks to form a price process endogenously by requiring it to balance the\nflow of sales-and-purchase orders in the exchange market, where a large number\nof agents are interacting through the market price. Adopting a mean field game\n(MFG) approach, we find a special form of forward-backward stochastic\ndifferential equations of McKean-Vlasov type with common noise whose solution\nprovides a good approximate of the market price. We show the convergence of the\nnet order flow to zero in the large N-limit and get the order of convergence in\nN under some conditions. We also extend the model to a setup with multiple\npopulations where the agents within each population share the same cost and\ncoefficient functions but they can be different population by population.\n"
    },
    {
        "paper_id": 2003.03076,
        "authors": "Rachid Guennouni Hassani (X), Alexis Gilles, Emmanuel Lassalle, Arthur\n  D\\'enouveaux",
        "title": "Predicting Stock Returns with Batched AROW",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We extend the AROW regression algorithm developed by Vaits and Crammer in\n[VC11] to handle synchronous mini-batch updates and apply it to stock return\nprediction. By design, the model should be more robust to noise and adapt\nbetter to non-stationarity compared to a simple rolling regression. We\nempirically show that the new model outperforms more classical approaches by\nbacktesting a strategy on S\\&P500 stocks.\n"
    },
    {
        "paper_id": 2003.03403,
        "authors": "Chris Kenyon, Mourad Berrahoui and Benjamin Poncet",
        "title": "Model independent WWR for regulatory CVA and for accounting CVA and FVA",
        "comments": "23 pages, 4 figures, 6 tables",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  General wrong way risk (WWR) estimation is necessary for regulatory CVA\ncapital and useful for pricing CVA and FVA. We introduce a model independent\nmethod for calculating WWR and update the definition of WWR to deal with the\nlack of replication instruments (calibration data) transparently. This model\nindependent approach is extremely simple: we just re-write the CVA and FVA\nintegral expressions in terms of their components and then calibrate these\ncomponents. This provides transparency between component calibration and\nCVA/FVA effect because there is no model interpretation in between. Including\nfunding in WWR means that there are now two WWR terms rather than the usual\none. Using a regulatory inspired calibration from MAR50 we investigate WWR\neffects for vanilla interest rate swaps and show that the WWR effects for FVA\nare significantly more material than for CVA. This model independent approach\ncan also be used to compare any WWR model by simply calibrating to it for a\nportfolio and counterparty, to demonstrate the effects of the model under\ninvestigation in terms of components of CVA/FVA calculations.\n"
    },
    {
        "paper_id": 2003.0354,
        "authors": "Jay Gupta and Swaprava Nath",
        "title": "SkillCheck: An Incentive-based Certification System using Blockchains",
        "comments": "9 pages, 4 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Skill verification is a central problem in workforce hiring. Companies and\nacademia often face the difficulty of ascertaining the skills of an applicant\nsince the certifications of the skills claimed by a candidate are generally not\nimmediately verifiable and costly to test. Blockchains have been proposed in\nthe literature for skill verification and tamper-proof information storage in a\ndecentralized manner. However, most of these approaches deal with storing the\ncertificates issued by traditional universities on the blockchain. Among the\nfew techniques that consider the certification procedure itself, questions like\n(a) scalability with limited staff, (b) uniformity of grades over multiple\nevaluators, or (c) honest effort extraction from the evaluators are usually not\naddressed. We propose a blockchain-based platform named SkillCheck, which\nconsiders the questions above, and ensure several desirable properties. The\nplatform incentivizes effort in grading via payments with tokens which it\ngenerates from the payments of the users of the platform, e.g., the recruiters\nand test-takers. We provide a detailed description of the design of the\nplatform along with the provable properties of the algorithm.\n"
    },
    {
        "paper_id": 2003.03848,
        "authors": "Thiemo Fetzer, Lukas Hensel, Johannes Hermle, Christopher Roth",
        "title": "Coronavirus Perceptions And Economic Anxiety",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We provide one of the first systematic assessments of the development and\ndeterminants of economic anxiety at the onset of the coronavirus pandemic.\nUsing a global dataset on internet searches and two representative surveys from\nthe US, we document a substantial increase in economic anxiety during and after\nthe arrival of the coronavirus. We also document a large dispersion in beliefs\nabout the pandemic risk factors of the coronavirus, and demonstrate that these\nbeliefs causally affect individuals' economic anxieties. Finally, we show that\nindividuals' mental models of infectious disease spread understate non-linear\ngrowth and shape the extent of economic anxiety.\n"
    },
    {
        "paper_id": 2003.03851,
        "authors": "Jose Cruz, Daniel Sevcovic",
        "title": "On solutions of a partial integro-differential equation in Bessel\n  potential spaces with applications in option pricing models",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper we focus on qualitative properties of solutions to a nonlocal\nnonlinear partial integro-differential equation (PIDE). Using the theory of\nabstract semilinear parabolic equations we prove existence and uniqueness of a\nsolution in the scale of Bessel potential spaces. Our aim is to generalize\nknown existence results for a wide class of L\\'evy measures including with a\nstrong singular kernel.\n  As an application we consider a class of PIDEs arising in the financial\nmathematics. The classical linear Black-Scholes model relies on several\nrestrictive assumptions such as liquidity and completeness of the market.\nRelaxing the complete market hypothesis and assuming a Levy stochastic process\ndynamics for the underlying stock price process we obtain a model for pricing\noptions by means of a PIDE. We investigate a model for pricing call and put\noptions on underlying assets following a Levy stochastic process with jumps. We\nprove existence and uniqueness of solutions to the penalized PIDE representing\napproximation of the linear complementarity problem arising in pricing American\nstyle of options under Levy stochastic processes. We also present numerical\nresults and comparison of option prices for various Levy stochastic processes\nmodelling underlying asset dynamics.\n"
    },
    {
        "paper_id": 2003.03876,
        "authors": "Ben Boukai",
        "title": "How much is your Strangle worth? On the relative value of the\n  $\\delta-$Symmetric Strangle under the Black-Scholes model",
        "comments": "15 pages with 3 Figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Trading option strangles is a highly popular strategy often used by market\nparticipants to mitigate volatility risks in their portfolios. In this paper we\npropose a measure of the relative value of a delta-Symmetric Strangle and\ncompute it under the standard Black-Scholes option pricing model. This new\nmeasure accounts for the price of the strangle, relative to the Present Value\nof the spread between the two strikes, all expressed, after a natural\nre-parameterization, in terms of delta and a volatility parameter. We show that\nunder the standard BS option pricing model, this measure of relative value is\nbounded by a simple function of delta only and is independent of the time to\nexpiry, the price of the underlying security or the prevailing volatility used\nin the pricing model. We demonstrate how this bound can be used as a quick {\\it\nbenchmark} to assess, regardless the market volatility, the duration of the\ncontract or the price of the underlying security, the market (relative) value\nof the $\\delta-$strangle in comparison to its BS (relative) price. In fact, the\nexplicit and simple expression for this measure and bound allows us to also\nstudy in detail the strangle's exit strategy and the corresponding {\\it\noptimal} choice for a value of delta.\n"
    },
    {
        "paper_id": 2003.04005,
        "authors": "Claudiu Albulescu (CRIEF)",
        "title": "Coronavirus and financial volatility: 40 days of fasting and fear",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  40 days after the start of the international monitoring of COVID-19, we\nsearch for the effect of official announcements regarding new cases of\ninfection and death ratio on the financial markets volatility index (VIX).\nWhereas the new cases reported in China and outside China have a mixed effect\non financial volatility, the death ratio positively influences VIX, that\noutside China triggering a more important impact. In addition, the higher the\nnumber of affected countries, the higher the financial volatility is.\n"
    },
    {
        "paper_id": 2003.04007,
        "authors": "Claudiu Albulescu (CRIEF), Aviral Tiwari, Qiang Ji",
        "title": "Copula-based local dependence between energy, agriculture and metal\n  commodity markets",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper studies the extreme dependencies between energy, agriculture and\nmetal commodity markets, with a focus on local co-movements, allowing the\nidentification of asymmetries and changing trend in the degree of co-movements.\nMore precisely, starting from a non-parametric mixture copula, we use a novel\ncopula-based local Kendall's tau approach to measure nonlinear local dependence\nin regions. In all pairs of commodity indexes, we find increased co-movements\nin extreme situations, a stronger dependence between energy and other commodity\nmarkets at lower tails, and a 'V-type' local dependence for the energy-metal\npairs. The three-dimensional Kendall's tau plot for upper tails in quantiles\nshows asymmetric co-movements in the energy-metal pairs, which tend to become\nnegative at peak returns. Therefore, we show that the energy market can offer\ndiversification solutions for risk management in the case of extreme bull\nmarket events.\n"
    },
    {
        "paper_id": 2003.0406,
        "authors": "Osman Gulseven",
        "title": "Favoritism in Research Assistantship Selection in Turkish Academia",
        "comments": "7 pages, 3 tables",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This article analyzes the procedure for the initial employment of research\nassistants in Turkish universities to see if it complies with the rules and\nregulations. We manually collected 2409 applicant data from 53 Turkish\nuniversities to see if applicants are ranked according to the rules suggested\nby the Higher Education Council of Turkey. The rulebook states that applicants\nshould be ranked according to a final score based on the weighted average of\ntheir GPA, graduate examination score, academic examination score, and foreign\nlanguage skills score. Thus, the research assistant selection is supposed to be\na fair process where each applicant is evaluated based on objective metrics.\nHowever, our analysis of data suggests that the final score of the applicants\nis almost entirely based on the highly subjective academic examination\nconducted by the hiring institution. Thus, the applicants GPA, standardized\ngraduate examination score, standardized foreign language score are irrelevant\nin the selection process, making it a very unfair process based on favoritism.\n"
    },
    {
        "paper_id": 2003.04129,
        "authors": "Lennart Fernandes and Jacques Tempere",
        "title": "Effect of segregation on inequality in kinetic models of wealth exchange",
        "comments": null,
        "journal-ref": "Eur. Phys. J. B 93, 37 (2020)",
        "doi": "10.1140/epjb/e2020-100534-7",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Empirical distributions of wealth and income can be reproduced using\nsimplified agent-based models of economic interactions, analogous to\nmicroscopic collisions of gas particles. Building upon these models of freely\ninteracting agents, we explore the effect of a segregated economic network in\nwhich interactions are restricted to those between agents of similar wealth.\nAgents on a 2D lattice undergo kinetic exchanges with their nearest neighbours,\nwhile continuously switching places to minimize local wealth differences. A\nspatial concentration of wealth leads to a steady state with increased global\ninequality and a magnified distinction between local and global measures of\ncombatting poverty. Individual saving propensity proves ineffective in the\nsegregated economy, while redistributive taxation transcends the spatial\ninhomogeneity and greatly reduces inequality. Adding fluctuations to the\nsegregation dynamics, we observe a sharp phase transition to lower inequality\nat a critical temperature, accompanied by a sudden change in the distribution\nof the wealthy elite.\n"
    },
    {
        "paper_id": 2003.04307,
        "authors": "M. Okimoto (University of Shizuoka)",
        "title": "Optimal trade strategy of a regional economy by food exports",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper examines the export promotion of processed foods by a regional\neconomy and regional vitalisation policy. We employ Bertrand models that\ncontain a major home producer and a home producer in a local area. In our\nmodel, growth in the profit of one producer does not result in an increase in\nthe profit of the other, despite strategic complements. We show that the profit\nof the producer in the local area decreases because of the deterioration of a\nlocation condition, and its profit increases through the reinforcement of the\nadministrative guidance. Furthermore, when the inefficiency of the location\nworsens, the local government should optimally decrease the level of\nadministrative guidance. Hence, the local government should strategically\neliminate this inefficiency to maintain a sufficient effect of administrative\nguidance.\n"
    },
    {
        "paper_id": 2003.04425,
        "authors": "Umut \\c{C}etin and Henri Waelbroeck",
        "title": "Informed trading, limit order book and implementation shortfall:\n  equilibrium and asymptotics",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We propose a static equilibrium model for limit order book where\nprofit-maximizing investors receive an information signal regarding the\nliquidation value of the asset and execute via a competitive dealer with random\ninitial inventory, who trades against a competitive limit order book populated\nby liquidity suppliers. We show that an equilibrium exists for bounded signal\ndistributions, obtain closed form solutions for Bernoulli-type signals and\npropose a straightforward iterative algorithm to compute the equilibrium order\nbook for the general case. We obtain the exact analytic asymptotics for the\nmarket impact of large trades and show that the functional form depends on the\ntail distribution of the private signal of the insiders. In particular, the\nimpact follows a power law if the signal has fat tails while the law is\nlogarithmic in case of lighter tails. Moreover, the tail distribution of the\ntrade volume in equilibrium obeys a power law in our model. We find that the\nliquidity suppliers charge a minimum bid-ask spread that is independent of the\namount of `noise' trading but increasing in the degree of informational\nadvantage of insiders in equilibrium. The model also predicts that the order\nbook flattens as the amount of noise trading increases converging to a model\nwith proportional transactions costs.. Competition among the insiders leads to\naggressive trading causing the aggregate profit to vanish in the limiting case\n$N\\to\\infty$. The numerical results also show that the spread increases with\nthe number of insiders keeping the other parameters fixed. Finally, an\nequilibrium may not exist if the liquidation value is unbounded. We conjecture\nthat existence of equilibrium requires a sufficient amount of competition among\ninsiders if the signal distribution exhibit fat tails.\n"
    },
    {
        "paper_id": 2003.04452,
        "authors": "Mohsen Momenitabar, Zhila Dehdari Ebrahimi, Mohammad Arani",
        "title": "A Systematic and Analytical Review of the Socioeconomic and\n  Environmental Impact of the Deployed High-Speed Rail (HSR) Systems on the\n  World",
        "comments": "This paper has hes submitted to the journal of Transportation\n  Research Part D: Transport and Environment",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The installation of high-speed rail in the world during the last two decades\nresulted in significant socioeconomic and environmental changes. The U.S. has\nthe longest rail network in the world, but the focus is on carrying a wide\nvariety of loads including coal, farm crops, industrial products, commercial\ngoods, and miscellaneous mixed shipments. Freight and passenger services in the\nU.S. dates to 1970, with both carried out by private railway companies.\nRailways were the main means of transport between cities from the late 19th\ncentury through the middle of the 20th century. However, rapid growth in\nproduction and improvements in technologies changed those dynamics. The fierce\ncompetition for comfortability and pleasantness in passenger travel and the\nproliferation of aviation services in the U.S. channeled federal and state\nbudgets towards motor vehicle infrastructure, which brought demand for\nrailroads to a halt in the 1950s. Presently, the U.S. has no high-speed trains,\naside from sections of Amtrak s Acela line in the Northeast Corridor that can\nreach 150 mph for only 34 miles of its 457-mile span. The average speed between\nNew York and Boston is about 65 mph. On the other hand, China has the world s\nfastest and largest high-speed rail network, with more than 19,000 miles, of\nwhich the vast majority was built in the past decade. Japan s bullet trains can\nreach nearly 200 miles per hour and dates to the 1960s. That system moved more\nthan 9 billion people without a single passenger casualty. In this systematic\nreview, we studied the effect of High-Speed Rail (HSR) on the U.S. and other\ncountries including France, Japan, Germany, Italy, and China in terms of energy\nconsumption, land use, economic development, travel behavior, time use, human\nhealth, and quality of life.\n"
    },
    {
        "paper_id": 2003.04459,
        "authors": "Seyed Hassan Hosseini and Ahmad Mehrabian and Zhila Dehdari Ebrahimi\n  and Mohsen Momenitabar and Mohammad Arani",
        "title": "A New Approach for Macroscopic Analysis to Improve the Technical and\n  Economic Impacts of Urban Interchanges on Traffic Networks",
        "comments": "This paper has been Accepted by the 4th International Conference on\n  Intelligent Decision Science (IDS), and published by Advances in Intelligent\n  Systems and Computing",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Pursuing three important elements including economic, safety, and traffic are\nthe overall objective of decision evaluation across all transport projects. In\nthis study, we investigate the feasibility of the development of city\ninterchanges and road connections for network users. To achieve this goal, a\nseries of minor goals are required to be met in advance including determining\nbenefits, costs of implement-ing new highway interchanges, quantifying the\neffective parameters, the increase in fuel consumption, the reduction in travel\ntime, and finally influence on travel speed. In this study, geometric\nadvancement of Hakim highway, and Yadegar-e-Emam Highway were investigated in\nthe Macro view from the cloverleaf inter-section with a low capacity to a\nthree-level directional intersection of the enhanced cloverleaf. For this\npurpose, the simulation was done by EMME software of INRO Company. The results\nof the method were evaluated by the objective of net present value (NPV), and\nthe benefit and cost of each one was stated precisely in different years. At\nthe end, some suggestion has been provided.\n"
    },
    {
        "paper_id": 2003.04606,
        "authors": "Julian H\\\"olzermann",
        "title": "Pricing Interest Rate Derivatives under Volatility Uncertainty",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper, we study the pricing of contracts in fixed income markets\nunder volatility uncertainty in the sense of Knightian uncertainty or model\nuncertainty. The starting point is an arbitrage-free bond market under\nvolatility uncertainty. The uncertainty about the volatility is modeled by a\nG-Brownian motion, which drives the forward rate dynamics. The absence of\narbitrage is ensured by a drift condition. Such a setting leads to a sublinear\npricing measure for additional contracts, which yields either a single price or\na range of prices. Similar to the forward measure approach, we define the\nforward sublinear expectation to simplify the pricing of cashflows. Under the\nforward sublinear expectation, we obtain a robust version of the expectations\nhypothesis, and we show how to price options on forward prices. In addition, we\ndevelop pricing methods for contracts consisting of a stream of cashflows,\nsince the nonlinearity of the pricing measure implies that we cannot price a\nstream of cashflows by pricing each cashflow separately. With these tools, we\nderive robust pricing formulas for all major interest rate derivatives. The\npricing formulas provide a link to the pricing formulas of traditional models\nwithout volatility uncertainty and show that volatility uncertainty naturally\nleads to unspanned stochastic volatility.\n"
    },
    {
        "paper_id": 2003.0462,
        "authors": "Hiromitsu Goto, Wataru Souma, Mari Jibu and Yuichi Ikeda",
        "title": "Multilayer Network Analysis of the Drug Pipeline in the Global\n  Pharmaceutical Industry",
        "comments": "18 pages, 8 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Generally, open innovation is a lucrative research topic within industries\nrelying on innovation, such as the pharmaceutical industry, which are also\nknown as knowledge-intensive industries. However, the dynamics of drug\npipelines within a small-medium enterprise level in the global economy remains\nconcerning. To reveal the actual situation of pharmaceutical innovation, we\ninvestigate the feature of knowledge flows between the licensor and licensee in\nthe drug pipeline based on a multilayer network constructed with the drug\npipeline, global supply chain, and ownership data. Thus, our results\ndemonstrate proven similarities between the knowledge flows in the drug\npipeline among the supply chains, which generally agrees with the situation of\npharmaceutical innovation collaborated with other industries, such as the\nartificial intelligence industry.\n"
    },
    {
        "paper_id": 2003.04646,
        "authors": "Joachim de Lataillade and Ayman Chaouki",
        "title": "Equations and Shape of the Optimal Band Strategy",
        "comments": "Adding a link to the references",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We consider the problem of the optimal trading strategy in the presence of a\nprice predictor, linear trading costs and a quadratic risk control. The\nsolution is known to be a band system, a policy that induces a no-trading zone\nin the positions space. Using a path-integral method introduced in a previous\nwork, we give equations for the upper and lower edges of this band, and solve\nthem explicitly in the case of an Ornstein-Uhlenbeck predictor. We then explore\nthe shape of this solution and derive its asymptotic behavior for large values\nof the predictor, without requiring trading costs to be small.\n"
    },
    {
        "paper_id": 2003.04938,
        "authors": "Arvind Shrivats, Dena Firoozi, Sebastian Jaimungal",
        "title": "A Mean-Field Game Approach to Equilibrium Pricing in Solar Renewable\n  Energy Certificate Markets",
        "comments": "40 pages, 8 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Solar Renewable Energy Certificate (SREC) markets are a market-based system\nthat incentivizes solar energy generation. A regulatory body imposes a lower\nbound on the amount of energy each regulated firm must generate via solar\nmeans, providing them with a tradeable certificate for each MWh generated.\nFirms seek to navigate the market optimally by modulating their SREC generation\nand trading rates. As such, the SREC market can be viewed as a stochastic game,\nwhere agents interact through the SREC price. We study this stochastic game by\nsolving the mean-field game (MFG) limit with sub-populations of heterogeneous\nagents. Market participants optimize costs accounting for trading frictions,\ncost of generation, non-linear non-compliance costs, and generation\nuncertainty. Moreover, we endogenize SREC price through market clearing. We\ncharacterize firms' optimal controls as the solution of McKean-Vlasov (MV)\nFBSDEs and determine the equilibrium SREC price. We establish the existence and\nuniqueness of a solution to this MV-FBSDE, and prove that the MFG strategies\nform an $\\epsilon$-Nash equilibrium for the finite player game. Finally, we\ndevelop a numerical scheme for solving the MV-FBSDEs and conduct a simulation\nstudy.\n"
    },
    {
        "paper_id": 2003.04967,
        "authors": "Shubhankar Mohapatra, Nauman Ahmed and Paulo Alencar",
        "title": "KryptoOracle: A Real-Time Cryptocurrency Price Prediction Platform Using\n  Twitter Sentiments",
        "comments": "7 pages, 8 figures, IEEE Big Data 2019",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Cryptocurrencies, such as Bitcoin, are becoming increasingly popular, having\nbeen widely used as an exchange medium in areas such as financial transaction\nand asset transfer verification. However, there has been a lack of solutions\nthat can support real-time price prediction to cope with high currency\nvolatility, handle massive heterogeneous data volumes, including social media\nsentiments, while supporting fault tolerance and persistence in real time, and\nprovide real-time adaptation of learning algorithms to cope with new price and\nsentiment data. In this paper we introduce KryptoOracle, a novel real-time and\nadaptive cryptocurrency price prediction platform based on Twitter sentiments.\nThe integrative and modular platform is based on (i) a Spark-based architecture\nwhich handles the large volume of incoming data in a persistent and fault\ntolerant way; (ii) an approach that supports sentiment analysis which can\nrespond to large amounts of natural language processing queries in real time;\nand (iii) a predictive method grounded on online learning in which a model\nadapts its weights to cope with new prices and sentiments. Besides providing an\narchitectural design, the paper also describes the KryptoOracle platform\nimplementation and experimental evaluation. Overall, the proposed platform can\nhelp accelerate decision-making, uncover new opportunities and provide more\ntimely insights based on the available and ever-larger financial data volume\nand variety.\n"
    },
    {
        "paper_id": 2003.05095,
        "authors": "Zura Kakushadze and Willie Yu",
        "title": "Machine Learning Treasury Yields",
        "comments": "68 pages",
        "journal-ref": "Bulletin of Applied Economics 7(1) (2020) 1-65",
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We give explicit algorithms and source code for extracting factors underlying\nTreasury yields using (unsupervised) machine learning (ML) techniques, such as\nnonnegative matrix factorization (NMF) and (statistically deterministic)\nclustering. NMF is a popular ML algorithm (used in computer vision,\nbioinformatics/computational biology, document classification, etc.), but is\noften misconstrued and misused. We discuss how to properly apply NMF to\nTreasury yields. We analyze the factors based on NMF and clustering and their\ninterpretation. We discuss their implications for forecasting Treasury yields\nin the context of out-of-sample ML stability issues.\n"
    },
    {
        "paper_id": 2003.05114,
        "authors": "John F. Raffensperger",
        "title": "A price on warming with a supply chain directed market",
        "comments": "1 page, 0 figures",
        "journal-ref": "Discov Sustain 2, 2 (2021)",
        "doi": "10.1007/s43621-021-00011-4",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Existing emissions trading system (ETS) designs inhibit emissions but do not\nconstrain warming to any fxed level, preventing certainty of the global path of\nwarming. Instead, they have the indirect objective of reducing emissions. They\nprovide poor future price information. And they have high transaction costs for\nimplementation, requiring treaties and laws. To address these shortcomings,\nthis paper proposes a novel double-sided auction mechanism of emissions permits\nand sequestration contracts tied to temperature. This mechanism constrains\nwarming for many (e.g., 150) years into the future and every auction would\nprovide price information for this time range. In addition, this paper proposes\na set of market rules and a bottom-up implementation path. A coalition of\nbusinesses begin implementation with jurisdictions joining as they are ready.\nThe combination of the selected market rules and the proposed implementation\npath appear to incentivize participation. This design appears to be closer to\n\"first best\" with a lower cost of mitigation than any in the literature, while\nincreasing the certainty of avoiding catastrophic warming. This design should\nalso have a faster pathway to implementation. A numerical simulation shows\nsurprising results, e.g., that static prices are wrong, prices should evolve\nover time in a way that contradicts other recent proposals, and \"global warming\npotential\" as used in existing ETSs are generally erroneous.\n"
    },
    {
        "paper_id": 2003.05204,
        "authors": "Olivera Kostoska, Viktor Stojkoski and Ljupco Kocarev",
        "title": "On the structure of the world economy: An absorbing Markov chain\n  approach",
        "comments": null,
        "journal-ref": null,
        "doi": "10.3390/e22040482",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The expansion of global production networks has raised many important\nquestions about the interdependence among countries and how future changes in\nthe world economy are likely to affect the countries' positioning in global\nvalue chains. We are approaching the structure and lengths of value chains from\na completely different perspective than has been available so far. By assigning\na random endogenous variable to a network linkage representing the number of\nintermediate sales/purchases before absorption (final use or value added), the\ndiscrete-time absorbing Markov chains proposed here shed new light on the world\ninput/output networks. The variance of this variable can help assess the risk\nwhen shaping the chain length and optimize the level of production. Contrary to\nwhat might be expected simply on the basis of comparative advantage, the\nresults reveal that both the input and output chains exhibit the same\nquasi-stationary product distribution. Put differently, the expected proportion\nof time spent in a state before absorption is invariant to changes of the\nnetwork type. Finally, the several global metrics proposed here, including the\nprobability distribution of global value added/final output, provide guidance\nfor policy makers when estimating the resilience of world trading system and\nforecasting the macroeconomic developments.\n"
    },
    {
        "paper_id": 2003.05358,
        "authors": "Grzegorz Krzy\\.zanowski, Marcin Magdziarz",
        "title": "A computational weighted finite difference method for American and\n  barrier options in subdiffusive Black-Scholes model",
        "comments": null,
        "journal-ref": "Communications in Nonlinear Science and Numerical Simulation 96\n  (2021): 105676",
        "doi": "10.1016/j.cnsns.2020.105676",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Subdiffusion is a well established phenomenon in physics. In this paper we\napply the subdiffusive dynamics to analyze financial markets. We focus on the\nfinancial aspect of time fractional diffusion model with moving boundary i.e.\nAmerican and barrier option pricing in the subdiffusive Black-Scholes (B-S)\nmodel. Two computational methods for valuing American options in the considered\nmodel are proposed - the weighted finite difference (FD) and the\nLongstaff-Schwartz method. In the article it is also shown how to valuate\nnumerically wide range of barrier options using the FD approach.\n"
    },
    {
        "paper_id": 2003.05708,
        "authors": "Christian Bayer, Chiheb Ben Hammouda, Raul Tempone",
        "title": "Multilevel Monte Carlo with Numerical Smoothing for Robust and Efficient\n  Computation of Probabilities and Densities",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The multilevel Monte Carlo (MLMC) method is highly efficient for estimating\nexpectations of a functional of a solution to a stochastic differential\nequation (SDE). However, MLMC estimators may be unstable and have a poor\n(noncanonical) complexity in the case of low regularity of the functional. To\novercome this issue, we extend our previously introduced idea of numerical\nsmoothing in (Quantitative Finance, 23(2), 209-227, 2023), in the context of\ndeterministic quadrature methods to the MLMC setting. The numerical smoothing\ntechnique is based on root-finding methods combined with one-dimensional\nnumerical integration with respect to a single well-chosen variable. This study\nis motivated by the computation of probabilities of events, pricing options\nwith a discontinuous payoff, and density estimation problems for dynamics where\nthe discretization of the underlying stochastic processes is necessary. The\nanalysis and numerical experiments reveal that the numerical smoothing\nsignificantly improves the strong convergence, and consequently, the complexity\nand robustness (by making the kurtosis at deep levels bounded) of the MLMC\nmethod. In particular, we show that numerical smoothing enables recovering the\nMLMC complexities obtained for Lipschitz functionals due to the optimal\nvariance decay rate when using the Euler--Maruyama scheme. For the Milstein\nscheme, numerical smoothing recovers the canonical MLMC complexity even for the\nnonsmooth integrand mentioned above. Finally, our approach efficiently\nestimates univariate and multivariate density functions.\n"
    },
    {
        "paper_id": 2003.05725,
        "authors": "Serkan Kucuksenel, Osman Gulseven",
        "title": "Electoral systems and international trade policy",
        "comments": "5 pages",
        "journal-ref": "Actual Problems of Economics, 7(121), pages 366-371 (2011)",
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We develop a simple theoretic game a model to analyze the relationship\nbetween electoral sys tems and governments' choice in trade policies. We show\nthat existence of international pressure or foreign lobby changes a\ngovernment's final decision on trade policy, and trade policy in countries with\nproportional electoral system is more protectionist than in countries with\nmajoritarian electoral system. Moreover, lobbies pay more to affect the trade\npolicy outcomes in countries with proportional representation systems.\n"
    },
    {
        "paper_id": 2003.05726,
        "authors": "Osman Gulseven, Kasirga Yildirak",
        "title": "Indemnity Payments in Agricultural Insurance: Risk Exposure of EU States",
        "comments": "8 pages, 3 tables",
        "journal-ref": "Risk Exposure of EU States. Actual Problems of Economics, (127),\n  381-388 (2012)",
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This study estimates the risk contributions of individual European countries\nregarding the indemnity payments in agricultural insurance. We model the total\nrisk exposure as an insurance portfolio where each country is unique in terms\nof its risk characteristics. The data has been collected from the recent\nsurveys conducted by the European Commission and the World Bank. Farm\nAccountancy Data Network is used as well. 22 out of 26 member states are\nincluded in the study. The results suggest that the EuroMediterranean countries\nare the major risk contributors. These countries not only have the highest\nexpected loss but also high volatility of indemnity payments. Nordic countries\nhave the lowest indemnity payments and risk exposure.\n"
    },
    {
        "paper_id": 2003.0575,
        "authors": "Osman Gulseven",
        "title": "Multidimensional Analysis of Monthly Stock Market Returns",
        "comments": null,
        "journal-ref": "Annals of the Alexandru Ioan Cuza University - Economics, 61(2),\n  181 - 196 (2014)",
        "doi": "10.2478/aicue-2014-0013",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This study examines the monthly returns in Turkish and American stock market\nindices to investigate whether these markets experience abnormal returns during\nsome months of the calendar year. The data used in this research includes 212\nobservations between January 1996 and August 2014. I apply statistical summary\nanalysis, decomposition technique, dummy variable estimation, and binary\nlogistic regression to check for the monthly market anomalies. The\nmultidimensional methods used in this article suggest weak evidence against the\nefficient market hypothesis on monthly returns. While some months tend to show\nabnormal returns, there is no absolute unanimity in the applied approaches.\nNevertheless, there is a strikingly negative May effect on the Turkish stocks\nfollowing a positive return in April. Stocks tend to be bullish in December in\nboth markets, yet we do not observe anya significant January effect is not\nobserved.\n"
    },
    {
        "paper_id": 2003.05797,
        "authors": "Marcelo Brutti Righi and Marlon Ruoso Moresco",
        "title": "Inf-convolution and optimal risk sharing with countable sets of risk\n  measures",
        "comments": "Ann Oper Res (2022)",
        "journal-ref": null,
        "doi": "10.1007/s10479-022-04593-8",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The inf-convolution of risk measures is directly related to risk sharing and\ngeneral equilibrium, and it has attracted considerable attention in\nmathematical finance and insurance problems. However, the theory is restricted\nto finite sets of risk measures. This study extends the inf-convolution of risk\nmeasures in its convex-combination form to a countable (not necessarily finite)\nset of alternatives. The intuitive meaning of this approach is to represent a\ngeneralization of the current finite convex weights to the countable case.\nSubsequently, we extensively generalize known properties and results to this\nframework. Specifically, we investigate the preservation of properties, dual\nrepresentations, optimal allocations, and self-convolution.\n"
    },
    {
        "paper_id": 2003.05807,
        "authors": "Christian Bongiorno and Damien Challet",
        "title": "Covariance matrix filtering with bootstrapped hierarchies",
        "comments": "10 pages, 9 figures",
        "journal-ref": null,
        "doi": "10.1371/journal.pone.0245092",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Statistical inference of the dependence between objects often relies on\ncovariance matrices. Unless the number of features (e.g. data points) is much\nlarger than the number of objects, covariance matrix cleaning is necessary to\nreduce estimation noise. We propose a method that is robust yet flexible enough\nto account for fine details of the structure covariance matrix. Robustness\ncomes from using a hierarchical ansatz and dependence averaging between\nclusters; flexibility comes from a bootstrap procedure. This method finds\nseveral possible hierarchical structures in DNA microarray gene expression\ndata, and leads to lower realized risk in global minimum variance portfolios\nthan current filtering methods when the number of data points is relatively\nsmall.\n"
    },
    {
        "paper_id": 2003.05895,
        "authors": "Michael Filletti",
        "title": "Investigating the influence Brexit had on Financial Markets, in\n  particular the GBP/EUR exchange rate",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  On 23rd June 2016, 51.9% of British voters voted to leave the European Union,\ntriggering a process and events that have led to the United Kingdom leaving the\nEU, an event that has become known as 'Brexit'. In this piece of research, we\ninvestigate the effects of this entire process on the currency markets,\nspecifically the GBP/EUR exchange rate. Financial markets are known to be\nsensitive to news articles and media, and the aim of this research is to\nevaluate the magnitude of impact of relevant events, as well as whether the\nimpact was positive or negative for the GBP.\n"
    },
    {
        "paper_id": 2003.05958,
        "authors": "Paul Jusselin",
        "title": "Optimal market making with persistent order flow",
        "comments": "4 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  \\noindent We address the issue of market making on electronic markets when\ntaking into account the clustering and long memory properties of market order\nflows. We consider a market model with one market maker and order flows driven\nby general Hawkes processes. We formulate the market maker's objective as a\nstochastic control problem. We characterize an optimal control by proving\nexistence and uniqueness of a viscosity solution to the associated\nHamilton-Jacobi-Bellman equation. Finally we propose a fully consistent\nnumerical method allowing to implement this optimal strategy in practice.\n"
    },
    {
        "paper_id": 2003.06019,
        "authors": "Taurai Muvunza, Terrill Frantz",
        "title": "Disturbing the Peace: Anatomy of the Hostile Takeover of China Vanke Co",
        "comments": "Latex version 3.9, double pages, 10 pages, 5 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Wang Shi, a business mogul who created his empire of wealth from scratch,\nrelished in his fame and basked in the glory of his affluent business. Nothing\nlasts forever! After mastering the turbulent business of real estate\ndevelopment in his country and therefore enjoying a rising and robust stock\nprice, China Vanke Co. Ltd (\"Vanke\") founder and Chairman of the Board of\nDirectors, Wang Shi was suddenly presented with a scathing notice from the Hong\nKong Stock Exchange: rival Baoneng Group (\"Baoneng\") filed the regulatory\ndocumentation indicating that it had nicodemously acquired 5% of his company\nand was looking to buy more. Vanke case became brutal and sparked national\ncontroversy over corporate governance and the role of Chinese government in\ncapital markets.\n"
    },
    {
        "paper_id": 2003.06184,
        "authors": "Claudiu Albulescu (CRIEF)",
        "title": "Coronavirus and oil price crash",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Coronavirus (COVID-19) creates fear and uncertainty, hitting the global\neconomy and amplifying the financial markets volatility. The oil price reaction\nto COVID-19 was gradually accommodated until March 09, 2020, when, 49 days\nafter the release of the first coronavirus monitoring report by the World\nHealth Organization (WHO), Saudi Arabia floods the market with oil. As a\nresult, international prices drop with more than 20% in one single day. Against\nthis background, the purpose of this paper is to investigate the impact of\nCOVID-19 numbers on crude oil prices, while controlling for the impact of\nfinancial volatility and the United States (US) economic policy uncertainty.\nOur ARDL estimation shows that the COVID-19 daily reported cases of new\ninfections have a marginal negative impact on the crude oil prices in the long\nrun. Nevertheless, by amplifying the financial markets volatility, COVID-19\nalso has an indirect effect on the recent dynamics of crude oil prices.\n"
    },
    {
        "paper_id": 2003.06218,
        "authors": "Fan Jiang, Xin Zang, Jingping Yang",
        "title": "Asymptotic expansion for the transition densities of stochastic\n  differential equations driven by the gamma processes",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper, enlightened by the asymptotic expansion methodology developed\nby Li(2013b) and Li and Chen (2016), we propose a Taylor-type approximation for\nthe transition densities of the stochastic differential equations (SDEs) driven\nby the gamma processes, a special type of Levy processes. After representing\nthe transition density as a conditional expectation of Dirac delta function\nacting on the solution of the related SDE, the key technical method for\ncalculating the expectation of multiple stochastic integrals conditional on the\ngamma process is presented. To numerically test the efficiency of our method,\nwe examine the pure jump Ornstein--Uhlenbeck (OU) model and its extensions to\ntwo jump-diffusion models. For each model, the maximum relative error between\nour approximated transition density and the benchmark density obtained by the\ninverse Fourier transform of the characteristic function is sufficiently small,\nwhich shows the efficiency of our approximated method.\n"
    },
    {
        "paper_id": 2003.06249,
        "authors": "Cheng Cai, Tiziano De Angelis, Jan Palczewski",
        "title": "Optimal hedging of a perpetual American put with a single trade",
        "comments": "Section 6 added and Section 7 expanded",
        "journal-ref": null,
        "doi": "10.1137/20M1325265",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  It is well-known that using delta hedging to hedge financial options is not\nfeasible in practice. Traders often rely on discrete-time hedging strategies\nbased on fixed trading times or fixed trading prices (i.e., trades only occur\nif the underlying asset's price reaches some predetermined values). Motivated\nby this insight and with the aim of obtaining explicit solutions, we consider\nthe seller of a perpetual American put option who can hedge her portfolio once\nuntil the underlying stock price leaves a certain range of values $(a,b)$. We\ndetermine optimal trading boundaries as functions of the initial stock holding,\nand an optimal hedging strategy for a bond/stock portfolio. Optimality here\nrefers to the variance of the hedging error at the (random) time when the stock\nleaves the interval $(a,b)$. Our study leads to analytical expressions for both\nthe optimal boundaries and the optimal stock holding, which can be evaluated\nnumerically with no effort.\n"
    },
    {
        "paper_id": 2003.06365,
        "authors": "Ziming Gao, Yuan Gao, Yi Hu, Zhengyong Jiang, Jionglong Su",
        "title": "Application of Deep Q-Network in Portfolio Management",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Machine Learning algorithms and Neural Networks are widely applied to many\ndifferent areas such as stock market prediction, face recognition and\npopulation analysis. This paper will introduce a strategy based on the classic\nDeep Reinforcement Learning algorithm, Deep Q-Network, for portfolio management\nin stock market. It is a type of deep neural network which is optimized by Q\nLearning. To make the DQN adapt to financial market, we first discretize the\naction space which is defined as the weight of portfolio in different assets so\nthat portfolio management becomes a problem that Deep Q-Network can solve.\nNext, we combine the Convolutional Neural Network and dueling Q-net to enhance\nthe recognition ability of the algorithm. Experimentally, we chose five\nlowrelevant American stocks to test the model. The result demonstrates that the\nDQN based strategy outperforms the ten other traditional strategies. The profit\nof DQN algorithm is 30% more than the profit of other strategies. Moreover, the\nSharpe ratio associated with Max Drawdown demonstrates that the risk of policy\nmade with DQN is the lowest.\n"
    },
    {
        "paper_id": 2003.06497,
        "authors": "Ayman Chaouki, Stephen Hardiman, Christian Schmidt, Emmanuel\n  S\\'eri\\'e, and Joachim de Lataillade",
        "title": "Deep Deterministic Portfolio Optimization",
        "comments": "Minor typo",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Can deep reinforcement learning algorithms be exploited as solvers for\noptimal trading strategies? The aim of this work is to test reinforcement\nlearning algorithms on conceptually simple, but mathematically non-trivial,\ntrading environments. The environments are chosen such that an optimal or\nclose-to-optimal trading strategy is known. We study the deep deterministic\npolicy gradient algorithm and show that such a reinforcement learning agent can\nsuccessfully recover the essential features of the optimal trading strategies\nand achieve close-to-optimal rewards.\n"
    },
    {
        "paper_id": 2003.06903,
        "authors": "Alexander Lipton",
        "title": "Old Problems, Classical Methods, New Solutions",
        "comments": "29 pages, 9 figures, Conference Proceedings \"45 Years after the\n  publication of the Black Scholes Merton Model\", Jerusalem, 2018",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We use a powerful extension of the classical method of heat potentials,\nrecently developed by the present author and his collaborators, to solve\nseveral significant problems of financial mathematics. We consider the\nfollowing problems in detail: (A) calibrating the default boundary in the\nstructural default framework to a constant default intensity; (B) calculating\ndefault probability for a representative bank in the mean-field framework; (C)\nfinding the hitting time probability density of an Ornstein-Uhlenbeck process.\nSeveral other problems, including pricing American put options and finding\noptimal mean-reverting trading strategies, are mentioned in passing. Besides,\ntwo non-financial applications -- the supercooled Stefan problem and the\nintegrate-and-fire neuroscience problem -- are briefly discussed as well.\n"
    },
    {
        "paper_id": 2003.06987,
        "authors": "Kelvin Say, Wolf-Peter Schill, Michele John",
        "title": "Degrees of displacement: The impact of household PV battery prosumage on\n  utility generation and storage",
        "comments": null,
        "journal-ref": "Applied Energy, Volume 276, 15 October 2020, 115466",
        "doi": "10.1016/j.apenergy.2020.115466",
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Reductions in the cost of PV and batteries encourage households to invest in\nPV battery prosumage. We explore the implications for the rest of the power\nsector by applying two open-source techno-economic models to scenarios in\nWestern Australia for the year 2030. Household PV capacity generally\nsubstitutes utility PV, but slightly less so as additional household batteries\nare installed. Wind power is less affected, especially in scenarios with higher\nshares of renewables. With household batteries operating to maximise\nself-consumption, utility battery capacities are hardly substituted. Wholesale\nprices to supply households, including those not engaging in prosumage,\nslightly decrease, while prices for other consumers slightly increase. We\nconclude that the growth of prosumage has implications on the various elements\nof the power sector and should be more thoroughly considered by investors,\nregulators, and power sector planners.\n"
    },
    {
        "paper_id": 2003.07058,
        "authors": "Hirdesh K. Pharasi, Eduard Seligman, and Thomas H. Seligman",
        "title": "Market states: A new understanding",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We present the clustering analysis of the financial markets of S&P 500 (USA)\nand Nikkei 225 (JPN) markets over a period of 2006-2019 as an example of a\ncomplex system. We investigate the statistical properties of correlation\nmatrices constructed from the sliding epochs. The correlation matrices can be\nclassified into different clusters, named as market states based on the\nsimilarity of correlation structures. We cluster the S&P 500 market into four\nand Nikkei 225 into six market states by optimizing the value of intracluster\ndistances. The market shows transitions between these market states and the\nstatistical properties of the transitions to critical market states can\nindicate likely precursors to the catastrophic events. We also analyze the same\nclustering technique on surrogate data constructed from average correlations of\nmarket states and the fluctuations arise due to the white noise of short time\nseries. We use the correlated Wishart orthogonal ensemble for the construction\nof surrogate data whose average correlation equals the average of the real\ndata.\n"
    },
    {
        "paper_id": 2003.07197,
        "authors": "Osman Gulseven, Michael Wohlgenant",
        "title": "A Hedonic Metric Approach to Estimating the Demand for Differentiated\n  Products: An Application to Retail Milk Demand",
        "comments": "27 pages, 7 tables. Presented at The 84th Annual Conference of the\n  Agricultural Economics Society at Edinburgh, UK. An Application to Retail\n  Milk Demand. 84th Annual Conference, March, 2010, Edinburgh, Scotland",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This article introduces the Hedonic Metric (HM) approach as an original\nmethod to model the demand for differentiated products. Using this approach,\ninitially, we create an n-dimensional hedonic space based on the characteristic\ninformation available to consumers. Next, we allocate products into this space\nand estimate the elasticities using distances. Our model makes it possible to\nestimate a large number of differentiated products in a single demand system.\nWe applied our model to estimate the retail demand for fluid milk products.\n"
    },
    {
        "paper_id": 2003.07591,
        "authors": "Claudiu Albulescu (CRIEF)",
        "title": "Do COVID-19 and crude oil prices drive the US economic policy\n  uncertainty?",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper investigates the effect of the novel coronavirus and crude oil\nprices on the United States (US) economic policy uncertainty (EPU). Using daily\ndata for the period January 21-March 13, 2020, our Autoregressive Distributed\nLag (ARDL) model shows that the new infection cases reported at global level,\nand the death ratio, have no significant effect on the US EPU, whereas the oil\nprice negative dynamics leads to increased uncertainty. However, analyzing the\nsituation outside China, we discover that both new case announcements and the\nCOVID-19 associated death ratio have a positive influence on the US EPU.\n"
    },
    {
        "paper_id": 2003.07648,
        "authors": "Paul Dommel, Alois Pichler",
        "title": "Convex Risk Measures based on Divergence",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Risk measures connect probability theory or statistics to optimization,\nparticularly to convex optimization. They are nowadays standard in applications\nof finance and in insurance involving risk aversion. This paper investigates a\nwide class of risk measures on Orlicz spaces. The characterizing function\ndescribes the decision maker's risk assessment towards increasing losses. We\nlink the risk measures to a crucial formula developed by Rockafellar for the\nAverage Value-at-Risk based on convex duality, which is fundamental in\ncorresponding optimization problems. We characterize the dual and provide\ncomplementary representations.\n"
    },
    {
        "paper_id": 2003.0786,
        "authors": "Eric Blankmeyer",
        "title": "NISE Estimation of an Economic Model of Crime",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  An economic model of crime is used to explore the consistent estimation of a\nsimultaneous linear equation without recourse to instrumental variables. A\nmaximum-likelihood procedure (NISE) is introduced, and its results are compared\nto ordinary least squares and two-stage least squares. The paper is motivated\nby previous research on the crime model and by the well-known practical problem\nthat valid instruments are frequently unavailable.\n"
    },
    {
        "paper_id": 2003.07967,
        "authors": "Lane P. Hughston and Leandro S\\'anchez-Betancourt",
        "title": "Pricing with Variance Gamma Information",
        "comments": "24 pages, 4 figures, to appear in Risks",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In the information-based pricing framework of Brody, Hughston and Macrina,\nthe market filtration $\\{ \\mathcal F_t\\}_{t\\geq 0}$ is generated by an\ninformation process $\\{ \\xi_t\\}_{t\\geq0}$ defined in such a way that at some\nfixed time $T$ an $\\mathcal F_T$-measurable random variable $X_T$ is\n\"revealed\". A cash flow $H_T$ is taken to depend on the market factor $X_T$,\nand one considers the valuation of a financial asset that delivers $H_T$ at\n$T$. The value $S_t$ of the asset at any time $t\\in[0,T)$ is the discounted\nconditional expectation of $H_T$ with respect to $\\mathcal F_t$, where the\nexpectation is under the risk neutral measure and the interest rate is\nconstant. Then $S_{T^-} = H_T$, and $S_t = 0$ for $t\\geq T$. In the general\nsituation one has a countable number of cash flows, and each cash flow can\ndepend on a vector of market factors, each associated with an information\nprocess. In the present work, we construct a new class of models for the market\nfiltration based on the variance-gamma process. The information process is\nobtained by subordinating a particular type of Brownian random bridge with a\ngamma process. The filtration is taken to be generated by the information\nprocess together with the gamma bridge associated with the gamma subordinator.\nWe show that the resulting extended information process has the Markov property\nand hence can be used to price a variety of different financial assets, several\nexamples of which are discussed in detail.\n"
    },
    {
        "paper_id": 2003.07992,
        "authors": "Andrew Lesniewski and Nicholas Lesniewski",
        "title": "Options on infectious diseases",
        "comments": "Typo fixed",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We present a parsimonious stochastic model for valuation of options on the\nfraction of infected individuals during an epidemic. The underlying stochastic\ndynamical system is a stochastic differential version of the SIR model of\nmathematical epidemiology.\n"
    },
    {
        "paper_id": 2003.08064,
        "authors": "Hector Galindo-Silva",
        "title": "Ethnic Groups' Access to State Power and Group Size",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Many countries are ethnically diverse. However, despite the benefits of\nethnic heterogeneity, ethnic-based political inequality and discrimination are\npervasive. Why is this? This study suggests that part of the variation in\nethnic-based political inequality depends on the relative size of ethnic groups\nwithin each country. Using group-level data for 569 ethnic groups in 175\ncountries from 1946 to 2017, I find evidence of an inverted-U-shaped\nrelationship between an ethnic group's relative size and its access to power.\nThis single-peaked relationship is robust to many alternative specifications,\nand a battery of robustness checks suggests that relative size influences\naccess to power. Through a very simple model, I propose an explanation based on\nan initial high level of political inequality, and on the incentives that more\npowerful groups have to continue limiting other groups' access to power. This\nexplanation incorporates essential elements of several existing theories on the\nrelationship between group size and discrimination, and suggests a new\nempirical prediction: the single-peaked pattern should be weaker in countries\nwhere political institutions have historically been less open. This additional\nprediction is supported by the data.\n"
    },
    {
        "paper_id": 2003.08091,
        "authors": "Dimitrios Tsiotas",
        "title": "Modeling of the Greek road transportation network using complex network\n  analysis",
        "comments": "in Greek",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This article studies the interregional Greek road network (GRN) by applying\ncomplex network analysis (CNA) and an empirical approach. The study aims to\nextract the socioeconomic information immanent to the GRN's topology and to\ninterpret the way in which this road network serves and promotes the regional\ndevelopment. The analysis shows that the topology of the GRN is submitted to\nspatial constraints, having lattice-like characteristics. Also, the GRN's\nstructure is described by a gravity pattern, where places of higher population\nenjoy greater functionality, and its interpretation in regional terms\nillustrates the elementary pattern expressed by regional development through\nroad construction. The study also reveals some interesting contradictions\nbetween the metropolitan and non-metropolitan (excluding Attica and\nThessaloniki) comparison. Overall, the article highlights the effectiveness of\nusing complex network analysis in the modeling of spatial networks and in\nparticular of transportation systems and promotes the use of the network\nparadigm in the spatial and regional research.\n"
    },
    {
        "paper_id": 2003.08094,
        "authors": "Dimitrios Tsiotas, Martha Geraki, Spyros Niavis",
        "title": "Transportation networks and their significance to economic development",
        "comments": "in Greek",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This article attempts to highlight the importance that transportation has in\nthe economic development of Greece and in particular the importance of the\ntransportation infrastructure and transportation networks, which suggest a\nfixed structured capital covering the total of the country. For this purpose,\nlongitudinal and cross-sectoral statistical data are examined over a set of\nfundamental macroeconomic measures and metrics. Furthermore, the study attempts\nto highlight the structural and functional aspects composing the concept of\ntransportation networks and to highlight the necessity of their joint\nconsideration on the relevant research. The transportation networks that are\nexamined in this paper are the Greek road (GRN), rail (GRAN), maritime (GMN)\nand air transport network (GAN), which are studied both in terms of their\ngeometry and technical characteristics, as well as of their historical, traffic\nand political framework. For the empirical assessment of the transportation\nnetworks importance in Greece an econometric model is constructed, expressing\nthe welfare level of the Greek regions as a multivariate function of their\ntransportation infrastructure and of their socioeconomic environment. The\nfurther purpose of the article is to highlight, macroscopically, all the\naspects related the study of transportation infrastructure and networks.\n"
    },
    {
        "paper_id": 2003.08096,
        "authors": "Dimitrios Tsiotas and Konstantinos Raptopoulos",
        "title": "The commuting phenomenon as a complex network: The case of Greece",
        "comments": "in Greek",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This article studies the Greek interregional commuting network (GRN) by using\nmeasures and methods of complex network analysis and empirical techniques. The\nstudy aims to detect structural characteristics of the commuting phenomenon,\nwhich are configured by the functionality of the land transport\ninfrastructures, and to interpret how this network serves and promotes the\nregional development. In the empirical analysis, a multiple linear regression\nmodel for the number of commuters is constructed, which is based on the\nconceptual framework of the term network, in effort to promote the\ninterdisciplinary dialogue. The analysis highlights the effect of the spatial\nconstraints on the network's structure, provides information on the major road\ntransport infrastructure projects that constructed recently and influenced the\ncountry capacity, and outlines a gravity pattern describing the commuting\nphenomenon, which expresses that cities of high population attract large\nvolumes of commuting activity within their boundaries, a fact that contributes\nto the reduction of their outgoing commuting and consequently to the increase\nof their inbound productivity. Overall, this paper highlights the effectiveness\nof complex network analysis in the modeling of spatial and particularly of\ntransportation network and promotes the use of the network paradigm in the\nspatial and regional research.\n"
    },
    {
        "paper_id": 2003.08137,
        "authors": "Xinyi Guo and Jinfeng Li",
        "title": "A Novel Twitter Sentiment Analysis Model with Baseline Correlation for\n  Financial Market Prediction with Improved Efficiency",
        "comments": "2019 Sixth IEEE International Conference on Social Networks Analysis,\n  Management and Security (SNAMS)",
        "journal-ref": "Proceedings of 2019 Sixth IEEE International Conference on Social\n  Networks Analysis, Management and Security (SNAMS), Granada, Spain, 2019, pp.\n  472-477",
        "doi": "10.1109/SNAMS.2019.8931720",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  A novel social networks sentiment analysis model is proposed based on Twitter\nsentiment score (TSS) for real-time prediction of the future stock market price\nFTSE 100, as compared with conventional econometric models of investor\nsentiment based on closed-end fund discount (CEFD). The proposed TSS model\nfeatures a new baseline correlation approach, which not only exhibits a decent\nprediction accuracy, but also reduces the computation burden and enables a fast\ndecision making without the knowledge of historical data. Polynomial\nregression, classification modelling and lexicon-based sentiment analysis are\nperformed using R. The obtained TSS predicts the future stock market trend in\nadvance by 15 time samples (30 working hours) with an accuracy of 67.22% using\nthe proposed baseline criterion without referring to historical TSS or market\ndata. Specifically, TSS's prediction performance of an upward market is found\nfar better than that of a downward market. Under the logistic regression and\nlinear discriminant analysis, the accuracy of TSS in predicting the upward\ntrend of the future market achieves 97.87%.\n"
    },
    {
        "paper_id": 2003.08302,
        "authors": "Robert A. Jarrow, Rinald Murataj, Martin T. Wells, Liao Zhu",
        "title": "The Low-volatility Anomaly and the Adaptive Multi-Factor Model",
        "comments": "29 pages, 11 figures, 10 tables",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The paper provides a new explanation of the low-volatility anomaly. We use\nthe Adaptive Multi-Factor (AMF) model estimated by the Groupwise Interpretable\nBasis Selection (GIBS) algorithm to find those basis assets significantly\nrelated to low and high volatility portfolios. These two portfolios load on\nvery different factors, indicating that volatility is not an independent risk,\nbut that it's related to existing risk factors. The out-performance of the\nlow-volatility portfolio is due to the (equilibrium) performance of these\nloaded risk factors. The AMF model outperforms the Fama-French 5-factor model\nboth in-sample and out-of-sample.\n"
    },
    {
        "paper_id": 2003.0845,
        "authors": "Ali Al-Aradi, Sebastian Jaimungal",
        "title": "A Variational Analysis Approach to Solving the Merton Problem",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We address the Merton problem of maximizing the expected utility of terminal\nwealth using techniques from variational analysis. Under a general continuous\nsemimartingale market model with stochastic parameters, we obtain a\ncharacterization of the optimal portfolio for general utility functions in\nterms of a forward-backward stochastic differential equation (FBSDE) and derive\nsolutions for a number of well-known utility functions. Our results complement\na previous studies conducted on optimal strategies in markets driven by\nBrownian noise with random drift and volatility parameters.\n"
    },
    {
        "paper_id": 2003.0881,
        "authors": "Nicola Cufaro Petroni and Piergiacomo Sabino",
        "title": "Gamma Related Ornstein-Uhlenbeck Processes and their Simulation",
        "comments": "27 pages, 2 figures, 4 tables. arXiv admin note: substantial text\n  overlap with arXiv:1908.03137",
        "journal-ref": "Journal of Statistical Computation and Simulation, 2020",
        "doi": "10.1080/00949655.2020.1842408",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We investigate the distributional properties of two generalized\nOrnstein-Uhlenbeck (OU) processes whose stationary distributions are the gamma\nlaw and the bilateral gamma law, respectively. The said distributions turn out\nto be related to the self-decomposable gamma and bilateral gamma laws, and\ntheir densities and characteristic functions are here given in closed-form.\nAlgorithms for the exact generation of such processes are accordingly derived\nwith the advantage of being significantly faster than those available in the\nliterature and therefore suitable for real-time simulations.\n"
    },
    {
        "paper_id": 2003.08853,
        "authors": "Peter Carr and Andrey Itkin",
        "title": "Semi-closed form solutions for barrier and American options written on a\n  time-dependent Ornstein Uhlenbeck process",
        "comments": "16 pages, 5 figures, 2 tables",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper we develop a semi-closed form solutions for the barrier\n(perhaps, time-dependent) and American options written on the underlying stock\nwhich follows a time-dependent OU process with a log-normal drift. This model\nis equivalent to the familiar Hull-White model in FI, or a time dependent OU\nmodel in FX. Semi-closed form means that given the time-dependent interest\nrate, continuous dividend and volatility functions, one need to solve\nnumerically a linear (for the barrier option) or nonlinear (for the American\noption) Fredholm equation of the first kind. After that the option prices in\nall cases are presented as one-dimensional integrals of combination of the\nabove solutions and Jacobi theta functions. We also demonstrate that\ncomputationally our method is more efficient than the backward finite\ndifference method used for solving these problems, and can also be as efficient\nas the forward finite difference solver while providing better accuracy and\nstability.\n"
    },
    {
        "paper_id": 2003.09167,
        "authors": "Luca De Benedictis, Silvia Leoni",
        "title": "Gender bias in the Erasmus students network",
        "comments": null,
        "journal-ref": null,
        "doi": "10.1007/s41109-020-00297-9",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The Erasmus Program (EuRopean community Action Scheme for the Mobility of\nUniversity Students), the most important student exchange program in the world,\nfinanced by the European Union and started in 1987, is characterized by a\nstrong gender bias. Girls participate to the program more than boys. This work\nquantifies the gender bias in the Erasmus program between 2008 and 2013, using\nnovel data at the university level. It describes the structure of the program\nin great details, carrying out the analysis across fields of study, and\nidentifies key universities as senders and receivers. In addition, it tests the\ndifference in the degree distribution of the Erasmus network along time and\nbetween genders, giving evidence of a greater density in the female Erasmus\nnetwork with respect to the one of the male Erasmus network.\n"
    },
    {
        "paper_id": 2003.09225,
        "authors": "A. B. Leoneti and G. A. Prataviera",
        "title": "Entropy-Norm space for geometric selection of strict Nash equilibria in\n  n-person games",
        "comments": "10 pages, 1 table, 1 figure, accepted in Physica A",
        "journal-ref": null,
        "doi": "10.1016/j.physa.2020.124407",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Motivated by empirical evidence that individuals within group decision making\nsimultaneously aspire to maximize utility and avoid inequality we propose a\ncriterion based on the entropy-norm pair for geometric selection of strict Nash\nequilibria in n-person games. For this, we introduce a mapping of an n-person\nset of Nash equilibrium utilities in an Entropy-Norm space. We suggest that the\nmost suitable group choice is the equilibrium closest to the largest\nentropy-norm pair of a rescaled Entropy-Norm space. Successive application of\nthis criterion permits an ordering of the possible Nash equilibria in an\nn-person game accounting simultaneously equality and utility of players\npayoffs. Limitations of this approach for certain exceptional cases are\ndiscussed. In addition, the criterion proposed is applied and compared with the\nresults of a group decision making experiment.\n"
    },
    {
        "paper_id": 2003.09255,
        "authors": "Fei Sun, Yichuan Dong",
        "title": "Complex risk statistics with scenario analysis",
        "comments": "arXiv admin note: text overlap with arXiv:1812.06185",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Complex risk is a critical factor for both intelligent systems and risk\nmanagement. In this paper, we consider a special class of risk statistics,\nnamed complex risk statistics. Our result provides a new approach for\naddressing complex risk, especially in deep neural networks. By further\ndeveloping the properties related to complex risk statistics, we are able to\nderive dual representation for such risk.\n"
    },
    {
        "paper_id": 2003.09276,
        "authors": "Richard S.J. Tol",
        "title": "Kernel density decomposition with an application to the social cost of\n  carbon",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  A kernel density is an aggregate of kernel functions, which are itself\ndensities and could be kernel densities. This is used to decompose a kernel\ninto its constituent parts. Pearson's test for equality of proportions is\napplied to quantiles to test whether the component distributions differ from\none another. The proposed methods are illustrated with a meta-analysis of the\nsocial cost of carbon. Different discount rates lead to significantly different\nPigou taxes, but not different growth rates. Estimates have not varied over\ntime. Different authors have contributed different estimates, but these\ndifferences are insignificant. Kernel decomposition can be applied in many\nother fields with discrete explanatory variables.\n"
    },
    {
        "paper_id": 2003.09298,
        "authors": "Andreas A. Aigner, Walter Schrabmair",
        "title": "Power Assisted Trend Following",
        "comments": "13 pages, 16 figures",
        "journal-ref": null,
        "doi": "10.13140/RG.2.2.20898.17605",
        "license": "http://creativecommons.org/licenses/by-sa/4.0/",
        "abstract": "  'The trend is your friend' is a common saying, the difficulty lies in\ndetermining if and when you are in a trend. Is the trend strong enough to\ntrade? When does the trend reverse and how are you going to determine this? We\nwill try and answer at least some of these questions here. We are deriving a\nnovel indicator to measure the power of a trend using digital signal processing\ntechniques, separating the Signal from the Noise. We apply these to examples as\nwell as real data and evaluate the accuracy of these and the relation to PNL\nperformance of the 'Volatility Index' trend following algorithm devised by J.\nWelles Wilder Jr. in 1978.\n"
    },
    {
        "paper_id": 2003.093,
        "authors": "Andreas A. Aigner, Walter Schrabmair",
        "title": "Graham's Formula for Valuing Growth Stocks",
        "comments": "9 pages, 5 figures",
        "journal-ref": null,
        "doi": "10.13140/RG.2.2.25154.32969",
        "license": "http://creativecommons.org/licenses/by-sa/4.0/",
        "abstract": "  Benjamin Graham introduced a very simple formula for valuing a growth stock\nin 1962. How does it work and why? What is a sensible way to calculate this\nacross many stocks and provide a scoring system to compare stocks amongst each\nother? We are presenting a methodology here which is put into practice.\n"
    },
    {
        "paper_id": 2003.0972,
        "authors": "Aurelio F. Bariviera",
        "title": "One model is not enough: heterogeneity in cryptocurrencies' multifractal\n  profiles",
        "comments": null,
        "journal-ref": "Finance Research Letters, 2020, 101649",
        "doi": "10.1016/j.frl.2020.101649",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper studies of the multifractal dynamics in 84 cryptocurrencies. It\nfills an important gap in the literature, by studying this market using two\nalternative multi-scaling methodologies. We find compelling evidence that\ncryptocurrencies have different degree of long range dependence, and --more\nimportantly -- follow different stochastic processes. Some of them follow\nmodels closer to monofractal fractional Gaussian noises, while others exhibit\ncomplex multifractal dynamics. Regarding the source of multifractality, our\nresults are mixed. Time series shuffling produces a reduction in the level of\nmultifractality, but not enough to offset it. We find an association of\nkurtosis with multifractality.\n"
    },
    {
        "paper_id": 2003.09723,
        "authors": "Aurelio F. Bariviera, Ignasi Merediz-Sol\\`a",
        "title": "Where do we stand in cryptocurrencies economic research? A survey based\n  on hybrid analysis",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This survey develops a dual analysis, consisting, first, in a bibliometric\nexamination and, second, in a close literature review of all the scientific\nproduction around cryptocurrencies conducted in economics so far. The aim of\nthis paper is twofold. On the one hand, proposes a methodological hybrid\napproach to perform comprehensive literature reviews. On the other hand, we\nprovide an updated state of the art in cryptocurrency economic literature. Our\nmethodology emerges as relevant when the topic comprises a large number of\npapers, that make unrealistic to perform a detailed reading of all the papers.\nThis dual perspective offers a full landscape of cryptocurrency economic\nresearch. Firstly, by means of the distant reading provided by machine learning\nbibliometric techniques, we are able to identify main topics, journals, key\nauthors, and other macro aggregates. Secondly, based on the information\nprovided by the previous stage, the traditional literature review provides a\ncloser look at methodologies, data sources and other details of the papers. In\nthis way, we offer a classification and analysis of the mounting research\nproduced in a relative short time span.\n"
    },
    {
        "paper_id": 2003.0994,
        "authors": "Francesca Biagini, Andrea Mazzon, Ari-Pekka Perkki\u007f\\\"o",
        "title": "Optional projection under equivalent local martingale measures",
        "comments": "30 pages, no figures, no tables",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Motivation for this paper is to understand the impact of information on asset\nprice bubbles and perceived arbitrage opportunities. This boils down to study\noptional projections of $\\mathbb{G}$-adapted strict local martingales into a\nsmaller filtration $\\mathbb{F}$ under equivalent martingale measures. We give\nsome general results as well as analyze in details two specific examples given\nby the inverse three dimensional Bessel process and a class of stochastic\nvolatility models.\n"
    },
    {
        "paper_id": 2003.09943,
        "authors": "Zachary Feinstein",
        "title": "Reanimating a Dead Economy: Financial and Economic Analysis of a Zombie\n  Outbreak",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper, we study the financial and economic implications of a zombie\nepidemic on a major industrialized nation. We begin with a consideration of the\nepidemiological modeling of the zombie contagion. The emphasis of this work is\non the computation of direct and indirect financial consequences of this\ncontagion of the walking dead. A moderate zombie outbreak leaving 1 million\npeople dead in a major industrialized nation could result in GDP losses of\n23.44% over the subsequent year and a drop in financial market of 29.30%. We\nconclude by recommending policy actions necessary to prevent this potential\neconomic collapse.\n"
    },
    {
        "paper_id": 2003.10001,
        "authors": "Guillermo Angeris, Tarun Chitra",
        "title": "Improved Price Oracles: Constant Function Market Makers",
        "comments": null,
        "journal-ref": null,
        "doi": "10.1145/3419614.3423251",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Automated market makers, first popularized by Hanson's logarithmic market\nscoring rule (or LMSR) for prediction markets, have become important building\nblocks, called 'primitives,' for decentralized finance. A particularly useful\nprimitive is the ability to measure the price of an asset, a problem often\nknown as the pricing oracle problem. In this paper, we focus on the analysis of\na very large class of automated market makers, called constant function market\nmakers (or CFMMs) which includes existing popular market makers such as\nUniswap, Balancer, and Curve, whose yearly transaction volume totals to\nbillions of dollars. We give sufficient conditions such that, under fairly\ngeneral assumptions, agents who interact with these constant function market\nmakers are incentivized to correctly report the price of an asset and that they\ncan do so in a computationally efficient way. We also derive several other\nuseful properties that were previously not known. These include lower bounds on\nthe total value of assets held by CFMMs and lower bounds guaranteeing that no\nagent can, by any set of trades, drain the reserves of assets held by a given\nCFMM.\n"
    },
    {
        "paper_id": 2003.10014,
        "authors": "Arthur Charpentier and Romuald Elie and Carl Remlinger",
        "title": "Reinforcement Learning in Economics and Finance",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Reinforcement learning algorithms describe how an agent can learn an optimal\naction policy in a sequential decision process, through repeated experience. In\na given environment, the agent policy provides him some running and terminal\nrewards. As in online learning, the agent learns sequentially. As in\nmulti-armed bandit problems, when an agent picks an action, he can not infer\nex-post the rewards induced by other action choices. In reinforcement learning,\nhis actions have consequences: they influence not only rewards, but also future\nstates of the world. The goal of reinforcement learning is to find an optimal\npolicy -- a mapping from the states of the world to the set of actions, in\norder to maximize cumulative reward, which is a long term strategy. Exploring\nmight be sub-optimal on a short-term horizon but could lead to optimal\nlong-term ones. Many problems of optimal control, popular in economics for more\nthan forty years, can be expressed in the reinforcement learning framework, and\nrecent advances in computational science, provided in particular by deep\nlearning algorithms, can be used by economists in order to solve complex\nbehavioral problems. In this article, we propose a state-of-the-art of\nreinforcement learning techniques, and present applications in economics, game\ntheory, operation research and finance.\n"
    },
    {
        "paper_id": 2003.10121,
        "authors": "Kerstin Awiszus, Agostino Capponi and Stefan Weber",
        "title": "Market Efficient Portfolios in a Systemic Economy",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We study the ex-ante minimization of market inefficiency, defined in terms of\nminimum deviation of market prices from fundamental values, from a centralized\nplanner's perspective. Prices are pressured from exogenous trading actions of\nleverage targeting banks, which rebalance their portfolios in response to asset\nshocks. We characterize market inefficiency in terms of two key drivers, the\nbanks' systemic significance and the statistical moments of asset shocks, and\ndevelop an explicit expression for the matrix of asset holdings which minimizes\nsuch inefficiency. Our analysis shows that to reduce inefficiencies, portfolio\nholdings should deviate more from a full diversification strategy if there is\nlittle heterogeneity in banks' systemic significance.\n"
    },
    {
        "paper_id": 2003.10234,
        "authors": "Romit Maulik, Junghwa Choi, Wesley Wehde, Prasanna Balaprakash",
        "title": "Determining feature importance for actionable climate change mitigation\n  policies",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Given the importance of public support for policy change and implementation,\npublic policymakers and researchers have attempted to understand the factors\nassociated with this support for climate change mitigation policy. In this\narticle, we compare the feasibility of using different supervised learning\nmethods for regression using a novel socio-economic data set which measures\npublic support for potential climate change mitigation policies. Following this\nmodel selection, we utilize gradient boosting regression, a well-known\ntechnique in the machine learning community, but relatively uncommon in public\npolicy and public opinion research, and seek to understand what factors among\nthe several examined in previous studies are most central to shaping public\nsupport for mitigation policies in climate change studies. The use of this\nmethod provides novel insights into the most important factors for public\nsupport for climate change mitigation policies. Using national survey data, we\nfind that the perceived risks associated with climate change are more decisive\nfor shaping public support for policy options promoting renewable energy and\nregulating pollutants. However, we observe a very different behavior related to\npublic support for increasing the use of nuclear energy where climate change\nrisk perception is no longer the sole decisive feature. Our findings indicate\nthat public support for renewable energy is inherently different from that for\nnuclear energy reliance with the risk perception of climate change, dominant\nfor the former, playing a subdued role for the latter.\n"
    },
    {
        "paper_id": 2003.10353,
        "authors": "Mike Derksen, Bas Kleijn and Robin de Vilder",
        "title": "Effects of MiFID II on stock price formation",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper examines effects of MiFID II on European stock markets. We study\nthe effects of the new tick size regime, both intraday and in the closing\nauction. An increase (decrease) in tick size is associated with a decrease\n(increase) in intraday liquidity, but a more (less) stable market. In the\nclosing auction an increase in tick size has a positive effect on liquidity.\nMoreover, we report a positive relationship between tick size and transacted\nvolume, in particular in the closing auction. Finally, closing auction volumes\nincreased heavily since MiFID II and price formation in closing auctions became\nmore efficient.\n"
    },
    {
        "paper_id": 2003.10419,
        "authors": "Florent Benaych-Georges, Jean-Philippe Bouchaud, Stefano Ciliberti",
        "title": "Equity Factors: To Short Or Not To Short, That Is The Question",
        "comments": "11 pages, 11 figures. To appear in Journal of Investing",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  What is the best market-neutral implementation of classical Equity Factors?\nShould one use the specific predictability of the short-leg to build a zero\nbeta Long-Short portfolio, in spite of the specific costs associated to\nshorting, or is it preferable to ban the shorts and hedge the long-leg with --\nsay -- an index future? We revisit this question by focusing on the relative\npredictability of the two legs, the issue of diversification, and various\nsources of costs. Our conclusion is that, using the same Factors, a Long-Short\nimplementation leads to superior risk-adjusted returns than its Hedged\nLong-Only counterpart, at least when Assets Under Management are not too large.\n"
    },
    {
        "paper_id": 2003.10479,
        "authors": "Daniel Bartl and Ludovic Tangpi",
        "title": "Non-asymptotic convergence rates for the plug-in estimation of risk\n  measures",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Let $\\rho$ be a general law--invariant convex risk measure, for instance the\naverage value at risk, and let $X$ be a financial loss, that is, a real random\nvariable. In practice, either the true distribution $\\mu$ of $X$ is unknown, or\nthe numerical computation of $\\rho(\\mu)$ is not possible. In both cases, either\nrelying on historical data or using a Monte-Carlo approach, one can resort to\nan i.i.d.\\ sample of $\\mu$ to approximate $\\rho(\\mu)$ by the finite sample\nestimator $\\rho(\\mu_N)$ (where $\\mu_N$ denotes the empirical measure of $\\mu$).\nIn this article we investigate convergence rates of $\\rho(\\mu_N)$ to\n$\\rho(\\mu)$. We provide non-asymptotic convergence rates for both the deviation\nprobability and the expectation of the estimation error. The sharpness of these\nconvergence rates is analyzed. Our framework further allows for hedging, and\nthe convergence rates we obtain depend neither on the dimension of the\nunderlying assets, nor on the number of options available for trading.\n"
    },
    {
        "paper_id": 2003.10502,
        "authors": "Alexander Lipton and Marcos Lopez de Prado",
        "title": "A closed-form solution for optimal mean-reverting trading strategies",
        "comments": "32 pages, 12 figures, 1 table",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  When prices reflect all available information, they oscillate around an\nequilibrium level. This oscillation is the result of the temporary market\nimpact caused by waves of buyers and sellers. This price behavior can be\napproximated through an Ornstein-Uhlenbeck (O-U) process.\n  Market makers provide liquidity in an attempt to monetize this oscillation.\nThey enter a long position when a security is priced below its estimated\nequilibrium level, and they enter a short position when a security is priced\nabove its estimated equilibrium level. They hold that position until one of\nthree outcomes occur: (1) they achieve the targeted profit; (2) they experience\na maximum tolerated loss; (3) the position is held beyond a maximum tolerated\nhorizon.\n  All market makers are confronted with the problem of defining profit-taking\nand stop-out levels. More generally, all execution traders acting on behalf of\na client must determine at what levels an order must be fulfilled. Those\noptimal levels can be determined by maximizing the trader's Sharpe ratio in the\ncontext of O-U processes via Monte Carlo experiments. This paper develops an\nanalytical framework and derives those optimal levels by using the method of\nheat potentials.\n"
    },
    {
        "paper_id": 2003.10674,
        "authors": "Kevin Kuo, Daniel Lupton",
        "title": "Towards Explainability of Machine Learning Models in Insurance Pricing",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Machine learning methods have garnered increasing interest among actuaries in\nrecent years. However, their adoption by practitioners has been limited, partly\ndue to the lack of transparency of these methods, as compared to generalized\nlinear models. In this paper, we discuss the need for model interpretability in\nproperty & casualty insurance ratemaking, propose a framework for explaining\nmodels, and present a case study to illustrate the framework.\n"
    },
    {
        "paper_id": 2003.10922,
        "authors": "Pier Francesco Procacci and Carolyn E. Phelan and Tomaso Aste",
        "title": "Market structure dynamics during COVID-19 outbreak",
        "comments": "1 page, figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this note, we discuss the impact of the COVID-19 outbreak from the\nperspective of the market-structure. We observe that the US market-structure\nhas dramatically changed during the past four weeks and that the level of\nchange has followed the number of infected cases reported in the USA.\nPresently, market-structure resembles most closely the structure during the\nmiddle of the 2008 crisis but there are signs that it may be starting to evolve\ninto a new structure altogether. This is the first article of a series where we\nwill be analyzing and discussing market-structure as it evolves to a state of\nfurther instability or, more optimistically, stabilization and recovery.\n"
    },
    {
        "paper_id": 2003.10998,
        "authors": "Artur Strzelecki",
        "title": "The Second Worldwide Wave of Interest in Coronavirus since the COVID-19\n  Outbreaks in South Korea, Italy and Iran: A Google Trends Study",
        "comments": "4 pages, 1 figure, 2 tables",
        "journal-ref": "Brain, Behavior, and Immunity 88 (2020) 950-951",
        "doi": "10.1016/j.bbi.2020.04.042",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The recent emergence of a new coronavirus, COVID-19, has gained extensive\ncoverage in public media and global news. As of 24 March 2020, the virus has\ncaused viral pneumonia in tens of thousands of people in Wuhan, China, and\nthousands of cases in 184 other countries and territories. This study explores\nthe potential use of Google Trends (GT) to monitor worldwide interest in this\nCOVID-19 epidemic. GT was chosen as a source of reverse engineering data, given\nthe interest in the topic. Current data on COVID-19 is retrieved from (GT)\nusing one main search topic: Coronavirus. Geographical settings for GT are\nworldwide, China, South Korea, Italy and Iran. The reported period is 15\nJanuary 2020 to 24 March 2020. The results show that the highest worldwide peak\nin the first wave of demand for information was on 31 January 2020. After the\nfirst peak, the number of new cases reported daily rose for 6 days. A second\nwave started on 21 February 2020 after the outbreaks were reported in Italy,\nwith the highest peak on 16 March 2020. The second wave is six times as big as\nthe first wave. The number of new cases reported daily is rising day by day.\nThis short communication gives a brief introduction to how the demand for\ninformation on coronavirus epidemic is reported through GT.\n"
    },
    {
        "paper_id": 2003.11021,
        "authors": "Gian Maria Campedelli, Alberto Aziani, Serena Favarin",
        "title": "Exploring the Effects of COVID-19 Containment Policies on Crime: An\n  Empirical Analysis of the Short-term Aftermath in Los Angeles",
        "comments": "40 pages, 3 figures. Forthcoming at American Journal of Criminal\n  Justice",
        "journal-ref": "Am J Crim Just (2020)",
        "doi": "10.1007/s12103-020-09578-6",
        "license": "http://creativecommons.org/licenses/by-sa/4.0/",
        "abstract": "  This work investigates whether and how COVID-19 containment policies had an\nimmediate impact on crime trends in Los Angeles. The analysis is conducted\nusing Bayesian structural time-series and focuses on nine crime categories and\non the overall crime count, daily monitored from January 1st 2017 to March 28th\n2020. We concentrate on two post-intervention time windows - from March 4th to\nMarch 16th and from March 4th to March 28th 2020 - to dynamically assess the\nshort-term effects of mild and strict policies. In Los Angeles, overall crime\nhas significantly decreased, as well as robbery, shoplifting, theft, and\nbattery. No significant effect has been detected for vehicle theft, burglary,\nassault with a deadly weapon, intimate partner assault, and homicide. Results\nsuggest that, in the first weeks after the interventions are put in place,\nsocial distancing impacts more directly on instrumental and less serious\ncrimes. Policy implications are also discussed.\n"
    },
    {
        "paper_id": 2003.11027,
        "authors": "Osman Gulseven",
        "title": "Turn-of-the Year Affect in Gold Prices: Decomposition Analysis",
        "comments": "12 pages, 6 tables, 2 graphs. Open Access Article which can be\n  downloaded here:\n  http://ekonomikarastirmalar.org/index.php/UEAD/article/view/53/32",
        "journal-ref": "International Journal of Economic Studies, 2(3), 1-12 (2016)",
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this article, we examine whether the gold market returns show abnormally\npositive or negative returns in some months of the calendar year. The\nstatistical analysis and the decomposition techniques suggest that gold prices\nshow some seasonal behavior during the turn of the year. We observe a strong\ncyclical behavior in gold markets during the turn-of-the-year period. January\nis likely to offer the highest return whereas significant negative returns are\nexpected in July.\n"
    },
    {
        "paper_id": 2003.11221,
        "authors": "Alexis Akira Toda",
        "title": "Susceptible-Infected-Recovered (SIR) Dynamics of COVID-19 and Economic\n  Impact",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  I estimate the Susceptible-Infected-Recovered (SIR) epidemic model for\nCoronavirus Disease 2019 (COVID-19). The transmission rate is heterogeneous\nacross countries and far exceeds the recovery rate, which enables a fast\nspread. In the benchmark model, 28% of the population may be simultaneously\ninfected at the peak, potentially overwhelming the healthcare system. The peak\nreduces to 6.2% under the optimal mitigation policy that controls the timing\nand intensity of social distancing. A stylized asset pricing model suggests\nthat the stock price temporarily decreases by 50% in the benchmark case but\nshows a W-shaped, moderate but longer bear market under the optimal policy.\n"
    },
    {
        "paper_id": 2003.11312,
        "authors": "Edward Hoyle and Levent Ali Meng\\\"ut\\\"urk",
        "title": "Generalised Liouville Processes and their Properties",
        "comments": null,
        "journal-ref": "J. Appl. Probab. 57 (2020) 1088-1110",
        "doi": "10.1017/jpr.2020.61",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We define a new family of multivariate stochastic processes over a finite\ntime horizon that we call Generalised Liouville Processes (GLPs). GLPs are\nMarkov processes constructed by splitting L\\'evy random bridges into\nnon-overlapping subprocesses via time changes. We show that the terminal values\nand the increments of GLPs have generalised multivariate Liouville\ndistributions, justifying their name. We provide various other properties of\nGLPs and some examples.\n"
    },
    {
        "paper_id": 2003.11347,
        "authors": "Andreas Dietrich, Reto Rey",
        "title": "What Matters to Individual Investors: Price Setting in Online Auctions\n  of P2P Consumer Loans",
        "comments": "22 Pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We analyze how retail investors price the credit risk of online P\"P consumer\nloans in a reverse auction framework where personal interaction is absent. The\nexplained interest rate variance is considerably larger than in comparable\nstudies using bank loan data. This is unexpected, given the difference in\nexperience and the limited set of information provided to the investor. Factors\nrepresenting economic status significantly influence lender evaluations of the\nborrower's credit risk. Previous studies on gender discrimination found mixed\nresults. We show that this is likely due to simplified specifications and find\nsurprising results: male borrowers are charged a higher interest rates,\nconditional on being married and having children. Overall, our results indicate\nthat retail investors exhibit a strong degree of predictability in this weakly\nregulated market for online P2P consumer loans.\n"
    },
    {
        "paper_id": 2003.11352,
        "authors": "Fan Fang, Carmine Ventre, Michail Basios, Leslie Kanthan, Lingbo Li,\n  David Martinez-Regoband, Fan Wu",
        "title": "Cryptocurrency Trading: A Comprehensive Survey",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In recent years, the tendency of the number of financial institutions\nincluding cryptocurrencies in their portfolios has accelerated.\nCryptocurrencies are the first pure digital assets to be included by asset\nmanagers. Although they have some commonalities with more traditional assets,\nthey have their own separate nature and their behaviour as an asset is still in\nthe process of being understood. It is therefore important to summarise\nexisting research papers and results on cryptocurrency trading, including\navailable trading platforms, trading signals, trading strategy research and\nrisk management. This paper provides a comprehensive survey of cryptocurrency\ntrading research, by covering 146 research papers on various aspects of\ncryptocurrency trading (e.g., cryptocurrency trading systems, bubble and\nextreme conditions, prediction of volatility and return, crypto-assets\nportfolio construction and crypto-assets, technical trading and others). This\npaper also analyses datasets, research trends and distribution among research\nobjects(contents/properties) and technologies, concluding with some promising\nopportunities that remain open in cryptocurrency trading.\n"
    },
    {
        "paper_id": 2003.11471,
        "authors": "Alexander Lipton",
        "title": "Physics and Derivatives -- Interview Questions and Answers",
        "comments": "5 pages",
        "journal-ref": "Journal of Derivatives, Special Issue, Physics and Derivatives,\n  2020",
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Answers to interview questions sent to a selected group of former physicists\nworking in finance. The interview will be published as part of a Special Issue\non Physics and Derivatives by The Journal of Derivatives in the second half of\n2020.\n"
    },
    {
        "paper_id": 2003.11473,
        "authors": "Yang Chen and Emerson Li",
        "title": "EB-dynaRE: Real-Time Adjustor for Brownian Movement with Examples of\n  Predicting Stock Trends Based on a Novel Event-Based Supervised Learning\n  Algorithm",
        "comments": "Subject to updates",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Stock prices are influenced over time by underlying macroeconomic factors.\nJumping out of the box of conventional assumptions about the unpredictability\nof the market noise, we modeled the changes of stock prices over time through\nthe Markov Decision Process, a discrete stochastic control process that aids\ndecision making in a situation that is partly random. We then did a \"Region of\nInterest\" (RoI) Pooling of the stock time-series graphs in order to predict\nfuture prices with existing ones. Generative Adversarial Network (GAN) is then\nused based on a competing pair of supervised learning algorithms, to regenerate\nfuture stock price projections on a real-time basis. The supervised learning\nalgorithm used in this research, moreover, is original to this study and will\nhave wider uses. With the ensemble of these algorithms, we are able to\nidentify, to what extent, each specific macroeconomic factor influences the\nchange of the Brownian/random market movement. In addition, our model will have\na wider influence on the predictions of other Brownian movements.\n"
    },
    {
        "paper_id": 2003.11496,
        "authors": "Ana Fernandes, Martin Huber, Giannina Vaccaro",
        "title": "Gender Differences in Wage Expectations",
        "comments": null,
        "journal-ref": null,
        "doi": "10.1371/journal.pone.0250892",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Using a survey on wage expectations among students at two Swiss institutions\nof higher education, we examine the wage expectations of our respondents along\ntwo main lines. First, we investigate the rationality of wage expectations by\ncomparing average expected wages from our sample with those of similar\ngraduates; we further examine how our respondents revise their expectations\nwhen provided information about actual wages. Second, using causal mediation\nanalysis, we test whether the consideration of a rich set of personal and\nprofessional controls, namely concerning family formation and children in\naddition to professional preferences, accounts for the difference in wage\nexpectations across genders. We find that males and females overestimate their\nwages compared to actual ones, and that males respond in an overconfident\nmanner to information about outside wages. Despite the attenuation of the\ngender difference in wage expectations brought about by the comprehensive set\nof controls, gender generally retains a significant direct, unexplained effect\non wage expectations.\n"
    },
    {
        "paper_id": 2003.11565,
        "authors": "Marijn A. Bolhuis and Judd N. L. Cramer",
        "title": "The Millennial Boom, the Baby Bust, and the Housing Market",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  As baby boomers have begun to downsize and retire, their preferences now\noverlap with millennials' predilection for urban amenities and smaller living\nspaces. This confluence in tastes between the two largest age segments of the\nU.S. population has meaningfully changed the evolution of home prices in the\nUnited States. Utilizing a Bartik shift-share instrument for demography-driven\ndemand shocks, we show that from 2000 to 2018 (i) the price growth of four- and\nfive-bedroom houses has lagged the prices of one- and two-bedroom homes, (ii)\nwithin local labor markets, the relative home prices in baby boomer-rich zip\ncodes have declined compared with millennial-rich neighborhoods, and (iii) the\nzip codes with the largest relative share of smaller homes have grown fastest.\nThese patterns have become more pronounced during the latest economic cycle. We\nshow that the effects are concentrated in areas where housing supply is most\ninelastic. If this pattern in the housing market persists or expands, the\napproximately 16.5 trillion in real estate wealth held by households headed by\nthose aged 55 or older will be significantly affected. We find little evidence\nthat these upcoming changes have been incorporated into current prices.\n"
    },
    {
        "paper_id": 2003.12112,
        "authors": "Joshua Becker",
        "title": "Network Structure and Collective Intelligence in the Diffusion of\n  Innovation",
        "comments": "43 pages, 9 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  When multiple innovations compete for adoption, historical chance leading to\nearly advantage can generate lock-in effects that allow suboptimal innovations\nto succeed at the expense of superior alternatives. Research on the diffusion\nof innovafacetion has identified many possible sources of early advantage, but\nthese mechanisms can benefit both optimal and suboptimal innovations. This\npaper moves beyond chance-as-explanation to identify structural principles that\nsystematically impact the likelihood that the optimal strategy will spread. A\nformal model of innovation diffusion shows that the network structure of\norganizational relationships can systematically impact the likelihood that\nwidely adopted innovations will be payoff optimal. Building on prior diffusion\nresearch, this paper focuses on the role of central actors i.e. well-connected\npeople or firms. While contagion models of diffusion highlight the benefits of\ncentral actors for spreading innovations further and faster, the present\nanalysis reveals a dark side to this influence: the mere presence of central\nactors in a network increases rates of adoption but also increases the\nlikelihood of suboptimal outcomes. This effect, however, does not represent a\nspeed-optimality tradeoff, as dense networks are both fast and optimal. This\nfinding is consistent with related research showing that network centralization\nundermines collective intelligence.\n"
    },
    {
        "paper_id": 2003.12198,
        "authors": "Xingwei Hu",
        "title": "Sorting Big Data by Revealed Preference with Application to College\n  Ranking",
        "comments": "43 pages, 1 figure, 5 theorems, and 1 lemma",
        "journal-ref": "Journal of Big Data, 2020",
        "doi": "10.1186/s40537-020-00300-1",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  When ranking big data observations such as colleges in the United States,\ndiverse consumers reveal heterogeneous preferences. The objective of this paper\nis to sort out a linear ordering for these observations and to recommend\nstrategies to improve their relative positions in the ranking. A properly\nsorted solution could help consumers make the right choices, and governments\nmake wise policy decisions. Previous researchers have applied exogenous\nweighting or multivariate regression approaches to sort big data objects,\nignoring their variety and variability. By recognizing the diversity and\nheterogeneity among both the observations and the consumers, we instead apply\nendogenous weighting to these contradictory revealed preferences. The outcome\nis a consistent steady-state solution to the counterbalance equilibrium within\nthese contradictions. The solution takes into consideration the spillover\neffects of multiple-step interactions among the observations. When information\nfrom data is efficiently revealed in preferences, the revealed preferences\ngreatly reduce the volume of the required data in the sorting process. The\nemployed approach can be applied in many other areas, such as sports team\nranking, academic journal ranking, voting, and real effective exchange rates.\n"
    },
    {
        "paper_id": 2003.12432,
        "authors": "Fabian Stephany, Niklas Stoehr, Philipp Darius, Leonie Neuh\\\"auser,\n  Ole Teutloff, Fabian Braesemann",
        "title": "The CoRisk-Index: A data-mining approach to identify industry-specific\n  risk assessments related to COVID-19 in real-time",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  While the coronavirus spreads, governments are attempting to reduce contagion\nrates at the expense of negative economic effects. Market expectations\nplummeted, foreshadowing the risk of a global economic crisis and mass\nunemployment. Governments provide huge financial aid programmes to mitigate the\neconomic shocks. To achieve higher effectiveness with such policy measures, it\nis key to identify the industries that are most in need of support. In this\nstudy, we introduce a data-mining approach to measure industry-specific risks\nrelated to COVID-19. We examine company risk reports filed to the U.S.\nSecurities and Exchange Commission (SEC). This alternative data set can\ncomplement more traditional economic indicators in times of the fast-evolving\ncrisis as it allows for a real-time analysis of risk assessments. Preliminary\nfindings suggest that the companies' awareness towards corona-related business\nrisks is ahead of the overall stock market developments. Our approach allows to\ndistinguish the industries by their risk awareness towards COVID-19. Based on\nnatural language processing, we identify corona-related risk topics and their\nperceived relevance for different industries. The preliminary findings are\nsummarised as an up-to-date online index. The CoRisk-Index tracks the\nindustry-specific risk assessments related to the crisis, as it spreads through\nthe economy. The tracking tool is updated weekly. It could provide relevant\nempirical data to inform models on the economic effects of the crisis. Such\ncomplementary empirical information could ultimately help policymakers to\neffectively target financial support in order to mitigate the economic shocks\nof the crisis.\n"
    },
    {
        "paper_id": 2003.12474,
        "authors": "Samuel Shye and Ido Haber",
        "title": "Challenge Theory: The Structure and Measurement of Risky Binary Choice\n  Behavior",
        "comments": "16 pages, 10 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Challenge Theory (Shye & Haber 2015; 2020) has demonstrated that a newly\ndevised challenge index (CI) attributable to every binary choice problem\npredicts the popularity of the bold option, the one of lower probability to\ngain a higher monetary outcome (in a gain problem); and the one of higher\nprobability to lose a lower monetary outcome (in a loss problem). In this paper\nwe show how Facet Theory structures the choice-behavior concept-space and\nyields rationalized measurements of gambling behavior. The data of this study\nconsist of responses obtained from 126 student, specifying their preferences in\n44 risky decision problems. A Faceted Smallest Space Analysis (SSA) of the 44\nproblems confirmed the hypothesis that the space of binary risky choice\nproblems is partitionable by two binary axial facets: (a) Type of Problem (gain\nvs. loss); and (b) CI (Low vs. High). Four composite variables, representing\nthe validated constructs: Gain, Loss, High-CI and Low-CI, were processed using\nMultiple Scaling by Partial Order Scalogram Analysis with base Coordinates\n(POSAC), leading to a meaningful and intuitively appealing interpretation of\ntwo necessary and sufficient gambling-behavior measurement scales.\n"
    },
    {
        "paper_id": 2003.12655,
        "authors": "Z. Koohi Lai, A. Namaki, A. Hosseiny, G.R. Jafari, M. Ausloos",
        "title": "Coupled criticality analysis of inflation and unemployment",
        "comments": "13 pages, 3 figures",
        "journal-ref": null,
        "doi": "10.3390/e23010042",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper, we are interested to focus on the critical periods in the\neconomy which are characterized by large fluctuations in macroeconomic\nindicators.\n  To capture unusual and large fluctuations of inflation and unemployment, we\nconcentrate on the non-Gaussianity of their distributions.\n  To this aim, by using the coupled multifractal approach, we analyze US data\nfor a period of 70 years from 1948 until 2018 and measure the non-Gausianity of\nthe distributions. Then, we investigate how the non-Gaussianity of the\nvariables affects the coupling structure of them. By applying the multifractal\nmethod, one can see that the non-Gaussianity depends on the scales. While the\nnon-Gaussianity of unemployment is noticeable only for periods smaller than 1\nyear and for longer periods tends to Gaussian behavior, the non-Gaussianities\nof inflation persist for all time scales. Also, it is observed that the\ncoupling structure of these variables tends to a Gaussian behavior after $2$\nyears.\n"
    },
    {
        "paper_id": 2003.12825,
        "authors": "Stefan Gerhold, Christoph Gerstenecker, Archil Gulisashvili",
        "title": "Large deviations for fractional volatility models with non-Gaussian\n  volatility driver",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We study stochastic volatility models in which the volatility process is a\nfunction of a continuous fractional stochastic process, which is an integral\ntransform of the solution of an SDE satisfying the Yamada-Watanabe condition.\nWe establish a small-noise large deviation principle for the log-price, and,\nfor a special case of our setup, obtain logarithmic call price asymptotics for\nlarge strikes.\n"
    },
    {
        "paper_id": 2003.12934,
        "authors": "Hongshan Li, Zhongyi Huang",
        "title": "An iterative splitting method for pricing European options under the\n  Heston model",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper, we propose an iterative splitting method to solve the partial\ndifferential equations in option pricing problems. We focus on the Heston\nstochastic volatility model and the derived two-dimensional partial\ndifferential equation (PDE). We take the European option as an example and\nconduct numerical experiments using different boundary conditions. The\niterative splitting method transforms the two-dimensional equation into two\nquasi one-dimensional equations with the variable on the other dimension fixed,\nwhich helps to lower the computational cost. Numerical results show that the\niterative splitting method together with an artificial boundary condition (ABC)\nbased on the method by Li and Huang (2019) gives the most accurate option price\nand Greeks compared to the classic finite difference method with the\ncommonly-used boundary conditions in Heston (1993).\n"
    },
    {
        "paper_id": 2003.13062,
        "authors": "Vladim\\'ir Hol\\'y and Petra Tomanov\\'a",
        "title": "Streaming Approach to Quadratic Covariation Estimation Using Financial\n  Ultra-High-Frequency Data",
        "comments": null,
        "journal-ref": null,
        "doi": "10.1007/s10614-021-10210-w",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We investigate the computational issues related to the memory size in the\nestimation of quadratic covariation, taking into account the specifics of\nfinancial ultra-high-frequency data. In multivariate price processes, we\nconsider both contamination by the market microstructure noise and the\nnon-synchronicity of the observations. We formulate a multi-scale, flat-top\nrealized kernel, non-flat-top realized kernel, pre-averaging and modulated\nrealized covariance estimators in quadratic form and fix their bandwidth\nparameter at a constant value. This allows us to operate with limited memory\nand formulate this estimation as a streaming algorithm. We compare the\nperformance of the estimators with fixed bandwidth parameter in a simulation\nstudy. We find that the estimators ensuring positive semidefiniteness require\nmuch higher bandwidth than the estimators without this constraint.\n"
    },
    {
        "paper_id": 2003.1322,
        "authors": "Nathan Dahlin and Rahul Jain",
        "title": "Scheduling Flexible Non-Preemptive Loads in Smart-Grid Networks",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  A market consisting of a generator with thermal and renewable generation\ncapability, a set of non-preemptive loads (i.e., loads which cannot be\ninterrupted once started), and an independent system operator (ISO) is\nconsidered. Loads are characterized by durations, power demand rates and\nutility for receiving service, as well as disutility functions giving\npreferences for time slots in which service is preferred. Given this\ninformation, along with the generator's thermal generation cost function and\nforecast renewable generation, the social planner solves a mixed integer\nprogram to determine a load activation schedule which maximizes social welfare.\nAssuming price taking behavior, we develop a competitive equilibrium concept\nbased on a relaxed version of the social planner's problem which includes\nprices for consumption and incentives for flexibility, and allows for\nprobabilistic allocation of power to loads. Considering each load as\nrepresentative of a population of identical loads with scaled characteristics,\nwe demonstrate that the relaxed social planner's problem gives an exact\nsolution to the original mixed integer problem in the large population limit,\nand give a market mechanism for implementing the competitive equilibrium.\nFinally, we evaluate via case study the benefit of incorporating load\nflexibility information into power consumption and generation scheduling in\nterms of proportion of loads served and overall social welfare.\n"
    },
    {
        "paper_id": 2003.13275,
        "authors": "Benjamin Avanzi and Hayden Lau and Bernard Wong",
        "title": "Optimal periodic dividend strategies for spectrally positive L\\'evy risk\n  processes with fixed transaction costs",
        "comments": "Accepted for publication in Insurance: Mathematics and Economics",
        "journal-ref": "Insurance: Mathematics and Economics, Volume 93, July 2020, Pages\n  315-332",
        "doi": "10.1016/j.insmatheco.2020.05.012",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We consider the general class of spectrally positive L\\'evy risk processes,\nwhich are appropriate for businesses with continuous expenses and lump sum\ngains whose timing and sizes are stochastic. Motivated by the fact that\ndividends cannot be paid at any time in real life, we study $\\textit{periodic}$\ndividend strategies whereby dividend decisions are made according to a separate\narrival process.\n  In this paper, we investigate the impact of fixed transaction costs on the\noptimal periodic dividend strategy, and show that a periodic $(b_u,b_l)$\nstrategy is optimal when decision times arrive according to an independent\nPoisson process. Such a strategy leads to lump sum dividends that bring the\nsurplus back to $b_l$ as long as it is no less than $b_u$ at a dividend\ndecision time. The expected present value of dividends (net of transaction\ncosts) is provided explicitly with the help of scale functions. Results are\nillustrated.\n"
    },
    {
        "paper_id": 2003.13317,
        "authors": "Dariusz Zawisza",
        "title": "On the parabolic equation for portfolio problems",
        "comments": "(v2) - a few minor typos and omissions corrected, (13 pages).\n  Forthcoming in Banach Center Publications - Conference on stochastic modeling\n  in finance and insurance, B\\k{e}dlewo 11.02.2019--15.02.2019, X Simons\n  Semester",
        "journal-ref": "Banach Center Publications 122 (2020), 287-302",
        "doi": "10.4064/bc122-16",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We consider a semilinear equation linked to the finite horizon consumption -\ninvestment problem under the stochastic factor framework and we prove it admits\na classical solution and provide all obligatory estimates to successfully apply\na verification reasoning. The paper covers the standard time additive utility,\nas well as the recursive utility framework. We extend existing results by\nconsidering more general factor dynamics including a non-trivial diffusion part\nand a stochastic correlation between assets and factors. In addition, this is\nthe first paper which compromises many other optimization problems in finance,\nfor example those related to the indifference pricing or the quadratic hedging\nproblem. The extension of the result to the stochastic differential utility and\nrobust portfolio optimization is provided as well. The essence of our paper\nlays in using improved stochastic methods to prove gradient estimates for\nsuitable HJB equations with restricted control space.\n"
    },
    {
        "paper_id": 2003.1336,
        "authors": "Andrew Paskaramoorthy (1), Terence van Zyl (1), Tim Gebbie (2)",
        "title": "A Framework for Online Investment Algorithms",
        "comments": "for associated code patterns, see\n  https://github.com/apaskara/Online_Invest_Algo",
        "journal-ref": "Investment Analysts Journal, 2020, 49:3",
        "doi": "10.1080/10293523.2020.1806460",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The artificial segmentation of an investment management process into a\nworkflow with silos of offline human operators can restrict silos from\ncollectively and adaptively pursuing a unified optimal investment goal. To meet\nthe investor's objectives, an online algorithm can provide an explicit\nincremental approach that makes sequential updates as data arrives at the\nprocess level. This is in stark contrast to offline (or batch) processes that\nare focused on making component level decisions prior to process level\nintegration. Here we present and report results for an integrated, and online\nframework for algorithmic portfolio management. This article provides a\nworkflow that can in-turn be embedded into a process level learning framework.\nThe workflow can be enhanced to refine signal generation and asset-class\nevolution and definitions. Our results confirm that we can use our framework in\nconjunction with resampling methods to outperform naive market capitalisation\nbenchmarks while making clear the extent of back-test over-fitting. We consider\nsuch an online update framework to be a crucial step towards developing\nintelligent portfolio selection algorithms that integrate financial theory,\ninvestor views, and data analysis with process-level learning.\n"
    },
    {
        "paper_id": 2003.13385,
        "authors": "Ergun Yukseltan, Ahmet Yucekaya, Ayse Humeyra Bilge, Esra Agca Aktunc",
        "title": "Forecasting Models for Daily Natural Gas Consumption Considering\n  Periodic Variations and Demand Segregation",
        "comments": "21 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Due to expensive infrastructure and the difficulties in storage, supply\nconditions of natural gas are different from those of other traditional energy\nsources like petroleum or coal. To overcome these challenges, supplier\ncountries require take-or-pay agreements for requested natural gas quantities.\nThese contracts have many pre-clauses; if they are not met due to low/high\nconsumption or other external factors, buyers must completely fulfill them. A\nsimilar contract is then imposed on distributors and wholesale consumers. It is\nthus important for all parties to forecast their daily, monthly, and annual\nnatural gas demand to minimize their risk. In this paper, a model consisting of\na modulated expansion in Fourier series, supplemented by deviations from\ncomfortable temperatures as a regressor is proposed for the forecast of monthly\nand weekly consumption over a one-year horizon. This model is supplemented by a\nday-ahead feedback mechanism for the forecast of daily consumption. The method\nis applied to the study of natural gas consumption for major residential areas\nin Turkey, on a yearly, monthly, weekly, and daily basis. It is shown that\nresidential heating dominates winter consumption and masks all other\nvariations. On the other hand, weekend and holiday effects are visible in\nsummer consumption and provide an estimate for residential and industrial use.\nThe advantage of the proposed method is the capability of long term projections\nand to outperform time series methods.\n"
    },
    {
        "paper_id": 2003.13395,
        "authors": "Carlos S.Ciria, Carlos M.Sastre, Juan Carrasco and Pilar Ciria",
        "title": "Tall wheatgrass (Thinopyrum ponticum (Podp)) in a real farm context, a\n  sustainable perennial alternative to rye (Secale cereale L.) cultivation in\n  marginal lands",
        "comments": "8 pages, 4 figures, 2 tables",
        "journal-ref": "Industrial Crops and Products Volume 146, April 2020, 112184",
        "doi": "10.1016/j.indcrop.2020.112184",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In order to face the expected increasing demand of energy crops without\ncreating conflicts of land occupation sustainability, farmers need to find\nreliable alternatives in marginal agricultural areas where the production of\nfood hardly ever is economically and environmentally sustainable. The purpose\nof this work was the study of the viability of the introduction of new non food\ncrops in marginal areas of real farms. This study compares the profit margin\nand the energy and environmental performance of growing tall wheatgrass, in the\nmarginal area of a rainfed farm versus rye, the annual crop sowed traditionally\nin the marginal area of the farm. The cited farm owned 300 ha of which about 13\npercent was marginal. The methodology was based on the use of the profit margin\nof the crops as indicator for the economic assessment and Life Cycle Assessment\nLCA as technique for the energy and the environmental evaluations. Results of\nthe economic analysis showed a slight enhancement of the profit margin for tall\nwheatgrass 156 Euro ha-1 y-1 compared to rye 145 Euro ha-1 y-1. Environmental\nLCA was driven by CO2 fixation due to soil organic matter increase and reduced\ninputs consumption for tall wheatgrass that produced a Global Warming Potential\nGWP of -1.9 Mg CO2 eq ha-1 y-1 versus 1.6 Mg CO2 eq ha-1 y-1 obtained for rye.\nTall wheatgrass cultivation primary energy consumption was less than 40 percent\nof rye s consumption. According to the results achieved it was concluded that\ntall wheatgrass is better option than rye from the energy and the environmental\npoint of views and slight better option from the economic view. Considering\nthese results, monetarization of the CO2 eq reductions of tall wheatgrass\ncompared to rye is essential to improve its profit margin and promote the\nimplantation of this new crop in marginal areas of farms.\n"
    },
    {
        "paper_id": 2003.13414,
        "authors": "Michael Filletti and Aaron Grech",
        "title": "Using News Articles and Financial Data to predict the likelihood of\n  bankruptcy",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Over the past decade, millions of companies have filed for bankruptcy. This\nhas been caused by a plethora of reasons, namely, high interest rates, heavy\ndebts and government regulations. The effect of a company going bankrupt can be\ndevastating, hurting not only workers and shareholders, but also clients,\nsuppliers and any related external companies. One of the aims of this paper is\nto provide a framework for company bankruptcy to be predicted by making use of\nfinancial figures, provided by our external dataset, in conjunction with the\nsentiment of news articles about certain sectors. News articles are used to\nattempt to quantify the sentiment on a company and its sector from an external\nperspective, rather than simply using internal figures. This work builds on\nprevious studies carried out by multiple researchers, to bring us closer to\nlessening the impact of such events.\n"
    },
    {
        "paper_id": 2003.13422,
        "authors": "Saeed Nosratabadi, Amir Mosavi, Puhong Duan, Pedram Ghamisi",
        "title": "Data Science in Economics",
        "comments": "22pages, 4 figures, 9 tables",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  This paper provides the state of the art of data science in economics.\nThrough a novel taxonomy of applications and methods advances in data science\nare investigated. The data science advances are investigated in three\nindividual classes of deep learning models, ensemble models, and hybrid models.\nApplication domains include stock market, marketing, E-commerce, corporate\nbanking, and cryptocurrency. Prisma method, a systematic literature review\nmethodology is used to ensure the quality of the survey. The findings revealed\nthat the trends are on advancement of hybrid models as more than 51% of the\nreviewed articles applied hybrid model. On the other hand, it is found that\nbased on the RMSE accuracy metric, hybrid models had higher prediction accuracy\nthan other algorithms. While it is expected the trends go toward the\nadvancements of deep learning models.\n"
    },
    {
        "paper_id": 2003.13423,
        "authors": "Saeed Nosratabadi, Gergo Pinter, Amir Mosavi, and Sandor Semperger",
        "title": "Sustainable Banking; Evaluation of the European Business Models",
        "comments": null,
        "journal-ref": "Sustainability 2020, 12, 2314",
        "doi": "10.3390/su12062314",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Sustainable business models also offer banks competitive advantages such as\nincreasing brand reputation and cost reduction. However, no framework is\npresented to evaluate the sustainability of banking business models. To bridge\nthis theoretical gap, the current study using A Delphi-Analytic Hierarchy\nProcess method, firstly, developed a sustainable business model to evaluate the\nsustainability of the business model of banks. In the second step, the\nsustainability performance of sixteen banks from eight European countries\nincluding Norway, the UK, Poland, Hungary, Germany, France, Spain, and Italy,\nassessed. The proposed business model components of this study were ranked in\nterms of their impact on achieving sustainability goals. Consequently, the\nproposed model components of this study, based on their impact on\nsustainability, are respectively value proposition, core competencies,\nfinancial aspects, business processes, target customers, resources, technology,\ncustomer interface, and partner network. The results of the comparison of the\nbanks studied by each country disclosed that the sustainability of the\nNorwegian and German banks business models is higher than in other counties.\nThe studied banks of Hungary and Spain came in second, the banks of the UK,\nPoland, and France ranked third, and finally, the Italian banks ranked fourth\nin the sustainability of their business models.\n"
    },
    {
        "paper_id": 2003.13517,
        "authors": "Eugene Tartakovsky, Ksenia Plesovskikh, Anastasiia Sarmakeeva,\n  Alexander Bibik",
        "title": "Autocorrelation of returns in major cryptocurrency markets",
        "comments": "11 pages, 12 figures. LaTeX. Update: Typos fixed. Used datasets,\n  source code, and a runnable Jupyter Notebook are available on GitHub via\n  https://github.com/3jane/articles/tree/master/1-autocorrelation-time-bars",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper is the first of a series of short articles that explore the\nefficiency of major cryptocurrency markets. A number of statistical tests and\nproperties of statistical distributions will be used to assess if\ncryptocurrency markets are efficient, and how their efficiency changes over\ntime. In this paper, we analyze autocorrelation of returns in major\ncryptocurrency markets using the following methods: Pearson's autocorrelation\ncoefficient of different orders, Ljung-Box test, and first-order Pearson's\nautocorrelation coefficient in a rolling window. All experiments are conducted\non the BTC/USD, ETH/USD, ETH/BTC markets on Bitfinex exchange, and the XBT/USD\nmarket on Bitmex exchange, each on 5-minute, 1-hour, 1-day, and 1-week time\nframes. The results are represented visually on charts. Statistically\nsignificant autocorrelation is persistently present on the 5m and 1H time\nframes on all markets. The tests disagree on the 1D and 1W time frames. The\nresults of this article are fully reproducible. Used datasets, source code, and\na runnable Jupyter Notebook are available on GitHub.\n"
    },
    {
        "paper_id": 2003.13601,
        "authors": "Martin Larsson and Johannes Ruf",
        "title": "Relative Arbitrage: Sharp Time Horizons and Motion by Curvature",
        "comments": "Accepted by Mathematical Finance",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We characterize the minimal time horizon over which any equity market with $d\n\\geq 2$ stocks and sufficient intrinsic volatility admits relative arbitrage\nwith respect to the market portfolio. If $d \\in \\{2,3\\}$, the minimal time\nhorizon can be computed explicitly, its value being zero if $d=2$ and\n$\\sqrt{3}/(2\\pi)$ if $d=3$. If $d \\geq 4$, the minimal time horizon can be\ncharacterized via the arrival time function of a geometric flow of the unit\nsimplex in $\\mathbb R^d$ that we call the minimum curvature flow.\n"
    },
    {
        "paper_id": 2003.1366,
        "authors": "Jos\\'e Moran, Antoine Fosset, Davide Luzzati, Jean-Philippe Bouchaud,\n  and Michael Benzaquen",
        "title": "By Force of Habit: Self-Trapping in a Dynamical Utility Landscape",
        "comments": "7 pages, 4 figures, 1 table",
        "journal-ref": null,
        "doi": "10.1063/5.0009518",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Historically, rational choice theory has focused on the utility maximization\nprinciple to describe how individuals make choices. In reality, there is a\ncomputational cost related to exploring the universe of available choices and\nit is often not clear whether we are truly maximizing an underlying utility\nfunction. In particular, memory effects and habit formation may dominate over\nutility maximisation. We propose a stylized model with a history-dependent\nutility function where the utility associated to each choice is increased when\nthat choice has been made in the past, with a certain decaying memory kernel.\nWe show that self-reinforcing effects can cause the agent to get stuck with a\nchoice by sheer force of habit. We discuss the special nature of the transition\nbetween free exploration of the space of choice and self-trapping. We find in\nparticular that the trapping time distribution is precisely a Zipf law at the\ntransition, and that the self-trapped phase exhibits super-aging behaviour.\n"
    },
    {
        "paper_id": 2003.13888,
        "authors": "Benjamin Avanzi, Greg Taylor, Bernard Wong and Alan Xian",
        "title": "Modelling and understanding count processes through a Markov-modulated\n  non-homogeneous Poisson process framework",
        "comments": "For simulated data sets and code, please go to\n  https://github.com/agi-lab/reserving-MMNPP",
        "journal-ref": null,
        "doi": "10.1016/j.ejor.2020.07.022",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The Markov-modulated Poisson process is utilised for count modelling in a\nvariety of areas such as queueing, reliability, network and insurance claims\nanalysis. In this paper, we extend the Markov-modulated Poisson process\nframework through the introduction of a flexible frequency perturbation\nmeasure. This contribution enables known information of observed event arrivals\nto be naturally incorporated in a tractable manner, while the hidden Markov\nchain captures the effect of unobservable drivers of the data. In addition to\nincreases in accuracy and interpretability, this method supplements analysis of\nthe latent factors. Further, this procedure naturally incorporates data\nfeatures such as over-dispersion and autocorrelation. Additional insights can\nbe generated to assist analysis, including a procedure for iterative model\nimprovement.\n  Implementation difficulties are also addressed with a focus on dealing with\nlarge data sets, where latent models are especially advantageous due the large\nnumber of observations facilitating identification of hidden factors. Namely,\ncomputational issues such as numerical underflow and high processing cost arise\nin this context and in this paper, we produce procedures to overcome these\nproblems.\n  This modelling framework is demonstrated using a large insurance data set to\nillustrate theoretical, practical and computational contributions and an\nempirical comparison to other count models highlight the advantages of the\nproposed approach.\n"
    },
    {
        "paper_id": 2003.13983,
        "authors": "Mikl\\'os Koren, Rita Pet\\H{o}",
        "title": "Business disruptions from social distancing",
        "comments": "16 pages, 4 figures, 3 tables",
        "journal-ref": null,
        "doi": "10.1371/journal.pone.0239113",
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Social distancing interventions can be effective against epidemics but are\npotentially detrimental for the economy. Businesses that rely heavily on\nface-to-face communication or close physical proximity when producing a product\nor providing a service are particularly vulnerable. There is, however, no\nsystematic evidence about the role of human interactions across different lines\nof business and about which will be the most limited by social distancing. Here\nwe provide theory-based measures of the reliance of U.S. businesses on human\ninteraction, detailed by industry and geographic location. We find that 49\nmillion workers work in occupations that rely heavily on face-to-face\ncommunication or require close physical proximity to other workers. Our model\nsuggests that when businesses are forced to reduce worker contacts by half,\nthey need a 12 percent wage subsidy to compensate for the disruption in\ncommunication. Retail, hotels and restaurants, arts and entertainment and\nschools are the most affected sectors. Our results can help target fiscal\nassistance to businesses that are most disrupted by social distancing.\n"
    },
    {
        "paper_id": 2003.14002,
        "authors": "Hiroyasu Inoue and Yasuyuki Todo",
        "title": "The propagation of the economic impact through supply chains: The case\n  of a mega-city lockdown against the spread of COVID-19",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This study quantifies the economic effect of a possible lockdown of Tokyo to\nprevent spread of COVID-19. The negative effect of the lockdown may propagate\nto other regions through supply chains because of shortage of supply and\ndemand. Applying an agent-based model to the actual supply chains of nearly 1.6\nmillion firms in Japan, we simulate what would happen to production activities\noutside Tokyo when production activities that are not essential to citizens'\nsurvival in Tokyo were shut down for a certain period. We find that when Tokyo\nis locked down for a month, the indirect effect on other regions would be twice\nas large as the direct effect on Tokyo, leading to a total production loss of\n27 trillion yen in Japan, or 5.3% of its annual GDP. Although the production\nshut down in Tokyo accounts for 21% of the total production in Japan, the\nlockdown would result in a reduction of the daily production in Japan by 86% in\na month.\n"
    },
    {
        "paper_id": 2003.14133,
        "authors": "J. Raimbault, J. Broere, M. Somveille, J. M. Serna, E. Strombom, C.\n  Moore, B. Zhu, L. Sugar",
        "title": "A spatial agent based model for simulating and optimizing networked\n  eco-industrial systems",
        "comments": "23 pages, 9 figures, 2 tables",
        "journal-ref": "Resources, Conservation and Recycling, 155, 104538 (2020)",
        "doi": "10.1016/j.resconrec.2019.104538",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Industrial symbiosis involves creating integrated cycles of by-products and\nwaste between networks of industrial actors in order to maximize economic\nvalue, while at the same time minimizing environmental strain. In such a\nnetwork, the global environmental strain is no longer equal to the sum of the\nenvironmental strain of the individual actors, but it is dependent on how well\nthe network performs as a whole. The development of methods to understand,\nmanage or optimize such networks remains an open issue. In this paper we put\nforward a simulation model of by-product flow between industrial actors. The\ngoal is to introduce a method for modelling symbiotic exchanges from a macro\nperspective. The model takes into account the effect of two main mechanisms on\na multi-objective optimization of symbiotic processes. First it allows us to\nstudy the effect of geographical properties of the economic system, said\ndifferently, where actors are divided in space. Second, it allows us to study\nthe effect of clustering complementary actors together as a function of\ndistance, by means of a spatial correlation between the actors' by-products.\nOur simulations unveil patterns that are relevant for macro-level policy.\nFirst, our results show that the geographical properties are an important\nfactor for the macro performance of symbiotic processes. Second, spatial\ncorrelations, which can be interpreted as planned clusters such as\nEco-industrial parks, can lead to a very effective macro performance, but only\nif these are strictly implemented. Finally, we provide a proof of concept by\ncomparing the model to real world data from the European Pollutant Release and\nTransfer Register database using georeferencing of the companies in the\ndataset. This work opens up research opportunities in interactive data-driven\nmodels and platforms to support real-world implementation of industrial\nsymbiosis.\n"
    },
    {
        "paper_id": 2003.14359,
        "authors": "Giorgio Ferrari, Hanwu Li, Frank Riedel",
        "title": "A Knightian Irreversible Investment Problem",
        "comments": "32 pages. Relaxed one assumption",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper, we study an irreversible investment problem under Knightian\nuncertainty. In a general framework, in which Knightian uncertainty is modeled\nthrough a set of multiple priors, we prove existence and uniqueness of the\noptimal investment plan, and derive necessary and sufficient conditions for\noptimality. This allows us to construct the optimal policy in terms of the\nsolution to a stochastic backward equation under the worst-case scenario. In a\ntime-homogeneous setting - where risk is driven by a geometric Brownian motion\nand Knightian uncertainty is realized through a so-called \"k-ignorance\" - we\nare able to provide the explicit form of the optimal irreversible investment\nplan.\n"
    },
    {
        "paper_id": 2004.00047,
        "authors": "Ladislav Kristoufek",
        "title": "Grandpa, grandpa, tell me the one about Bitcoin being a safe haven:\n  Evidence from the COVID-19 pandemics",
        "comments": "8 pages, 4 figures",
        "journal-ref": "Frontiers in Physics 8:296 (2020)",
        "doi": "10.3389/fphy.2020.00296",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Bitcoin being a safe haven asset is one of the traditional stories in the\ncryptocurrency community. However, during its existence and relevant presence,\ni.e. approximately since 2013, there has been no severe situation on the\nfinancial markets globally to prove or disprove this story until the COVID-19\npandemics. We study the quantile correlations of Bitcoin and two benchmarks --\nS\\&P500 and VIX -- and we make comparison with gold as the traditional safe\nhaven asset. The Bitcoin safe haven story is shown and discussed to be\nunsubstantiated and far-fetched, while gold comes out as a clear winner in this\ncontest.\n"
    },
    {
        "paper_id": 2004.00111,
        "authors": "Gregor Semieniuk and Victor M. Yakovenko",
        "title": "Historical Evolution of Global Inequality in Carbon Emissions and\n  Footprints versus Redistributive Scenarios",
        "comments": "26 pages, 6 figures, accepted to Journal of Cleaner Production",
        "journal-ref": "Journal of Cleaner Production 264, 121420 (2020)",
        "doi": "10.1016/j.jclepro.2020.121420",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Ambitious scenarios of carbon emission redistribution for mitigating climate\nchange in line with the Paris Agreement and reaching the sustainable\ndevelopment goal of eradicating poverty have been proposed recently. They imply\na strong reduction in carbon footprint inequality by 2030 that effectively\nhalves the Gini coefficient to about 0.25. This paper examines feasibility of\nthese scenarios by analyzing the historical evolution of both weighted\ninternational inequality in CO2 emissions attributed territorially and global\ninequality in carbon footprints attributed to end consumers. For the latter, a\nnew dataset is constructed that is more comprehensive than existing ones. In\nboth cases, we find a decreasing trend in global inequality, partially\nattributed to the move of China from the lower to the middle part of the\ndistribution, with footprints more unequal than territorial emissions. These\nresults show that realization of the redistributive scenarios would require an\nunprecedented reduction in global inequality far below historical levels.\nMoreover, the territorial emissions data, available for more recent years up to\n2017, show a saturation of the decreasing Gini coefficient at a level of 0.5.\nThis observation confirms an earlier prediction based on maximal entropy\nreasoning that the Lorenz curve converges to the exponential distribution. This\nsaturation further undermines feasibility of the redistributive scenarios,\nwhich are also hindered by structural tendencies that reinforce carbon\nfootprint inequality under global capitalism. One way out of this conundrum is\na fast decarbonization of the global energy supply in order to decrease global\ncarbon emissions without relying crucially on carbon inequality reduction.\n"
    },
    {
        "paper_id": 2004.00201,
        "authors": "Jianbin Lin, Zhiqiang Zhang, Jun Zhou, Xiaolong Li, Jingli Fang,\n  Yanming Fang, Quan Yu, Yuan Qi",
        "title": "NetDP: An Industrial-Scale Distributed Network Representation Framework\n  for Default Prediction in Ant Credit Pay",
        "comments": "2018 IEEE International Conference on Big Data (Big Data)",
        "journal-ref": null,
        "doi": "10.1109/BigData.2018.8622169",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Ant Credit Pay is a consumer credit service in Ant Financial Service Group.\nSimilar to credit card, loan default is one of the major risks of this credit\nproduct. Hence, effective algorithm for default prediction is the key to losses\nreduction and profits increment for the company. However, the challenges facing\nin our scenario are different from those in conventional credit card service.\nThe first one is scalability. The huge volume of users and their behaviors in\nAnt Financial requires the ability to process industrial-scale data and perform\nmodel training efficiently. The second challenges is the cold-start problem.\nDifferent from the manual review for credit card application in conventional\nbanks, the credit limit of Ant Credit Pay is automatically offered to users\nbased on the knowledge learned from big data. However, default prediction for\nnew users is suffered from lack of enough credit behaviors. It requires that\nthe proposal should leverage other new data source to alleviate the cold-start\nproblem. Considering the above challenges and the special scenario in Ant\nFinancial, we try to incorporate default prediction with network information to\nalleviate the cold-start problem. In this paper, we propose an industrial-scale\ndistributed network representation framework, termed NetDP, for default\nprediction in Ant Credit Pay. The proposal explores network information\ngenerated by various interaction between users, and blends unsupervised and\nsupervised network representation in a unified framework for default prediction\nproblem. Moreover, we present a parameter-server-based distributed implement of\nour proposal to handle the scalability challenge. Experimental results\ndemonstrate the effectiveness of our proposal, especially in cold-start\nproblem, as well as the efficiency for industrial-scale dataset.\n"
    },
    {
        "paper_id": 2004.00493,
        "authors": "Claudius Gros, Roser Valenti, Lukas Schneider, Kilian Valenti, Daniel\n  Gros",
        "title": "Containment efficiency and control strategies for the Corona pandemic\n  costs",
        "comments": "Scientific Reports, in press",
        "journal-ref": "Scientific Reports 11, 6848 (2021)",
        "doi": "10.1038/s41598-021-86072-x",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The rapid spread of the Coronavirus (COVID-19) confronts policy makers with\nthe problem of measuring the effectiveness of containment strategies, balancing\npublic health considerations with the economic costs of social distancing\nmeasures. We introduce a modified epidemic model that we name the\ncontrolled-SIR model, in which the disease reproduction rate evolves\ndynamically in response to political and societal reactions. An analytic\nsolution is presented. The model reproduces official COVID-19 cases counts of a\nlarge number of regions and countries that surpassed the first peak of the\noutbreak. A single unbiased feedback parameter is extracted from field data and\nused to formulate an index that measures the efficiency of containment\nstrategies (the CEI index). CEI values for a range of countries are given. For\ntwo variants of the controlled-SIR model, detailed estimates of the total\nmedical and socio-economic costs are evaluated over the entire course of the\nepidemic. Costs comprise medical care cost, the economic cost of social\ndistancing, as well as the economic value of lives saved. Under plausible\nparameters, strict measures fare better than a hands-off policy. Strategies\nbased on current case numbers lead to substantially higher total costs than\nstrategies based on the overall history of the epidemic.\n"
    },
    {
        "paper_id": 2004.0055,
        "authors": "Irena Barja\\v{s}i\\'c and Nino Antulov-Fantulin",
        "title": "Time-varying volatility in Bitcoin market and information flow at\n  minute-level frequency",
        "comments": "17 pages,11 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper, we analyze the time-series of minute price returns on the\nBitcoin market through the statistical models of generalized autoregressive\nconditional heteroskedasticity (GARCH) family. Several mathematical models have\nbeen proposed in finance, to model the dynamics of price returns, each of them\nintroducing a different perspective on the problem, but none without\nshortcomings. We combine an approach that uses historical values of returns and\ntheir volatilities - GARCH family of models, with a so-called \"Mixture of\nDistribution Hypothesis\", which states that the dynamics of price returns are\ngoverned by the information flow about the market. Using time-series of\nBitcoin-related tweets and volume of transactions as external information, we\ntest for improvement in volatility prediction of several GARCH model variants\non a minute level Bitcoin price time series. Statistical tests show that the\nsimplest GARCH(1,1) reacts the best to the addition of external signal to model\nvolatility process on out-of-sample data.\n"
    },
    {
        "paper_id": 2004.00669,
        "authors": "Monica Anna Giovanniello and Simone Tonin",
        "title": "A Note on the Provision of a Public Service of Different Qualities",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We study how the quality dimension affects the social optimum in a model of\nspatial differentiation where two facilities provide a public service. If\nquality enters linearly in the individuals' utility function, a symmetric\nconfiguration, in which both facilities have the same quality and serve groups\nof individuals of the same size, does not maximize the social welfare. This is\na surprising result as all individuals are symmetrically identical having the\nsame quality valuation. We also show that a symmetric configuration of\nfacilities may maximize the social welfare if the individuals' marginal utility\nof quality is decreasing.\n"
    },
    {
        "paper_id": 2004.0079,
        "authors": "David Evangelista and Yuri Thamsten",
        "title": "On finite population games of optimal trading",
        "comments": "36 pages, 4 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We investigate stochastic differential games of optimal trading comprising a\nfinite population. There are market frictions in the present framework, which\ntake the form of stochastic permanent and temporary price impacts. Moreover,\ninformation is asymmetric among the traders, with mild assumptions. For\nconstant market parameters, we provide specialized results. Each player selects\nher parameters based not only on her informational level but also on her\nparticular preferences. The first part of the work is where we examine the\nunconstrained problem, in which traders do not necessarily have to reach the\nend of the horizon with vanishing inventory. In the sequel, we proceed to\nanalyze the constrained situation as an asymptotic limit of the previous one.\nWe prove the existence and uniqueness of a Nash equilibrium in both frameworks,\nalongside a characterization, under suitable assumptions. We conclude the paper\nby presenting an extension of the basic model to a hierarchical market, for\nwhich we establish the existence, uniqueness, and characterization of a\nStackelberg-Nash equilibrium.\n"
    },
    {
        "paper_id": 2004.00944,
        "authors": "Hsuan-Wei Lee, Yen-Ping Chang, and Yen-Sheng Chiang",
        "title": "Status hierarchy and group cooperation: A generalized model",
        "comments": "42 pages, 11 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In a refreshing mathematical investigation, Mark (2018) shows that status\nhierarchy may facilitate the emergence of cooperation in groups. Despite the\ncontribution, the present paper notes that there are limitations in Mark's\nmodel that makes it less realistic than it could in explaining real-world\nexperiences. Consequently, we present a more generalized modified framework in\nwhich his model is a special case, by developing and introducing a new\nhierarchy measure into the model to estimate the cooperation level in a set of\nhierarchical structures omitted in Mark's work yet common in everyday\nlife--those with multiple leaders. We derived the conditions under which\ncooperation can emerge in these groups, and verified our analytical predictions\nin agent-based computer simulations. In so doing, not only does our model\nelaborate on its predecessor and support Mark's general prediction. For theory,\nour work further reveals two novel phenomena of group cooperation: Both the\nrelative number of cooperators to defectors in groups and the assortativity\namong these different roles can backfire; they are not always the higher, the\nbetter for cooperation to thrive. For methodology, the hierarchy measure\ndeveloped and our model using the measure may also be applied in future\nresearch on a wide range of related topics.\n"
    },
    {
        "paper_id": 2004.00999,
        "authors": "Fangzhou Xie",
        "title": "Pruned Wasserstein Index Generation Model and wigpy Package",
        "comments": "fix typos and errors",
        "journal-ref": null,
        "doi": "10.4995/CARMA2020.2020.11557",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Recent proposal of Wasserstein Index Generation model (WIG) has shown a new\ndirection for automatically generating indices. However, it is challenging in\npractice to fit large datasets for two reasons. First, the Sinkhorn distance is\nnotoriously expensive to compute and suffers from dimensionality severely.\nSecond, it requires to compute a full $N\\times N$ matrix to be fit into memory,\nwhere $N$ is the dimension of vocabulary. When the dimensionality is too large,\nit is even impossible to compute at all. I hereby propose a Lasso-based\nshrinkage method to reduce dimensionality for the vocabulary as a\npre-processing step prior to fitting the WIG model. After we get the word\nembedding from Word2Vec model, we could cluster these high-dimensional vectors\nby $k$-means clustering, and pick most frequent tokens within each cluster to\nform the \"base vocabulary\". Non-base tokens are then regressed on the vectors\nof base token to get a transformation weight and we could thus represent the\nwhole vocabulary by only the \"base tokens\". This variant, called pruned WIG\n(pWIG), will enable us to shrink vocabulary dimension at will but could still\nachieve high accuracy. I also provide a \\textit{wigpy} module in Python to\ncarry out computation in both flavor. Application to Economic Policy\nUncertainty (EPU) index is showcased as comparison with existing methods of\ngenerating time-series sentiment indices.\n"
    },
    {
        "paper_id": 2004.01304,
        "authors": "Ariah Klages-Mundt, Andreea Minca",
        "title": "While Stability Lasts: A Stochastic Model of Non-Custodial Stablecoins",
        "comments": null,
        "journal-ref": null,
        "doi": "10.1111/mafi.12357",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The `Black Thursday' crisis in cryptocurrency markets demonstrated\ndeleveraging risks in over-collateralized non-custodial stablecoins. We develop\na stochastic model that helps explain deleveraging crises in these\nover-collateralized systems. In our model, the stablecoin supply is decided by\nspeculators who optimize the profitability of a leveraged position while\nincorporating the forward-looking cost of collateral liquidations, which\ninvolves the endogenous price of the stablecoin. We formally characterize\nregimes that are interpreted as stable and unstable for the stablecoin. We\nprove bounds on quadratic variation and the probability of large deviations in\nthe stable domain and we demonstrate distinctly greater price variance in the\nunstable domain. We identify a deflationary deleveraging spiral by means of a\nsubmartingale. These deleveraging spirals, which resemble short squeezes, lead\nto faster collateral drawdown (and potential shortfalls) and are accompanied by\nhigher price variance, as experienced on Black Thursday. We conclude by\ndiscussing non-custodial ways in which the issues raised in this paper can be\nmitigated.\n"
    },
    {
        "paper_id": 2004.01311,
        "authors": "Nik Dawson, Marian-Andrei Rizoiu, Benjamin Johnston and Mary-Anne\n  Williams",
        "title": "Predicting Skill Shortages in Labor Markets: A Machine Learning Approach",
        "comments": null,
        "journal-ref": "Workshop on Human-in-the-Loop Methods and Future of Work in\n  BigData (HMData'20), 2020",
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Skill shortages are a drain on society. They hamper economic opportunities\nfor individuals, slow growth for firms, and impede labor productivity in\naggregate. Therefore, the ability to understand and predict skill shortages in\nadvance is critical for policy-makers and educators to help alleviate their\nadverse effects. This research implements a high-performing Machine Learning\napproach to predict occupational skill shortages. In addition, we demonstrate\nmethods to analyze the underlying skill demands of occupations in shortage and\nthe most important features for predicting skill shortages. For this work, we\ncompile a unique dataset of both Labor Demand and Labor Supply occupational\ndata in Australia from 2012 to 2018. This includes data from 7.7 million job\nadvertisements (ads) and 20 official labor force measures. We use these data as\nexplanatory variables and leverage the XGBoost classifier to predict yearly\nskills shortage classifications for 132 standardized occupations. The models we\nconstruct achieve macro-F1 average performance scores of up to 83 per cent. Our\nresults show that job ads data and employment statistics were the highest\nperforming feature sets for predicting year-to-year skills shortage changes for\noccupations. We also find that features such as 'Hours Worked', years of\n'Education', years of 'Experience', and median 'Salary' are highly important\nfeatures for predicting occupational skill shortages. This research provides a\nrobust data-driven approach for predicting and analyzing skill shortages, which\ncan assist policy-makers, educators, and businesses to prepare for the future\nof work.\n"
    },
    {
        "paper_id": 2004.01489,
        "authors": "Bohdan M. Pavlyshenko",
        "title": "Regression Approach for Modeling COVID-19 Spread and its Impact On Stock\n  Market",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The paper studies different regression approaches for modeling COVID-19\nspread and its impact on the stock market. The logistic curve model was used\nwith Bayesian regression for predictive analytics of the coronavirus spread.\nThe impact of COVID-19 was studied using regression approach and compared to\nother crises influence. In practical analytics, it is important to find the\nmaximum of coronavirus cases per day, this point means the estimated half time\nof coronavirus spread in the region under investigation. The obtained results\nshow that different crises with different reasons have different impact on the\nsame stocks. It is important to analyze their impact separately. Bayesian\ninference makes it possible to analyze the uncertainty of crisis impacts.\n"
    },
    {
        "paper_id": 2004.01496,
        "authors": "Sven Husmann, Antoniya Shivarova, Rick Steinert",
        "title": "Company classification using machine learning",
        "comments": "16 pages, 6 figures, 1 table",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The recent advancements in computational power and machine learning\nalgorithms have led to vast improvements in manifold areas of research.\nEspecially in finance, the application of machine learning enables both\nresearchers and practitioners to gain new insights into financial data and\nwell-studied areas such as company classification. In our paper, we demonstrate\nthat unsupervised machine learning algorithms can be used to visualize and\nclassify company data in an economically meaningful and effective way. In\nparticular, we implement the data-driven dimension reduction and visualization\ntool t-distributed stochastic neighbor embedding (t-SNE) in combination with\nspectral clustering. The resulting company groups can then be utilized by\nexperts in the field for empirical analysis and optimal decision making. By\nproviding an exemplary out-of-sample study within a portfolio optimization\nframework, we show that the application of t-SNE and spectral clustering\nimproves the overall portfolio performance. Therefore, we introduce our\napproach to the financial community as a valuable technique in the context of\ndata analysis and company classification.\n"
    },
    {
        "paper_id": 2004.01497,
        "authors": "Mojtaba Nabipour, Pooyan Nayyeri, Hamed Jabani, Amir Mosavi",
        "title": "Deep learning for Stock Market Prediction",
        "comments": "25 pages, 35 tables, 6 figures",
        "journal-ref": null,
        "doi": "10.3390/e22080840",
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Prediction of stock groups' values has always been attractive and challenging\nfor shareholders. This paper concentrates on the future prediction of stock\nmarket groups. Four groups named diversified financials, petroleum,\nnon-metallic minerals and basic metals from Tehran stock exchange are chosen\nfor experimental evaluations. Data are collected for the groups based on ten\nyears of historical records. The values predictions are created for 1, 2, 5,\n10, 15, 20 and 30 days in advance. The machine learning algorithms utilized for\nprediction of future values of stock market groups. We employed Decision Tree,\nBagging, Random Forest, Adaptive Boosting (Adaboost), Gradient Boosting and\neXtreme Gradient Boosting (XGBoost), and Artificial neural network (ANN),\nRecurrent Neural Network (RNN) and Long short-term memory (LSTM). Ten technical\nindicators are selected as the inputs into each of the prediction models.\nFinally, the result of predictions is presented for each technique based on\nthree metrics. Among all the algorithms used in this paper, LSTM shows more\naccurate results with the highest model fitting ability. Also, for tree-based\nmodels, there is often an intense competition between Adaboost, Gradient\nBoosting, and XGBoost.\n"
    },
    {
        "paper_id": 2004.01498,
        "authors": "Ye-Sheen Lim, Denise Gorse",
        "title": "Deep Probabilistic Modelling of Price Movements for High-Frequency\n  Trading",
        "comments": "8 pages, 2 columns, IJCNN",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper we propose a deep recurrent architecture for the probabilistic\nmodelling of high-frequency market prices, important for the risk management of\nautomated trading systems. Our proposed architecture incorporates probabilistic\nmixture models into deep recurrent neural networks. The resulting deep mixture\nmodels simultaneously address several practical challenges important in the\ndevelopment of automated high-frequency trading strategies that were previously\nneglected in the literature: 1) probabilistic forecasting of the price\nmovements; 2) single objective prediction of both the direction and size of the\nprice movements. We train our models on high-frequency Bitcoin market data and\nevaluate them against benchmark models obtained from the literature. We show\nthat our model outperforms the benchmark models in both a metric-based test and\nin a simulated trading scenario\n"
    },
    {
        "paper_id": 2004.01499,
        "authors": "Ye-Sheen Lim, Denise Gorse",
        "title": "Deep Recurrent Modelling of Stationary Bitcoin Price Formation Using the\n  Order Flow",
        "comments": "10 pages, The 19th International Conference on Artificial\n  Intelligence and Soft Computing",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper we propose a deep recurrent model based on the order flow for\nthe stationary modelling of the high-frequency directional prices movements.\nThe order flow is the microsecond stream of orders arriving at the exchange,\ndriving the formation of prices seen on the price chart of a stock or currency.\nTo test the stationarity of our proposed model we train our model on data\nbefore the 2017 Bitcoin bubble period and test our model during and after the\nbubble. We show that without any retraining, the proposed model is temporally\nstable even as Bitcoin trading shifts into an extremely volatile \"bubble\ntrouble\" period. The significance of the result is shown by benchmarking\nagainst existing state-of-the-art models in the literature for modelling price\nformation using deep learning.\n"
    },
    {
        "paper_id": 2004.01502,
        "authors": "Jonghyeon Min",
        "title": "Financial Market Trend Forecasting and Performance Analysis Using LSTM",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The financial market trend forecasting method is emerging as a hot topic in\nfinancial markets today. Many challenges still currently remain, and various\nresearches related thereto have been actively conducted. Especially, recent\nresearch of neural network-based financial market trend prediction has\nattracted much attention. However, previous researches do not deal with the\nfinancial market forecasting method based on LSTM which has good performance in\ntime series data. There is also a lack of comparative analysis in the\nperformance of neural network-based prediction techniques and traditional\nprediction techniques. In this paper, we propose a financial market trend\nforecasting method using LSTM and analyze the performance with existing\nfinancial market trend forecasting methods through experiments. This method\nprepares the input data set through the data preprocessing process so as to\nreflect all the fundamental data, technical data and qualitative data used in\nthe financial data analysis, and makes comprehensive financial market analysis\nthrough LSTM. In this paper, we experiment and compare performances of existing\nfinancial market trend forecasting models, and performance according to the\nfinancial market environment. In addition, we implement the proposed method\nusing open sources and platform and forecast financial market trends using\nvarious financial data indicators.\n"
    },
    {
        "paper_id": 2004.01504,
        "authors": "Philip Ndikum",
        "title": "Machine Learning Algorithms for Financial Asset Price Forecasting",
        "comments": "16 pages, 4 Figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This research paper explores the performance of Machine Learning (ML)\nalgorithms and techniques that can be used for financial asset price\nforecasting. The prediction and forecasting of asset prices and returns remains\none of the most challenging and exciting problems for quantitative finance and\npractitioners alike. The massive increase in data generated and captured in\nrecent years presents an opportunity to leverage Machine Learning algorithms.\nThis study directly compares and contrasts state-of-the-art implementations of\nmodern Machine Learning algorithms on high performance computing (HPC)\ninfrastructures versus the traditional and highly popular Capital Asset Pricing\nModel (CAPM) on U.S equities data. The implemented Machine Learning models -\ntrained on time series data for an entire stock universe (in addition to\nexogenous macroeconomic variables) significantly outperform the CAPM on\nout-of-sample (OOS) test data.\n"
    },
    {
        "paper_id": 2004.01506,
        "authors": "John Armstrong and Cristin Buescu",
        "title": "Asymptotically Optimal Management of Heterogeneous Collectivised\n  Investment Funds",
        "comments": "arXiv admin note: substantial text overlap with arXiv:1909.12730",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  A collectivised fund is a proposed form of pension investment, in which all\ninvestors agree that any funds associated with deceased members should be split\namong survivors. For this to be a viable financial product, it is necessary to\nknow how to manage the fund even when it is heterogeneous: that is when\ndifferent investors have different preferences, wealth and mortality. There is\nno obvious way to define a single objective for a heterogeneous fund, so this\nis not an optimal control problem. In lieu of an objective function, we take an\naxiomatic approach. Subject to our axioms on the management of the fund, we\nfind an upper bound on the utility that can be achieved for each investor,\nassuming a complete markets and the absence of systematic longevity risk. We\ngive a strategy for the management of such heterogeneous funds which achieves\nthis bound asymptotically as the number of investors tends to infinity.\n"
    },
    {
        "paper_id": 2004.01509,
        "authors": "Amir Mosavi, Pedram Ghamisi, Yaser Faghan, Puhong Duan",
        "title": "Comprehensive Review of Deep Reinforcement Learning Methods and\n  Applications in Economics",
        "comments": "42 pages, 26 figures",
        "journal-ref": null,
        "doi": "10.20944/preprints202003.0309.v1",
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  The popularity of deep reinforcement learning (DRL) methods in economics have\nbeen exponentially increased. DRL through a wide range of capabilities from\nreinforcement learning (RL) and deep learning (DL) for handling sophisticated\ndynamic business environments offers vast opportunities. DRL is characterized\nby scalability with the potential to be applied to high-dimensional problems in\nconjunction with noisy and nonlinear patterns of economic data. In this work,\nwe first consider a brief review of DL, RL, and deep RL methods in diverse\napplications in economics providing an in-depth insight into the state of the\nart. Furthermore, the architecture of DRL applied to economic applications is\ninvestigated in order to highlight the complexity, robustness, accuracy,\nperformance, computational tasks, risk constraints, and profitability. The\nsurvey results indicate that DRL can provide better performance and higher\naccuracy as compared to the traditional algorithms while facing real economic\nproblems at the presence of risk parameters and the ever-increasing\nuncertainties.\n"
    },
    {
        "paper_id": 2004.01624,
        "authors": "Mehdi Tomas, Iacopo Mastromatteo, Michael Benzaquen",
        "title": "How to build a cross-impact model from first principles: Theoretical\n  requirements and empirical results",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Trading a financial instrument pushes its price and those of other assets, a\nphenomenon known as cross-impact. To be of use, cross-impact models must fit\ndata and be well-behaved so they can be applied in applications such as optimal\ntrading. To address these issues, we introduce a set of desirable properties\nwhich constrain cross-impact models. We classify cross-impact models according\nto which properties they satisfy and stress them on three different asset\nclasses to evaluate goodness-of-fit. We find that two models are robust across\nmarkets, but only one satisfies all desirable properties and is appropriate for\napplications.\n"
    },
    {
        "paper_id": 2004.01831,
        "authors": "Stefano Giglio, Matteo Maggiori, Johannes Stroebel, Stephen Utkus",
        "title": "Inside the Mind of a Stock Market Crash",
        "comments": "13 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We analyze how investor expectations about economic growth and stock returns\nchanged during the February-March 2020 stock market crash induced by the\nCOVID-19 pandemic, as well as during the subsequent partial stock market\nrecovery. We surveyed retail investors who are clients of Vanguard at three\npoints in time: (i) on February 11-12, around the all-time stock market high,\n(ii) on March 11-12, after the stock market had collapsed by over 20\\%, and\n(iii) on April 16-17, after the market had rallied 25\\% from its lowest point.\nFollowing the crash, the average investor turned more pessimistic about the\nshort-run performance of both the stock market and the real economy. Investors\nalso perceived higher probabilities of both further extreme stock market\ndeclines and large declines in short-run real economic activity. In contrast,\ninvestor expectations about long-run (10-year) economic and stock market\noutcomes remained largely unchanged, and, if anything, improved. Disagreement\namong investors about economic and stock market outcomes also increased\nsubstantially following the stock market crash, with the disagreement\npersisting through the partial market recovery. Those respondents who were the\nmost optimistic in February saw the largest decline in expectations, and sold\nthe most equity. Those respondents who were the most pessimistic in February\nlargely left their portfolios unchanged during and after the crash.\n"
    },
    {
        "paper_id": 2004.01838,
        "authors": "Benjamin Avanzi and Hayden Lau and Bernard Wong",
        "title": "Optimal periodic dividend strategies for spectrally negative L\\'evy\n  processes with fixed transaction costs",
        "comments": null,
        "journal-ref": null,
        "doi": "10.1080/03461238.2020.1869069",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Maximising dividends is one classical stability criterion in actuarial risk\ntheory. Motivated by the fact that dividends are paid periodically in real\nlife, $\\textit{periodic}$ dividend strategies were recently introduced\n(Albrecher, Gerber and Shiu, 2011). In this paper, we incorporate fixed\ntransaction costs into the model and study the optimal periodic dividend\nstrategy with fixed transaction costs for spectrally negative L\\'evy processes.\n  The value function of a periodic $(b_u,b_l)$ strategy is calculated by means\nof exiting identities and It\\^o's excusion when the surplus process is of\nunbounded variation. We show that a sufficient condition for optimality is that\nthe L\\'evy measure admits a density which is completely monotonic. Under such\nassumptions, a periodic $(b_u,b_l)$ strategy is confirmed to be optimal.\n  Results are illustrated.\n"
    },
    {
        "paper_id": 2004.01865,
        "authors": "Jos\\'e E. Figueroa-L\\'opez and Bei Wu",
        "title": "Kernel Estimation of Spot Volatility with Microstructure Noise Using\n  Pre-Averaging",
        "comments": "53 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We first revisit the problem of estimating the spot volatility of an It\\^o\nsemimartingale using a kernel estimator. We prove a Central Limit Theorem with\noptimal convergence rate for a general two-sided kernel. Next, we introduce a\nnew pre-averaging/kernel estimator for spot volatility to handle the\nmicrostructure noise of ultra high-frequency observations. We prove a Central\nLimit Theorem for the estimation error with an optimal rate and study the\noptimal selection of the bandwidth and kernel functions. We show that the\npre-averaging/kernel estimator's asymptotic variance is minimal for exponential\nkernels, hence, justifying the need of working with kernels of unbounded\nsupport as proposed in this work. We also develop a feasible implementation of\nthe proposed estimators with optimal bandwidth. Monte Carlo experiments confirm\nthe superior performance of the devised method.\n"
    },
    {
        "paper_id": 2004.01917,
        "authors": "Xiaoling Tan, Jichang Zhao",
        "title": "The illiquidity network of stocks in China's market crash",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The Chinese stock market experienced an abrupt crash in 2015, and over\none-third of its market value evaporated. Given its associations with fear and\nthe fine resolution with respect to frequency, the illiquidity of stocks may\noffer a promising perspective for understanding and even signaling a market\ncrash. In this study, by connecting stocks with illiquidity comovements, an\nilliquidity network is established to model the market. Compared to noncrash\ndays, on crash days, the market is more densely connected due to heavier but\nmore homogeneous illiquidity dependencies that facilitate abrupt collapses.\nCritical stocks in the illiquidity network, particularly those in the finance\nsector, are targeted for inspection because of their crucial roles in\naccumulating and passing on illiquidity losses. The cascading failures of\nstocks in market crashes are profiled as disseminating from small degrees to\nhigh degrees that are usually located in the core of the illiquidity network\nand then back to the periphery. By counting the days with random failures in\nthe previous five days, an early signal is implemented to successfully predict\nmore than half of the crash days, especially consecutive days in the early\nphase. Additional evidence from both the Granger causality network and the\nrandom network further testifies to the robustness of the signal. Our results\ncould help market practitioners such as regulators detect and prevent the risk\nof crashes in advance.\n"
    },
    {
        "paper_id": 2004.02198,
        "authors": "Ivan Guo, Gregoire Loeper, Jan Obloj, Shiyi Wang",
        "title": "Joint Modelling and Calibration of SPX and VIX by Optimal Transport",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper addresses the joint calibration problem of SPX options and VIX\noptions or futures. We show that the problem can be formulated as a\nsemimartingale optimal transport problem under a finite number of discrete\nconstraints, in the spirit of [arXiv:1906.06478]. We introduce a PDE\nformulation along with its dual counterpart. The solution, a calibrated\ndiffusion process, can be represented via the solutions of\nHamilton-Jacobi-Bellman equations arising from the dual formulation. The method\nis tested on both simulated data and market data. Numerical examples show that\nthe model can be accurately calibrated to SPX options, VIX options and VIX\nfutures simultaneously.\n"
    },
    {
        "paper_id": 2004.02296,
        "authors": "Christopher S. Carpenter, Gilbert Gonzales, Tara McKay, Dario Sansone",
        "title": "Effects of the Affordable Care Act Dependent Coverage Mandate on Health\n  Insurance Coverage for Individuals in Same-Sex Couples",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  A large body of research documents that the 2010 dependent coverage mandate\nof the Affordable Care Act was responsible for significantly increasing health\ninsurance coverage among young adults. No prior research has examined whether\nsexual minority young adults also benefitted from the dependent coverage\nmandate, despite previous studies showing lower health insurance coverage among\nsexual minorities and the fact that their higher likelihood of strained\nrelationships with their parents might predict a lower ability to use parental\ncoverage. Our estimates from the American Community Surveys using\ndifference-in-differences and event study models show that men in same-sex\ncouples age 21-25 were significantly more likely to have any health insurance\nafter 2010 compared to the associated change for slightly older 27 to\n31-year-old men in same-sex couples. This increase is concentrated among\nemployer-sponsored insurance, and it is robust to permutations of time periods\nand age groups. Effects for women in same-sex couples and men in different-sex\ncouples are smaller than the associated effects for men in same-sex couples.\nThese findings confirm the broad effects of expanded dependent coverage and\nsuggest that eliminating the federal dependent mandate could reduce health\ninsurance coverage among young adult sexual minorities in same-sex couples.\n"
    },
    {
        "paper_id": 2004.02312,
        "authors": "Richard J. Martin",
        "title": "Fixed income portfolio optimisation: Interest rates, credit, and the\n  efficient frontier",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Fixed income has received far less attention than equity portfolio\noptimisation since Markowitz' original work of 1952, partly as a result of the\nneed to model rates and credit risk. We argue that the shape of the efficient\nfrontier is mainly controlled by linear constraints, with the standard\ndeviation relatively unimportant, and propose a two-factor model for its time\nevolution.\n"
    },
    {
        "paper_id": 2004.0267,
        "authors": "Stelios Arvanitis, Olivier Scaillet, Nikolas Topaloglou",
        "title": "Spanning analysis of stock market anomalies under Prospect Stochastic\n  Dominance",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We develop and implement methods for determining whether introducing new\nsecurities or relaxing investment constraints improves the investment\nopportunity set for prospect investors. We formulate a new testing procedure\nfor prospect spanning for two nested portfolio sets based on subsampling and\nLinear Programming. In an application, we use the prospect spanning framework\nto evaluate whether well-known anomalies are spanned by standard factors. We\nfind that of the strategies considered, many expand the opportunity set of the\nprospect type investors, thus have real economic value for them. In-sample and\nout-of-sample results prove remarkably consistent in identifying genuine\nanomalies for prospect investors.\n"
    },
    {
        "paper_id": 2004.02706,
        "authors": "Michele Loberto, Andrea Luciani, Marco Pangallo",
        "title": "What do online listings tell us about the housing market?",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Traditional data sources for the analysis of housing markets show several\nlimitations, that recently started to be overcome using data coming from\nhousing sales advertisements (ads) websites. In this paper, using a large\ndataset of ads in Italy, we provide the first comprehensive analysis of the\nproblems and potential of these data. The main problem is that multiple ads\n(\"duplicates\") can correspond to the same housing unit. We show that this issue\nis mainly caused by sellers' attempt to increase visibility of their listings.\nDuplicates lead to misrepresentation of the volume and composition of housing\nsupply, but this bias can be corrected by identifying duplicates with machine\nlearning tools. We then focus on the potential of these data. We show that the\ntimeliness, granularity, and online nature of these data allow monitoring of\nhousing demand, supply and liquidity, and that the (asking) prices posted on\nthe website can be more informative than transaction prices.\n"
    },
    {
        "paper_id": 2004.03107,
        "authors": "Dirk Bergemann, Alessandro Bonatti, Tan Gan",
        "title": "The Economics of Social Data",
        "comments": null,
        "journal-ref": "The RAND Journal of Economics, 53: 263-296 (2022)",
        "doi": "10.1111/1756-2171.12407",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  A data intermediary acquires signals from individual consumers regarding\ntheir preferences. The intermediary resells the information in a product market\nwherein firms and consumers tailor their choices to the demand data. The social\ndimension of the individual data -- whereby a consumer's data are predictive of\nothers' behavior -- generates a data externality that can reduce the\nintermediary's cost of acquiring the information. The intermediary optimally\npreserves the privacy of consumers' identities if and only if doing so\nincreases social surplus. This policy enables the intermediary to capture the\ntotal value of the information as the number of consumers becomes large.\n"
    },
    {
        "paper_id": 2004.03165,
        "authors": "Christian Bongiorno",
        "title": "Bootstraps Regularize Singular Correlation Matrices",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  I show analytically that the average of $k$ bootstrapped correlation matrices\nrapidly becomes positive-definite as $k$ increases, which provides a simple\napproach to regularize singular Pearson correlation matrices. If $n$ is the\nnumber of objects and $t$ the number of features, the averaged correlation\nmatrix is almost surely positive-definite if $k> \\frac{e}{e-1}\\frac{n}{t}\\simeq\n1.58\\frac{n}{t}$ in the limit of large $t$ and $n$. The probability of\nobtaining a positive-definite correlation matrix with $k$ bootstraps is also\nderived for finite $n$ and $t$. Finally, I demonstrate that the number of\nrequired bootstraps is always smaller than $n$. This method is particularly\nrelevant in fields where $n$ is orders of magnitude larger than the size of\ndata points $t$, e.g., in finance, genetics, social science, or image\nprocessing.\n"
    },
    {
        "paper_id": 2004.0319,
        "authors": "Wei-Zhen Li (ECUST), Jin-Rui Zhai (ECUST), Zhi-Qiang Jiang (ECUST),\n  Gang-Jin Wang (HNU), Wei-Xing Zhou (ECUST)",
        "title": "Predicting tail events in a RIA-EVT-Copula framework",
        "comments": "14 pages, 5 figures, and 4 tables",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Predicting the occurrence of tail events is of great importance in financial\nrisk management. By employing the method of peak-over-threshold (POT) to\nidentify the financial extremes, we perform a recurrence interval analysis\n(RIA) on these extremes. We find that the waiting time between consecutive\nextremes (recurrence interval) follow a $q$-exponential distribution and the\nsizes of extremes above the thresholds (exceeding size) conform to a\ngeneralized Pareto distribution. We also find that there is a significant\ncorrelation between recurrence intervals and exceeding sizes. We thus model the\njoint distribution of recurrence intervals and exceeding sizes through\nconnecting the two corresponding marginal distributions with the Frank and AMH\ncopula functions, and apply this joint distribution to estimate the hazard\nprobability to observe another extreme in $\\Delta t$ time since the last\nextreme happened $t$ time ago. Furthermore, an extreme predicting model based\non RIA-EVT-Copula is proposed by applying a decision-making algorithm on the\nhazard probability. Both in-sample and out-of-sample tests reveal that this new\nextreme forecasting framework has better performance in prediction comparing\nwith the forecasting model based on the hazard probability only estimated from\nthe distribution of recurrence intervals. Our results not only shed a new light\non understanding the occurring pattern of extremes in financial markets, but\nalso improve the accuracy to predict financial extremes for risk management.\n"
    },
    {
        "paper_id": 2004.03319,
        "authors": "Pawe{\\l} O\\'swi\\k{e}cimka, Stanis{\\l}aw Dro\\.zd\\.z, Mattia Frasca,\n  Robert G\\k{e}barowski, Natsue Yoshimura, Luciano Zunino, Ludovico Minati",
        "title": "Wavelet-based discrimination of isolated singularities masquerading as\n  multifractals in detrended fluctuation analyses",
        "comments": "To appear in Nonlinear Dynamics",
        "journal-ref": null,
        "doi": "10.1007/s11071-020-05581-y",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The robustness of two widespread multifractal analysis methods, one based on\ndetrended fluctuation analysis and one on wavelet leaders, is discussed in the\ncontext of time-series containing non-uniform structures with only isolated\nsingularities. Signals generated by simulated and experimentally-realized chaos\ngenerators, together with synthetic data addressing particular aspects, are\ntaken into consideration. The results reveal essential limitations affecting\nthe ability of both methods to correctly infer the non-multifractal nature of\nsignals devoid of a cascade-like hierarchy of singularities. Namely, signals\nharboring only isolated singularities are found to artefactually give rise to\nbroad multifractal spectra, resembling those expected in the presence of a\nwell-developed underlying multifractal structure. Hence, there is a real risk\nof incorrectly inferring multifractality due to isolated singularities. The\ncareful consideration of local scaling properties and the distribution of\nH\\\"older exponent obtained, for example, through wavelet analysis, is\nindispensable for rigorously assessing the presence or absence of\nmultifractality.\n"
    },
    {
        "paper_id": 2004.0333,
        "authors": "Zbigniew Palmowski, Jos\\'e Luis P\\'erez, Kazutoshi Yamazaki",
        "title": "Double continuation regions for American options under Poisson exercise\n  opportunities",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We consider the L\\'evy model of the perpetual American call and put options\nwith a negative discount rate under Poisson observations. Similar to the\ncontinuous observation case as in De Donno et al. [24], the stopping region\nthat characterizes the optimal stopping time is either a half-line or an\ninterval. The objective of this paper is to obtain explicit expressions of the\nstopping and continuation regions and the value function, focusing on\nspectrally positive and negative cases. To this end, we compute the identities\nrelated to the first Poisson arrival time to an interval via the scale function\nand then apply those identities to the computation of the optimal strategies.\nWe also discuss the convergence of the optimal solutions to those in the\ncontinuous observation case as the rate of observation increases to infinity.\nNumerical experiments are also provided.\n"
    },
    {
        "paper_id": 2004.03445,
        "authors": "Adriano Koshiyama, Sebastian Flennerhag, Stefano B. Blumberg, Nick\n  Firoozye and Philip Treleaven",
        "title": "QuantNet: Transferring Learning Across Systematic Trading Strategies",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Systematic financial trading strategies account for over 80% of trade volume\nin equities and a large chunk of the foreign exchange market. In spite of the\navailability of data from multiple markets, current approaches in trading rely\nmainly on learning trading strategies per individual market. In this paper, we\ntake a step towards developing fully end-to-end global trading strategies that\nleverage systematic trends to produce superior market-specific trading\nstrategies. We introduce QuantNet: an architecture that learns market-agnostic\ntrends and use these to learn superior market-specific trading strategies. Each\nmarket-specific model is composed of an encoder-decoder pair. The encoder\ntransforms market-specific data into an abstract latent representation that is\nprocessed by a global model shared by all markets, while the decoder learns a\nmarket-specific trading strategy based on both local and global information\nfrom the market-specific encoder and the global model. QuantNet uses recent\nadvances in transfer and meta-learning, where market-specific parameters are\nfree to specialize on the problem at hand, whilst market-agnostic parameters\nare driven to capture signals from all markets. By integrating over\nidiosyncratic market data we can learn general transferable dynamics, avoiding\nthe problem of overfitting to produce strategies with superior returns. We\nevaluate QuantNet on historical data across 3103 assets in 58 global equity\nmarkets. Against the top performing baseline, QuantNet yielded 51% higher\nSharpe and 69% Calmar ratios. In addition we show the benefits of our approach\nover the non-transfer learning variant, with improvements of 15% and 41% in\nSharpe and Calmar ratios. Code available in appendix.\n"
    },
    {
        "paper_id": 2004.03546,
        "authors": "Francesco Cordoni and Fabrizio Lillo",
        "title": "Instabilities in Multi-Asset and Multi-Agent Market Impact Games",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We consider the general problem of a set of agents trading a portfolio of\nassets in the presence of transient price impact and additional quadratic\ntransaction costs and we study, with analytical and numerical methods, the\nresulting Nash equilibria. Extending significantly the framework of Schied &\nZhang (2019) and Luo & Schied (2020), who considered the single asset case, we\nprove the existence and uniqueness of the corresponding Nash equilibria for the\nrelated mean-variance optimization problem. We then focus our attention on the\nconditions on the model parameters making the trading profile of the agents at\nequilibrium, and as a consequence the price trajectory, wildly oscillating and\nthe market unstable. While Schied & Zhang (2019) and Luo & Schied (2020)\nhighlighted the importance of the value of transaction cost in determining the\ntransition between a stable and an unstable phase, we show that also the\nscaling of market impact with the number of agents J and the number of assets M\ndetermines the asymptotic stability (in J and M ) of markets.\n"
    },
    {
        "paper_id": 2004.03715,
        "authors": "Frank Tietze, Pratheeba Vimalnath, Leonidas Aristodemou, Jenny Molloy",
        "title": "Crisis-Critical Intellectual Property: Findings from the COVID-19\n  Pandemic",
        "comments": "18 pages, 4 figures, 3 tables",
        "journal-ref": null,
        "doi": "10.17863/CAM.51142",
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Within national and international innovation systems a pandemic calls for\nlarge-scale action by many actors across sectors, to mobilise resources,\ndeveloping and manufacturing Crisis-Critical Products (CC-Products) efficiently\nand in the huge quantities needed. Nowadays, this also includes digital\ninnovations from complex epidemiological models, AI, to open data platforms for\nprevention, diagnostic and treatment. Amongst the many challenges during a\npandemic, innovation and manufacturing stakeholders find themselves engaged in\nnew relationships, and are likely to face intellectual property (IP) related\nchallenges. This paper adopts an IP perspective on the COVID-19 pandemic to\nidentify pandemic related IP considerations and IP challenges. The focus is on\nchallenges related to research, development and urgent upscaling of capacity to\nmanufacture CC-Products in the huge volumes suddenly in demand. Its purpose is\nto provide a structure for steering clear of IP challenges to avoid delays in\nfighting a pandemic. We identify 4 stakeholder groups concerned with IP\nchallenges: (i) governments, (ii) organisations owning existing Crisis-Critical\nIP, described as incumbents in Crisis-Critical Sectors (CC-Sectors), (iii)\nmanufacturing firms from other sectors normally not producing CC-Products\nsuddenly rushing into CC-Sectors to support the manufacturing of CC-Products\n(new entrants), and (iv) voluntary grassroot initiatives that are formed during\na pandemic. This paper discusses IP challenges related to the development and\nmanufacturing of technologies and products for (i) prevention (of spread), (ii)\ndiagnosis of infected patients and (iii) the development of treatments. We\noffer an initial discussion of potential response measures to reduce IP\nassociated risks among industrial stakeholders during a pandemic.\n"
    },
    {
        "paper_id": 2004.04015,
        "authors": "Maria Elvira Mancino, Simone Scotti, Giacomo Toscano",
        "title": "Is the variance swap rate affine in the spot variance? Evidence from\n  S&P500 data",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We empirically investigate the functional link between the variance swap rate\nand the spot variance. Using S\\&P500 data over the period 2006-2018, we find\noverwhelming empirical evidence supporting the affine link analytically found\nby Kallsen et al. (2011) in the context of exponentially affine stochastic\nvolatility models. Tests on yearly subsamples suggest that exponentially\nmean-reverting variance models provide a good fit during periods of extreme\nvolatility, while polynomial models, introduced in Cuchiero (2011), are suited\nfor years characterized by more frequent price jumps.\n"
    },
    {
        "paper_id": 2004.04048,
        "authors": "Matteo Gardini, Piergiacomo Sabino, Emanuela Sasso",
        "title": "Correlating L\\'evy processes with Self-Decomposability: Applications to\n  Energy Markets",
        "comments": "22 pages, 4 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Based on the concept of self-decomposability, we extend some recent\nmultivariate L\\'evy models built using multivariate subordination with the aim\nof capturing situations in which a sudden event in one market is propagated\nonto related markets after a certain stochastic time delay. Consequently, we\nstudy the properties of such processes, derive closed form expressions for the\ncharacteristic function and detail how a Monte Carlo scheme can be easily\nimplemented. We illustrate the applicability of our approach in the context of\ngas and power Energy markets focusing on the calibration and on the pricing of\nspread options written on different underlying assets using simulations\ntechniques.\n"
    },
    {
        "paper_id": 2004.04247,
        "authors": "Tatyana Deryugina, Frances Moore, Richard S.J. Tol",
        "title": "Applications of the Coase Theorem",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The Coase Theorem has a central place in the theory of environmental\neconomics and regulation. But its applicability for solving real-world\nexternality problems remains debated. In this paper, we first place this\nseminal contribution in its historical context. We then survey the experimental\nliterature that has tested the importance of the many, often tacit assumptions\nin the Coase Theorem in the laboratory. We discuss a selection of applications\nof the Coase Theorem to actual environmental problems, distinguishing between\nsituations in which the polluter or the pollutee pays. While limited in scope,\nCoasian bargaining over externalities offers a pragmatic solution to problems\nthat are difficult to solve in any other way.\n"
    },
    {
        "paper_id": 2004.04384,
        "authors": "Jussi T. S. Heikkil\\\"a",
        "title": "Classifying economics for the common good: Connecting sustainable\n  development goals to JEL codes",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
        "abstract": "  How does economics research help in solving societal challenges? This brief\nnote sheds additional light on this question by providing ways to connect\nJournal of Economic Literature (JEL) codes and Sustainable Development Goals\n(SDGs) of the United Nations. These simple linkages illustrate that the themes\nof SDGs have corresponding JEL classification codes. As the mappings presented\nhere are necessarily imperfect and incomplete, there is plenty of room for\nimprovements. In an ideal world, there would be a JEL classification system for\nSDGs, a separate JEL code for each of the 17 SDGs.\n"
    },
    {
        "paper_id": 2004.04397,
        "authors": "Alois Pichler, Ruben Schlotter",
        "title": "Quantification of Risk in Classical Models of Finance",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper enhances the pricing of derivatives as well as optimal control\nproblems to a level comprising risk. We employ nested risk measures to quantify\nrisk, investigate the limiting behavior of nested risk measures within the\nclassical models in finance and characterize existence of the risk-averse\nlimit. As a result we demonstrate that the nested limit is unique, irrespective\nof the initially chosen risk measure. Within the classical models risk aversion\ngives rise to a stream of risk premiums, comparable to dividend payments. In\nthis context we connect coherent risk measures with the Sharpe ratio from\nmodern portfolio theory and extract the Z-spread -- a widely accepted quantity\nin economics to hedge risk. The results for European option pricing are then\nextended to risk-averse American options, where we study the impact of risk on\nthe price as well as the optimal time to exercise the option. We also extend\nMerton's optimal consumption problem to the risk-averse setting.\n"
    },
    {
        "paper_id": 2004.04501,
        "authors": "Sander Willems",
        "title": "SABR smiles for RFR caplets",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We present a natural extension of the SABR model to price both backward and\nforward-looking RFR caplets in a post-Libor world. Forward-looking RFR caplets\ncan be priced using the market standard approximations of Hagan et al. (2002).\nWe provide closed-form effective SABR parameters for pricing backward-looking\nRFR caplets. These results are useful for smile interpolation and for analyzing\nbackward and forward-looking smiles in normalized units.\n"
    },
    {
        "paper_id": 2004.04605,
        "authors": "Yo-Der Song and Tomaso Aste (University College London)",
        "title": "The cost of Bitcoin mining has never really increased",
        "comments": "16 pages, 6 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The Bitcoin network is burning a large amount of energy for mining. In this\npaper we estimate the lower bound for the global energy cost for a period of\nten years from 2010, taking into account changing oil costs, improvements in\nhashing technologies and hashing activity. Despite a ten-billion-fold increase\nin hashing activity and a ten-million-fold increase in total energy\nconsumption, we find the cost relative to the volume of transactions has not\nincreased nor decreased since 2010. This is consistent with the perspective\nthat, in order to keep a the Blockchain system secure from double spending\nattacks, the proof or work must cost a sizable fraction of the value that can\nbe transferred through the network. We estimate that in the Bitcoin network\nthis fraction is of the order of 1%.\n"
    },
    {
        "paper_id": 2004.04867,
        "authors": "Zachary Barnett-Howell and Ahmed Mushfiq Mobarak",
        "title": "The Benefits and Costs of Social Distancing in Rich and Poor Countries",
        "comments": null,
        "journal-ref": "Transactions of The Royal Society of Tropical Medicine and Hygiene\n  Vol. 115 No. 7 (2021): 807-819",
        "doi": "10.1093/trstmh/traa140",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Social distancing is the primary policy prescription for combating the\nCOVID-19 pandemic, and has been widely adopted in Europe and North America. We\nestimate the value of disease avoidance using an epidemiological model that\nprojects the spread of COVID-19 across rich and poor countries. Social\ndistancing measures that \"flatten the curve\" of the disease to bring demand\nwithin the capacity of healthcare systems are predicted to save many lives in\nhigh-income countries, such that practically any economic cost is worth\nbearing. These social distancing policies are estimated to be less effective in\npoor countries with younger populations less susceptible to COVID-19, and more\nlimited healthcare systems, which were overwhelmed before the pandemic.\nMoreover, social distancing lowers disease risk by limiting people's economic\nopportunities. Poorer people are less willing to make those economic\nsacrifices. They place relatively greater value on their livelihood concerns\ncompared to contracting COVID-19. Not only are the epidemiological and economic\nbenefits of social distancing much smaller in poorer countries, such policies\nmay exact a heavy toll on the poorest and most vulnerable. Workers in the\ninformal sector lack the resources and social protections to isolate themselves\nand sacrifice economic opportunities until the virus passes. By limiting their\nability to earn a living, social distancing can lead to an increase in hunger,\ndeprivation, and related mortality and morbidity. Rather than a blanket\nadoption of social distancing measures, we advocate for the exploration of\nalternative harm-reduction strategies, including universal mask adoption and\nincreased hygiene measures.\n"
    },
    {
        "paper_id": 2004.05229,
        "authors": "Sonja Radosavljevic, L. Jamila Haider, Steven J. Lade, Maja Schluter",
        "title": "Effective alleviation of rural poverty depends on the interplay between\n  productivity, nutrients, water and soil quality",
        "comments": null,
        "journal-ref": "Ecological Economics, Volume 169, March 2020, 106494",
        "doi": "10.1016/j.ecolecon.2019.106494",
        "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
        "abstract": "  Most of the world poorest people come from rural areas and depend on their\nlocal ecosystems for food production. Recent research has highlighted the\nimportance of self-reinforcing dynamics between low soil quality and persistent\npoverty but little is known on how they affect poverty alleviation. We\ninvestigate how the intertwined dynamics of household assets, nutrients\n(especially phosphorus), water and soil quality influence food production and\ndetermine the conditions for escape from poverty for the rural poor. We have\ndeveloped a suite of dynamic, multidimensional poverty trap models of\nhouseholds that combine economic aspects of growth with ecological dynamics of\nsoil quality, water and nutrient flows to analyze the effectiveness of common\npoverty alleviation strategies such as intensification through agrochemical\ninputs, diversification of energy sources and conservation tillage. Our results\nshow that (i) agrochemical inputs can reinforce poverty by degrading soil\nquality, (ii) diversification of household energy sources can create\npossibilities for effective application of other strategies, and (iii)\nsequencing of interventions can improve effectiveness of conservation tillage.\nOur model-based approach demonstrates the interdependence of economic and\necological dynamics which preclude blanket solution for poverty alleviation.\nStylized models as developed here can be used for testing effectiveness of\ndifferent strategies given biophysical and economic settings in the target\nregion.\n"
    },
    {
        "paper_id": 2004.05322,
        "authors": "Huimin Peng",
        "title": "Holding-Based Evaluation upon Actively Managed Stock Mutual Funds in\n  China",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We analyze actively managed mutual funds in China from 2005 to 2017. We\ndevelop performance measures for asset allocation and selection. We find that\nstock selection ability from holding-based model is positively correlated with\nselection ability estimated from Fama-French three-factor model, which is\nprice-based regression model. We also find that industry allocation from\nholding-based model is positively correlated with timing ability estimated from\nprice-based Treynor-Mazuy model most of the time. We conclude that most\nactively managed funds have positive stock selection ability but not asset\nallocation ability, which is due to the difficulty in predicting policy\nchanges.\n"
    },
    {
        "paper_id": 2004.05325,
        "authors": "Wen-Jie Xie, Na Wei, Wei-Xing Zhou",
        "title": "Evolving efficiency and robustness of global oil trade networks",
        "comments": "19 pages, 9 figures",
        "journal-ref": "Journal of Statistical Mechanics 2021, 103401 (2021)",
        "doi": "10.1088/1742-5468/ac21da",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  As a vital strategic resource, oil has an essential influence on the world\neconomy, diplomacy and military development. Using oil trade data to\ndynamically monitor and warn about international trade risks is an urgent need.\nBased on the UN Comtrade data from 1988 to 2017, we construct unweighted and\nweighted global oil trade networks (OTNs). Complex network theories have some\nadvantages in analyzing global oil trade as a system with numerous economies\nand complicated relationships. This paper establishes a trading-based network\nmodel for global oil trade to study the evolving efficiency, criticality and\nrobustness of economies and the relationships between oil trade partners. The\nresults show that for unweighted OTNs, the efficiency of oil flows gradually\nincreases with growing complexity of the OTNs, and the weighted efficiency\nindicators are more capable of highlighting the impact of major events on the\nOTNs. The identified critical economies and trade relationships have more\nimportant strategic significance in the real market. The simulated deliberate\nattacks corresponding to national bankruptcy, trade blockade, and economic\nsanctions have a more significant impact on the robustness than random attacks.\nWhen the economies are promoting high-quality economic development, and\ncontinuously enhancing positions in the OTN, more attention needs be paid to\nthe identified critical economies and trade relationships. To conclude, some\nsuggestions for application are given according to the results.\n"
    },
    {
        "paper_id": 2004.05367,
        "authors": "Giuseppe Brandi and T. Di Matteo",
        "title": "A new multilayer network construction via Tensor learning",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Multilayer networks proved to be suitable in extracting and providing\ndependency information of different complex systems. The construction of these\nnetworks is difficult and is mostly done with a static approach, neglecting\ntime delayed interdependences. Tensors are objects that naturally represent\nmultilayer networks and in this paper, we propose a new methodology based on\nTucker tensor autoregression in order to build a multilayer network directly\nfrom data. This methodology captures within and between connections across\nlayers and makes use of a filtering procedure to extract relevant information\nand improve visualization. We show the application of this methodology to\ndifferent stationary fractionally differenced financial data. We argue that our\nresult is useful to understand the dependencies across three different aspects\nof financial risk, namely market risk, liquidity risk, and volatility risk.\nIndeed, we show how the resulting visualization is a useful tool for risk\nmanagers depicting dependency asymmetries between different risk factors and\naccounting for delayed cross dependencies. The constructed multilayer network\nshows a strong interconnection between the volumes and prices layers across all\nthe stocks considered while a lower number of interconnections between the\nuncertainty measures is identified.\n"
    },
    {
        "paper_id": 2004.0587,
        "authors": "F.N.M. de Sousa Filho, J.N. Silva, M.A. Bertella and E. Brigatti",
        "title": "The leverage effect and other stylized facts displayed by Bitcoin\n  returns",
        "comments": "10 pages, 6 figures",
        "journal-ref": null,
        "doi": "10.1007/s13538-020-00846-8",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper, we explore some stylized facts of the Bitcoin market using the\nBTC-USD exchange rate time series of historical intraday data from 2013 to\n2020. Bitcoin presents some very peculiar idiosyncrasies, like the absence of\nmacroeconomic fundamentals or connections with underlying assets or benchmarks,\nan asymmetry between demand and supply and the presence of inefficiency in the\nform of strong arbitrage opportunity. Nevertheless, all these elements seem to\nbe marginal in the definition of the structural statistical properties of this\nvirtual financial asset, which result to be analogous to general individual\nstocks or indices. In contrast, we find some clear differences, compared to\nfiat money exchange rates time series, in the values of the linear\nautocorrelation and, more surprisingly, in the presence of the leverage effect.\nWe also explore the dynamics of correlations, monitoring the shifts in the\nevolution of the Bitcoin market. This analysis is able to distinguish between\ntwo different regimes: a stochastic process with weaker memory signatures and\ncloser to Gaussianity between the Mt. Gox incident and the late 2015, and a\ndynamics with relevant correlations and strong deviations from Gaussianity\nbefore and after this interval.\n"
    },
    {
        "paper_id": 2004.05894,
        "authors": "Nassim Nicholas Taleb",
        "title": "What You See and What You Don't See: The Hidden Moments of a Probability\n  Distribution",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Empirical distributions have their in-sample maxima as natural censoring. We\nlook at the \"hidden tail\", that is, the part of the distribution in excess of\nthe maximum for a sample size of $n$. Using extreme value theory, we examine\nthe properties of the hidden tail and calculate its moments of order $p$. The\nmethod is useful in showing how large a bias one can expect, for a given $n$,\nbetween the visible in-sample mean and the true statistical mean (or higher\nmoments), which is considerable for $\\alpha$ close to 1. Among other\nproperties, we note that the \"hidden\" moment of order $0$, that is, the\nexceedance probability for power law distributions, follows an exponential\ndistribution and has for expectation $\\frac{1}{n}$ regardless of the\nparametrization of the scale and tail index.\n"
    },
    {
        "paper_id": 2004.0594,
        "authors": "Ioannis Boukas, Damien Ernst, Thibaut Th\\'eate, Adrien Bolland,\n  Alexandre Huynen, Martin Buchwald, Christelle Wynants, Bertrand Corn\\'elusse",
        "title": "A Deep Reinforcement Learning Framework for Continuous Intraday Market\n  Bidding",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The large integration of variable energy resources is expected to shift a\nlarge part of the energy exchanges closer to real-time, where more accurate\nforecasts are available. In this context, the short-term electricity markets\nand in particular the intraday market are considered a suitable trading floor\nfor these exchanges to occur. A key component for the successful renewable\nenergy sources integration is the usage of energy storage. In this paper, we\npropose a novel modelling framework for the strategic participation of energy\nstorage in the European continuous intraday market where exchanges occur\nthrough a centralized order book. The goal of the storage device operator is\nthe maximization of the profits received over the entire trading horizon, while\ntaking into account the operational constraints of the unit. The sequential\ndecision-making problem of trading in the intraday market is modelled as a\nMarkov Decision Process. An asynchronous distributed version of the fitted Q\niteration algorithm is chosen for solving this problem due to its sample\nefficiency. The large and variable number of the existing orders in the order\nbook motivates the use of high-level actions and an alternative state\nrepresentation. Historical data are used for the generation of a large number\nof artificial trajectories in order to address exploration issues during the\nlearning process. The resulting policy is back-tested and compared against a\nbenchmark strategy that is the current industrial standard. Results indicate\nthat the agent converges to a policy that achieves in average higher total\nrevenues than the benchmark strategy.\n"
    },
    {
        "paper_id": 2004.06098,
        "authors": "James H. Fowler, Seth J. Hill, Remy Levin, Nick Obradovich",
        "title": "The effect of stay-at-home orders on COVID-19 cases and fatalities in\n  the United States",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Governments issue \"stay at home\" orders to reduce the spread of contagious\ndiseases, but the magnitude of such orders' effectiveness is uncertain. In the\nUnited States these orders were not coordinated at the national level during\nthe coronavirus disease 2019 (COVID-19) pandemic, which creates an opportunity\nto use spatial and temporal variation to measure the policies' effect with\ngreater accuracy. Here, we combine data on the timing of stay-at-home orders\nwith daily confirmed COVID-19 cases and fatalities at the county level in the\nUnited States. We estimate the effect of stay-at-home orders using a\ndifference-in-differences design that accounts for unmeasured local variation\nin factors like health systems and demographics and for unmeasured temporal\nvariation in factors like national mitigation actions and access to tests.\nCompared to counties that did not implement stay-at-home orders, the results\nshow that the orders are associated with a 30.2 percent (11.0 to 45.2)\nreduction in weekly cases after one week, a 40.0 percent (23.4 to 53.0)\nreduction after two weeks, and a 48.6 percent (31.1 to 61.7) reduction after\nthree weeks. Stay-at-home orders are also associated with a 59.8 percent (18.3\nto 80.2) reduction in weekly fatalities after three weeks. These results\nsuggest that stay-at-home orders reduced confirmed cases by 390,000 (170,000 to\n680,000) and fatalities by 41,000 (27,000 to 59,000) within the first three\nweeks in localities where they were implemented.\n"
    },
    {
        "paper_id": 2004.06144,
        "authors": "Youssef Nassef",
        "title": "The PCL Framework: A strategic approach to comprehensive risk management\n  in response to climate change impacts",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The PCL framework provides a comprehensive climate risk management approach\ngrounded in the assessment of societal values of financial and non-financial\nloss tolerability. The framework optimizes response action across three main\nclusters, namely preemptive adaptation (P) or risk reduction, contingent\narrangements (C), and loss acceptance (L); without a predetermined hierarchy\nacross them. The PCL Framework aims at including the three clusters of outlay\nwithin a single continuum, and with the main policy outcome being a balanced\nportfolio of actions across the three clusters by way of an optimization\nmodule, such that the aggregate outlay is optimized in the long-term. It is\nproposed that the approach be applied separately for each hazard to which the\ntarget community is exposed. While it is currently applied to climate-related\nrisk management, the methodology can be repurposed for use in other contexts\nwhere societal buy-in is central.\n"
    },
    {
        "paper_id": 2004.062,
        "authors": "P. B. Lerner",
        "title": "Dual State-Space Model of Market Liquidity: The Chinese Experience\n  2009-2010",
        "comments": "Only the abstract has been changed from the previous version",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  This paper proposes and motivates a dynamical model of the Chinese stock\nmarket based on a linear regression in a dual state space connected to the\noriginal state space of correlations between the volume-at-price buckets by a\nFourier transform. We apply our model to the price migration of executed orders\nby the Chinese brokerages in 2009-2010. Regulatory brokerage tapes were used to\nconduct a natural experiment assuming that tapes correspond to randomly\nassigned, informed and uninformed traders. Our analysis demonstrated that\ncustomers' orders were tightly correlated--in a highly nonlinear sense of the\nneural networks--with the Chinese market sentiment index, significantly\ncorrelated with the stock returns and exhibited no correlation with the\nbellwether bond of the Bank of China. We did not notice any spike of\nilliquidity transmitting from the US Flash Crash in May 2010 to trading in\nChina.\n"
    },
    {
        "paper_id": 2004.0642,
        "authors": "Tomaso Aste",
        "title": "Stress testing and systemic risk measures using multivariate conditional\n  probability",
        "comments": "19 pages, 4 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The multivariate conditional probability distribution models the effects of a\nset of variables onto the statistical properties of another set of variables.\nIn the study of systemic risk in a financial system, the multivariate\nconditional probability distribution can be used for stress-testing by\nquantifying the propagation of losses from a set of `stressing' variables to\nanother set of `stressed' variables. In this paper I describe how to compute\nsuch conditional probability distributions for the vast family of multivariate\nelliptical distributions, and in particular for the multivariate Student-t and\nthe multivariate Normal distributions. Measures of stress impact and systemic\nrisk are proposed. An application to the US equity market illustrates the\npotentials of this approach.\n"
    },
    {
        "paper_id": 2004.06542,
        "authors": "Fei Liu, Aaron Page, Sarah A. Strode, Yasuko Yoshida, Sungyeon Choi,\n  Bo Zheng, Lok N. Lamsal, Can Li, Nickolay A. Krotkov, Henk Eskes, Ronald van\n  der A, Pepijn Veefkind, Pieternel Levelt, Joanna Joiner, Oliver P. Hauser",
        "title": "Abrupt declines in tropospheric nitrogen dioxide over China after the\n  outbreak of COVID-19",
        "comments": "29 pages, 11 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  China's policy interventions to reduce the spread of the coronavirus disease\n2019 have environmental and economic impacts. Tropospheric nitrogen dioxide\nindicates economic activities, as nitrogen dioxide is primarily emitted from\nfossil fuel consumption. Satellite measurements show a 48% drop in tropospheric\nnitrogen dioxide vertical column densities from the 20 days averaged before the\n2020 Lunar New Year to the 20 days averaged after. This is 20% larger than that\nfrom recent years. We relate to this reduction to two of the government's\nactions: the announcement of the first report in each province and the date of\na province's lockdown. Both actions are associated with nearly the same\nmagnitude of reductions. Our analysis offers insights into the unintended\nenvironmental and economic consequences through reduced economic activities.\n"
    },
    {
        "paper_id": 2004.06565,
        "authors": "Chirag Nagpal, Robert E. Tillman, Prashant Reddy, Manuela Veloso",
        "title": "Bayesian Consensus: Consensus Estimates from Miscalibrated Instruments\n  under Heteroscedastic Noise",
        "comments": null,
        "journal-ref": "NeurIPS 2019 Workshop on Robust AI in Financial Services: Data,\n  Fairness, Explainability, Trustworthiness and Privacy",
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We consider the problem of aggregating predictions or measurements from a set\nof human forecasters, models, sensors or other instruments which may be subject\nto bias or miscalibration and random heteroscedastic noise. We propose a\nBayesian consensus estimator that adjusts for miscalibration and noise and show\nthat this estimator is unbiased and asymptotically more efficient than naive\nalternatives. We further propose a Hierarchical Bayesian Model that leverages\nour proposed estimator and apply it to two real world forecasting challenges\nthat require consensus estimates from error prone individual estimates:\nforecasting influenza like illness (ILI) weekly percentages and forecasting\nannual earnings of public companies. We demonstrate that our approach is\neffective at mitigating bias and error and results in more accurate forecasts\nthan existing consensus models.\n"
    },
    {
        "paper_id": 2004.06586,
        "authors": "Carol Alexander, Xiaochun Meng, Wei Wei",
        "title": "Targetting Kollo Skewness with Random Orthogonal Matrix Simulation",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Modelling multivariate systems is important for many applications in\nengineering and operational research. The multivariate distributions under\nscrutiny usually have no analytic or closed form. Therefore their modelling\nemploys a numerical technique, typically multivariate simulations, which can\nhave very high dimensions. Random Orthogonal Matrix (ROM) simulation is a\nmethod that has gained some popularity because of the absence of certain\nsimulation errors. Specifically, it exactly matches a target mean, covariance\nmatrix and certain higher moments with every simulation. This paper extends the\nROM simulation algorithm presented by Hanke et al. (2017), hereafter referred\nto as HPSW, which matches the target mean, covariance matrix and Kollo skewness\nvector exactly. Our first contribution is to establish necessary and sufficient\nconditions for the HPSW algorithm to work. Our second contribution is to\ndevelop a general approach for constructing admissible values in the HPSW. Our\nthird theoretical contribution is to analyse the effect of multivariate sample\nconcatenation on the target Kollo skewness. Finally, we illustrate the\nextensions we develop here using a simulation study.\n"
    },
    {
        "paper_id": 2004.06626,
        "authors": "J. L. Subias",
        "title": "Potential in the Schrodinger equation: estimation from empirical data",
        "comments": "17 pages, 24 figures, LaTeX",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  A recent model for the stock market calculates future price distributions of\na stock as a wave function of a quantum particle confined in an infinite\npotential well. In such a model the question arose as to how to estimate the\nclassical potential needed for solving the Schrodinger equation. In the present\narticle the method used in that work for evaluating the potential is described,\nin the simplest version to implement, and more sophisticated implementations\nare suggested later.\n"
    },
    {
        "paper_id": 2004.06627,
        "authors": "Thibaut Th\\'eate, Damien Ernst",
        "title": "An Application of Deep Reinforcement Learning to Algorithmic Trading",
        "comments": "Preprint submitted to Elsevier journal \"Expert Systems with\n  Applications\"",
        "journal-ref": "Expert Systems with Applications, Volume 173, 1 July 2021, 114632",
        "doi": "10.1016/j.eswa.2021.114632",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This scientific research paper presents an innovative approach based on deep\nreinforcement learning (DRL) to solve the algorithmic trading problem of\ndetermining the optimal trading position at any point in time during a trading\nactivity in stock markets. It proposes a novel DRL trading strategy so as to\nmaximise the resulting Sharpe ratio performance indicator on a broad range of\nstock markets. Denominated the Trading Deep Q-Network algorithm (TDQN), this\nnew trading strategy is inspired from the popular DQN algorithm and\nsignificantly adapted to the specific algorithmic trading problem at hand. The\ntraining of the resulting reinforcement learning (RL) agent is entirely based\non the generation of artificial trajectories from a limited set of stock market\nhistorical data. In order to objectively assess the performance of trading\nstrategies, the research paper also proposes a novel, more rigorous performance\nassessment methodology. Following this new performance assessment approach,\npromising results are reported for the TDQN strategy.\n"
    },
    {
        "paper_id": 2004.06636,
        "authors": "Felix-Benedikt Liebrich, Marco Maggis, Gregor Svindland",
        "title": "Model Uncertainty: A Reverse Approach",
        "comments": "37 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Robust models in mathematical finance replace the classical single\nprobability measure by a sufficiently rich set of probability measures on the\nfuture states of the world to capture (Knightian) uncertainty about the \"right\"\nprobabilities of future events. If this set of measures is nondominated, many\nresults known from classical dominated frameworks cease to hold as\nprobabilistic and analytic tools crucial for the handling of dominated models\nfail. We investigate the consequences for the robust model when prominent\nresults from the mathematical finance literature are postulate. In this vein,\nwe categorise the Kreps-Yan property, robust variants of the\nBrannath-Schachermayer Bipolar Theorem, Fatou representations of risk measures,\nand aggregation in robust models.\n"
    },
    {
        "paper_id": 2004.06642,
        "authors": "Jim Samuel",
        "title": "Information Token Driven Machine Learning for Electronic Markets:\n  Performance Effects in Behavioral Financial Big Data Analytics",
        "comments": "Post-print, to be cited as (APA): Samuel, J. (2017). Information\n  Token Driven Machine Learning for Electronic Markets: Performance Effects in\n  Behavioral Financial Big Data Analytics. JISTEM-Journal of Information\n  Systems and Technology Management, 14(3), 371-383",
        "journal-ref": "JISTEM - Journal of Information Systems and Technology Management,\n  2017, vol.14 no.3, On-line version ISSN 1807-1775",
        "doi": "10.4301/s1807-17752017000300005",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Conjunct with the universal acceleration in information growth, financial\nservices have been immersed in an evolution of information dynamics. It is not\njust the dramatic increase in volumes of data, but the speed, the complexity\nand the unpredictability of big-data phenomena that have compounded the\nchallenges faced by researchers and practitioners in financial services. Math,\nstatistics and technology have been leveraged creatively to create analytical\nsolutions. Given the many unique characteristics of financial bid data (FBD) it\nis necessary to gain insights into strategies and models that can be used to\ncreate FBD specific solutions. Behavioral finance data, a subset of FBD, is\nseeing exponential growth and this presents an unprecedented opportunity to\nstudy behavioral finance employing big data analytics methodologies. The\npresent study maps machine learning (ML) techniques and behavioral finance\ncategories to explore the potential for using ML techniques to address\nbehavioral aspects in FBD. The ontological feasibility of such an approach is\npresented and the primary purpose of this study is propositioned- ML based\nbehavioral models can effectively estimate performance in FBD. A simple machine\nlearning algorithm is successfully employed to study behavioral performance in\nan artificial stock market to validate the propositions.\n  Keywords: Information; Big Data; Electronic Markets; Analytics; Behavior\n"
    },
    {
        "paper_id": 2004.06667,
        "authors": "Jos\\'e Moran and Antoine Fosset and Michael Benzaquen and\n  Jean-Philippe Bouchaud",
        "title": "Schr\\\"odinger's ants: A continuous description of Kirman's recruitment\n  model",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We show how the approach to equilibrium in Kirman's ants model can be fully\ncharacterized in terms of the spectrum of a Schr\\\"odinger equation with a\nP\\\"oschl-Teller ($\\tan^2$) potential. Among other interesting properties, we\nhave found that in the bimodal phase where ants visit mostly one food site at a\ntime, the switch time between the two sources only depends on the ``spontaneous\nconversion\" rate and not on the recruitment rate. More complicated correlation\nfunctions can be computed exactly, and involve higher and higher eigenvalues\nand eigenfunctions of the Schr\\\"odinger operator, which can be expressed in\nterms of hypergeometric functions.\n"
    },
    {
        "paper_id": 2004.06676,
        "authors": "Erick Trevi\\~no Aguilar",
        "title": "The interdependency structure in the Mexican stock exchange: A network\n  approach",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Our goal in this paper is to study and characterize the interdependency\nstructure of the Mexican Stock Exchange (mainly stocks from BMV) in the period\n2000-2019 and provide visualizations which in a one shot provide a big-picture\npanorama. To this end, we estimate correlation/concentration matrices from\ndifferent models and then compute metrics from network theory including\neigencentralities and network modularity\n"
    },
    {
        "paper_id": 2004.0668,
        "authors": "Andrew Lesniewski",
        "title": "Epidemic control via stochastic optimal control",
        "comments": "30 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We study the problem of optimal control of the stochastic SIR model. Models\nof this type are used in mathematical epidemiology to capture the time\nevolution of highly infectious diseases such as COVID-19. Our approach relies\non reformulating the Hamilton-Jacobi-Bellman equation as a stochastic minimum\nprinciple. This results in a system of forward backward stochastic differential\nequations, which is amenable to numerical solution via Monte Carlo simulations.\nWe present a number of numerical solutions of the system under a variety of\nscenarios.\n"
    },
    {
        "paper_id": 2004.06759,
        "authors": "R. Maria del Rio-Chanona, Penny Mealy, Anton Pichler, Francois Lafond,\n  Doyne Farmer",
        "title": "Supply and demand shocks in the COVID-19 pandemic: An industry and\n  occupation perspective",
        "comments": null,
        "journal-ref": null,
        "doi": "10.1093/oxrep/graa033",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We provide quantitative predictions of first order supply and demand shocks\nfor the U.S. economy associated with the COVID-19 pandemic at the level of\nindividual occupations and industries. To analyze the supply shock, we classify\nindustries as essential or non-essential and construct a Remote Labor Index,\nwhich measures the ability of different occupations to work from home. Demand\nshocks are based on a study of the likely effect of a severe influenza epidemic\ndeveloped by the US Congressional Budget Office. Compared to the pre-COVID\nperiod, these shocks would threaten around 22% of the US economy's GDP,\njeopardise 24% of jobs and reduce total wage income by 17%. At the industry\nlevel, sectors such as transport are likely to have output constrained by\ndemand shocks, while sectors relating to manufacturing, mining and services are\nmore likely to be constrained by supply shocks. Entertainment, restaurants and\ntourism face large supply and demand shocks. At the occupation level, we show\nthat high-wage occupations are relatively immune from adverse supply and\ndemand-side shocks, while low-wage occupations are much more vulnerable. We\nshould emphasize that our results are only first-order shocks -- we expect them\nto be substantially amplified by feedback effects in the production network.\n"
    },
    {
        "paper_id": 2004.06786,
        "authors": "Piergiacomo Sabino",
        "title": "Exact Simulation of Variance Gamma related OU processes: Application to\n  the Pricing of Energy Derivatives",
        "comments": "17 pages, 2 figure, 4 tables",
        "journal-ref": "Applied Mathematical Finance,Volume 27, 2020 - Issue 3",
        "doi": "10.1080/1350486X.2020.1813040",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this study we define a three-step procedure to relate the\nself-decomposability of the stationary law of a generalized Ornstein-Uhlenbeck\nprocess to the law of the increments of such processes. Based on this procedure\nand the results of Qu et al. (2019), we derive the exact simulation, without\nnumerical inversion, of the skeleton of a Variance Gamma, and of a symmetric\nVariance Gamma driven Ornstein-Uhlenbeck process. Extensive numerical\nexperiments are reported to demonstrate the accuracy and efficiency of our\nalgorithms. These results are instrumental to simulate the spot price dynamics\nin energy markets and to price Asian options and gas storages by Monte Carlo\nsimulations in a framework similar to the one discussed in Cummins et al.\n(2017, 2018).\n"
    },
    {
        "paper_id": 2004.0688,
        "authors": "Benjamin Avanzi, Gregory Clive Taylor, Phuong Anh Vu, and Bernard Wong",
        "title": "A multivariate evolutionary generalised linear model framework with\n  adaptive estimation for claims reserving",
        "comments": "Accepted for publication in Insurance: Mathematics and Economics",
        "journal-ref": "Insurance: Mathematics and Economics, Volume 93, July 2020, Pages\n  50-71",
        "doi": "10.1016/j.insmatheco.2020.04.007",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper, we develop a multivariate evolutionary generalised linear\nmodel (GLM) framework for claims reserving, which allows for dynamic features\nof claims activity in conjunction with dependency across business lines to\naccurately assess claims reserves. We extend the traditional GLM reserving\nframework on two fronts: GLM fixed factors are allowed to evolve in a recursive\nmanner, and dependence is incorporated in the specification of these factors\nusing a common shock approach.\n  We consider factors that evolve across accident years in conjunction with\nfactors that evolve across calendar years. This two-dimensional evolution of\nfactors is unconventional as a traditional evolutionary model typically\nconsiders the evolution in one single time dimension. This creates challenges\nfor the estimation process, which we tackle in this paper. We develop the\nformulation of a particle filtering algorithm with parameter learning\nprocedure. This is an adaptive estimation approach which updates evolving\nfactors of the framework recursively over time.\n  We implement and illustrate our model with a simulated data set, as well as a\nset of real data from a Canadian insurer.\n"
    },
    {
        "paper_id": 2004.06982,
        "authors": "Maria B. Chiarolla, Tiziano De Angelis, Gabriele Stabile",
        "title": "An analytical study of participating policies with minimum rate\n  guarantee and surrender option",
        "comments": "38 pages, 4 figures, 1 table; accepted for publication in Finance and\n  Stochastics",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We perform a detailed theoretical study of the value of a class of\nparticipating policies with four key features: $(i)$ the policyholder is\nguaranteed a minimum interest rate on the policy reserve; $(ii)$ the contract\ncan be terminated by the holder at any time until maturity (surrender option);\n$(iii)$ at the maturity (or upon surrender) a bonus can be credited to the\nholder if the portfolio backing the policy outperforms the current policy\nreserve; $(iv)$ due to solvency requirements the contract ends if the value of\nthe underlying portfolio of assets falls below the policy reserve.\n  Our analysis is probabilistic and it relies on optimal stopping and free\nboundary theory. We find a structure of the optimal surrender strategy which\nwas undetected by previous (mostly numerical) studies on the same topic.\nOptimal surrender of the contract is triggered by two `stop-loss' boundaries\nand by a `too-good-to-persist' boundary (in the language of \\cite{EV20}).\nFinancial implications of this strategy are discussed in detail and supported\nby extensive numerical experiments.\n"
    },
    {
        "paper_id": 2004.06985,
        "authors": "Jonathan Sadighian",
        "title": "Extending Deep Reinforcement Learning Frameworks in Cryptocurrency\n  Market Making",
        "comments": "16 pages, 3 figures, 5 tables, 23 equations",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  There has been a recent surge in interest in the application of artificial\nintelligence to automated trading. Reinforcement learning has been applied to\nsingle- and multi-instrument use cases, such as market making or portfolio\nmanagement. This paper proposes a new approach to framing cryptocurrency market\nmaking as a reinforcement learning challenge by introducing an event-based\nenvironment wherein an event is defined as a change in price greater or less\nthan a given threshold, as opposed to by tick or time-based events (e.g., every\nminute, hour, day, etc.). Two policy-based agents are trained to learn a market\nmaking trading strategy using eight days of training data and evaluate their\nperformance using 30 days of testing data. Limit order book data recorded from\nBitmex exchange is used to validate this approach, which demonstrates improved\nprofit and stability compared to a time-based approach for both agents when\nusing a simple multi-layer perceptron neural network for function approximation\nand seven different reward functions.\n"
    },
    {
        "paper_id": 2004.0729,
        "authors": "Lorenzo Lucchini, Laura Alessandretti, Bruno Lepri, Angela Gallo, and\n  Andrea Baronchelli",
        "title": "From code to market: Network of developers and correlated returns of\n  cryptocurrencies",
        "comments": null,
        "journal-ref": "Science Advances, 16 Dec 2020: Vol. 6, no. 51, eabd2204",
        "doi": "10.1126/sciadv.abd2204",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  \"Code is law\" is the funding principle of cryptocurrencies. The security,\ntransferability, availability and other properties of a crypto-asset are\ndetermined by the code through which it is created. If code is open source, as\nit happens for most cryptocurrencies, this principle would prevent\nmanipulations and grant transparency to users and traders. However, this\napproach considers cryptocurrencies as isolated entities thus neglecting\npossible connections between them. Here, we show that 4% of developers\ncontribute to the code of more than one cryptocurrency and that the market\nreflects these cross-asset dependencies. In particular, we reveal that the\nfirst coding event linking two cryptocurrencies through a common developer\nleads to the synchronisation of their returns in the following months. Our\nresults identify a clear link between the collaborative development of\ncryptocurrencies and their market behaviour. More broadly, our work reveals a\nso-far overlooked systemic dimension for the transparency of code-based\necosystems and we anticipate it will be of interest to researchers, investors\nand regulators.\n"
    },
    {
        "paper_id": 2004.07571,
        "authors": "Kirill S. Glavatskiy, Mikhail Prokopenko, Adrian Carro, Paul Ormerod,\n  Michael Harre",
        "title": "Explaining herding and volatility in the cyclical price dynamics of\n  urban housing markets using a large scale agent-based model",
        "comments": "18 pages, 3 figures, plus supplementary materials",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Urban housing markets, along with markets of other assets, universally\nexhibit periods of strong price increases followed by sharp corrections. The\nmechanisms generating such non-linearities are not yet well understood. We\ndevelop an agent-based model populated by a large number of heterogeneous\nhouseholds. The agents' behavior is compatible with economic rationality, with\nthe trend-following behavior found to be essential in replicating market\ndynamics. The model is calibrated using several large and distributed datasets\nof the Greater Sydney region (demographic, economic and financial) across three\nspecific and diverse periods since 2006. The model is not only capable of\nexplaining price dynamics during these periods, but also reproduces the novel\nbehavior actually observed immediately prior to the market peak in 2017, namely\na sharp increase in the variability of prices. This novel behavior is related\nto a combination of trend-following aptitude of the household agents (rational\nherding) and their propensity to borrow.\n"
    },
    {
        "paper_id": 2004.07612,
        "authors": "Peng Yue (ECUST), Yaodong Fan (UTS), Jonathan A. Batten (UUM),\n  Wei-Xing Zhou (ECUST)",
        "title": "Information transfer between stock market sectors: A comparison between\n  the USA and China",
        "comments": "12 pages including 8 figures",
        "journal-ref": "Entropy 22 (2), 194 (2020)",
        "doi": "10.3390/e22020194",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Information diffusion within financial markets plays a crucial role in the\nprocess of price formation and the propagation of sentiment and risk. We\nperform a comparative analysis of information transfer between industry sectors\nof the Chinese and the USA stock markets, using daily sector indices for the\nperiod from 2000 to 2017. The information flow from one sector to another is\nmeasured by the transfer entropy of the daily returns of the two sector\nindices. We find that the most active sector in information exchange (i.e., the\nlargest total information inflow and outflow) is the {\\textit{non-bank\nfinancial}} sector in the Chinese market and the {\\textit{technology}} sector\nin the USA market. This is consistent with the role of the non-bank sector in\ncorporate financing in China and the impact of technological innovation in the\nUSA. In each market, the most active sector is also the largest information\nsink that has the largest information inflow (i.e., inflow minus outflow). In\ncontrast, we identify that the main information source is the {\\textit{bank}}\nsector in the Chinese market and the {\\textit{energy}} sector in the USA\nmarket. In the case of China, this is due to the importance of net bank lending\nas a signal of corporate activity and the role of energy pricing in affecting\ncorporate profitability. There are sectors such as the {\\textit{real estate}}\nsector that could be an information sink in one market but an information\nsource in the other, showing the complex behavior of different markets.\nOverall, these findings show that stock markets are more synchronized, or\nordered, during periods of turmoil than during periods of stability.\n"
    },
    {
        "paper_id": 2004.07736,
        "authors": "Sandrine G\\\"umbel, Thorsten Schmidt",
        "title": "Machine learning for multiple yield curve markets: fast calibration in\n  the Gaussian affine framework",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Calibration is a highly challenging task, in particular in multiple yield\ncurve markets. This paper is a first attempt to study the chances and\nchallenges of the application of machine learning techniques for this. We\nemploy Gaussian process regression, a machine learning methodology having many\nsimilarities with extended Kalman filtering - a technique which has been\napplied many times to interest rate markets and term structure models.\n  We find very good results for the single curve markets and many challenges\nfor the multi curve markets in a Vasicek framework. The Gaussian process\nregression is implemented with the Adam optimizer and the non-linear conjugate\ngradient method, where the latter performs best. We also point towards future\nresearch.\n"
    },
    {
        "paper_id": 2004.07814,
        "authors": "Tom\\'a\\v{s} Evan and Vladim\\'ir Hol\\'y",
        "title": "Economic Conditions for Innovation: Private vs. Public Sector",
        "comments": null,
        "journal-ref": "Evan, T. & Hol\\'y, V. (2021). Economic Conditions for Innovation:\n  Private vs. Public Sector. Socio-Economic Planning Sciences, 76, 100966",
        "doi": "10.1016/j.seps.2020.100966",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The Hicks induced innovation hypothesis states that a price increase of a\nproduction factor is a spur to invention. We propose an alternative hypothesis\nrestating that a spur to invention require not only an increase of one factor\nbut also a decrease of at least one other factor to offset the companies' cost.\nWe illustrate the need for our alternative hypothesis in a historical example\nof the industrial revolution in the United Kingdom. Furthermore, we\neconometrically evaluate both hypotheses in a case study of research and\ndevelopment (R&D) in 29 OECD countries from 2003 to 2017. Specifically, we\ninvestigate dependence of investments to R&D on economic environment\nrepresented by average wages and oil prices using panel regression. We find\nthat our alternative hypothesis is supported for R&D funded and/or performed by\nbusiness enterprises while the original Hicks hypothesis holds for R&D funded\nby the government and R&D performed by universities. Our results reflect that\nbusiness sector is significantly influenced by market conditions, unlike the\ngovernment and higher education sectors.\n"
    },
    {
        "paper_id": 2004.07827,
        "authors": "Pietro Battiston, Simona Gamba",
        "title": "COVID-19: $R_0$ is lower where outbreak is larger",
        "comments": "Data and code are available upon request",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We use daily data from Lombardy, the Italian region most affected by the\nCOVID-19 outbreak, to calibrate a SIR model individually on each municipality.\nThese are all covered by the same health system and, in the post-lockdown phase\nwe focus on, all subject to the same social distancing regulations. We find\nthat municipalities with a higher number of cases at the beginning of the\nperiod analyzed have a lower rate of diffusion, which cannot be imputed to herd\nimmunity. In particular, there is a robust and strongly significant negative\ncorrelation between the estimated basic reproduction number ($R_0$) and the\ninitial outbreak size, in contrast with the role of $R_0$ as a \\emph{predictor}\nof outbreak size. We explore different possible explanations for this\nphenomenon and conclude that a higher number of cases causes changes of\nbehavior, such as a more strict adoption of social distancing measures among\nthe population, that reduce the spread. This result calls for a transparent,\nreal-time distribution of detailed epidemiological data, as such data affects\nthe behavior of populations in areas affected by the outbreak.\n"
    },
    {
        "paper_id": 2004.07947,
        "authors": "Viktor Stojkoski, Zoran Utkovski, Petar Jolakoski, Dragan Tevdovski\n  and Ljupco Kocarev",
        "title": "Correlates of the country differences in the infection and mortality\n  rates during the first wave of the COVID-19 pandemic: Evidence from Bayesian\n  model averaging",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In the initial wave of the COVID-19 pandemic we observed great discrepancies\nin both infection and mortality rates between countries. Besides the biological\nand epidemiological factors, a multitude of social and economic criteria also\ninfluence the extent to which these discrepancies appear. Consequently, there\nis an active debate regarding the critical socio-economic and health factors\nthat correlate with the infection and mortality rates outcome of the pandemic.\nHere, we leverage Bayesian model averaging techniques and country level data to\ninvestigate the potential of 28 variables, describing a diverse set of health\nand socio-economic characteristics, in being correlates of the final number of\ninfections and deaths during the first wave of the coronavirus pandemic. We\nshow that only few variables are able to robustly correlate with these\noutcomes. To understand the relationship between the potential correlates in\nexplaining the infection and death rates, we create a Jointness Space. Using\nthis space, we conclude that the extent to which each variable is able to\nprovide a credible explanation for the COVID-19 infections/mortality outcome\nvaries between countries because of their heterogeneous features.\n"
    },
    {
        "paper_id": 2004.08124,
        "authors": "Linlin Tian, Lihua Bai",
        "title": "Minimizing the Ruin Probability under the Sparre Andersen Model",
        "comments": "17 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper, we consider the problem of minimizing the ruin probability of\nan insurance company in which the surplus process follows the Sparre Andersen\nmodel. Similar to Bai et al. \\cite{bai2017optimal}, we recast this problem in a\nMarkovian framework by adding another dimension representing the time elapsed\nsince the last claim. After Markovization, We investigate the regularity\nproperties of the value function and state the dynamic programming principle.\nFurthermore, we show that the value function is the unique constrained\nviscosity solution to the associated Hamilton-Jacobi-Bellman equation. It\nshould be noted that there is no discount factor in our paper, which makes it\ntricky to prove the uniqueness. To overcome this difficulty, we construct the\nstrict viscosity supersolution. Then instead of comparing the usual viscosity\nsupersolution and subsolution, we compare the supersolution and the strict\nsubsolution. Eventually we show that all viscosity subsolution is less than the\nsupersolution.\n"
    },
    {
        "paper_id": 2004.08167,
        "authors": "Charles Bertucci (1), Louis Bertucci (2 and 3), Jean-Michel Lasry (4),\n  Pierre-Louis Lions (4 and 5) ((1) CMAP, Ecole Polytechnique, Palaiseau,\n  France, (2) Institut Louis Bachelier, Paris, France, (3) Haas School of\n  Business, UC Berkeley, Berkeley, California, (4) Universit\\'e Paris-Dauphine,\n  PSL Research University, CEREMADE, Paris, France, (5) Coll\\`ege de France,\n  Paris, France)",
        "title": "Mean Field Game Approach to Bitcoin Mining",
        "comments": "35 pages, 3 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We present an analysis of the Proof-of-Work consensus algorithm, used on the\nBitcoin blockchain, using a Mean Field Game framework. Using a master equation,\nwe provide an equilibrium characterization of the total computational power\ndevoted to mining the blockchain (hashrate). From a simple setting we show how\nthe master equation approach allows us to enrich the model by relaxing most of\nthe simplifying assumptions. The essential structure of the game is preserved\nacross all the enrichments. In deterministic settings, the hashrate ultimately\nreaches a steady state in which it increases at the rate of technological\nprogress. In stochastic settings, there exists a target for the hashrate for\nevery possible random state. As a consequence, we show that in equilibrium the\nsecurity of the underlying blockchain is either $i)$ constant, or $ii)$\nincreases with the demand for the underlying cryptocurrency.\n"
    },
    {
        "paper_id": 2004.08204,
        "authors": "Tam Tran-The",
        "title": "Modeling Institutional Credit Risk with Financial News",
        "comments": "Accepted to the AAAI-20 Workshop on Knowledge Discovery from\n  Unstructured Data in Financial Services",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Credit risk management, the practice of mitigating losses by understanding\nthe adequacy of a borrower's capital and loan loss reserves, has long been\nimperative to any financial institution's long-term sustainability and growth.\nMassMutual is no exception. The company is keen on effectively monitoring\ndowngrade risk, or the risk associated with the event when credit rating of a\ncompany deteriorates. Current work in downgrade risk modeling depends on\nmultiple variations of quantitative measures provided by third-party rating\nagencies and risk management consultancy companies. As these structured\nnumerical data become increasingly commoditized among institutional investors,\nthere has been a wide push into using alternative sources of data, such as\nfinancial news, earnings call transcripts, or social media content, to possibly\ngain a competitive edge in the industry. The volume of qualitative information\nor unstructured text data has exploded in the past decades and is now available\nfor due diligence to supplement quantitative measures of credit risk. This\npaper proposes a predictive downgrade model using solely news data represented\nby neural network embeddings. The model standalone achieves an Area Under the\nReceiver Operating Characteristic Curve (AUC) of more than 80 percent. The\noutput probability from this news model, as an additional feature, improves the\nperformance of our benchmark model using only quantitative measures by more\nthan 5 percent in terms of both AUC and recall rate. A qualitative evaluation\nalso indicates that news articles related to our predicted downgrade events are\nspecially relevant and high-quality in our business context.\n"
    },
    {
        "paper_id": 2004.0824,
        "authors": "Samudra Dasgupta, Kathleen E. Hamilton, and Arnab Banerjee",
        "title": "Characterizing the memory capacity of transmon qubit reservoirs",
        "comments": "Published in Proceedings of IEEE Quantum Computing and Engineering\n  Conference, Denver, Colorado, USA, 2022 (https://qce.quantum.ieee.org/2022/)",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Quantum Reservoir Computing (QRC) exploits the dynamics of quantum ensemble\nsystems for machine learning. Numerical experiments show that quantum systems\nconsisting of 5-7 qubits possess computational capabilities comparable to\nconventional recurrent neural networks of 100 to 500 nodes. Unlike traditional\nneural networks, we do not understand the guiding principles of reservoir\ndesign for high-performance information processing. Understanding the memory\ncapacity of quantum reservoirs continues to be an open question. In this study,\nwe focus on the task of characterizing the memory capacity of quantum\nreservoirs built using transmon devices provided by IBM. Our hybrid reservoir\nachieved a Normalized Mean Square Error (NMSE) of 6x10^{-4} which is comparable\nto recent benchmarks. The Memory Capacity characterization of a n-qubit\nreservoir showed a systematic variation with the complexity of the topology and\nexhibited a peak for the configuration with n-1 self-loops. Such a peak\nprovides a basis for selecting the optimal design for forecasting tasks.\n"
    },
    {
        "paper_id": 2004.0829,
        "authors": "Anastasia Bugaenko",
        "title": "Empirical Study of Market Impact Conditional on Order-Flow Imbalance",
        "comments": "This copy of the research does not include the source code. Please\n  contact the author for reference to the source code",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this research, we have empirically investigated the key drivers affecting\nliquidity in equity markets. We illustrated how theoretical models, such as\nKyle's model, of agents' interplay in the financial markets, are aligned with\nthe phenomena observed in publicly available trades and quotes data.\nSpecifically, we confirmed that for small signed order-flows, the price impact\ngrows linearly with increase in the order-flow imbalance. We have, further,\nimplemented a machine learning algorithm to forecast market impact given a\nsigned order-flow. Our findings suggest that machine learning models can be\nused in estimation of financial variables; and predictive accuracy of such\nlearning algorithms can surpass the performance of traditional statistical\napproaches.\n  Understanding the determinants of price impact is crucial for several\nreasons. From a theoretical stance, modelling the impact provides a statistical\nmeasure of liquidity. Practitioners adopt impact models as a pre-trade tool to\nestimate expected transaction costs and optimize the execution of their\nstrategies. This further serves as a post-trade valuation benchmark as\nsuboptimal execution can significantly deteriorate a portfolio performance.\n  More broadly, the price impact reflects the balance of liquidity across\nmarkets. This is of central importance to regulators as it provides an\nall-encompassing explanation of the correlation between market design and\nsystemic risk, enabling regulators to design more stable and efficient markets.\n"
    },
    {
        "paper_id": 2004.08504,
        "authors": "Eric Friedman, John Friedman, Simon Johnson, Adam Landsberg",
        "title": "Transitioning out of the Coronavirus Lockdown: A Framework for\n  Zone-Based Social Distancing",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In the face of elevated pandemic risk, is it necessary to completely lock\ndown the population, imposing extreme social distancing? Canonical\nepidemiological models suggest this may be unavoidable for months at a time,\ndespite the heavy social and human cost of physically isolating people.\nAlternatively, people could retreat into socially or economically defined\ndefensive zones, with more interactions inside their zone than across zones.\nStarting from a complete lockdown, zones could facilitate responsible reopening\nof education, government, and firms, as a well-implemented structure can\ndramatically slow the diffusion of the disease. This paper provides a framework\nfor understanding and evaluating the effectiveness of zones for social\ndistancing.\n"
    },
    {
        "paper_id": 2004.08533,
        "authors": "Tanmay Sen, Ritwik Bhattacharya, Biswabrata Pradhan and Yogesh Mani\n  Tripathi",
        "title": "Determination of Bayesian optimal warranty length under Type-II unified\n  hybrid censoring scheme",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Determination of an appropriate warranty length for the lifetime of the\nproduct is an important issue to the manufacturer. In this article, optimal\nwarranty length of the product for the combined free replacement and the\npro-rata warranty policy is computed based on the Type-II unified hybrid\ncensored data. A non-linear pro-rata warranty policy is proposed in this\ncontext. The optimal warranty length is obtained by maximizing an expected\nutility function. The expectation is taken with respect to the posterior\npredictive model for the time-to-failure data. It is observed that the\nnon-linear pro-rata warranty policy gives a larger warranty length with maximum\nprofit as compared to linear warranty policy. Finally, a real-data set is\nanalyzed in order to illustrate the advantage of using non-linear pro-rata\nwarranty policy.\n"
    },
    {
        "paper_id": 2004.0855,
        "authors": "Avishek Bhandari and Bandi Kamaiah",
        "title": "Long memory in select stock returns using an alternative wavelet\n  log-scale alignment approach",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This study investigates the efficiency of some select stock markets. Using an\nimproved wavelet estimator of long range dependence, we show evidence of long\nmemory in the stock returns of some emerging Asian economies. However,\ndeveloped markets of Europe and the United States did not exhibit long memory\nthereby confirming the efficiency of developed stock markets. On the other\nhand, emerging Asian markets are found to be less efficient as long memory is\nmore pronounced in these markets.\n"
    },
    {
        "paper_id": 2004.0865,
        "authors": "Fabien Le Floc'h",
        "title": "An arbitrage-free interpolation of class $C^2$ for option prices",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  This paper presents simple formulae for the local variance gamma model of\nCarr and Nadtochiy, extended with a piecewise-linear local variance function.\nThe new formulae allow to calibrate the model efficiently to market option\nquotes. On a small set of quotes, exact calibration is achieved under one\nmillisecond. This effectively results in an arbitrage-free interpolation of\nclass $C^2$. The paper proposes a good regularization when the quotes are\nnoisy. Finally, it puts in evidence an issue of the model at-the-money, which\nis also present in the related one-step finite difference technique of\nAndreasen and Huge, and gives two solutions for it.\n"
    },
    {
        "paper_id": 2004.08759,
        "authors": "Peng Yue (ECUST), Qing Cai, Wanfeng Yan (Zhicang Tech) and Wei-Xing\n  Zhou (ECUST)",
        "title": "Information flow networks of Chinese stock market sectors",
        "comments": "12 pages including 9 figures",
        "journal-ref": "IEEE Access 8 (1), 13066-13077 (2020)",
        "doi": "10.1109/ACCESS.2020.2966278",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Transfer entropy measures the strength and direction of information flow\nbetween different time series. We study the information flow networks of the\nChinese stock market and identify important sectors and information flow paths.\nThis paper uses the daily closing price data of the 28 level-1 sectors from\nShenyin \\& Wanguo Securities ranging from 2000 to 2017 to study the information\ntransmission between different sectors. We construct information flow networks\nwith the sectors as the nodes and the transfer entropy between them as the\ncorresponding edges. Then we adopt the maximum spanning arborescence (MSA) to\nextracting important information flows and the hierarchical structure of the\nnetworks. We find that, during the whole sample period, the \\textit{composite}\nsector is an information source of the whole stock market, while the\n\\textit{non-bank financial} sector is the information sink. We also find that\nthe \\textit{non-bank finance}, \\textit{bank}, \\textit{computer},\n\\textit{media}, \\textit{real estate}, \\textit{medical biology} and\n\\textit{non-ferrous metals} sectors appear as high-degree root nodes in the\noutgoing and incoming information flow MSAs. Especially, the \\textit{non-bank\nfinance} and \\textit{bank} sectors have significantly high degrees after 2008\nin the outgoing information flow networks. We uncover how stock market turmoils\naffect the structure of the MSAs. Finally, we reveal the specificity of\ninformation source and sink sectors and make a conclusion that the root node\nsector as the information sink of the incoming information flow networks.\nOverall, our analyses show that the structure of information flow networks\nchanges with time and the market exhibits a sector rotation phenomenon. Our\nwork has important implications for market participants and policy makers in\nmanaging market risks and controlling the contagion of risks.\n"
    },
    {
        "paper_id": 2004.08889,
        "authors": "Michael Roberts, Indranil SenGupta",
        "title": "Sequential hypothesis testing in machine learning, and crude oil price\n  jump size detection",
        "comments": "24 pages, 7 figures",
        "journal-ref": "Applied Mathematical Finance, 2020",
        "doi": "10.1080/1350486X.2020.1859943",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper we present a sequential hypothesis test for the detection of\ngeneral jump size distrubution. Infinitesimal generators for the corresponding\nlog-likelihood ratios are presented and analyzed. Bounds for infinitesimal\ngenerators in terms of super-solutions and sub-solutions are computed. This is\nshown to be implementable in relation to various classification problems for a\ncrude oil price data set. Machine and deep learning algorithms are implemented\nto extract a specific deterministic component from the crude oil data set, and\nthe deterministic component is implemented to improve the Barndorff-Nielsen and\nShephard model, a commonly used stochastic model for derivative and commodity\nmarket analysis.\n"
    },
    {
        "paper_id": 2004.08891,
        "authors": "Johannes Ruf, Weiguan Wang",
        "title": "Hedging with Linear Regressions and Neural Networks",
        "comments": "Forthcoming in the Journal of Business & Economic Statistics",
        "journal-ref": null,
        "doi": "10.1080/07350015.2021.1931241",
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  We study neural networks as nonparametric estimation tools for the hedging of\noptions. To this end, we design a network, named HedgeNet, that directly\noutputs a hedging strategy. This network is trained to minimise the hedging\nerror instead of the pricing error. Applied to end-of-day and tick prices of\nS&P 500 and Euro Stoxx 50 options, the network is able to reduce the mean\nsquared hedging error of the Black-Scholes benchmark significantly. However, a\nsimilar benefit arises by simple linear regressions that incorporate the\nleverage effect.\n"
    },
    {
        "paper_id": 2004.08917,
        "authors": "Stephan Leitner",
        "title": "On the dynamics emerging from pandemics and infodemics",
        "comments": "7 pages. Mind & Society (2020)",
        "journal-ref": null,
        "doi": "10.1007/s11299-020-00256-y",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This position paper discusses emerging behavioral, social, and economic\ndynamics related to the COVID-19 pandemic and puts particular emphasis on two\nemerging issues: First, delayed effects (or second strikes) of pandemics caused\nby dread risk effects are discussed whereby two factors which might influence\nthe existence of such effects are identified, namely the accessibility of\n(mis-)information and the effects of policy decisions on adaptive behavior.\nSecond, the issue of individual preparedness to hazardous events is discussed.\nAs events such as the COVID-19 pandemic unfolds complex behavioral patterns\nwhich are hard to predict, sophisticated models which account for behavioral,\nsocial, and economic dynamics are required to assess the effectivity and\nefficiency of decision-making.\n"
    },
    {
        "paper_id": 2004.09042,
        "authors": "Misha van Beek",
        "title": "Consistent Calibration of Economic Scenario Generators: The Case for\n  Conditional Simulation",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Economic Scenario Generators (ESGs) simulate economic and financial variables\nforward in time for risk management and asset allocation purposes. It is often\nnot feasible to calibrate the dynamics of all variables within the ESG to\nhistorical data alone. Calibration to forward-information such as future\nscenarios and return expectations is needed for stress testing and portfolio\noptimization, but no generally accepted methodology is available. This paper\nintroduces the Conditional Scenario Simulator, which is a framework for\nconsistently calibrating simulations and projections of economic and financial\nvariables both to historical data and forward-looking information. The\nframework can be viewed as a multi-period, multi-factor generalization of the\nBlack-Litterman model, and can embed a wide array of financial and\nmacroeconomic models. Two practical examples demonstrate this in a frequentist\nand Bayesian setting.\n"
    },
    {
        "paper_id": 2004.09087,
        "authors": "Matz Dahlberg, Per-Anders Edin, Erik Gr\\\"onqvist, Johan Lyhagen, John\n  \\\"Osth, Alexey Siretskiy, Marina Toger",
        "title": "Effects of the COVID-19 Pandemic on Population Mobility under Mild\n  Policies: Causal Evidence from Sweden",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Sweden has adopted far less restrictive social distancing policies than most\ncountries following the COVID-19 pandemic. This paper uses data on all mobile\nphone users, from one major Swedish mobile phone network, to examine the impact\nof the Coronavirus outbreak under the Swedish mild recommendations and\nrestrictions regime on individual mobility and if changes in geographical\nmobility vary over different socio-economic strata. Having access to data for\nJanuary-March in both 2019 and 2020 enables the estimation of causal effects of\nthe COVID-19 outbreak by adopting a Difference-in-Differences research design.\nThe paper reaches four main conclusions: (i) The daytime population in\nresidential areas increased significantly (64 percent average increase); (ii)\nThe daytime presence in industrial and commercial areas decreased significantly\n(33 percent average decrease); (iii) The distance individuals move from their\nhomes during a day was substantially reduced (38 percent decrease in the\nmaximum distance moved and 36 percent increase in share of individuals who move\nless than one kilometer from home); (iv) Similar reductions in mobility were\nfound for residents in areas with different socioeconomic and demographic\ncharacteristics. These results show that mild government policies can compel\npeople to adopt social distancing behavior.\n"
    },
    {
        "paper_id": 2004.09212,
        "authors": "Davide Lasi and Lukas Saul",
        "title": "A System Dynamics Model of Bitcoin: Mining as an Efficient Market and\n  the Possibility of \"Peak Hash\"",
        "comments": "17 pages, 11 figures. For associated model, see:\n  https://github.com/davidelasi/bitcoin",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The mining of bitcoin is modeled using system dynamics, showing that the past\nevolution of the network hash rate can be explained to a large extent by an\nefficient market hypothesis applied to the mining of blocks. The possibility of\na decrease in the network hash rate from the next halving event (May 2020) is\nexposed, implying that the network may be close to 'peak hash', if the price of\nbitcoin and the revenues from transaction fees will remain at approximately the\npresent level.\n"
    },
    {
        "paper_id": 2004.09225,
        "authors": "L\\'aszl\\'o Csat\\'o and D\\'ora Gr\\'eta Petr\\'oczy",
        "title": "Fairness in penalty shootouts: Is it worth using dynamic sequences?",
        "comments": "23 pages, 3 figures, 4 tables",
        "journal-ref": "Journal of Sports Sciences, 40(12): 1392-1398, 2022",
        "doi": "10.1080/02640414.2022.2081402",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The sequence of moves in a dynamic team tournament may distort the ex-ante\nwinning probabilities and harm efficiency. This paper compares seven soccer\npenalty shootout rules that determine the kicking order, from a theoretical\nperspective. Their fairness is evaluated in a reasonable model of First Mover\nAdvantage. We also discuss the probability of reaching the sudden death stage.\nIn the case of stationary scoring probabilities, dynamic mechanisms are not\nbetter than static rules. However, it is worth compensating the second-mover by\nmaking it the first-mover in the sudden death stage. Our work has the potential\nto impact decision-makers who can guarantee fairer outcomes in dynamic\ntournaments by a carefully chosen sequence of actions.\n"
    },
    {
        "paper_id": 2004.09418,
        "authors": "Chiara Perillo (1) and Stefano Battiston (1) ((1) University of\n  Zurich, Department of Banking and Finance, Zurich, Switzerland)",
        "title": "Real implications of Quantitative Easing in the euro area: a\n  complex-network perspective",
        "comments": "This is a pre-print of a contribution published in Cherifi C.,\n  Cherifi H., Karsai M., Musolesi M. (eds) Complex Networks & Their\n  Applications VI published by Springer International Publishing AG 2018. The\n  final authenticated version is available online at:\n  https://doi.org/10.1007/978-3-319-72150-7_94",
        "journal-ref": "Cherifi C., Cherifi H., Karsai M., Musolesi M. (eds) Complex\n  Networks & Their Applications VI. COMPLEX NETWORKS 2017. Studies in\n  Computational Intelligence, vol 689. Springer, Cham",
        "doi": "10.1007/978-3-319-72150-7_94",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The long-lasting socio-economic impact of the global financial crisis has\nquestioned the adequacy of traditional tools in explaining periods of financial\ndistress, as well as the adequacy of the existing policy response. In\nparticular, the effect of complex interconnections among financial institutions\non financial stability has been widely recognized. A recent debate focused on\nthe effects of unconventional policies aimed at achieving both price and\nfinancial stability. In particular, Quantitative Easing (QE, i.e., the\nlarge-scale asset purchase programme conducted by a central bank upon the\ncreation of new money) has been recently implemented by the European Central\nBank (ECB). In this context, two questions deserve more attention in the\nliterature. First, to what extent, by injecting liquidity, the QE may alter the\nbank-firm lending level and stimulate the real economy. Second, to what extent\nthe QE may also alter the pattern of intra-financial exposures among financial\nactors (including banks, investment funds, insurance corporations, and pension\nfunds) and what are the implications in terms of financial stability. Here, we\naddress these two questions by developing a methodology to map the\nmacro-network of financial exposures among institutional sectors across\nfinancial instruments (e.g., equity, bonds, and loans) and we illustrate our\napproach on recently available data (i.e., data on loans and private and public\nsecurities purchased within the QE). We then test the effect of the\nimplementation of ECB's QE on the time evolution of the financial linkages in\nthe macro-network of the euro area, as well as the effect on macroeconomic\nvariables, such as output and prices.\n"
    },
    {
        "paper_id": 2004.09421,
        "authors": "Fernando Delbianco, Federico Fioravanti and Fernando Tohm\\'e",
        "title": "The Impact of Birth Order on Behavior in Contact Team Sports: the\n  Evidence of Rugby Teams in Argentina",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Several studies have shown that birth order and the sex of siblings may have\nan influence on individual behavioral traits. In particular, it has been found\nthat second brothers (of older male siblings) tend to have more disciplinary\nproblems. If this is the case, this should also be shown in contact sports. To\nassess this hypothesis we use a data set from the South Rugby Union (URS) from\nBah\\'ia Blanca, Argentina, and information obtained by surveying more than four\nhundred players of that league. We find a statistically significant positive\nrelation between being a second-born male rugby player with an older male\nbrother and the number of yellow cards received.\n  \\textbf{Keywords:} Birth Order; Behavior; Contact Sports; Rugby.\n"
    },
    {
        "paper_id": 2004.09432,
        "authors": "Derek Singh, Shuzhong Zhang",
        "title": "Robust Arbitrage Conditions for Financial Markets",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper investigates arbitrage properties of financial markets under\ndistributional uncertainty using Wasserstein distance as the ambiguity measure.\nThe weak and strong forms of the classical arbitrage conditions are considered.\nA relaxation is introduced for which we coin the term statistical arbitrage.\nThe simpler dual formulations of the robust arbitrage conditions are derived. A\nnumber of interesting questions arise in this context. One question is: can we\ncompute a critical Wasserstein radius beyond which an arbitrage opportunity\nexists? What is the shape of the curve mapping the degree of ambiguity to\nstatistical arbitrage levels? Other questions arise regarding the structure of\nbest (worst) case distributions and optimal portfolios. Towards answering these\nquestions, some theory is developed and computational experiments are conducted\nfor specific problem instances. Finally some open questions and suggestions for\nfuture research are discussed.\n"
    },
    {
        "paper_id": 2004.09448,
        "authors": "Vishwas Kukreti, Hirdesh K. Pharasi, Priya Gupta, and Sunil Kumar",
        "title": "A perspective on correlation-based financial networks and entropy\n  measures",
        "comments": "10 pages, 2 figures. This paper is submitted to Frontiers in Physics,\n  Research Topic \"From Physics to Econophysics and Back: Methods and Insights\";\n  Topic Editor(s): Siew Ann Cheong, Takayuki Mizuno, Wei-Xing Zhou, Gabjin Oh,\n  Anirban Chakraborti, Damien Challet",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this brief review, we critically examine the recent work done on\ncorrelation-based networks in financial systems. The structure of empirical\ncorrelation matrices constructed from the financial market data changes as the\nindividual stock prices fluctuate with time, showing interesting evolutionary\npatterns, especially during critical events such as market crashes, bubbles,\netc. We show that the study of correlation-based networks and their evolution\nwith time is useful for extracting important information of the underlying\nmarket dynamics. We, also, present our perspective on the use of recently\ndeveloped entropy measures such as structural entropy and eigen-entropy for\ncontinuous monitoring of correlation-based networks.\n"
    },
    {
        "paper_id": 2004.09591,
        "authors": "Andrey Itkin and Dmitry Muravey",
        "title": "Semi-closed form prices of barrier options in the Hull-White model",
        "comments": "15 pages, 4 figures, 1 table",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper we derive semi-closed form prices of barrier (perhaps,\ntime-dependent) options for the Hull-White model, ie., where the underlying\nfollows a time-dependent OU process with a mean-reverting drift. Our approach\nis similar to that in (Carr and Itkin, 2020) where the method of generalized\nintegral transform is applied to pricing barrier options in the time-dependent\nOU model, but extends it to an infinite domain (which is an unsolved problem\nyet). Alternatively, we use the method of heat potentials for solving the same\nproblems. By semi-closed solution we mean that first, we need to solve\nnumerically a linear Volterra equation of the first kind, and then the option\nprice is represented as a one-dimensional integral. Our analysis shows that\ncomputationally our method is more efficient than the backward and even forward\nfinite difference methods (if one uses them to solve those problems), while\nproviding better accuracy and stability.\n"
    },
    {
        "paper_id": 2004.09835,
        "authors": "Jean-Philippe Bouchaud (CFM)",
        "title": "How Much Income Inequality Is Too Much?",
        "comments": "Working paper, 11 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We propose a highly schematic economic model in which, in some cases, wage\ninequalities lead to higher overall social welfare. This is due to the fact\nthat high earners can consume low productivity, non essential products, which\nallows everybody to remain employed even when the productivity of essential\ngoods is high and producing them does not require everybody to work. We derive\na relation between heterogeneities in technologies and the minimum Gini\ncoefficient required to maximize global welfare. Stronger inequalities appear\nto be economically unjustified. Our model may shed light on the role of\nnon-essential goods in the economy, a topical issue when thinking about the\npost-Covid-19 world.\n"
    },
    {
        "paper_id": 2004.09959,
        "authors": "Kerstin H\\\"otte, Anton Pichler, Fran\\c{c}ois Lafond",
        "title": "The rise of science in low-carbon energy technologies",
        "comments": null,
        "journal-ref": null,
        "doi": "10.1016/j.rser.2020.110654",
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Successfully combating climate change will require substantial technological\nimprovements in Low-Carbon Energy Technologies (LCETs), but designing efficient\nallocation of R\\&D budgets requires a better understanding of how LCETs rely on\nscientific knowledge. Using data covering almost all US patents and scientific\narticles that are cited by them over the past two centuries, we describe the\nevolution of knowledge bases of ten key LCETs and show how technological\ninterdependencies have changed over time. The composition of low-carbon energy\ninnovations shifted over time, from Hydro and Wind energy in the 19th and early\n20th century, to Nuclear fission after World War II, and more recently to Solar\nPV and back to Wind. In recent years, Solar PV, Nuclear fusion and Biofuels\n(including energy from waste) have 35-65\\% of their citations directed toward\nscientific papers, while this ratio is less than 10\\% for Wind, Solar thermal,\nHydro, Geothermal, and Nuclear fission. Over time, the share of patents citing\nscience and the share of citations that are to scientific papers has been\nincreasing for all technology types. The analysis of the scientific knowledge\nbase of each LCET reveals three fairly separate clusters, with nuclear energy\ntechnologies, Biofuels and Waste, and all the other LCETs. Our detailed\ndescription of knowledge requirements for each LCET helps to design of targeted\ninnovation policies.\n"
    },
    {
        "paper_id": 2004.09963,
        "authors": "Arjun Prakash, Nick James, Max Menzies, Gilad Francis",
        "title": "Structural clustering of volatility regimes for dynamic trading\n  strategies",
        "comments": "Accepted manuscript",
        "journal-ref": "Applied Mathematical Finance 28, 236-274 (2022)",
        "doi": "10.1080/1350486X.2021.2007146",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We develop a new method to find the number of volatility regimes in a\nnonstationary financial time series by applying unsupervised learning to its\nvolatility structure. We use change point detection to partition a time series\ninto locally stationary segments and then compute a distance matrix between\nsegment distributions. The segments are clustered into a learned number of\ndiscrete volatility regimes via an optimization routine. Using this framework,\nwe determine a volatility clustering structure for financial indices, large-cap\nequities, exchange-traded funds and currency pairs. Our method overcomes the\nrigid assumptions necessary to implement many parametric regime-switching\nmodels, while effectively distilling a time series into several characteristic\nbehaviours. Our results provide significant simplification of these time series\nand a strong descriptive analysis of prior behaviours of volatility. Finally,\nwe create and validate a dynamic trading strategy that learns the optimal match\nbetween the current distribution of a time series and its past regimes, thereby\nmaking online risk-avoidance decisions in the present.\n"
    },
    {
        "paper_id": 2004.10096,
        "authors": "Chenxu Li, Olivier Scaillet, Yiwen Shen",
        "title": "Wealth Effect on Portfolio Allocation in Incomplete Markets",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We develop a novel five-component decomposition of optimal dynamic portfolio\nchoice, which reveals the simultaneous impacts from market incompleteness and\nwealth-dependent utilities. Under the HARA utility and a nonrandom interest\nrate, we can explicitly solve for the optimal policy as a combination of a bond\nholding scheme and the corresponding simpler CRRA strategy. Under a stochastic\nvolatility model estimated on US equity data, we use closed-form solution to\ndemonstrate the sophisticated impacts from the wealth-dependent utilities,\nincluding cycle-dependence and hysteresis effect in optimal portfolio\nallocation, as well as a risk-return trade-off in investment performance.\n"
    },
    {
        "paper_id": 2004.10119,
        "authors": "Luigi Bellomarini, Marco Benedetti, Andrea Gentili, Rosario Laurendi,\n  Davide Magnanimi, Antonio Muci, Emanuel Sallinger",
        "title": "COVID-19 and Company Knowledge Graphs: Assessing Golden Powers and\n  Economic Impact of Selective Lockdown via AI Reasoning",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In the COVID-19 outbreak, governments have applied progressive restrictions\nto production activities, permitting only those that are considered strategic\nor that provide essential services. This is particularly apparent in countries\nthat have been stricken hard by the virus, with Italy being a major example.\nYet we know that companies are not just isolated entities: They organize\nthemselves into intricate shareholding structures --- forming company networks\n--- distributing decision power and dividends in sophisticated schemes for\nvarious purposes.\n  One tool from the Artificial Intelligence (AI) toolbox that is particularly\neffective to perform reasoning tasks on domains characterized by many entities\nhighly interconnected with one another is Knowledge Graphs (KG). In this work,\nwe present a visionary opinion and report on ongoing work about the application\nof Automated Reasoning and Knowledge Graph technology to address the impact of\nthe COVID-19 outbreak on the network of Italian companies and support the\napplication of legal instruments for the protection of strategic companies from\ntakeovers.\n"
    },
    {
        "paper_id": 2004.10178,
        "authors": "Pushpendu Ghosh, Ariel Neufeld, Jajati Keshari Sahoo",
        "title": "Forecasting directional movements of stock prices for intraday trading\n  using LSTM and random forests",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We employ both random forests and LSTM networks (more precisely CuDNNLSTM) as\ntraining methodologies to analyze their effectiveness in forecasting\nout-of-sample directional movements of constituent stocks of the S&P 500 from\nJanuary 1993 till December 2018 for intraday trading. We introduce a\nmulti-feature setting consisting not only of the returns with respect to the\nclosing prices, but also with respect to the opening prices and intraday\nreturns. As trading strategy, we use Krauss et al. (2017) and Fischer & Krauss\n(2018) as benchmark. On each trading day, we buy the 10 stocks with the highest\nprobability and sell short the 10 stocks with the lowest probability to\noutperform the market in terms of intraday returns -- all with equal monetary\nweight. Our empirical results show that the multi-feature setting provides a\ndaily return, prior to transaction costs, of 0.64% using LSTM networks, and\n0.54% using random forests. Hence we outperform the single-feature setting in\nFischer & Krauss (2018) and Krauss et al. (2017) consisting only of the daily\nreturns with respect to the closing prices, having corresponding daily returns\nof 0.41% and of 0.39% with respect to LSTM and random forests, respectively.\n"
    },
    {
        "paper_id": 2004.10318,
        "authors": "Wanling Qiu, Simon Rudkin, Pawel Dlotko",
        "title": "Refining Understanding of Corporate Failure through a Topological Data\n  Analysis Mapping of Altman's Z-Score Model",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Corporate failure resonates widely leaving practitioners searching for\nunderstanding of default risk. Managers seek to steer away from trouble, credit\nproviders to avoid risky loans and investors to mitigate losses. Applying\nTopological Data Analysis tools this paper explores whether failing firms from\nthe United States organise neatly along the five predictors of default proposed\nby the Z-score models. Firms are represented as a point cloud in a five\ndimensional space, one axis for each predictor. Visualising that cloud using\nBall Mapper reveals failing firms are not often neighbours. As new modelling\napproaches vie to better predict firm failure, often using black boxes to\ndeliver potentially over-fitting models, a timely reminder is sounded on the\nimportance of evidencing the identification process. Value is added to the\nunderstanding of where in the parameter space failure occurs, and how firms\nmight act to move away from financial distress. Further, lenders may find\nopportunity amongst subsets of firms that are traditionally considered to be in\ndanger of bankruptcy but actually sit in characteristic spaces where failure\nhas not occurred.\n"
    },
    {
        "paper_id": 2004.10324,
        "authors": "David Gershon, Alexander Lipton, Hagai Levine",
        "title": "Managing COVID-19 Pandemic without Destructing the Economy",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We analyze an approach to managing the COVID-19 pandemic without shutting\ndown the economy while staying within the capacity of the healthcare system. We\nbase our analysis on a detailed heterogeneous epidemiological model, which\ntakes into account different population groups and phases of the disease,\nincluding incubation, infection period, hospitalization, and treatment in the\nintensive care unit (ICU). We model the healthcare capacity as the total number\nof hospital and ICU beds for the whole country. We calibrate the model\nparameters to data reported in several recent research papers. For high- and\nlow-risk population groups, we calculate the number of total and intensive care\nhospitalizations, and deaths as functions of time. The main conclusion is that\ncountries, which enforce reasonable hygienic measures on time can avoid\nlockdowns throughout the pandemic provided that the number of spare ICU beds\nper million is above the threshold of about 100. In countries where the total\nnumber of ICU beds is below this threshold, a limited period quarantine to\nspecific high-risk groups of the population suffices. Furthermore, in the case\nof an inadequate capacity of the healthcare system, we incorporate a feedback\nloop and demonstrate that quantitative impact of the lack of ICU units on the\ndeath curve. In the case of inadequate ICU beds, full- and partial-quarantine\nscenarios outcomes are almost identical, making it unnecessary to shut down the\nwhole economy. We conclude that only a limited-time quarantine of the high-risk\ngroup might be necessary, while the rest of the economy can remain operational.\n"
    },
    {
        "paper_id": 2004.10537,
        "authors": "Dominik Martin, Philipp Spitzer, Niklas K\\\"uhl",
        "title": "A New Metric for Lumpy and Intermittent Demand Forecasts:\n  Stock-keeping-oriented Prediction Error Costs",
        "comments": "Proceedings of the 53rd Annual Hawaii International Conference on\n  System Sciences (HICSS-53), Grand Wailea, Maui, HI, January 7-10, 2020",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Forecasts of product demand are essential for short- and long-term\noptimization of logistics and production. Thus, the most accurate prediction\npossible is desirable. In order to optimally train predictive models, the\ndeviation of the forecast compared to the actual demand needs to be assessed by\na proper metric. However, if a metric does not represent the actual prediction\nerror, predictive models are insufficiently optimized and, consequently, will\nyield inaccurate predictions. The most common metrics such as MAPE or RMSE,\nhowever, are not suitable for the evaluation of forecasting errors, especially\nfor lumpy and intermittent demand patterns, as they do not sufficiently account\nfor, e.g., temporal shifts (prediction before or after actual demand) or\ncost-related aspects. Therefore, we propose a novel metric that, in addition to\nstatistical considerations, also addresses business aspects. Additionally, we\nevaluate the metric based on simulated and real demand time series from the\nautomotive aftermarket.\n"
    },
    {
        "paper_id": 2004.10548,
        "authors": "Alje van Dam, Andres Gomez-Lievano, Frank Neffke, Koen Frenken",
        "title": "An information-theoretic approach to the analysis of location and\n  co-location patterns",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We propose a statistical framework to quantify location and co-location\nassociations of economic activities using information-theoretic measures. We\nrelate the resulting measures to existing measures of revealed comparative\nadvantage, localization and specialization and show that they can all be seen\nas part of the same framework. Using a Bayesian approach, we provide measures\nof uncertainty of the estimated quantities. Furthermore, the\ninformation-theoretic approach can be readily extended to move beyond pairwise\nco-locations and instead capture multivariate associations. To illustrate the\nframework, we apply our measures to the co-location of occupations in US\ncities, showing the associations between different groups of occupations.\n"
    },
    {
        "paper_id": 2004.1056,
        "authors": "Kartikay Gupta and Niladri Chatterjee",
        "title": "Examining Lead-Lag Relationships In-Depth, With Focus On FX Market As\n  Covid-19 Crises Unfolds",
        "comments": "Suggestions are welcome. In the second version, a citation has been\n  updated on request from the corresponding author",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The lead-lag relationship plays a vital role in financial markets. It is the\nphenomenon where a certain price-series lags behind and partially replicates\nthe movement of leading time-series. The present research proposes a new\ntechnique which helps better identify the lead-lag relationship empirically.\nApart from better identifying the lead-lag path, the technique also gives a\nmeasure for adjudging closeness between financial time-series. Also, the\nproposed measure is closely related to correlation, and it uses Dynamic\nProgramming technique for finding the optimal lead-lag path. Further, it\nretains most of the properties of a metric, so much so, it is termed as loose\nmetric. Tests are performed on Synthetic Time Series (STS) with known lead-lag\nrelationship and comparisons are done with other state-of-the-art models on the\nbasis of significance and forecastability. The proposed technique gives the\nbest results in both the tests. It finds paths which are all statistically\nsignificant, and its forecasts are closest to the target values. Then, we use\nthe measure to study the topology evolution of the Foreign Exchange market, as\nthe COVID-19 pandemic unfolds. Here, we study the FX currency prices of 29\nprominent countries of the world. It is observed that as the crises unfold, all\nthe currencies become strongly interlinked to each other. Also, USA Dollar\nstarts playing even more central role in the FX market. Finally, we mention\nseveral other application areas of the proposed technique for designing\nintelligent systems.\n"
    },
    {
        "paper_id": 2004.10562,
        "authors": "Pedro V Hernandez Serrano, Amrapali Zaveri",
        "title": "Venturing the Definition of Green Energy Transition: A systematic\n  literature review",
        "comments": "16 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-sa/4.0/",
        "abstract": "  The issue of climate change has become increasingly noteworthy in the past\nyears, the transition towards a renewable energy system is a priority in the\ntransition to a sustainable society. In this document, we explore the\ndefinition of green energy transition, how it is reached, and what are the\ndriven factors to achieve it. To answer that firstly, we have conducted a\nliterature review discovering definitions from different disciplines, secondly,\ngathering the key factors that are drivers for energy transition, finally, an\nanalysis of the factors is conducted within the context of European Union data.\nPreliminary results have shown that household net income and governmental legal\nactions related to environmental issues are potential candidates to predict\nenergy transition within countries. With this research, we intend to spark new\nresearch directions in order to get a common social and scientific\nunderstanding of green energy transition.\n"
    },
    {
        "paper_id": 2004.10571,
        "authors": "Antoine Jacquier and Alexandre Pannier",
        "title": "Large and moderate deviations for stochastic Volterra systems",
        "comments": "39 pages",
        "journal-ref": "Stochastic Processes and their Applications, vol. 149, pp.\n  142-187, 2022",
        "doi": "10.1016/j.spa.2022.03.017",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We provide a unified treatment of pathwise Large and Moderate deviations\nprinciples for a general class of multidimensional stochastic Volterra\nequations with singular kernels, not necessarily of convolution form. Our\nmethodology is based on the weak convergence approach by Budhijara, Dupuis and\nEllis. We show in particular how this framework encompasses most rough\nvolatility models used in mathematical finance and generalises many recent\nresults in the literature.\n"
    },
    {
        "paper_id": 2004.10631,
        "authors": "Yi Cao",
        "title": "The new methods for equity fund selection and optimal portfolio\n  construction",
        "comments": "4 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We relook at the classic equity fund selection and portfolio construction\nproblems from a new perspective and propose an easy-to-implement framework to\ntackle the problem in practical investment. Rather than the conventional way by\nconstructing a long only portfolio from a big universe of stocks or macro\nfactors, we show how to produce a long-short portfolio from a smaller pool of\nstocks from mutual fund top holdings and generate impressive results. As these\nmethods are based on statistical evidence, we need closely monitoring the model\nvalidity, and prepare repair strategies.\n"
    },
    {
        "paper_id": 2004.10632,
        "authors": "Helder Rojas, Artem Logachov, Anatoly Yambartsev",
        "title": "Order book dynamics with liquidity fluctuations: limit theorems and\n  large deviations",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We propose a class of stochastic models for a dynamics of limit order book\nwith different type of liquidities. Within this class of models we study the\none where a spread decreases uniformly, belonging to the class of processes\nknown as a population processes with uniform catastrophes. The law of large\nnumbers (LLN), central limit theorem (CLT) and large deviations (LD) are proved\nfor our model with uniform catastrophes. Our results allow us to satisfactorily\nexplain the volatility and local trends in the prices, relevant empirical\ncharacteristics that are observed in this type of markets. Furthermore, it\nshows us how these local trends and volatility are determined by the typical\nvalues of the bid-ask spread. In addition, we use our model to show how large\ndeviations occur in the spread and prices, such as those observed in flash\ncrashes.\n"
    },
    {
        "paper_id": 2004.10869,
        "authors": "Yosuke A. Yamashiki, Moe Fujita, Tatsuhiko Sato, Hiroyuki Maehara,\n  Yuta Notsu, Kazunari Shibata",
        "title": "Cost estimation for alternative aviation plans against potential\n  radiation exposure associated with solar proton events for the airline\n  industry",
        "comments": "16 pages, 3 figures, published in Evolutionary and Institutional\n  Economics Review, 2020 (The accepted version of the manuscript is uploaded in\n  arXiv. The final published version is available in the journal website.)",
        "journal-ref": null,
        "doi": "10.1007/s40844-020-00163-4",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We present a systematic approach to effectively evaluate potential risk cost\ncaused by exposure to solar proton events (SPEs) from solar flares for the\nairline industry. We also evaluate associated health risks from radiation, to\nprovide relevant alternative ways to minimize economic loss and opportunity.\nThe estimated radiation dose induced by each SPE for the passengers of each\nflight is calculated using ExoKyoto and PHITS. We determine a few scenarios for\nthe estimated dose limit at 1 and 20mSv, corresponding to the effective dose\nlimit for the general public and occupational exposure, respectively, as well\nas a higher dose induced an extreme superflare. We set a hypothetical airline\nshutdown scenario at 1mSv for a single flight per passenger, due to legal\nrestrictions under the potential radiation dose. In such a scenario, we\ncalculate the potential loss in direct and opportunity cost under the\ncancelation of the flight. At the same time, we considered that, even under\nsuch a scenario, if the airplane flies at a slightly lower altitude (from 12 to\n9.5km: atmospheric depth from 234 to 365g/cm$^{2}$), the total loss becomes\nmuch smaller than flight cancelation, and the estimated total dose goes down\nfrom 1.2 to 0.45mSv, which is below the effective dose limit for the general\npublic. In case of flying at an even lower altitude (7km: atmospheric depth\n484g/cm$^{2}$), the estimated total dose becomes much smaller, 0.12 mSv. If we\nassume the increase of fuel cost is proportional to the increase in atmospheric\ndepth, the increase in cost becomes 1.56 and 2.07 for the case of flying at 9.5\nkm and at 7 km, respectively. Lower altitude flights provide more safety for\nthe potential risk of radiation doses induced by severe SPEs. At the same time,\nsince there is total loss caused by flight cancelation, we propose that\nconsidering lower flight altitude is the best protection against solar flares.\n"
    },
    {
        "paper_id": 2004.10951,
        "authors": "Hyoeun Lee, Kiseop Lee",
        "title": "Optimal execution with liquidity risk in a diffusive order book market",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We study the optimal order placement strategy with the presence of a\nliquidity cost. In this problem, a stock trader wishes to clear her large\ninventory by a predetermined time horizon $T$. A trader uses both limit and\nmarket orders, and a large market order faces an adverse price movement caused\nby the liquidity risk. First, we study a single period model where the trader\nplaces a limit order and/or a market order at the beginning. We show the\nbehavior of optimal amount of market order, $m^*$, and optimal placement of\nlimit order, $y^*$, under different market conditions. Next, we extend it to a\nmulti-period model, where the trader makes sequential decisions of limit and\nmarket orders at multiple time points.\n"
    },
    {
        "paper_id": 2004.11118,
        "authors": "Le Anh Vu, Duong Quang Hoa, Nguyen Minh Tri and Ha Van Hieu",
        "title": "Some Applications of Lie Groups in Theory of Technical Progress",
        "comments": "13 pages, 1 figure",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  In recent decades, we have known some interesting applications of Lie theory\nin the theory of technological progress. Firstly, we will discuss some results\nof R. Saito in \\cite{rS1980} and \\cite{rS1981} about the application modeling\nof Lie groups in the theory of technical progress. Next, we will describe the\nresult on Romanian economy of G. Zaman and Z. Goschin in \\cite{ZG2010}.\nFinally, by using Sato's results and applying the method of G. Zaman and Z.\nGoschin, we give an estimation of the GDP function of Viet Nam for the\n1995-2018 period and give several important observations about the impact of\ntechnical progress on economic growth of Viet Nam.\n"
    },
    {
        "paper_id": 2004.11121,
        "authors": "Takahiro Yabe, Yunchang Zhang, Satish Ukkusuri",
        "title": "Quantifying the Economic Impact of Extreme Shocks on Businesses using\n  Human Mobility Data: a Bayesian Causal Inference Approach",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In recent years, extreme shocks, such as natural disasters, are increasing in\nboth frequency and intensity, causing significant economic loss to many cities\naround the world. Quantifying the economic cost of local businesses after\nextreme shocks is important for post-disaster assessment and pre-disaster\nplanning. Conventionally, surveys have been the primary source of data used to\nquantify damages inflicted on businesses by disasters. However, surveys often\nsuffer from high cost and long time for implementation, spatio-temporal\nsparsity in observations, and limitations in scalability. Recently, large scale\nhuman mobility data (e.g. mobile phone GPS) have been used to observe and\nanalyze human mobility patterns in an unprecedented spatio-temporal granularity\nand scale. In this work, we use location data collected from mobile phones to\nestimate and analyze the causal impact of hurricanes on business performance.\nTo quantify the causal impact of the disaster, we use a Bayesian structural\ntime series model to predict the counterfactual performances of affected\nbusinesses (what if the disaster did not occur?), which may use performances of\nother businesses outside the disaster areas as covariates. The method is tested\nto quantify the resilience of 635 businesses across 9 categories in Puerto Rico\nafter Hurricane Maria. Furthermore, hierarchical Bayesian models are used to\nreveal the effect of business characteristics such as location and category on\nthe long-term resilience of businesses. The study presents a novel and more\nefficient method to quantify business resilience, which could assist policy\nmakers in disaster preparation and relief processes.\n"
    },
    {
        "paper_id": 2004.11122,
        "authors": "Vadlamani Ravi and Vadlamani Madhav",
        "title": "Optimizing the reliability of a bank with Logistic Regression and\n  Particle Swarm Optimization",
        "comments": "11 Pages, 2 Figures, 2 Tables",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
        "abstract": "  It is well-known that disciplines such as mechanical engineering, electrical\nengineering, civil engineering, aerospace engineering, chemical engineering and\nsoftware engineering witnessed successful applications of reliability\nengineering concepts. However, the concept of reliability in its strict sense\nis missing in financial services. Therefore, in order to fill this gap, in a\nfirst-of-its-kind-study, we define the reliability of a bank/firm in terms of\nthe financial ratios connoting the financial health of the bank to withstand\nthe likelihood of insolvency or bankruptcy. For the purpose of estimating the\nreliability of a bank, we invoke a statistical and machine learning algorithm\nnamely, logistic regression (LR). Once, the parameters are estimated in the 1st\nstage, we fix them and treat the financial ratios as decision variables. Thus,\nin the 1st stage, we accomplish the hitherto unknown way of estimating the\nreliability of a bank. Subsequently, in the 2nd stage, in order to maximize the\nreliability of the bank, we formulate an unconstrained optimization problem in\na single-objective environment and solve it using the well-known particle swarm\noptimization (PSO) algorithm. Thus, in essence, these two stages correspond to\npredictive and prescriptive analytics respectively. The proposed 2-stage\nstrategy of using them in tandem is beneficial to the decision-makers within a\nbank who can try to achieve the optimal or near-optimal values of the financial\nratios in order to maximize the reliability which is tantamount to safeguarding\ntheir bank against solvency or bankruptcy.\n"
    },
    {
        "paper_id": 2004.11148,
        "authors": "Min-Young Lee, Woo-Sung Jung, Gabjin Oh",
        "title": "Trading characteristics of member firms on the Korea Exchange",
        "comments": null,
        "journal-ref": null,
        "doi": "10.3938/jkps.76.1144",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper, we study the characteristics of the member firms on the Korea\nExchange. The member firms intermediate between the market participants and the\nexchange, and all the participants should trade stocks through members. To\nidentify the characteristics of member firms, all member firms are categorized\ninto three groups, such as the domestic members similar to individuals (DIMs),\nthe domestic members similar to institutions (DSMs), and the foreign members\n(FRMs), in terms of the type of investor. We examine the dynamics of the member\nfirms. The trading characteristics of members are revealed through the\ndirectionality and trend. While FRMs tend to trade one-way and move with the\nprice change, DIMs are the opposite. In the market, DIMs and DSMs do herd and\nthe herding moves in the opposite direction of the price change. One the other\nhand, FRMs do herd in the direction of the price change. The network analysis\nsupports that the members are clustered into three groups similar to DIMs,\nDSMs, and FRMs. Finally, random matrix theory and a cross-sectional regression\nshow that the inventory variation of members possesses significant information\nabout stock prices and that member herding helps to price the stocks.\n"
    },
    {
        "paper_id": 2004.11169,
        "authors": "Benjamin Avanzi, Gregory Clive Taylor, Bernard Wong, and Xinda Yang",
        "title": "On the modelling of multivariate counts with Cox processes and dependent\n  shot noise intensities",
        "comments": null,
        "journal-ref": null,
        "doi": "10.1016/j.insmatheco.2021.01.002",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper, we develop a method to model and estimate several, _dependent_\ncount processes, using granular data. Specifically, we develop a multivariate\nCox process with shot noise intensities to jointly model the arrival process of\ncounts (e.g. insurance claims). The dependency structure is introduced via\nmultivariate shot noise _intensity_ processes which are connected with the help\nof L\\'evy copulas. In aggregate, our approach allows for (i) over-dispersion\nand auto-correlation within each line of business; (ii) realistic features\ninvolving time-varying, known covariates; and (iii) parsimonious dependence\nbetween processes without requiring simultaneous primary (e.g. accidents)\nevents.\n  The explicit incorporation of time-varying, known covariates can accommodate\ncharacteristics of real data and hence facilitate implementation in practice.\nIn an insurance context, these could be changes in policy volumes over time, as\nwell as seasonality patterns and trends, which may explain some of the\nrelationship (dependence) between multiple claims processes, or at least help\ntease out those relationships.\n  Finally, we develop a filtering algorithm based on the reversible-jump Markov\nChain Monte Carlo (RJMCMC) method to estimate the latent stochastic intensities\nand illustrate model calibration using real data from the AUSI data set.\n"
    },
    {
        "paper_id": 2004.11235,
        "authors": "Osman Gulseven, Abdulrahman Elmi, Odai Bataineh",
        "title": "The Divergence Between Industrial Infrastructure and Research Output\n  among the GCC Member States",
        "comments": "10 figures, 2 tables, 12 pages",
        "journal-ref": "International Journal of Business & Applied Sciences, 9(2), pp.\n  21-32 (2020)",
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this article, we provide a comparative analysis of the industry,\ncommunication, and research infrastructure among the GCC member states as\nmeasured by the United Nations sustainable development goal 9. SDG 9 provides a\nclear framework for measuring the performance of nations in achieving\nsustainable industrialization. Three pillars of this goal are defined as\nquality logistics and efficient transportation, availability of mobile-cellular\nnetwork with high-speed internet access, and quality research output. Based on\nthe data from both the United Nations' SDG database and the Bertelsmann\nStiftung SDG-index, our results suggest that while most of the sub-goals in SDG\n9 are achieved, significant challenges remain ahead. Notably, the research\noutput of the GCC member states is not in par with that of the developed world.\nWe suggest the GCC decisionmakers initiate national and supranational research\nschemes in order to boost research and development in the region.\n"
    },
    {
        "paper_id": 2004.1127,
        "authors": "Ivan Arraut, Alan Au and Alan Ching-biu Tse",
        "title": "On the multiplicity of the martingale condition: Spontaneous symmetry\n  breaking in Quantum Finance",
        "comments": "14 pages, submitted to Physica A",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We demonstrate that the martingale condition in the stock market can be\ninterpreted as a vacuum condition when we express the financial equations in\nthe Hamiltonian form. We then show that the symmetry under the changes of the\nprices is spontaneously broken in general and the symmetry under changes in the\nvolatility, for the case of the Merton-Garman (MG) equation, is also\nspontaneously broken. This reproduces a vacuum degeneracy for the system. In\nthis way, we find the conditions under which, the martingale condition can be\nconsidered to be a non-degenerate vacuum. This gives us a surprising connection\nbetween spontaneous symmetry breaking and the flow of information through the\nboundaries for the financial systems. Subsequently, we find an extended\nmartingale condition for the MG equation, depending not only prices but also on\nthe volatility and finally, we show what happens if we include additional\nnon-derivative terms on the Black Scholes and on the MG equations, breaking\nthen some other symmetries of the system spontaneously.\n"
    },
    {
        "paper_id": 2004.11279,
        "authors": "Osman Gulseven",
        "title": "Estimating the Demand Factors and Willingness to Pay for Agricultural\n  Insurance",
        "comments": "6 pages, 3 graphs, 3 figures",
        "journal-ref": "Australian Journal of Engineering Research, 1(4), pp 13 to 18\n  (2014)",
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This article investigates the effect of prices and socio-demographic\nvariables on the farmers decision to purchase agricultural insurance. A survey\nhas been conducted to 200 farmers most of whom are engaged in diversified\nincome-generating activities. The logistic estimation results suggest that\neducation and household income from farming activities positively affect the\nlikelihood of purchasing insurance. The demand for insurance is negatively\ncorrelated with the premium paid per insured value, suggesting that insurance\nis a normal good. Farmers are willing to pay (WTP) increasingly higher premiums\nfor contracts with a higher coverage ratio. According to the valuation model,\nthe WTP declines sharply for coverage ratios under 70%.\n"
    },
    {
        "paper_id": 2004.11485,
        "authors": "Dimitris Korobilis",
        "title": "High-dimensional macroeconomic forecasting using message passing\n  algorithms",
        "comments": "89 pages; to appear in Journal of Business and Economic Statistics",
        "journal-ref": null,
        "doi": "10.1080/07350015.2019.1677472",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper proposes two distinct contributions to econometric analysis of\nlarge information sets and structural instabilities. First, it treats a\nregression model with time-varying coefficients, stochastic volatility and\nexogenous predictors, as an equivalent high-dimensional static regression\nproblem with thousands of covariates. Inference in this specification proceeds\nusing Bayesian hierarchical priors that shrink the high-dimensional vector of\ncoefficients either towards zero or time-invariance. Second, it introduces the\nframeworks of factor graphs and message passing as a means of designing\nefficient Bayesian estimation algorithms. In particular, a Generalized\nApproximate Message Passing (GAMP) algorithm is derived that has low\nalgorithmic complexity and is trivially parallelizable. The result is a\ncomprehensive methodology that can be used to estimate time-varying parameter\nregressions with arbitrarily large number of exogenous predictors. In a\nforecasting exercise for U.S. price inflation this methodology is shown to work\nvery well.\n"
    },
    {
        "paper_id": 2004.11674,
        "authors": "Roy Cerqueti and Massimiliano Giacalone and Raffaele Mattera",
        "title": "Skewed non-Gaussian GARCH models for cryptocurrencies volatility\n  modelling",
        "comments": "54 pages, 7 figures, 34 tables. To be published in Information\n  Sciences",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Recently, cryptocurrencies have attracted a growing interest from investors,\npractitioners and researchers. Nevertheless, few studies have focused on the\npredictability of them. In this paper we propose a new and comprehensive study\nabout cryptocurrency market, evaluating the forecasting performance for three\nof the most important cryptocurrencies (Bitcoin, Ethereum and Litecoin) in\nterms of market capitalization. At this aim, we consider non-Gaussian GARCH\nvolatility models, which form a class of stochastic recursive systems commonly\nadopted for financial predictions. Results show that the best specification and\nforecasting accuracy are achieved under the Skewed Generalized Error\nDistribution when Bitcoin/USD and Litecoin/USD exchange rates are considered,\nwhile the best performances are obtained for skewed Distribution in the case of\nEthereum/USD exchange rate. The obtain findings state the effectiveness -- in\nterms of prediction performance -- of relaxing the normality assumption and\nconsidering skewed distributions.\n"
    },
    {
        "paper_id": 2004.11686,
        "authors": "Hasan Fallahgoul",
        "title": "Inside the Mind of Investors During the COVID-19 Pandemic: Evidence from\n  the StockTwits Data",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We study the investor beliefs, sentiment and disagreement, about stock market\nreturns during the COVID-19 pandemic using a large number of messages of\ninvestors on a social media investing platform, \\textit{StockTwits}. The rich\nand multimodal features of StockTwits data allow us to explore the evolution of\nsentiment and disagreement within and across investors, sectors, and even\nindustries. We find that the sentiment (disagreement) has a sharp decrease\n(increase) across all investors with any investment philosophy, horizon, and\nexperience between February 19, 2020, and March 23, 2020, where a historical\nmarket high followed by a record drop. Surprisingly, these measures have a\nsharp reverse toward the end of March. However, the performance of these\nmeasures across various sectors is heterogeneous. Financial and healthcare\nsectors are the most pessimistic and optimistic divisions, respectively.\n"
    },
    {
        "paper_id": 2004.11697,
        "authors": "Sidra Mehtab and Jaydip Sen",
        "title": "A Time Series Analysis-Based Stock Price Prediction Using Machine\n  Learning and Deep Learning Models",
        "comments": "This is the preprint of our paper accepted for publication in the\n  Inderscience Journal International Journal of Business Forecasting and\n  Marketing Intelligence. The paper consists of 53 pages, 26 Tables, and 46\n  Figures",
        "journal-ref": "International Journal of Business Forecasting and Marketing\n  Intelligence (IJBFMI), Vol 6, No 4, pp. 272 - 335, 2020. Inderscience\n  Publishers",
        "doi": "10.1504/IJBFMI.2020.115691",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Prediction of future movement of stock prices has always been a challenging\ntask for the researchers. While the advocates of the efficient market\nhypothesis (EMH) believe that it is impossible to design any predictive\nframework that can accurately predict the movement of stock prices, there are\nseminal work in the literature that have clearly demonstrated that the\nseemingly random movement patterns in the time series of a stock price can be\npredicted with a high level of accuracy. Design of such predictive models\nrequires choice of appropriate variables, right transformation methods of the\nvariables, and tuning of the parameters of the models. In this work, we present\na very robust and accurate framework of stock price prediction that consists of\nan agglomeration of statistical, machine learning and deep learning models. We\nuse the daily stock price data, collected at five minutes interval of time, of\na very well known company that is listed in the National Stock Exchange (NSE)\nof India. The granular data is aggregated into three slots in a day, and the\naggregated data is used for building and training the forecasting models. We\ncontend that the agglomerative approach of model building that uses a\ncombination of statistical, machine learning, and deep learning approaches, can\nvery effectively learn from the volatile and random movement patterns in a\nstock price data. We build eight classification and eight regression models\nbased on statistical and machine learning approaches. In addition to these\nmodels, a deep learning regression model using a long-and-short-term memory\n(LSTM) network is also built. Extensive results have been presented on the\nperformance of these models, and the results are critically analyzed.\n"
    },
    {
        "paper_id": 2004.1178,
        "authors": "Ruda Zhang and Patrick Wingo and Rodrigo Duran and Kelly Rose and\n  Jennifer Bauer and Roger Ghanem",
        "title": "Environmental Economics and Uncertainty: Review and a Machine Learning\n  Outlook",
        "comments": "24 pages, 7 figures, 1 table. In Oxford Research Encyclopedia of\n  Environmental Science. Oxford University Press",
        "journal-ref": null,
        "doi": "10.1093/acrefore/9780199389414.013.572",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Economic assessment in environmental science concerns the measurement or\nvaluation of environmental impacts, adaptation, and vulnerability. Integrated\nassessment modeling is a unifying framework of environmental economics, which\nattempts to combine key elements of physical, ecological, and socioeconomic\nsystems. Uncertainty characterization in integrated assessment varies by\ncomponent models: uncertainties associated with mechanistic physical models are\noften assessed with an ensemble of simulations or Monte Carlo sampling, while\nuncertainties associated with impact models are evaluated by conjecture or\neconometric analysis. Manifold sampling is a machine learning technique that\nconstructs a joint probability model of all relevant variables which may be\nconcentrated on a low-dimensional geometric structure. Compared with\ntraditional density estimation methods, manifold sampling is more efficient\nespecially when the data is generated by a few latent variables. The\nmanifold-constrained joint probability model helps answer policy-making\nquestions from prediction, to response, and prevention. Manifold sampling is\napplied to assess risk of offshore drilling in the Gulf of Mexico.\n"
    },
    {
        "paper_id": 2004.11953,
        "authors": "Johannes Bleher, Michael Bleher and Thomas Dimpfl",
        "title": "From orders to prices: A stochastic description of the limit order book\n  to forecast intraday returns",
        "comments": "82 pages, 18 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We propose a microscopic model to describe the dynamics of the fundamental\nevents in the limit order book (LOB): order arrivals and cancellations. It is\nbased on an operator algebra for individual orders and describes their effect\non the LOB. The model inputs are arrival and cancellation rate distributions\nthat emerge from individual behavior of traders, and we show how prices and\nliquidity arise from the LOB dynamics. In a simulation study we illustrate how\nthe model works and highlight its sensitivity with respect to assumptions\nregarding the collective behavior of market participants. Empirically, we test\nthe model on a LOB snapshot of XETRA, estimate several linearized model\nspecifications, and conduct in- and out-of-sample forecasts.The in-sample\nresults based on contemporaneous information suggest that our model describes\nreturns very well, resulting in an adjusted $R^2$ of roughly 80%. In the more\nrealistic setting where only past information enters the model, we observe an\nadjusted $R^2$ around 15%. The direction of the next return can be predicted\n(out-of-sample) with an accuracy above 75% for time horizons below 10 minutes.\nOn average, we obtain an RMSPE that is 10 times lower than values documented in\nthe literature.\n"
    },
    {
        "paper_id": 2004.12011,
        "authors": "\\'Alvaro Cartea, Sebastian Jaimungal, Tianyi Jia",
        "title": "Trading Foreign Exchange Triplets",
        "comments": "35 pages, 14 figures, 1 table",
        "journal-ref": "Forthcoming, SIAM J. Financial Mathematics, 2020",
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We develop the optimal trading strategy for a foreign exchange (FX) broker\nwho must liquidate a large position in an illiquid currency pair. To maximize\nrevenues, the broker considers trading in a currency triplet which consists of\nthe illiquid pair and two other liquid currency pairs. The liquid pairs in the\ntriplet are chosen so that one of the pairs is redundant. The broker is\nrisk-neutral and accounts for model ambiguity in the FX rates to make her\nstrategy robust to model misspecification. When the broker is ambiguity neutral\n(averse) the trading strategy in each pair is independent (dependent) of the\ninventory in the other two pairs in the triplet. We employ simulations to\nillustrate how the robust strategies perform. For a range of ambiguity aversion\nparameters, we find the mean Profit and Loss (P&L) of the strategy increases\nand the standard deviation of the P&L decreases as ambiguity aversion\nincreases.\n"
    },
    {
        "paper_id": 2004.12099,
        "authors": "Chung-Han Hsieh",
        "title": "Necessary and Sufficient Conditions for Frequency-Based Kelly Optimal\n  Portfolio",
        "comments": "Submitted to IEEE Control Systems Letter",
        "journal-ref": "IEEE Control Systems Letter, vol 5, no 1, pp. 349-354, 2021",
        "doi": "10.1109/LCSYS.2020.3002214",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper, we consider a discrete-time portfolio with $m \\geq 2$ assets\noptimization problem which includes the rebalancing~frequency as an additional\nparameter in the maximization. The so-called Kelly Criterion is used as the\nperformance metric; i.e., maximizing the expected logarithmic growth of a\ntrader's account, and the portfolio obtained is called the frequency-based\nKelly optimal portfolio. The focal point of this paper is to extend upon the\nresults of our previous work to obtain various optimality characterizations on\nthe portfolio. To be more specific, using Kelly's criterion in our\nfrequency-based formulation, we first prove necessary and sufficient conditions\nfor the frequency-based Kelly optimal portfolio. With the aid of these\nconditions, we then show several new optimality characterizations such as\nexpected ratio optimality and asymptotic relative optimality, and a result\nwhich we call the Extended Dominant Asset Theorem. That is, we prove that the\n$i$th asset is dominant in the portfolio if and only if the Kelly optimal\nportfolio consists of that asset only. The word \"extended\" on the theorem comes\nfrom the fact that it was only a sufficiency result that was proved in our\nprevious work. Hence, in this paper, we improve it to involve a proof of the\nnecessity part. In addition, the trader's survivability issue (no bankruptcy\nconsideration) is also studied in detail in our frequency-based trading\nframework. Finally, to bridge the theory and practice, we propose a simple\ntrading algorithm using the notion called dominant asset condition to decide\nwhen should one triggers a trade. The corresponding trading performance using\nhistorical price data is reported as supporting evidence.\n"
    },
    {
        "paper_id": 2004.12336,
        "authors": "Anton J. Heckens, Sebastian M. Krause, Thomas Guhr",
        "title": "Uncovering the Dynamics of Correlation Structures Relative to the\n  Collective Market Motion",
        "comments": "30 pages, 17 figures",
        "journal-ref": "J. Stat. Mech. (2020) 103402",
        "doi": "10.1088/1742-5468/abb6e2",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The measured correlations of financial time series in subsequent epochs\nchange considerably as a function of time. When studying the whole correlation\nmatrices, quasi-stationary patterns, referred to as market states, are seen by\napplying clustering methods. They emerge, disappear or reemerge, but they are\ndominated by the collective motion of all stocks. In the jargon, one speaks of\nthe market motion, it is always associated with the largest eigenvalue of the\ncorrelation matrices. Thus the question arises, if one can extract more refined\ninformation on the system by subtracting the dominating market motion in a\nproper way. To this end we introduce a new approach by clustering reduced-rank\ncorrelation matrices which are obtained by subtracting the dyadic matrix\nbelonging to the largest eigenvalue from the standard correlation matrices. We\nanalyze daily data of 262 companies of the S&P 500 index over a period of\nalmost 15 years from 2002 to 2016. The resulting dynamics is remarkably\ndifferent, and the corresponding market states are quasi-stationary over a long\nperiod of time. Our approach adds to the attempts to separate endogenous from\nexogenous effects.\n"
    },
    {
        "paper_id": 2004.12392,
        "authors": "Thomas Krabichler, Josef Teichmann",
        "title": "The Jarrow & Turnbull setting revisited",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We consider a financial market with zero-coupon bonds that are exposed to\ncredit and liquidity risk. We revisit the famous Jarrow & Turnbull setting in\norder to account for these two intricately intertwined risk types. We utilise\nthe foreign exchange analogy that interprets defaultable zero-coupon bonds as a\nconversion of non-defaultable foreign counterparts. The relevant exchange rate\nis only partially observable in the market filtration, which leads us naturally\nto an application of the concept of platonic financial markets. We provide an\nexample of tractable term structure models that are driven by a two-dimensional\naffine jump diffusion. Furthermore, we derive explicit valuation formulae for\nmarketable products, e.g., for credit default swaps.\n"
    },
    {
        "paper_id": 2004.12394,
        "authors": "Thomas Krabichler, Josef Teichmann",
        "title": "A constraint-based notion of illiquidity",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This article introduces a new mathematical concept of illiquidity that goes\nhand in hand with credit risk. The concept is not volume- but constraint-based,\ni.e., certain assets cannot be shorted and are ineligible as num\\'eraire. If\nthose assets are still chosen as num\\'eraire, we arrive at a two-price economy.\nWe utilise Jarrow & Turnbull's foreign exchange analogy that interprets\ndefaultable zero-coupon bonds as a conversion of non-defaultable foreign\ncounterparts. In the language of structured derivatives, the impact of credit\nrisk is disabled through quanto-ing. In a similar fashion, we look at bond\nprices as if perfect liquidity was given. This corresponds to asset pricing\nwith respect to an ineligible num\\'eraire and necessitates F\\\"ollmer measures.\n"
    },
    {
        "paper_id": 2004.124,
        "authors": "Fabrizio Cipollini, Giampiero M. Gallo, Alessandro Palandri",
        "title": "A dynamic conditional approach to portfolio weights forecasting",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We build the time series of optimal realized portfolio weights from\nhigh-frequency data and we suggest a novel Dynamic Conditional Weights (DCW)\nmodel for their dynamics. DCW is benchmarked against popular model-based and\nmodel-free specifications in terms of weights forecasts and portfolio\nallocations. Next to portfolio variance, certainty equivalent and turnover, we\nintroduce the break-even transaction costs as an additional measure that\nidentifies the range of transaction costs for which one allocation is preferred\nto another. By comparing minimum-variance portfolios built on the components of\nthe Dow Jones 30 Index, the proposed DCW overall attains the best allocations\nwith respect to the measures considered, for any degree of risk-aversion,\ntransaction costs and exposure.\n"
    },
    {
        "paper_id": 2004.12791,
        "authors": "Claudiu Albulescu (CRIEF)",
        "title": "Bank financial stability, bank valuation and international oil prices:\n  Evidence from listed Russian public banks",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Using data on 17 listed public banks from Russia over the period 2008 to\n2016, we analyze whether international oil prices affect the bank stability in\nan oil-dependent country. We posit that a decrease in international oil prices\nhas a negative long-run macroeconomic impact for an oil-exporting country,\nwhich further deteriorates the bank financial stability. More specifically, a\ndecrease in international oil prices leads for an oil-exporting country as\nRussia to a currency depreciation and to a deterioration of the fiscal stance.\nIn addition, given the positive correlation of oil and stock prices documented\nby numerous previous studies, a decrease in international oil prices represents\na negative signal for the stock markets investors, negatively affecting banks'\nshare prices and thus, their capacity to generate sustainable earnings. In this\ncontext, the bank financial stability can be menaced. With a focus on public\nlisted banks and using a Pool Mean Group (PMG) estimator, we show that an\nincrease in international oil prices and in the price to book value ratio has a\nlong-run positive effect on Russian public banks stability, and conversely.\nWhile positive oil-price shocks contribute to bank stability in the long run,\nan opposite effect is recorded for negative shocks. However, no significant\nimpact is documented in the short run. Our findings are robust to different\nbank stability specifications, different samples and control variables.\n"
    },
    {
        "paper_id": 2004.12848,
        "authors": "Chung-Han Hsieh",
        "title": "Generalization of Affine Feedback Stock Trading Results to Include\n  Stop-Loss Orders",
        "comments": "SIAM Journal on Control and Optimization (SICON)",
        "journal-ref": "Automatica, vol. 136, pp. 110051:1-110051:7, 2022",
        "doi": "10.1016/j.automatica.2021.110051",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The takeoff point of this paper is to generalize the existing stock trading\nresults for a class of affine feedback controller to include consideration of a\nstop-loss order. Using the geometric Brownian motion as the underlying stock\nprice model, our main result is to provide a closed-form expression for the\ncumulative distribution function for the trading profit or loss. In addition,\nwe show that the affine feedback controller with stop-loss order indeed\ngeneralizes the result without stop order in the sense of distribution\nfunction. Some simulations and illustrative examples are also provided as\nsupporting evidence of the theory. Moreover, we provide some technical results\naimed at addressing the issues about survivability, cash-financing\nconsiderations, long-only property, and lower bound of the expected gain or\nloss.\n"
    },
    {
        "paper_id": 2004.13,
        "authors": "Wenjuan Hou, Tao Fang, Zhi Pei, Qiao-Chu He",
        "title": "Integrated Design of Unmanned Aerial Mobility Network: A Data-Driven\n  Risk-Averse Approach",
        "comments": "31pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The real challenge in drone-logistics is to develop an economically-feasible\nUnmanned Aerial Mobility Network (UAMN). In this paper, we propose an\nintegrated airport location (strategic decision) and routes planning\n(operational decision) optimization framework to minimize the total cost of the\nnetwork, while guaranteeing flow constraints, capacity constraints, and\nelectricity constraints. To facility expensive long-term infrastructure\nplanning facing demand uncertainty, we develop a data-driven risk-averse\ntwo-stage stochastic optimization model based on the Wasserstein distance. We\ndevelop a reformulation technique which simplifies the worst-case expectation\nterm in the original model, and obtain a fractable Min-Max solution procedure\ncorrespondingly. Using Lagrange multipliers, we successfully decompose decision\nvariables and reduce the complexity of computation. To provide managerial\ninsights, we design specific numerical examples. For example, we find that the\noptimal network configuration is affected by the \"pooling effects\" in channel\ncapacities. A nice feature of our DRO framework is that the optimal network\ndesign is relatively robust under demand uncertainty. Interestingly, a\ncandidate node without historical demand records can be chosen to locate an\nairport. We demonstrate the application of our model for a real medical\nresources transportation problem with our industry partner, collecting donated\nblood to a blood bank in Hangzhou, China.\n"
    },
    {
        "paper_id": 2004.13008,
        "authors": "Ion Spanulescu, Anca Gheorghiu",
        "title": "Econophysics Approach and Model on Mixed Economy",
        "comments": "15 pages, 4 figures, ENEC 2019, Bucharest, Romania, 2019",
        "journal-ref": "Hyperion International Journal of Econophysics and New Economy,vol\n  12 (2019),nr.2, pp.19-34",
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper the general principles and categories of mixed economy that\ncurrently exist in almost all countries of the world are presented. The paper\nalso presents an Advanced Model of Mixed Economy with Threshold (AMMET), which\nis characterized by a reduced value (approx. 10-15%) of the State and public\nsector participation in the national economy and proposes and analyzes an\neconophysics model for the mixed economy.\n"
    },
    {
        "paper_id": 2004.13135,
        "authors": "Calypso Herrera, Florian Krach, Josef Teichmann",
        "title": "Local Lipschitz Bounds of Deep Neural Networks",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
        "abstract": "  The Lipschitz constant is an important quantity that arises in analysing the\nconvergence of gradient-based optimization methods. It is generally unclear how\nto estimate the Lipschitz constant of a complex model. Thus, this paper studies\nan important problem that may be useful to the broader area of non-convex\noptimization. The main result provides a local upper bound on the Lipschitz\nconstants of a multi-layer feed-forward neural network and its gradient.\nMoreover, lower bounds are established as well, which are used to show that it\nis impossible to derive global upper bounds for the Lipschitz constants. In\ncontrast to previous works, we compute the Lipschitz constants with respect to\nthe network parameters and not with respect to the inputs. These constants are\nneeded for the theoretical description of many step size schedulers of gradient\nbased optimization schemes and their convergence analysis. The idea is both\nsimple and effective. The results are extended to a generalization of neural\nnetworks, continuously deep neural networks, which are described by controlled\nODEs.\n"
    },
    {
        "paper_id": 2004.13235,
        "authors": "Takaaki Koike and Yuri F. Saporito and Rodrigo S. Targino",
        "title": "Avoiding zero probability events when computing Value at Risk\n  contributions",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper is concerned with the process of risk allocation for a generic\nmultivariate model when the risk measure is chosen as the Value-at-Risk (VaR).\nWe recast the traditional Euler contributions from an expectation conditional\non an event of zero probability to a ratio involving conditional expectations\nwhose conditioning events have strictly positive probability. We derive an\nanalytical form of the proposed representation of VaR contributions for various\nparametric models. Our numerical experiments show that the estimator using this\nnovel representation outperforms the standard Monte Carlo estimator in terms of\nbias and variance. Moreover, unlike the existing estimators, the proposed\nestimator is free from hyperparameters under a parametric setting.\n"
    },
    {
        "paper_id": 2004.13332,
        "authors": "Stephan Zheng, Alexander Trott, Sunil Srinivasa, Nikhil Naik, Melvin\n  Gruesbeck, David C. Parkes, Richard Socher",
        "title": "The AI Economist: Improving Equality and Productivity with AI-Driven Tax\n  Policies",
        "comments": "46 pages, 21 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Tackling real-world socio-economic challenges requires designing and testing\neconomic policies. However, this is hard in practice, due to a lack of\nappropriate (micro-level) economic data and limited opportunity to experiment.\nIn this work, we train social planners that discover tax policies in dynamic\neconomies that can effectively trade-off economic equality and productivity. We\npropose a two-level deep reinforcement learning approach to learn dynamic tax\npolicies, based on economic simulations in which both agents and a government\nlearn and adapt. Our data-driven approach does not make use of economic\nmodeling assumptions, and learns from observational data alone. We make four\nmain contributions. First, we present an economic simulation environment that\nfeatures competitive pressures and market dynamics. We validate the simulation\nby showing that baseline tax systems perform in a way that is consistent with\neconomic theory, including in regard to learned agent behaviors and\nspecializations. Second, we show that AI-driven tax policies improve the\ntrade-off between equality and productivity by 16% over baseline policies,\nincluding the prominent Saez tax framework. Third, we showcase several emergent\nfeatures: AI-driven tax policies are qualitatively different from baselines,\nsetting a higher top tax rate and higher net subsidies for low incomes.\nMoreover, AI-driven tax policies perform strongly in the face of emergent\ntax-gaming strategies learned by AI agents. Lastly, AI-driven tax policies are\nalso effective when used in experiments with human participants. In experiments\nconducted on MTurk, an AI tax policy provides an equality-productivity\ntrade-off that is similar to that provided by the Saez framework along with\nhigher inverse-income weighted social welfare.\n"
    },
    {
        "paper_id": 2004.13347,
        "authors": "Kei Nakagawa, Shuhei Noma, Masaya Abe",
        "title": "RM-CVaR: Regularized Multiple $\\beta$-CVaR Portfolio",
        "comments": "accepted by the IJCAI-PRICAI 2020 Special Track AI in FinTech",
        "journal-ref": null,
        "doi": "10.24963/ijcai.2020/629",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The problem of finding the optimal portfolio for investors is called the\nportfolio optimization problem. Such problem mainly concerns the expectation\nand variability of return (i.e., mean and variance). Although the variance\nwould be the most fundamental risk measure to be minimized, it has several\ndrawbacks. Conditional Value-at-Risk (CVaR) is a relatively new risk measure\nthat addresses some of the shortcomings of well-known variance-related risk\nmeasures, and because of its computational efficiencies, it has gained\npopularity. CVaR is defined as the expected value of the loss that occurs\nbeyond a certain probability level ($\\beta$). However, portfolio optimization\nproblems that use CVaR as a risk measure are formulated with a single $\\beta$\nand may output significantly different portfolios depending on how the $\\beta$\nis selected. We confirm even small changes in $\\beta$ can result in huge\nchanges in the whole portfolio structure. In order to improve this problem, we\npropose RM-CVaR: Regularized Multiple $\\beta$-CVaR Portfolio. We perform\nexperiments on well-known benchmarks to evaluate the proposed portfolio.\nCompared with various portfolios, RM-CVaR demonstrates a superior performance\nof having both higher risk-adjusted returns and lower maximum drawdown.\n"
    },
    {
        "paper_id": 2004.13463,
        "authors": "Menghan Sun, Jichang Zhao",
        "title": "How do online consumers review negatively?",
        "comments": "The dataset will be publicly available through a permanent link of\n  Figshare repository later",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Negative reviews on e-commerce platforms, mainly in the form of texts, are\nposted by online consumers to express complaints about unsatisfactory\nexperiences, providing a proxy of big data for sellers to consider\nimprovements. However, the exact knowledge that lies beyond the negative\nreviewing still remains unknown. Aimed at a systemic understanding of how\nonline consumers post negative reviews, using 1, 450, 000 negative reviews from\nJD.com, the largest B2C platform in China, the behavioral patterns from\ntemporal, perceptional and emotional perspectives are comprehensively explored\nin the present study. Massive consumers behind these reviews across four\nsectors in the most recent 10 years are further split into five levels to\nreveal group discriminations at a fine resolution. Circadian rhythms of\nnegative reviewing after making purchases were found, and the periodic\nintervals suggest stable habits in online consumption and that consumers tend\nto negatively review at the same hour of the purchase. Consumers from lower\nlevels express more intensive negative feelings, especially on product pricing\nand seller attitudes, while those from upper levels demonstrate a stronger\nmomentum of negative emotion. The value of negative reviews from higher-level\nconsumers is thus unexpectedly highlighted because of less emotionalization and\nless biased narration, while the longer-lasting characteristic of these\nconsumers' negative responses also stresses the need for more attention from\nsellers. Our results shed light on implementing distinguished proactive\nstrategies in different buyer groups to help mitigate the negative impact due\nto negative reviews.\n"
    },
    {
        "paper_id": 2004.13536,
        "authors": "Jamshid Ardalankia, Jafar Askari, Somaye Sheykhali, Emmanuel Haven, G.\n  Reza Jafari",
        "title": "Mapping Coupled Time-series Onto Complex Network",
        "comments": "7 pages, 4 figures",
        "journal-ref": null,
        "doi": "10.1209/0295-5075/132/58002",
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  In order to extract hidden joint information from two possibly uncorrelated\ntime-series, we explored the measures of network science. Alongside common\nmethods in time-series analysis of the economic markets, mapping the joint\nstructure of two time-series onto a network provides insight into hidden\naspects embedded in the couplings. We discretize the amplitude of two\ntime-series and investigate relative simultaneous locations of those\namplitudes. Each segment of a discretized amplitude is considered as a node.\nThe simultaneity of the amplitudes of the two time-series is considered as the\nedges in the network. The frequency of occurrences forms the weighted edges. In\norder to extract information, we need to measure that to what extent the\ncoupling deviates from the coupling of two uncoupled series. Also, we need to\nmeasure that to what extent the couplings inherit their characteristics from a\nGaussian distribution or a non-Gaussian distribution. We mapped the network\nfrom two surrogate time-series. The results show that the couplings of markets\npossess some features which diverge from the same features of the network\nmapped from white noise, and from the network mapped from two surrogate\ntime-series. These deviations prove that there exist joint information and\ncross-correlation therein. By applying the network's topological and\nstatistical measures and the deformation ratio in the joint probability\ndistribution, we distinguished basic structures of cross-correlation and\ncoupling of cross-markets. It was discovered that even two possibly known\nuncorrelated markets may possess some joint patterns with each other. Thereby,\nthose markets should be examined as coupled and \\textit{weakly} coupled\nmarkets.\n"
    },
    {
        "paper_id": 2004.13601,
        "authors": "Peter Grandits and Maike Klein",
        "title": "Ruin probability in a two-dimensional model with correlated Brownian\n  motions",
        "comments": "17 pages, 17 figures, Both authors gratefully acknowledge the support\n  by the Austrian Science Fund (Fonds zur F\\\"orderung der wissenschaftlichen\n  Forschung) under grant P30864-N35",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We consider two insurance companies with endowment processes given by\nBrownian motions with drift. The firms can collaborate by transfer payments in\norder to maximize the probability that none of them goes bankrupt. We show that\npushing maximally the company with less endowment is the optimal strategy for\nthe collaboration if the Brownian motions are correlated and the transfer rate\ncan exceed the drift rates. Moreover, we obtain an explicit formula for the\nminimal ruin probability in case of perfectly positively correlated Brownian\nmotions where we also allow for different diffusion coefficients.\n"
    },
    {
        "paper_id": 2004.13612,
        "authors": "Calypso Herrera, Florian Krach, Anastasis Kratsios, Pierre Ruyssen,\n  Josef Teichmann",
        "title": "Denise: Deep Robust Principal Component Analysis for Positive\n  Semidefinite Matrices",
        "comments": null,
        "journal-ref": "Transactions on Machine Learning Research (2023)",
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The robust PCA of covariance matrices plays an essential role when isolating\nkey explanatory features. The currently available methods for performing such a\nlow-rank plus sparse decomposition are matrix specific, meaning, those\nalgorithms must re-run for every new matrix. Since these algorithms are\ncomputationally expensive, it is preferable to learn and store a function that\nnearly instantaneously performs this decomposition when evaluated. Therefore,\nwe introduce Denise, a deep learning-based algorithm for robust PCA of\ncovariance matrices, or more generally, of symmetric positive semidefinite\nmatrices, which learns precisely such a function. Theoretical guarantees for\nDenise are provided. These include a novel universal approximation theorem\nadapted to our geometric deep learning problem and convergence to an optimal\nsolution to the learning problem. Our experiments show that Denise matches\nstate-of-the-art performance in terms of decomposition quality, while being\napproximately $2000\\times$ faster than the state-of-the-art, principal\ncomponent pursuit (PCP), and $200 \\times$ faster than the current\nspeed-optimized method, fast PCP.\n"
    },
    {
        "paper_id": 2004.13614,
        "authors": "Zhu Liu, Philippe Ciais, Zhu Deng, Ruixue Lei, Steven J. Davis, Sha\n  Feng, Bo Zheng, Duo Cui, Xinyu Dou, Pan He, Biqing Zhu, Chenxi Lu, Piyu Ke,\n  Taochun Sun, Yuan Wang, Xu Yue, Yilong Wang, Yadong Lei, Hao Zhou, Zhaonan\n  Cai, Yuhui Wu, Runtao Guo, Tingxuan Han, Jinjun Xue, Olivier Boucher, Eulalie\n  Boucher, Frederic Chevallier, Yimin Wei, Haiwang Zhong, Chongqing Kang, Ning\n  Zhang, Bin Chen, Fengming Xi, Fran\\c{c}ois Marie, Qiang Zhang, Dabo Guan,\n  Peng Gong, Daniel M. Kammen, Kebin He, Hans Joachim Schellnhuber",
        "title": "COVID-19 causes record decline in global CO2 emissions",
        "comments": null,
        "journal-ref": null,
        "doi": "10.1038/s41467-020-18922-7",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The considerable cessation of human activities during the COVID-19 pandemic\nhas affected global energy use and CO2 emissions. Here we show the\nunprecedented decrease in global fossil CO2 emissions from January to April\n2020 was of 7.8% (938 Mt CO2 with a +6.8% of 2-{\\sigma} uncertainty) when\ncompared with the period last year. In addition other emerging estimates of\nCOVID impacts based on monthly energy supply or estimated parameters, this\nstudy contributes to another step that constructed the near-real-time daily CO2\nemission inventories based on activity from power generation (for 29\ncountries), industry (for 73 countries), road transportation (for 406 cities),\naviation and maritime transportation and commercial and residential sectors\nemissions (for 206 countries). The estimates distinguished the decline of CO2\ndue to COVID-19 from the daily, weekly and seasonal variations as well as the\nholiday events. The COVID-related decreases in CO2 emissions in road\ntransportation (340.4 Mt CO2, -15.5%), power (292.5 Mt CO2, -6.4% compared to\n2019), industry (136.2 Mt CO2, -4.4%), aviation (92.8 Mt CO2, -28.9%),\nresidential (43.4 Mt CO2, -2.7%), and international shipping (35.9Mt CO2,\n-15%). Regionally, decreases in China were the largest and earliest (234.5 Mt\nCO2,-6.9%), followed by Europe (EU-27 & UK) (138.3 Mt CO2, -12.0%) and the U.S.\n(162.4 Mt CO2, -9.5%). The declines of CO2 are consistent with regional\nnitrogen oxides concentrations observed by satellites and ground-based\nnetworks, but the calculated signal of emissions decreases (about 1Gt CO2) will\nhave little impacts (less than 0.13ppm by April 30, 2020) on the overserved\nglobal CO2 concertation. However, with observed fast CO2 recovery in China and\npartial re-opening globally, our findings suggest the longer-term effects on\nCO2 emissions are unknown and should be carefully monitored using multiple\nmeasures.\n"
    },
    {
        "paper_id": 2004.1362,
        "authors": "G. Dimarco, L. Pareschi, G. Toscani, M. Zanella",
        "title": "Wealth distribution under the spread of infectious diseases",
        "comments": null,
        "journal-ref": "Phys. Rev. E 102, 022303 (2020)",
        "doi": "10.1103/PhysRevE.102.022303",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We develop a mathematical framework to study the economic impact of\ninfectious diseases by integrating epidemiological dynamics with a kinetic\nmodel of wealth exchange. The multi-agent description leads to study the\nevolution over time of a system of kinetic equations for the wealth densities\nof susceptible, infectious and recovered individuals, whose proportions are\ndriven by a classical compartmental model in epidemiology. Explicit\ncalculations show that the spread of the disease seriously affects the\ndistribution of wealth, which, unlike the situation in the absence of\nepidemics, can converge towards a stationary state with a bimodal form.\nFurthermore, simulations confirm the ability of the model to describe different\nphenomena characteristics of economic trends in situations compromised by the\nrapid spread of an epidemic, such as the unequal impact on the various wealth\nclasses and the risk of a shrinking middle class.\n"
    },
    {
        "paper_id": 2004.13696,
        "authors": "Yuxi Cai, Fan Long, Andreas Park, Andreas Veneris",
        "title": "Engineering Economics in the Conflux Network",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Proof-of-work blockchains need to be carefully designed so as to create the\nproper incentives for miners to faithfully maintain the network in a\nsustainable way. This paper describes how the economic engineering of the\nConflux Network, a high throughput proof-of-work blockchain, leads to sound\neconomic incentives that support desirable and sustainable mining behavior. In\ndetail, this paper parameterizes the level of income, and thus network\nsecurity, that Conflux can generate, and it describes how this depends on user\nbehavior and \"policy variables'' such as block and interest inflation. It also\ndiscusses how the underlying economic engineering design makes the Conflux\nNetwork resilient against double spending and selfish mining attacks.\n"
    },
    {
        "paper_id": 2004.13708,
        "authors": "Victor Olkhov",
        "title": "Classical Option Pricing and Some Steps Further",
        "comments": "16 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper considers the asset price p as relations C=pV between the value C\nand the volume V of the executed transactions and studies the consequences of\nthis definition for the option pricing equations. We show that the classical\nBSM model implicitly assumes that value C and volume V of transactions follow\nidentical Brownian processes. Violation of this identity leads to 2-dimensional\nBSM-like equation with two constant volatilities. We show that agents\nexpectations those approve execution of transactions can further increase the\ndimension of the BSM model. We study the case when agents expectations may\ndepend on the option price data and show that such assumption can lead to the\nnonlinear BSM-like equations. We reconsider the Heston stochastic volatility\nmodel for the price determined by the value and the volume and derive\n3-dimensional BSM-like model with stochastic value volatility and constant\nvolume volatility. Variety of the BSM-like equations states the problem of\nreasonable balance between the accuracy and the complexity of the option\npricing equations.\n"
    },
    {
        "paper_id": 2004.13797,
        "authors": "Jackie Jianhong Shen",
        "title": "A Stochastic LQR Model for Child Order Placement in Algorithmic Trading",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Modern Algorithmic Trading (\"Algo\") allows institutional investors and\ntraders to liquidate or establish big security positions in a fully automated\nor low-touch manner. Most existing academic or industrial Algos focus on how to\n\"slice\" a big parent order into smaller child orders over a given time horizon.\nFew models rigorously tackle the actual placement of these child orders.\nInstead, placement is mostly done with a combination of empirical signals and\nheuristic decision processes. A self-contained, realistic, and fully functional\nChild Order Placement (COP) model may never exist due to all the inherent\ncomplexities, e.g., fragmentation due to multiple venues, dynamics of limit\norder books, lit vs. dark liquidity, different trading sessions and rules. In\nthis paper, we propose a reductionism COP model that focuses exclusively on the\ninterplay between placing passive limit orders and sniping using aggressive\ntakeout orders. The dynamic programming model assumes the form of a stochastic\nlinear-quadratic regulator (LQR) and allows closed-form solutions under the\nbackward Bellman equations. Explored in detail are model assumptions and\ngeneral settings, the choice of state and control variables and the cost\nfunctions, and the derivation of the closed-form solutions.\n"
    },
    {
        "paper_id": 2004.13871,
        "authors": "Alan L. Lewis",
        "title": "US Equity Risk Premiums during the COVID-19 Pandemic",
        "comments": "14 pages, 17 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We study equity risk premiums in the United States during the COVID-19\npandemic.\n"
    },
    {
        "paper_id": 2004.13919,
        "authors": "Anuraag Singh, Giorgio Triulzi and Christopher L. Magee",
        "title": "Technological improvement rate estimates for all technologies: Use of\n  patent data and an extended domain description",
        "comments": null,
        "journal-ref": "Technological Improvement Rate Predictions for All Technologies:\n  Use of Patent Data and an Extended Domain Description. Research Policy 50\n  (9): 104294. 2021",
        "doi": "10.1016/j.respol.2021.104294",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this work, we attempt to provide a comprehensive granular account of the\npace of technological change. More specifically, we survey estimated yearly\nperformance improvement rates for nearly all definable technologies for the\nfirst time. We do this by creating a correspondence of all patents within the\nUS patent system to a set of technology domains. A technology domain is a body\nof patented inventions achieving the same technological function using the same\nknowledge and scientific principles. We obtain a set of 1757 domains using an\nextension of the previously defined classification overlap method (COM). These\ndomains contain 97.14% of all patents within the entire US patent system. From\nthe identified patent sets, we calculated the average centrality of the patents\nin each domain to estimate their improvement rates, following a methodology\ntested in prior work. The estimated improvement rates vary from a low of 1.9%\nper year for the Mechanical Skin treatment - Hair Removal and wrinkles domain\nto a high of 228.8% per year for the Network management - client-server\napplications domain. We developed a one-line descriptor identifying the\ntechnological function achieved and the underlying knowledge base for the\nlargest 50, fastest 20 as well as slowest 20 of these domains, which cover more\nthan forty percent of the patent system. In general, the rates of improvement\nwere not a strong function of the patent set size and the fastest improving\ndomains are predominantly software-based. We make available an online system\nthat allows for automated searching for domains and improvement rates\ncorresponding to any technology of interest to researchers, strategists and\npolicy formulators.\n"
    },
    {
        "paper_id": 2004.14048,
        "authors": "Chung-Han Hsieh",
        "title": "On Feedback Control in Kelly Betting: An Approximation Approach",
        "comments": "To appear in the proceedings of the 2020 IEEE Conference on Control\n  Technology and Applications (CCTA)",
        "journal-ref": null,
        "doi": "10.1109/CCTA41146.2020.9206338",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper, we consider a simple discrete-time optimal betting problem\nusing the celebrated Kelly criterion, which calls for maximization of the\nexpected logarithmic growth of wealth. While the classical Kelly betting\nproblem can be solved via standard concave programming technique, an\nalternative but attractive approach is to invoke a Taylor-based approximation,\nwhich recasts the problem into quadratic programming and obtain the closed-form\napproximate solution. The focal point of this paper is to fill some voids in\nthe existing results by providing some interesting properties when such an\napproximate solution is used. Specifically, the best achievable betting\nperformance, positivity of expected cumulative gain or loss and its associated\nvariance, expected growth property, variance of logarithmic growth, and results\nrelated to the so-called survivability (no bankruptcy) are provided.\n"
    },
    {
        "paper_id": 2004.14149,
        "authors": "Lucio Fernandez-Arjona (University of Zurich), Damir Filipovi\\'c (EPFL\n  and Swiss Finance Institute)",
        "title": "A machine learning approach to portfolio pricing and risk management for\n  high-dimensional problems",
        "comments": "26 pages (main), 13 pages (appendix), 3 figures, 20 tables",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We present a general framework for portfolio risk management in discrete\ntime, based on a replicating martingale. This martingale is learned from a\nfinite sample in a supervised setting. The model learns the features necessary\nfor an effective low-dimensional representation, overcoming the curse of\ndimensionality common to function approximation in high-dimensional spaces. We\nshow results based on polynomial and neural network bases. Both offer superior\nresults to naive Monte Carlo methods and other existing methods like\nleast-squares Monte Carlo and replicating portfolios.\n"
    },
    {
        "paper_id": 2004.14485,
        "authors": "Ashish Kumar, Anindya S. Chakrabarti, Anirban Chakraborti, and Tushar\n  Nandi",
        "title": "Distress propagation on production networks: Coarse-graining and\n  modularity of linkages",
        "comments": "18 pages, 11 figures",
        "journal-ref": null,
        "doi": "10.1016/j.physa.2020.125714",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Distress propagation occurs in connected networks, its rate and extent being\ndependent on network topology. To study this, we choose economic production\nnetworks as a paradigm. An economic network can be examined at many levels:\nlinkages among individual agents (microscopic), among firms/sectors\n(mesoscopic) or among countries (macroscopic). New emergent dynamical\nproperties appear at every level, so the granularity matters. For viral\nepidemics, even an individual node may act as an epicenter of distress and\npotentially affect the entire network. Economic networks, however, are known to\nbe immune at the micro-levels and more prone to failure in the\nmeso/macro-levels. We propose a dynamical interaction model to characterize the\nmechanism of distress propagation, across different modules of a network,\ninitiated at different epicenters. Vulnerable modules often lead to large\ndegrees of destabilization. We demonstrate our methodology using a unique\nempirical data-set of input-output linkages across 0.14 million firms in one\nadministrative state of India, a developing economy. The network has multiple\nhub-and-spoke structures that exhibits moderate disassortativity, which varies\nwith the level of coarse-graining. The novelty lies in characterizing the\nproduction network at different levels of granularity or modularity, and\nfinding `too-big-to-fail' modules supersede `too-central-to-fail' modules in\ndistress propagation.\n"
    },
    {
        "paper_id": 2004.14627,
        "authors": "Dingqian Sun",
        "title": "The convergence rate from discrete to continuous optimal investment\n  stopping problem",
        "comments": "22 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We study the optimal investment stopping problem in both continuous and\ndiscrete case, where the investor needs to choose the optimal trading strategy\nand optimal stopping time concurrently to maximize the expected utility of\nterminal wealth. Based on the work [9] with an additional stochastic payoff\nfunction, we characterize the value function for the continuous problem via the\ntheory of quadratic reflected backward stochastic differential equation (BSDE\nfor short) with unbounded terminal condition. In regard to discrete problem, we\nget the discretization form composed of piecewise quadratic BSDEs recursively\nunder Markovian framework and the assumption of bounded obstacle, and provide\nsome useful prior estimates about the solutions with the help of auxiliary\nforward-backward SDE system and Malliavin calculus. Finally, we obtain the\nuniform convergence and relevant rate from discretely to continuously quadratic\nreflected BSDE, which arise from corresponding optimal investment stopping\nproblem through above characterization.\n"
    },
    {
        "paper_id": 2004.14736,
        "authors": "Pietro Murialdo, Linda Ponta, Anna Carbone",
        "title": "Long-Range Dependence in Financial Markets: a Moving Average Cluster\n  Entropy Approach",
        "comments": null,
        "journal-ref": null,
        "doi": "10.3390/e22060634",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  A perspective is taken on the intangible complexity of economic and social\nsystems by investigating the underlying dynamical processes that produce, store\nand transmit information in financial time series in terms of the\n\\textit{moving average cluster entropy}. An extensive analysis has evidenced\nmarket and horizon dependence of the \\textit{moving average cluster entropy} in\nreal world financial assets. The origin of the behavior is scrutinized by\napplying the \\textit{moving average cluster entropy} approach to long-range\ncorrelated stochastic processes as the Autoregressive Fractionally Integrated\nMoving Average (ARFIMA) and Fractional Brownian motion (FBM). To that end, an\nextensive set of series is generated with a broad range of values of the Hurst\nexponent $H$ and of the autoregressive, differencing and moving average\nparameters $p,d,q$. A systematic relation between \\textit{moving average\ncluster entropy}, \\textit{Market Dynamic Index} and long-range correlation\nparameters $H$, $d$ is observed. This study shows that the characteristic\nbehaviour exhibited by the horizon dependence of the cluster entropy is related\nto long-range positive correlation in financial markets. Specifically, long\nrange positively correlated ARFIMA processes with differencing parameter $\nd\\simeq 0.05$, $d\\simeq 0.15$ and $ d\\simeq 0.25$ are consistent with\n\\textit{moving average cluster entropy} results obtained in time series of\nDJIA, S\\&P500 and NASDAQ.\n"
    },
    {
        "paper_id": 2004.14862,
        "authors": "Humayra Shoshi and Indranil SenGupta",
        "title": "Hedging and machine learning driven crude oil data analysis using a\n  refined Barndorff-Nielsen and Shephard model",
        "comments": "27 pages, 9 figures. arXiv admin note: text overlap with\n  arXiv:1911.13300",
        "journal-ref": "International Journal of Financial Engineering, 2021",
        "doi": "10.1142/S2424786321500158",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper, a refined Barndorff-Nielsen and Shephard (BN-S) model is\nimplemented to find an optimal hedging strategy for commodity markets. The\nrefinement of the BN-S model is obtained with various machine and deep learning\nalgorithms. The refinement leads to the extraction of a deterministic parameter\nfrom the empirical data set. The problem is transformed to an appropriate\nclassification problem with a couple of different approaches: the volatility\napproach and the duration approach. The analysis is implemented to the Bakken\ncrude oil data and the aforementioned deterministic parameter is obtained for a\nwide range of data sets. With the implementation of this parameter in the\nrefined model, the resulting model performs much better than the classical BN-S\nmodel.\n"
    },
    {
        "paper_id": 2004.14953,
        "authors": "Daniel Fershtman, Alessandro Pavan",
        "title": "Soft Affirmative Action and Minority Recruitment",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We study search, evaluation, and selection of candidates of unknown quality\nfor a position. We examine the effects of \"soft\" affirmative action policies\nincreasing the relative percentage of minority candidates in the candidate\npool. We show that, while meant to encourage minority hiring, such policies may\nbackfire if the evaluation of minority candidates is noisier than that of\nnon-minorities. This may occur even if minorities are at least as qualified and\nas valuable as non-minorities. The results provide a possible explanation for\nwhy certain soft affirmative action policies have proved counterproductive,\neven in the absence of (implicit) biases.\n"
    },
    {
        "paper_id": 2005.00114,
        "authors": "Nicol\\`o Vallarano, Claudio Tessone, Tiziano Squartini",
        "title": "Bitcoin Transaction Networks: an overview of recent results",
        "comments": "15 pages, 7 figures",
        "journal-ref": "Front. Phys. 8, 286 (2020)",
        "doi": "10.3389/fphy.2020.00286",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Cryptocurrencies are distributed systems that allow exchanges of native (and\nnon-) tokens among participants. The complete historical bookkeeping and its\nwide availability opens up an unprecedented possibility, i.e. that of\nunderstanding the evolution of their network structure while gaining useful\ninsight on the relationships between user' behaviour and cryptocurrency pricing\nin exchange markets. In this contribution we review some of the most recent\nresults concerning the structural properties of Bitcoin Transaction Networks, a\ngeneric name referring to a set of different constructs: the Bitcoin Address\nNetwork, the Bitcoin User Network and the Bitcoin Lightning Network. The\npicture that emerges is that of system growing over time, which becomes\nincreasingly sparse and whose mesoscopic structural organization is\ncharacterised by the presence of an increasingly significant core-periphery\nstructure. Such a peculiar topology is matched by a highly uneven distribution\nof bitcoins, a result suggesting that Bitcoin is becoming an increasingly\ncentralized system at different levels.\n"
    },
    {
        "paper_id": 2005.00137,
        "authors": "Marina Toger, Ian Shuttleworth, John \\\"Osth",
        "title": "How average is average? Temporal patterns in human behaviour as measured\n  by mobile phone data -- or why chose Thursdays",
        "comments": "10 pages, 4 figures, 3 tables",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
        "abstract": "  Mobile phone data -- with file sizes scaling into terabytes -- easily\noverwhelm the computational capacity available to some researchers. Moreover,\nfor ethical reasons, data access is often granted only to particular subsets,\nrestricting analyses to cover single days, weeks, or geographical areas.\nConsequently, it is frequently impossible to set a particular analysis or event\nin its context and know how typical it is, compared to other days, weeks or\nmonths. This is important for academic referees questioning research on mobile\nphone data and for the analysts in deciding how to sample, how much data to\nprocess, and which events are anomalous. All these issues require an\nunderstanding of variability in Big Data to answer the question of how average\nis average? This paper provides a method, using a large mobile phone dataset,\nto answer these basic but necessary questions. We show that file size is a\nrobust proxy for the activity level of phone users by profiling the temporal\nvariability of the data at an hourly, daily and monthly level. We then apply\ntime-series analysis to isolate temporal periodicity. Finally, we discuss\nconfidence limits to anomalous events in the data. We recommend an analytical\napproach to mobile phone data selection which suggests that ideally data should\nbe sampled across days, across working weeks, and across the year, to obtain a\nrepresentative average. However, where this is impossible, the temporal\nvariability is such that specific weekdays' data can provide a fair picture of\nother days in their general structure.\n"
    },
    {
        "paper_id": 2005.00399,
        "authors": "Martin Keller-Ressel, Stephanie Nargang",
        "title": "The hyperbolic geometry of financial networks",
        "comments": "corrected typo in author's name",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Based on data from the European banking stress tests of 2014, 2016 and the\ntransparency exercise of 2018 we demonstrate for the first time that the latent\ngeometry of financial networks can be well-represented by geometry of negative\ncurvature, i.e., by hyperbolic geometry. This allows us to connect the network\nstructure to the popularity-vs-similarity model of Papdopoulos et al., which is\nbased on the Poincar\\'e disc model of hyperbolic geometry. We show that the\nlatent dimensions of `popularity' and `similarity' in this model are strongly\nassociated to systemic importance and to geographic subdivisions of the banking\nsystem. In a longitudinal analysis over the time span from 2014 to 2018 we find\nthat the systemic importance of individual banks has remained rather stable,\nwhile the peripheral community structure exhibits more (but still moderate)\nvariability.\n"
    },
    {
        "paper_id": 2005.00715,
        "authors": "John Dagpunar",
        "title": "Closed-form Solutions for an Explicit Modern Ideal Tontine with Bequest\n  Motive",
        "comments": "21 pages, 6 figures",
        "journal-ref": "Insurance: Mathematics and Economics Volume 100, September 2021,\n  Pages 261-273",
        "doi": "10.1016/j.insmatheco.2021.05.008",
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  In this paper I extend the work of Bernhardt and Donnelly (2019) dealing with\nmodern explicit tontines, as a way of providing income under a specified\nbequest motive, from a defined contribution pension pot. A key feature of the\npresent paper is that it relaxes the assumption of fixed proportions invested\nin tontine and bequest accounts. In making the bequest proportion an additional\ncontrol function I obtain, hitherto unavailable, closed-form solutions for the\nfractional consumption rate, wealth, bequest amount, and bequest proportion\nunder a constant relative risk averse utility. I show that the optimal bequest\nproportion is the product of the optimum fractional consumption rate and an\nexponentiated bequest parameter. I show that under certain circumstances, such\nas a very high bequest motive, a life-cycle utility maximisation strategy will\nnecessitate negative mortality credits analogous to a member paying life\ninsurance premiums. Typical scenarios are explored using UK Office of National\nStatistics life tables.\n"
    },
    {
        "paper_id": 2005.0116,
        "authors": "Piero Mazzarisi, Silvia Zaoli, Carlo Campajola, Fabrizio Lillo",
        "title": "Tail Granger causalities and where to find them: extreme risk spillovers\n  vs. spurious linkages",
        "comments": "22 pages, 7 Figures, 4 Tables",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Identifying risk spillovers in financial markets is of great importance for\nassessing systemic risk and portfolio management. Granger causality in tail (or\nin risk) tests whether past extreme events of a time series help predicting\nfuture extreme events of another time series. The topology and connectedness of\nnetworks built with Granger causality in tail can be used to measure systemic\nrisk and to identify risk transmitters. Here we introduce a novel test of\nGranger causality in tail which adopts the likelihood ratio statistic and is\nbased on the multivariate generalization of a discrete autoregressive process\nfor binary time series describing the sequence of extreme events of the\nunderlying price dynamics. The proposed test has very good size and power in\nfinite samples, especially for large sample size, allows inferring the correct\ntime scale at which the causal interaction takes place, and it is flexible\nenough for multivariate extension when more than two time series are considered\nin order to decrease false detections as spurious effect of neglected\nvariables. An extensive simulation study shows the performances of the proposed\nmethod with a large variety of data generating processes and it introduces also\nthe comparison with the test of Granger causality in tail by [Hong et al.,\n2009]. We report both advantages and drawbacks of the different approaches,\npointing out some crucial aspects related to the false detections of Granger\ncausality for tail events. An empirical application to high frequency data of a\nportfolio of US stocks highlights the merits of our novel approach.\n"
    },
    {
        "paper_id": 2005.01273,
        "authors": "Ritwik Banerjee, Joydeep Bhattacharya, Priyama Majumdar",
        "title": "Exponential-growth prediction bias and compliance with safety measures\n  in the times of COVID-19",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We conduct a unique, Amazon MTurk-based global experiment to investigate the\nimportance of an exponential-growth prediction bias (EGPB) in understanding why\nthe COVID-19 outbreak has exploded. The scientific basis for our inquiry is the\nwell-established fact that disease spread, especially in the initial stages,\nfollows an exponential function meaning few positive cases can explode into a\nwidespread pandemic if the disease is sufficiently transmittable. We define\nprediction bias as the systematic error arising from faulty prediction of the\nnumber of cases x-weeks hence when presented with y-weeks of prior, actual data\non the same. Our design permits us to identify the root of this\nunder-prediction as an EGPB arising from the general tendency to underestimate\nthe speed at which exponential processes unfold. Our data reveals that the\n\"degree of convexity\" reflected in the predicted path of the disease is\nsignificantly and substantially lower than the actual path. The bias is\nsignificantly higher for respondents from countries at a later stage relative\nto those at an early stage of disease progression. We find that individuals who\nexhibit EGPB are also more likely to reveal markedly reduced compliance with\nthe WHO-recommended safety measures, find general violations of safety\nprotocols less alarming, and show greater faith in their government's actions.\nA simple behavioral nudge which shows prior data in terms of raw numbers, as\nopposed to a graph, causally reduces EGPB. Clear communication of risk via raw\nnumbers could increase accuracy of risk perception, in turn facilitating\ncompliance with suggested protective behaviors.\n"
    },
    {
        "paper_id": 2005.01365,
        "authors": "Micha{\\l} Narajewski and Florian Ziel",
        "title": "Ensemble Forecasting for Intraday Electricity Prices: Simulating\n  Trajectories",
        "comments": "accepted for publication in Applied Energy",
        "journal-ref": "Applied Energy 2020, 279",
        "doi": "10.1016/j.apenergy.2020.115801",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Recent studies concerning the point electricity price forecasting have shown\nevidence that the hourly German Intraday Continuous Market is weak-form\nefficient. Therefore, we take a novel, advanced approach to the problem. A\nprobabilistic forecasting of the hourly intraday electricity prices is\nperformed by simulating trajectories in every trading window to receive a\nrealistic ensemble to allow for more efficient intraday trading and redispatch.\nA generalized additive model is fitted to the price differences with the\nassumption that they follow a zero-inflated distribution, precisely a mixture\nof the Dirac and the Student's t-distributions. Moreover, the mixing term is\nestimated using a high-dimensional logistic regression with lasso penalty. We\nmodel the expected value and volatility of the series using i.a. autoregressive\nand no-trade effects or load, wind and solar generation forecasts and\naccounting for the non-linearities in e.g. time to maturity. Both the in-sample\ncharacteristics and forecasting performance are analysed using a rolling window\nforecasting study. Multiple versions of the model are compared to several\nbenchmark models and evaluated using probabilistic forecasting measures and\nsignificance tests. The study aims to forecast the price distribution in the\nGerman Intraday Continuous Market in the last 3 hours of trading, but the\napproach allows for application to other continuous markets, especially in\nEurope. The results prove superiority of the mixture model over the benchmarks\ngaining the most from the modelling of the volatility. They also indicate that\nthe introduction of XBID reduced the market volatility.\n"
    },
    {
        "paper_id": 2005.01686,
        "authors": "Alexander Arimond, Damian Borth, Andreas Hoepner, Michael Klawunn and\n  Stefan Weisheit",
        "title": "Neural Networks and Value at Risk",
        "comments": "2019 Financial Data Science Association Paper, San Francisco",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Utilizing a generative regime switching framework, we perform Monte-Carlo\nsimulations of asset returns for Value at Risk threshold estimation. Using\nequity markets and long term bonds as test assets in the global, US, Euro area\nand UK setting over an up to 1,250 weeks sample horizon ending in August 2018,\nwe investigate neural networks along three design steps relating (i) to the\ninitialization of the neural network, (ii) its incentive function according to\nwhich it has been trained and (iii) the amount of data we feed. First, we\ncompare neural networks with random seeding with networks that are initialized\nvia estimations from the best-established model (i.e. the Hidden Markov). We\nfind latter to outperform in terms of the frequency of VaR breaches (i.e. the\nrealized return falling short of the estimated VaR threshold). Second, we\nbalance the incentive structure of the loss function of our networks by adding\na second objective to the training instructions so that the neural networks\noptimize for accuracy while also aiming to stay in empirically realistic regime\ndistributions (i.e. bull vs. bear market frequencies). In particular this\ndesign feature enables the balanced incentive recurrent neural network (RNN) to\noutperform the single incentive RNN as well as any other neural network or\nestablished approach by statistically and economically significant levels.\nThird, we half our training data set of 2,000 days. We find our networks when\nfed with substantially less data (i.e. 1,000 days) to perform significantly\nworse which highlights a crucial weakness of neural networks in their\ndependence on very large data sets ...\n"
    },
    {
        "paper_id": 2005.01692,
        "authors": "Matthew Olckers",
        "title": "On Track for Retirement?",
        "comments": "Revised version conditionally accepted at Journal of Economics\n  Behavior & Organization",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Over sixty percent of employees at a large South African company contribute\nthe minimum rate of 7.5 percent to a retirement fund, far below the rate of 15\npercent recommended by financial advisers. I use a field experiment to\ninvestigate whether providing employees with a retirement calculator, which\nshows projections of retirement income, leads to increases in contributions.\nThe impact is negligible. The lack of response to the calculator suggests many\nemployees may wish to save less than the minimum. I use a model of asymmetric\ninformation to explain why the employer sets a binding minimum.\n"
    },
    {
        "paper_id": 2005.01706,
        "authors": "Michael C. Nwogugu",
        "title": "Some Issues In Securitization And Disintermediation",
        "comments": null,
        "journal-ref": "Applied Mathematics & Computation, 186(2): 1031-1039 (2007)",
        "doi": "10.1016/j.amc.2006.08.052",
        "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
        "abstract": "  Securitization has become prevalent in many countries, and has substantial\nimpact on government monetary policy and fiscal policy which have not yet been\nadequately analyzed in the existing literature. This article develops optimal\nconditions for efficient securitization, identifies constraints on\nsecuritization, and analyzes the interactions of capital-reserve requirements\nand securitization. This article introduces new decision models and theories of\nasset-securitization.\n"
    },
    {
        "paper_id": 2005.01707,
        "authors": "Michael C. Nwogugu",
        "title": "On The Choice Between A Sale-Leaseback And Debt",
        "comments": null,
        "journal-ref": "Corporate Ownership & Control, 5(4), 326-329 (2008)",
        "doi": "10.22495/cocv5i4c2p6",
        "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
        "abstract": "  This article introduces decision models for commercial real estate leasing.\nThe concepts and models developed in the article can also be applied to\nequipment leasing and other types of leasing.\n"
    },
    {
        "paper_id": 2005.01708,
        "authors": "Michael C. Nwogugu",
        "title": "Decision-Making, Sub-Additive Recursive \"Matching\" Noise And Biases In\n  Risk-Weighted Stock/Bond Index Calculation Methods In Incomplete Markets With\n  Partially Observable Multi-Attribute Preferences",
        "comments": null,
        "journal-ref": "Discrete Mathematics, Algorithms & Applications, 05 (2013)",
        "doi": "10.1142/S1793830913500201",
        "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
        "abstract": "  While Indices, Index tracking funds and ETFs have grown in popularity during\nthen last ten years, there are many structural problems inherent in Index\ncalculation methodologies and the legal/economic structure of ETFs. These\nproblems raise actionable issues of Suitability and fraud under US securities\nlaws, because most Indices and ETFs are misleading, have substantial tracking\nerrors and dont reflect what they are supposed to track. This article\ncontributes to the existing literature by: a) introducing and characterizing\nthe errors and Biases inherent in risk-adjusted index weighting methods and the\nassociated adverse effects; b) showing how these biases/effects inherent in\nIndex calculation methods reduce social welfare, and can form the basis for\nharmful arbitrage activities.\n"
    },
    {
        "paper_id": 2005.01709,
        "authors": "Michael Nwogugu",
        "title": "Regret Theory And Asset Pricing Anomalies In Incomplete Markets With\n  Dynamic Un-Aggregated Preferences",
        "comments": "Chapter-3 in: Nwogugu, M. (2017). Anomalies in Net Present Value,\n  Returns and Polynomials, and Regret Theory in Decision-Making (Palgrave\n  MacMillan). Electronic ISBN: 978-1-137-44698-5",
        "journal-ref": null,
        "doi": "10.1057/978-1-137-44698-5",
        "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
        "abstract": "  Although the CML (Capital Market Line), the Intertemporal-CAPM, the CAPM/SML\n(Security Market Line) and the Intertemporal Arbitrage Pricing Theory (IAPT)\nare widely used in portfolio management, valuation and capital markets\nfinancing; these theories are inaccurate and can adversely affect risk\nmanagement and portfolio management processes. This article introduces several\nempirically testable financial theories that provide insights, and can be\ncalibrated to real data and used to solve problems, and contributes to the\nliterature by: i) explaining the conditions under which ICAPM/CAPM, IAPT and\nCML may be accurate, and why such conditions are not feasible; and explaining\nwhy the existence of incomplete markets and dynamic un-aggregated markets\nrender CML, IAPT and ICAPM inaccurate; ii) explaining why the\nConsumption-Savings-InvestmentProduction framework is insufficient for asset\npricing and analysis of changes in risk and asset values; and introducing a\nunified approach to asset pricing that simultaneously considers six factors,\nand the conditions under which this approach will work; iii) explaining why\nleisure, taxes and housing are equally as important as consumption and\ninvestment in asset pricing; iv) introducing the Marginal Rate of Intertemporal\nJoint Substitution (MRIJS) among Consumption, Taxes, Investment, Leisure,\nIntangibles and Housing - this model incorporates Regret Theory and captures\nfeatures of reality that dont fit well into standard asset pricing models, and\nthis framework can support specific or very general finance theories and or\nvery complicated models; v) showing why the Elasticity of Intertemporal\nSubstitution (EIS) is inaccurate and is insufficient for asset pricing and\nanalysis of investor preferences.\n"
    },
    {
        "paper_id": 2005.0171,
        "authors": "Michael C. Nwogugu",
        "title": "Issues In Disintermediation In The Real Estate Brokerage Sector",
        "comments": null,
        "journal-ref": "Applied Mathematics & Computation, 186(2), 1054-1064 (2007)",
        "doi": "10.1016/j.amc.2006.08.053",
        "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
        "abstract": "  This article introduces new models of disintermediation of the real estate\nbroker by the buyer or the seller. The decision to retain a real estate broker\nis critical in the property purchase/sale process. The existing literature does\nnot contain analysis of: 1) information asymmetry, 2) the conditions under\nwhich it will be optimal to disintermediate the broker, 3) social capital and\nreputation, 4) the impact of different types of real estate brokerage\ncontracts. The article shows that dis-intermediation of the real estate broker\nby the seller or buyer may be optimal in certain conditions.\n"
    },
    {
        "paper_id": 2005.01882,
        "authors": "Torsten Heinrich and Jangho Yang and Shuanping Dai",
        "title": "Levels of structural change: An analysis of China's development push\n  1998-2014",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We investigate structural change in the PR China during a period of\nparticularly rapid growth 1998-2014. For this, we utilize sectoral data from\nthe World Input-Output Database and firm-level data from the Chinese Industrial\nEnterprise Database. Starting with correlation laws known from the literature\n(Fabricant's laws), we investigate which empirical regularities hold at the\nsectoral level and show that many of these correlations cannot be recovered at\nthe firm level. For a more detailed analysis, we propose a multi-level\nframework, which is validated with empirically. For this, we perform a robust\nregression, since various input variables at the firm-level as well as the\nresiduals of exploratory OLS regressions are found to be heavy-tailed. We\nconclude that Fabricant's laws and other regularities are primarily\ncharacteristics of the sectoral level which rely on aspects like\ninfrastructure, technology level, innovation capabilities, and the knowledge\nbase of the relevant labor force. We illustrate our analysis by showing the\ndevelopment of some of the larger sectors in detail and offer some policy\nimplications in the context of development economics, evolutionary economics,\nand industrial organization.\n"
    },
    {
        "paper_id": 2005.01904,
        "authors": "Shuzhen Yang",
        "title": "Bellman type strategy for the continuous time mean-variance model",
        "comments": "30 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  To investigate a time-consistent optimal strategy for the continuous time\nmean-variance model, we develop a new method to establish the Bellman\nprinciple. Based on this new method, we obtain a time-consistent dynamic\noptimal strategy that differs from the pre-committed and game-theoretic\nstrategies. A comparison with the existing results on the continuous time\nmean-variance model shows that our method has several advantages. The explicit\nsolutions of the dynamic optimal strategy and optimal wealth are given. When\nthe dynamic optimal strategy is given at the initial time, we do not change it\nin the following investment time interval.\n"
    },
    {
        "paper_id": 2005.02217,
        "authors": "Manuel Nunes, Enrico Gerding, Frank McGroarty, Mahesan Niranjan",
        "title": "Long short-term memory networks and laglasso for bond yield forecasting:\n  Peeping inside the black box",
        "comments": "27 pages, 16 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Modern decision-making in fixed income asset management benefits from\nintelligent systems, which involve the use of state-of-the-art machine learning\nmodels and appropriate methodologies. We conduct the first study of bond yield\nforecasting using long short-term memory (LSTM) networks, validating its\npotential and identifying its memory advantage. Specifically, we model the\n10-year bond yield using univariate LSTMs with three input sequences and five\nforecasting horizons. We compare those with multilayer perceptrons (MLP),\nunivariate and with the most relevant features. To demystify the notion of\nblack box associated with LSTMs, we conduct the first internal study of the\nmodel. To this end, we calculate the LSTM signals through time, at selected\nlocations in the memory cell, using sequence-to-sequence architectures, uni and\nmultivariate. We then proceed to explain the states' signals using exogenous\ninformation, for what we develop the LSTM-LagLasso methodology. The results\nshow that the univariate LSTM model with additional memory is capable of\nachieving similar results as the multivariate MLP using macroeconomic and\nmarket information. Furthermore, shorter forecasting horizons require smaller\ninput sequences and vice-versa. The most remarkable property found consistently\nin the LSTM signals, is the activation/deactivation of units through time, and\nthe specialisation of units by yield range or feature. Those signals are\ncomplex but can be explained by exogenous variables. Additionally, some of the\nrelevant features identified via LSTM-LagLasso are not commonly used in\nforecasting models. In conclusion, our work validates the potential of LSTMs\nand methodologies for bonds, providing additional tools for financial\npractitioners.\n"
    },
    {
        "paper_id": 2005.02283,
        "authors": "R. Mansilla",
        "title": "How to manage the post pandemic opening? A Pontryagin Maximum Principle\n  approach",
        "comments": "10 pages",
        "journal-ref": "Applied Economics and Finance, vol. 7(4), pp. 121-125, 2020",
        "doi": "10.11114/aef.v7i4.4912",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The COVID-19 pandemic has completely disrupted the operation of our\nsocieties. Its elusive transmission process, characterized by an unusually long\nincubation period, as well as a high contagion capacity, has forced many\ncountries to take quarantine and social isolation measures that conspire\nagainst the performance of national economies. This situation confronts\ndecision makers in different countries with the alternative of reopening the\neconomies, thus facing the unpredictable cost of a rebound of the infection.\nThis work tries to offer an initial theoretical framework to handle this\nalternative.\n"
    },
    {
        "paper_id": 2005.02318,
        "authors": "Lucio Fernandez-Arjona",
        "title": "A neural network model for solvency calculations in life insurance",
        "comments": "20 pages, 6 figures, 4 tables",
        "journal-ref": "Ann. actuar. sci. 15 (2021) 259-275",
        "doi": "10.1017/S1748499520000330",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Insurance companies make extensive use of Monte Carlo simulations in their\ncapital and solvency models. To overcome the computational problems associated\nwith Monte Carlo simulations, most large life insurance companies use proxy\nmodels such as replicating portfolios.\n  In this paper, we present an example based on a variable annuity guarantee,\nshowing the main challenges faced by practitioners in the construction of\nreplicating portfolios: the feature engineering step and subsequent basis\nfunction selection problem.\n  We describe how neural networks can be used as a proxy model and how to apply\nrisk-neutral pricing on a neural network to integrate such a model into a\nmarket risk framework. The proposed model naturally solves the feature\nengineering and feature selection problems of replicating portfolios.\n"
    },
    {
        "paper_id": 2005.02337,
        "authors": "J{\\o}rgen Vitting Andersen and Philippe de Peretti",
        "title": "Heuristics in experiments with infinitely large strategy spaces",
        "comments": "29 pages, 6 figures, 10 tables",
        "journal-ref": "Journal of Business Research (2020)",
        "doi": "10.1016/j.jbusres.2019.12.034",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We introduce a new methodology that enables detection of the onset of\nconvergence towards Nash equilibria in simple repeated games with infinitely\nlarge strategy spaces, thereby revealing the heuristics used in\ndecision-making. The method works by constraining on a special finite subset of\nstrategies, called decoupled strategies. We show how the technique can be\napplied to understand price formation in financial market experiments by\nintroducing a predictive measure {\\Delta}D: the different between positive\ndecoupled strategies (recommending to buy) and negative decoupled strategies\n(recommending to sell). Using {\\Delta}D we illustrate how the method can\npredict (at certain special times) participants' actions with a high success\nrate in a series of experiments\n"
    },
    {
        "paper_id": 2005.02347,
        "authors": "Brian Huge and Antoine Savine",
        "title": "Differential Machine Learning",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Differential machine learning combines automatic adjoint differentiation\n(AAD) with modern machine learning (ML) in the context of risk management of\nfinancial Derivatives. We introduce novel algorithms for training fast,\naccurate pricing and risk approximations, online, in real-time, with\nconvergence guarantees. Our machinery is applicable to arbitrary Derivatives\ninstruments or trading books, under arbitrary stochastic models of the\nunderlying market variables. It effectively resolves computational bottlenecks\nof Derivatives risk reports and capital calculations.\n  Differential ML is a general extension of supervised learning, where ML\nmodels are trained on examples of not only inputs and labels but also\ndifferentials of labels wrt inputs. It is also applicable in many situations\noutside finance, where high quality first-order derivatives wrt training inputs\nare available. Applications in Physics, for example, may leverage differentials\nknown from first principles to learn function approximations more effectively.\n  In finance, AAD computes pathwise differentials with remarkable efficacy so\ndifferential ML algorithms provide extremely effective pricing and risk\napproximations. We can produce fast analytics in models too complex for closed\nform solutions, extract the risk factors of complex transactions and trading\nbooks, and effectively compute risk management metrics like reports across a\nlarge number of scenarios, backtesting and simulation of hedge strategies, or\nregulations like XVA, CCR, FRTB or SIMM-MVA.\n  TensorFlow implementation is available on\nhttps://github.com/differential-machine-learning\n"
    },
    {
        "paper_id": 2005.02351,
        "authors": "Naji Massad and J{\\o}rgen Vitting Andersen",
        "title": "Defining an intrinsic stickiness parameter of stock price returns",
        "comments": "22 pages, 4 figures, 3 tables",
        "journal-ref": "Physica A (2020)",
        "doi": "10.1016/j.physa.2020.124464",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We introduce a non linear pricing model of individual stock returns that\ndefines a stickiness parameter of the returns. The pricing model resembles the\ncapital asset pricing model used in finance but has a non linear component\ninspired from models of earth quake tectonic plate movements. The link to\ntectonic plate movements happens, since price movements of a given stock index\nis seen adding stress to its components of individual stock returns, in order\nto follow the index. How closely individual stocks follow the indexs price\nmovements, can then be used to define their stickiness\n"
    },
    {
        "paper_id": 2005.02482,
        "authors": "Abhijit Chakraborty, Soumya Easwaran and Sitabhra Sinha",
        "title": "Uncovering the hierarchical structure of the international FOREX market\n  by using similarity metric between the fluctuation distributions of\n  currencies",
        "comments": "10 pages, 5 figures. arXiv admin note: substantial text overlap with\n  arXiv:1606.06111",
        "journal-ref": "Acta Physica Polonica, A. . Jul2020, Vol. 138 Issue 1, p105-115.\n  11p",
        "doi": "10.12693/APhysPolA.138.105",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The decentralized international market of currency trading is a prototypical\ncomplex system having a highly heterogeneous composition. To understand the\nhierarchical structure relating the price movement of different currencies in\nthe market, we have focused on quantifying the degree of similarity between the\ndistributions of exchange rate fluctuations. For this purpose we use a metric\nconstructed using the Jensen-Shannon divergence between the normalized\nlogarithmic return distributions of the different currencies. This provides a\nnovel method for revealing associations between currencies in terms of the\nstatistical nature of their rate fluctuations, which is distinct from the\nconventional correlation-based methods. The resulting clusters are consistent\nwith the nature of the underlying economies but also show striking divergences\nduring periods of major international crises.\n"
    },
    {
        "paper_id": 2005.02505,
        "authors": "Christa Cuchiero and Wahid Khosrawi and Josef Teichmann",
        "title": "A generative adversarial network approach to calibration of local\n  stochastic volatility models",
        "comments": "Replacement for previous version: Major update of previous version to\n  match the content of the published version",
        "journal-ref": "Risks 2020, 8, 101",
        "doi": "10.3390/risks8040101",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We propose a fully data-driven approach to calibrate local stochastic\nvolatility (LSV) models, circumventing in particular the ad hoc interpolation\nof the volatility surface. To achieve this, we parametrize the leverage\nfunction by a family of feed-forward neural networks and learn their parameters\ndirectly from the available market option prices. This should be seen in the\ncontext of neural SDEs and (causal) generative adversarial networks: we\ngenerate volatility surfaces by specific neural SDEs, whose quality is assessed\nby quantifying, possibly in an adversarial manner, distances to market prices.\nThe minimization of the calibration functional relies strongly on a variance\nreduction technique based on hedging and deep hedging, which is interesting in\nits own right: it allows the calculation of model prices and model implied\nvolatilities in an accurate way using only small sets of sample paths. For\nnumerical illustration we implement a SABR-type LSV model and conduct a\nthorough statistical performance analysis on many samples of implied volatility\nsmiles, showing the accuracy and stability of the method.\n"
    },
    {
        "paper_id": 2005.02527,
        "authors": "Tian Guo, Nicolas Jamet, Valentin Betrix, Louis-Alexandre Piquet,\n  Emmanuel Hauptmann",
        "title": "ESG2Risk: A Deep Learning Framework from ESG News to Stock Volatility\n  Prediction",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Incorporating environmental, social, and governance (ESG) considerations into\nsystematic investments has drawn numerous attention recently. In this paper, we\nfocus on the ESG events in financial news flow and exploring the predictive\npower of ESG related financial news on stock volatility. In particular, we\ndevelop a pipeline of ESG news extraction, news representations, and Bayesian\ninference of deep learning models. Experimental evaluation on real data and\ndifferent markets demonstrates the superior predicting performance as well as\nthe relation of high volatility prediction to stocks with potential high risk\nand low return. It also shows the prospect of the proposed pipeline as a\nflexible predicting framework for various textual data and target variables.\n"
    },
    {
        "paper_id": 2005.02633,
        "authors": "Alessandro Gnoatto, Athena Picarelli, Christoph Reisinger",
        "title": "Deep xVA solver -- A neural network based counterparty credit risk\n  management framework",
        "comments": "35 pages. Accepted on SIAM Journal on Financial Mathematics",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper, we present a novel computational framework for portfolio-wide\nrisk management problems, where the presence of a potentially large number of\nrisk factors makes traditional numerical techniques ineffective. The new method\nutilises a coupled system of BSDEs for the valuation adjustments (xVA) and\nsolves these by a recursive application of a neural network based BSDE solver.\nThis not only makes the computation of xVA for high-dimensional problems\nfeasible, but also produces hedge ratios and dynamic risk measures for xVA, and\nallows simulations of the collateral account.\n"
    },
    {
        "paper_id": 2005.0295,
        "authors": "Takaaki Koike and Marius Hofert",
        "title": "Modality for Scenario Analysis and Maximum Likelihood Allocation",
        "comments": "41 pages, 4 figures, 4 tables",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We study the variability of a risk from the statistical viewpoint of\nmultimodality of the conditional loss distribution given that the aggregate\nloss equals an exogenously provided capital. This conditional distribution\nserves as a building block for calculating risk allocations such as the Euler\ncapital allocation of Value-at-Risk. A superlevel set of this conditional\ndistribution can be interpreted as a set of severe and plausible stress\nscenarios the given capital is supposed to cover. We show that various\ndistributional properties of this conditional distribution, such as modality,\ndependence and tail behavior, are inherited from those of the underlying joint\nloss distribution. Among these properties, we find that modality of the\nconditional distribution is an important feature in risk assessment related to\nthe variety of risky scenarios likely to occur in a stressed situation. Under\nunimodality, we introduce a novel risk allocation method called maximum\nlikelihood allocation (MLA), defined as the mode of the conditional\ndistribution given the total capital. Under multimodality, a single vector of\nallocations can be less sound. To overcome this issue, we investigate the\nso-called multimodalty adjustment to increase the soundness of risk\nallocations. Properties of the conditional distribution, MLA and multimodality\nadjustment are demonstrated in numerical experiments. In particular, we observe\nthat negative dependence among losses typically leads to multimodality, and\nthus a higher multimodality adjustment can be required.\n"
    },
    {
        "paper_id": 2005.02953,
        "authors": "Rafael Felipe Carmargo Prudencio, Christian D. J\\\"akel",
        "title": "The Pricing of Quanto Options: An empirical copula approach",
        "comments": "Some issues with the generated pdf-file have been addressed",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The quanto option is a cross-currency derivative in which the pay-off is\ngiven in foreign currency and then converted to domestic currency, through a\nconstant exchange rate, used for the conversion and determined at contract\ninception. Hence, the dependence relation between the option underlying asset\nprice and the exchange rate plays an important role in quanto option pricing.\n  In this work, we suggest to use empirical copulas to price quanto options.\nNumerical illustrations show that the flexibility provided by this approach,\nconcerning the dependence relation of the two underlying stochastic processes,\nresults in non-negligible pricing differences when contrasted to other models.\n"
    },
    {
        "paper_id": 2005.0301,
        "authors": "Jizhou Huang, Haifeng Wang, Haoyi Xiong, Miao Fan, An Zhuo, Ying Li,\n  Dejing Dou",
        "title": "Quantifying the Economic Impact of COVID-19 in Mainland China Using\n  Human Mobility Data",
        "comments": "29 pages, 10 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  To contain the pandemic of coronavirus (COVID-19) in Mainland China, the\nauthorities have put in place a series of measures, including quarantines,\nsocial distancing, and travel restrictions. While these strategies have\neffectively dealt with the critical situations of outbreaks, the combination of\nthe pandemic and mobility controls has slowed China's economic growth,\nresulting in the first quarterly decline of Gross Domestic Product (GDP) since\nGDP began to be calculated, in 1992. To characterize the potential shrinkage of\nthe domestic economy, from the perspective of mobility, we propose two new\neconomic indicators: the New Venues Created (NVC) and the Volumes of Visits to\nVenue (V^3), as the complementary measures to domestic investments and\nconsumption activities, using the data of Baidu Maps. The historical records of\nthese two indicators demonstrated strong correlations with the past figures of\nChinese GDP, while the status quo has dramatically changed this year, due to\nthe pandemic. We hereby presented a quantitative analysis to project the impact\nof the pandemic on economies, using the recent trends of NVC and V^3. We found\nthat the most affected sectors would be travel-dependent businesses, such as\nhotels, educational institutes, and public transportation, while the sectors\nthat are mandatory to human life, such as workplaces, residential areas,\nrestaurants, and shopping sites, have been recovering rapidly. Analysis at the\nprovincial level showed that the self-sufficient and self-sustainable economic\nregions, with internal supplies, production, and consumption, have recovered\nfaster than those regions relying on global supply chains.\n"
    },
    {
        "paper_id": 2005.03204,
        "authors": "Michael Curran, Patrick O'Sullivan and Ryan Zalla",
        "title": "Can Volatility Solve the Naive Portfolio Puzzle?",
        "comments": "36 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We investigate whether sophisticated volatility estimation improves the\nout-of-sample performance of mean-variance portfolio strategies relative to the\nnaive 1/N strategy. The portfolio strategies rely solely upon second moments.\nUsing a diverse group of econometric and portfolio models across multiple\ndatasets, most models achieve higher Sharpe ratios and lower portfolio\nvolatility that are statistically and economically significant relative to the\nnaive rule, even after controlling for turnover costs. Our results suggest\nbenefits to employing more sophisticated econometric models than the sample\ncovariance matrix, and that mean-variance strategies often outperform the naive\nportfolio across multiple datasets and assessment criteria.\n"
    },
    {
        "paper_id": 2005.0334,
        "authors": "Claude Martini, Arianna Mingone",
        "title": "No arbitrage SVI",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We fully characterize the absence of Butterfly arbitrage in the SVI formula\nfor implied total variance proposed by Gatheral in 2004. The main ingredient is\nan intermediary characterization of the necessary condition for no arbitrage\nobtained for any model by Fukasawa in 2012 that the inverse functions of the\n-d1 and -d2 of the Black-Scholes formula, viewed as functions of the\nlog-forward moneyness, should be increasing. A natural rescaling of the SVI\nparameters and a meticulous analysis of the Durrleman condition allow then to\nobtain simple range conditions on the parameters. This leads to a\nstraightforward implementation of a least-squares calibration algorithm on the\nno arbitrage domain, which yields an excellent fit on the market data we used\nfor our tests, with the guarantee to yield smiles with no Butterfly arbitrage.\n"
    },
    {
        "paper_id": 2005.03464,
        "authors": "Fabian Stockl, Wolf-Peter Schill, Alexander Zerrahn",
        "title": "Optimal supply chains and power sector benefits of green hydrogen",
        "comments": "This version includes a few minor editorial corrections and a change\n  of the title in order to perfectly resemble the version published in\n  Scientific Reports",
        "journal-ref": "Scientific Reports, 92511 (2021)",
        "doi": "10.1038/s41598-021-92511-6",
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Green hydrogen can help to decarbonize parts of the transportation sector,\nbut its power sector interactions are not well understood. It may contribute to\nintegrating variable renewable energy sources if production is sufficiently\nflexible in time. Using an open-source co-optimization model of the power\nsector and four options for supplying hydrogen at German filling stations, we\nfind a trade-off between energy efficiency and temporal flexibility: for lower\nshares of renewables and hydrogen, more energy-efficient and less flexible\nsmall-scale on-site electrolysis is optimal. For higher shares of renewables\nand/or hydrogen, more flexible but less energy-efficient large-scale hydrogen\nsupply chains gain importance as they allow disentangling hydrogen production\nfrom demand via storage. Liquid hydrogen emerges as particularly beneficial,\nfollowed by liquid organic hydrogen carriers and gaseous hydrogen. Large-scale\nhydrogen supply chains can deliver substantial power sector benefits, mainly\nthrough reduced renewable surplus generation. Energy modelers and system\nplanners should consider the distinct flexibility characteristics of hydrogen\nsupply chains in more detail when assessing the role of green hydrogen in\nfuture energy transition scenarios.\n"
    },
    {
        "paper_id": 2005.03491,
        "authors": "Neil W Bailey, Daniel West",
        "title": "Are the COVID19 restrictions really worth the cost? A comparison of\n  estimated mortality in Australia from COVID19 and economic recession",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
        "abstract": "  There has been considerable public debate about whether the economic impact\nof the current COVID19 restrictions are worth the costs. Although the potential\nimpact of COVID19 has been modelled extensively, very few numbers have been\npresented in the discussions about potential economic impacts. For a good\nanswer to the question - will the restrictions cause as much harm as COVID19? -\ncredible evidence-based estimates are required, rather than simply rhetoric.\nHere we provide some preliminary estimates to compare the impact of the current\nrestrictions against the direct impact of the virus. Since most countries are\ncurrently taking an approach that reduces the number of COVID19 deaths, the\nestimates we provide for deaths from COVID19 are deliberately taken from the\nlow end of the estimates of the infection fatality rate, while estimates for\ndeaths from an economic recession are deliberately computed from double the\nhigh end of confidence interval for severe economic recessions. This ensures\nthat an adequate challenge to the status quo of the current restrictions is\nprovided. Our analysis shows that strict restrictions to eradicate the virus\nare likely to lead to at least eight times fewer total deaths than an immediate\nreturn to work scenario.\n"
    },
    {
        "paper_id": 2005.035,
        "authors": "Benjamin Avanzi and Gregory Clive Taylor and Phuong Anh Vu and Bernard\n  Wong",
        "title": "On unbalanced data and common shock models in stochastic loss reserving",
        "comments": null,
        "journal-ref": "Ann. actuar. sci. 15 (2021) 173-203",
        "doi": "10.1017/S1748499520000196",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Introducing common shocks is a popular dependence modelling approach, with\nsome recent applications in loss reserving. The main advantage of this approach\nis the ability to capture structural dependence coming from known\nrelationships. In addition, it helps with the parsimonious construction of\ncorrelation matrices of large dimensions. However, complications arise in the\npresence of \"unbalanced data\", that is, when (expected) magnitude of\nobservations over a single triangle, or between triangles, can vary\nsubstantially. Specifically, if a single common shock is applied to all of\nthese cells, it can contribute insignificantly to the larger values and/or\nswamp the smaller ones, unless careful adjustments are made. This problem is\nfurther complicated in applications involving negative claim amounts. In this\npaper, we address this problem in the loss reserving context using a common\nshock Tweedie approach for unbalanced data. We show that the solution not only\nprovides a much better balance of the common shock proportions relative to the\nunbalanced data, but it is also parsimonious. Finally, the common shock Tweedie\nmodel also provides distributional tractability.\n"
    },
    {
        "paper_id": 2005.03554,
        "authors": "Yerkin Kitapbayev, Scott Robertson",
        "title": "Mortgage Contracts and Underwater Default",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We analyze recently proposed mortgage contracts that aim to eliminate\nselective borrower default when the loan balance exceeds the house price (the\n``underwater'' effect). We show contracts that automatically reduce the\noutstanding balance in the event of house price decline remove the default\nincentive, but may induce prepayment in low price states. However, low state\nprepayments vanish if the benefit from home ownership is sufficiently high. We\nalso show that capital gain sharing features, such as prepayment penalties in\nhigh house price states, are ineffective as they virtually eliminate\nprepayment. For observed foreclosure costs, we find that contracts with\nautomatic balance adjustments become preferable to the traditional fixed-rate\ncontracts at mortgage rate spreads between 20-50 basis points. We obtain these\nresults for perpetual versions of the contracts using American options pricing\nmethodology, in a continuous-time model with diffusive home prices. The\ncontracts' values and optimal decision rules are associated with free boundary\nproblems, which admit semi-explicit solutions.\n"
    },
    {
        "paper_id": 2005.03698,
        "authors": "Dirk Tasche",
        "title": "Proving prediction prudence",
        "comments": "21 pages, 2 appendices",
        "journal-ref": "Data Science in Finance and Economics, 2022, 2(4):359-379",
        "doi": "10.3934/DSFE.2022017",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We study how to perform tests on samples of pairs of observations and\npredictions in order to assess whether or not the predictions are prudent.\nPrudence requires that that the mean of the difference of the\nobservation-prediction pairs can be shown to be significantly negative. For\nsafe conclusions, we suggest testing both unweighted (or equally weighted) and\nweighted means and explicitly taking into account the randomness of individual\npairs. The test methods presented are mainly specified as bootstrap and normal\napproximation algorithms. The tests are general but can be applied in\nparticular in the area of credit risk, both for regulatory and accounting\npurposes.\n"
    },
    {
        "paper_id": 2005.03843,
        "authors": "P\\'ia Amigo, Sebasti\\'an Cea-Echenique, Felipe Feijoo",
        "title": "An Emissions Trading System to reach NDC targets in the Chilean electric\n  sector",
        "comments": null,
        "journal-ref": null,
        "doi": "10.1016/j.energy.2021.120129",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In the context of the Paris Agreement, Chile has pledged to reduce Greenhouse\nGases (GHG) intensity by at least 30% below 2007 levels by 2030, and to phase\nout coal as a energy source by 2040, among other strategies. In pursue of these\ngoals, Chile has implemented a $5 per tonne of CO2 emission tax, first of its\nkind in Latin America. However, such a low price has proven to be insufficient.\nIn our work, we study an alternative approach for capping and pricing carbon\nemissions in the Chilean electric sector; the cap and trade paradigm. We model\nthe Chilean electric market (generators and emissions auctioneer) as a two\nstage capacity expansion equilibrium problem, where we allow future investment\nand trading of emission permits among generator agents. The model studies\ngeneration and future investments in the Chilean electric sector in two regimes\nof demand: deterministic and stochastic. We show that the current Chilean\nGreenhouse Gases (GHG) intensity pledge does not drive an important shift in\nthe future Chilean electric matrix. To encourage a shift to greener\ntechnologies, a more stringent carbon budget must be considered, resulting in a\ncarbon price approximately ten times higher than the present one. We also show\nthat achieving the emissions reduction goal does not necessarily results in\nfurther reductions of carbon generation, or phasing out coal in the longer\nterm. Finally, we demonstrate that under technology change costs reductions,\nhigher demand scenarios will relax the need for stringent carbon budgets to\nachieve new renewable energy investments and hence meet the Chilean pledges.\nThese results suggest that some aspects of the Chilean pledge require further\nanalysis, of the economic impact, particularly with the recent announcement of\nachieving carbon neutrality towards 2050.\n"
    },
    {
        "paper_id": 2005.03963,
        "authors": "Tristan Millington, Mahesan Niranjan",
        "title": "Construction of Minimum Spanning Trees from Financial Returns using Rank\n  Correlation",
        "comments": null,
        "journal-ref": null,
        "doi": "10.1016/j.physa.2020.125605",
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  The construction of minimum spanning trees (MSTs) from correlation matrices\nis an often used method to study relationships in the financial markets.\nHowever most of the work on this topic tends to use the Pearson correlation\ncoefficient, which relies on the assumption of normality and can be brittle to\nthe presence of outliers, neither of which is ideal for the study of financial\nreturns. In this paper we study the inference of MSTs from daily US, UK and\nGerman financial returns using Pearson and two rank correlation methods,\nSpearman and Kendall's $\\tau$. MSTs constructed using these rank methods tend\nto be more stable and maintain more edges over the dataset than those\nconstructed using Pearson correlation. The edge agreement between the Pearson\nand rank MSTs varies significantly depending on the state of the markets, but\nthe rank MSTs generally show strong agreement at all times. Deviation from\nunivariate normality can be related to changes in the correlation matrices but\nis more difficult to connect to changes in the MSTs. Irrelevant of coefficient,\nthe trees tend to have similar topologies. Portfolios constructed from the MST\ncorrelation matrices have a smaller turnover than those from the full\ncovariance matrix for the larger markets, but not for the smaller German\nmarket. Using a bootstrap method we find that the correlation matrices\nconstructed using the rank correlations are more robust, but there is little\ndifference between the robustness of the MSTs.\n"
    },
    {
        "paper_id": 2005.03969,
        "authors": "Karina Arias-Calluari, Fernando Alonso-Marroquin, Morteza\n  Nattagh-Najafi and Michael Harr\\'e",
        "title": "Methods for forecasting the effect of exogenous risk on stock markets",
        "comments": "9 pages, 8 figures, journal paper",
        "journal-ref": null,
        "doi": "10.1016/j.physa.2020.125587",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Markets are subjected to both endogenous and exogenous risks that have caused\ndisruptions to financial and economic markets around the globe, leading\neventually to fast stock market declines. In the past, markets have recovered\nafter any economic disruption. On this basis, we focus on the outbreak of\nCOVID-19 as a case study of an exogenous risk and analyze its impact on the\nStandard and Poor's 500 (S\\&P500) index. We assumed that the S\\&P500 index\nreaches a minimum before rising again in the not-too-distant future. Here we\npresent two cases to forecast the S\\&P500 index. The first case uses an\nestimation of expected deaths released on 02/04/2020 by the University of\nWashington. For the second case, it is assumed that the peak number of deaths\nwill occur 2-months since the first confirmed case occurred in the USA. The\ndecline and recovery in the index were estimated for the following three months\nafter the initial point of the predicted trend. The forecast is a projection of\na prediction with stochastic fluctuations described by $q$-gaussian diffusion\nprocess with three spatio-temporal regimes. Our forecast was made on the\npremise that any market response can be decomposed into an overall\ndeterministic trend and a stochastic term. The prediction was based on the\ndeterministic part and for this case study is approximated by the extrapolation\nof the S\\&P500 data trend in the initial stages of the outbreak. The stochastic\nfluctuations have the same structure as the one derived from the past 24 years.\nA reasonable forecast was achieved with 85\\% of accuracy.\n"
    },
    {
        "paper_id": 2005.04297,
        "authors": "Yuri F. Saporito",
        "title": "Pricing Path-Dependent Derivatives under Multiscale Stochastic\n  Volatility Models: a Malliavin Representation",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper we derive a efficient Monte Carlo approximation for the price\nof path-dependent derivatives under the multiscale stochastic volatility models\nof Fouque \\textit{et al}. Using the formulation of this pricing problem under\nthe functional It\\^o calculus framework and making use of Greek formulas from\nMalliavin calculus, we derive a representation for the first-order\napproximation of the price of path-dependent derivatives in the form\n$\\mathbb{E}[\\mbox{payoff} \\times \\mbox{weight}]$. The weight is known in closed\nform and depends only on the group market parameters arising from the\ncalibration of the multiscale stochastic volatility to the market's implied\nvolatility. Moreover, only simulations of the Black-Scholes model is required.\nWe exemplify the method for a couple path-dependent derivatives.\n"
    },
    {
        "paper_id": 2005.04312,
        "authors": "Thai Nguyen and Mitja Stadje",
        "title": "Utility maximization under endogenous pricing",
        "comments": "An earlier draft of the paper was disseminated under the title\n  \"Forward BSDEs and backward SPDEs for utility maximization under endogenous\n  pricing\"",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We study the expected utility maximization problem of a large investor who is\nallowed to make transactions on tradable assets in an incomplete financial\nmarket with endogenous permanent market impacts. The asset prices are assumed\nto follow a nonlinear price curve quoted in the market as the utility\nindifference curve of a representative liquidity supplier. We show that\noptimality can be fully characterized via a system of coupled forward-backward\nstochastic differential equations (FBSDEs) which corresponds to a non-linear\nbackward stochastic partial differential equation (BSPDE). We show existence of\nsolutions to the optimal investment problem and the FBSDEs in the case where\nthe driver function of the representative market maker grows at least\nquadratically or the utility function of the large investor falls faster than\nquadratically or is exponential. Furthermore, we derive smoothness results for\nthe existence of solutions of BSPDEs. Examples are provided when the market is\ncomplete or the utility function is exponential.\n"
    },
    {
        "paper_id": 2005.0463,
        "authors": "Asger Lau Andersen, Emil Toft Hansen, Niels Johannesen, Adam Sheridan",
        "title": "Pandemic, Shutdown and Consumer Spending: Lessons from Scandinavian\n  Policy Responses to COVID-19",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper uses transaction data from a large bank in Scandinavia to estimate\nthe effect of social distancing laws on consumer spending in the COVID-19\npandemic. The analysis exploits a natural experiment to disentangle the effects\nof the virus and the laws aiming to contain it: Denmark and Sweden were\nsimilarly exposed to the pandemic but only Denmark imposed significant\nrestrictions on social and economic activities. We estimate that aggregate\nspending dropped by around 25 percent in Sweden and, as a result of the\nshutdown, by 4 additional percentage points in Denmark. This implies that most\nof the economic contraction is caused by the virus itself and occurs regardless\nof social distancing laws. The age gradient in the estimates suggest that\nsocial distancing reinforces the virus-induced drop in spending for low\nhealth-risk individuals but attenuates it for high-risk individuals by lowering\nthe overall prevalence of the virus in the society.\n"
    },
    {
        "paper_id": 2005.04761,
        "authors": "Taras Bodnar, Solomiia Dmytriv, Yarema Okhrin, Nestor Parolya and\n  Wolfgang Schmid",
        "title": "Statistical inference for the EU portfolio in high dimensions",
        "comments": "27 pages, 5 figures, 2 tables",
        "journal-ref": "IEEE Transactions on Signal Processing, Volume 69, 2020",
        "doi": "10.1109/TSP.2020.3037369",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper, using the shrinkage-based approach for portfolio weights and\nmodern results from random matrix theory we construct an effective procedure\nfor testing the efficiency of the expected utility (EU) portfolio and discuss\nthe asymptotic behavior of the proposed test statistic under the\nhigh-dimensional asymptotic regime, namely when the number of assets $p$\nincreases at the same rate as the sample size $n$ such that their ratio $p/n$\napproaches a positive constant $c\\in(0,1)$ as $n\\to\\infty$. We provide an\nextensive simulation study where the power function and receiver operating\ncharacteristic curves of the test are analyzed. In the empirical study, the\nmethodology is applied to the returns of S\\&P 500 constituents.\n"
    },
    {
        "paper_id": 2005.04868,
        "authors": "Giuseppe Storti and Chao Wang",
        "title": "Nonparametric Expected Shortfall Forecasting Incorporating Weighted\n  Quantiles",
        "comments": "38 pages, 2 figures and 6 tables",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  A new semi-parametric Expected Shortfall (ES) estimation and forecasting\nframework is proposed. The proposed approach is based on a two-step estimation\nprocedure. The first step involves the estimation of Value-at-Risk (VaR) at\ndifferent quantile levels through a set of quantile time series regressions.\nThen, the ES is computed as a weighted average of the estimated quantiles. The\nquantiles weighting structure is parsimoniously parameterized by means of a\nBeta weight function whose coefficients are optimized by minimizing a joint VaR\nand ES loss function of the Fissler-Ziegel class. The properties of the\nproposed approach are first evaluated with an extensive simulation study using\ntwo data generating processes. Two forecasting studies with different\nout-of-sample sizes are then conducted, one of which focuses on the 2008 Global\nFinancial Crisis (GFC) period. The proposed models are applied to 7 stock\nmarket indices and their forecasting performances are compared to those of a\nrange of parametric, non-parametric and semi-parametric models, including\nGARCH, Conditional AutoRegressive Expectile (CARE), joint VaR and ES quantile\nregression models and simple average of quantiles. The results of the\nforecasting experiments provide clear evidence in support of proposed models.\n"
    },
    {
        "paper_id": 2005.04923,
        "authors": "Eckhard Platen and Stefan Tappe",
        "title": "No-arbitrage concepts in topological vector lattices",
        "comments": "35 pages",
        "journal-ref": "Positivity 25(5):1853-1898, 2021",
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We provide a general framework for no-arbitrage concepts in topological\nvector lattices, which covers many of the well-known no-arbitrage concepts as\nparticular cases. The main structural condition we impose is that the outcomes\nof trading strategies with initial wealth zero and those with positive initial\nwealth have the structure of a convex cone. As one consequence of our approach,\nthe concepts NUPBR, NAA$_1$ and NA$_1$ may fail to be equivalent in our general\nsetting. Furthermore, we derive abstract versions of the fundamental theorem of\nasset pricing (FTAP), including an abstract FTAP on Banach function spaces, and\ninvestigate when the FTAP is warranted in its classical form with a separating\nmeasure. We also consider a financial market with semimartingales which does\nnot need to have a num\\'{e}raire, and derive results which show the links\nbetween the no-arbitrage concepts by only using the theory of topological\nvector lattices and well-known results from stochastic analysis in a sequence\nof short proofs.\n"
    },
    {
        "paper_id": 2005.04955,
        "authors": "Jiexia Ye and Juanjuan Zhao and Kejiang Ye and Chengzhong Xu",
        "title": "Multi-Graph Convolutional Network for Relationship-Driven Stock Movement\n  Prediction",
        "comments": "8pages, 4figures",
        "journal-ref": "2020 25th International Conference on Pattern Recognition (ICPR)",
        "doi": "10.1109/ICPR48806.2021.9412695",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Stock price movement prediction is commonly accepted as a very challenging\ntask due to the volatile nature of financial markets. Previous works typically\npredict the stock price mainly based on its own information, neglecting the\ncross effect among involved stocks. However, it is well known that an\nindividual stock price is correlated with prices of other stocks in complex\nways. To take the cross effect into consideration, we propose a deep learning\nframework, called Multi-GCGRU, which comprises graph convolutional network\n(GCN) and gated recurrent unit (GRU) to predict stock movement. Specifically,\nwe first encode multiple relationships among stocks into graphs based on\nfinancial domain knowledge and utilize GCN to extract the cross effect based on\nthese pre-defined graphs. To further get rid of prior knowledge, we explore an\nadaptive relationship learned by data automatically. The cross-correlation\nfeatures produced by GCN are concatenated with historical records and then fed\ninto GRU to model the temporal dependency of stock prices. Experiments on two\nstock indexes in China market show that our model outperforms other baselines.\nNote that our model is rather feasible to incorporate more effective stock\nrelationships containing expert knowledge, as well as learn data-driven\nrelationship.\n"
    },
    {
        "paper_id": 2005.05244,
        "authors": "Andreas M. Hein and Jean-Baptiste Rudelle",
        "title": "Energy Limits to the Gross Domestic Product on Earth",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Once carbon emission neutrality and other sustainability goals have been\nachieved, a widespread assumption is that economic growth at current rates can\nbe sustained beyond the 21st century. However, even if we achieve these goals,\nthis article shows that the overall size of Earth's global economy is facing an\nupper limit purely due to energy and thermodynamic factors. For that, we break\ndown global warming into two components: the greenhouse gas effect and heat\ndissipation from energy consumption related to economic activities. For the\ntemperature increase due to greenhouse gas emissions, we take 2 {\\deg}C and 5\n{\\deg}C as our lower and upper bounds. For the warming effect of heat\ndissipation related to energy consumption, we use a simplified model for global\nwarming and an extrapolation of the historical correlation between global gross\ndomestic product (GDP) and primary energy production. Combining the two\neffects, we set the acceptable global warming temperature limit to 7 {\\deg}C\nabove pre-industrial levels. We develop four scenarios, based on the viability\nof large-scale deployment of carbon-neutral energy sources. Our results\nindicate that for a 2% annual GDP growth, the upper limit will be reached at\nbest within a few centuries, even in favorable scenarios where new energy\nsources such as fusion power are deployed on a massive scale. We conclude that\nunless GDP can be largely decoupled from energy consumption, thermodynamics\nwill put a hard cap on the size of Earth's economy. Further economic growth\nwould necessarily require expanding economic activities into space.\n"
    },
    {
        "paper_id": 2005.0531,
        "authors": "Jiexin Dai, Abootaleb Shirvani, and Frank J. Fabozzi",
        "title": "Rational Finance Approach to Behavioral Option Pricing",
        "comments": "arXiv admin note: text overlap with arXiv:1710.03205",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  When pricing options, there may be different views on the instantaneous mean\nreturn of the underlying price process. According to Black (1972), where there\nexist heterogeneous views on the instantaneous mean return, this will result in\narbitrage opportunities. Behavioral finance proponents argue that such\nheterogenous views are likely to occur and this will not impact option pricing\nmodels proposed by rational dynamic asset pricing theory and will not give rise\nto volatility smiles. To rectify this, a leading advocate of behavioral finance\nhas proposed a behavioral option pricing model. As there may be unexplored\nlinks between the behavioral and rational approaches to option pricing, in this\npaper we revisit Shefrin (2008) option pricing model as an example and suggest\none approach to modify this behavioral finance option pricing formula to be\nconsistent with rational dynamic asset pricing theory by introducing arbitrage\ntransaction costs which offset the gains from arbitrage trades.\n"
    },
    {
        "paper_id": 2005.05364,
        "authors": "Maxim Bichuch, Zachary Feinstein",
        "title": "A Repo Model of Fire Sales with VWAP and LOB Pricing Mechanisms",
        "comments": "35 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We consider a network of banks that optimally choose a strategy of asset\nliquidations and borrowing in order to cover short term obligations. The\nborrowing is done in the form of collateralized repurchase agreements, the\nhaircut level of which depends on the total liquidations of all the banks.\nSimilarly the fire-sale price of the asset obtained by each of the banks\ndepends on the amount of assets liquidated by the bank itself and by other\nbanks. By nature of this setup, banks' behavior is considered as a Nash\nequilibrium. This paper provides two forms for market clearing to occur:\nthrough a common closing price and through an application of the limit order\nbook. The main results of this work are providing the existence of maximal and\nminimal clearing solutions (i.e., liquidations, borrowing, fire sale prices,\nand haircut levels) as well as sufficient conditions for uniqueness of the\nclearing solutions.\n"
    },
    {
        "paper_id": 2005.05428,
        "authors": "Vsevolod Malinovskii",
        "title": "Value-at-Risk substitute for non-ruin capital is fallacious and\n  redundant",
        "comments": "21 pages, 12 figures, 1 table",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This seemed impossible to use a theoretically adequate but too sophisticated\nrisk measure called non-ruin capital, whence its widespread (including\nregulatory documents) replacement with an inadequate, but simple risk measure\ncalled Value-at-Risk. Conflicting with the idea by Albert Einstein that\n\"everything should be made as simple as possible, but not simpler\", this led to\nfallacious, and even deceitful (but generally accepted) standards and\nrecommendations. Arguing from the standpoint of mathematical theory of risk, we\naim to break this impasse.\n"
    },
    {
        "paper_id": 2005.05459,
        "authors": "Peter Carr, Andrey Itkin and Dmitry Muravey",
        "title": "Semi-closed form prices of barrier options in the time-dependent CEV and\n  CIR models",
        "comments": "32 pages, 4 figures, 4 tabkes",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We continue a series of papers where prices of the barrier options written on\nthe underlying, which dynamics follows some one factor stochastic model with\ntime-dependent coefficients and the barrier, are obtained in semi-closed form,\nsee (Carr and Itkin, 2020, Itkin and Muravey, 2020). This paper extends this\nmethodology to the CIR model for zero-coupon bonds, and to the CEV model for\nstocks which are used as the corresponding underlying for the barrier options.\nWe describe two approaches. One is generalization of the method of heat\npotentials for the heat equation to the Bessel process, so we call it the\nmethod of Bessel potentials. We also propose a general scheme how to construct\nthe potential method for any linear differential operator with time-independent\ncoefficients. The second one is the method of generalized integral transform,\nwhich is also extended to the Bessel process. In all cases, a semi-closed\nsolution means that first, we need to solve numerically a linear Volterra\nequation of the second kind, and then the option price is represented as a\none-dimensional integral. We demonstrate that computationally our method is\nmore efficient than both the backward and forward finite difference methods\nwhile providing better accuracy and stability. Also, it is shown that both\nmethod don't duplicate but rather compliment each other, as one provides very\naccurate results at small maturities, and the other one - at high maturities.\n"
    },
    {
        "paper_id": 2005.05469,
        "authors": "M. Keith Chen, Yilin Zhuo, Malena de la Fuente, Ryne Rohla, and Elisa\n  F. Long",
        "title": "Causal Estimation of Stay-at-Home Orders on SARS-CoV-2 Transmission",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Accurately estimating the effectiveness of stay-at-home orders (SHOs) on\nreducing social contact and disease spread is crucial for mitigating pandemics.\nLeveraging individual-level location data for 10 million smartphones, we\nobserve that by April 30th---when nine in ten Americans were under a\nSHO---daily movement had fallen 70% from pre-COVID levels. One-quarter of this\ndecline is causally attributable to SHOs, with wide demographic differences in\ncompliance, most notably by political affiliation. Likely Trump voters reduce\nmovement by 9% following a local SHO, compared to a 21% reduction among their\nClinton-voting neighbors, who face similar exposure risks and identical\ngovernment orders. Linking social distancing behavior with an epidemic model,\nwe estimate that reductions in movement have causally reduced SARS-CoV-2\ntransmission rates by 49%.\n"
    },
    {
        "paper_id": 2005.0553,
        "authors": "Orcan Ogetbil and Bernhard Hientzsch",
        "title": "Extensions of Dupire Formula: Stochastic Interest Rates and Stochastic\n  Local Volatility",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We derive generalizations of Dupire formula to the cases of general\nstochastic drift and/or stochastic local volatility. First, we handle a case in\nwhich the drift is given as difference of two stochastic short rates. Such a\nsetting is natural in foreign exchange context where the short rates correspond\nto the short rates of the two currencies, equity single-currency context with\nstochastic dividend yield, or commodity context with stochastic convenience\nyield. We present the formula both in a call surface formulation as well as\ntotal implied variance formulation where the latter avoids calendar spread\narbitrage by construction. We provide derivations for the case where both short\nrates are given as single factor processes and present the limits for a single\nstochastic rate or all deterministic short rates. The limits agree with\npublished results. Then we derive a formulation that allows a more general\nstochastic drift and diffusion including one or more stochastic local\nvolatility terms. In the general setting, our derivation allows the computation\nand calibration of the leverage function for stochastic local volatility\nmodels. Despite being implicit, the generalized Dupire formulae can be used\nnumerically in a fixed-point iterative scheme.\n"
    },
    {
        "paper_id": 2005.05549,
        "authors": "Henry Zhao, Zhilan Feng, Carlos Castillo-Chavez, and Simon A. Levin",
        "title": "Staggered Release Policies for COVID-19 Control: Costs and Benefits of\n  Sequentially Relaxing Restrictions by Age",
        "comments": "22 pages (including Appendix), 10 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Strong social distancing restrictions have been crucial to controlling the\nCOVID-19 outbreak thus far, and the next question is when and how to relax\nthese restrictions. A sequential timing of relaxing restrictions across groups\nis explored in order to identify policies that simultaneously reduce health\nrisks and economic stagnation relative to current policies. The goal will be to\nmitigate health risks, particularly among the most fragile sub-populations,\nwhile also managing the deleterious effect of restrictions on economic\nactivity. The results of this paper show that a properly constructed sequential\nrelease of age-defined subgroups from strict social distancing protocols can\nlead to lower overall fatality rates than the simultaneous release of all\nindividuals after a lockdown. The optimal release policy, in terms of\nminimizing overall death rate, must be sequential in nature, and it is\nimportant to properly time each step of the staggered release. This model\nallows for testing of various timing choices for staggered release policies,\nwhich can provide insights that may be helpful in the design, testing, and\nplanning of disease management policies for the ongoing COVID-19 pandemic and\nfuture outbreaks.\n"
    },
    {
        "paper_id": 2005.05575,
        "authors": "Eckhard Platen and Stefan Tappe",
        "title": "No arbitrage and multiplicative special semimartingales",
        "comments": "37 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Consider a financial market with nonnegative semimartingales which does not\nneed to have a num\\'{e}raire. We are interested in the absence of arbitrage in\nthe sense that no self-financing portfolio gives rise to arbitrage\nopportunities, where we are allowed to add a savings account to the market. We\nwill prove that in this sense the market is free of arbitrage if and only if\nthere exists an equivalent local martingale deflator which is a multiplicative\nspecial semimartingale. In this case, the additional savings account relates to\nthe finite variation part of the multiplicative decomposition of the deflator.\n"
    },
    {
        "paper_id": 2005.0573,
        "authors": "Antoine Fosset, Jean-Philippe Bouchaud and Michael Benzaquen",
        "title": "Non-parametric Estimation of Quadratic Hawkes Processes for Order Book\n  Events",
        "comments": "17 pages, 9 figures, 3 tables",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We propose an actionable calibration procedure for general Quadratic Hawkes\nmodels of order book events (market orders, limit orders, cancellations). One\nof the main features of such models is to encode not only the influence of past\nevents on future events but also, crucially, the influence of past price\nchanges on such events. We show that the empirically calibrated quadratic\nkernel is well described by a diagonal contribution (that captures past\nrealised volatility), plus a rank-one \"Zumbach\" contribution (that captures the\neffect of past trends). We find that the Zumbach kernel is a power-law of time,\nas are all other feedback kernels. As in many previous studies, the rate of\ntruly exogenous events is found to be a small fraction of the total event rate.\nThese two features suggest that the system is close to a critical point -- in\nthe sense that stronger feedback kernels would lead to instabilities.\n"
    },
    {
        "paper_id": 2005.05945,
        "authors": "Amory Martin, Maryia Markhvida, St\\'ephane Hallegatte, Brian Walsh",
        "title": "Socio-Economic Impacts of COVID-19 on Household Consumption and Poverty",
        "comments": null,
        "journal-ref": "EconDisCliCha (2020)",
        "doi": "10.1007/s41885-020-00070-3",
        "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
        "abstract": "  The COVID-19 pandemic has caused a massive economic shock across the world\ndue to business interruptions and shutdowns from social-distancing measures. To\nevaluate the socio-economic impact of COVID-19 on individuals, a micro-economic\nmodel is developed to estimate the direct impact of distancing on household\nincome, savings, consumption, and poverty. The model assumes two periods: a\ncrisis period during which some individuals experience a drop in income and can\nuse their precautionary savings to maintain consumption; and a recovery period,\nwhen households save to replenish their depleted savings to pre-crisis level.\nThe San Francisco Bay Area is used as a case study, and the impacts of a\nlockdown are quantified, accounting for the effects of unemployment insurance\n(UI) and the CARES Act federal stimulus. Assuming a shelter-in-place period of\nthree months, the poverty rate would temporarily increase from 17.1% to 25.9%\nin the Bay Area in the absence of social protection, and the lowest income\nearners would suffer the most in relative terms. If fully implemented, the\ncombination of UI and CARES could keep the increase in poverty close to zero,\nand reduce the average recovery time, for individuals who suffer an income\nloss, from 11.8 to 6.7 months. However, the severity of the economic impact is\nspatially heterogeneous, and certain communities are more affected than the\naverage and could take more than a year to recover. Overall, this model is a\nfirst step in quantifying the household-level impacts of COVID-19 at a regional\nscale. This study can be extended to explore the impact of indirect\nmacroeconomic effects, the role of uncertainty in households' decision-making\nand the potential effect of simultaneous exogenous shocks (e.g., natural\ndisasters).\n"
    },
    {
        "paper_id": 2005.06015,
        "authors": "Jun Deng and Bin Zou",
        "title": "Quadratic Hedging for Sequential Claims with Random Weights in Discrete\n  Time",
        "comments": "20 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We study a quadratic hedging problem for a sequence of contingent claims with\nrandom weights in discrete time. We obtain the optimal hedging strategy\nexplicitly in a recursive representation, without imposing the non-degeneracy\n(ND) condition on the model and square integrability on hedging strategies. We\nrelate the general results to hedging under random horizon and fair pricing in\nthe quadratic sense. We illustrate the significance of our results in an\nexample in which the ND condition fails.\n"
    },
    {
        "paper_id": 2005.06093,
        "authors": "Assimakis Kattis and Fabian Trottner",
        "title": "Stabilizing Congestion in Decentralized Record-Keepers",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We argue that recent developments in proof-of-work consensus mechanisms can\nbe used in accordance with advancements in formal verification techniques to\nbuild a distributed payment protocol that addresses important economic\ndrawbacks from cost efficiency, scalability and adaptablity common to current\ndecentralized record-keeping systems. We enable the protocol to autonomously\nadjust system throughput according to a feasibly computable statistic - system\ndifficulty. We then provide a formal economic analysis of a decentralized\nmarket place for record-keeping that is consistent with our protocol design and\nshow that, when block rewards are zero, the system admits stable,\nself-regulating levels of transaction fees and wait-times across varying levels\nof demand. We also provide an analysis of the various technological\nrequirements needed to instantiate such a system in a commercially viable\nsetting, and identify relevant research directions.\n"
    },
    {
        "paper_id": 2005.06106,
        "authors": "Jos\\'e Roberto Iglesias, Ben-Hur Francisco Cardoso and Sebasti\\'an\n  Gon\\c{c}alves",
        "title": "Inequality, a scourge of the XXI century",
        "comments": null,
        "journal-ref": null,
        "doi": "10.1016/j.cnsns.2020.105646",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Social and economic inequality is a plague of the XXI Century. It is\ncontinuously widening, as the wealth of a relatively small group increases and,\ntherefore, the rest of the world shares a shrinking fraction of resources. This\nsituation has been predicted and denounced by economists and econophysicists.\nThe latter ones have widely used models of market dynamics which consider that\nwealth distribution is the result of wealth exchanges among economic agents. A\nsimple analogy relates the wealth in a society with the kinetic energy of the\nmolecules in a gas, and the trade between agents to the energy exchange between\nthe molecules during collisions. However, while in physical systems, thanks to\nthe equipartition of energy, the gas eventually arrives at an equilibrium\nstate, in many exchange models the economic system never equilibrates. Instead,\nit moves toward a \"condensed\" state, where one or a few agents concentrate all\nthe wealth of the society and the rest of agents shares zero or a very small\nfraction of the total wealth. Here we discuss two ways of avoiding the\n\"condensed\" state. On one hand, we consider a regulatory policy that favors the\npoorest agent in the exchanges, thus increasing the probability that the wealth\ngoes from the richest to the poorest agent. On the other hand, we study a tax\nsystem and its effects on wealth distribution. We compare the redistribution\nprocesses and conclude that complete control of the inequalities can be\nattained with simple regulations or interventions.\n"
    },
    {
        "paper_id": 2005.06171,
        "authors": "Steven E. Pav",
        "title": "Inference on Achieved Signal Noise Ratio",
        "comments": "v2 adds analysis for hedged portfolios",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We describe a procedure to perform approximate inference on the achieved\nsignal-noise ratio of the Markowitz Portfolio under Gaussian i.i.d. returns.\nThe procedure relies on a statistic similar to the Sharpe Ratio Information\nCriterion. Testing indicates the procedure is somewhat conservative, but\notherwise works well for reasonable values of sample and asset universe sizes.\nWe adapt the procedure to deal with generalizations of the portfolio\noptimization problem.\n"
    },
    {
        "paper_id": 2005.06386,
        "authors": "Ivan Slobozhan, Peter Ormosi, Rajesh Sharma",
        "title": "Which bills are lobbied? Predicting and interpreting lobbying activity\n  in the US",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Using lobbying data from OpenSecrets.org, we offer several experiments\napplying machine learning techniques to predict if a piece of legislation (US\nbill) has been subjected to lobbying activities or not. We also investigate the\ninfluence of the intensity of the lobbying activity on how discernible a\nlobbied bill is from one that was not subject to lobbying. We compare the\nperformance of a number of different models (logistic regression, random\nforest, CNN and LSTM) and text embedding representations (BOW, TF-IDF, GloVe,\nLaw2Vec). We report results of above 0.85% ROC AUC scores, and 78% accuracy.\nModel performance significantly improves (95% ROC AUC, and 88% accuracy) when\nbills with higher lobbying intensity are looked at. We also propose a method\nthat could be used for unlabelled data. Through this we show that there is a\nconsiderably large number of previously unlabelled US bills where our\npredictions suggest that some lobbying activity took place. We believe our\nmethod could potentially contribute to the enforcement of the US Lobbying\nDisclosure Act (LDA) by indicating the bills that were likely to have been\naffected by lobbying but were not filed as such.\n"
    },
    {
        "paper_id": 2005.0639,
        "authors": "Michele Leonardo Bianchi, Asmerilda Hitaj, Gian Luca Tassinari",
        "title": "Multivariate non-Gaussian models for financial applications",
        "comments": "38 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper we consider several continuous-time multivariate non-Gaussian\nmodels applied to finance and proposed in the literature in the last years. We\nstudy the models focusing on the parsimony of the number of parameters, the\nproperties of the dependence structure, and the computational tractability. For\neach model we analyze the main features, we provide the characteristic\nfunction, the marginal moments up to order four, the covariances and the\ncorrelations. Thus, we describe how to calibrate them on the time-series of\nlog-returns with a view toward practical applications and possible numerical\nissues. To empirically compare these models, we conduct an analysis on a\nfive-dimensional series of stock index log-returns.\n"
    },
    {
        "paper_id": 2005.06461,
        "authors": "Amarendra Das, Subhankar Mishra",
        "title": "India Growth Forecast for 2020-21",
        "comments": "9 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  COVID-19 has put a severe dent on the global economy and Indian Economy.\nInternational Monetary Fund has projected 1.9 percent for India. However, we\nbelieve that due to extended lockdown, the output in the first quarter is\nalmost wiped out. The situation may improve in the second quarter onwards.\nNevertheless, due to demand and supply constraints, input constraints and\ndisruption in the supply chain, except agriculture, no other sector would be\nable to achieve full capacity of production in 2020-21. The signals from power\nconsumption, GST collection, contraction in the core sectors hint towards a\nslump in the total output production in 2020-21. We derive the quarterly GVA\nfor 2020-21 by using certain assumptions on the capacity utilisation in\ndifferent sectors and using the quarterly data of 2019-20. We provide quarterly\nestimates of Gross Value Addition for 2020-21 under two scenarios. We have also\nestimated the fourth quarter output for 2019-20 under certain assumptions. We\nestimate\n"
    },
    {
        "paper_id": 2005.06576,
        "authors": "Yuval Heller and Amnon Schreiber",
        "title": "Short-Term Investments and Indices of Risk",
        "comments": "20 pages main text + 19 pages of appendices and reference list",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We study various decision problems regarding short-term investments in risky\nassets whose returns evolve continuously in time. We show that in each problem,\nall risk-averse decision makers have the same (problem-dependent) ranking over\nshort-term risky assets. Moreover, in each problem, the ranking is represented\nby the same risk index as in the case of CARA utility agents and normally\ndistributed risky assets.\n"
    },
    {
        "paper_id": 2005.0661,
        "authors": "Massimo La Morgia, Alessandro Mei, Francesco Sassi, Julinda Stefa",
        "title": "Pump and Dumps in the Bitcoin Era: Real Time Detection of Cryptocurrency\n  Market Manipulations",
        "comments": "Accepted for publication at The 29th International Conference on\n  Computer Communications and Networks (ICCCN 2020)",
        "journal-ref": null,
        "doi": "10.1109/ICCCN49398.2020.9209660",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In the last years, cryptocurrencies are increasingly popular. Even people who\nare not experts have started to invest in these securities and nowadays\ncryptocurrency exchanges process transactions for over 100 billion US dollars\nper month. However, many cryptocurrencies have low liquidity and therefore they\nare highly prone to market manipulation schemes. In this paper, we perform an\nin-depth analysis of pump and dump schemes organized by communities over the\nInternet. We observe how these communities are organized and how they carry out\nthe fraud. Then, we report on two case studies related to pump and dump groups.\nLastly, we introduce an approach to detect the fraud in real time that\noutperforms the current state of the art, so to help investors stay out of the\nmarket when a pump and dump scheme is in action.\n"
    },
    {
        "paper_id": 2005.06664,
        "authors": "Dario Sansone and Christopher S. Carpenter",
        "title": "Turing's Children: Representation of Sexual Minorities in STEM",
        "comments": "Keywords: sexual minorities; representation; LGBTQ; STEM",
        "journal-ref": null,
        "doi": "10.1371/journal.pone.0241596",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We provide the first nationally representative estimates of sexual minority\nrepresentation in STEM fields by studying 142,641 men and women in same-sex\ncouples from the 2009-2018 American Community Surveys. These data indicate that\nmen in same-sex couples are 12 percentage points less likely to have completed\na bachelor's degree in a STEM field compared to men in different-sex couples;\nthere is no gap observed for women in same-sex couples compared to women in\ndifferent-sex couples. The STEM gap between men in same-sex and different-sex\ncouples is larger than the STEM gap between white and black men but is smaller\nthan the gender STEM gap. We also document a gap in STEM occupations between\nmen in same-sex and different-sex couples, and we replicate this finding using\nindependently drawn data from the 2013-2018 National Health Interview Surveys.\nThese differences persist after controlling for demographic characteristics,\nlocation, and fertility. Our findings further the call for interventions\ndesigned at increasing representation of sexual minorities in STEM.\n"
    },
    {
        "paper_id": 2005.06771,
        "authors": "Vinay Reddy Venumuddala",
        "title": "Patterns of social mobility across social groups in India",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Social mobility captures the extent to which socio-economic status of\nchildren, is independent of status of their respective parents. In order to\nmeasure social mobility, most widely used indicators of socio-economic status\nare income, education and occupation. While social mobility measurement based\non income is less contested, data availability in Indian context limits us to\nobserving mobility patterns along the dimensions of either education or\noccupation. In this study we observe social mobility patterns for different\nsocial groups along these two main dimensions, and find that while upward and\ndownward mobility prospects in education for SCs/STs is somewhat improving in\nthe recent times, occupational mobility patterns are rather worrisome. These\nresults motivate the need for reconciling disparate trends along education and\noccupation, in order to get a more comprehensive picture of social mobility in\nthe country.\n"
    },
    {
        "paper_id": 2005.06782,
        "authors": "Ben-Zhang Yang, Xin-Jiang He, Song-Ping Zhu",
        "title": "Continuous time mean-variance-utility portfolio problem and its\n  equilibrium strategy",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper, we propose a new class of optimization problems, which\nmaximize the terminal wealth and accumulated consumption utility subject to a\nmean variance criterion controlling the final risk of the portfolio. The\nmultiple-objective optimization problem is firstly transformed into a\nsingle-objective one by introducing the concept of overall \"happiness\" of an\ninvestor defined as the aggregation of the terminal wealth under the\nmean-variance criterion and the expected accumulated utility, and then solved\nunder a game theoretic framework. We have managed to maintain analytical\ntractability; the closed-form solutions found for a set of special utility\nfunctions enable us to discuss some interesting optimal investment strategies\nthat have not been revealed before in literature.\n"
    },
    {
        "paper_id": 2005.06795,
        "authors": "Vinay Reddy Venumuddala",
        "title": "Informal Labour in India",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  India like many other developing countries is characterized by huge\nproportion of informal labour in its total workforce. The percentage of\nInformal Workforce is close to 92% of total as computed from NSSO 68th round on\nEmployment and Unemployment, 2011-12. There are many traditional and\ngeographical factors which might have been responsible for this staggering\nproportion of Informality in our country. As a part of this study, we focus\nmainly on finding out how Informality varies with Region, Sector, Gender,\nSocial Group, and Working Age Groups. Further we look at how Total Inequality\nis contributed by Formal and Informal Labour, and how much do\noccupations/industries contribute to inequality within each of formal and\ninformal labour groups separately. For the purposes of our study we use NSSO\nrounds 61 (2004-05) and 68 (2011-12) on employment and unemployment. The study\nintends to look at an overall picture of Informality, and based on the data\nhighlight any inferences which are visible from the data.\n"
    },
    {
        "paper_id": 2005.06796,
        "authors": "Michele Costola, Matteo Iacopini and Carlo R.M.A. Santagiustina",
        "title": "Public Concern and the Financial Markets during the COVID-19 outbreak",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We measure the public concern during the outbreak of COVID-19 disease using\nthree data sources from Google Trends (YouTube, Google News, and Google\nSearch). Our findings are three-fold. First, the public concern in Italy is\nfound to be a driver of the concerns in other countries. Second, we document\nthat Google Trends data for Italy better explains the stock index returns of\nFrance, Germany, Great Britain, the United States, and Spain with respect to\ntheir country-based indicators. Finally, we perform a time-varying analysis and\nidentify that the most severe impacts in the financial markets occur at each\nstep of the Italian lock-down process.\n"
    },
    {
        "paper_id": 2005.06802,
        "authors": "Vinay Reddy Venumuddala",
        "title": "Determinants of occupational mobility within the social stratification\n  structure in India",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this study, we make use of empirically observed occupational\nstratification patterns, in order to identify the relationship between\neducation and social mobility of individuals - the latter is approximated by\nthe social distance of an individual's occupation from his/her household's\ntraditional niche occupation. Our study draws upon a novel occupational network\nconstruction proposed in Lambert et.al (2018), with slight adjustments, to\nempirically identify social stratification patterns using cross sectional\nhousehold surveys available in the Indian context. We use IHDS-2 data-set for\nthe purpose of our study.\n"
    },
    {
        "paper_id": 2005.0684,
        "authors": "Shunyao Yan, Klaus M. Miller and Bernd Skiera",
        "title": "How Does the Adoption of Ad Blockers Affect News Consumption?",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Ad blockers allow users to browse websites without viewing ads. Online news\nproviders that rely on advertising revenue tend to perceive users adoption of\nad blockers purely as a threat to revenue. Yet, this perception ignores the\npossibility that avoiding ads, which users presumably dislike, may affect users\nonline news consumption behavior in positive ways. Using 3.1 million anonymized\nvisits from 79,856 registered users on a news website, we find that adopting an\nad blocker has a robust positive effect on the quantity and variety of articles\nusers consume (21.5% - 43.3% more articles and 13.4% - 29.1% more content\ncategories). An increase in repeat user visits of the news website, rather than\nthe number of page impressions per visit, drives the news consumption. These\nvisits tend to start with direct navigation to the news website, indicating\nuser loyalty. The increase in news consumption is more substantial for users\nwho have less prior experience with the website. We discuss how news publishers\ncould benefit from these findings, including exploring revenue models that\nconsider users desire to avoid ads.\n"
    },
    {
        "paper_id": 2005.07346,
        "authors": "Jiashuo Li, Sili Zhou, Wendong Wei, Jianchuan Qi, Yumeng Li, Bin Chen,\n  Ning Zhang, Dabo Guan, Haoqi Qian, Xiaohui Wu, Jiawen Miao, Long Chen, Sai\n  Liang, Kuishuang Feng",
        "title": "Mercury-related health benefits from retrofitting coal-fired power\n  plants in China",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  China has implemented retrofitting measures in coal-fired power plants\n(CFPPs) to reduce air pollution through small unit shutdown (SUS), the\ninstallation of air pollution control devices (APCDs) and power generation\nefficiency (PGE) improvement. The reductions in highly toxic Hg emissions and\ntheir related health impacts by these measures have not been well studied. To\nrefine mitigation options, we evaluated the health benefits of reduced Hg\nemissions via retrofitting measures during China's 12th Five-Year Plan by\ncombining plant-level Hg emission inventories with the China Hg Risk\nSource-Tracking Model. We found that the measures reduced Hg emissions by 23.5\ntons (approximately 1/5 of that from CFPPs in 2010), preventing 0.0021 points\nof per-foetus intelligence quotient (IQ) decrements and 114 deaths from fatal\nheart attacks. These benefits were dominated by CFPP shutdowns and APCD\ninstallations. Provincial health benefits were largely attributable to Hg\nreductions in other regions. We also demonstrated the necessity of considering\nhuman health impacts, rather than just Hg emission reductions, in selecting Hg\ncontrol devices. This study also suggests that Hg control strategies should\nconsider various factors, such as CFPP locations, population densities and\ntrade-offs between reductions of total Hg (THg) and Hg2+.\n"
    },
    {
        "paper_id": 2005.07393,
        "authors": "Takuji Arai",
        "title": "Al\\`os type decomposition formula for Barndorff-Nielsen and Shephard\n  model",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The objective is to provide an Al\\`os type decomposition formula of call\noption prices for the Barndorff-Nielsen and Shephard model: an\nOrnstein-Uhlenbeck type stochastic volatility model driven by a subordinator\nwithout drift. Al\\`os (2012) introduced a decomposition expression for the\nHeston model by using Ito's formula. In this paper, we extend it to the\nBarndorff-Nielsen and Shephard model. As far as we know, this is the first\nresult on the Al\\`os type decomposition formula for models with infinite active\njumps.\n"
    },
    {
        "paper_id": 2005.07538,
        "authors": "Vinay Reddy Venumuddala",
        "title": "Farmers' situation in agriculture markets and role of public\n  interventions in India",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In our country, majority of agricultural workers (who may include farmers\nworking within a cooperative framework, or those who work individually either\nas owners or tenants) are shown to be reaping the least amount of profits in\nthe agriculture value chain when compared to the effort they put in. There is a\ngood amount of literature which broadly substantiates this situation in our\ncountry. Main objective of this study is to have a broad understanding of the\nrole played by public systems in this value chain, particularly in the segment\nthat interacts with farmers. As a starting point, we first try to get a better\nunderstanding of how farmers are placed in a typical agriculture value chain.\nFor this we take the help of recent seminal works on this topic that captured\nthe situation of farmers' within certain types of value chains. Then, we\nisolate the segment which interacts with farmers and deep-dive into data to\nunderstand the role played by public interventions in determining farmers'\nincome from agriculture. NSSO 70th round on Situation Assessment Survey of\nfarmers has data pertaining to the choices of farmers and the type of their\ninteraction with different players in the value chain. Using this data we tried\nto get a econometric picture of the role played by government interventions and\nthe extent to which they determine the incomes that a typical farming household\nderives out of agriculture.\n"
    },
    {
        "paper_id": 2005.07575,
        "authors": "Emir Zunic, Kemal Korjenic, Kerim Hodzic and Dzenana Donko",
        "title": "Application of Facebook's Prophet Algorithm for Successful Sales\n  Forecasting Based on Real-world Data",
        "comments": null,
        "journal-ref": "International Journal of Computer Science & Information Technology\n  (IJCSIT) Vol 12, No 2, April 2020",
        "doi": "10.5121/ijcsit.2020.12203",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper presents a framework capable of accurately forecasting future\nsales in the retail industry and classifying the product portfolio according to\nthe expected level of forecasting reliability. The proposed framework, that\nwould be of great use for any company operating in the retail industry, is\nbased on Facebook's Prophet algorithm and backtesting strategy. Real-world\nsales forecasting benchmark data obtained experimentally in a production\nenvironment in one of the biggest retail companies in Bosnia and Herzegovina is\nused to evaluate the framework and demonstrate its capabilities in a real-world\nuse case scenario.\n"
    },
    {
        "paper_id": 2005.07732,
        "authors": "K.M. Golam Muhiuddin and Nusrat Jahan",
        "title": "Parameters of Profitability: Evidence From Conventional and Islamic\n  Banks of Bangladesh",
        "comments": null,
        "journal-ref": null,
        "doi": "10.5296/ijafr.v8i4.13760",
        "license": "http://creativecommons.org/licenses/by-sa/4.0/",
        "abstract": "  This paper evaluates the commercial banks of Bangladesh in terms of\nprofitability dimension of performance and also examines the impact of selected\ndeterminants and banking system on this dimension of performance. Evaluation of\ntrend in profitability of listed commercial banks of Bangladesh reveals that,\non an average, profitability is exhibiting a decreasing trend over the selected\nperiod; however, the profitability performance of Islamic banks remained rather\nhigh compared to Conventional banks. Profitability measured by Return on Asset\nis found to be significantly affected by the bank-specific factors,\nindustry-specific factor and the banking system. However, macro-economic\nfactors evidently have no significant impact on profitability of commercial\nbanks of Bangladesh.\n"
    },
    {
        "paper_id": 2005.07967,
        "authors": "Masato Hisakado, Shintaro Mori",
        "title": "Parameter estimation of default portfolios using the Merton model and\n  Phase transition",
        "comments": "19 pages, 5 figures",
        "journal-ref": "Physica A,vol. 563, 1 February, 2021, 125435",
        "doi": "10.1016/j.physa.2020.125435",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We discuss the parameter estimation of the probability of default (PD), the\ncorrelation between the obligors, and a phase transition. In our previous work,\nwe studied the problem using the beta-binomial distribution. A non-equilibrium\nphase transition with an order parameter occurs when the temporal correlation\ndecays by power law. In this article, we adopt the Merton model, which uses an\nasset correlation as the default correlation, and find that a phase transition\noccurs when the temporal correlation decays by power law. When the power index\nis less than one, the PD estimator converges slowly. Thus, it is difficult to\nestimate PD with limited historical data. Conversely, when the power index is\ngreater than one, the convergence speed is inversely proportional to the number\nof samples. We investigate the empirical default data history of several rating\nagencies. The estimated power index is in the slow convergence range when we\nuse long history data. This suggests that PD could have a long memory and that\nit is difficult to estimate parameters due to slow convergence.\n"
    },
    {
        "paper_id": 2005.08273,
        "authors": "Saket Saurabh, Ayush Trivedi, Nithilaksh P. Lokesh, Bhagyashree\n  Gaikwad",
        "title": "Sustaining the economy under partial lockdown: A pandemic centric\n  approach",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  As the world fights to contain and control the spread of the Novel\nCoronavirus, countries are imposing severe measures from restrictions on travel\nand social gatherings to complete lockdowns. Lockdowns, though effective in\ncontrolling the virus spread, leaves a massive economic impact. In a country\nlike India with 21.9 % of its population below the poverty line, lockdowns have\na direct impact on the livelihood of a large part of the population. Our\napproach conforms to healthcare and state practices of reducing human to human\ncontact, by optimizing the lockdown strategy. We propose resuming economic\nactivities while keeping healthcare facilities from being overwhelmed. We model\nthe coronavirus pandemic as SEIR dynamic model for a set of states as nodes\nwith certain population and analyze the model output before and after complete\nlockdown. Social distancing that people would willingly follow, in the no\nlockdown situation is modeled as being influenced with the knowledge of the\ncurrent number of infection by imitating Granovetter threshold model. We then\nprovide optimal lockdown policy solutions for the duration of ten weeks using\nNSGA-II optimization algorithm. While there are many studies that focus on\nmodelling the transmission of COVID-19, ours is one of the few attempts to\nstrike a balance between number of infections and economic operations.\n"
    },
    {
        "paper_id": 2005.08293,
        "authors": "Mario Coccia",
        "title": "How sustainable environments have reduced the diffusion of coronavirus\n  disease 2019: the interaction between spread of COVID-19 infection, polluting\n  industrialization, wind (renewable) energy",
        "comments": "21 pages; 2 figures; 6 tables",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  This study endeavors to explain the relation between air pollution and\nparticulate compounds emissions, wind resources and energy, and the diffusion\nof COVID-19 infection to provide insights of sustainable policy to prevent\nfuture epidemics. The statistical analysis here focuses on case study of Italy,\none of the countries to experience a rapid increase in confirmed cases and\ndeaths. Results reveal two main findings: 1) cities in regions with high wind\nspeed and a high wind energy production in MW have a lower number of infected\nindividuals of COVID-19 infection and total deaths; 2) cities located in\nhinterland zones (mostly those bordering large urban conurbations) with high\npolluting industrialization, low wind speed and less cleaner production have a\ngreater number of infected individuals and total deaths. Hence, cities with\npollution industrialization and low renewable energy have also to consider low\nwind speed and other climatological factors that can increase stagnation of the\nair in the atmosphere with potential problems for public health in the presence\nof viral agents. Results here suggest that current pandemic of Coronavirus\ndisease and future epidemics similar to COVID-19 infection cannot be solved\nonly with research and practice of medicine, immunology and microbiology but\nalso with a proactive strategy directed to interventions for a sustainable\ndevelopment. Overall, then, this study has to conclude that a strategy to\nprevent future epidemics similar to COVID-19 infection must also be based on\nsustainability science to support a higher level of renewable energy and\ncleaner production to reduce polluting industrialization and, as result, the\nfactors determining the spread of coronavirus disease and other infections in\nsociety.\n"
    },
    {
        "paper_id": 2005.08568,
        "authors": "Alison Fairbrass (1 and 2), Georgina Mace (2), Paul Ekins (1), Ben\n  Milligan (1 and 3) ((1) Institute for Sustainable Resources, University\n  College London, London, UK, (2) Centre for Biodiversity and Environment\n  Research, University College London, London, UK, (3) Faculty of Law,\n  University of New South Wales, Sydney, Australia)",
        "title": "The Natural Capital Indicator Framework (NCIF): A framework of\n  indicators for national natural capital reporting",
        "comments": "26 pages, 3 figures, 1 table, 1 graphical abstract",
        "journal-ref": null,
        "doi": "10.1016/j.ecoser.2020.101198",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  It is now widely recognised that components of the environment play the role\nof economic assets, termed natural capital, that are a foundation of social and\neconomic development. National governments monitor the state and trends of\nnatural capital through a range of activities including natural capital\naccounting, national ecosystem assessments, ecosystem service valuation, and\neconomic and environmental analyses. Indicators play an integral role in these\nactivities as they facilitate the reporting of complex natural capital\ninformation. One factor that hinders the success of these activities and their\ncomparability across countries is the absence of a coherent framework of\nindicators concerning natural capital (and its benefits) that can aid\ndecision-making. Here we present an integrated Natural Capital Indicator\nFramework (NCIF) alongside example indicators, which provides an illustrative\nstructure for countries to select and organise indicators to assess their use\nof and dependence on natural capital. The NCIF sits within a wider context of\nindicators related to natural, human, social and manufactured capital, and\nassociated flows of benefits. The framework provides decision-makers with a\nstructured approach to selecting natural capital indicators with which to make\ndecisions about economic development that take into account national natural\ncapital and associated flows of benefits.\n"
    },
    {
        "paper_id": 2005.08703,
        "authors": "Christian Bongiorno and Damien Challet",
        "title": "Reactive Global Minimum Variance Portfolios with $k-$BAHC covariance\n  cleaning",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We introduce a $k$-fold boosted version of our Boostrapped Average\nHierarchical Clustering cleaning procedure for correlation and covariance\nmatrices. We then apply this method to global minimum variance portfolios for\nvarious values of $k$ and compare their performance with other state-of-the-art\nmethods. Generally, we find that our method yields better Sharpe ratios after\ntransaction costs than competing filtering methods, despite requiring a larger\nturnover.\n"
    },
    {
        "paper_id": 2005.08734,
        "authors": "Nusrat Jahan and M. Ayub Islam",
        "title": "Evaluation of Accounting and Market Performance: A Study on Listed\n  Islamic Banks of Bangladesh",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-sa/4.0/",
        "abstract": "  This study compared accounting performance of Islamic banks with their market\nperformance and also assessed the effect of firm-specific determinants and\ncross-sectional effect on accounting and market performance. This study\nselected all six listed Islamic banks of Chittagong Stock Exchange and the data\nwere collected for the period of 2009 to 2013. This study reported that Social\nIslamic Bank Limited exhibits superior accounting performance whereas Islami\nBank Bangladesh Limited holds better market performance. However, banks\nexhibiting superior accounting performance reported to have inferior market\nperformance. Further, random-effect model for ROA reports that there exist\nsignificant entity or crosssectional effect on ROA; and operational efficiency\nand bank size are significantly negatively associated with ROA. However,\nrandom-effect model for Tobins Q failed to ascertain entity or cross-sectional\neffect on Tobins Q and also reveals that firm-specific determinants have no\nsignificant impact on Tobins Q.\n"
    },
    {
        "paper_id": 2005.08735,
        "authors": "Liyang Tang",
        "title": "Application of Nonlinear Autoregressive with Exogenous Input (NARX)\n  neural network in macroeconomic forecasting, national goal setting and global\n  competitiveness assessment",
        "comments": "179 pages, 81 figures, People's Bank of China Working Paper",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper selects the NARX neural network as the method through literature\nreview, and constructs specific NARX neural networks under application\nscenarios involving macroeconomic forecasting, national goal setting and global\ncompetitiveness assessment. Through case studies on China, US and Eurozone,\nthis study explores how those limited & partial exogenous inputs or abundant &\ncomprehensive exogenous inputs, a small set of most relevant exogenous inputs\nor a large set of exogenous inputs covering all major aspects of the macro\neconomy, whole area related exogenous inputs or both whole area and subdivision\narea related exogenous inputs specifically affect the forecasting performance\nof NARX neural networks for specific macroeconomic indicators or indices.\nThrough the case study on Russia this paper explores how the limited & most\nrelevant exogenous inputs set or the abundant & comprehensive exogenous inputs\nset specifically influences the prediction performance of those specific NARX\nneural networks for national goal setting. Finally, comparative studies on the\napplication of NARX neural networks for the forecasts of Global Competitiveness\nIndices (GCIs) of various economies are conducted, in order to explore whether\nthe specific NARX neural network trained on the basis of the GCI related data\nof some economies can make sufficiently accurate predictions about GCIs of\nother economies, and whether the specific NARX neural network trained on the\nbasis of the data of some type of economies can give more accurate predictions\nabout GCIs of the same type of economies than those of different type of\neconomies. Based on all of the above successful application, this paper\nprovides policy recommendations on applying fully trained NARX neural networks\nthat are assessed as qualified to assist or even replace the deductive and\ninductive abilities of the human brain in a variety of appropriate tasks.\n"
    },
    {
        "paper_id": 2005.08759,
        "authors": "Nusrat Jahan",
        "title": "Determinants of Profitability of Banks: Evidence from Islamic Banks of\n  Bangladesh",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-sa/4.0/",
        "abstract": "  This empirical study is conducted on randomly selected six Islamic banks of\nBangladesh. This study utilizes widely used Measures of banks profitability\nwhich are Return on Asset (ROA), Return on Equity (ROE) and Return on Deposit\n(ROD) and these are also commonly suggested tools by Bangladesh Bank to\nevaluate banks performance. In addition, this study examined the relationship\nof ROA with Asset Utilization (AU), Operational Efficiency (OE)and ROD. The\nresult reveals that EXIM Bank Limited is performing very good in terms of all\nprofitability measures ROA, ROE and ROD even though average asset size of\nIslami Bank Bangladesh Limited is found to be largest among all six Islamic\nBanks. The result of regression found the explanatory variable ROD is\nsignificantly associated with ROA but failed to establish any significant\nassociation with operational efficiency and asset utilization.\n"
    },
    {
        "paper_id": 2005.08762,
        "authors": "Suchismita Banerjee, Bikas K. Chakrabarti, Manipushpak Mitra, Suresh\n  Mutuswami",
        "title": "Inequality Measures: The Kolkata index in comparison with other measures",
        "comments": "31 pages, 9 figures, invited review article for the special issue\n  'From Physics to Econophysics and that: methods and insights'; Frontiers in\n  Physics",
        "journal-ref": "Frontiers in Physics, Volume:8, Article:562182, 14 December 2020",
        "doi": "10.3389/fphy.2020.562182",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We provide a survey of the Kolkata index of social inequality, focusing in\nparticular on income inequality. Based on the observation that inequality\nfunctions (such as the Lorenz function), giving the measures of income or\nwealth against that of the population, to be generally nonlinear, we show that\nthe fixed point (like Kolkata index k) of such a nonlinear function (or\nrelated, like the complementary Lorenz function) offer better measure of\ninequality than the average quantities (like Gini index). Indeed the Kolkata\nindex can be viewed as a generalized Hirsch index for a normalized inequality\nfunction and gives the fraction k of the total wealth possessed by the rich\n(1-k) fraction of the population. We analyze the structures of the inequality\nindices for both continuous and discrete income distributions. We also compare\nthe Kolkata index to some other measures like the Gini coefficient and the\nPietra index. Lastly, we provide some empirical studies which illustrate the\ndifferences between the Kolkata index and the Gini coefficient.\n"
    },
    {
        "paper_id": 2005.08763,
        "authors": "Yonatan Berman",
        "title": "The Distributional Short-Term Impact of the COVID-19 Crisis on Wages in\n  the United States",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper uses Bureau of Labor Statistics employment and wage data to study\nthe distributional impact of the COVID-19 crisis on wages in the United States\nby mid-April. It answers whether wages of lower-wage workers decreased more\nthan others', and to what extent. We find that the COVID-19 outbreak\nexacerbates existing inequalities. Workers at the bottom quintile in mid-March\nwere three times more likely to be laid off by mid-April compared to\nhigher-wage workers. Weekly wages of workers at the bottom quintile decreased\nby 6% on average between mid-February and mid-March and by 26% between\nmid-March and mid-April. The average decrease for higher quintiles was less\nthan 1% between mid-February and mid-March and about 10% between mid-March and\nmid-April. We also find that workers aged 16-24 were hit much harder than older\nworkers. Hispanic workers were also hurt more than other racial groups. Their\nwages decreased by 2-3 percentage points more than other workers' between\nmid-March and mid-April.\n"
    },
    {
        "paper_id": 2005.08929,
        "authors": "Marco Pagano, Christian Wagner, Josef Zechner",
        "title": "Disaster Resilience and Asset Prices",
        "comments": "40 pages, 11 figures, 7 tables",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper investigates whether security markets price the effect of social\ndistancing on firms' operations. We document that firms that are more resilient\nto social distancing significantly outperformed those with lower resilience\nduring the COVID-19 outbreak, even after controlling for the standard risk\nfactors. Similar cross-sectional return differentials already emerged before\nthe COVID-19 crisis: the 2014-19 cumulative return differential between more\nand less resilient firms is of similar size as during the outbreak, suggesting\ngrowing awareness of pandemic risk well in advance of its materialization.\nFinally, we use stock option prices to infer the market's return expectations\nafter the onset of the pandemic: even at a two-year horizon, stocks of more\npandemic-resilient firms are expected to yield significantly lower returns than\nless resilient ones, reflecting their lower exposure to disaster risk. Hence,\ngoing forward, markets appear to price exposure to a new risk factor, namely,\npandemic risk.\n"
    },
    {
        "paper_id": 2005.08961,
        "authors": "Vinay Reddy Venumuddala",
        "title": "Patterns in demand side financial inclusion in India -- An inquiry using\n  IHDS Panel Data",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In the following study, we inquire into the financial inclusion from a demand\nside perspective. Utilizing IHDS round-1 (2004-05) and round-2 (2011-12),\nstarting from a broad picture of demand side access to finance at the country\nlevel, we venture into analysing the patterns at state level, and then lastly\nat district level. Particularly at district level, we focus on agriculture\nhouseholds in rural areas to identify if there is a shift in the demand side\nfinancial access towards non-agriculture households in certain parts of the\ncountry. In order to do this, we use District level 'Basic Statistical Returns\nof Scheduled Commercial Banks' for the years 2004 and 2011, made available by\nRBI, to first construct supply side financial inclusion indices, and then infer\nabout a relative shift in access to formal finance away from agriculture\nhouseholds, using a logistic regression framework.\n"
    },
    {
        "paper_id": 2005.09036,
        "authors": "Ahmad Hajihasani, Ali Namaki, Nazanin Asadi and Reza Tehrani",
        "title": "Non-Extensive Value-at-Risk Estimation During Times of Crisis",
        "comments": null,
        "journal-ref": null,
        "doi": "10.1142/S0129183121500996",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Value-at-risk is one of the important subjects that extensively used by\nresearchers and practitioners for measuring and managing uncertainty in\nfinancial markets. Although value-at-risk is a common risk control instrument,\nbut there are criticisms about its performance. One of these cases, which has\nbeen studied in this research, is the value-at-risk underestimation during\ntimes of crisis. In these periods, the non-Gaussian behavior of markets\nintensifies and the estimated value-at-risks by normal models are lower than\nthe real values. In fact, during times of crisis, the probability density of\nextreme values in financial return series increases and this heavy-tailed\nbehavior of return series reduces the accuracy of the normal value-at-risk\nestimation models. A potential approach that can be used to describe\nnon-Gaussian behavior of return series, is Tsallis entropy framework and\nnon-extensive statistical methods. In this paper, we have used non-extensive\nvalue at risk model for analyzing the behavior of financial markets during\ntimes of crisis. By applying q-Gaussian probability density function, we can\nsee a better value-at-risk estimation in comparison with the normal models,\nespecially during times of crisis. We showed that q-Gaussian model estimates\nvalue-at-risk better than normal model. Also we saw in the mature markets, it\nis obvious that the difference of value-at-risk between normal condition and\nnon-extensive approach increase more than one standard deviation during times\nof crisis, but in the emerging markets we cannot see a specific pattern.\n"
    },
    {
        "paper_id": 2005.09066,
        "authors": "Shane Barratt, Stephen Boyd",
        "title": "Multi-Period Liability Clearing via Convex Optimal Control",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We consider the problem of determining a sequence of payments among a set of\nentities that clear (if possible) the liabilities among them. We formulate this\nas an optimal control problem, which is convex when the objective function is,\nand therefore readily solved. For this optimal control problem, we give a\nnumber of useful and interesting convex costs and constraints that can be\ncombined in any way for different applications. We describe a number of\nextensions, for example to handle unknown changes in cash and liabilities, to\nallow bailouts, to find the minimum time to clear the liabilities, or to\nminimize the number of non-cleared liabilities, when fully clearing the\nliabilities is impossible.\n"
    },
    {
        "paper_id": 2005.091,
        "authors": "Kostadin Kushlev and Matthew R Leitao",
        "title": "The Effects of Smartphones on Well-Being: Theoretical Integration and\n  Research Agenda",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  As smartphones become ever more integrated in peoples lives, a burgeoning new\narea of research has emerged on their well-being effects. We propose that\ndisparate strands of research and apparently contradictory findings can be\nintegrated under three basic hypotheses, positing that smartphones influence\nwell-being by (1) replacing other activities (displacement hypothesis), (2)\ninterfering with concurrent activities (interference hypothesis), and (3)\naffording access to information and activities that would otherwise be\nunavailable (complementarity hypothesis). Using this framework, we highlight\nmethodological issues and go beyond net effects to examine how and when phones\nboost versus hurt well-being. We examine both psychological and contextual\nmediators and moderators of the effects, thus outlining an agenda for future\nresearch.\n"
    },
    {
        "paper_id": 2005.09166,
        "authors": "Samuel Gingras and William J. McCausland",
        "title": "A Flexible Stochastic Conditional Duration Model",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We introduce a new stochastic duration model for transaction times in asset\nmarkets. We argue that widely accepted rules for aggregating seemingly related\ntrades mislead inference pertaining to durations between unrelated trades:\nwhile any two trades executed in the same second are probably related, it is\nextremely unlikely that all such pairs of trades are, in a typical sample. By\nplacing uncertainty about which trades are related within our model, we improve\ninference for the distribution of durations between unrelated trades,\nespecially near zero. We introduce a normalized conditional distribution for\ndurations between unrelated trades that is both flexible and amenable to\nshrinkage towards an exponential distribution, which we argue is an appropriate\nfirst-order model. Thanks to highly efficient draws of state variables,\nnumerical efficiency of posterior simulation is much higher than in previous\nstudies. In an empirical application, we find that the conditional hazard\nfunction for durations between unrelated trades varies much less than what most\nstudies find. We claim that this is because we avoid statistical artifacts that\narise from deterministic trade-aggregation rules and unsuitable parametric\ndistributions.\n"
    },
    {
        "paper_id": 2005.09214,
        "authors": "Budhi Surya, Wenyuan Wang, Xianghua Zhao, Xiaowen Zhou",
        "title": "Parisian excursion with capital injection for draw-down reflected Levy\n  insurance risk process",
        "comments": "16 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper discusses Parisian ruin problem with capital injection for Levy\ninsurance risk process. Capital injection takes place at the draw-down time of\nthe surplus process when it drops below a pre-specified function of its last\nrecord maximum. The capital is continuously paid to keep the surplus above the\ndraw-down level until either the surplus process goes above the record high or\na Parisian type ruin occurs, which is announced at the first instance the\nsurplus process has undergone an excursion below the record for an independent\nexponential period of time consecutively since the time the capital was first\ninjected. Some distributional identities concerning the excursion are\npresented. Firstly, we give the Parisian ruin probability and the joint Laplace\ntransform (possibly killed at the first passage time above a fixed level of the\nsurplus process) of the ruin time, surplus position at ruin, and the total\ncapital injection at ruin. Secondly, we obtain the $q$-potential measure of the\nsurplus process killed at Parisian ruin. Finally, we give expected present\nvalue of the total discounted capital payments up to the Parisian ruin time.\nThe results are derived using recent developments in fluctuation and excursion\ntheory of spectrally negative Levy process and are presented semi explicitly in\nterms of the scale function of the Levy process. Some numerical examples are\ngiven to facilitate the analysis of the impact of initial surplus and frequency\nof observation period to the ruin probability and to the expected total capital\ninjection.\n"
    },
    {
        "paper_id": 2005.09356,
        "authors": "Nino Antulov-Fantulin, Tian Guo, Fabrizio Lillo",
        "title": "Temporal mixture ensemble models for intraday volume forecasting in\n  cryptocurrency exchange markets",
        "comments": "36 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We study the problem of the intraday short-term volume forecasting in\ncryptocurrency exchange markets. The predictions are built by using transaction\nand order book data from different markets where the exchange takes place.\nMethodologically, we propose a temporal mixture ensemble, capable of adaptively\nexploiting, for the forecasting, different sources of data and providing a\nvolume point estimate, as well as its uncertainty. We provide evidence of the\noutperformance of our model by comparing its outcomes with those obtained with\ndifferent time series and machine learning methods. Finally, we discuss the\npredictions conditional to volume and we find that also in this case machine\nlearning methods outperform econometric models.\n"
    },
    {
        "paper_id": 2005.09461,
        "authors": "Goncalo dos Reis and Vadim Platonov",
        "title": "Forward utilities and Mean-field games under relative performance\n  concerns",
        "comments": "24 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We introduce the concept of mean field games for agents using Forward\nutilities of CARA type to study a family of portfolio management problems under\nrelative performance concerns. Under asset specialization of the fund managers,\nwe solve the forward-utility finite player game and the forward-utility\nmean-field game. We study best response and equilibrium strategies in the\nsingle common stock asset and the asset specialization with common noise. As an\napplication, we draw on the core features of the forward utility paradigm and\ndiscuss a problem of time-consistent mean-field dynamic model selection in\nsequential time-horizons.\n"
    },
    {
        "paper_id": 2005.09482,
        "authors": "Nusrat Jahan",
        "title": "An Empirical Investigation of Cash Conversion Cycle of Manufacturing\n  Firms and its Association with Firm Size and Profitability",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-sa/4.0/",
        "abstract": "  The purpose of this empirical study is to investigate Cash Conversion Cycle\nof thirty manufacturing firms listed in Dhaka Stock Exchanges under six\ndifferent categories, which are, Food and allied, Pharmaceuticals and chemical,\nCement, Textile, Engineering and Miscellaneous. This paper sets industry\naverage Cash Conversion Cycle for these six industries and examines the\nrelationship of Cash Conversion Cycle with firm size and profitability. This\nstudy did not find statistically significant differences among the Cash\nConversion Cycle of varying manufacturing industries. The result of this study\nindicates a statistically significant negative relationship between the Cash\nConversion Cycle and profitability, especially in terms of Return on Equity.\nThe result also shows that the Cash Conversion Cycle of manufacturing firm also\nhas significant negative relationship with firm size, when measured in terms of\nnet sales. The present study contributes to the literature on working capital\nmanagement written in the context of Bangladesh.\n"
    },
    {
        "paper_id": 2005.09483,
        "authors": "Nusrat Jahan",
        "title": "An Investigation into the Equivalency of Three Performance Dimensions:\n  Evidence from Commercial Banks in Bangladesh",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-sa/4.0/",
        "abstract": "  This study evaluated the three dimensions of performance of commercial banks\nin Bangladesh by analyzing the trend of the Malmquist Productivity Index (MPI)\nof the Total Factor Productivity (TFP), Return on Asset (ROA) and Total Stock\nReturn (TSR) over the period 2011 to 2015. The study developed an empirical\nframework with the intention to examine the equivalency of three dimensions of\nperformance. Since, the measures of performance are different, they cannot be\ntested in their original form; hence, the growth rate of each category of\nperformance measures were estimated and tested to examine the comparability\namong them. Evaluation of profitability revealed a decreasing trend and\nevaluation of stock performance suggests that investors are incurring losses on\ntheir investment over the selected period. Evaluation of productivity indicates\nthat productivity regress was recorded initially but at the end of the studied\nperiod a modest productivity growth was recorded. Finally, this study was able\nto ascertain the anticipated equivalency of outcome of the three dimensions of\nperformance.\n"
    },
    {
        "paper_id": 2005.09605,
        "authors": "Brantly Callaway and Tong Li",
        "title": "Evaluating Policies Early in a Pandemic: Bounding Policy Effects with\n  Nonrandomly Missing Data",
        "comments": "shortened abstract, fixed typos",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  During the early part of the Covid-19 pandemic, national and local\ngovernments introduced a number of policies to combat the spread of Covid-19.\nIn this paper, we propose a new approach to bound the effects of such\nearly-pandemic policies on Covid-19 cases and other outcomes while dealing with\ncomplications arising from (i) limited availability of Covid-19 tests, (ii)\ndifferential availability of Covid-19 tests across locations, and (iii)\neligibility requirements for individuals to be tested. We use our approach\nstudy the effects of Tennessee's expansion of Covid-19 testing early in the\npandemic and find that the policy decreased Covid-19 cases.\n"
    },
    {
        "paper_id": 2005.09794,
        "authors": "Guang Zhang",
        "title": "Pairs Trading with Nonlinear and Non-Gaussian State Space Models",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper studies pairs trading using a nonlinear and non-Gaussian\nstate-space model framework. We model the spread between the prices of two\nassets as an unobservable state variable and assume that it follows a\nmean-reverting process. This new model has two distinctive features: (1) The\ninnovations to the spread is non-Gaussianity and heteroskedastic. (2) The mean\nreversion of the spread is nonlinear. We show how to use the filtered spread as\nthe trading indicator to carry out statistical arbitrage. We also propose a new\ntrading strategy and present a Monte Carlo based approach to select the optimal\ntrading rule. As the first empirical application, we apply the new model and\nthe new trading strategy to two examples: PEP vs KO and EWT vs EWH. The results\nshow that the new approach can achieve a 21.86% annualized return for the\nPEP/KO pair and a 31.84% annualized return for the EWT/EWH pair. As the second\nempirical application, we consider all the possible pairs among the largest and\nthe smallest five US banks listed on the NYSE. For these pairs, we compare the\nperformance of the proposed approach with that of the existing popular\napproaches, both in-sample and out-of-sample. Interestingly, we find that our\napproach can significantly improve the return and the Sharpe ratio in almost\nall the cases considered.\n"
    },
    {
        "paper_id": 2005.09958,
        "authors": "Jos\\'e Vin\\'icius de Miranda Cardoso and Daniel P. Palomar",
        "title": "Learning Undirected Graphs in Financial Markets",
        "comments": "5 pages, 13 figures, accepted at Asilomar Conference on Signals,\n  Systems, and Computers, 2020",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  We investigate the problem of learning undirected graphical models under\nLaplacian structural constraints from the point of view of financial market\ndata. We show that Laplacian constraints have meaningful physical\ninterpretations related to the market index factor and to the conditional\ncorrelations between stocks. Those interpretations lead to a set of guidelines\nthat users should be aware of when estimating graphs in financial markets. In\naddition, we propose algorithms to learn undirected graphs that account for\nstylized facts and tasks intrinsic to financial data such as non-stationarity\nand stock clustering.\n"
    },
    {
        "paper_id": 2005.09974,
        "authors": "Sergio Alvares Maffra, John Armstrong, Teemu Pennanen",
        "title": "Stochastic modeling of assets and liabilities with mortality risk",
        "comments": null,
        "journal-ref": null,
        "doi": "10.1111/j.1365-2966.2005.09974.x",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper describes a general approach for stochastic modeling of assets\nreturns and liability cash-flows of a typical pensions insurer. On the asset\nside, we model the investment returns on equities and various classes of\nfixed-income instruments including short- and long-maturity fixed-rate bonds as\nwell as index-linked and corporate bonds. On the liability side, the risks are\ndriven by future mortality developments as well as price and wage inflation.\nAll the risk factors are modeled as a multivariate stochastic process that\ncaptures the dynamics and the dependencies across different risk factors. The\nmodel is easy to interpret and to calibrate to both historical data and to\nforecasts or expert views concerning the future. The simple structure of the\nmodel allows for efficient computations. The construction of a million\nscenarios takes only a few minutes on a personal computer. The approach is\nillustrated with an asset-liability analysis of a defined benefit pension fund.\n"
    },
    {
        "paper_id": 2005.0998,
        "authors": "Nils K\\\"obis, Luca Mossink",
        "title": "Artificial Intelligence versus Maya Angelou: Experimental evidence that\n  people cannot differentiate AI-generated from human-written poetry",
        "comments": "Computers in Human Behavior 2020",
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
        "abstract": "  The release of openly available, robust natural language generation\nalgorithms (NLG) has spurred much public attention and debate. One reason lies\nin the algorithms' purported ability to generate human-like text across various\ndomains. Empirical evidence using incentivized tasks to assess whether people\n(a) can distinguish and (b) prefer algorithm-generated versus human-written\ntext is lacking. We conducted two experiments assessing behavioral reactions to\nthe state-of-the-art Natural Language Generation algorithm GPT-2 (Ntotal =\n830). Using the identical starting lines of human poems, GPT-2 produced samples\nof poems. From these samples, either a random poem was chosen\n(Human-out-of-the-loop) or the best one was selected (Human-in-the-loop) and in\nturn matched with a human-written poem. In a new incentivized version of the\nTuring Test, participants failed to reliably detect the\nalgorithmically-generated poems in the Human-in-the-loop treatment, yet\nsucceeded in the Human-out-of-the-loop treatment. Further, people reveal a\nslight aversion to algorithm-generated poetry, independent on whether\nparticipants were informed about the algorithmic origin of the poem\n(Transparency) or not (Opacity). We discuss what these results convey about the\nperformance of NLG algorithms to produce human-like text and propose\nmethodologies to study such learning algorithms in human-agent experimental\nsettings.\n"
    },
    {
        "paper_id": 2005.10064,
        "authors": "Joaquin Fernandez-Tapia, Olivier Gu\\'eant",
        "title": "Recipes for hedging exotics with illiquid vanillas",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper, we address the question of the optimal Delta and Vega hedging\nof a book of exotic options when there are execution costs associated with the\ntrading of vanilla options. In a framework where exotic options are priced\nusing a market model (e.g. a local volatility model recalibrated continuously\nto vanilla option prices) and vanilla options prices are driven by a stochastic\nvolatility model, we show that, using simple approximations, the optimal\ndynamic Delta and Vega hedging strategies can be computed easily using\nvariational techniques.\n"
    },
    {
        "paper_id": 2005.101,
        "authors": "Rene Scheurwater",
        "title": "Reduction of valuation risk by Kalman filtering in business valuation\n  models",
        "comments": "23 pages, 6 figures, 3 tables",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  A recursive free cash flow model (FCFF) is proposed to determine the\ncorporate value of a company in an efficient market in which new market and\ncompany-specific information is modelled by additive white noise. The\nstochastic equations of the FCFF model are solved explicitly to obtain the\naverage corporate value and valuation risk. It is pointed out that valuation\nrisk can be reduced significantly by implementing a conventional two-step\nKalman filter in the recursive FCFF model, thus improving its predictive power.\nSystematic errors of the Kalman filter, caused by intermediate changes in risk\nand hence in the weighted average cost of capital (WACC), are detected by\nmeasuring the residuals. By including an additional adjustment step in the\nconventional Kalman filtering algorithm, it is shown that systematic errors can\nbe eliminated by recursively adjusting the WACC. The performance of the\nthree-step adaptive Kalman filter is tested by Monte Carlo simulation which\ndemonstrates the reliability and robustness against systematic errors. It is\nalso proved that the conventional and adaptive Kalman filtering algorithms can\nbe implemented into other valuation models such as the economic value added\nmodel (EVA) and free cash flow to equity model (FCFE).\n"
    },
    {
        "paper_id": 2005.1013,
        "authors": "Lorenzo Mercuri and Andrea Perchiazzo and Edit Rroji",
        "title": "Finite Mixture Approximation of CARMA(p,q) Models",
        "comments": "30 Pages, 13 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper we show how to approximate the transition density of a CARMA(p,\nq) model driven by means of a time changed Brownian Motion based on the\nGauss-Laguerre quadrature. We then provide an analytical formula for option\nprices when the log price follows a CARMA(p, q) model. We also propose an\nestimation procedure based on the approximated likelihood density.\n"
    },
    {
        "paper_id": 2005.10154,
        "authors": "Zura Kakushadze and Jim Kyung-Soo Liew",
        "title": "Coronavirus: Case for Digital Money?",
        "comments": "12 pages",
        "journal-ref": "World Economics 21(1) (2020) 177-190",
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We discuss the pros of adopting government-issued digital currencies as well\nas a supranational digital iCurrency. One such pro is to get rid of paper money\n(and coinage), a ubiquitous medium for spreading germs, as highlighted by the\nrecent coronavirus outbreak. We set forth three policy recommendations for\nadapting mobile devices as new digital wallets, regulatory oversight of\nsovereign digital currencies and user data protection, and a supranational\ndigital iCurrency for facilitating international digital monetary linkages.\n"
    },
    {
        "paper_id": 2005.10158,
        "authors": "David M. Kryskowski and David Kryskowski",
        "title": "Applying the Nash Bargaining Solution for a Reasonable Royalty",
        "comments": "10 pages, 7 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  There has been limited success applying the Nash Bargaining Solution (NBS) in\nassigning intellectual property damages due to the difficulty of relating it to\nthe specific facts of the case. Because of this, parties are not taking\nadvantage of Georgia-Pacific factor fifteen. This paper intends to bring\nclarity to the NBS so it can be applied to the facts of a case. This paper\nnormalizes the NBS and provides a methodology for determining the bargaining\nweight in Nash's solution. Several examples demonstrate this normalized form,\nand a nomograph is added for computational ease.\n"
    },
    {
        "paper_id": 2005.10488,
        "authors": "Takanobu Mizuta",
        "title": "Does an artificial intelligence perform market manipulation with its own\n  discretion? -- A genetic algorithm learns in an artificial market simulation",
        "comments": null,
        "journal-ref": "2020 IEEE Symposium Series on Computational Intelligence (SSCI)",
        "doi": "10.1109/SSCI47803.2020.9308349",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Who should be charged with responsibility for an artificial intelligence\nperforming market manipulation have been discussed. In this study, I\nconstructed an artificial intelligence using a genetic algorithm that learns in\nan artificial market simulation, and investigated whether the artificial\nintelligence discovers market manipulation through learning with an artificial\nmarket simulation despite a builder of artificial intelligence has no intention\nof market manipulation. As a result, the artificial intelligence discovered\nmarket manipulation as an optimal investment strategy. This result suggests\nnecessity of regulation, such as obligating builders of artificial intelligence\nto prevent artificial intelligence from performing market manipulation.\n"
    },
    {
        "paper_id": 2005.10504,
        "authors": "T. van der Zwaard, L.A. Grzelak, C.W. Oosterlee",
        "title": "A Computational Approach to Hedging Credit Valuation Adjustment in a\n  Jump-Diffusion Setting",
        "comments": null,
        "journal-ref": null,
        "doi": "10.1016/j.amc.2020.125671",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This study contributes to understanding Valuation Adjustments (xVA) by\nfocussing on the dynamic hedging of Credit Valuation Adjustment (CVA),\ncorresponding Profit & Loss (P&L) and the P&L explain. This is done in a Monte\nCarlo simulation setting, based on a theoretical hedging framework discussed in\nexisting literature. We look at hedging CVA market risk for a portfolio with\nEuropean options on a stock, first in a Black-Scholes setting, then in a Merton\njump-diffusion setting. Furthermore, we analyze the trading business at a bank\nafter including xVAs in pricing. We provide insights into the hedging of\nderivatives and their xVAs by analyzing and visualizing the cash-flows of a\nportfolio from a desk structure perspective. The case study shows that not\ncharging CVA at trade inception results in an expected loss. Furthermore,\nhedging CVA market risk is crucial to end up with a stable trading strategy. In\nthe Black-Scholes setting this can be done using the underlying stock, whereas\nin the Merton jump-diffusion setting we need to add extra options to the hedge\nportfolio to properly hedge the jump risk. In addition to the simulation, we\nderive analytical results that explain our observations from the numerical\nexperiments. Understanding the hedging of CVA helps to deal with xVAs in a\npractical setting.\n"
    },
    {
        "paper_id": 2005.10568,
        "authors": "Patrick Chang, Etienne Pienaar, Tim Gebbie",
        "title": "Using the Epps effect to detect discrete processes",
        "comments": "Updated with enhanced discussion and context. 18 pages, 14 figures, 1\n  table. Link to our supporting Julia code:\n  https://github.com/CHNPAT005/PCEPTG-EC",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The Epps effect is key phenomenology relating to high frequency correlation\ndynamics in financial markets. We argue that it can be used to provide insight\ninto whether tick data is best represented as samples from Brownian diffusions,\nor as samples from truly discrete events represented as connected point\nprocesses. We derive the Epps effect arising from asynchrony and provide a\nrefined method to correct for the effect. We then propose three experiments\nwhich show how to discriminate between possible underlying representations.\nThese in turn demonstrate how a simple Hawkes representation recovers\nphenomenology reported in the literature that cannot be recovered using a\nBrownian representation without additional ad hoc model complexity. However,\ncomplex ad hoc noise models built on Brownian motions cannot in general be\ndiscriminated relative to a Hawkes representation. Nevertheless, we argue that\nhigh frequency correlation dynamics are most faithfully recovered when tick\ndata is represented as a web of interconnected discrete events rather than\nbeing samples from continuous Brownian diffusions even when combined with\nnoise.\n"
    },
    {
        "paper_id": 2005.10585,
        "authors": "Anton Pichler, Marco Pangallo, R. Maria del Rio-Chanona, Fran\\c{c}ois\n  Lafond, J. Doyne Farmer",
        "title": "Production networks and epidemic spreading: How to restart the UK\n  economy?",
        "comments": "73 pages, 24 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We analyse the economics and epidemiology of different scenarios for a phased\nrestart of the UK economy. Our economic model is designed to address the unique\nfeatures of the COVID-19 pandemic. Social distancing measures affect both\nsupply and demand, and input-output constraints play a key role in restricting\neconomic output. Standard models for production functions are not adequate to\nmodel the short-term effects of lockdown. A survey of industry analysts\nconducted by IHS Markit allows us to evaluate which inputs for each industry\nare absolutely necessary for production over a two month period. Our model also\nincludes inventory dynamics and feedback between unemployment and consumption.\nWe demonstrate that economic outcomes are very sensitive to the choice of\nproduction function, show how supply constraints cause strong network effects,\nand find some counter-intuitive effects, such as that reopening only a few\nindustries can actually lower aggregate output. Occupation-specific data and\ncontact surveys allow us to estimate how different industries affect the\ntransmission rate of the disease. We investigate six different re-opening\nscenarios, presenting our best estimates for the increase in R0 and the\nincrease in GDP. Our results suggest that there is a reasonable compromise that\nyields a relatively small increase in R0 and delivers a substantial boost in\neconomic output. This corresponds to a situation in which all non-consumer\nfacing industries reopen, schools are open only for workers who need childcare,\nand everyone who can work from home continues to work from home.\n"
    },
    {
        "paper_id": 2005.10603,
        "authors": "Makoto Naraoka, Teruaki Hayashi, Takaaki Yoshino, Toshiaki Sugie, Kota\n  Takano, Yukio Ohsawa",
        "title": "Detecting and explaining changes in various assets' relationships in\n  financial markets",
        "comments": "12 pages, 6 figures",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We study the method for detecting relationship changes in financial markets\nand providing human-interpretable network visualization to support the\ndecision-making of fund managers dealing with multi-assets. First, we construct\nco-occurrence networks with each asset as a node and a pair with a strong\nrelationship in price change as an edge at each time step. Second, we calculate\nGraph-Based Entropy to represent the variety of price changes based on the\nnetwork. Third, we apply the Differential Network to finance, which is\ntraditionally used in the field of bioinformatics. By the method described\nabove, we can visualize when and what kind of changes are occurring in the\nfinancial market, and which assets play a central role in changes in financial\nmarkets. Experiments with multi-asset time-series data showed results that were\nwell fit with actual events while maintaining high interpretability. It is\nsuggested that this approach is useful for fund managers to use as a new option\nfor decision making.\n"
    },
    {
        "paper_id": 2005.1066,
        "authors": "Juan Li, Wenqiang Li, Gechun Liang",
        "title": "A game theoretical approach to homothetic robust forward investment\n  performance processes in stochastic factor models",
        "comments": "36 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper studies an optimal forward investment problem in an incomplete\nmarket with model uncertainty, in which the underlying stocks depend on the\ncorrelated stochastic factors. The uncertainty stems from the probability\nmeasure chosen by an investor to evaluate the performance. We obtain directly\nthe representation of the homothetic robust forward performance processes in\nfactor-form by combining the zero-sum stochastic differential game and ergodic\nBSDE approach. We also establish the connections with the risk-sensitive\nzero-sum stochastic differential games over an infinite horizon with ergodic\npayoff criteria, as well as with the classical robust expected utilities for\nlong time horizons. Finally, we give an example to illustrate that our approach\ncan be applied to address a type of robust forward investment performance\nprocesses with negative realization processes.\n"
    },
    {
        "paper_id": 2005.10661,
        "authors": "Xiao Xu",
        "title": "The optimal investment strategy of a DC pension plan under deposit loan\n  spread and the O-U process",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper is devoted to invest an optimal investment strategy for a\ndefined-contribution (DC) pension plan under the Ornstein-Uhlenbeck (O-U)\nprocess and the loan. By considering risk-free asset, a risky asset driven by\nO-U process and a loan in the financial market, we firstly set up the dynamic\nequation and the asset market model which are instrumental in achieving the\nexpected utility of ultimate wealth at retirement. Secondly, the corresponding\nHamilton-Jacobi-Bellman(HJB) equation is derived by means of dynamic\nprogramming principle. The explicit expression for the optimal investment\nstrategy is obtained by Legendre transform method. Finally, different\nparameters are selected to simulate the explicit solution and the financial\ninterpretation of the optimal investment strategy is given.\n"
    },
    {
        "paper_id": 2005.10966,
        "authors": "Narayan Ganesan, Yajie Yu, Bernhard Hientzsch",
        "title": "Pricing Barrier Options with DeepBSDEs",
        "comments": "20 pages",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper presents a novel and direct approach to price boundary and\nfinal-value problems, corresponding to barrier options, using forward deep\nlearning to solve forward-backward stochastic differential equations (FBSDEs).\nBarrier instruments are instruments that expire or transform into another\ninstrument if a barrier condition is satisfied before maturity; otherwise they\nperform like the instrument without the barrier condition. In the PDE\nformulation, this corresponds to adding boundary conditions to the final value\nproblem. The deep BSDE methods developed so far have not addressed\nbarrier/boundary conditions directly. We extend the forward deep BSDE to the\nbarrier condition case by adding nodes to the computational graph to explicitly\nmonitor the barrier conditions for each realization of the dynamics as well as\nnodes that preserve the time, state variables, and trading strategy value at\nbarrier breach or at maturity otherwise. Given these additional nodes in the\ncomputational graph, the forward loss function quantifies the replication of\nthe barrier or final payoff according to a chosen risk measure such as squared\nsum of differences. The proposed method can handle any barrier condition in the\nFBSDE set-up and any Dirichlet boundary conditions in the PDE set-up, both in\nlow and high dimensions.\n"
    },
    {
        "paper_id": 2005.11022,
        "authors": "Philippe Artzner and Karl-Theodor Eisele and Thorsten Schmidt",
        "title": "Insurance-Finance Arbitrage",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Most insurance contracts are inherently linked to financial markets, be it\nvia interest rates, or -- as hybrid products like equity-linked life insurance\nand variable annuities -- directly to stocks or indices. However, insurance\ncontracts are not for trade except sometimes as surrender to the selling\noffice. This excludes the situation of arbitrage by buying and selling\ninsurance contracts at different prices. Furthermore, the insurer uses private\ninformation on top of the publicly available one about financial market. This\npaper provides a study of the consistency of insurance contracts in connection\nwith trades in the financial market with explicit mention of the information\ninvolved.\n  By defining strategies on an insurance portfolio and combining them with\nfinancial trading strategies, we arrive at the notion of insurance-finance\narbitrage (IFA). In analogy to the classical fundamental theorem of asset\npricing, we give a fundamental theorem on the absence of IFA, leading to the\nexistence of an insurance-finance-consistent probability. In addition, we study\nwhen this probability gives the expected discounted cash-flows required by the\nEIOPA best estimate.\n  The generality of our approach allows to incorporate many important aspects,\nlike mortality risk or general levels of dependence between mortality and stock\nmarkets. Utilizing the theory of enlargements of filtrations, we construct a\ntractable framework for insurance-finance consistent valuation.\n"
    },
    {
        "paper_id": 2005.11233,
        "authors": "Jacek Bia{\\l}ek, Maciej Ber\\k{e}sewicz",
        "title": "Scanner data in inflation measurement: from raw data to price indices",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Scanner data offer new opportunities for CPI or HICP calculation. They can be\nobtained from a~wide variety of~retailers (supermarkets, home electronics,\nInternet shops, etc.) and provide information at the level of~the barcode. One\nof~advantages of~using scanner data is the fact that they contain complete\ntransaction information, i.e. prices and quantities for every sold item. To use\nscanner data, it must be carefully processed. After clearing data and unifying\nproduct names, products should be carefully classified (e.g. into COICOP 5 or\nbelow), matched, filtered and aggregated. These procedures often require\ncreating new IT or writing custom scripts (R, Python, Mathematica, SAS,\nothers). One of~new challenges connected with scanner data is the appropriate\nchoice of~the index formula. In this article we present a~proposal for the\nimplementation of~individual stages of~handling scanner data. We also point out\npotential problems during scanner data processing and their solutions. Finally,\nwe compare a~large number of~price index methods based on real scanner datasets\nand we verify their sensitivity on adopted data filtering and aggregating\nmethods.\n"
    },
    {
        "paper_id": 2005.11285,
        "authors": "Fernando DePaolis, Phil Murphy, M. Clara DePaolis Kaluza",
        "title": "Identifying Key Sectors in the Regional Economy: A Network Analysis\n  Approach Using Input-Output Data",
        "comments": "under consideration for publishing elsewhere",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  By applying network analysis techniques to large input-output system, we\nidentify key sectors in the local/regional economy. We overcome the limitations\nof traditional measures of centrality by using random-walk based measures, as\nan extension of Blochl et al. (2011). These are more appropriate to analyze\nvery dense networks, i.e. those in which most nodes are connected to all other\nnodes. These measures also allow for the presence of recursive ties (loops),\nsince these are common in economic systems (depending to the level of\naggregation, most firms buy from and sell to other firms in the same industrial\nsector). The centrality measures we present are well suited for capturing\nsectoral effects missing from the usual output and employment multipliers. We\nalso develop an R package (xtranat) for the processing of data from IMPLAN(R)\nmodels and for computing the newly developed measures.\n"
    },
    {
        "paper_id": 2005.11318,
        "authors": "Reto Hofstetter, Klaus M. Miller, Harley Krohmer, Z. John Zhang",
        "title": "A De-biased Direct Question Approach to Measuring Consumers' Willingness\n  to Pay",
        "comments": "Market Research, Pricing, Demand Estimation, Direct Estimation,\n  Single Question Approach, Choice Experiments, Willingness to Pay,\n  Hypothetical Bias",
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Knowledge of consumers' willingness to pay (WTP) is a prerequisite to\nprofitable price-setting. To gauge consumers' WTP, practitioners often rely on\na direct single question approach in which consumers are asked to explicitly\nstate their WTP for a product. Despite its popularity among practitioners, this\napproach has been found to suffer from hypothetical bias. In this paper, we\npropose a rigorous method that improves the accuracy of the direct single\nquestion approach. Specifically, we systematically assess the hypothetical\nbiases associated with the direct single question approach and explore ways to\nde-bias it. Our results show that by using the de-biasing procedures we\npropose, we can generate a de-biased direct single question approach that is\naccu-rate enough to be useful for managerial decision-making. We validate this\napproach with two studies in this paper.\n"
    },
    {
        "paper_id": 2005.115,
        "authors": "Neha Deopa and Daniele Rinaldo",
        "title": "Quickest Detection of Ecological Regimes for Natural Resource Management",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "abstract": "  We study the stochastic dynamics of natural resources under the threat of\necological regime shifts. We establish a Pareto optimal framework of regime\nshift detection under uncertainty that minimizes the delay with which economic\nagents become aware of the shift. We integrate ecosystem surveillance in the\nformation of optimal resource extraction policies. We fully solve the case of a\nprofit-maximizing monopolist, study its response to regime shift detection and\nshow the generality of our framework by extending our results to other decision\nmakers and functional forms.We apply our framework to the case of the\nCantareira water reservoir in S\\~ao Paulo, Brazil, and study the events that\nled to its depletion and the consequent water supply crisis.\n"
    },
    {
        "paper_id": 2005.11669,
        "authors": "Nusrat Jahan, K.M. Golam Muhiuddin",
        "title": "Evaluation of Banking Sectors Development in Bangladesh in light of\n  Financial Reform",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by-sa/4.0/",
        "abstract": "  Historically, the performance of the banking sector has been weak,\ncharacterized by weak asset quality, inadequate provisioning, and negative\ncapitalization of state-owned banks. To overcome these problems, the initial\nphase of banking reform (1980-1990) focused on the promotion of private\nownership and denationalization of nationalized commercial banks (SCBs). During\nthe second phase of reform, Financial Sector Reform Project (FSRP) of World\nBank was launched in 1990 with the focus on gradual deregulations of the\ninterest rate structure, providing market-oriented incentives for priority\nsector lending and improvement in the debt recovery environment. Moreover, a\nlarge number of private commercial banks were granted licenses during the\nsecond phase of reforms. Bangladesh Bank adopted Basel-I norms in 1996 and\nBasel-II during 2010. Moreover, the Central Bank Strengthening Project\ninitiated in 2003 focused on effective regulatory and supervisory system,\nparticularly strengthening the legal framework of banking sector. This study\nevaluates how successfully the banking sector of Bangladesh has evolved over\nthe past decades in light of financial reform measures undertaken to strengthen\nthis sector.\n"
    },
    {
        "paper_id": 2005.11748,
        "authors": "Dhruv Sharma, Jean-Philippe Bouchaud, Marco Tarzia, Francesco Zamponi",
        "title": "Good speciation and endogenous business cycles in a constraint\n  satisfaction macroeconomic model",
        "comments": "14 Pages, 11 Figures. Updated Journal Reference",
        "journal-ref": "J. Stat. Mech. (2021) 063403",
        "doi": "10.1088/1742-5468/ac014a",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We introduce a prototype agent-based model of the macroeconomy, with\nbudgetary constraints at its core. The model is related to a class of\nconstraint satisfaction problems (CSPs), which has been thoroughly investigated\nin computer science. The CSP paradigm allows us to propose an alternative\nprice-setting mechanism: given agents' preferences and budgets, what set of\nprices satisfies the maximum number of agents? Such an approach permits the\ncoupling of production and output within the economy to the allowed level of\ndebt in a simplified framework. Within our model, we identify three different\nregimes upon varying the amount of debt that each agent can accumulate before\ndefaulting. In presence of a very loose constraint on debt, endogenous crises\nleading to waves of synchronized bankruptcies are present. In the opposite\nregime of very tight debt constraining, the bankruptcy rate is extremely high\nand the economy remains structure-less. In an intermediate regime, the economy\nis stable with very low bankruptcy rate and no aggregate-level crises. This\nthird regime displays a rich phenomenology:the system spontaneously and\ndynamically self-organizes in a set of cheap and expensive goods (i.e. some\nkind of \"speciation\"), with switches triggered by random fluctuations and\nfeedback loops. Our analysis confirms the central role that debt levels play in\nthe stability of the economy. More generally, our model shows that constraints\nat the individual scale can generate highly complex patterns at the aggregate\nlevel.\n"
    },
    {
        "paper_id": 2005.12059,
        "authors": "Beatriz Salvador, Cornelis W. Oosterlee, Remco van der Meer",
        "title": "Financial option valuation by unsupervised learning with artificial\n  neural networks",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Artificial neural networks (ANNs) have recently also been applied to solve\npartial differential equations (PDEs). In this work, the classical problem of\npricing European and American financial options, based on the corresponding PDE\nformulations, is studied. Instead of using numerical techniques based on finite\nelement or difference methods, we address the problem using ANNs in the context\nof unsupervised learning. As a result, the ANN learns the option values for all\npossible underlying stock values at future time points, based on the\nminimization of a suitable loss function. For the European option, we solve the\nlinear Black-Scholes equation, whereas for the American option, we solve the\nlinear complementarity problem formulation. Two-asset exotic option values are\nalso computed, since ANNs enable the accurate valuation of high-dimensional\noptions. The resulting errors of the ANN approach are assessed by comparing to\nthe analytic option values or to numerical reference solutions (for American\noptions, computed by finite elements).\n"
    },
    {
        "paper_id": 2005.12102,
        "authors": "Riccardo Gianluigi Serio, Maria Michela Dickson, Diego Giuliani,\n  Giuseppe Espa",
        "title": "Green production as a factor of survival for innovative startups.\n  Evidence from Italy",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Many studies have analyzed empirically the determinants of survival for\ninnovative startup companies using data about the characteristics of\nentrepreneurs and management or focusing on firm- and industry-specific\nvariables. However, no attempts have been made so far to assess the role of the\nenvironmental sustainability of the production process. Based on data\ndescribing the characteristics of the Italian innovative startups in the period\n2009-2018, this article studies the differences in survival between green and\nnon-green companies. We show that, while controlling for other confounding\nfactors, startups characterized by a green production process tend to survive\nlonger than their counterparts. In particular, we estimate that a green\ninnovative startup is more than twice as likely to survive than a non-green\none. This evidence may support the idea that environment sustainability can\nhelp economic development.\n"
    },
    {
        "paper_id": 2005.12173,
        "authors": "Andrey Leonidov, Ilya Tipunin, Ekaterina Serebryannikova",
        "title": "On Evaluation of Risky Investment Projects. Investment Certainty\n  Equivalence",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The purpose of the study is to propose a methodology for evaluation and\nranking of risky investment projects.An investment certainty equivalence\napproach dual to the conventional separation of riskless and risky\ncontributions based on cash flow certainty equivalence is introduced. Proposed\nranking of investment projects is based on gauging them with the Omega measure,\nwhich is defined as the ratio of chances to obtain profit/return greater than\nsome critical (minimal acceptable) profitability over the chances to obtain the\nprofit/return less than the critical one.Detailed consideration of alternative\nriskless investment is presented. Various performance measures characterizing\ninvestment projects with a special focus on the role of reinvestment are\ndiscussed. Relation between the proposed methodology and the conventional\napproach based on utilization of risk-adjusted discount rate (RADR) is\ndiscussed. Findings are supported with an illustrative example.The methodology\nproposed can be used to rank projects of different nature, scale and lifespan.\nIn contrast to the conventional RADR approach for investment project\nevaluation, in the proposed method a risk profile of a specific project is\nexplicitly analyzed in terms of appropriate performance measure distribution.\nNo ad-hoc assumption about suitable risk-premium is made.\n"
    }
]